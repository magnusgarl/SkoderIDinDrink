{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moderate-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "potential-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file into a pandas DataFrame\n",
    "combined_df = pd.read_csv(\"journal_merged.csv\")\n",
    "\n",
    "# Step 2: Rename the column \"Introduction\" to \"Intro\"\n",
    "combined_df.rename(columns={\"Introduction\": \"Intro\"}, inplace=True)\n",
    "\n",
    "# Step 3: Drop rows where the \"Intro\" column has NaN values\n",
    "combined_df.dropna(subset=['Intro'], inplace=True)\n",
    "\n",
    "# Save the modified dataframe to a new CSV if needed\n",
    "# df.to_csv(\"/path_to_save/new_journal_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "possible-wilson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records for cleaning.\n",
      "Processed 20000 records for cleaning.\n",
      "Processed 30000 records for cleaning.\n",
      "Processed 40000 records for cleaning.\n",
      "Processed 50000 records for cleaning.\n",
      "Processed 60000 records for cleaning.\n",
      "Processed 70000 records for cleaning.\n",
      "Processed 80000 records for cleaning.\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already loaded combined_df somewhere above in your code\n",
    "\n",
    "# Make a copy of the original dataframe to work on\n",
    "df_cleaned = combined_df.copy()\n",
    "\n",
    "# Set up the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove content between parentheses using regex\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags/markup\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove punctuations, numbers and other non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize_text(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Process tracking counter\n",
    "counter = 0\n",
    "\n",
    "# Apply the cleaning function\n",
    "def process_and_track(func, text):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter % 10000 == 0:\n",
    "        print(f\"Processed {counter} records for cleaning.\")\n",
    "    return func(text)\n",
    "\n",
    "# Apply the cleaning function\n",
    "df_cleaned['Intro_1'] = df_cleaned['Intro'].apply(lambda x: process_and_track(clean_text, x))\n",
    "df_cleaned['Intro_Cleaned'] = df_cleaned['Intro'].apply(lambda x: process_and_track(clean_text, x))\n",
    "# Apply removal of stop words\n",
    "df_cleaned['Intro_2'] = df_cleaned['Intro'].apply(lambda x: process_and_track(remove_stop_words, x))\n",
    "df_cleaned['Intro_Cleaned'] = df_cleaned['Intro_Cleaned'].apply(lambda x: process_and_track(remove_stop_words, x))\n",
    "df_cleaned['Intro_3'] = df_cleaned['Intro_1'].apply(lambda x: process_and_track(remove_stop_words, x))\n",
    "# Apply lemmatization\n",
    "df_cleaned['Intro_Cleaned'] = df_cleaned['Intro_Cleaned'].apply(lambda x: process_and_track(lemmatize_text, x))\n",
    "\n",
    "# Save cleaned version\n",
    "df_cleaned.to_csv('data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-airline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import resample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-defensive",
   "metadata": {},
   "source": [
    "# UNIGRAM & BIGRAM with/without dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-structure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column Intro_Cleaned without dummies...\n",
      "Processing for ngram (1, 1)...\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv('data_cleaned.csv')\n",
    "\n",
    "def process_column_both_grams(column_name, data):\n",
    "    print(f\"Processing column {column_name} without dummies...\")\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "    results = {}\n",
    "    \n",
    "    for ngram, label in [((1, 1), \"ug\"), ((1, 2), \"bg\")]:\n",
    "        print(f\"Processing for ngram {ngram}...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=ngram)\n",
    "        X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Oversampling to address class imbalance\n",
    "        data_female = data_cleaned[data_cleaned['Article_Gender'] == 'Female']\n",
    "        data_male = data_cleaned[data_cleaned['Article_Gender'] == 'Male']\n",
    "        data_female_oversampled = resample(data_female, replace=True, n_samples=len(data_male), random_state=42)\n",
    "        data_oversampled = pd.concat([data_male, data_female_oversampled])\n",
    "        X_oversampled = tfidf_vectorizer.transform(data_oversampled[column_name])\n",
    "        y_oversampled = data_oversampled['Article_Gender']\n",
    "        X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Hyperparameter tuning for Logistic Regression\n",
    "        logreg_params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        grid_search_logreg = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga'),\n",
    "                                          logreg_params, \n",
    "                                          cv=5, \n",
    "                                          scoring='accuracy', \n",
    "                                          n_jobs=-1)\n",
    "        grid_search_logreg.fit(X_train_os, y_train_os)\n",
    "        best_params_logreg = grid_search_logreg.best_params_\n",
    "        best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "        # Hyperparameter tuning optimized for predicting 'Female'\n",
    "        custom_scorer = make_scorer(precision_score, pos_label='Female', zero_division=0)\n",
    "        param_grid_custom = {\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "        }\n",
    "        grid_search_custom = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga'), param_grid_custom, scoring=custom_scorer, cv=5)\n",
    "        grid_search_custom.fit(X, y)\n",
    "        best_params_custom = grid_search_custom.best_params_\n",
    "        best_score_custom = grid_search_custom.best_score_\n",
    "\n",
    "        results[label] = {\n",
    "            f'best_params_logreg_{label}': best_params_logreg,\n",
    "            f'best_score_logreg_{label}': best_score_logreg,\n",
    "            f'best_params_custom_{label}': best_params_custom,\n",
    "            f'best_score_custom_{label}': best_score_custom\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "def process_column_both_grams_with_dummies(column_name, data):\n",
    "    print(f\"Processing column {column_name} with dummies...\")\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "    journal_dummies = pd.get_dummies(data_cleaned['Journal Name'], prefix='Journal')\n",
    "    results = {}\n",
    "\n",
    "    for ngram, label in [((1, 1), \"ug\"), ((1, 2), \"bg\")]:\n",
    "        print(f\"Processing for ngram {ngram}...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=ngram)\n",
    "        X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "        X_combined = hstack([X, journal_dummies.values])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Oversampling to address class imbalance\n",
    "        data_female = data_cleaned[data_cleaned['Article_Gender'] == 'Female']\n",
    "        data_male = data_cleaned[data_cleaned['Article_Gender'] == 'Male']\n",
    "        data_female_oversampled = resample(data_female, replace=True, n_samples=len(data_male), random_state=42)\n",
    "        data_oversampled = pd.concat([data_male, data_female_oversampled])\n",
    "        X_oversampled = tfidf_vectorizer.transform(data_oversampled[column_name])\n",
    "        X_combined_oversampled = hstack([X_oversampled, journal_dummies.loc[data_oversampled.index].values])\n",
    "        y_oversampled = data_oversampled['Article_Gender']\n",
    "        X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_combined_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Hyperparameter tuning for Logistic Regression\n",
    "        logreg_params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        grid_search_logreg = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga'),\n",
    "                                          logreg_params, \n",
    "                                          cv=5, \n",
    "                                          scoring='accuracy', \n",
    "                                          n_jobs=-1)\n",
    "        grid_search_logreg.fit(X_train_os, y_train_os)\n",
    "        best_params_logreg = grid_search_logreg.best_params_\n",
    "        best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "        # Hyperparameter tuning optimized for predicting 'Female'\n",
    "        custom_scorer = make_scorer(precision_score, pos_label='Female', zero_division=0)\n",
    "        param_grid_custom = {\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "        }\n",
    "        grid_search_custom = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga'), param_grid_custom, scoring=custom_scorer, cv=5)\n",
    "        grid_search_custom.fit(X_combined, y)\n",
    "        best_params_custom = grid_search_custom.best_params_\n",
    "        best_score_custom = grid_search_custom.best_score_\n",
    "\n",
    "        results[label] = {\n",
    "            f'best_params_logreg_{label}': best_params_logreg,\n",
    "            f'best_score_logreg_{label}': best_score_logreg,\n",
    "            f'best_params_custom_{label}': best_params_custom,\n",
    "            f'best_score_custom_{label}': best_score_custom\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "columns_to_process = ['Intro_Cleaned', 'Intro_1', 'Intro_2', 'Intro_3']\n",
    "\n",
    "results_all = {}\n",
    "\n",
    "for col in columns_to_process:\n",
    "    results_all[col] = process_column_both_grams(col, data)\n",
    "    results_all[f\"{col}_with_dummies\"] = process_column_both_grams_with_dummies(col, data)\n",
    "\n",
    "# Display the results in a structured manner\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "for col in columns_to_process:\n",
    "    print(f\"\\nResults without Dummies for '{col}':\")\n",
    "    pp.pprint(results_all[col])\n",
    "    print(f\"\\nResults with Dummies for '{col}':\")\n",
    "    pp.pprint(results_all[f\"{col}_with_dummies\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ranking-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# After obtaining the results\n",
    "with open('results_all.pkl', 'wb') as f:\n",
    "    pickle.dump(results_all, f)\n",
    "\n",
    "# ... Later, to retrieve the results\n",
    "with open('results_all.pkl', 'rb') as f:\n",
    "    results_all_retrieved = pickle.load(f)\n",
    "\n",
    "# Now you can use `results_all_retrieved` as if it were the original `results_all` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Accuracy Calculation\n",
    "baseline_accuracy_data_cleaned = max(data_cleaned['Article_Gender'].value_counts(normalize=True))\n",
    "baseline_accuracy_intro_1 = max(data['Intro_1'].value_counts(normalize=True))\n",
    "baseline_accuracy_intro_2 = max(data['Intro_2'].value_counts(normalize=True))\n",
    "baseline_accuracy_intro_3 = max(data['Intro_3'].value_counts(normalize=True))\n",
    "\n",
    "# List of accuracies for each model type\n",
    "labels = [\n",
    "    \"Unigram\", \n",
    "    \"Bigram\", \n",
    "    \"Unigram with Dummies\", \n",
    "    \"Bigram with Dummies\",\n",
    "    \"Baseline\"\n",
    "]\n",
    "\n",
    "def plot_accuracies(results, results_dummies, baseline_accuracy, title, ylabel):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.bar(labels, [\n",
    "        results['ug']['best_score_logreg_ug'], \n",
    "        results['bg']['best_score_logreg_bg'],\n",
    "        results_dummies['ug']['best_score_logreg_ug'], \n",
    "        results_dummies['bg']['best_score_logreg_bg'],\n",
    "        baseline_accuracy\n",
    "    ], color=['blue', 'green', 'purple', 'cyan', 'red'], alpha=0.7)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.ylim([0.5, 1.0])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1. Best Hyperparameter Tuned Accuracy for data_cleaned\n",
    "plot_accuracies(results_intro_cleaned, results_intro_cleaned_dummies, baseline_accuracy_data_cleaned, \n",
    "                \"Comparison of Best Hyperparameter Tuned Model Accuracies for data_cleaned\", \n",
    "                \"Best Tuned Accuracy\")\n",
    "\n",
    "# Best Hyperparameter Tuned Accuracy for Intro_1\n",
    "plot_accuracies(results_intro_1, results_intro_1_dummies, baseline_accuracy_intro_1, \n",
    "                \"Comparison of Best Hyperparameter Tuned Model Accuracies for Intro_1\", \n",
    "                \"Best Tuned Accuracy\")\n",
    "\n",
    "# Best Hyperparameter Tuned Accuracy for Intro_2\n",
    "plot_accuracies(results_intro_2, results_intro_2_dummies, baseline_accuracy_intro_2, \n",
    "                \"Comparison of Best Hyperparameter Tuned Model Accuracies for Intro_2\", \n",
    "                \"Best Tuned Accuracy\")\n",
    "\n",
    "# Best Hyperparameter Tuned Accuracy for Intro_3\n",
    "plot_accuracies(results_intro_3, results_intro_3_dummies, baseline_accuracy_intro_3, \n",
    "                \"Comparison of Best Hyperparameter Tuned Model Accuracies for Intro_3\", \n",
    "                \"Best Tuned Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "abroad-welcome",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_oversampled_ug' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-1f750f7421fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# List to hold datasets and corresponding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m datasets = [\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mX_oversampled_ug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oversampled_ug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_cleaned Unigram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mX_oversampled_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oversampled_bg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_cleaned Bigram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mX_oversampled_dum_ug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_oversampled_dum_ug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_cleaned Unigram with Dummies\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_oversampled_ug' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, cv=None):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming logreg_balanced is already defined\n",
    "logreg_balanced = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga')\n",
    "\n",
    "# List to hold datasets and corresponding labels\n",
    "datasets = [\n",
    "    (X_oversampled_ug, y_oversampled_ug, \"data_cleaned Unigram\"),\n",
    "    (X_oversampled_bg, y_oversampled_bg, \"data_cleaned Bigram\"),\n",
    "    (X_oversampled_dum_ug, y_oversampled_dum_ug, \"data_cleaned Unigram with Dummies\"),\n",
    "    (X_oversampled_dum_bg, y_oversampled_dum_bg, \"data_cleaned Bigram with Dummies\"),\n",
    "    (X_oversampled_ug_intro_1, y_oversampled_ug_intro_1, \"Intro_1 Unigram\"),\n",
    "    (X_oversampled_bg_intro_1, y_oversampled_bg_intro_1, \"Intro_1 Bigram\"),\n",
    "    (X_oversampled_dum_ug_intro_1, y_oversampled_dum_ug_intro_1, \"Intro_1 Unigram with Dummies\"),\n",
    "    (X_oversampled_dum_bg_intro_1, y_oversampled_dum_bg_intro_1, \"Intro_1 Bigram with Dummies\"),\n",
    "    (X_oversampled_ug_intro_2, y_oversampled_ug_intro_2, \"Intro_2 Unigram\"),\n",
    "    (X_oversampled_bg_intro_2, y_oversampled_bg_intro_2, \"Intro_2 Bigram\"),\n",
    "    (X_oversampled_dum_ug_intro_2, y_oversampled_dum_ug_intro_2, \"Intro_2 Unigram with Dummies\"),\n",
    "    (X_oversampled_dum_bg_intro_2, y_oversampled_dum_bg_intro_2, \"Intro_2 Bigram with Dummies\"),\n",
    "    (X_oversampled_ug_intro_3, y_oversampled_ug_intro_3, \"Intro_3 Unigram\"),\n",
    "    (X_oversampled_bg_intro_3, y_oversampled_bg_intro_3, \"Intro_3 Bigram\"),\n",
    "    (X_oversampled_dum_ug_intro_3, y_oversampled_dum_ug_intro_3, \"Intro_3 Unigram with Dummies\"),\n",
    "    (X_oversampled_dum_bg_intro_3, y_oversampled_dum_bg_intro_3, \"Intro_3 Bigram with Dummies\")\n",
    "]\n",
    "\n",
    "# Iterate through each dataset and plot\n",
    "for X_data, y_data, label in datasets:\n",
    "    # Plot learning curve\n",
    "    title = f\"Learning Curve (Logistic Regression, {label})\"\n",
    "    plot_learning_curve(logreg_balanced, title, X_data, y_data, cv=5)\n",
    "    plt.show()\n",
    "\n",
    "    # Validation curve for the C parameter in logistic regression\n",
    "    param_range = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga'),\n",
    "        X_data, y_data, param_name=\"C\", param_range=param_range, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.title(f\"Validation Curve with Logistic Regression ({label})\")\n",
    "    plt.xlabel(\"C\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.5, 1.1)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-ceramic",
   "metadata": {},
   "source": [
    "# Old version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "def process_column_both_grams(column_name, data):\n",
    "    print(f\"Processing column {column_name} without dummies...\")\n",
    "    # Step 1: Data Preprocessing\n",
    "    print(\"Step 1: Data Preprocessing...\")\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "\n",
    "    results = {}  # Dictionary to store results\n",
    "\n",
    "    for ngram, label in [((1, 1), \"ug\"), ((1, 2), \"bg\")]:\n",
    "        print(f\"Processing for ngram {ngram}...\")\n",
    "        # Step 2: Vectorization\n",
    "        print(\"Step 2: Vectorization...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=ngram)\n",
    "        X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "\n",
    "        # Step 3: Train-Test Split\n",
    "        print(\"Step 3: Train-Test Split...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Step 4: Model Building & Regularization\n",
    "        print(\"Step 4: Model Building & Regularization\")\n",
    "        logreg = LogisticRegression(max_iter=1000)\n",
    "        logreg.fit(X_train, y_train)\n",
    "\n",
    "        # Step 5: Basic Model Validation\n",
    "        print(\"Step 5: Basic Model Validation\")\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        basic_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Step 6: Cross-Validation\n",
    "        print(\"Step 6: Cross-Validation\")\n",
    "        cv_scores = cross_val_score(logreg, X, y, cv=5, scoring='accuracy')\n",
    "        avg_cv_accuracy = cv_scores.mean()\n",
    "\n",
    "        # Step 7: Addressing Class Imbalance & Feature Engineering\n",
    "        print(\"Step 7: Addressing Class Imbalance & Feature Engineering\")\n",
    "        data_female = data_cleaned[data_cleaned['Article_Gender'] == 'Female']\n",
    "        data_male = data_cleaned[data_cleaned['Article_Gender'] == 'Male']\n",
    "        data_female_oversampled = resample(data_female, replace=True, n_samples=len(data_male), random_state=42)\n",
    "        data_oversampled = pd.concat([data_male, data_female_oversampled])\n",
    "        X_oversampled = tfidf_vectorizer.transform(data_oversampled[column_name])\n",
    "        y_oversampled = data_oversampled['Article_Gender']\n",
    "        X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Step 8: Complex Models & Cost-sensitive Learning\n",
    "        print(\"Step 8: Complex Models & Cost-sensitive Learning\")\n",
    "        logreg_balanced = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "        logreg_balanced.fit(X_train_os, y_train_os)\n",
    "        y_pred_os = logreg_balanced.predict(X_test_os)\n",
    "        accuracy_os = accuracy_score(y_test_os, y_pred_os)\n",
    "\n",
    "        # Step 9: Performance Evaluation\n",
    "        print(\"Step 9: Performance Evaluation\")\n",
    "        evaluation_metrics_logreg = classification_report(y_test_os, y_pred_os, target_names=[\"Female\", \"Male\"])\n",
    "\n",
    "        # Step 10: Hyperparameter tuning for Logistic Regression\n",
    "        print(\"Step 10: Hyperparameter tuning for Logistic Regression\")\n",
    "        logreg_params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        grid_search_logreg = GridSearchCV(LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga'),\n",
    "                                          logreg_params, \n",
    "                                          cv=5, \n",
    "                                          scoring='accuracy', \n",
    "                                          n_jobs=-1)\n",
    "        grid_search_logreg.fit(X_train_os, y_train_os)\n",
    "        best_params_logreg = grid_search_logreg.best_params_\n",
    "        best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "        # Step 11: Hyperparameter tuning optimized for predicting 'Female'\n",
    "        print(\"Step 11: Hyperparameter tuning optimized for predicting 'Female'\")\n",
    "        custom_scorer = make_scorer(precision_score, pos_label='Female', zero_division=0)\n",
    "        model = LogisticRegression()\n",
    "        param_grid_custom = {\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'max_iter': [50, 400, 800, 1200]\n",
    "        }\n",
    "        grid_search_custom = GridSearchCV(model, param_grid_custom, scoring=custom_scorer, cv=5)\n",
    "        grid_search_custom.fit(X, y)\n",
    "        best_params_custom = grid_search_custom.best_params_\n",
    "        best_score_custom = grid_search_custom.best_score_\n",
    "\n",
    "        results[label] = {\n",
    "            f'basic_accuracy_{label}': basic_accuracy,\n",
    "            f'avg_cv_accuracy_{label}': avg_cv_accuracy,\n",
    "            f'accuracy_os_{label}': accuracy_os,\n",
    "            f'evaluation_metrics_logreg_{label}': evaluation_metrics_logreg,\n",
    "            f'best_params_logreg_{label}': best_params_logreg,\n",
    "            f'best_score_logreg_{label}': best_score_logreg,\n",
    "            f'best_params_custom_{label}': best_params_custom,\n",
    "            f'best_score_custom_{label}': best_score_custom\n",
    "        }\n",
    "\n",
    "    print(f\"Finished processing column {column_name} without dummies.\")\n",
    "    return results\n",
    "\n",
    "def process_column_both_grams_with_dummies(column_name, data):\n",
    "    print(f\"Processing column {column_name} with dummies...\")\n",
    "    # Step 1: Data Preprocessing\n",
    "    print(\"Step 1: Data Preprocessing...\")\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "    journal_dummies = pd.get_dummies(data_cleaned['Journal_name'], prefix='Journal')\n",
    "    results = {}\n",
    "\n",
    "    for ngram, label in [((1, 1), \"ug\"), ((1, 2), \"bg\")]:\n",
    "        print(f\"Processing for ngram {ngram}...\")\n",
    "        # Step 2: Vectorization\n",
    "        print(\"Step 2: Vectorization...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=ngram)\n",
    "        X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "        X_combined = hstack([X, journal_dummies.values])\n",
    "\n",
    "        # Step 3: Train-Test Split\n",
    "        print(\"Step 3: Train-Test Split...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Step 4: Model Building & Regularization\n",
    "        print(\"Step 4: Model Building & Regularization...\")\n",
    "        logreg = LogisticRegression(max_iter=1000)\n",
    "        logreg.fit(X_train, y_train)\n",
    "\n",
    "        # Step 5: Basic Model Validation\n",
    "        print(\"Step 5: Basic Model Validation...\")\n",
    "        y_pred = logreg.predict(X_test)\n",
    "        basic_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # Step 6: Cross-Validation\n",
    "        print(\"Step 6: Cross-Validation...\")\n",
    "        cv_scores = cross_val_score(logreg, X_combined, y, cv=5, scoring='accuracy')\n",
    "        avg_cv_accuracy = cv_scores.mean()\n",
    "\n",
    "        # Step 7: Addressing Class Imbalance & Feature Engineering\n",
    "        print(\"Step 7: Addressing Class Imbalance & Feature Engineering...\")\n",
    "        data_female = data_cleaned[data_cleaned['Article_Gender'] == 'Female']\n",
    "        data_male = data_cleaned[data_cleaned['Article_Gender'] == 'Male']\n",
    "        data_female_oversampled = resample(data_female, replace=True, n_samples=len(data_male), random_state=42)\n",
    "        data_oversampled = pd.concat([data_male, data_female_oversampled])\n",
    "        X_oversampled = tfidf_vectorizer.transform(data_oversampled[column_name])\n",
    "        X_combined_oversampled = hstack([X_oversampled, journal_dummies.loc[data_oversampled.index].values])\n",
    "        y_oversampled = data_oversampled['Article_Gender']\n",
    "        X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_combined_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Step 8: Complex Models & Cost-sensitive Learning\n",
    "        print(\"Step 8: Complex Models & Cost-sensitive Learning...\")\n",
    "        logreg_balanced = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "        logreg_balanced.fit(X_train_os, y_train_os)\n",
    "        y_pred_os = logreg_balanced.predict(X_test_os)\n",
    "        accuracy_os = accuracy_score(y_test_os, y_pred_os)\n",
    "\n",
    "        # Step 9: Performance Evaluation\n",
    "        print(\"Step 9: Performance Evaluation...\")\n",
    "        evaluation_metrics_logreg = classification_report(y_test_os, y_pred_os, target_names=[\"Female\", \"Male\"])\n",
    "\n",
    "        # Step 10: Hyperparameter tuning for Logistic Regression\n",
    "        print(\"Step 10: Hyperparameter tuning for Logistic Regression...\")\n",
    "        logreg_params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        grid_search_logreg = GridSearchCV(LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga'),\n",
    "                                          logreg_params, \n",
    "                                          cv=5, \n",
    "                                          scoring='accuracy', \n",
    "                                          n_jobs=-1)\n",
    "        grid_search_logreg.fit(X_train_os, y_train_os)\n",
    "        best_params_logreg = grid_search_logreg.best_params_\n",
    "        best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "        # Step 11: Hyperparameter tuning optimized for predicting 'Female'\n",
    "        print(\"Step 11: Hyperparameter tuning optimized for predicting 'Female'...\")\n",
    "        custom_scorer = make_scorer(precision_score, pos_label='Female', zero_division=0)\n",
    "        model = LogisticRegression()\n",
    "        param_grid_custom = {\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'max_iter': [50, 400, 800, 1200]\n",
    "        }\n",
    "        grid_search_custom = GridSearchCV(model, param_grid_custom, scoring=custom_scorer, cv=5)\n",
    "        grid_search_custom.fit(X_combined, y)\n",
    "        best_params_custom = grid_search_custom.best_params_\n",
    "        best_score_custom = grid_search_custom.best_score_\n",
    "\n",
    "        results[label] = {\n",
    "            f'basic_accuracy_{label}': basic_accuracy,\n",
    "            f'avg_cv_accuracy_{label}': avg_cv_accuracy,\n",
    "            f'accuracy_os_{label}': accuracy_os,\n",
    "            f'evaluation_metrics_logreg_{label}': evaluation_metrics_logreg,\n",
    "            f'best_params_logreg_{label}': best_params_logreg,\n",
    "            f'best_score_logreg_{label}': best_score_logreg,\n",
    "            f'best_params_custom_{label}': best_params_custom,\n",
    "            f'best_score_custom_{label}': best_score_custom\n",
    "        }\n",
    "\n",
    "    print(f\"Finished processing column {column_name} without dummies.\")\n",
    "    return results\n",
    "\n",
    "# Assuming you've loaded your data into a DataFrame named 'data':\n",
    "\n",
    "# Call the functions for each of the desired columns without dummies\n",
    "results_intro_cleaned = process_column_both_grams('Intro_Cleaned', data)\n",
    "results_intro_1 = process_column_both_grams('Intro_1', data)\n",
    "results_intro_2 = process_column_both_grams('Intro_2', data)\n",
    "results_intro_3 = process_column_both_grams('Intro_3', data)\n",
    "\n",
    "print(\"All processing completed without dummies.\")\n",
    "\n",
    "results_intro_cleaned, results_intro_1, results_intro_2, results_intro_3\n",
    "\n",
    "# Call the functions for each of the desired columns with dummies\n",
    "results_intro_cleaned_dummies = process_column_both_grams_with_dummies('Intro_Cleaned', data)\n",
    "results_intro_1_dummies = process_column_both_grams_with_dummies('Intro_1', data)\n",
    "results_intro_2_dummies = process_column_both_grams_with_dummies('Intro_2', data)\n",
    "results_intro_3_dummies = process_column_both_grams_with_dummies('Intro_3', data)\n",
    "\n",
    "print(\"All processing completed with dummies.\")\n",
    "\n",
    "results_intro_cleaned_dummies, results_intro_1_dummies, results_intro_2_dummies, results_intro_3_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-seating",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "relevant-conditions",
   "metadata": {},
   "source": [
    "# RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "domestic-diana",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_intro_cleaned_dummies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-67484cad4174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Call the print_results function for each set of results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Intro_Cleaned'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_cleaned\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_cleaned_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_cleaned_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'intro_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_1_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_1_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Intro_2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_2_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ug'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_intro_2_dummies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_intro_cleaned_dummies' is not defined"
     ]
    }
   ],
   "source": [
    "def print_results(column_name, results_ug, results_bg, results_ug_dummies, results_bg_dummies):\n",
    "    # Header for the column\n",
    "    print(f\"\\nResults for {column_name}:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Iterate through each variant (ug, bg, ug_dummies, bg_dummies)\n",
    "    for variant, metrics in {\"ug\": results_ug, \"bg\": results_bg, \"ug_dummies\": results_ug_dummies, \"bg_dummies\": results_bg_dummies}.items():\n",
    "        if \"_dummies\" in variant:\n",
    "            model_type = \"with Journal Dummies\"\n",
    "        else:\n",
    "            model_type = \"without Journal Dummies\"\n",
    "        \n",
    "        ngram_type = \"Unigram\" if \"ug\" in variant else \"Bigram\"\n",
    "        \n",
    "        # Basic Information\n",
    "        print(f\"\\nModel Performance Summary ({ngram_type}, {model_type}):\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Basic Logistic Regression Accuracy: {metrics[f'basic_accuracy_{variant}'] * 100:.2f}%\")\n",
    "        print(f\"Cross-Validation Accuracy: {metrics[f'avg_cv_accuracy_{variant}'] * 100:.2f}%\")\n",
    "        print(f\"Oversampled Logistic Regression Accuracy: {metrics[f'accuracy_os_{variant}'] * 100:.2f}%\")\n",
    "        print(f\"Best Hyperparameter Tuned Model Accuracy: {metrics[f'best_score_logreg_{variant}'] * 100:.2f}%\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "# Call the print_results function for each set of results\n",
    "print_results('Intro_Cleaned', results_intro_cleaned['ug'], results_intro_cleaned['bg'], results_intro_cleaned_dummies['ug'], results_intro_cleaned_dummies['bg'])\n",
    "print_results('intro_1', results_intro_1['ug'], results_intro_1['bg'], results_intro_1_dummies['ug'], results_intro_1_dummies['bg'])\n",
    "print_results('Intro_2', results_intro_2['ug'], results_intro_2['bg'], results_intro_2_dummies['ug'], results_intro_2_dummies['bg'])\n",
    "print_results('Intro_3', results_intro_3['ug'], results_intro_3['bg'], results_intro_3_dummies['ug'], results_intro_3_dummies['bg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-portsmouth",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Accuracy Calculation\n",
    "baseline_accuracy = max(data_cleaned['Article_Gender'].value_counts(normalize=True))\n",
    "\n",
    "# List of accuracies for each model type\n",
    "labels = [\n",
    "    \"Unigram\", \n",
    "    \"Bigram\", \n",
    "    \"Unigram with Dummies\", \n",
    "    \"Bigram with Dummies\",\n",
    "    \"Baseline\"\n",
    "]\n",
    "\n",
    "def plot_accuracies(accuracies, title, ylabel):\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.bar(labels, accuracies, color=['blue', 'green', 'purple', 'cyan', 'red'], alpha=0.7)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.ylim([0.5, 1.0])\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 1. Basic Accuracy\n",
    "plot_accuracies([\n",
    "    results['basic_accuracy_ug'], \n",
    "    results_bg['basic_accuracy_bg'],\n",
    "    results_dum_ug['basic_accuracy_ug'], \n",
    "    results_dum_bg['basic_accuracy_bg'],\n",
    "    baseline_accuracy\n",
    "], \"Comparison of Basic Model Accuracies\", \"Basic Accuracy\")\n",
    "\n",
    "# 2. Cross-Validation Accuracy\n",
    "plot_accuracies([\n",
    "    results['avg_cv_accuracy_ug'], \n",
    "    results_bg['avg_cv_accuracy_bg'],\n",
    "    results_dum_ug['avg_cv_accuracy_ug'], \n",
    "    results_dum_bg['avg_cv_accuracy_bg'],\n",
    "    baseline_accuracy\n",
    "], \"Comparison of CV Model Accuracies\", \"Cross-Validation Accuracy\")\n",
    "\n",
    "# 3. Oversampled Accuracy\n",
    "plot_accuracies([\n",
    "    results['accuracy_os_ug'], \n",
    "    results_bg['accuracy_os_bg'],\n",
    "    results_dum_ug['accuracy_os_ug'], \n",
    "    results_dum_bg['accuracy_os_bg'],\n",
    "    baseline_accuracy\n",
    "], \"Comparison of Oversampled Model Accuracies\", \"Oversampled Accuracy\")\n",
    "\n",
    "# 4. Best Hyperparameter Tuned Accuracy\n",
    "plot_accuracies([\n",
    "    results['best_score_logreg'], \n",
    "    results_bg['best_score_logreg_bg'],\n",
    "    results_dum_ug['best_score_logreg'], \n",
    "    results_dum_bg['best_score_logreg_bg'],\n",
    "    baseline_accuracy\n",
    "], \"Comparison of Best Hyperparameter Tuned Model Accuracies\", \"Best Tuned Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "interpreted-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, make_scorer, precision_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "import pandas as pd\n",
    "\n",
    "def process_column_both_grams(column_name, data):\n",
    "    print(f\"Processing column {column_name} without dummies...\")\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for ngram, label in [((1, 1), \"ug\"), ((1, 2), \"bg\")]:\n",
    "        print(f\"Processing for ngram {ngram}...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=ngram)\n",
    "        X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Oversampling to address class imbalance\n",
    "        data_female = data_cleaned[data_cleaned['Article_Gender'] == 'Female']\n",
    "        data_male = data_cleaned[data_cleaned['Article_Gender'] == 'Male']\n",
    "        data_female_oversampled = resample(data_female, replace=True, n_samples=len(data_male), random_state=42)\n",
    "        data_oversampled = pd.concat([data_male, data_female_oversampled])\n",
    "        X_oversampled = tfidf_vectorizer.transform(data_oversampled[column_name])\n",
    "        y_oversampled = data_oversampled['Article_Gender']\n",
    "        X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Step 10: Hyperparameter tuning for Logistic Regression\n",
    "        print(\"Step 10: Hyperparameter tuning for Logistic Regression\")\n",
    "        logreg_params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        grid_search_logreg = GridSearchCV(LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga'),\n",
    "                                          logreg_params, \n",
    "                                          cv=5, \n",
    "                                          scoring='accuracy', \n",
    "                                          n_jobs=-1)\n",
    "        grid_search_logreg.fit(X_train_os, y_train_os)\n",
    "        best_params_logreg = grid_search_logreg.best_params_\n",
    "        best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "        # Step 11: Hyperparameter tuning optimized for predicting 'Female'\n",
    "        print(\"Step 11: Hyperparameter tuning optimized for predicting 'Female'\")\n",
    "        custom_scorer = make_scorer(precision_score, pos_label='Female', zero_division=0)\n",
    "        param_grid_custom = {\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'max_iter': [50, 400, 800, 1200]\n",
    "        }\n",
    "        grid_search_custom = GridSearchCV(LogisticRegression(), param_grid_custom, scoring=custom_scorer, cv=5)\n",
    "        grid_search_custom.fit(X, y)\n",
    "        best_params_custom = grid_search_custom.best_params_\n",
    "        best_score_custom = grid_search_custom.best_score_\n",
    "\n",
    "        results[label] = {\n",
    "            f'best_params_logreg_{label}': best_params_logreg,\n",
    "            f'best_score_logreg_{label}': best_score_logreg,\n",
    "            f'best_params_custom_{label}': best_params_custom,\n",
    "            f'best_score_custom_{label}': best_score_custom\n",
    "        }\n",
    "\n",
    "    print(f\"Finished processing column {column_name} without dummies.\")\n",
    "    return results\n",
    "\n",
    "def process_column_both_grams_with_dummies(column_name, data):\n",
    "    print(f\"Processing column {column_name} with dummies...\")\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "    journal_dummies = pd.get_dummies(data_cleaned['Journal_name'], prefix='Journal')\n",
    "    results = {}\n",
    "\n",
    "    for ngram, label in [((1, 1), \"ug\"), ((1, 2), \"bg\")]:\n",
    "        print(f\"Processing for ngram {ngram}...\")\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=ngram)\n",
    "        X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "        X_combined = hstack([X, journal_dummies.values])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Oversampling to address class imbalance\n",
    "        data_female = data_cleaned[data_cleaned['Article_Gender'] == 'Female']\n",
    "        data_male = data_cleaned[data_cleaned['Article_Gender'] == 'Male']\n",
    "        data_female_oversampled = resample(data_female, replace=True, n_samples=len(data_male), random_state=42)\n",
    "        data_oversampled = pd.concat([data_male, data_female_oversampled])\n",
    "        X_oversampled = tfidf_vectorizer.transform(data_oversampled[column_name])\n",
    "        X_combined_oversampled = hstack([X_oversampled, journal_dummies.loc[data_oversampled.index].values])\n",
    "        y_oversampled = data_oversampled['Article_Gender']\n",
    "        X_train_os, X_test_os, y_train_os, y_test_os = train_test_split(X_combined_oversampled, y_oversampled, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Step 10: Hyperparameter tuning for Logistic Regression\n",
    "        print(\"Step 10: Hyperparameter tuning for Logistic Regression...\")\n",
    "        logreg_params = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "        grid_search_logreg = GridSearchCV(LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga'),\n",
    "                                          logreg_params, \n",
    "                                          cv=5, \n",
    "                                          scoring='accuracy', \n",
    "                                          n_jobs=-1)\n",
    "        grid_search_logreg.fit(X_train_os, y_train_os)\n",
    "        best_params_logreg = grid_search_logreg.best_params_\n",
    "        best_score_logreg = grid_search_logreg.best_score_\n",
    "\n",
    "        # Step 11: Hyperparameter tuning optimized for predicting 'Female'\n",
    "        print(\"Step 11: Hyperparameter tuning optimized for predicting 'Female'...\")\n",
    "        custom_scorer = make_scorer(precision_score, pos_label='Female', zero_division=0)\n",
    "        param_grid_custom = {\n",
    "            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 500, 1000],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'max_iter': [50, 400, 800, 1200]\n",
    "        }\n",
    "        grid_search_custom = GridSearchCV(LogisticRegression(), param_grid_custom, scoring=custom_scorer, cv=5)\n",
    "        grid_search_custom.fit(X_combined, y)\n",
    "        best_params_custom = grid_search_custom.best_params_\n",
    "        best_score_custom = grid_search_custom.best_score_\n",
    "\n",
    "        results[label] = {\n",
    "            f'best_params_logreg_{label}': best_params_logreg,\n",
    "            f'best_score_logreg_{label}': best_score_logreg,\n",
    "            f'best_params_custom_{label}': best_params_custom,\n",
    "            f'best_score_custom_{label}': best_score_custom\n",
    "        }\n",
    "\n",
    "    print(f\"Finished processing column {column_name} with dummies.\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-swaziland",
   "metadata": {},
   "source": [
    "# Learning curve & Validation curve - ikke lykkes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List to hold datasets and corresponding labels\n",
    "datasets = [\n",
    "    (X_oversampled_ug, y_oversampled_ug, \"Unigram\"),\n",
    "    (X_oversampled_bg, y_oversampled_bg, \"Bigram\"),\n",
    "    (X_oversampled_dum_ug, y_oversampled_dum_ug, \"Unigram with Dummies\"),\n",
    "    (X_oversampled_dum_bg, y_oversampled_dum_bg, \"Bigram with Dummies\")\n",
    "]\n",
    "\n",
    "# Iterate through each dataset and plot\n",
    "for X_data, y_data, label in datasets:\n",
    "    # Plot learning curve\n",
    "    title = f\"Learning Curve (Logistic Regression, {label})\"\n",
    "    plot_learning_curve(logreg_balanced, title, X_data, y_data, cv=5)\n",
    "    plt.show()\n",
    "\n",
    "    # Validation curve for the C parameter in logistic regression\n",
    "    param_range = [0.001, 0.01, 0.1, 1, 10, 100, 500, 1000]\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga'),\n",
    "        X_data, y_data, param_name=\"C\", param_range=param_range, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.title(f\"Validation Curve with Logistic Regression ({label})\")\n",
    "    plt.xlabel(\"C\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.ylim(0.5, 1.1)\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"r\")\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"r\")\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"g\")\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"g\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-health",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

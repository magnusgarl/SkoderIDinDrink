{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07ea37d-b108-478c-8dda-99fd0f9e0e12",
   "metadata": {},
   "source": [
    "# Selenium Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ead45a5-ca62-46aa-9d99-295249eb2a9a",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c08921c3-f614-4869-915a-4c3b919b7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Define the URL and user-agent to mimic a real browser\n",
    "url = 'https://www.sciencedirect.com/science/article/pii/S0927537123000404'\n",
    "user_agent = (\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.1234.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Use a non-headless browser with a custom user-agent\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument('--headless=new')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4ac4a-2aea-4bb7-9454-29691f602e7e",
   "metadata": {},
   "source": [
    "## Find the first article of every volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac8896-2497-4d43-8921-8e269bc2344d",
   "metadata": {},
   "source": [
    "### Journals to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d98e27e8-7b41-4d3d-b7da-aba510579d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = [('economics-letters',231),('economic-modelling', 127),\n",
    "           ('journal-of-econometrics',236),('journal-of-development-economics',165),\n",
    "           ('journal-of-applied-economics',20),('the-journal-of-socio-economics',48),('journal-of-economic-theory',212)]\n",
    "n_vol = 45 #No. of volumes to scrape per journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c081de5-5798-4ecf-b7f0-667862a6f203",
   "metadata": {},
   "source": [
    "### Index articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e83295f-b2e0-40b9-86d0-43ce5bfd8c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_volumes(journal,vol, n_vol):\n",
    "    url_base = \"https://www.sciencedirect.com/journal/\" + str(journal) + \"/vol/\"\n",
    "    url_list = []\n",
    "    for i in range(n_vol): \n",
    "        url_pre = url_base + str(vol-i) + \"/suppl/C\"\n",
    "        url_list.append(url_pre)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    first_page_url = []\n",
    "    \n",
    "    for it, url in enumerate(url_list):\n",
    "        print(f\"Processing URL {it+1}/{len(url_list)}: {url}\")\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        \n",
    "            # Wait for the first link with class \"article-content-title\" to be clickable\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.article-content-title')))\n",
    "        \n",
    "            # Click the link\n",
    "            link.click()\n",
    "        \n",
    "            # Get the current URL of the page\n",
    "            first_page_url.append(driver.current_url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for URL {url}\")\n",
    "            \n",
    "    return first_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e417cb-b2d4-492d-a52b-7c06f84d2570",
   "metadata": {},
   "source": [
    "### Scrape journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3cba50be-ea5d-4c30-a275-77c80892e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(first_page_url):\n",
    "    journal = []\n",
    "    for it, url in enumerate(first_page_url):\n",
    "        print(f\"Processing URL {it+1}/{len(first_page_url)}: {url}\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        page_source = []\n",
    "\n",
    "        while i < 50:\n",
    "            time.sleep(5)  # Wait for a longer time between requests\n",
    "\n",
    "            #print(i)\n",
    "            page_source.append(driver.page_source)\n",
    "\n",
    "            try:\n",
    "                # Wait for the next button to become clickable before clicking it\n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.button-alternative-icon-right')))\n",
    "                next_button.click()\n",
    "\n",
    "                i += 1\n",
    "            except Exception as e:\n",
    "                print(\"Error with article\")\n",
    "                break\n",
    "    \n",
    "        journal.append(page_source)\n",
    "        \n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    return journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc2ea21-2327-4aed-ac9b-c1f6c97229d5",
   "metadata": {},
   "source": [
    "## Data Treatment Stage 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec4b0a92-d811-4af8-8828-f13bc7f582ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(page_source):\n",
    "    df_data = []\n",
    "\n",
    "    for j in range(len(page_source)):\n",
    "        authors_dict = {}  # Create a new dictionary for each row\n",
    "        journal_name = []\n",
    "        citations = []\n",
    "        year = []\n",
    "        introduction = []\n",
    "        possible_id = ['sec_0001','sec_001','sec_01','sec_1','sec1']\n",
    "        \n",
    "        soup = bs(page_source[j], 'html.parser')\n",
    "        \n",
    "        for ids in possible_id:\n",
    "            element_with_id = soup.select_one('section#' + ids)\n",
    "            if element_with_id:\n",
    "                break \n",
    "            \n",
    "        if element_with_id:\n",
    "            introduction = element_with_id.get_text(strip=True)\n",
    "        else:\n",
    "            introduction = None  # Handle case when element is not found\n",
    "            #print('Intro Error')\n",
    "        \n",
    "        # Find all <span> elements with class=\"given-name\"\n",
    "        author_spans = soup.find_all('span', class_='given-name')\n",
    "\n",
    "        # Loop through the author spans and populate the dictionary\n",
    "        for i, author_span in enumerate(author_spans):\n",
    "            try:\n",
    "                if i < 5:\n",
    "                    authors_dict[f'Author {i+1}'] = author_span.get_text().split()[0]\n",
    "                    if \".\" in authors_dict[f'Author {i+1}']:\n",
    "                        author_name = \"Unknown\"\n",
    "                        authors_dict[f'Author {i+1}'] = author_name\n",
    "                else:\n",
    "                    break\n",
    "            except AttributeError:\n",
    "                authors_dict[f'Author {i+1}'] = \"None\"\n",
    "                #print('error author')\n",
    "        \n",
    "        # Fill in remaining entries with \"None\" for Author 4 and Author 5\n",
    "        for i in range(len(author_spans), 5):\n",
    "            authors_dict[f'Author {i+1}'] = \"None\"\n",
    "        \n",
    "        try:\n",
    "            j_name = soup.find('a', class_=\"publication-title-link\").get_text()\n",
    "            journal_name = str(j_name)\n",
    "        except:\n",
    "            journal_name = \"Error no journal name\"\n",
    "            #print('error journal')\n",
    "            \n",
    "        try:\n",
    "            cit = soup.find('header', id=\"citing-articles-header\").get_text().split()[2]\n",
    "            #cit = cit[-1:][:-1]\n",
    "            #citations = int(cit)\n",
    "            cit = ''.join(filter(str.isdigit, cit))\n",
    "            citations = int(cit)\n",
    "            #citations = cit\n",
    "        except:\n",
    "            citations = None\n",
    "            #print('error citations')\n",
    "            \n",
    "        try:\n",
    "            yr = soup.find('div', class_=\"text-xs\").get_text()\n",
    "            year = yr\n",
    "\n",
    "        except:\n",
    "            year = None\n",
    "            #print('error yr')\n",
    "        \n",
    "        \n",
    "        # Append the data for this row to the list of rows\n",
    "        df_data.append({'Introduction': introduction, **authors_dict, 'Journal': journal_name, 'Year': year, 'Citations': citations})\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5529e466-bfd2-457d-8639-f0e74831a4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract Ã¥r ###\n",
    "import re\n",
    "\n",
    "def extract_year(s):\n",
    "    year_match = re.search(r'(\\d{4})', s)\n",
    "    if year_match:\n",
    "        return year_match.group(1)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06116f1d-456e-4cb7-89f5-ddea883b3dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(list_of_lists):\n",
    "    combined_df = pd.DataFrame()  # Initialize an empty DataFrame to store the combined data\n",
    "\n",
    "    for sublist in list_of_lists:\n",
    "        # Apply create_df function to the sublist and get a dataframe\n",
    "        df = create_df(sublist)\n",
    "        \n",
    "        # Concatenate the current dataframe with the combined dataframe\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    df['Year'] = df['Year'].apply(extract_year)\n",
    "    #df['Author 4'] = df['Author 4'].astype(str)\n",
    "    #df['Author 5'] = df['Author 5'].astype(str)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0f3efc-a6bd-4f9f-bed4-45e7a66aca67",
   "metadata": {},
   "source": [
    "## Execute Scraping and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e93b9dbc-edeb-40c6-9dbf-60d32ce744af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('economic-modelling', 127)\n",
      "economics-letters\n",
      "Processing URL 1/1: https://www.sciencedirect.com/journal/economics-letters/vol/231/suppl/C\n",
      "no of vols 67\n",
      "Processing URL 1/1: https://www.sciencedirect.com/science/article/pii/S0165176523002823\n",
      "Error with article\n"
     ]
    }
   ],
   "source": [
    "sub_scrape = scrape[1]\n",
    "print(sub_scrape)\n",
    "articles = []\n",
    "\n",
    "for j_1 in scrape:\n",
    "    name_of_journal, j_vol = j_1\n",
    "    print(name_of_journal)\n",
    "    url_index = index_volumes(name_of_journal , j_vol, n_vol)\n",
    "    print('no of vols ' + str(len(url)))\n",
    "    \n",
    "    data = scrape_article(url_index)\n",
    "    data_treated = combine_dataframes(data)\n",
    "    \n",
    "    #articles.append(data)\n",
    "    \n",
    "    # Specify the CSV file name\n",
    "    csv_file = 'articles_out_' + str(name_of_journal) + '.csv'\n",
    "    data_treated.to_csv(csv_file, index=False)\n",
    "    \n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cc3338a-3265-4470-aeb1-6302c3e5d48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Introduction Author 1  Author 2  \\\n",
      "0  1. IntroductionTheories of imitation, conformi...    Paola   Georgia   \n",
      "1  1. IntroductionThe all-or-nothing mechanism is...  Timothy  Robertas   \n",
      "2  1. IntroductionA bilateral exchange rate is th...  Michael       NaN   \n",
      "3  1. IntroductionPerformance-contingent bonuses ...     Timo       NaN   \n",
      "4  1. IntroductionSocial media has become an esse...       Ho       Ali   \n",
      "5  1. IntroductionA series of theoretical models ...  Antoine       NaN   \n",
      "\n",
      "  Author 3            Journal                              Year  Citations  \\\n",
      "0     Luis  Economics Letters  Volume 231, October 2023, 111257          0   \n",
      "1      NaN  Economics Letters  Volume 231, October 2023, 111265          0   \n",
      "2      NaN  Economics Letters  Volume 231, October 2023, 111267          0   \n",
      "3      NaN  Economics Letters  Volume 231, October 2023, 111266          0   \n",
      "4   Sascha  Economics Letters  Volume 231, October 2023, 111270          0   \n",
      "5      NaN  Economics Letters  Volume 231, October 2023, 111272          0   \n",
      "\n",
      "  Author 4  \n",
      "0      NaN  \n",
      "1      NaN  \n",
      "2      NaN  \n",
      "3      NaN  \n",
      "4    Benno  \n",
      "5      NaN  \n"
     ]
    }
   ],
   "source": [
    "print(data_treated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb3082-c38d-4443-b244-c66e9de3686a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

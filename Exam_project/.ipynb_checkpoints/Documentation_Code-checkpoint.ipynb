{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "silent-binding",
   "metadata": {},
   "source": [
    "# Code for Paper  \n",
    "# “How Blind is Blind? Predicting Gendered Writing Styles in Academic Articles”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-equipment",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import All Necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-covering",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import csv\n",
    "import concurrent.futures\n",
    "import os\n",
    "import re\n",
    "import gender_guesser.detector as gender\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
    "from sklearn.utils import resample\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import hstack\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve, learning_curve\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import datetime\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EvalPrediction\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expected-russell",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scraping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-sample",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Springer \n",
    "Firstly we collect the ID-number for the 109 journals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-ireland",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "base_url = \"https://link.springer.com\"\n",
    "search_url = base_url + \"/search/page/{}?facet-discipline=%22Economics%22&facet-content-type=%22Journal%22&facet-language=%22En%22\"\n",
    "\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Magnus Berg (University of Copenhagen), Accessing OA data for project ascertaining gendered writing styles in academia, email@edurome.ku.dk\"\n",
    "}\n",
    "\n",
    "Journal_list = []\n",
    "\n",
    "# Iterate over pages from 1 to 6\n",
    "for page_number in range(1, 7):\n",
    "    url = search_url.format(page_number)\n",
    "    response = requests.get(url, headers=headers)  # Use the headers in the request\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    journal_links = soup.find_all(\"a\", class_=\"title\")\n",
    "    for link in journal_links:\n",
    "        journal_url = link.get(\"href\")\n",
    "        journal_number = journal_url.split(\"/journal/\")[-1]\n",
    "        Journal_list.append(journal_number)\n",
    "\n",
    "    # Sleep for 1 second before making the next request\n",
    "    time.sleep(1)\n",
    "\n",
    "# Print the collected journal numbers with an index\n",
    "for index, number in enumerate(Journal_list, 1):\n",
    "    print(f\"{index}. {number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97b618-fcd2-45d0-ac52-6efa5c81be00",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scraping information\n",
    "Now we scrape the information and genderclasify the aurthors and saving each journal as a CSV file so progress won't get lost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482896f4-a0e6-435c-a02b-3ac396ba6791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_to_save = \"path_to_save\"\n",
    "\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': \"Magnus Berg (University of Copenhagen), Accessing OA data for project ascertaining gendered writing styles in academia, email@edurome.ku.dk\"\n",
    "}\n",
    "\n",
    "# Initialize gender detector\n",
    "detector = gender.Detector()\n",
    "\n",
    "def get_first_name(full_name):\n",
    "    if full_name:\n",
    "        return full_name.split()[0]\n",
    "    return \"None\"\n",
    "\n",
    "def predict_gender_first_name(name):\n",
    "    return detector.get_gender(name) if name != \"None\" else \"unknown\"\n",
    "\n",
    "def fetch_soup(session, url):\n",
    "    response = session.get(url, headers=headers)\n",
    "    # Check for \"Too Many Requests\" and pause if encountered\n",
    "    if response.status_code == 429:\n",
    "        print(\"429 Too Many Requests encountered. Pausing for 10 minutes...\")\n",
    "        time.sleep(600)  # Sleep for 10 minutes\n",
    "        return fetch_soup(session, url)  # Retry after pausing\n",
    "    time.sleep(1)  # Sleep for 1 second between requests\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    return None\n",
    "\n",
    "\n",
    "MAX_AUTHORS = 10\n",
    "\n",
    "def process_journal(journal_number):\n",
    "    data = {\n",
    "        'Volume': [],\n",
    "        'Issue': [],\n",
    "        'Journal Name': [],\n",
    "        'Published Date': [],\n",
    "        'Link': [],\n",
    "        'Title': [],\n",
    "        'Journal Year': []\n",
    "    }\n",
    "    \n",
    "    for i in range(1, MAX_AUTHORS+1):\n",
    "        data[f'Author {i}'] = []\n",
    "\n",
    "    print(f\"Scraping data for journal number {journal_number}...\")\n",
    "\n",
    "    with requests.Session() as session:\n",
    "        base_url = f'https://link.springer.com/journal/{journal_number}/volumes-and-issues/'\n",
    "        volume = 1\n",
    "\n",
    "        while True:\n",
    "            print(f\"Scraping volume {volume}...\")\n",
    "\n",
    "            issue = 1\n",
    "            while True:\n",
    "                url = base_url + f'{volume}-{issue}'\n",
    "                soup = fetch_soup(session, url)\n",
    "\n",
    "                if soup:\n",
    "                    articles = soup.find_all('li', class_='c-list-group__item')\n",
    "                    if not articles:\n",
    "                        break\n",
    "\n",
    "                    journal_tag = soup.find('div', {'id': 'journalTitle'}).find('a')\n",
    "                    journal_name = journal_tag.text.strip() if journal_tag else None\n",
    "                    journal_year = soup.find('h1', class_='u-mb-8')\n",
    "                    journal_year = journal_year.text.split(\",\")[-1].strip() if journal_year else None\n",
    "\n",
    "                    for article in articles:\n",
    "                        link = article.find('a', href=True)['href']\n",
    "                        authors = [author.text.strip() for author in article.select('ul.c-author-list li span')]\n",
    "                        title = article.find('a', attrs={\"data-track\": \"click\"}).text.strip()\n",
    "                        published_date_tag = article.find('li', attrs={'data-test': 'published-on'})\n",
    "                        published_date = published_date_tag.text.split(': ')[1].strip() if published_date_tag else None\n",
    "\n",
    "                        data['Volume'].append(volume)\n",
    "                        data['Issue'].append(issue)\n",
    "                        data['Journal Name'].append(journal_name)\n",
    "                        data['Published Date'].append(published_date)\n",
    "                        data['Link'].append(link)\n",
    "                        data['Title'].append(title)\n",
    "                        data['Journal Year'].append(journal_year)\n",
    "\n",
    "                        # Fill in authors or None values\n",
    "                        for i in range(MAX_AUTHORS):\n",
    "                            if i < len(authors):\n",
    "                                data[f'Author {i+1}'].append(authors[i])\n",
    "                            else:\n",
    "                                data[f'Author {i+1}'].append(None)\n",
    "\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "                issue += 1\n",
    "\n",
    "            time.sleep(2)  # Sleep for 2 seconds after processing each volume\n",
    "            next_soup = fetch_soup(session, base_url + f'{volume+1}-1')\n",
    "            if not next_soup or (next_soup and not next_soup.find_all('li', class_='c-list-group__item')):\n",
    "                break\n",
    "\n",
    "            volume += 1\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    for i in range(MAX_AUTHORS):\n",
    "        col_name = f'Author {i+1}'\n",
    "        df[f'Gender_{col_name}'] = df[col_name].apply(get_first_name).apply(predict_gender_first_name)\n",
    "        df[f'Gender_{col_name}'] = df[f'Gender_{col_name}'].map({\n",
    "            'male': 'Male',\n",
    "            'female': 'Female',\n",
    "            'unknown': 'Unknown',\n",
    "            'None': 'Unknown'\n",
    "        })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Dictionary to hold DataFrames for each journal\n",
    "dfs = {}\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists(path_to_save):\n",
    "    os.makedirs(path_to_save)\n",
    "\n",
    "\n",
    "# Use ThreadPoolExecutor to scrape multiple journals concurrently with limited threads\n",
    "MAX_THREADS = 4\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    futures = {executor.submit(process_journal, journal): journal for journal in Journal_list}\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        journal_num = futures[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            dfs[journal_num] = pd.DataFrame(result)\n",
    "            # Save the DataFrame to a CSV file in the specified directory\n",
    "            dfs[journal_num].to_csv(f\"{path_to_save}/journal_{journal_num}.csv\", index=False)\n",
    "            print(f\"Saved CSV for journal {journal_num}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing journal {journal_num}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Print out the saved DataFrames\n",
    "for journal, df in dfs.items():\n",
    "    print(f\"DataFrame for journal {journal}:\\n\")\n",
    "    print(df)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3dc3f-6124-49f6-86ef-66d631d8d970",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Saving all to same document and dropping exces aurthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44ec93c-c676-42cd-99c6-71a5005cf80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_save = \"path_to_save\"\n",
    "MAX_AUTHORS = 10\n",
    "\n",
    "# List all CSV files in the directory\n",
    "all_files = [f for f in os.listdir(path_to_save) if os.path.isfile(os.path.join(path_to_save, f)) and f.endswith('.csv')]\n",
    "\n",
    "# Read and combine all CSV files\n",
    "dfs = [pd.read_csv(os.path.join(path_to_save, f)) for f in all_files]\n",
    "all_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop columns where all observations are empty and their corresponding Gender_Author columns\n",
    "for i in range(1, MAX_AUTHORS + 1):\n",
    "    author_col = f'Author {i}'\n",
    "    gender_col = f'Gender_{author_col}'\n",
    "    if all_data[author_col].isna().all():  # Check if all values in the column are NaN\n",
    "        all_data.drop(columns=[author_col, gender_col], inplace=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file\n",
    "all_data.to_csv(f\"{path_to_save}/all_journals.csv\", index=False)\n",
    "print(\"Saved all data to all_journals.csv\")\n",
    "\n",
    "# Print out the combined dataframe\n",
    "print(all_data)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670dab7-02ca-4028-9754-510b9b6230b8",
   "metadata": {},
   "source": [
    "Get artical genders based on all aurthor genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b455b782-e9e9-488b-8138-db6e39069e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the article gender based on author genders\n",
    "def get_article_gender(row):\n",
    "    # Extract genders of authors, ignoring 'Unknown'\n",
    "    genders = [row[f'Gender_Author {i+1}'] for i in range(3) if row[f'Gender_Author {i+1}'] != 'Unknown']\n",
    "    \n",
    "    # If there are no known gender values\n",
    "    if len(genders) == 0:\n",
    "        return 'Unknown'\n",
    "    # If all known gender values are 'Male'\n",
    "    elif all(gender == 'Male' for gender in genders):\n",
    "        return 'Male'\n",
    "    # If all known gender values are 'Female'\n",
    "    elif all(gender == 'Female' for gender in genders):\n",
    "        return 'Female'\n",
    "    # If there's a mix of male and female among the known gender values\n",
    "    else:\n",
    "        return 'Mix'\n",
    "\n",
    "# Apply the function to create the \"Article_Gender\" column\n",
    "all_data['Article_Gender'] = all_data.apply(get_article_gender, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(all_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107bc938-0c2f-4fa7-b6fd-0880d2d10c5a",
   "metadata": {},
   "source": [
    "Now scraping all the intros of all the papers which is clasified as Male or Female "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ccb15-2b15-4799-94ff-6ec2c93baecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': \"Magnus Berg (University of Copenhagen), Accessing OA data for project ascertaining gendered writing styles in academia, email@edurome.ku.dk\"\n",
    "}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "counter = [0]\n",
    "\n",
    "def scrape_intro_and_citations(row):\n",
    "    global counter\n",
    "    url = row[all_data.columns.tolist().index('Link') + 1]\n",
    "    \n",
    "    if not url:  # Check if the URL is None or empty\n",
    "        return (row[0], None, None)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    time.sleep(1)  # Sleep for 1 second after each request\n",
    "    \n",
    "\n",
    "    \n",
    "    # Increment the counter and print if it's a multiple of 100\n",
    "    counter[0] += 1\n",
    "    if counter[0] % 100 == 0:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        print(f\"{timestamp}: {journal_name} {counter[0]}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        webpage = response.text\n",
    "        soup = BeautifulSoup(webpage, 'html.parser')\n",
    "        \n",
    "        # Scrape the introduction\n",
    "        intro_div = soup.find('div', class_='c-article-section__content', id='Sec1-content')\n",
    "        intro = ' '.join(p.get_text() for p in intro_div.find_all('p')) if intro_div else None\n",
    "        \n",
    "        # Scrape the number of citations\n",
    "        citations_tag = soup.find('span', string='Citations')\n",
    "        citations = citations_tag.find_parent('p', class_='c-article-metrics-bar__count').get_text().split()[0] if citations_tag else None\n",
    "        \n",
    "        return (row[0], intro, citations)\n",
    "    elif response.status_code == 429:  # Rate limit error\n",
    "        time.sleep(300)\n",
    "        return scrape_intro_and_citations(row)\n",
    "    return (row[0], None, None)\n",
    "\n",
    "path_to_save = \"path_to_save\"\n",
    "\n",
    "for journal_name in all_data['Journal Name'].unique():\n",
    "    file_path = f\"{path_to_save}/{journal_name}.csv\"\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Journal {journal_name} already processed. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    counter[0] = 0  # Reset counter for each journal\n",
    "    print(f\"Processing journal: {journal_name}\")\n",
    "    \n",
    "    # Get all rows for the particular journal\n",
    "    journal_data = all_data[all_data['Journal Name'] == journal_name].copy()\n",
    "    \n",
    "    # Filter only the rows with 'Female' or 'Male' for scraping\n",
    "    scrape_data = journal_data[journal_data['Article_Gender'].isin(['Female', 'Male'])]\n",
    "\n",
    "    # Use ThreadPoolExecutor to scrape data concurrently\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        results = list(executor.map(scrape_intro_and_citations, scrape_data.itertuples(index=True, name=None)))\n",
    "\n",
    "    # Update the journal_data DataFrame directly with the scraped values\n",
    "    for index, intro, citations in results:\n",
    "        journal_data.at[index, 'Intro'] = intro\n",
    "        journal_data.at[index, 'Citations'] = citations\n",
    "\n",
    "    # Save the entire journal_data DataFrame (including rows with 'Mix' and 'Unknown') to a CSV\n",
    "    journal_data.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"Saved data for journal {journal_name} to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d85316-4134-4c40-b210-111e9cfbd934",
   "metadata": {},
   "source": [
    "Combine all the journal introes to one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6aef7-386e-440e-b4ca-825e1e083389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "path_to_save = \"path_to_save\"\n",
    "\n",
    "# List to store dataframes\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(path_to_save):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(path_to_save, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "total_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Save the total dataframe to a new CSV file\n",
    "output_file_path = os.path.join(path_to_save, \"Alle_Journal_intro.csv\")\n",
    "total_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Saved the concatenated DataFrame to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e57d5f-512d-463b-8c14-9954bf9c9f8e",
   "metadata": {},
   "source": [
    "Make a dummy for observatons containing an introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443a147e-4015-4899-b7be-5d29b9f8c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Copy the original DataFrame\n",
    "total_df_check = total_df.copy()\n",
    "\n",
    "# Step 2: Add a new column to the copied DataFrame\n",
    "total_df_check['Intro_check'] = total_df['Intro'].notnull().astype(int)\n",
    "\n",
    "# Step 3: Display the updated DataFrame\n",
    "print(total_df_check)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-chair",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ScienceDirect (Elsevier)\n",
    "To limit the data quantity transferred between steps, a lot of the preliminary data cleaning occurs concurrently with the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.sciencedirect.com/science/article/pii/S0927537123000404'\n",
    "user_agent = (\n",
    "    \"Magnus Eldrup (University of Copenhagen), Accessing OA data for project ascertaining gendered writing styles in academia\"\n",
    ")\n",
    "\n",
    "# Use a non-headless browser with a custom user-agent\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(f\"user-agent={user_agent}\")\n",
    "options.add_argument('--headless=new')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-engineer",
   "metadata": {},
   "source": [
    "#### Index articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Journals to scrape\n",
    "scrape = [('the-journal-of-socio-economics',48),('journal-of-economic-theory',212),('journal-of-international-economics', 145),\n",
    "('international-economics', 175),('resource-and-energy-economics',74),('economics-letters',231),('economic-modelling', 127),('journal-of-public-economics', 225),('economics-letters',231),('economic-modelling', 127),\n",
    "('journal-of-econometrics',236),('journal-of-development-economics',165),('journal-of-applied-economics',20)]\n",
    "\n",
    "n_vol = 45 #No. of volumes to scrape per journal\n",
    "\n",
    "def index_volumes(journal,vol, n_vol):\n",
    "    url_base = \"https://www.sciencedirect.com/journal/\" + str(journal) + \"/vol/\"\n",
    "    url_list = []\n",
    "    for i in range(n_vol): \n",
    "        url_pre = url_base + str(vol-i) + \"/suppl/C\"\n",
    "        url_list.append(url_pre)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    first_page_url = []\n",
    "    \n",
    "    for it, url in enumerate(url_list):\n",
    "        print(f\"Processing URL {it+1}/{len(url_list)}: {url}\")\n",
    "        try:\n",
    "            driver.get(url)\n",
    "        \n",
    "            # Wait for the first link with class \"article-content-title\" to be clickable\n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            link = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.article-content-title')))\n",
    "        \n",
    "            # Click the link\n",
    "            link.click()\n",
    "        \n",
    "            # Get the current URL of the page\n",
    "            first_page_url.append(driver.current_url)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for URL {url}\")\n",
    "            \n",
    "    return first_page_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-surname",
   "metadata": {},
   "source": [
    "#### Scrape Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article(first_page_url):\n",
    "    journal = []\n",
    "    for it, url in enumerate(first_page_url):\n",
    "        print(f\"Processing URL {it+1}/{len(first_page_url)}: {url}\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        i = 0\n",
    "\n",
    "        page_source = []\n",
    "\n",
    "        while i < 50:\n",
    "            time.sleep(5)  # Wait for a longer time between requests\n",
    "\n",
    "            #print(i)\n",
    "            page_source.append(driver.page_source)\n",
    "\n",
    "            try:\n",
    "                # Wait for the next button to become clickable before clicking it\n",
    "                wait = WebDriverWait(driver, 10)\n",
    "                next_button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, 'a.button-alternative-icon-right')))\n",
    "                next_button.click()\n",
    "\n",
    "                i += 1\n",
    "            except Exception as e:\n",
    "                print(\"Error with article\")\n",
    "                break\n",
    "    \n",
    "        journal.append(page_source)\n",
    "        \n",
    "\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    return journal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-auckland",
   "metadata": {},
   "source": [
    "#### Data Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-indie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(page_source):\n",
    "    df_data = []\n",
    "\n",
    "    for j in range(len(page_source)):\n",
    "        authors_dict = {}  # Create a new dictionary for each row\n",
    "        journal_name = []\n",
    "        citations = []\n",
    "        year = []\n",
    "        introduction = []\n",
    "        possible_id = ['sec_0001','sec_001','sec_01','sec_1','sec1']\n",
    "        \n",
    "        soup = bs(page_source[j], 'html.parser')\n",
    "        \n",
    "        \"\"\"\n",
    "        for ids in possible_id:\n",
    "            element_with_id = soup.select_one('section#' + ids)\n",
    "            if element_with_id:\n",
    "                break \n",
    "            \n",
    "        if element_with_id:\n",
    "            introduction = element_with_id.get_text(strip=True)\n",
    "        else:\n",
    "            introduction = None  # Handle case when element is not found\n",
    "            #print('Intro Error')\n",
    "        \"\"\"\n",
    "        # Find the <div> with id=\"body\"\n",
    "        body_div = soup.find('div', id='body')\n",
    "\n",
    "        # Find the first <section> within the <div>\n",
    "        try:\n",
    "            first_section = body_div.find('section')\n",
    "        except:\n",
    "            first_section = None\n",
    "            \n",
    "        if first_section:\n",
    "            introduction = first_section.get_text(strip=True)\n",
    "        else:\n",
    "            introduction = None\n",
    "        \n",
    "        # Find all <span> elements with class=\"given-name\"\n",
    "        author_spans = soup.find_all('span', class_='given-name')\n",
    "\n",
    "        # Loop through the author spans and populate the dictionary\n",
    "        for i, author_span in enumerate(author_spans):\n",
    "            try:\n",
    "                if i < 5:\n",
    "                    authors_dict[f'Author {i+1}'] = author_span.get_text().split()[0]\n",
    "                    if \".\" in authors_dict[f'Author {i+1}']:\n",
    "                        author_name = \"Unknown\"\n",
    "                        authors_dict[f'Author {i+1}'] = author_name\n",
    "                else:\n",
    "                    break\n",
    "            except AttributeError:\n",
    "                authors_dict[f'Author {i+1}'] = \"None\"\n",
    "                #print('error author')\n",
    "        \n",
    "        # Fill in remaining entries with \"None\" for Author 4 and Author 5\n",
    "        for i in range(len(author_spans), 5):\n",
    "            authors_dict[f'Author {i+1}'] = \"None\"\n",
    "        \n",
    "        try:\n",
    "            j_name = soup.find('a', class_=\"publication-title-link\").get_text()\n",
    "            journal_name = str(j_name)\n",
    "        except:\n",
    "            journal_name = \"Error no journal name\"\n",
    "            #print('error journal')\n",
    "            \n",
    "        try:\n",
    "            cit = soup.find('header', id=\"citing-articles-header\").get_text().split()[2]\n",
    "            #cit = cit[-1:][:-1]\n",
    "            #citations = int(cit)\n",
    "            cit = ''.join(filter(str.isdigit, cit))\n",
    "            citations = int(cit)\n",
    "            #citations = cit\n",
    "        except:\n",
    "            citations = None\n",
    "            #print('error citations')\n",
    "            \n",
    "        try:\n",
    "            yr = soup.find('div', class_=\"text-xs\").get_text()\n",
    "            year = yr\n",
    "\n",
    "        except:\n",
    "            year = None\n",
    "            #print('error yr')\n",
    "        \n",
    "        \n",
    "        # Append the data for this row to the list of rows\n",
    "        df_data.append({'Introduction': introduction, **authors_dict, 'Journal': journal_name, 'Year': year, 'Citations': citations})\n",
    "\n",
    "    df = pd.DataFrame(df_data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract the year from the code ###\n",
    "def extract_year(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    year_match = re.search(r'(\\d{4})', s)\n",
    "    if year_match:\n",
    "        return year_match.group(1)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dataframes(list_of_lists):\n",
    "    combined_df = pd.DataFrame()  # Initialize an empty DataFrame to store the combined data\n",
    "\n",
    "    for sublist in list_of_lists:\n",
    "        # Apply create_df function to the sublist and get a dataframe\n",
    "        df = create_df(sublist)\n",
    "        \n",
    "        # Concatenate the current dataframe with the combined dataframe\n",
    "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "\n",
    "    df['Year'] = df['Year'].apply(extract_year)\n",
    "    #df['Author 4'] = df['Author 4'].astype(str)\n",
    "    #df['Author 5'] = df['Author 5'].astype(str)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-brief",
   "metadata": {},
   "source": [
    "#### Execute Scraping & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-metadata",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles = []\n",
    "\n",
    "for j_1 in scrape:\n",
    "    name_of_journal, j_vol = j_1\n",
    "    print(name_of_journal)\n",
    "    url_index = index_volumes(name_of_journal , j_vol, n_vol)\n",
    "    print('no of vols ' + str(len(url_index)))\n",
    "    \n",
    "    data = scrape_article(url_index)\n",
    "    #with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    #    data = executor.map(scrape_article, url_index)\n",
    "    data_treated = combine_dataframes(data)\n",
    "    \n",
    "    #articles.append(data)\n",
    "    \n",
    "    # Specify the CSV file name\n",
    "    csv_file = 'articles_out_' + str(name_of_journal) + '.csv'\n",
    "    data_treated.to_csv(csv_file, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-drunk",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cleaning & Combining Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-republic",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ScienceDirect\n",
    "This first piece of code takes all the individual scraped journals from ScienceDirect and assigns genders and exports them. Be aware of the directory from which it pulls the files. It may be necessary to move some files for this process to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to your own file directory!\n",
    "directory = 'scrape_raw'\n",
    "\n",
    "file_list = []\n",
    "\n",
    "def index_files():\n",
    "    for filename in os.listdir(directory):\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            file_list.append(filename)\n",
    "    print(file_list)\n",
    "    return file_list\n",
    "\n",
    "\n",
    "def import_file(name):\n",
    "    file_path = os.path.join(directory, name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "## ASSIGNING GENDER ##\n",
    "\n",
    "# Initialize the gender detector\n",
    "detector = gender.Detector()\n",
    "\n",
    "# Function to predict gender using the genderize.io API\n",
    "def predict_gender_first_name_api(name):\n",
    "    try:\n",
    "        api_url = f'https://api.genderize.io?name={name}'\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            try:\n",
    "                gender = data[\"gender\"].capitalize()\n",
    "                return gender\n",
    "            except Exception as e:\n",
    "                print(f\"Error predicting gender for {name}: {e}\")\n",
    "                return \"Unknown\"\n",
    "        else:\n",
    "            #print(f\"Request failed. Status code: {response.status_code}\")\n",
    "            return \"Unknown\"\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    \n",
    "\n",
    "# Function to predict gender for the first name\n",
    "def predict_gender_first_name(name):\n",
    "    gender = detector.get_gender(name)\n",
    "    if name == \"None\":\n",
    "        return \"None\"\n",
    "    elif \".\" in name:\n",
    "        return \"None\"\n",
    "        print(name)\n",
    "    elif gender == \"unknown\":\n",
    "        #print(\"api\")\n",
    "        gender_api = predict_gender_first_name_api(name)\n",
    "        return gender_api\n",
    "    else:\n",
    "        if gender in {'male', 'female'}:\n",
    "            return gender\n",
    "        else:\n",
    "            return \"Unknown\" \n",
    "\n",
    "    \n",
    "# Function to get the article gender based on author genders\n",
    "def get_article_gender(row):\n",
    "    genders = [row[f'Gender_Author {i+1}'] for i in range(5) if not pd.isnull(row[f'Gender_Author {i+1}'])]\n",
    "    if len(genders) == 0:\n",
    "        return 'Unknown'\n",
    "    elif all(gender == 'male' for gender in genders):\n",
    "        return 'Male'\n",
    "    elif all(gender == 'female' for gender in genders):\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'Mix'\n",
    "    \n",
    "# Exporting the processed dataframes as .csv files\n",
    "def export_treated(df):\n",
    "    # Get the name from the first row of the 'Journal' column\n",
    "    name = df.loc[0, 'Journal']\n",
    "    \n",
    "    # Remove any special characters and spaces from the name to make it suitable for a filename\n",
    "    name = name.replace(' ', '_').replace('.', '').replace(',', '')\n",
    "\n",
    "    # Define the directory to save the treated data\n",
    "    treated_directory = 'treated_data'\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(treated_directory):\n",
    "        os.makedirs(treated_directory)\n",
    "\n",
    "    # Export the DataFrame to a CSV file within the treated_data directory\n",
    "    csv_filename = os.path.join(treated_directory, name + '_treated.csv')\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"DataFrame exported as {csv_filename}\")\n",
    "    \n",
    "    return 'Success'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-selection",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Execute gender prediction & export ScienceDirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-pound",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Executes the code above\n",
    "files = index_files()\n",
    "Status = []\n",
    "\n",
    "for i, j_title in enumerate(files):\n",
    "    print(j_title)\n",
    "    df = import_file(j_title)\n",
    "    \n",
    "    # Apply gender prediction to the first name in each author column and create new gender columns\n",
    "    for i in range(1, 6):\n",
    "        author_col = f'Author {i}'\n",
    "        gender_col = f'Gender_{author_col}'\n",
    "        df[gender_col] = df[author_col].apply(predict_gender_first_name)\n",
    "        \n",
    "    for i in range(1, 6):\n",
    "        gender_col = f'Gender_Author {i}'\n",
    "        df[gender_col] = df[gender_col].replace('None', None)\n",
    "        df[gender_col] = df[gender_col].str.lower()\n",
    "        \n",
    "    # Apply the function to create the \"Article_Gender\" column\n",
    "    df['Article_Gender'] = df.apply(get_article_gender, axis=1)\n",
    "\n",
    "    # Display the updated DataFrame\n",
    "    df\n",
    "    female_count = len(df[df['Article_Gender'] == 'Female'])\n",
    "    print(f\"Number of rows with 'Article_Gender' == 'Female': {female_count}\")\n",
    "    female_count2 = len(df[df['Gender_Author 1'] == 'female'])\n",
    "    print(f\"Number of rows with first author == 'Female': {female_count2}\")\n",
    "    \n",
    "    s = export_treated(df)\n",
    "    Status.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-harrison",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Merge Documents, Science Direct\n",
    "This following section combines all the treated journals from ScienceDirect into a single .csv file. Once again, it is essential to ensure that the directory of the code matches the directory where your actual treated files (and only your treated files) are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-raise",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Get a list of all .csv files in the current directory\n",
    "csv_files = [file for file in os.listdir(current_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "print(csv_files)\n",
    "\n",
    "# Read each .csv file and append its content to the list\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(current_directory, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Export the combined dataframe to a new .csv file\n",
    "output_file = \"ScienceDirectData.csv\"\n",
    "output_path = os.path.join(current_directory, output_file)\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined dataframe exported to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-artwork",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Loade Springer and SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d063d0-af2f-4d33-b257-8e50b57b1926",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"file_path\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "SD = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "SD = SD.dropna(subset=['Introduction'])\n",
    "\n",
    "# Reset the index\n",
    "SD = SD.reset_index(drop=True)\n",
    "\n",
    "print(SD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"file_path\"\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "springer = pd.read_csv(file_path)\n",
    "\n",
    "springer = springer.dropna(subset=['Intro'])\n",
    "\n",
    "# Reset the index\n",
    "springer = springer.reset_index(drop=True)\n",
    "\n",
    "print(springer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-spectacular",
   "metadata": {},
   "source": [
    "#### combine data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926039fd-c2f3-4e05-a2cd-8b7210bef77e",
   "metadata": {},
   "source": [
    "Rename and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-square",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SD = SD.rename(columns={\"Journal\": \"Journal Name\", \"Year\" : \"Journal Year\"})\n",
    "springer = springer.rename(columns={\"Intro\": \"Introduction\"})\n",
    "\n",
    "\n",
    "columns_df1 = set(SD.columns)\n",
    "columns_df2 = set(springer.columns)\n",
    "\n",
    "# Find common column names\n",
    "common_columns = columns_df1.intersection(columns_df2)\n",
    "\n",
    "# Find differing column names\n",
    "different_columns_df1 = columns_df1 - common_columns\n",
    "different_columns_df2 = columns_df2 - common_columns\n",
    "\n",
    "# Print results\n",
    "print(\"Common columns:\", common_columns)\n",
    "print(\"Columns in DataFrame 1 only:\", different_columns_df1)\n",
    "print(\"Columns in DataFrame 2 only:\", different_columns_df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Identify columns to keep from DataFrame 1\n",
    "cols_to_keep_from_df1 = {'Gender_Author 4', 'Author 4', 'Gender_Author 5', 'Author 5'}\n",
    "\n",
    "# Identify columns to exclude from DataFrame 2\n",
    "cols_to_exclude_from_df2 = {'Volume', 'Published Date', 'Issue', 'Intro_check', 'Title', 'Link'}\n",
    "\n",
    "# Filter the dataframes\n",
    "SD_filtered = SD[list(columns_df1.intersection(columns_df2).union(cols_to_keep_from_df1))]\n",
    "springer_filtered = springer[list(columns_df2 - cols_to_exclude_from_df2)]\n",
    "\n",
    "# Fill the non-existing columns in SD with NaN\n",
    "for col in cols_to_keep_from_df1:\n",
    "    if col not in SD_filtered.columns:\n",
    "        SD_filtered[col] = np.nan\n",
    "\n",
    "# Concatenate the two dataframes vertically\n",
    "Journal_merged = pd.concat([SD_filtered, springer_filtered], axis=0, ignore_index=True)\n",
    "\n",
    "Journal_merged = Journal_merged.reset_index(drop=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV\n",
    "Journal_merged.to_csv(\"Journal_merged_path\", index=False)\n",
    "\n",
    "\n",
    "# Use a regular expression to extract the month and year\n",
    "Journal_merged['Journal Year'] = Journal_merged['Journal Year'].str.extract(r'(\\w+ \\d{4})')\n",
    "\n",
    "# Save the modified DataFrame to a CSV\n",
    "Journal_merged.to_csv(\"/Journal_merged_path\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Filter the rows where Article_Gender is either 'Female' or 'Male'\n",
    "Journal_merged = Journal_merged[Journal_merged['Article_Gender'].isin(['Female', 'Male'])]\n",
    "\n",
    "# Create a new column gen_dummy: 1 for Female and 0 for Male\n",
    "Journal_merged['gen_dummy'] = Journal_merged['Article_Gender'].map({'Female': 1, 'Male': 0})\n",
    "\n",
    "# Reset the index after filtering\n",
    "Journal_merged.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save the cleaned DataFrame to a CSV\n",
    "Journal_merged.to_csv(\"Journal_merged_path.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Specify the desired column order\n",
    "column_order = [\n",
    "    'Introduction', \n",
    "    'Article_Gender', \n",
    "    \"gen_dummy\",\n",
    "    'Journal Year', \n",
    "    'Journal Name', \n",
    "    'Citations', \n",
    "    'Author 1', \n",
    "    'Gender_Author 1', \n",
    "    'Author 2', \n",
    "    'Gender_Author 2', \n",
    "    'Author 3', \n",
    "    'Gender_Author 3', \n",
    "    'Author 4', \n",
    "    'Gender_Author 4', \n",
    "    'Author 5', \n",
    "    'Gender_Author 5'\n",
    "]\n",
    "\n",
    "# Reorder the columns in the DataFrame\n",
    "Journal_merged = Journal_merged[column_order]\n",
    "\n",
    "# Save the rearranged DataFrame to a CSV\n",
    "Journal_merged.to_csv(\"Journal_merged_path.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-identity",
   "metadata": {},
   "source": [
    "#### Final cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the CSV file into a pandas DataFrame\n",
    "combined_df = pd.read_csv(\"journal_merged.csv\")\n",
    "\n",
    "# Step 2: Rename the column \"Introduction\" to \"Intro\"\n",
    "combined_df.rename(columns={\"Introduction\": \"Intro\"}, inplace=True)\n",
    "\n",
    "# Step 3: Drop rows where the \"Intro\" column has NaN values\n",
    "combined_df.dropna(subset=['Intro'], inplace=True)\n",
    "\n",
    "# Make a copy of the original dataframe to work on\n",
    "df_cleaned = combined_df.copy()\n",
    "\n",
    "# Set up the stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove content between parentheses using regex\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags/markup\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove punctuations, numbers and other non-alphabetic characters\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stop_words(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatize_text(text):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Process tracking counter\n",
    "counter = 0\n",
    "\n",
    "# Apply the cleaning function\n",
    "def process_and_track(func, text):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    if counter % 10000 == 0:\n",
    "        print(f\"Processed {counter} records for cleaning.\")\n",
    "    return func(text)\n",
    "\n",
    "# Apply the cleaning function\n",
    "df_cleaned['Intro_Cleaned'] = df_cleaned['Intro'].apply(lambda x: process_and_track(clean_text, x))\n",
    "# Apply removal of stop words\n",
    "df_cleaned['Intro_Cleaned'] = df_cleaned['Intro_Cleaned'].apply(lambda x: process_and_track(remove_stop_words, x))\n",
    "# Apply lemmatization\n",
    "df_cleaned['Intro_Cleaned'] = df_cleaned['Intro_Cleaned'].apply(lambda x: process_and_track(lemmatize_text, x))\n",
    "\n",
    "# Save cleaned version\n",
    "df_cleaned.to_csv('data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-consultation",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-position",
   "metadata": {},
   "source": [
    "Be aware that most of the analysis of this paper was run on external computational servers and may as such be difficult to run on a personal computer given current specifications. Additionally, the RF and logistic regression only run when importing the final 'data_cleaned.csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-album",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "economic-pepper",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-biography",
   "metadata": {},
   "source": [
    "#### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "\n",
    "def process_column_rf(column_name, data):\n",
    "    \"\"\"Function to process data using RandomForest.\"\"\"\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    y = data_cleaned['Article_Gender']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Vectorize using unigrams & bigram\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "    X = tfidf_vectorizer.fit_transform(data_cleaned[column_name])\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train RandomForest without cross-validation\n",
    "    rf = RandomForestClassifier(n_estimators=100, class_weight='balanced', oob_score=True, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "    # Get scores for Female\n",
    "    results['precision_female'] = precision_score(y_test, y_pred, pos_label='Female', zero_division=0)\n",
    "    results['recall_female'] = recall_score(y_test, y_pred, pos_label='Female', zero_division=0)\n",
    "    results['f1_score_female'] = f1_score(y_test, y_pred, pos_label='Female', zero_division=0)\n",
    "    \n",
    "    # Get scores for Male\n",
    "    results['precision_male'] = precision_score(y_test, y_pred, pos_label='Male', zero_division=0)\n",
    "    results['recall_male'] = recall_score(y_test, y_pred, pos_label='Male', zero_division=0)\n",
    "    results['f1_score_male'] = f1_score(y_test, y_pred, pos_label='Male', zero_division=0)\n",
    "\n",
    "    # Accuracy score\n",
    "    results['accuracy_score'] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Process 'Intro_Cleaned' column without dummies\n",
    "results_intro_cleaned = process_column_rf('Intro_Cleaned', df)\n",
    "\n",
    "for metric, score in results_intro_cleaned.items():\n",
    "    print(f\"Results for 'Intro_Cleaned' without dummies using {metric}:\")\n",
    "    print(f\"Score: {score}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-nickname",
   "metadata": {},
   "source": [
    "#### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('data_cleaned.csv')\n",
    "df['gen_dummy'] = df['gen_dummy'].replace({1: 0, 0: 1})\n",
    "data_cleaned = df.dropna(subset=['Intro_Cleaned'])\n",
    "y = data_cleaned['gen_dummy']\n",
    "x = data_cleaned['Intro_Cleaned']\n",
    "\n",
    "results = {}\n",
    "\n",
    "f1_wom_vec = []\n",
    "acc_vec = []\n",
    "\n",
    "# Create a custom scorer for F1 score for women (pos=0)\n",
    "def f1_score_women(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "\n",
    "custom_scorer = make_scorer(f1_score_women)\n",
    "    \n",
    "# Vectorize using unigrams & bigram\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(2, 2))\n",
    "X = tfidf_vectorizer.fit_transform(x)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_pre, X_test, y_pre, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "learning_vec = [0.0001]\n",
    "#[0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [30,70,150],\n",
    "    'min_samples_split': [10,20,40],\n",
    "    'min_samples_leaf': [4,8,12]\n",
    "}\n",
    "\n",
    "class_weights_d = dict()\n",
    "class_weights_d[0] = 4\n",
    "class_weights_d[1] = 1\n",
    "\n",
    "for i, share in enumerate(learning_vec):\n",
    "    print(share)\n",
    "    X_train, X_discard, y_train, y_discard = train_test_split(X_pre, y_pre, test_size=share, random_state=42)\n",
    "    \n",
    "    grid_search_rf = GridSearchCV(RandomForestClassifier(class_weight=class_weights_d, oob_score=True, random_state=42),\n",
    "                                      rf_params,\n",
    "                                      cv=5,\n",
    "                                      #scoring='f1',\n",
    "                                      scoring=custom_scorer\n",
    "                                      )\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    best_params_rf = grid_search_rf.best_params_\n",
    "        \n",
    "    # Use the best estimator for predictions\n",
    "    best_rf = grid_search_rf.best_estimator_\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "        \n",
    "    # Calculate scores for female\n",
    "    precision_female = precision_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
    "    recall_female = recall_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
    "    f1_female = f1_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
    "    \n",
    "    # Calculate scores for male\n",
    "    precision_male = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    recall_male = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    f1_male = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    f1_wom_vec.append(f1_female)\n",
    "    acc_vec.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-antique",
   "metadata": {},
   "source": [
    "Printing results from cross-validated bigram RF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Scores \\n')\n",
    "print('Accuracy ' + str(accuracy))\n",
    "print('F1 Female ' + str(f1_female))\n",
    "print('Recall Female ' + str(recall_female))\n",
    "print('Precision Female ' + str(precision_female))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-vermont",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf\n",
    "\n",
    "learning_vec = [0.99,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0.01]\n",
    "#[0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'n_estimators': [500],\n",
    "    'max_depth': [150],\n",
    "    'min_samples_split': [40],\n",
    "    'min_samples_leaf': [8]\n",
    "}\n",
    "\n",
    "class_weights_d = dict()\n",
    "class_weights_d[0] = 4\n",
    "class_weights_d[1] = 1\n",
    "\n",
    "for i, share in enumerate(learning_vec):\n",
    "    print(share)\n",
    "    X_train, X_discard, y_train, y_discard = train_test_split(X_pre, y_pre, test_size=share, random_state=42)\n",
    "    \n",
    "    # Create a RandomForestClassifier instance with the best parameters\n",
    "    best_rf = RandomForestClassifier(class_weight=class_weights_d,\n",
    "                                 oob_score=True,\n",
    "                                 random_state=42,\n",
    "                                 **best_params_rf)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    best_rf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the trained model to make predictions on the test data\n",
    "    y_pred = best_rf.predict(X_test)    \n",
    "    # Calculate scores for female\n",
    "    precision_female = precision_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
    "    recall_female = recall_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
    "    f1_female = f1_score(y_test, y_pred, pos_label=0, zero_division=0)\n",
    "    \n",
    "    # Calculate scores for male\n",
    "    precision_male = precision_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    recall_male = recall_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    f1_male = f1_score(y_test, y_pred, pos_label=1, zero_division=0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    f1_wom_vec.append(f1_female)\n",
    "    acc_vec.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-answer",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BERT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c488a5-9525-416d-9dba-00cb2ac3c290",
   "metadata": {},
   "source": [
    "Make a dataset with binary clasification varaible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f57d029-a521-4785-ae1b-fd3367908bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the \"Introduction\" and \"gen_dummy\" columns from Journal_merged\n",
    "df_extracted = Journal_merged[['Introduction', 'gen_dummy']]\n",
    "\n",
    "# Save the resulting DataFrame back to a new CSV\n",
    "df_extracted.to_csv('Journal_Introduction_GenDummy.csv', index=False)\n",
    "\n",
    "print(df_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77936e-1508-4c50-95cc-40e587c92fe7",
   "metadata": {},
   "source": [
    "Removing HTML for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9a47d-c10d-4bed-9939-084890fa0511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('PATH/Journal_Introduction_GenDummy.csv')\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "\n",
    "    # Remove the prefix \"1. Introduction\"\n",
    "    text = re.sub(r'^1\\. Introduction', '', text)\n",
    "\n",
    "    # Remove excessive whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'Introduction' column\n",
    "df['Introduction'] = df['Introduction'].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef8985-84c5-4d26-bb9f-f3192a71dd97",
   "metadata": {},
   "source": [
    "Load BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79b416-726d-4ae5-a894-61975350d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer and model\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e76dbc-a8fe-41c8-b2a9-0d22cb51a4e8",
   "metadata": {},
   "source": [
    "Tokenize the text and split to train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864a366-13cf-4a56-8b8b-3a6932273431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Split your dataframe into training and testing data. You can use sklearn's train_test_split for this\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize your training and testing data\n",
    "train_encodings = tokenizer.batch_encode_plus(train_df['Introduction'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer.batch_encode_plus(test_df['Introduction'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GenderDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = GenderDataset(train_encodings, train_df['gen_dummy'].tolist())\n",
    "test_dataset = GenderDataset(test_encodings, test_df['gen_dummy'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef0132-2f07-4474-b1a1-6d3b4f415a6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Running the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268a866-d573-47b6-96f4-d35f563bf18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)  # num_labels=2 for binary classification\n",
    "\n",
    "# Define training arguments and initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optim.AdamW(model.parameters(), lr=5e-5), None)  # Using PyTorch's AdamW\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db02516c-3334-4eeb-ba1a-3623dc9d5849",
   "metadata": {},
   "source": [
    "Then Hyperparameter tuning with male and female equaly weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dcf0da-c599-4e5c-8adc-2c59a3e8b453",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting labels and calculating class weights\n",
    "\n",
    "labels = [entry[\"labels\"] for entry in train_dataset]\n",
    "class_counts = Counter(labels)\n",
    "\n",
    "total_samples = sum(class_counts.values())\n",
    "weights = [total_samples/class_counts.get(i, 1) for i in range(2)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(weights).to(device))\n",
    "\n",
    "# Custom Trainer with overridden compute_loss\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        labels = inputs[\"labels\"]\n",
    "        loss = criterion(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter space\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16])\n",
    "\n",
    "    # Initialize BERT model for sequence classification\n",
    "    model2 = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = CustomTrainer(\n",
    "        model=model2,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        optimizers=(optim.AdamW(model2.parameters(), lr=lr), None)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "        results = trainer.evaluate()\n",
    "        return results[\"eval_loss\"]\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(\"CUDA out of memory. Trying a smaller batch size...\")\n",
    "            trial.set_user_attr(\"OOM\", True)\n",
    "            return float('inf')\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters found: {best_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7caefc4-45f9-4109-9af1-d773a4a89135",
   "metadata": {},
   "source": [
    "Runining the model with different weights, using the found hyperparameters from stage before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2ce60-5c31-4e7c-bbfe-c26c78e6184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Extracting labels and calculating class weights as before\n",
    "labels = [entry[\"labels\"] for entry in train_dataset]\n",
    "class_counts = Counter(labels)\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize BERT model for sequence classification\n",
    "model_base = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def train_and_evaluate_with_weight(weight_for_positive_class):\n",
    "    # Set the weights for the classes\n",
    "    weights = [1.0, weight_for_positive_class]\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(weights, dtype=torch.float).to(device))\n",
    "\n",
    "    # Adjust logging directory for each weight\n",
    "    unique_log_dir = f'./logs_best_weight_{weight_for_positive_class}'\n",
    "    \n",
    "    training_args_best = TrainingArguments(\n",
    "        output_dir=f'./results_best_weight_{weight_for_positive_class}', \n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=unique_log_dir,\n",
    "        logging_steps=100,           # Log and evaluate every 100 steps\n",
    "        learning_rate=1.1836694153539516e-05,\n",
    "        evaluation_strategy='steps',  # Evaluate every logging_steps\n",
    "        eval_steps=100                # Evaluate every 100 steps\n",
    "    )\n",
    "\n",
    "    # Custom Trainer with overridden compute_loss\n",
    "    class CustomTrainerForEvaluation(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            labels = inputs[\"labels\"]\n",
    "            loss = criterion(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        preds = np.argmax(p.predictions, axis=1)\n",
    "        labels = p.label_ids\n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds)\n",
    "        precision = precision_score(labels, preds)\n",
    "        recall = recall_score(labels, preds)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "\n",
    "    # Instantiate a new model for every run\n",
    "    model_best = model_base.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    trainer_best = CustomTrainerForEvaluation(\n",
    "        model=model_best,\n",
    "        args=training_args_best,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    trainer_best.train()\n",
    "    results = trainer_best.evaluate()\n",
    "\n",
    "    # Print metrics for this weight\n",
    "    for key, value in results.items():\n",
    "        print(f\"Weight {weight_for_positive_class}, {key}: {value}\")\n",
    "\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Iterate over the desired weights for females only: 1, 3, 5, 7, 9, 11\n",
    "desired_weights = [1, 3, 5, 7, 9, 11]\n",
    "for weight in desired_weights:\n",
    "    train_and_evaluate_with_weight(weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "black-vegetarian",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generating Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-causing",
   "metadata": {},
   "source": [
    "### Performance Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tuning_params = 18\n",
    "Tuning_param_vec = np.logspace(np.log10(0.01), np.log10(8), num=num_tuning_params)\n",
    "lambdaa = 1/ Tuning_param_vec\n",
    "\n",
    "# Apply the fivethirtyeight style to the plot\n",
    "plt.style.use('seaborn')\n",
    "# Set the font to \"Verdana\"\n",
    "plt.rcParams['font.family'] = 'Verdana'\n",
    "colors_vec = ['#003f5c', '#bc5090', '#ffa600']\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "file_path = '2gram_performance_scores.csv'\n",
    "df1 = pd.read_csv(file_path)\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "file_path = '2GRAMPR.csv'\n",
    "df2 = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "file_path = 'plot_data.csv'\n",
    "df3 = pd.read_csv(file_path)\n",
    "df3\n",
    "\n",
    "x_11 = df3['Run_Number']\n",
    "x_11 = x_11[:-1]\n",
    "x_11 = x_11.append(pd.Series([11]))\n",
    "y_11 = df3['Best_F1_Score']\n",
    "y_11 = y_11[:-1]\n",
    "y_11 = y_11.append(pd.Series([0.356]))\n",
    "y_12 = df3['Accuracy']\n",
    "y_12 = y_12[:-1]\n",
    "y_12 = y_12.append(pd.Series([0.56]))\n",
    "\n",
    "\n",
    "# Generate fem_weight values using np.linspace\n",
    "fem_weight = np.linspace(1, 11, 22)\n",
    "\n",
    "# Create a 1x2 grid of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4), facecolor='white')  # Two identical square figures\n",
    "\n",
    "input_params = lambdaa\n",
    "\n",
    "label1 = ['Male F1 (Logistic)', 'Female F1 (Logistic)', 'Accuracy (Logistic)']\n",
    "\n",
    "for i, score_type in enumerate(['Male F1', 'Female F1', 'Accuracy']):\n",
    "    axs[0].plot(fem_weight, df1[score_type], color=colors_vec[i], label=label1[i])\n",
    "\n",
    "    \n",
    "axs[0].plot(x_11, y_11, linestyle='--', linewidth=2.5, alpha=0.89, color=colors_vec[1], label='Female F1 (BERT)')\n",
    "axs[0].plot(x_11, y_12, linestyle='--', linewidth=2.5, alpha=0.89, color=colors_vec[2], label='Accuracy (BERT)')\n",
    "    \n",
    "scatter = axs[1].scatter(df2['Female_Recall'], df2['Female_Precision'], c=input_params, cmap='viridis', norm=LogNorm())\n",
    "\n",
    "# Plotting for both subplots\n",
    "for axs_idx in axs:\n",
    "\n",
    "    axs[0].set_xlabel('Relative Weight, women', fontsize=10)\n",
    "    axs[0].set_ylabel('F1 Score', fontsize=10)\n",
    "    axs[1].set_xlabel('Recall', fontsize=10)\n",
    "    axs[1].set_ylabel('Precision', fontsize=10)\n",
    "    axs[0].set_title('Performance for Different Weights', fontsize=12.5, pad=19)\n",
    "    axs[1].set_title('Precision Recall Trade-Off (Women)', fontsize=12.5, pad=19)\n",
    "    axs_idx.legend()\n",
    "\n",
    "    # Customize grid style with thicker lines and different linestyle\n",
    "    axs_idx.grid(True, linestyle='-', linewidth=1.9, alpha=0.99, color='white')\n",
    "\n",
    "    # Add plot borders\n",
    "    for spine in axs_idx.spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "    # Adjust tick labels and sizes\n",
    "    axs_idx.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "    # Set a light background color\n",
    "    axs_idx.set_facecolor('#f2f2f2')\n",
    "    #axs_idx.set_facecolor('#ffffff')\n",
    "\n",
    "    # Add a vertical dotted line at x = 5.13 with text\n",
    "    axs[0].axvline(x=5.13, color='#424242', linestyle='dotted')\n",
    "    axs[0].text(7.6, 0.95, 'Inverse Sample Weights', fontsize=8, color='#424242', ha='center')\n",
    "\n",
    "    # Adjust legend style and position\n",
    "    legend = axs[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.18), frameon=False, fontsize=7, ncol=3)\n",
    "    legend.get_frame().set_alpha(0.5)\n",
    "\n",
    "    # Set x-axis and y-axis limits\n",
    "    axs[0].set_xlim(0, 12)\n",
    "    axs[1].set_xlim(0.3, 0.6)\n",
    "    axs[0].set_ylim(0, 1)\n",
    "    axs[1].set_ylim(0.25, 0.35)\n",
    "\n",
    "# Adjust legend style and position\n",
    "legend = axs[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.18), frameon=False, fontsize=9, ncol=3)\n",
    "legend.get_frame().set_alpha(0.5)\n",
    "\n",
    "# Create a colorbar to show the mapping of values to colors\n",
    "cbar = plt.colorbar(scatter, )\n",
    "cbar.set_label('CV Tuning parameter, $\\lambda$ ')\n",
    "\n",
    "# Create a ScalarMappable object from the colormap for the legend\n",
    "sm = ScalarMappable(cmap='viridis', norm=LogNorm())\n",
    "sm.set_array([])  # Dummy array\n",
    "\n",
    "# Set the background color of the figure to white\n",
    "fig.set_facecolor('white')\n",
    "\n",
    "# Show the plots\n",
    "plt.savefig('PR_weight2.jpg', dpi=300, bbox_inches='tight', facecolor='white', format='jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-hearts",
   "metadata": {},
   "source": [
    "#### Deskriptive analyses plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_cleaned.csv')\n",
    "\n",
    "# Apply the seaborn style and set font to \"Verdana\"\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['font.family'] = 'Verdana'\n",
    "colors_vec = ['#003f5c','#bc5090','#ffa600']\n",
    "\n",
    "# Top 20 Journals by Gender Distribution\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "top_ratios_percentage.sort_values(by='Female', ascending=False).plot(kind='bar', stacked=True, ax=ax, color=colors_vec[:2], alpha=0.75)\n",
    "\n",
    "# Calculate the average percentage of female authors across all journals and add it as a line\n",
    "average_female_percentage = df['Article_Gender'].value_counts(normalize=True)['Female'] * 100\n",
    "ax.axhline(y=average_female_percentage, color=colors_vec[2], linestyle='--', label=f'Average Female % Across All Journals ({average_female_percentage:.2f}%)')\n",
    "\n",
    "plt.title('Top 20 Journals by Gender Distribution', fontsize=16, pad=19)\n",
    "plt.ylabel('Percentage (%)', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "ax.grid(True, linestyle='-', linewidth=1.9, alpha=0.99, color='white')\n",
    "ax.set_facecolor('#f2f2f2')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Reordering the legend labels, increasing the font size, and positioning it further below the x-axis labels\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "order = [1, 0, 2]\n",
    "ax.legend([handles[idx] for idx in order], [labels[idx] for idx in order], loc='upper center', \n",
    "          bbox_to_anchor=(0.5, -0.55), ncol=3, fontsize='large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"top_journals_gender_distribution.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style and set font to \"Verdana\"\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['font.family'] = 'Verdana'\n",
    "colors_vec = ['#003f5c','#bc5090','#ffa600']\n",
    "\n",
    "# Calculate overall averages for male and female from the yearly data\n",
    "avg_female = yearly_gender_ratio['Female'].mean()\n",
    "\n",
    "# 2. Annual Gender Distribution in Articles\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "yearly_gender_ratio.plot(kind='bar', stacked=True, ax=ax, color=colors_vec, alpha=0.75)\n",
    "plt.title('Annual Gender Distribution in Articles', fontsize=16, pad=19)\n",
    "plt.ylabel('Percentage (%)', fontsize=14)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.xticks(rotation=0, fontsize=12)\n",
    "plt.grid(True, linestyle='-', linewidth=1.9, alpha=0.99,  color='white')\n",
    "ax.set_facecolor('#f2f2f2')\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_visible(False)\n",
    "\n",
    "# Add the average line for female\n",
    "ax.axhline(avg_female, color=colors_vec[2], linestyle='--', label=f'Average Female: {avg_female:.2f}%') \n",
    "\n",
    "# Reordering the legend labels and increasing the font size\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "order = [1, 0, 2]\n",
    "ax.legend([handles[idx] for idx in order], [labels[idx] for idx in order], loc='upper center', \n",
    "          bbox_to_anchor=(0.5, -0.15), ncol=3, fontsize='large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"annual_gender_distribution_with_avg_female.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male data\n",
    "male_data = {\n",
    "    'Type': ['Male'] * 10,\n",
    "    'Feature': ['real world', 'innovation systems', 'marginal cost', 'labor productivity', 'general equilibrium', 'industrial organization', 'united kingdom', 'host country', 'economic theory', 'Game Theory'],\n",
    "    'Coefficient': [2.036885586402584, 2.0005216149368934, 1.9900478322321766, 1.9709577684657895, 1.8614445247273377, 1.8510086473847436, 1.8300256906013528, 1.7787337070611517, 1.7562447711011193, 1.752180558]\n",
    "}\n",
    "\n",
    "# Female data\n",
    "female_data = {\n",
    "    'Type': ['Female'] * 10,\n",
    "    'Feature': ['child care', 'gap literature', 'provides detailed', 'young children', 'socio economic', 'health education', 'time period', 'data collection', 'unintended consequences', 'empirical evidence'],\n",
    "    'Coefficient': [ -2.1256116253124717, -2.181425890667425, -2.2456961604462324, -2.2818509794621864, -2.304889283248064, -2.329391345506251, -2.4106971165924493, -2.511812252882433, -2.513238243724366, -2.65146320504483]\n",
    "}\n",
    "\n",
    "# Journal coefficients\n",
    "journal_data = {\n",
    "    'Type': ['Journal'] * 93,  # Adjust this number to match the actual number of journals\n",
    "    'Feature': [\n",
    "        'Evolutionary and Institutional Economics Review',\n",
    "        'Journal of Geographical Systems',\n",
    "        'Transportation',\n",
    "        'International Journal of Game Theory',\n",
    "        'De Economist',\n",
    "        'International Economics and Economic Policy',\n",
    "        'Economics Letters',\n",
    "        'Environmental Economics and Policy Studies',\n",
    "        'NETNOMICS: Economic Research and Electronic Networking',\n",
    "        'The Japanese Economic Review',\n",
    "        # ... Add the rest of the journal names here\n",
    "        'SERIEs',\n",
    "        'Review of Agricultural, Food and Environmental Studies',\n",
    "        'Journal of Cultural Economics',\n",
    "        'Triple Helix',\n",
    "        'Journal of Economics, Race, and Policy',\n",
    "        'IZA Journal of Development and Migration',\n",
    "        'Indian Economic Review',\n",
    "        'China Finance and Economic Review',\n",
    "        'The Indian Journal of Labour Economics',\n",
    "        'International Advances in Economic Research'\n",
    "    ],\n",
    "    'Coefficient': [\n",
    "        1.1215714798392926,\n",
    "        1.0558490511098158,\n",
    "        1.0004738210890898,\n",
    "        0.9044355292488584,\n",
    "        0.8892826453881203,\n",
    "        0.866873388706106,\n",
    "        0.8495579822106452,\n",
    "        0.8420989827196874,\n",
    "        0.8350479119981685,\n",
    "        0.8168913094406065,\n",
    "         -0.9900800457162767,\n",
    "        -1.0180035864757404,\n",
    "        -1.0266764081142343,\n",
    "        -1.1807176468137563,\n",
    "        -1.254879064568282,\n",
    "        -1.265042814451788,\n",
    "        -1.370155888419316,\n",
    "        -1.3935800152507558,\n",
    "        -1.3951647824296334,\n",
    "        -1.5794244346661876\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Adjusting the 'Type' length in journal_data to match the actual number of journals\n",
    "journal_data['Type'] = ['Journal'] * len(journal_data['Feature'])\n",
    "\n",
    "# Convert the dictionaries to pandas dataframes again\n",
    "journal_df = pd.DataFrame(journal_data)\n",
    "\n",
    "# Separate the journals based on coefficients\n",
    "female_journal_df = journal_df[journal_df['Coefficient'] < 0].copy()\n",
    "male_journal_df = journal_df[journal_df['Coefficient'] > 0].copy()\n",
    "\n",
    "# Set 'Type' for these separated journals\n",
    "female_journal_df['Type'] = ['Female'] * len(female_journal_df)\n",
    "male_journal_df['Type'] = ['Male'] * len(male_journal_df)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "all_data = pd.concat([male_df, female_df, female_journal_df, male_journal_df])\n",
    "\n",
    "all_data\n",
    "\n",
    "\n",
    "# Convert the dictionaries to pandas dataframes\n",
    "male_df = pd.DataFrame(male_data)\n",
    "female_df = pd.DataFrame(female_data)\n",
    "journal_df = pd.DataFrame(journal_data)\n",
    "\n",
    "\n",
    "all_data.to_csv('all_coefficients.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the seaborn style and set font to \"Verdana\"\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['font.family'] = 'Verdana'\n",
    "colors_vec = ['#003f5c','#bc5090','#ffa600']\n",
    "\n",
    "# Convert dictionaries to dataframes\n",
    "female_df = pd.DataFrame(female_data)\n",
    "male_df = pd.DataFrame(male_data)\n",
    "\n",
    "# Sort female data so the largest magnitude (smallest in value) is at the top\n",
    "female_df = female_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Sort male data so the largest value is at the top\n",
    "male_df = male_df.sort_values(by='Coefficient', ascending=True)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(figsize=(10,5), ncols=2, sharey=False)  # Setting sharey to False\n",
    "\n",
    "# Female features on the left\n",
    "axes[0].barh(female_df['Feature'], female_df['Coefficient'], align='center', color=colors_vec[0], alpha=0.75, zorder=10, label='Female Features')  \n",
    "axes[0].set(yticks=range(len(female_df['Feature'])), yticklabels=female_df['Feature'])  \n",
    "axes[0].set_xlim([-3, 0])\n",
    "\n",
    "# Male features on the right\n",
    "axes[1].barh(male_df['Feature'], male_df['Coefficient'], align='center', color=colors_vec[1], alpha=0.75, zorder=10, label='Male Features')  \n",
    "axes[1].set(yticks=range(len(male_df['Feature'])), yticklabels=male_df['Feature'])  \n",
    "axes[1].yaxis.tick_right()\n",
    "axes[1].set_xlim([0, 3])\n",
    "\n",
    "# Adjusting ticks and labels\n",
    "for ax in axes:\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set(fontsize=13)\n",
    "\n",
    "# Add a centralized title for the entire figure closer to the plots\n",
    "fig.suptitle(\"Top 10 Coefficient Features\", fontsize=10, y=0.92)\n",
    "\n",
    "# Adjust subplots\n",
    "plt.subplots_adjust(wspace=0, top=0.85, bottom=0.2, left=0.3, right=0.7)\n",
    "\n",
    "# Add legends under the graph\n",
    "fig.legend(loc='lower center', ncol=2, fontsize=12)\n",
    "plt.savefig(\"new\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusting the font size for journal names even smaller\n",
    "\n",
    "fig, axes = plt.subplots(figsize=(10,5), ncols=2, sharey=False)  # Setting sharey to False\n",
    "\n",
    "# Female journals on the left\n",
    "axes[0].barh(female_journal_df['Feature'], female_journal_df['Coefficient'], align='center', color=colors_vec[0], alpha=0.75, zorder=10, label='Female Journals')  \n",
    "axes[0].set(yticks=range(len(female_journal_df['Feature'])), yticklabels=female_journal_df['Feature'])  \n",
    "axes[0].set_xlim([-2, 0])\n",
    "\n",
    "# Male journals on the right\n",
    "axes[1].barh(male_journal_df['Feature'], male_journal_df['Coefficient'], align='center', color=colors_vec[1], alpha=0.75, zorder=10, label='Male Journals')  \n",
    "axes[1].set(yticks=range(len(male_journal_df['Feature'])), yticklabels=male_journal_df['Feature'])  \n",
    "axes[1].yaxis.tick_right()\n",
    "axes[1].set_xlim([0, 2])\n",
    "\n",
    "# Adjusting ticks and labels with even smaller font size for y-axis labels\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='y', labelsize=9)  # Reduced font size to 9\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set(fontsize=13)\n",
    "\n",
    "# Add a centralized title for the entire figure closer to the plots\n",
    "fig.suptitle(\"Top 10 Coefficient Journals\", fontsize=10, y=0.92)\n",
    "\n",
    "# Adjust subplots\n",
    "plt.subplots_adjust(wspace=0, top=0.85, bottom=0.2, left=0.3, right=0.7)\n",
    "\n",
    "# Add legends under the graph\n",
    "fig.legend(loc='lower center', ncol=2, fontsize=12)\n",
    "plt.savefig(\"new2\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b79eec-6112-42b6-b5a0-b285c4ec069a",
   "metadata": {},
   "source": [
    "#### Training graphs for BERT with different weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7ae97-729a-43bb-932f-e2a0c6470780",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use the seaborn style\n",
    "plt.style.use('seaborn')\n",
    "# Set the font to \"Verdana\" and increase font size\n",
    "plt.rcParams['font.family'] = 'Verdana'\n",
    "plt.rcParams['font.size'] = 14\n",
    "colors = ['#003f5c', '#bc5090', '#ffa600', \"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\", \"#9467bd\", \"#8c564b\", \"#e377c2\"]\n",
    "\n",
    "# Load the dataset\n",
    "data_df = pd.read_csv('/Users/Magnus/Downloads/output_Final.csv')\n",
    "\n",
    "# List of metrics\n",
    "metrics = [\"eval/accuracy\", \"eval/f1\", \"eval/loss\", \"eval/precision\", \"eval/recall\"]\n",
    "\n",
    "# Create line charts with 2 columns and square plots\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 20))\n",
    "axes = axes.ravel()  # Flatten the axes array\n",
    "\n",
    "legend_handles, legend_labels = [], []\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_data = data_df[data_df['Tag'] == metric]\n",
    "    \n",
    "    for color_idx, (run, group) in enumerate(metric_data.groupby('Run')):\n",
    "        sorted_group = group.sort_values(by='Step')\n",
    "        \n",
    "        # Extract run number and rename the run\n",
    "        run_number = int(run.split('_')[-1])\n",
    "        run_name = f\"Weight {run_number}\"\n",
    "        \n",
    "        line, = axes[i].plot(sorted_group['Step'], sorted_group['Value'], color=colors[color_idx % len(colors)])\n",
    "        \n",
    "        # Gather legend info\n",
    "        if i == 0:  # Only do this for the first metric to avoid duplicates\n",
    "            legend_handles.append(line)\n",
    "            legend_labels.append(run_name)\n",
    "    \n",
    "    # Set title, labels, and remove individual legends\n",
    "    clean_metric = metric.replace(\"eval/\", \"\")  # Remove \"eval/\" prefix\n",
    "    axes[i].set_title(f\"{clean_metric.capitalize()} over Steps\", fontsize=18, pad=25)\n",
    "    axes[i].set_xlabel(\"Steps\", fontsize=16)\n",
    "    axes[i].set_ylabel(clean_metric.capitalize(), fontsize=16)\n",
    "    axes[i].grid(True, linestyle='-', linewidth=1.9, alpha=0.99, color='white')\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=14)\n",
    "    for spine in axes[i].spines.values():\n",
    "        spine.set_visible(False)\n",
    "\n",
    "# Remove the last unused subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "# Create one unified legend for the figure at the top, but slightly lower than before\n",
    "fig.legend(legend_handles, legend_labels, loc='upper center', ncol=len(legend_handles), bbox_to_anchor=(0.5, 1.05), frameon=False, fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

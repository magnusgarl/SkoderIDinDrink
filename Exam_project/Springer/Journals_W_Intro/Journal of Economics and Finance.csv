Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920078,Cointegration and capital market integration: An application of the error-correction mechanism to investigate the relationship between domestic and offshore U.S. dollar yields,March 1993,Peggy E. Swanson,,,Female,Unknown,Unknown,Female,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920079,Mexican integration into the North American free trade zone,March 1993,A. K. M. Matiur Rahman,M. M. Moosa Khan,M. Anisul Islam,Unknown,Unknown,Unknown,Unknown,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920080,"An assessment of human development, by region and country",March 1993,Edward Nissan,Ronnie Shahmoon,,Male,,Unknown,Mix,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920081,The reaction of financial markets to changes in FDIC policies on bank failures,March 1993,Steve A. Johnson,James T. Lindley,,Male,Male,Unknown,Male,,1
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920082,Market reaction to uniform capital adequacy guidelines in the banking industry,March 1993,Jeff Madura,Emilio R. Zarruk,,Male,Male,Unknown,Male,,4
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920083,Net selectivity revisited,March 1993,Ladd M. Kochman,Ravija Badarinathi,,Unknown,Unknown,Unknown,Unknown,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920084,"Institutional affiliation of contributors to the American Economic Review, 1981–1990",March 1993,Javed Ashraf,,,Male,Unknown,Unknown,Male,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920085,Dividend reinvestment plans and Pecking Order capital structure behavior: An empirical investigation,March 1993,Harold B. Tamule,Edward L. Bubnys,Timothy F. Sugrue,Male,Male,Male,Male,,5
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920086,"Profit, productivity, and profit-sharing",March 1993,Chun-Hao Chang,David J. Bjornstad,,,Male,Unknown,Mix,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920087,Election outcomes and economic conditions: An application of a logit model,March 1993,William Levernier,,,Male,Unknown,Unknown,Male,,3
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920088,Re-examination of the velocity-money growth variability debate,March 1993,James E. Payne,,,Male,Unknown,Unknown,Male,,1
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920089,The 1986 Tax Reform Act and ex-dividend day price and volume patterns: Evidence from organized exchanges and the OTC market,March 1993,Herman Manakyan,Kartono Liano,Gow Cheng Huang,Male,Unknown,Unknown,Male,,
17,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920090,Economies of scale in steam-electric power generation in East-North-Central U.S.,March 1993,Albert A. Okunade,,,Male,Unknown,Unknown,Male,,1
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920633,Economic incentives and the demand for accounting information: Perspectives from medieval English finance,June 1993,Alan Blankley,Dana Forgione,,Male,Female,Unknown,Mix,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920634,The effect of money-growth and interest-rate variability upon inflation,June 1993,James E. Payne,,,Male,Unknown,Unknown,Male,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920635,Financial characteristics of acquiring firms and their relation to the wealth effects of acquisition announcements,June 1993,Emery A. Trahan,,,Male,Unknown,Unknown,Male,,7
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920636,Oil and gas stock price reactions to federal tax legislation,June 1993,Timothy H. Mills,Dwight C. Anderson,Stuart Michelson,Male,Male,Male,Male,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920637,Development patterns of oil exporting countries,June 1993,Mohammad Khalil,Ali H. Mansour,,Male,Male,Unknown,Male,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920638,"The Cartagena Declaration, the Baker Plan, and U.S. bank security returns: An empirical investigation",June 1993,William H. Sackley,M. Kabir Hassan,,Male,Unknown,Unknown,Male,,2
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920639,Corporate mergers and union wage premiums,June 1993,James Peoples,Ali Hekmat,A. H. Moini,Male,Male,Unknown,Male,,5
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920640,Investment performance of high income stocks over up and down markets,June 1993,John M. Clinebell,Jan R. Squires,Jerry L. Stevens,Male,Male,Male,Male,,1
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920641,The consequences of mutually secured debts: The case of Israeli moshavim,June 1993,Amnon Levy,,,Male,Unknown,Unknown,Male,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920642,Insider ownership and dividend signals: Additional evidence from the banking industry,June 1993,Greg Filbeck,Patricia A. Chadwell,,Male,Female,Unknown,Mix,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920643,Labor composition and long-run employment in a minimum-wage economy,June 1993,William D. Robinson,,,Male,Unknown,Unknown,Male,,1
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920644,Forecasting annual and harvest pecan prices,June 1993,Xi-Ling Wu,Wojciech J. Florkowski,,,Male,Unknown,Mix,,
17,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920645,Unit root and cointegration tests on the efficiency and biasedness of forward exchange rates,June 1993,C. S. Pyun,Richard D. Evans,,Unknown,Male,Unknown,Male,,
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920027,Estimating cigarette-tax revenue,September 1993,Ted W. Chiles,David L. Sollars,,Male,Male,Unknown,Male,,1
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920028,An empirical investigation of the cross sectional determinants of lease use,September 1993,Suzanne M. Erickson,,,Female,Unknown,Unknown,Female,,4
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920029,Stock price reactions to increases in loan-loss reserves: A broader perspective,September 1993,Carol Lancaster,Gay Hatfield,Dwight C. Anderson,,Female,Male,Mix,,
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920030,The control hypothesis and recurring versus accumulated free cash flow,September 1993,Mike Cudd,Rakesh Duggal,,Male,Male,Unknown,Male,,1
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920031,Seasonality in industry specific indices,September 1993,Ann Marie Whyte,Armand Picou,,Female,Male,Unknown,Mix,,
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920032,Off-balance sheet activities and bank default-risk premia: A comparison of risk measures,September 1993,M. Kabir Hassan,Gordon V. Karels,Manferd O. Peterson,Unknown,Male,Unknown,Male,,24
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920033,Local measured service and the attributes of a sound rate structure,September 1993,Albert L. Danielsen,David R. Kamerschen,Christos L. Nicolaou,Male,Male,Male,Male,,2
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920034,"An integrative analysis of external debt, capital accumulation and production in Latin America, Asia-Pacific and Sub-Saharan Africa",September 1993,Amnon Levy,Khorshed Chowdhury,,Male,Unknown,Unknown,Male,,6
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920035,The impact of PPS legislation on the systematic risk of hospitals,September 1993,Elaine M. Asper,Mahmud Hassan,,Female,Male,Unknown,Mix,,
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920036,The association of default risk factors with the systematic risk of corporate bonds,September 1993,Richard M. Duvall,R. S. Rathinasamy,,Male,Unknown,Unknown,Male,,2
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920037,A study of the performance of Developmental Studies students in the principles of economics,September 1993,Bichaka Fayissa,Duane B. Graddy,Kenneth Smith,Unknown,Male,Male,Male,,
17,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920038,Further evidence on causality and feedback across international stocks,September 1993,Prakash L. Dheeriya,,,,Unknown,Unknown,Mix,,
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920218,"The consequences of a policy of necessity: Bank regulatory forbearance, 1986–1989",March 1994,Keith J. Leggett,,,Male,Unknown,Unknown,Male,,2
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920219,"Acquiring firms' stock returns: Methods of payment, change in leverage, and management ownership",March 1994,Elias Raad,H. K. Wu,,Male,Unknown,Unknown,Male,,4
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920220,Sources of change in U.S. manufactured exports during the eighties,March 1994,Golam Azam,Farida Azam,,Unknown,Female,Unknown,Female,,1
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920221,Developing a decision rule to predict failure: The case of savings and loan associations,March 1994,Phillip Fuller,Theodor Kohers,,Male,Male,Unknown,Male,,4
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920222,The relationship between the valuation effect of equity financing and firm-specific characteristics,March 1994,Huey-Lian Sun,,,Unknown,Unknown,Unknown,Unknown,,
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920223,"Human capital, investment and growth: Some results from an endogenous growth model",March 1994,Abdel-Hameed M. Bashir,Ali F. Darrat,,Unknown,Male,Unknown,Male,,4
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920224,Agency costs and the discount rate for public sector enterprises,March 1994,C. R. Krishnaswamy,R. S. Rathinasamy,Inayat U. Mangla,Unknown,Unknown,Male,Male,,2
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920225,The impact of option listing on the price volatility and trading volume of underlying OTC stocks,March 1994,D. Michael Long,Michael D. Schinski,Dennis T. Officer,Unknown,Male,Male,Male,,9
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920226,"Adoption, diffusion, and public R&D",March 1994,Walter G. Park,,,Male,Unknown,Unknown,Male,,2
18,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920227,An economic analysis of the earnings of industrial accountants,March 1994,Kenneth Rosenzweig,Elizabeth Gustafson,Lawrence Hadley,Male,Female,Male,Mix,,
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920518,The role of attitude in learning economics: Race and gender differences,June 1994,Luther D. Lawson,,,Male,Unknown,Unknown,Male,,6
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920519,Auditor realignment and 8−K disclosures—Usefulness for investor decision making,June 1994,Ram S. Sriram,P. R. Chandy,,Male,Unknown,Unknown,Male,,
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920520,An analysis of the factors affecting the supply and demand for teacher quality,June 1994,Kenneth E. Galchus,,,Male,Unknown,Unknown,Male,,2
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920521,"Loan loss reporting, early disclosure, and investor reactions",June 1994,Gordon V. Karels,Steven V. Mann,Stephen E. Wilcox,Male,Male,Male,Male,,
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920522,Net selectivity revisited: An extension,June 1994,Thomas M. Krueger,Richard E. Callaway,,Male,Male,Unknown,Male,,
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920523,Modeling returns to owner-occupied single-family residences,June 1994,Charles M. Ermer,Steven M. Cassidy,Michael J. Sullivan,Male,Male,Male,Male,,4
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920524,Index fund rebalancing and market efficiency,June 1994,A. Steven Graham,Wendy L. Pirie,,Unknown,Female,Unknown,Female,,7
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920525,Relative information asymmetry as a determinant of the market reaction to corporate financing announcements,June 1994,Ronald W. Best,,,Male,Unknown,Unknown,Male,,1
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920526,Announcement,June 1994,,,,Unknown,Unknown,Unknown,Unknown,,
18,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920527,Announcement,June 1994,,,,Unknown,Unknown,Unknown,Unknown,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920485,Investigating a new methodology for ranking international mutual funds,September 1994,Suresh C. Srivastava,Musa Essayyad,,,Male,Unknown,Mix,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920486,"FASB 95, Cash Flow and bankruptcy",September 1994,Michael Schellenger,Joann Noe Cross,,Male,Female,Unknown,Mix,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920487,Composite forecasts of inflation: An improvement in forecasting performance,September 1994,Jong C. Rhim,Mohammed F. Khayum,Timothy J. Schibik,,Male,Male,Mix,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920488,"The relationship among firm size, E/P, and share price anomalies: NASDAQ stocks versus NYSE and AMEX stocks",September 1994,Delbert C. Goff,,,Male,Unknown,Unknown,Male,,1
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920489,A historically significant case of labor-management problems: The newport torpedo plant before world war II,September 1994,Paul Merkle,Jacqueline McConnell,,Male,Female,Unknown,Mix,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920490,Market reaction to equity offer reasons: What information do managers reveal?,September 1994,Marlin R. H. Jensen,Claire E. Crutchley,Carl D. Hudson,Male,Female,Male,Mix,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920491,The issuer effect on default risk insured municipal bond yields,September 1994,C. Steven Cole,Pu Liu,Stanley D. Smith,Unknown,,Male,Mix,,
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920492,Should managers refrain from and investors ignore noisy dividend surprises? Some experimental evidence,September 1994,Steven Peterson,Dan Salandro,,Male,Male,Unknown,Male,,2
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920493,A note on odds in the cattle futures market,September 1994,Seth C. Anderson,John D. Jackson,Jeffrey W. Steagall,Male,Male,Male,Male,,1
18,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920494,Internet accessto the Journal of Economics and Finance,September 1994,,,,Unknown,Unknown,Unknown,Unknown,,
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920210,Modified duration and convexity with semiannual compounding,March 1995,C. Steven Cole,Philip J. Young,,Unknown,Male,Unknown,Male,,3
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920211,Economic determinants of the labor force withdrawal of Registered Nurses,March 1995,Carl Parker,Bill Rickman,,Male,Male,Unknown,Male,,15
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920212,"Cost-based decision rules for output, quality, and price",March 1995,Charles E. Hegji,,,Male,Unknown,Unknown,Male,,
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920213,An examination of the intraday behavior of the yen/dollar exchange rate: The relationship between trading activity and returns volatility,March 1995,Timothy R. Smaby,,,Male,Unknown,Unknown,Male,,1
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920214,U.S. interest rates and the exchange value of the dollar: A test of causality,March 1995,Bahram Adrangi,Mary Allender,,Male,,Unknown,Mix,,
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920215,"Deposit insurance subsidies, moral hazard, and bank regulation",March 1995,Chandrasekhar Mishra,Jorge L. Urrutia,,Unknown,Male,Unknown,Male,,4
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920216,Adjusted beta responses to dividend announcements,March 1995,Carolyn Carroll,Robert C. W. Fok,,Female,Male,Unknown,Mix,,
19,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920217,Bank lending rules for managing sovereign risk,March 1995,Swapan Sen,Manas K. Chattopadhyay,,Male,Unknown,Unknown,Male,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920506,Cooperative R&D ventures: Strategic behavior versus transactional efficiency,June 1995,Zaher Z. Zantout,,,Male,Unknown,Unknown,Male,,5
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920507,"Employment in the EC, EFTA, and the EEA: A regional analysis",June 1995,Margaret A. Ray,,,Female,Unknown,Unknown,Female,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920508,"Closed-end bond fund discounts: Agency costs, investor sentiment and portfolio content",June 1995,Gregory M. Noronha,Bruce L. Rubin,,Male,Male,Unknown,Male,,1
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920509,Estimation of a simultaneous equations model with an ordinal endogenous variable: The extent of teacher bargaining and the state legal environment,June 1995,R. Carter Hill,Melissa S. Waters,,Unknown,Female,Unknown,Female,,2
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920510,The wealth effects of unethical business behavior,June 1995,D. Michael Long,Spuma Rao,,Unknown,Unknown,Unknown,Unknown,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920511,An evaluation of the productive efficiency of savings and loans,June 1995,Robert C. W. Fok,Sung Ko Li,J. Howard Finch,Male,,Unknown,Mix,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920512,Rank order analysis of state general obligation bond ratings,June 1995,Bülent Uyar,Donald R. Escarraz,,Male,Male,Unknown,Male,,7
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920513,Scope and scale economies in merging hospitals prior to merger,June 1995,Ugur A. Sinay,Claudia R. Campbell,,Unknown,Female,Unknown,Female,,16
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920514,International trade and the U.S. wage structure,June 1995,Mark D. Partridge,,,Male,Unknown,Unknown,Male,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920515,Sustainable growth rate of firms in financial distress,June 1995,Harlan D. Platt,Marjorie B. Platt,Guangli Chen,Male,Female,Unknown,Mix,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920516,The impact of Medigap insurance on out-of-pocket health expenditures by older Americans over the 1980s,June 1995,Rose M. Rubin,Kenneth Koelln,Roger K. Speas,Female,Male,Male,Mix,,
19,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920517,Announcement,June 1995,,,,Unknown,Unknown,Unknown,Unknown,,
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920611,"Comparing classification accuracy of neural networks, binary logit regression and discriminant analysis for insolvency prediction of life insurers",September 1995,Ernest Preston Goss,Harish Ramchandani,,Male,Male,Unknown,Male,,14
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920612,An empirical analysis of cross-security information asymmetry and the “pecking order” hypothesis,September 1995,Roger J. Best,Ronald W. Best,,Male,Male,Unknown,Male,,3
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920613,Option introduction and insider trading in the NASDAQ/NMS equity market,September 1995,Rich Fortin,Stuart Michelson,,,Male,Unknown,Mix,,
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920614,The Phillips curve behavior over different horizons,September 1995,Jim Lee,,,Male,Unknown,Unknown,Male,,5
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920615,Insider Trading and regulation: A look at bank holding companies,September 1995,Greg Filbeck,Donald J. Mullineaux,,Male,Male,Unknown,Male,,
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920616,S&P 500 futures market and the normal backwardation hypothesis,September 1995,Mahamood M. Hassan,,,Unknown,Unknown,Unknown,Unknown,,
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920617,"Agency costs in the banking industry: An examination of ownership behavior, leverage and dividend policies",September 1995,Jose Mercado-Mendez,Thomas Willey,,Male,Male,Unknown,Male,,9
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920618,Long-term valuation effects of stock splits,September 1995,Aigbe Akhigbe,Jeff Madura,Stephen P. Zera,Unknown,Male,Male,Male,,
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920619,Precautionary behavior and consumer information: A Markov process approach,September 1995,Lawrence J. Belcher,,,Male,Unknown,Unknown,Male,,
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920620,Performance following ESOP formations by publicly-held corporations,September 1995,William R. McDaniel,Jeff Madura,Kenneth J. Wiant,Male,Male,Male,Male,,2
19,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920621,Ownership structure and returns to defensive stock repurchase,September 1995,Criss G. Woodruff,Khalil M. Torabzadeh,James B. Ross,Unknown,Male,Male,Male,,1
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920495,Hidden risks of hedge accounting in financial institutions: A study of Franklin Savings Association,March 1996,William H. Sackley,Michael B. Madaris,Suzanne M. Holifield,Male,Male,Female,Mix,,
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920496,Intra-regional wage variation and regional disparity in the relative earnings of blacks,March 1996,Don Bellante,Carl A. Kogut,,Male,Male,Unknown,Male,,1
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920497,Corporate returns and cash conversion cycles,March 1996,Manuel L. Jose,Carol Lancaster,Jerry L. Stevens,Male,,Male,Mix,,
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920498,Search and uncertainty: Determinants of the degree of underpricing of initial public offerings,March 1996,James Marchand,John Roufagalas,,Male,Male,Unknown,Male,,
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920499,The effect of rule 12b-1 on bond fund expense ratios,March 1996,Robert W. McLeod,D. K. Malhotra,,Male,Unknown,Unknown,Male,,1
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920500,The impact of implementing the airline deregulation act on stock returns,March 1996,Richard B. Edelman,H. Kent Baker,,Male,Unknown,Unknown,Male,,2
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920501,Duration of share repurchase programs and firm performance,March 1996,Daniel C. Indro,Glen A. Larsen,,Male,Male,Unknown,Male,,1
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920502,Determinants of bank merger premiums,March 1996,Hany A. Shawky,Tobias Kilb,Carsten F. W. Staas,Male,Male,Male,Male,,18
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920503,Stock price effect of high-yield debt issues,March 1996,Stephen P. Huffman,David J. Ward,,Male,Male,Unknown,Male,,3
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920504,The demand for imports and exports in the U.S.: A survey,March 1996,W. Charles Sawyer,Richard L. Sprinkle,,Unknown,Male,Unknown,Male,,23
20,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920505,Announcement,March 1996,,,,Unknown,Unknown,Unknown,Unknown,,
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920887,Can the interests of shareholders explain industry attitudes toward banking-insurance mergers?,June 1996,Vickie Lynn Bajtelsmit,James Allen Ligon,,Female,Male,Unknown,Mix,,
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920888,Defensive acquisitions in the banking industry: The takeover premium hypothesis,June 1996,Babu G. Baradwaj,David A. Dubofsky,Donald R. Fraser,Male,Male,Male,Male,,7
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920889,The home field advantage: Implications for the pricing of tickets to professional team sporting events,June 1996,David W. Boyd,Laura A. Boyd,,Male,Female,Unknown,Mix,,
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920890,The temporal behavior of scale economies within A banking firm,June 1996,Kenneth Daniels,Doĝan Tirtiroĝlu,,Male,Unknown,Unknown,Male,,
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920891,The pricing of dividend consistency,June 1996,John Dobson,W. Tawarangkoon,Uric Dufrene,Male,Unknown,Unknown,Male,,4
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920892,GNP forecasts: How credible are they? A sequential test of consistency and rationality,June 1996,Swarna D. Dutt,Dipak Ghosh,,Unknown,Male,Unknown,Male,,2
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920893,Trading cost expectations: Evidence from S&P 500 index replacement stock announcements,June 1996,Robert O. Edmister,A. Steven Graham,Wendy L. Pirie,Male,Unknown,Female,Mix,,
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920894,Tax loss selling and the contrarian investment strategy,June 1996,Ken Johnston,Don R. Cox,,Male,Male,Unknown,Male,,3
20,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920895,The investment decision of a Multinational Firm under uncertain quantity restrictions,June 1996,Seung-Dong Lee,Manabendra DasGupta,,Unknown,Unknown,Unknown,Unknown,,
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920601,A comparison of the performance of open-and closed-end investment companies,September 1996,Seth C. Anderson,B. Jay Coleman,Harlan Sunquist,Male,Unknown,Male,Male,,2
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920602,"Bank failure, risk, and capital regulation",September 1996,Joel R. Barber,Chun-Hao Chang,David C. Thurston,Male,,Male,Mix,,
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920603,Further examination of the velocity-money growth variability debate: Evidence from Japan using an error-correction model,September 1996,Bradley T. Ewing,,,Male,Unknown,Unknown,Male,,
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920604,The impact of operating and financial risk on equity risk,September 1996,Richard A. Lord,,,Male,Unknown,Unknown,Male,,16
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920605,Determinants of bidder competition in corporate takeovers,September 1996,Gregory M. Noronha,Nilanjan Sen,David M. Smith,Male,Unknown,Male,Male,,4
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920606,The effect of takeover defenses on the dividend decision,September 1996,Daniel E. Page,John S. Jahera,William N. Pugh,Male,Male,Male,Male,,1
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920607,Financial determinants of market value: The case of investor favorites,September 1996,Bruce C. Payne,Denis O. Boudreaux,Nancy C. Rumore,Male,Male,Female,Mix,,
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920608,The robustness of calendar anomalies in daily stock returns,September 1996,Douglas K. Pearce,,,Male,Unknown,Unknown,Male,,13
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920609,Index futures trading and stock price volatility: Evidence from Denmark and France,September 1996,Mario G. Reyes,,,Male,Unknown,Unknown,Male,,7
20,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920610,Airport privatization: A simple welfare analysis,September 1996,Bijan Vasigh,Mehdi Haririan,,,Male,Unknown,Mix,,
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929016,The economics of the radical right,March 1997,James Tobin,,,Male,Unknown,Unknown,Male,,
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929017,The monetary equivalence of vouchers,March 1997,William Gissy,,,Male,Unknown,Unknown,Male,,
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929018,Determinants of business failure: The role of firm size,March 1997,Afsaneh Assadian,Jon M. Ford,,Female,Male,Unknown,Mix,,
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929019,Market efficiency and cointegration: Some evidence in Pacific-Basin black exchange markets,March 1997,Kam C. Chan,Louis T. W. Cheng,Ming-Shiun Pan,Male,Male,Unknown,Male,,9
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929020,Volatility of exchange rate futures and high-low price spreads,March 1997,An-Sing Chen,,,Unknown,Unknown,Unknown,Unknown,,
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929021,The rise (or fall) of lottery adoption within the logic of collective action: Some empirical evidence,March 1997,Franklin G. Mixon,Steven B. Caudill,Ter Chao Peng,Male,Male,Unknown,Male,,8
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929022,Weekly pattern in higher moments: An empirical test in Hong Kong stock market,March 1997,Gordon Y. N. Tang,,,Male,Unknown,Unknown,Male,,8
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929023,Tournament performance and “Agency” problems: An empirical investigation of “March madness”,March 1997,James E. McClure,Lee C. Spector,,Male,,Unknown,Mix,,
21,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929024,"Sovereign debt: Reputation, seizure and reputation",March 1997,Amnon Levy,,,Male,Unknown,Unknown,Male,,
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920758,The value of tax benefits and the cost of liquidating versus selling failed thrift institutions,June 1997,James R. Barth,Philip F. Bartholomew,Peter J. Elmer,Male,Male,Male,Male,,1
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920759,Fisher equations inverted and not,June 1997,Clifford F. Thies,Robert G. Crawford,,Male,Male,Unknown,Male,,1
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920760,The causality between dollar and pound: An application of cointegration and error-correction modeling,June 1997,Benjamin S. Cheng,,,Male,Unknown,Unknown,Male,,1
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920761,Testing the effectiveness of regulatory interest rate risk measurement,June 1997,James H. Gilkeson,Sylvia C. Hudgins,Craig K. Ruff,Male,Female,Male,Mix,,
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920762,Optimal inventory order quantities when volume discounts are available: A wealth maximization approach,June 1997,Richard A. Followill,,,Male,Unknown,Unknown,Male,,
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920763,"Historical resources, uncertainty and preservation values: An application of option and optimal stopping models",June 1997,Catherine M. Chambers,Paul E. Chambers,John C. Whitehead,Female,Male,Male,Mix,,
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920764,Time on market and sales price of residential housing: A note,June 1997,Rajiv Kalra,Kam C. Chan,Pikki Lai,Male,Male,Unknown,Male,,6
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920765,Economic rents and mutual fund performance: An empirical investigation,June 1997,Vinod B. Agarwal,Larry J. Prather,,Male,Male,Unknown,Male,,2
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920766,Bond review and rating change announcements: An examination of informational value and market efficiency,June 1997,Richard A. Followill,Terrence Martell,,Male,Male,Unknown,Male,,24
21,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02920767,Using ex-day returns to separate the tax and information effects of dividend changes,June 1997,Mazhar A. Siddiqi,,,Male,Unknown,Unknown,Male,,2
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929033,"Federal deposit insurance coverage and bank failures: A cointegration analysis with semi-annual data, 1965–91",September 1997,Ira Saltz,,,Female,Unknown,Unknown,Female,,3
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929034,How should diversifiable and nondiversifiable portfolio risks be defined?,September 1997,P. A. V. B. Swamy,Thomas J. Lutton,George S. Tavlas,Unknown,Male,Male,Male,,3
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929035,Form of ownership and risk taking in banking: Some evidence from Massachusetts savings banks,September 1997,Neil B. Murphy,Dan Salandro,,Male,Male,Unknown,Male,,4
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929036,Causes of cross-autocorrelation in security returns: Transaction costs versus information quality,September 1997,Terry L. Richardson,David R. Peterson,,,Male,Unknown,Mix,,
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929037,The impact of technological change on bank performance,September 1997,Allen L. Webster,,,Male,Unknown,Unknown,Male,,2
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929038,Determinants of the stock price reaction to leveraged buyouts,September 1997,Kenneth A. Carow,Dianne M. Roden,,Male,Female,Unknown,Mix,,
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929039,A brief note examining the impact of federal deposit insurance on savings and loan failures using granger causality tests,September 1997,Usha Nair Reichert,,,Female,Unknown,Unknown,Female,,
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929040,Correlates of student performance in Business and Economics Statistics,September 1997,Randall G. Krieg,Bulent Uyar,,Male,Unknown,Unknown,Male,,6
21,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02929041,The valuation of life for damages purposes in lawsuits: A pedagogical note,September 1997,Richard J. Cebula,,,Male,Unknown,Unknown,Male,,
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823228,Stock returns and volatility: Another look,March 1998,Ramon P. DeGennaro,Yuzhen Lisa Zhao,,Male,Unknown,Unknown,Male,,3
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823229,The effect of NYSE listing on a firm’s media visibility,March 1998,H. Kent Baker,Gary E. Powell,Daniel G. Weaver,Unknown,Male,Male,Male,,8
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823230,The Massachusetts Classified Board Law,March 1998,L. Mick Swartz,,,Unknown,Unknown,Unknown,Unknown,,
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823231,Determinants of the stock price reaction to leveraged buyouts,March 1998,Kenneth A. Carow,Dianne M. Roden,,Male,Female,Unknown,Mix,,
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823232,"Corporate investment, dividend decisions, differential taxation and the no-arbitrage condition",March 1998,Kavous Ardalan,Eliezer Z. Prisman,,Unknown,Male,Unknown,Male,,
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823233,International transmission of stock price movements: Evidence from the U.S. and five Asian-Pacific markets,March 1998,Y. Angela Liu,Ming-Shiun Pan,Joseph C. P. Shieh,Unknown,Unknown,Male,Male,,30
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823234,Individual versus institutional investors and the weekend effect,March 1998,Paul Brockman,David Michayluk,,Male,Male,Unknown,Male,,11
22,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02823235,The no-arbitrage condition and financial markets with heterogeneous information,March 1998,Kavous Ardalan,Kevin Hebner,,Unknown,Male,Unknown,Male,,1
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771472,Reexamining the term structure of interest rates and the interwar demand for money,June 1998,Christopher F. Baum,Clifford F. Thies,,Male,Male,Unknown,Male,,1
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771473,Examining the credibility of macroeconomic forecasts: Null of cointegration approach,June 1998,Swarna D. Dutt,Dipak Ghosh,,Unknown,Male,Unknown,Male,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771474,"A comparative analysis of security pricing using factor, macrovariable and arbitrage pricing models",June 1998,Suat Teker,Oscar Varela,,,Male,Unknown,Mix,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771475,The long-run and short-run effects of exchange-rate volatility on exports: The case of Australia and New Zealand,June 1998,A. C. Arize,J. Malindretos,,Unknown,Unknown,Unknown,Unknown,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771476,Budget deficits and stock prices: International evidence,June 1998,Bahram Adrangi,Mary Allender,,Male,,Unknown,Mix,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771477,"The impact of the Financial Institutions Reform, Recovery, and Enforcement Act (FIRREA) on the value of S&L stocks",June 1998,Sarah K. Bryant,Spiros H. Martzoukos,,Female,Male,Unknown,Mix,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771478,Bond immunization for additive interest rate shocks,June 1998,Joel R. Barber,Mark L. Copper,,Male,Male,Unknown,Male,,4
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771479,Do physicians employ aides efficiently?: Some new evidence on solo practitioners,June 1998,James Thornton,,,Male,Unknown,Unknown,Male,,8
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771480,Return distributions and the day-of-the-week effects in the stock exchange of Thailand,June 1998,Ravindra R. Kamath,Rinjai Chakornpipat,Arjun Chatrath,Unknown,Unknown,Male,Male,,16
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771481,Assessing the economic costs of high school noncompletion,June 1998,Mark A. Thompson,,,Male,Unknown,Unknown,Male,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771482,Impact of diversification on the distribution of stock returns: International evidence,June 1998,Gordon Y. N. Tang,Daniel F. S. Choi,,Male,Male,Unknown,Male,,8
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771483,Spot and forward exchange rates as predictors of future spot rates: trends in exchange market value and the contribution of new information,June 1998,Peggy E. Swanson,,,Female,Unknown,Unknown,Female,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771484,An empirical investigation of U.S. bank risk and the Mexican peso crisis,June 1998,Osman Kilic,M. Kabir Hassan,David R. Tufte,Male,Unknown,Male,Male,,2
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771485,An integrative analysis of business bankruptcy in Australia,June 1998,Lillian Cheung,Amnon Levy,,Female,Male,Unknown,Mix,,
22,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02771486,The home field advantage: Implications for the pricing of tickets to professional team sporting events,June 1998,David W. Boyd,Laura A. Boyd,,Male,Female,Unknown,Mix,,
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752681,Feedback trading in exchange-rate markets: Evidence from within and across economic blocks,March 1999,Maria Sophia Aguirre,Reza Saidi,,Female,,Unknown,Mix,,
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752682,Abnormal returns of thrift versus non-thrift IPOs,March 1999,James R. Barth,Daniel E. Page,John S. Jahera,Male,Male,Male,Male,,3
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752683,Net treasury borrowing and interest-rate changes,March 1999,William Gissy,,,Male,Unknown,Unknown,Male,,1
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752684,Two approaches to measuring journal quality: Application to finance journals,March 1999,James E. McNulty,John Boekeloo,,Male,Male,Unknown,Male,,21
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752685,An error-correction model of the demand for equity mutual funds in the U.S. 1973–1994,March 1999,Nelson C. Modeste,Muhammad Mustafa,,Male,Male,Unknown,Male,,1
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752686,Factors affecting student retention probabilities: A case study,March 1999,James N. Wetzel,Dennis O’Toole,Steven Peterson,Male,Male,Male,Male,,30
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752687,"Market efficiency, discount-rate changes, and stock returns: A long-term perspective",March 1999,Laurie Prather,William J. Bertin,,Female,Male,Unknown,Mix,,
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752688,Self-selection bias and cost-of-living estimates,March 1999,Michael Raper,,,Male,Unknown,Unknown,Male,,5
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752689,"Returns to acquiring firms: The role of managerial ownership, managerial wealth, and outside owners",March 1999,Earl W. Shinn,,,Male,Unknown,Unknown,Male,,13
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752690,An examination of the causal relationship between savings and growth in the third world,March 1999,Ira S. Saltz,,,Female,Unknown,Unknown,Female,,21
23,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752691,FDICIA and bank failure contagion: Evidence from the two failures of first city bancorporation,March 1999,Robert A. Weigand,Donald R. Fraser,Babu G. Baradwaj,Male,Male,Male,Male,,1
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745945,Initial public offerings by mutual thrifts: The regulatory impact,June 1999,Steven R. Cox,Dianne M. Roden,,Male,Female,Unknown,Mix,,
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745946,The early stages of financial distress,June 1999,Richard B. Whitaker,,,Male,Unknown,Unknown,Male,,125
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745947,"Scaling laws, talent amplification, and executive compensation in the commercial bank industry",June 1999,W. David Walls,,,Unknown,Unknown,Unknown,Unknown,,
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745948,Latin American trade elasticities,June 1999,Thomas M. Fullerton,W. Charles Sawyer,Richard L. Sprinkle,Male,Unknown,Male,Male,,12
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745949,A note on the foreign exchange market efficiency hypothesis,June 1999,Swarna D. Dutt,Dipak Ghosh,,Unknown,Male,Unknown,Male,,11
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745950,An anomaly in the pricing of bank non-interest mortgage charges,June 1999,Chao-shun Hung,,,Unknown,Unknown,Unknown,Unknown,,
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745951,"ESOPs, takeover protection, and corporate decision-making",June 1999,William N. Pugh,John S. Jahera,Sharon Oswald,Male,Male,Female,Mix,,
23,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745952,Causality between taxes and expenditures: Evidence from Latin American countries,June 1999,Benjamin S. Cheng,,,Male,Unknown,Unknown,Male,,12
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757704,A note on health care inflation,September 1999,Usha Nair Reichert,Richard J. Cebula,,Female,Male,Unknown,Mix,,
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757705,Relative economic efficiency in Texas nursing facilities: A profit function analysis,September 1999,Kris Joseph Knox,Eric C. Blankmeyer,J. R. Stutzman,,Male,Unknown,Mix,,
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757706,Forecastable default risk premia and innovations,September 1999,Patrick A. Traichal,Steve A. Johnson,,Male,Male,Unknown,Male,,1
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757707,Do efforts to reduce the supply of illicit drugs increase turf war violence? a theoretical analysis,September 1999,Robert T. Burrus,,,Male,Unknown,Unknown,Male,,20
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757708,Multiple common bond credit unions and the allocation of benefits,September 1999,Keith J. Leggett,Yvonne H. Stewart,,Male,Female,Unknown,Mix,,
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757709,Wealth effects of the Basle Accord on small banks,September 1999,Chiuling Lu,Yangpin Shen,Raymond W. So,Unknown,Unknown,Male,Male,,1
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757710,Load and no-load mutual fund dynamics during the 1987 market crash: A stochastic dominance analysis,September 1999,Walton R. L. Taylor,James A. Yoder,,Male,Male,Unknown,Male,,7
23,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02757711,"Inflation, output, and stock prices: Evidence from two major emerging markets",September 1999,Bahram Adrangi,Arjun Chatrath,Kambiz Raffiee,Male,Male,Male,Male,,10
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759691,Derivative trading by utility firms,March 2000,Karyl B. Leggio,Donald Lien,,Unknown,Male,Unknown,Male,,2
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759692,An empirical study of depositor sensitivity to bank risk,March 2000,Jacqueline Khorassani,,,Female,Unknown,Unknown,Female,,9
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759693,Value stocks and market efficiency,March 2000,Roger J. Best,Ronald W. Best,James A. Yoder,Male,Male,Male,Male,,10
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759694,The reaction of security prices to tracking stock announcements,March 2000,John Elder,Peter Westra,,Male,Male,Unknown,Male,,
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759695,Revenue equivalence and income taxation,March 2000,Veronika Grimm,Ulrich Schmidt,,Female,Male,Unknown,Mix,,
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759696,Tax-free trading on calendar stock and bond market patterns,March 2000,William S. Compton,Robert A. Kunkel,,Male,Male,Unknown,Male,,4
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759697,Capital outflow liberalization and stock market reaction in an emerging market: Experience from Greece,March 2000,Georgios E. Chortareas,Titos E. Ritsatos,James M. Sfiridis,Male,Unknown,Male,Male,,5
24,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759698,Capital rationing: The general case and a better criterion for ranking,March 2000,Hoi T. Wong,,,,Unknown,Unknown,Mix,,
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752706,The impact of foreign variables on domestic money demand: Evidence from the United Kingdom,June 2000,C. James Hueng,,,Unknown,Unknown,Unknown,Unknown,,
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752707,"Life-cycle hypothesis, propensities to save, and demand for financial assets",June 2000,Jan Tin,,,Male,Unknown,Unknown,Male,,15
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752708,Rising productivity of Australian trading banks under deregulation 1986–1995,June 2000,Necmi K. Avkiran,,,Male,Unknown,Unknown,Male,,49
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752709,The relationship between the smoothing of reported income and risk-adjusted returns,June 2000,Stuart E. Michelson,James Jordan-Wagner,Charles W. Wootton,Male,Male,Male,Male,,38
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752710,Monetary policy implications of volatility linkages among long-term interest rates,June 2000,Nikiforos T. Laopodis,,,Male,Unknown,Unknown,Male,,6
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752711,"An empirical test of individual and institutional trading patterns in Japan, Hong Kong, and Taiwan",June 2000,Yung-Jang Wang,M. Mark Walker,,Unknown,Unknown,Unknown,Unknown,,
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752712,Savings and investment rates in Latin America: An error correction analysis,June 2000,Kristen N. Van Rensselaer,Joe B. Copeland,,Female,Male,Unknown,Mix,,
24,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752713,The true shadow price of foreign exchange,June 2000,Richard Dusansky,David Franck,Nadeem Naqvi,Male,Male,Male,Male,,
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752604,The effect of systematic risk factors on counterparty default and credit risk of interest rate swaps,September 2000,David A. Volkman,,,Male,Unknown,Unknown,Male,,
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752605,Earnings expectations and the relative information content of dividend and earnings announcements,September 2000,Roger J. Best,Ronald W. Best,,Male,Male,Unknown,Male,,2
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752606,Concentration on the nearby contract in financial futures markets: A stochastic model to explain the phenomenon,September 2000,Günter Bamberg,Gregor Dorfleitner,,Male,Male,Unknown,Male,,2
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752607,Stock prices and exchange rates in VEC model—The case of Singapore in the 1990s,September 2000,Ying Wu,,,,Unknown,Unknown,Mix,,
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752608,Output variability and economic growth: The case of Australia,September 2000,Joseph Macri,Dipendra Sinha,,Male,Unknown,Unknown,Male,,12
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752609,Monetary policy and real estate returns,September 2000,Robert R. Johnson,,,Male,Unknown,Unknown,Male,,5
24,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752610,Friday the 13th and the philosophical basis of financial economics,September 2000,Brian M. Lucey,,,Male,Unknown,Unknown,Male,,24
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759683,An empirical investigation of the consumption based Capital Asset Pricing Model using a modified variance-ratio test,March 2001,Petr Zemčík,,,Male,Unknown,Unknown,Male,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759684,Analyzing Fed behavior using a dynamic Taylor-type rule,March 2001,William L. Seyfried,Dale S. Bremmer,,Male,,Unknown,Mix,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759685,Macroeconomic influences on the stock market,March 2001,George Hondroyiannis,Evangelia Papapetrou,,Male,Female,Unknown,Mix,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759686,Stock returns and volatility: Evidence from the Athens Stock market index,March 2001,Nicholas Apergis,Sophia Eleptheriou,,Male,Female,Unknown,Mix,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759687,A strategy for trading the S&P 500 futures market,March 2001,Edward Olszewski,,,Male,Unknown,Unknown,Male,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759688,Corporate borrowing and growth option value: The limited liability effect,March 2001,Jyh-Bang Jou,,,Unknown,Unknown,Unknown,Unknown,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759689,Board ownership and IPO returns,March 2001,Shawn D. Howton,Shelly W. Howton,Gerard T. Olson,,Female,Male,Mix,,
25,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759690,Acquisitions by Real Estate Investment Trusts as a strategy for minimization of investor tax liability,March 2001,Jingyu Li,Fayez A. Elayan,Thomas O. Meyer,Unknown,Male,Male,Male,,7
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744518,Excess reserves during the 1930s: Empirical estimates of the costs of converting unintended cash inventory into income-producing assets,June 2001,James T. Lindley,Clifford B. Sowell,WM. Stewart Mounts,Male,Male,Unknown,Male,,2
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744519,Price dynamics when there are alternatives to cash payment,June 2001,Philip N. Jefferson,,,Male,Unknown,Unknown,Male,,1
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744520,Convergence-divergence of U.S. State unemployment rates,June 2001,Edward Nissan,George H. Carter,,Male,Male,Unknown,Male,,1
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744521,The day of the week effect on stock market volatility,June 2001,Hakan Berument,Halil Kiymaz,,Male,Male,Unknown,Male,,127
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744522,The post-offering performance of IPOs in the health care industry,June 2001,Hany S. Guirguis,Joseph Onochie,Harry Rosen,Male,Male,Male,Male,,4
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744523,Asymmetric volatility spillover in the Tokyo stock exchange,June 2001,Mario G. Reyes,,,Male,Unknown,Unknown,Male,,26
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744524,The underwriter’s early lock-up release: Empirical evidence,June 2001,Terrill R. Keasler,,,Male,Unknown,Unknown,Male,,7
25,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744525,Student performance in business and economics statistics: Does exam structure matter?,June 2001,Randall G. Krieg,Bulent Uyar,,Male,Unknown,Unknown,Male,,24
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745887,Recovery of hidden information from stock price data: A semiparametric approach,September 2001,George Vachadze,,,Male,Unknown,Unknown,Male,,1
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745888,Implied volatility surfaces and market activity over time,September 2001,Thierry Ané,Chiraz Labidi,,Male,Unknown,Unknown,Male,,12
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745889,Capturing the volatility smile of options on high-tech stocks—A combined GARCH-neural network approach,September 2001,Gunter Meissner,Noriko Kawano,,Male,Female,Unknown,Mix,,
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745890,The valuation of nature-linked bonds with exchange rate risk,September 2001,Patrice Poncet,Victor E. Vaugirard,,Male,Male,Unknown,Male,,3
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745891,Corporate income tax and futures hedging,September 2001,Donald Lien,Michael Metz,,Male,Male,Unknown,Male,,1
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745892,"Macroeconomic factors, consumer behavior, and bankcard default rates",September 2001,Terrance Grieb,Charles Hegji,Steven T. Jones,Male,Male,Male,Male,,18
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745893,Can money market mutual funds provide sufficient liquidity to replace deposit insurance?,September 2001,William Miles,,,Male,Unknown,Unknown,Male,,12
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745894,Assessing the usefulness of SEC Form 20-F disclosures using return and volume metrics: The case of U.K. firms,September 2001,Kingsley Onwunyiri Olibe,,,Male,Unknown,Unknown,Male,,4
25,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02745895,Error correction exchange rate modeling: Evidence for Mexico,September 2001,Thomas M. Fullerton,Miwa Hattori,Cuauhtémoc Calderón,Male,Female,Unknown,Mix,,
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744448,Policy on the margin: Evaluating the impact of margin debt requirements on stock valuations,March 2002,Christian E. Weller,,,Male,Unknown,Unknown,Male,,3
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744449,The effect of portfolio weighting on investment performance evaluation: The case of actively managed mutual funds,March 2002,Stanley B. Block,Dan W. French,,Male,Male,Unknown,Male,,9
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744450,Fee waivers for tax-exempt money market mutual funds,March 2002,Joseph A. Farinella,Randy D. Jorgensen,,Male,,Unknown,Mix,,
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744451,Do institutions care about market structure? A case study of listing firms,March 2002,Michele O’Neill,,,Female,Unknown,Unknown,Female,,1
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744452,Market reaction to large bank merger announcements in oligopolies,March 2002,Adham Chehab,,,Male,Unknown,Unknown,Male,,2
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744453,Wealth effects of time variation in investor risk preferences,March 2002,Bruce Niendorf,Thomas Ottaway,,Male,Male,Unknown,Male,,2
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744454,Hedging gas bills with weather derivatives,March 2002,Karyl B. Leggio,Donald Lien,,Unknown,Male,Unknown,Male,,14
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744455,Sources of real exchange rate movements in Saudi Arabia,March 2002,Eisa A. Aleisa,Sel Dibooĝlu,,Unknown,Unknown,Unknown,Unknown,,
26,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02744456,Cointegration of price measures: Evidence from the G-7,March 2002,Kay E. Strong,Subhash C. Sharma,,Male,Male,Unknown,Male,,
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755980,A data envelopment analysis of gas utilities' efficiency,June 2002,Daniel R. Hollas,Kenneth R. Macleod,Stanley R. Stansell,Male,Male,Male,Male,,23
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755981,Financial development and productive efficiency: A panel study of developed and developing countries,June 2002,Farrokh Nourzad,,,Male,Unknown,Unknown,Male,,16
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755982,Central bank transparency and market efficiency: An econometric analysis,June 2002,Matthew Rafferty,Marc Tomljanovich,,Male,Male,Unknown,Male,,21
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755983,The 2000 Presidential Election and the stock market,June 2002,Srinivas Nippani,W. Bobby Medlin,,Male,Unknown,Unknown,Male,,37
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755984,"CEO ownership, corporate control, and bank performance",June 2002,John M. Griffith,Lawrence Fogelberg,H. Shelton Weeks,Male,Male,Unknown,Male,,39
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755985,Predicting corporate financial distress: Reflections on choice-based sample bias,June 2002,Harlan D. Platt,Marjorie B. Platt,,Male,Female,Unknown,Mix,,
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755986,Determinants of corporate borrowing: Some evidence from the Indian corporate structure,June 2002,Saumitra N. Bhaduri,,,Unknown,Unknown,Unknown,Unknown,,
26,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02755987,Plastic choices: Consumer usage of bank cards versus proprietary credit cards,June 2002,Kenneth A. Carow,Michael E. Staten,,Male,Male,Unknown,Male,,4
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759709,Costly state verification and optimal investment,September 2002,Bappaditya Mukhopadhyay,,,Unknown,Unknown,Unknown,Unknown,,
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759710,Market mispricings and portfolio allocation to mutual fund classes,September 2002,Theodore Syriopoulos,,,Male,Unknown,Unknown,Male,,
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759711,Revisiting managerial perspectives on dividend policy,September 2002,H. Kent Baker,Gary E. Powell,E. Theodore Veit,Unknown,Male,Unknown,Male,,44
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759712,Why firms adopt and discontinue new-issue dividend reinvestment plans,September 2002,Tarun K. Mukherjee,H. Kent Baker,Vineeta L. Hingorani,Male,Unknown,Female,Mix,,
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759713,"Market index returns, macroeconomic variables, and tax-loss selling",September 2002,Ken Johnston,Don R. Cox,,Male,Male,Unknown,Male,,1
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759714,Black’s hypothesis and developed economies,September 2002,Matthew Rafferty,,,Male,Unknown,Unknown,Male,,
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759715,Macroeconomy and the well-being of low income African American families,September 2002,Vasudeva N. R. Murthy,,,Unknown,Unknown,Unknown,Unknown,,
26,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02759716,Using simulation as a tool in selecting a retirement age under defined benefit pension plans,September 2002,Richard F. Bieker,,,Male,Unknown,Unknown,Male,,3
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751587,Consumption insurance and entrepreneurial risk: Evidence from Italian micro-data,March 2003,Carlo Declich,Luigi Ventura,,Male,Male,Unknown,Male,,1
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751588,The reversal of large stock price declines: The case of large firms,March 2003,Georgina Benou,Nivine Richie,,Female,Unknown,Unknown,Female,,28
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751589,An international study of causality-in-variance: Interest rate and financial sector returns,March 2003,V. T. Alaganar,Ramaprasad Bhar,,Unknown,Unknown,Unknown,Unknown,,
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751590,Understanding the determinants of sovereign debt ratings: Evidence for the two leading agencies,March 2003,Antonio Afonso,,,Male,Unknown,Unknown,Male,,160
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751591,Determinants of credit card delinquency and bankruptcy: Macroeconomic factors,March 2003,Sumit Agarwal,Chunlin Liu,,Male,Unknown,Unknown,Male,,52
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751592,The impact of inflationary news on money market yields and volatilities,March 2003,Sanjay Ramchander,Marc W. Simpson,Mukesh Chaudhry,Male,Male,Male,Male,,6
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751593,A disaggregated approach to test the J-Curve phenomenon: Japan versus her major trading partners,March 2003,M. Mohsen Bahmani-Oskooee,Gour G. Goswami,,Unknown,Unknown,Unknown,Unknown,,
27,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751594,Estimating the sensitivity of state tax revenue to cyclical and wealth effects,March 2003,William Seyfried,Louis Pantuosco,,Male,Male,Unknown,Male,,4
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827214,Quantification of political risk with multiple dependent sources,June 2003,Ephraim Clark,Radu Tunaru,,Male,Male,Unknown,Male,,12
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827215,Long memory and structural breaks in hyperinflation countries,June 2003,Guglielmo Maria Caporale,Luis A. Gil-Alana,,Male,Male,Unknown,Male,,3
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827216,Relative importance of scheduled macroeconomic news for stock market investors,June 2003,Michael Graham,Jussi Nikkinen,Petri Sahlström,Male,Male,Male,Male,,45
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827217,Detecting speculative bubbles in an IT-intensive stock market,June 2003,Juha Junttila,,,Male,Unknown,Unknown,Male,,10
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827218,Variation in ex day dividend pricing: Myth or reality?,June 2003,Rakesh Bali,,,Male,Unknown,Unknown,Male,,6
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827219,An empirical examination of the effectiveness of dollar-cost averaging using downside risk performance measures,June 2003,Karyl B. Leggio,Donald Lien,,Unknown,Male,Unknown,Male,,13
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827220,Mutual fund managers: Does longevity imply expertise?,June 2003,Bruce A. Costa,Gary E. Porter,,Male,Male,Unknown,Male,,9
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827221,Fair bets and profitability in college football gambling,June 2003,Rodney J. Paul,Andrew P. Weinbach,J. Weinbach,Male,Male,Unknown,Male,,29
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827222,Contributions of state to regional income dispersion,June 2003,Edward Nissan,George H. Carter,,Male,Male,Unknown,Male,,2
27,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02827223,The effect of European integration on trade with the APEC countries: 1981–2000,June 2003,Donny Tang,,,,Unknown,Unknown,Mix,,
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761567,Client-firm market reaction to regulatory action against a major accounting firm,September 2003,Carl Pacini,William Hillison,,Male,Male,Unknown,Male,,2
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761568,"The role of U.S. Investors in international equity market inflows, outflows, and net flows for selected emerging asian markets",September 2003,Peggy E. Swanson,Anchor Y. Lin,,Female,Unknown,Unknown,Female,,2
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761569,Long-term performance of divesting firms and the effect of managerial ownership,September 2003,Robert C. Hanson,Moon H. Song,,Male,,Unknown,Mix,,
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761570,The informational content of credit rating announcements for share prices in a small market,September 2003,Fayez A. Elayan,Wei Huei Hsu,Thomas O. Meyer,Male,,Male,Mix,,
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761571,Cross-border linkages among Asian closed-end funds,September 2003,Emmanuel Anoruo,Sanjay Ramchander,Harold Thiewes,Male,Male,Male,Male,,2
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761572,Intra-industry and inter-country effects of European mergers,September 2003,Jorg Bley,Jeff Madura,,Male,Male,Unknown,Male,,6
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761573,A note on transfer prices and exchange rate pass-through,September 2003,Charles E. Hegji,,,Male,Unknown,Unknown,Male,,5
27,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761574,"Taxes, time diversification, and asset choice at retirement",September 2003,Thomas S. Howe,David L. Mistic,,Male,Male,Unknown,Male,,1
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761450,Some empirical evidence on the real effects of nominal volatility,March 2004,John Elder,,,Male,Unknown,Unknown,Male,,3
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761451,Consolidation in the financial services industry: Are there industry gains for acquisitions of security firms?,March 2004,Aigbe Akhigbe,Jarrod Johnston,Jeff Madura,Unknown,Male,Male,Male,,3
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761452,The J-curve dynamics of U.S. bilateral trade,March 2004,Mohsen Bahmani-Oskooee,Artatrana Ratha,,Male,Unknown,Unknown,Male,,48
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761453,Budget deficits and the current account balance: New evidence from panel data,March 2004,Hassan Mohammadi,,,Male,Unknown,Unknown,Male,,19
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761454,Empirical evidence on the effects of delisting from the National Market System,March 2004,P. R. Chandy,Salil K. Sarkar,Niranjan Tripathy,Unknown,Male,Unknown,Male,,4
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761455,IPO research symposium review,March 2004,Sanjay Varshney,Rich Robinson,,Male,,Unknown,Mix,,
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761456,Valuation uncertainty and IPOs: Investment bank versus commercial bank underwriters,March 2004,Gregory M. Hebb,Gregory H. MacKinnon,,Male,Male,Unknown,Male,,3
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761457,The effect of firm financial characteristics and the availability of alternative finance on IPO underpricing,March 2004,Beverly B. Marshall,,,Female,Unknown,Unknown,Female,,9
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761458,Early internet IPOs versus subsequent entrants,March 2004,Beverly B. Marshall,Claire E. Crutchley,Diane Lending,Female,Female,Female,Female,,5
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761459,Managerial bonding and stock liquidity: An analysis of dual-class firms,March 2004,Ekkehart Boehmer,Gary C. Sanger,Sanjay B. Varshney,Male,Male,Male,Male,,5
28,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761460,Underpricing and IPO ownership retention,March 2004,Richard M. Robinson,Mary Ann Robinson,Chien-Chih Peng,Male,,Unknown,Mix,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761607,Duration dependence testing for speculative bubbles,June 2004,Yvette S. Harman,Thomas W. Zuehlke,,Female,Male,Unknown,Mix,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761608,Dating structural changes: An illustration from financial deregulation,June 2004,Duane B. Graddy,Reuben Kyle,David Bass,Male,Male,Male,Male,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761609,"Firm risk, investment, and employment growth",June 2004,Matts M. Rosenberg,,,Male,Unknown,Unknown,Male,,14
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761610,A time-varying volatility approach to modeling the phillips curve: A cross-country analysis,June 2004,William L. Seyfried,Bradley T. Ewing,,Male,Male,Unknown,Male,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761611,"Random walks, cointegration, and the transmission of shocks across global real estate and equity markets",June 2004,James E. Payne,Anandi P. Sahu,,Male,Unknown,Unknown,Male,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761612,Modeling volatility in sector index returns with GARCH models using an iterated algorithm,June 2004,Farooq Malik,Syed Aun Hassan,,Male,Male,Unknown,Male,,21
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761613,An empirical analysis of market and institutional mechanisms for alleviating information asymmetry in the municipal bond market,June 2004,Jun Peng,Peter F. Brucato,,,Male,Unknown,Mix,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761614,International mergers and acquisitions: A jump diffusion model application,June 2004,Halil Kiymaz,Osman Kilic,,Male,Male,Unknown,Male,,4
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761615,"Stock market volatility, the news, and monetary policy",June 2004,Adrienne A. Kearney,Raymond E. Lombra,,Female,Male,Unknown,Mix,,
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761616,The causal relation between government revenue and spending: Evidence from Egypt and Jordan,June 2004,Bassam AbuAl-Foul,Hamid Baghestani,,Male,Male,Unknown,Male,,4
28,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761617,Compensation-based stock trading by corporate executive and aggregate management ownership of the firm: Some additional evidence,June 2004,Zahid Iqbal,Shekar Shetty,,Male,Unknown,Unknown,Male,,
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751733,"Return interval, dependence structure, and multivariate normality",September 2004,Thierry Ané,Chiraz Labidi,,Male,Unknown,Unknown,Male,,1
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751734,Market portfolio efficiency and value stocks,September 2004,Thierry Post,Pim van Vliet,,Male,Male,Unknown,Male,,1
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751735,Ownership structure and post-issue operating performance of firms conducting seasoned equity offerings in Thailand,September 2004,Piman Limpaphayom,Anchalee Ngamwutikul,,Unknown,Unknown,Unknown,Unknown,,
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751736,The impact of the Gramm-Leach-Bliley act on the financial services industry,September 2004,Abdullah Al Mamun,M. Kabir Hassan,Son Van Lai,Male,Unknown,,Mix,,
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751737,Marital status and the decision to file for personal bankruptcy: A duration model approach,September 2004,Jonathan D. Fisher,,,Male,Unknown,Unknown,Male,,2
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751738,Top finance journals: Do they add value?,September 2004,C. N. V. Krishnan,Robert Bricker,,Unknown,Male,Unknown,Male,,7
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751739,New developments in tobacco research: An introduction,September 2004,Rajeev K. Goel,,,Male,Unknown,Unknown,Male,,1
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751740,International patterns of cigarette smoking and global antismoking policies,September 2004,Rajeev K. Goel,Michael A. Nelson,,Male,Male,Unknown,Male,,7
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751741,The impact of nicotine replacement therapies on cigarette demand,September 2004,Frank J. Chaloupka,John A. Tauras,,Male,Male,Unknown,Male,,12
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751742,The efficacy of state-level antismoking laws: Demand and supply considerations,September 2004,Craig A. Gallet,,,Male,Unknown,Unknown,Male,,11
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751743,Quantile-regression estimates of cigarette demand elasticities for the United States,September 2004,Rajeev K. Goel,Rati Ram,,Male,Male,Unknown,Male,,16
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751744,State tax changes and quasi-experimental price elasticities of U.S. cigarette demand: An update,September 2004,Badi H. Baltagi,Rajeev K. Goel,,Male,Male,Unknown,Male,,2
28,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751745,Has a quarter-trillion-dollar settlement helped the tobacco industry?,September 2004,Stuart J. Fowler,William F. Ford,,Male,Male,Unknown,Male,,3
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761540,Cross-sectional differences in the liquidity effects of open market share repurchases,March 2005,Jaemin Kim,,,Unknown,Unknown,Unknown,Unknown,,
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761541,The 180-day lock-up period and insiders’ equity selling,March 2005,Ayi Ayayi,,,Unknown,Unknown,Unknown,Unknown,,
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761542,Corporate governance and the market impact of the Financial Services Modernization act of 1999 on bank returns and trading volume,March 2005,Carl Pacini,William Hillison,Deanna Burgess,Male,Male,Female,Mix,,
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761543,Decomposition of regional metropolitan and nonmetropolitan income inequality,March 2005,Edward Nissan,George Carter,,Male,Male,Unknown,Male,,4
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761544,Bank mergers and components of risk: An evaluation,March 2005,Suchismita Mishra,Arun J. Prakash,Manferd Peterson,Unknown,Male,Unknown,Male,,9
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761545,Does sentiment explain consumption?,March 2005,W. D. Anthony Bryant,Joseph Macri,,Unknown,Male,Unknown,Male,,16
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761546,Evidence of liquidity constraints found in theme park ticket auctions,March 2005,Richard J. Cebula,Richard D. McGrath,,Male,Male,Unknown,Male,,1
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761547,"Capital mobility, foreign aid, and openness: further panel data evidence from sub-Saharan Africa",March 2005,James E. Payne,Risa Kumazawa,,Male,Female,Unknown,Mix,,
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761548,Dating Business-Cycle turning points,March 2005,Rolando F. Peláez,,,Male,Unknown,Unknown,Male,,6
29,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761549,The cyclical nature of family income distribution in the united states: An empirical note,March 2005,Shahdad Naghshpour,,,Unknown,Unknown,Unknown,Unknown,,
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761550,Stratification economics: The role of intergroup inequality,June 2005,William Darity,,,Male,Unknown,Unknown,Male,,110
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761551,Managerial actions and stock transactions during financial distress: Some empirical evidence,June 2005,Zahid Iqbal,Dan French,,Male,Male,Unknown,Male,,3
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761552,Empirical analysis of up-or-out rules for promotion policies,June 2005,Rosemary L. Walker,,,Female,Unknown,Unknown,Female,,2
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761553,Accounting contagion: The case of Enron,June 2005,Aigbe Akhigbe,Jeff Madura,Anna D. Martin,Unknown,Male,Female,Mix,,
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761554,Impact of liquidity and information on the mispricing of newly public firms,June 2005,Joan Wiggenhorn,Jeff Madura,,Female,Male,Unknown,Mix,,
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761555,Rent seeking and the value of time,June 2005,Edmund H. Mantell,,,Male,Unknown,Unknown,Male,,1
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761556,"CEO compensation, shareholder rights, and corporate governance: An empirical investigation",June 2005,Pornsit Jiraporn,Young Sang Kim,Wallace N. Davidson,Unknown,,Male,Mix,,
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761557,Dollar-denominated accounts in Latin America during the 1990s,June 2005,Pere Gomis-Porqueras,Carlos Serrano,Alejandro Somuano,Male,Male,Male,Male,,
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761558,U.S. Presidential election impact on Canadian and Mexican stock markets,June 2005,Srinivas Nippani,Augustine C. Arize,,Male,Female,Unknown,Mix,,
29,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761559,Further evidence of the November effect,June 2005,Ken Johnston,Chris Paul,,Male,,Unknown,Mix,,
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761575,Profit efficiency sources and differences among small and large U.S. commercial banks,September 2005,Aigbe Akhigbe,James McNulty,,Unknown,Male,Unknown,Male,,33
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761576,What is the source of different levels of time-series return volatility? the intraday U-shaped pattern or time-series persistence,September 2005,Michael P. Hughes,Drew B. Winters,Jerry S. Rawls,Male,Male,Male,Male,,3
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761577,Why do reits engage in open-market repurchases?,September 2005,Chuo-Hsuan Lee,Chengo Hsieh,Xiaofeng Peng,Unknown,Unknown,Unknown,Unknown,,
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761578,"Equity private placements, liquid assets, and firm value",September 2005,Brooks,J. Edward Graham,,Male,Unknown,Unknown,Male,,7
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761579,Do Power GARCH models really improve value-at-risk forecasts?,September 2005,Ané,,,Unknown,Unknown,Unknown,Unknown,,
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761580,Biting the hand that fed it: Did the stock market boom of the late 1990s impede investment in manufacturing?,September 2005,Christian E. Weller,Brooke Helppie,,Male,Female,Unknown,Mix,,
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761581,Introduction to sports symposium,September 2005,Rodney Paul,Andrew Weinbach,,Male,Male,Unknown,Male,,
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761582,Appropriate statistical methodology for testing the efficiency of betting markets involving spread and totals,September 2005,M. Woodland,Linda M. Woodland,,Unknown,Female,Unknown,Female,,
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761583,Odds-setting efficiency in gambling markets: Evidence from the PGA TOUR,September 2005,Stephen Shmanske,,,Male,Unknown,Unknown,Male,,5
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761584,Market efficiency and NCAA college basketball gambling,September 2005,Rodney Paul,Andrew Weinbach,,Male,Male,Unknown,Male,,14
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761585,Bettor preferences and market efficiency in football totals markets,September 2005,Rodney Paul,Andrew P. Weinbach,,Male,Male,Unknown,Male,,12
29,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761586,The state of research on markets for sports betting and suggested future directions,September 2005,Raymond D. Sauer,,,Male,Unknown,Unknown,Male,,32
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834273,The most frequent contributors to the elite economics journals: Half century of contributions to the “Blue ribbon eight”,March 2006,Jean L. Heck,Peter A. Zaleski,,Male,Male,Unknown,Male,,3
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834274,Non-linear and non-symmetric exchange-rate adjustment: Evidence from medium- and high-inflation countries,March 2006,Michael G. Arghyrou,Virginie Boinet,Christopher Martin,Male,Female,Male,Mix,,
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834275,Do the black market and the official exchange rates converge in the long run?,March 2006,Mohsen Bahmani-Oskooee,Altin Tanku,,Male,Male,Unknown,Male,,1
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834276,Performance compensation contracts and CEOs’ incentive to shift risk to debtholders: An empirical analysis,March 2006,George J. Benston,Jocelyn D. Evan,,Male,Female,Unknown,Mix,,
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834277,The dangers of commitment: Monetary policy with adaptive learning,March 2006,George A. Waters,,,Male,Unknown,Unknown,Male,,3
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834278,Do self-serving managers choose Chapter 11 filing over out-of-court restructuring?,March 2006,Dong-Kyoon Kim,,,Unknown,Unknown,Unknown,Unknown,,
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834279,Identifying regime changes in closed-end fund discounts,March 2006,J. Christopher Hughen,Mark E. Wohar,,Unknown,Male,Unknown,Male,,1
30,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02834280,Arbitrage opportunities and immunization,March 2006,Joel R. Barber,Mark L. Copper,,Male,Male,Unknown,Male,,2
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761481,"The impact of productivity adjusted deviations from PPP on the U.S. inbound FDI: Evidence from Japan, U.K. and Germany",June 2006,Axel Grossmann,Gökçe Soydemir,,Male,,Unknown,Mix,,
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761482,Prudence and precautionary saving,June 2006,Luigi Ventura,Joseph G. Eisenhauer,,Male,Male,Unknown,Male,,11
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761483,Bubbles detection for inter-war European hyperinflation: A threshold cointegration approach,June 2006,Hing Lin Chan,Kai Yin Woo,,Unknown,Male,Unknown,Male,,2
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761484,The measurement of employment diversity for states and regions,June 2006,Edward Nissan,George Carter,,Male,Male,Unknown,Male,,2
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761485,Payroll tax incidence when the tax varies by jurisdiction: The case of the U.S. unemployment insurance tax,June 2006,Kevin J. Murphy,,,Male,Unknown,Unknown,Male,,
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761486,Empirics of corruption and crime: Symposium editor’s introduction,June 2006,Serdar Sayan,,,Male,Unknown,Unknown,Male,,
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761487,The effects of corruption on growth performance of the mena countries,June 2006,Imène Guetat,,,Unknown,Unknown,Unknown,Unknown,,
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761488,Corruption and distribution of public spending in developing countries,June 2006,Clara Delavallade,,,Female,Unknown,Unknown,Female,,135
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761489,"Corruption in transition economies: Effects of government size, country size and economic reforms",June 2006,Rajeev K. Goel,Jelena Budak,,Male,Female,Unknown,Mix,,
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761490,Corruption and the ownership composition of the multinational firm at the time of entry: Evidence from Turkey,June 2006,Ayça Tekin-Koru,,,Female,Unknown,Unknown,Female,,20
30,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02761491,Understanding the determinants of crime,June 2006,Ayse İmrohoroĝlu,Antonio Merlo,Peter Rupert,Unknown,Male,Male,Male,,8
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752735,To what extent are public savings offset by private savings in the OECD?,September 2006,Mark J. Holmes,,,Male,Unknown,Unknown,Male,,7
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752736,Accidents airline safety perceptions and consumer demand,September 2006,Jay Squalli,Mohsen Saad,,Male,Male,Unknown,Male,,8
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752737,Are there racial differences in faculty salaries?,September 2006,Javed Ashraf,Tayyeb Shabbir,,Male,Unknown,Unknown,Male,,2
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752738,Obesity: An economic and financial perspective,September 2006,Rajeev K. Goel,,,Male,Unknown,Unknown,Male,,8
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752739,The valuation effects of bank loan ratings in the presence of multiple monitors,September 2006,Thomas O. Meyer,Wei-Huei Hsu,Fayez A. Elayan,Male,Unknown,Male,Male,,2
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752740,Performance persistence of fixed income mutual funds,September 2006,William G. Droms,David A. Walker,,Male,Male,Unknown,Male,,9
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752741,Leveraged stock portfolios over long holding periods: A continuous-time model,September 2006,Dale L. Domian,Marie D. Racine,Craig A. Wilson,,Female,Male,Mix,,
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752742,Volatility transmission and financial crises,September 2006,Guglielmo Maria Caporale,Nikitas Pittis,Nicola Spagnolo,Male,Male,Female,Mix,,
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752743,Motives behind equity holding by banks: Evidence from India,September 2006,Mita Choudhury,,,Female,Unknown,Unknown,Female,,
30,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02752744,Explaining momentum profits with an epidemic diffusion model,September 2006,Nauzer J. Balsara,Lin Zheng,Luca Vidozzi,Unknown,Female,Male,Mix,,
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751507,Export growth and output growth: An application of bounds testing approach,March 2007,Mohsen Bahmani-Oskooee,Maharouf Oyolola,,Male,Unknown,Unknown,Male,,19
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751508,Spread volume for currency futures,March 2007,Robert T. Daigler,,,Male,Unknown,Unknown,Male,,3
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751509,A longitudinal study of net interest margin by bank asset size: 1992–2005,March 2007,Albert E. DePrince,Pamela D. Morris,,Male,Female,Unknown,Mix,,
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751510,"Ownership structure, expectations, and short sales on the Nasdaq",March 2007,J. Edward Graham,J. Christopher Hughen,,Unknown,Unknown,Unknown,Unknown,,
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751511,Empirical evidence of error in pricing of favorites and longshots in greyhound racing,March 2007,Anil Gulati,Shekar Shetty,,Male,Unknown,Unknown,Male,,1
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751512,Impact of regulation fair disclosure on the information flow associated with profit warnings,March 2007,Dave Jackson,Jeff Madura,,Male,Male,Unknown,Male,,12
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751513,Technical efficiency in texas nursing facilities: A stochastic production frontier approach,March 2007,Kris Joseph Knox,Eric C. Blankmeyer,J. R. Stutzman,,Male,Unknown,Mix,,
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751514,The impact of regulation fair disclosure on earnings management and analyst forecast bias,March 2007,Seung-Woog (Austin) Kwag,Kenneth Small,,Unknown,Male,Unknown,Male,,14
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751515,The long-run relationship between private and public savings: An empirical note,March 2007,Abir Mandal,James E. Payne,,Female,Male,Unknown,Mix,,
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751516,National culture and environmental sustainability: A cross-national analysis,March 2007,Hoon Park,Clifford Russell,Junsoo Lee,,Male,Unknown,Mix,,
31,1,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751517,An error-correction model for forecasting changes in foreign currency futures spreads,March 2007,Stephen E. Wilcox,John M. Geppert,,Male,Male,Unknown,Male,,1
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751640,A market model of analysts’ opinions to explain changes in the dispersion of opinions,June 2007,Erik Benrud,,,Male,Unknown,Unknown,Male,,1
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751641,Does attending predominately-female schools make a difference? Labor market outcomes for women,June 2007,Sherrilyn M. Billger,,,Unknown,Unknown,Unknown,Unknown,,
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751642,Surviving chapter 11: Why small firms prefer supplier financing,June 2007,Jocelyn Evans,Timothy Koch,,Female,Male,Unknown,Mix,,
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751643,An empirical investigation of going private decisions of U.S. firms,June 2007,Kim Gleason,Bruce Payne,Joan Wiggenhorn,,Male,Female,Mix,,
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751644,Hedge funds in portfolios of risk-averse investors,June 2007,Matthew Hood,John R. Nofsinger,,Male,Male,Unknown,Male,,1
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751645,Estimating the determinants of stock price changes,June 2007,C. N. V. Krishnan,,,Unknown,Unknown,Unknown,Unknown,,
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751646,Inter-market competition for exchange traded funds,June 2007,Vanthuan Nguyen,Bonnie F. Van Ness,Robert R. Van Ness,Unknown,,Male,Mix,,
31,2,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02751647,Keiretsu and risk: An examination of the risk exposure of keiretsu banks in Japan,June 2007,Rungrudee Suetorsak,,,Unknown,Unknown,Unknown,Unknown,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885720,Inferring trader behavior from transaction data: A trade count model,September 2007,David Jackson,,,Male,Unknown,Unknown,Male,,4
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885721,Optimal wage contracts under asymmetric information and moral hazard when investment decisions are delegated,September 2007,C. N. V. Krishnan,,,Unknown,Unknown,Unknown,Unknown,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885722,Forecast persistence and clustering: Additional evidence,September 2007,Seung-Woog Kwag,,,Unknown,Unknown,Unknown,Unknown,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885723,How useful are signals? A micro-structure analysis,September 2007,Michele O’Neill,Judith Swisher,,Female,Female,Unknown,Female,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885724,Enhancing managerial incentives and value creation: Evidence from corporate spinoffs,September 2007,Unyong Pyo,,,Unknown,Unknown,Unknown,Unknown,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885725,Why do companies choose to go ipos? New results using data from Taiwan,September 2007,Yang-Pin Shen,Peihwang Wei,,Unknown,Unknown,Unknown,Unknown,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885726,Ownership structure and golden parachutes: Evidence of credible commitment or incentive alignment?,September 2007,Kenneth Small,Jeff Smith,H. Semih Yildirim,Male,Male,Unknown,Male,,5
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885727,Recent perspectives in the economics of the arts,September 2007,Michael Toma,,,Male,Unknown,Unknown,Male,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885728,The economics of a good party: Social mechanics and the legitimization of art/culture,September 2007,Elizabeth Currid,,,Female,Unknown,Unknown,Female,,32
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885729,Preferred work patterns of creative artists,September 2007,David Throsby,,,Male,Unknown,Unknown,Male,,17
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885730,Does giving make us prosperous?,September 2007,Arthur C. Brooks,,,Male,Unknown,Unknown,Male,,21
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885731,Recent evidence on the determinants of concert attendance for mid-size symphonies,September 2007,Michael Toma,Holly Meads,,Male,Female,Unknown,Mix,,
31,3,Journal of Economics and Finance,,https://link.springer.com/article/10.1007/BF02885732,Spatial distribution of the specialization of arts,September 2007,Edward Nissan,George Carter,,Male,Male,Unknown,Male,,
32,1,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9007-1,Pricing behavior of exchange traded funds,January 2008,Jeff Madura,Thanh Ngo,,Male,,Unknown,Mix,,
32,1,Journal of Economics and Finance,05 October 2007,https://link.springer.com/article/10.1007/s12197-007-9005-3,Duration dependence in US business cycles: An analysis using the modulated power law process,January 2008,Haigang Zhou,Steven E. Rigdon,,Unknown,Male,Unknown,Male,"Previous studies have applied nonparametric and parametric methods to analyze the duration dependence in business cycles. Examples of early works that applied nonparametric χ2-type goodness-of-fit tests include McCulloch (1975), Savin (1977), So (1994), Leeuw (1987), and Diebold and Rudebusch (1987). Later parametric studies mainly used the Weibull distribution because of its parametric nature and of its flexibility in modeling different types of duration dependence. Using the National Bureau of Economic Research (NBER) monthly reference cycle chronology from 1854 to 1990, Sichel (1991) applied the Weibull hazard function to examine the duration dependence of business cycles and found positive duration dependence in prewar expansions and postwar contractions. Cochran and Defina (1995) also used the Weibull hazard function to investigate duration dependence in US stock market cycles over the January 1885 to July 1992 period. They found evidence of duration dependence in prewar expansions and in postwar contractions but did not find evidence of duration dependence in prewar contractions and postwar expansions. Using a generalized Weibull model to analyze the NBER monthly reference cycles chronology from December 1854 to March 2001, Zuehlke (2003) documented evidence of duration dependence for all samples in the study. Harman and Zuehlke (in press) applied the generalized Weibull model to analyze the US stock market data and document positive duration dependence for both prewar and postwar samples of stock market expansions and contractions. One potential drawback of using a renewal process is that it assumes the sample observations are from an independent and identically distributed (i.i.d.) stochastic process, i.e., the durations between consecutive events are positive, independent, and identically distributed. This assumption is sound if a physical renewal process involves successive replacements of failed mechanical components. However, with business cycles of the past 150 years, it may not be reasonable to assume the interarrival times between economic troughs, or peaks, follow the same distribution. For example, Basu and Taylor (1999) have observed a significant decline in the volatility of measured US output. Instead, this study uses a generalized model, the modulated power law process (MPLP), which is a compromise between a renewal process and a non-homogeneous Poisson process, to study the duration dependence in the US business cycles. Using the modulated power law process, the results indicate the presence of positive duration dependence in our samples of US business cycles. The results are robust to different sampling processes. Hypothesis tests of the parameters confirm that the MPLP model is appropriate in modeling US business cycles. This study contributes to the literature in that it introduces a new statistical model to analyze the duration dependence of business cycles. The model relies on less restrictive assumptions about the underlying process of business cycles, but offers a more complete description of the statistical process. While traditional models measure only the local performance, the MPLP model measures both the global and local performance of the underlying statistical process. The model can also be applied to analyze other economic or financial cycles. The remainder of the paper is organized as follows. Section 2 describes the modulated power law process and hypotheses regarding parameters in the MPLP model. The data used in this study and the sampling process are explained in Section 3. Section 4 presents the estimation of the MPLP parameters and the results of the hypothesis tests on the parameters, while Section 5 offers brief concluding remarks.",8
32,1,Journal of Economics and Finance,05 October 2007,https://link.springer.com/article/10.1007/s12197-007-9003-5,"Testing for infinite order stochastic dominance with applications to finance, risk and income inequality",January 2008,John Knight,Stephen Satchell,,Male,Male,Unknown,Male,"Economists have always been interested in the measurement and comparison of income inequality across groups and it is well known that difficulties in using inappropriate inequality measures can be avoided by the use of notions of stochastic dominance. Such measurement of inequality across income distributions has close parallels in the measurement of risk in payoffs or returns when we consider problems of decision making under uncertainty. However, the parallel breaks down in the following sense. While it may be natural to assume that the income distribution of, say, Latvia should be uncorrelated with the income distribution of New Zealand, it is not appropriate to assume that their stock indices are independent since they will both be correlated with the global market. Thus, stochastic dominance procedures that are based on the comparison of two marginal distributions and do not utilize the extra information in the joint distribution may well be inferior to procedures based on all the information. We present a dominance testing procedure that allows for cross-correlations. The procedure we discuss may well be useful in inequality studies. In the context of income inequality, a correlated example may be where we wish to compare the wage versus the dividend distribution for the same group of individuals. Indeed, the difficulties with independence have been recognized by a number of authors including Davidson and Duclos (1997, 2001) who present a Lorenz analysis for the correlated case; for the econometric techniques appropriate for testing stochastic dominance between two income distributions, see Anderson (1996) and Davidson and Duclos (op. cit.). In what follows we shall not only consider testing stochastic dominance over two correlated distributions but we shall also refine the definition of stochastic dominance used in our tests. The notion of stochastic dominance we use is that of infinite order degree stochastic dominance. This is well known, see Thistle (1993), to be equivalent to completely monotone marginal utility for wealth, an assumption deemed to be desirable by Scott and Horvath (1980) and Pratt and Zeckhauser (1987) among many others. These authors present many arguments in favour of the class of utility functions associated with the dominance concept, the simplest being that these utility functions include as examples, those members of the hyperbolic absolute risk aversion (HARA) class that have sensible comparative statics such as decreasing absolute risk aversion. Another argument is based around the concept of proper risk aversion, which is defined as a situation where an undesirable lottery can never be made desirable by the presence of an independent undesirable lottery. It is proved by Pratt and Zeckhauser (1987, Theorem 2, p. 148) that marginal utility being completely monotone, which they call completely proper, will imply proper risk aversion. The assumption of complete monotonicity has the behavioural implication that if we can remove some unfavourable risks, then agents will be more tolerant to other retained independent, unfavourable risks. In Section 2 we present a brief overview of the relevant ideas from utility theory before presenting our testing procedure in Section 3. Conclusions and examples follow in Section 4.",5
32,1,Journal of Economics and Finance,11 October 2007,https://link.springer.com/article/10.1007/s12197-007-9002-6,Predicting capacity utilization: Federal Reserve vs time-series models,January 2008,Hamid Baghestani,,,Male,Unknown,Unknown,Male,"Capacity utilization is one of several indicators considered by the Federal Reserve Open Market Committee (FOMC) in formulating monetary policy. The stated reason for monitoring this indicator is, “The level of capacity utilization in the industrial sector provides information on the level of resource utilization in the economy which may in turn provide information on the likely future course of inflation.”Footnote 1 For instance, high (low) capacity utilization, with the potential of causing an unfavorable balance between inflation and growth in real output, may signal the need for a contractionary (expansionary) monetary policy.Footnote 2
 In keeping the FOMC members informed about the future state of the economy, the staff at the Federal Reserve Board of Governors presents the committee with a briefing document called the Greenbook prior to every meeting. This document contains the forecasts of major macroeconomic variables, one of which is manufacturing capacity utilization. Using the relevant information available at the time, these forecasts are produced to help the committee with a successful formulation of monetary policy. This study evaluates the Greenbook or Federal Reserve forecasts of capacity utilization in order to see whether policymakers are provided with accurate and reliable predictions of this indicator.Footnote 3 For the forecasts made in 1985–2000, our evaluation involves answering the following three questions:
 Are the Federal Reserve forecasts “weakly” rational? Can the federal funds rate help produce superior forecasts? Are the Federal Reserve forecasts of value to the FOMC members? In answering the first two questions, we generate comparable forecasts of capacity utilization (1) from a univariate ARMA (autoregressive moving-average) model which efficiently uses past information in capacity utilization,Footnote 4 and (2) from an augmented-ARMA (A-A) model which utilizes past information in both capacity utilization and the federal funds rate. In addition to accurately predicting the directional change in capacity utilization, the Federal Reserve forecasts are “weakly” rational and generally superior to the bivariate forecasts. We also find that the federal funds rate serves as a leading indicator for capacity utilization, indicating that monetary policy is non-neutral.Footnote 5 Batra (2002) shows that the non-neutrality of policy can be due to the business cycle fluctuations of capacity utilization. This implies that accurate forecasts of capacity utilization, as one of many inputs to policymakers, are important to successfully formulate and implement monetary policy. This paper is organized as follows: Section 2 describes the Federal Reserve, ARMA, and A-A models of capacity utilization. Section 3 discusses the methodology and the empirical results. Section 4 concludes this study.",2
32,1,Journal of Economics and Finance,11 October 2007,https://link.springer.com/article/10.1007/s12197-007-9000-8,A closer look at Black–Scholes option thetas,January 2008,Douglas R. Emery,Weiyu Guo,Tie Su,Male,Unknown,,Mix,,
32,1,Journal of Economics and Finance,11 October 2007,https://link.springer.com/article/10.1007/s12197-007-9006-2,Day trading and stock price volatility,January 2008,Petri Kyröläinen,,,Male,Unknown,Unknown,Male,"“Day trading by small individual investors may also be an influence on the idiosyncratic volatility of some stocks, particularly at the end of our sample period” Campbell et al. (2001) Journal of Finance 56, page 40 “Some day traders may add to market depth by providing instant liquidity, while those who try to profit from short-term momentum cycles probably increase market volatility. Which effect dominates remains an unresolved empirical question.” Barber and Odean (2001) Journal of Economic Perspectives 15, page 51 When an investor buys and sells the same stock on the same day, he is said to have made a day trade. Day trading by individual investors became popular during the boom in technology stocks at the end of the millennium.Footnote 1 Not surprisingly, day trading tended to be heavily concentrated in the technology sector. In the related literature Campbell et al. (2001) and Schwert (2002) find that idiosyncratic stock price volatility increased during the boom period in technology stocks and was particularly high after the bubble burst. Furthermore, this unusual volatility tended to be associated with technology stocks. This raises the question of a connection between these two phenomena. If the earlier literature suggests a connection between day trading and stock price volatility, very little is actually known about whether such a connection really exists. There is, however, a great need for such information, as can be seen from the quotations above. The objective of this paper is therefore to study whether day trading by individual investors is associated with stock price volatility. Following the earlier literature on the volume–volatility relation, I study this issue in the time-series context.Footnote 2 This enables me to examine whether there is an additional effect from day trading on top of the well-documented positive relation between total volume (or number of transactions) and volatility. The empirical analysis in this study uses comprehensive trading records of Finnish investors. Although the main focus is on the day trading by individual investors, I also control for day trading by financial and non-financial firms. Day trading activity is measured both in terms of number of transactions and trade size. Both of these measures are expressed relative to the trading level of the group (e.g. individual day trade size relative to the size of individuals’ other trades). These measures capture the effect of day trading that is distinct from the “normal” trading of a group, and allow a clearer comparison across investor types. I find that the intraday stock price volatility is strongly and positively associated with the number of day trades by individual investors, even after controlling for the previously documented volume–volatility relationship. This relation is consistent both across sub-sample periods and individual stocks. On the other hand, there does not seem to be a relation between institutional day trading and volatility once the normal trading activity is controlled for. Although there may be causality issues involved, these results at least suggest that the liquidity effects of day trading play a minor role in the relation between day trading and volatility. Campbell et al. (2001), argue that day trading could have been one factor behind the increased volatility, particularly during the boom of the technology stocks. Empirical evidence on the potential role of day trading in increasing volatility is provided by Linnainmaa (2003), who finds that Finnish day traders tend to concentrate their trading near the opening and close of the market—buying at the beginning of the trading session and selling near the close of the market. On the other hand, he also finds that day traders are net providers of liquidity which, via increased market depth, could potentially reduce stock price volatility. He does not, however, explicitly test for the effect of day trading on stock price volatility. This paper seeks to extend the rich literature on the volume–volatility relation. The earliest studies to identify the positive relation between volume and the absolute value of stock price changes are those by Crouch (1970a,b) and Clark (1973).Footnote 3 More recently, Jones et al. (1994) established that the number of transactions explains price volatility better than trading volume. Chan and Fong (2000) find that the order imbalance accounts for a fraction of the volume–volatility relation mainly in large trade size categories that are likely to be dominated by institutional investors. Traditional explanations of the volume–volatility relation are a mixture of distribution hypothesis, asymmetric information, and differences in opinion. The link between trading activity and price impact can also be found in the studies on intraday stock price behavior. For example, Chan and Lakonishok (1995) find that investment managers with high turnover rates incur larger price impacts. There are also some other studies evincing behavioral-based explanations for stock price volatility. Avramov et al. (2006) present evidence that herding investors drive the asymmetric volatility phenomenon on the US stock markets. Goetzmann and Massa (2004) empirically test the prospect theory based model of Grinblatt and Han (2005) and find that, as predicted by the theory, disposition-prone investors have a decreasing impact on stock price volatility. Jackson (2003) finds that on the Australian markets volatility tends to be higher in stocks that are traded heavily by institutional investors compared to those traded heavily by individual investors. There are also a number of theoretical papers suggesting that noise trading increases the volatility of asset returns (see Black 1986; Delong et al. 1990; Campbell and Kyle 1993; Jackson 2003). In this study, I focus particularly on the relation between day trading and volatility from the point of view of individual investors, who are ex ante the most likely candidates for noise traders. Earlier empirical evidence also seems to support the idea that the day trading of individual investors is a good proxy for noise trading. Barber and Odean (2000) study retail customers of a large US brokerage house and find that individual investors tend to lose to risk-adjusted benchmarks. Morever, they find that the most active 20% of individuals perform worst. Grinblatt and Keloharju (2000) reports that individual Finnish investors tend to perform worse than any of the institutional investor categories. Individual investors also tend to lose on their own passive benchmarks, suggesting that they trade too much (see Odean 1999). The poor performance of active individual investors is also reflected in the finding from the Taiwanese market that day traders on average tend to lose money (see Barber et al. 2004). In addition, Linnainmaa (2003) finds that individual Finnish day traders do not earn better returns than other individual investors. The main empirical research question in this paper is: Is day trading related to stock price volatility? If day trading by individual investors is a good proxy for noise trading, we may also ask the more general question: Does noise trading matter in terms of asset pricing? I address these questions by examining complete trading records of individual Finnish investors regarding their most heavily day traded stocks. These stocks with speculative characteristics and a high share of individual investors’ day trade volume are good candidates for high noise-trading stocks. To achieve a more complete picture of the relation between day trading and volatility, I also analyze the trading records of the most important institutional Finnish investors—financial and non-financial firms. This paper therefore also provides some evidence of the relative importance of these different investor groups in determining the stock price volatility. This in turn may shed some light on the plausibility of information flow vs noise trading as explanations for volatility. The structure of the rest of the paper is as follows: Section 2 describes the data, Section 3 presents the volatility modeling methodology and empirical results, and Section 4 concludes the paper.",13
32,1,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9008-0,Degrees of tax indexation and nominal interest rates: Effects of inflation on incentives to save and invest,January 2008,Anandi P. Sahu,,,Unknown,Unknown,Unknown,Unknown,,
32,2,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9004-4,Interest rate and bank stock returns asymmetry: Evidence from U.S. banks,April 2008,Priti Verma,Dave O. Jackson,,Female,Male,Unknown,Mix,,
32,2,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9014-2,A macroeconomic analysis of foreign assets and foreign liabilities,April 2008,Ying Wu,,,,Unknown,Unknown,Mix,,
32,2,Journal of Economics and Finance,11 October 2007,https://link.springer.com/article/10.1007/s12197-007-9010-6,"An investigation of long memory in various measures of stock market volatility, using wavelets and aggregate series",April 2008,Robert DiSario,Hakan Saraoglu,H. C. Li,Male,Male,Unknown,Male,"In recent years, numerous researchers studied the existence of long-memory in return volatilities of financial assets. These studies focused on finding new measures for volatility as well as developing new methods to test for long-memory. Ding et al. (1993), Lobato and Savin (1998), and Ray and Tsay (2000) employed squared daily returns as a measure of volatility. Ding et al. (1993) showed evidence of long memory in squared daily returns on the S&P 500 index. They demonstrated significant autocorrelation for lags up to 10 years on squared daily returns. Using a sample of returns for the S&P 500 index between July 1962 to December 1994, Lobato and Savin (1998) also showed strong evidence of long memory in the squared daily returns. Ray and Tsay (2000) tested for long-range dependence in daily stock volatilities of selected companies in the S&P 500 index, and they found strong evidence of significant long-range dependence in the majority of the sampled companies. Using daily data of the thirty Dow stocks, Lobato and Velasco (2000) provided evidence of long memory in trading volume and return volatility of these stocks. Granger and Ding (1995) showed that the absolute value of returns, which is also a measure of volatility, contains long-memory. In addition to the dependence in squared daily returns, Ding et al. (1993) also found long memory in series derived from power transformations of the absolute daily returns. Assaf and Cavalcante (2005), and Ray and Tsay (2000) tested for long-memory in the log squared returns. Various methods have been used in the literature to examine long-memory characteristics of stock market volatility. Granger and Ding (1995) used a method developed by Geweke and Porter-Hudak (1983) to test for long-memory in absolute values of the S&P 500 Index returns. Lobato and Velasco (2000) utilized a two-step semiparametric estimator of the long-memory parameters of stock market volatility and trading volume. Their analysis was conducted in the frequency domain and involved tapering the data rather than detrending them. Ray and Tsay (2000) employed a canonical correlation test for common long-range dependence in the volatilities of a group of S&P 500 Index components. In a study that investigated long-memory in the volatility of Brazilian stock market returns, Assaf and Cavalcante (2005) used the modified rescaled range (R/S) statistic of Lo (1991), the rescaled variance measure of Giraitis et al. (2000), and the semiparametric estimator proposed by Robinson (1995). Jensen and Whitcher (2000) showed that sampling volatility in the wavelet domain, rather than in the time domain, leads to an efficient sampler of the posterior distribution of the long-memory parameter of the volatility. They tested their method using daily returns of the Aluminum Corporation of America stock. In this study, we use the discrete wavelet transform, the variance method, and the absolute value method to test for long memory in the absolute daily returns, squared daily returns, and log squared daily returns of the S&P 500 Index. We review the measures of return volatility and describe our data in the next section. The third section provides an overview of long memory in time series. Then, we discuss the methods and results of our analysis in Section 4, and conclude in Section 5.",5
32,2,Journal of Economics and Finance,07 November 2007,https://link.springer.com/article/10.1007/s12197-007-9016-0,Incorporating correlation regimes in an integrated stressed risk modeling process,April 2008,José R. Aragonés,Carlos Blanco,,Male,Male,Unknown,Male,"Value at Risk (VaR) has become a standard market risk measure for institutions worldwide, and is enjoying rapid and wide-ranging success. Its main appeal lies in its simplicity; a single number offers information about what a firm may expect to lose over a time horizon, uncovers uncertainties of the firm, and provides crucial information of the overall firm's risk profile to senior management, traders, shareholders, investors, auditors, rating agencies, and regulators. VaR is defined as the p-quantile of the probability distribution of future portfolio value, i.e.
 Where:
 
f(x) is the probability distribution of the future portfolio value 
p = 1−c 
c, is a given confidence level There are several approaches to estimating VaR (See Dowd 2002 for a comprehensive review of alternative VaR methodologies). Parametric approaches require estimating the parameters of the underlying distribution using observed data; under the assumption of normality VaR, for linear portfolios, may be estimated as a predetermined quantile of a Gaussian distribution. Another popular approach to estimating VaR is by using Monte Carlo simulation. This technique requires the assumption of a predetermined distribution of the stochastic variable(s). Hypothetical paths for the value of our portfolio are constructed by drawing a set of random values of the stochastic variables which determine the value of the instruments within the portfolio and revaluing that portfolio under the new simulated market prices. Monte Carlo simulation enables the calculation of quite complex portfolios including those with non-linear risk factors or path-dependent instruments. There are also a set of non parametric approaches which do not require making strong assumptions about the distribution of profits and losses or returns. The most popular non parametric approach is historical simulation where market prices are simulated according to their historical price behaviour on any given period and the current portfolio is revalued under each of those simulated market prices. After estimating and ordering the simulated historical profit and loss (return) observations of the portfolio under consideration, VaR is the predetermined p-quantile of the distribution for the given confidence level. It is well known that VaR as a risk measure suffers from certain limitations and in particular, does not provide an indication about what we can expect to lose if we exceed the VaR threshold. The Expected Tail Loss (ETL) is the expected value of our losses (X) when they exceed VaR, i.e.:
 While VaR does not provide any guidelines about what to expect when losses are greater than VaR itself, ETL does give us an indication of what we may expect in “bad times”. Therefore, Expected Tail Loss numbers are rapidly gaining acceptance amongst risk managers as a complementary measure to VaR. ETL provides an indication of the average expected loss in the tail, and therefore can complement VaR numbers to provide a more complete picture of portfolio tail risk. Artzner et al. (1999) developed a theory of coherent risk measures and concluded that VaR does not satisfy certain key conditions, in particular, it does not, in general, satisfy sub-additivity while ETL does and therefore benefits from all the attractions of sub-additivity. As Dowd (2002, p. 36) points out “Unlike VaR, the ETL satisfies the conditions for a coherent risk measure, and coherent risk measures have a number of attractive features. …users of VaR would be well advised to switch over”. Most VaR measures rely on too many simplistic assumptions about market and portfolio behavior, particularly with regards to extreme market conditions. Several firms relying on naive implementations of VaR methodologies have been surprised by large losses, considerably larger than the ones predicted by the model. One of the most unrealistic assumptions is the reliance on a single variance covariance matrix of risk factors representing the “average” individual and joint expected variability of those risk factors. However, markets rarely behave according to those “average” relationships, particularly in the tail of the distribution, which is where risk management analysis truly lies. As a response of the limitations of traditional VaR models, institutions conduct stress tests on a regular basis to capture portfolio risk under extreme market conditions not covered by the VaR analysis. The essence of stress testing is, in fact, the creation of custom-design scenarios, fed into a calculation engine to produce estimates of the profits or losses that can be expected under these scenarios. Despite their extensive use by risk managers, it is puzzling that relatively little attention has been paid to integrate extreme event measurement and modeling within the risk calculations. The current practice of using stress tests outside the formal risk modeling process has a number of problems:
 If an institution is using stress tests to complement VaR, that means that it has two sets of separate risk estimates – probabilistic estimates (such as VaR), and the loss estimates produced by stress tests – and no way of combining them. The results of stress tests are difficult to interpret because they give us no idea of the probabilities of the events concerned, and in the absence of such information the results of those stress tests may be absolutely irrelevant due to the fact that the importance of the results is a function of the probabilities attached to those events. The value of stress testing depends critically on the choice of scenarios by the risk modeler. This subjectivity creates a serious problem in order to conduct an objective assessment of the value of a stress test, and therefore poses obvious problems for senior management, regulators and other interested parties trying to assess a firm’s stress testing procedures. Therefore, a new backtesting framework to determine the accuracy of VaR and stress tests is needed. Some of the stress tests are conducted using a questionable approach. For example, a very common stress-testing procedure is to ‘shock’ certain prices or returns to particular values, assume that other prices take their usual values, and derive the firm’s risk exposure accordingly. This approach is open to objection because it ignores correlations between the stressed prices and other prices, and empirical evidence suggests that the failure to allow for cross market effects can make a big difference to results [See Kupiec (1998)]. A first coherent solution to these problems is to integrate stress testing into formal risk modeling by assigning probabilities to stress-test scenarios. The resulting risk estimates incorporate information from probabilistic scenarios as well as the outcomes from stress tests and offer risk managers a single, integrated set of risk estimates to work with. Of course, these estimates are now dependent on the judgmental factors that go into stress testing and into the evaluation of the probabilities of scenarios, but in our view it is better to incorporate our judgments of stress-test events than to ignore them completely in risk modeling. On the other hand, even though it is widely accepted that both volatilities and correlations break down under stress conditions, most risk models use average volatilities and correlations to describe joint market movements for all types of possible market movements. Therefore, it seems reasonable to assume that each “state of the world” should have an associated set of volatilities and correlations to describe the expected behavior of each risk factor individually and jointly under those conditions. When conducting stress tests, it is particularly important to correlate risk factor changes for specific extreme events using extreme correlations. Many authors have analyzed this issue, as is the case of Longin and Solnik (2001) which test the hypothesis that international equity market correlations increase in volatile times. Using extreme value theory to model the multivariate distribution tails they conclude that the correlations of large positive returns tend to decrease and converge to zero and the assumption of multivariate normality cannot be rejected. However, this is not the case with large negative returns, as correlation does not converge to zero but tends to increase with the threshold level and rejection of multivariate normality is highly significant statistically. They also conclude that correlation is not related to market volatility “per se” but rather to the overall market conditions; it increases in bear markets but not in bull markets. Kim and Finger (2000) suggest the use of a mixture of bivariate normals to specify the joint distribution of asset returns under normal and stress conditions. For the majority of cases (quiet days) the asset returns are drawn from a normal distribution with lower volatility and certain level of correlation; on rare hectic days asset returns are drawn from another normal distribution with higher volatility and a different set of correlations. They finally develop a predictive stress testing methodology based on the estimates from the hectic correlation levels. In this article, we present a simple and intuitive way to extend the work presented in Aragonés et al. (2001) to allow for different correlation and volatility regimes in the stress test calculations. In the following section we propose a stepwise methodology to introduce multiple correlation regimes that permits us to improve the definition of stress scenarios and incorporate them into a formal risk model.",1
32,2,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9009-z,Potential targets: An analysis of stock price reactions to acquisition program announcements,April 2008,Gurmeet Singh Bhabra,,,,Unknown,Unknown,Mix,,
32,2,Journal of Economics and Finance,11 October 2007,https://link.springer.com/article/10.1007/s12197-007-9012-4,The determinants and survival of reverse mergers vs IPOs,April 2008,Frederick Adjei,Ken B. Cyree,Mark M. Walker,Male,Male,Male,Male,"The most common means for private firms to go public is by an Initial Public Offering (IPO). However, firms that choose an IPO must trade off the high cost of going public via an IPO vs the benefits of an IPO. Costs of IPOs include direct fees as well as IPO underpricing. Benefits include that IPO underwriters provide liquidity and marketing, which could be important if founders desire to convert their wealth into cash in the future as in Ritter and Welch (2002) or if IPOs provide a signal about firm quality and risk as in Carter and Manaster (1990). In some cases, such as when firms are small, there is a question of whether or not going public via an IPO is worth the cost. The reduced benefits of an IPO for smaller firms have therefore prompted some small companies to go public via an alternative method such as a reverse merger (RM).Footnote 1 An RM is a transaction in which a private company’s owners gain control of a public company (a shell) by merging it with their private company. The owners of the private company receive most of the shares of the shell (more than 50%) and control of the shell’s board of directors. The transaction can be completed in as little as 3 months, and the private company then becomes a public company. If the benefits of RMs are equal to IPOs in the going public process, economic theory suggests RMs would eventually dominate the going public process because of the lower cost associated with RMs. Conversely, if there are other problems with RMs, such as poorer performing firms self-selecting this form of going public, then RMs will not likely threaten IPOs as the preferred way to go public. Financial analysts contend that RMs are a back door to going public and private firms using reverse mergers are inferior to those using IPOs. An example of this sentiment is contained in the following quote: For investors, Mello says, the fact that a company has completed a reverse merger should immediately raise a red flag. The company can still be a solid investment, but extra research is necessary. Some of the forms filed with the Securities and Exchange Commission for standard IPOs, such as registration statements, are never completed for reverse mergers”Footnote 2
 The purpose of this study is three-fold. First, we empirically test whether private firms which use RMs are different than those using IPOs. We identify the determinants of RMs, and analyze why a private firm would elect to go public using an RM rather than a traditional IPO. Second, we examine private firm features at the time of listing to ascertain whether they meet initial listing requirements. Finally, we compare the survival of RMs and IPOs in the aftermarket using a hazard model to estimate the conditional probability of delisting. We find that compared with private firms using IPOs, private firms using reverse mergers to go public are younger, smaller, and have poorer ex ante performance. Additionally, some of the private firms which reverse merge do not meet the initial listing requirements of the exchanges on which they list. However, all IPOs meet at least one of the listing requirements. Forty-two percent of RM firms are delisted within 3 years of listing on an exchange and the most probable delisting time of an average reverse merged firm is in the 24th month with a probability of delisting of 5.69%. In contrast, only 27% of IPOs are delisted in the 3-year study period and the most probable delisting time is in the 37th month with a probability of 5.12%. This is the first study to directly analyze the characteristics of a private firm that selects an RM or an IPO, and to examine whether or not an RM is a back door to going public. Additionally, our results, consistent with those of Gleason et al. (2005b), provide further support for the contention that RM firms have low profitability and survivability in the aftermarket. With the increased popularity and questions about the profitability of reverse mergers, corporate executives as well as financial analysts should find the results of this study relevant. As an example of a firm going public, Cancervax used an IPO in August 2003, listing on NASDAQ at $13.00 a share. In fiscal years 2004 and 2005, Cancervax recorded net losses of $56 million and 40 million, respectively. By December, 2006, the stock price had fallen to $3.46. In May, 2006, Micromet, a private German company, was formally taken over by CancerVax, a publicly held US company, by way of a reverse merger, forming a new NASDAQ-listed company which was renamed Micromet Inc. Although CancerVax acquired Micromet, shareholders of Micromet, received 67.5% of the shares of the new combined company, giving them control of Micromet Inc. Micromet Inc. started trading on NASDAQ on May 9, 2006, opening at $9.33 and closed at 6.54 with a volume of 40,925 shares traded. Six months later, on November 13, 2006, Micromet Inc., still listed on NASDAQ, had a closing price of $2.19 with a volume of 9,666 shares traded. Although this reverse merger of a newly issued IPO may not be typical, it illustrates the type of results that are publicized in the financial press that give RMs a bad reputation. Our aim in this study is to discover if this reputation is warranted or not and if the CancerVax/Micromet experience is typical.",57
32,2,Journal of Economics and Finance,05 October 2007,https://link.springer.com/article/10.1007/s12197-007-9013-3,Smoking prevalence in the United States: Differences across socioeconomic groups,April 2008,Rajeev K. Goel,,,Male,Unknown,Unknown,Male,"The prevalence of smoking varies significantly across various population groups and these groups have responded differently over time to various smoking control measures. For instance, Jha et al. (2002) estimate global smoking prevalence across various groups and find significant differences (also see European Commission 2006). This paper examines the determinants of smoking prevalence across various socioeconomic groups in the USA. Whereas our understanding of the smoking behavior of the adult population is quite good, especially for the USA and for a few other developed nations, attention to population subgroups has been limited (see Cameron 1998; CDC 2000; Chaloupka and Warner 2000 and Goel and Nelson 2006 for surveys of the literature). From a public policy perspective, an understanding of different smoking patterns and determinants of behavior across different groups would enable formulation of specific policies to be effective in each case. On the other hand, an absence of significant differences across groups would imply that uniform smoking-control policies may be implemented. Due to the habit-forming nature of tobacco products, a handle on prevalence is necessary for the design of early intervention programs. Currently, the effectiveness of smoking-control policies across nations and groups varies considerably (see Chaloupka and Saffer 1988 and Wakefield et al. 1992; also Goel and Nelson 2006 for a review). Four categories of socioeconomic groups considered in this paper deal with age, literacy, income and ethnicity. These groups can affect smoking participation in unique ways. For instance, in 2002 the average smoking prevalence in the US population without a high school education was 32.95%, while prevalence went down to nearly half that (17.49%) in population with more than high school education (Table 1). Age can have a bearing upon smoking due to the habit-forming nature of cigarettes. Thus, different smoking control policies may be suitable for youth and adults. A more educated population might be better informed about the potential risks of smoking and therefore be less likely to smoke. On the other hand, greater literacy could give a false sense of confidence to some in terms of their ability to deal with adverse health effects of smoking. In other words, some educated smokers might be more belligerent in their attitudes towards smoking. Further higher incomes negate the effects of high cigarette taxes and render price-based smoking control policies less effective.Footnote 1 Finally, the race or ethnicity of a person might play a significant role in smoking participation. Some cultures frown upon smoking, while others view smoking less unfavorably. Consideration of all these aspects would educate us as to which influences are more powerful across different socioeconomic groups. Smoking-control policies may then be better tailored to be more effective.
",5
32,3,Journal of Economics and Finance,25 January 2008,https://link.springer.com/article/10.1007/s12197-007-9024-0,The J-Curve: Evidence from commodity trade between Canada and the U.S.,July 2008,Mohsen Bahmani-Oskooee,Marzieh Bolhasani,,Male,Female,Unknown,Mix,,
32,3,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9011-5,The retention of CEOs that make poor acquisitions,July 2008,Carolyn Carroll,John M. Griffith,,Female,Male,Unknown,Mix,,
32,3,Journal of Economics and Finance,23 January 2008,https://link.springer.com/article/10.1007/s12197-007-9023-1,Applying intervention analysis to financial performance data: The case of US airlines and September 11th,July 2008,Vitaly S. Guzhva,,,Male,Unknown,Unknown,Male,"The tragic events of September 11, 2001 provide researchers with the natural experiment settings to study the effect of an unanticipated large-scale catastrophic event on the US economy and society. US airlines have been placed in the most vulnerable position by this devastating terrorist act. Immediately after the attack the Federal Aviation Administration shut down the National Airspace System and all air traffic in the US remained grounded for several days. US airlines have experienced substantial losses after the attack as a result of flight restrictions, enhanced security procedures and the abrupt decline in passenger traffic. Even though $5 billion in cash compensation and up to an additional $10 billion in loan guarantees were provided to the airlines by the federal government, US airlines posted total losses of $17.7 billion in 2001 and 2002.Footnote 1
 I assess the impact of the September 11 terrorist attack on the performance of US airlines using intervention analysis first suggested by Box and Tiao (1965, 1975) and further developed by Larcker et al. (1980), Enders et al. (1992) and others. Intervention analysis has advantages over the standard event study method first introduced by Ball and Brown (1968) and Fama et al. (1969). It allows the observed autocorrelation in the model residuals to be removed thus providing improved estimates for reliable statistical testing, and requires considering each time series on a case-by-case basis instead of forcing one model on all series when examining events that simultaneously affect several firms (Larcker et al. 1980). Also, intervention analysis provides an impulse response function to study the transitional effects of an event. Guzhva and Pagiavlas (2004) utilize vector autoregression (VAR) model to separate the effect of the attack on US airline performance from the downturn in general economic conditions. They confirm the US government assessment that US airlines lost about $5 billion due to the attack in September–December 2001. While VAR is very useful in decomposing the effect of the attack from macroeconomic influences, it does not provide the long-run effect, which can be estimated through intervention analysis as the change in the long-run mean of a series of financial data. I examine monthly revenue passenger miles (RPM) series to objectively assess the effect of the terrorist attack on the financial performance of US airlines. RPM are considered to be the industry standard for the airlines’ financial performance evaluation and have an advantage over typical accounting and financial statements as they provide unbiased performance information and are not influenced by creative accounting techniques. I perform intervention analysis with the aggregate industry data and with individual monthly RPM series for US majorFootnote 2 and regional airlines to detect if the terrorist act resulted in different effects for individual air carriers. As expected, the intervention analysis of the industry data reveals a statistically and economically significant effect of the attack on the performance of US airlines. The magnitude of the effect, however, is much smaller than losses reported by the US airline industry. The initial effect of the attack is found to be around $1.5 billion a month, while the long-run effect is estimated to be $5.13 billion. The magnitude of the initial effect confirms Guzhva and Pagiavlas (2004) findings and supports the federal government decision to provide a $5 billion cash compensation to the airlines. However, the long-run effect is much smaller than reported losses, suggesting that not all of the losses can be explained by the attack of September 11, 2001. Probably, the airlines’ failure to profitably operate in times of increased competition and slow economic recovery can be accountable for the rest of the US airline industry losses after September 11, 2001. The analysis at the individual air carrier level shows that not all of the US airlines were significantly hurt by the terrorist attack. Two out of seven major airlines and eight out of ten regional airlines managed to minimize the impact. Moreover, three regional airlines have demonstrated significant improvements in performance after the September 11 attack. The comparison of the results of the intervention analysis with the market reaction reported by Carter and Simkins (2004) reveals that investors were fairly rational with pricing major airline stocks on September 17, 2001, but the market was less accurate with many of the regional airline stocks.",10
32,3,Journal of Economics and Finance,14 December 2007,https://link.springer.com/article/10.1007/s12197-007-9019-x,Short-cuts in issuance decisions and subsequent small firm performance,July 2008,Mike Cudd,Marcelo Eduardo,Lloyd Roberts,Male,Male,Male,Male,"Accurate analysis of complicated decisions requires resources that many very small firms may lack. Consequently, the use of decision heuristics (i.e., short-cuts) may be especially appealing to this segment of the business community. There appears to be substantial evidence supporting the existence of herding behavior in several areas of business activity. For example, international bank lenders tend to follow the pattern of ignoring country analysis and simply copying the actions of other international banks (Gwynne 1986). In the area of mergers and acquisitions, there is evidence of merger waves (Auster and Sirower 2002), although the shareholders of the acquiring firms do not appear to benefit from the mergers (Roll 1986). The use of heuristics in equity valuation is well-documented and may be attributed to the inability of the investor to understand probability (Odean 2007). More pertinent to the current study, equity issues appear to occur in surges driven by past market and IPO returns (Ibbotson et al. 1994; Lowry and Schwert 2002). In addition, the tendency to mimic competitors in security repurchases appears to be more concentrated among small firms (Cudd et al. 2006), so the presence of mimicking patterns in security issuance decisions among the same group would not be surprising. Within the small business strategy area, the study of decision heuristics is receiving increased attention. This segment of financial research focuses on behavior or patterns in financial decision making not explained by traditional wealth-maximizing financial theory (Schleifer and Summers 1990; Barberis and Thaler 2002). Inclusive to this area of study is herding behavior, also known as decision cascades, in which managers exhibit a tendency to mimic the actions of their peers.",1
32,3,Journal of Economics and Finance,29 November 2007,https://link.springer.com/article/10.1007/s12197-007-9018-y,Noise trading and autocorrelation interactions in the foreign exchange market: Evidence from developed and emerging economies,July 2008,Nikiforos T. Laopodis,,,Male,Unknown,Unknown,Male,"The short-run behavior of financial data, apart from exhibiting leptokurtosis and conditional heteroskedasticity, also exhibits invariable and positive autocorrelation at high frequencies. The latter may stem from biases produced from nonsynchronous trading, transaction costs, time-varying risk premia and feedback trading. The empirical evidence on these biases has been varied. For instance, Lo and MacKinlay (1988) and Cutler et al. (1991) accept the microstructure biases generated by nonsynchronous trading, while Atchison et al. (1987) reject the notion. Mech (1993) and Ogden (1997) support trading costs, while Boudoukh et al. (1994) suggest that the peculiar non-trading patterns may help explain the observed autocorrelation levels. Finally, Fama and French (1988) and Conrad and Kaul (1988) assert that this positive autocorrelation at high frequencies is caused by time-varying short-term expected returns or risk premia. More recent evidence, however, hints that the autocorrelation patterns, at least in stock returns, might be more complex than originally thought. On one hand, LeBaron (1992) documents significant first-moment dependencies in U.S. stock returns which imply that first-order autocorrelations of stock price changes are higher during tranquil periods and lower during volatile periods, and on the other hand, Campbell et al. (1993) find that the trading volume and stock return autocorrelation are inversely related meaning that during high-volume days autocorrelations become negative. The last relationship implies that rational, risk-averse market participants accommodate buying and selling pressures from (relatively) uniformed investors or noise traders. This group of investors is made of agents who sell and buy assets based on short-run technical conditions rather than on long-run fundamentals. Specifically, regarding the foreign exchange market, Allen and Taylor (1990) indicate that most traders consider trend patterns at least as relevant as the market fundamentals in the determination of exchange-rate expectations in the short run. Similarly, Frankel and Froot (1987) find evidence of extrapolative expectations and credit it to the use of chartism by professional traders. Vitale (2000) finds that noise trading in the foreign exchange market may be used to exploit expectations and exchange rates in order to achieve an informational advantage and ultimately a profit opportunity. Finally, Laopodis (2005) reports that evidence of feedback trading and autocorrelation behavior is present in the foreign exchange market but not in the cases of Euro-using countries. He also finds evidence of volatility persistence in several exchange rates translating it into inefficiency in those markets. The concern about positive feedback trading is that it makes financial asset prices overreact to new information or to changes in the fundamentals and these overreactions can be considered desirable or undesirable. For instance, investors with positive feedback strategies can be regarded as destabilizers (or noise traders) because their sales contribute to the fall of the market while their purchases lead to further market advances. In other words, positive feedback trading exists when traders buy/sell after an appreciation/depreciation of the currency. As a result, if (heavy) trading by such investors takes place, for instance, in an emerging economy it may be possible to destabilize its currency, and the benefits from the liberalization of these economy’s markets can be seriously impaired. Further, in many instances such strategies generate volatility of returns and create bubbles which lead to market crashes when they burst. However, positive feedback trading does not necessarily have to be destabilizing when the trades of investors are related to changes in risk premia or permanent price changes, that is, changes in the fundamentals. Financial market liberalizations of emerging economies usually attract many and varied investors which result in currency appreciations and more inflows of capital. Finally, such trading represents an important aspect of the functioning of the global financial markets since it reduces the risk of market crashes and eases the flow of transactions among participants. By contrast, negative feedback trading occurs when traders buy or sell following a currency appreciation or depreciation. In this context, such trading tends to emerge from the traders’ efforts to realize profit as the exchange rate changes thereby driving the exchange rate’s value away from its (assumed) fundamental (or long-run) value. These situations can be plausibly assumed since emerging markets have fully opened up their markets to foreign investors and, additionally, their returns have become much more closely correlated with the returns of developed economies due to influences by higher foreign investment and global factors (as opposed to local factors). Therefore, in view of the above, the main purpose of this paper is to empirically determine whether noise trading results in stabilization or destabilization in the foreign exchange market and whether it is a distinctive characteristic of an emerging economy or it is common to both developed and emerging economies. Another important issue that this paper seeks to address is the possibility of asymmetric behavior in feedback trading. Specifically, under presence of asymmetry, we ask whether positive feedback trading is more intense during exchange-rate appreciations than during exchange-rate depreciations. In other words, do informed traders have an informational advantage over the noise traders or is it the other way around? Absence of asymmetric behavior in such trading strategies connotes that feedback trading is mainly conducted by informed (smart money) traders and/or by central banks when attempting to maintain a currency’s value. Evidence on these issues is mixed for the equity (e.g., Sentana and Wadhwani 1992) and foreign exchange markets (e.g., Frankel and Froot 1987; Aguirre and Saidi 1999; Vitale 2000). The presence of feedback trading along with asymmetry can also be regarded as a measure of a currency’s (or a country’s) market credibility. Positive feedback (or noise) traders expect a currency to further rise (or decline) and thus they buy (or sell) the currency only when it appreciates but not when it depreciates. This suggests that these traders believe in the currency’s sustainability when it has appreciated but not when it has depreciated implying thus asymmetric behavior in favor of exchange-rate changes. The last question this paper deals with is the impact of feedback trading on the autocorrelation structure of the returns. Put differently, is there high predictability in the returns from the actions of feedback traders since such actions do not necessarily produce exploitable profit opportunities for rational traders? The actions of positive feedback traders will induce negative autocorrelation, while those by negative feedback traders will result in positive return autocorrelation. If past depreciations or appreciations affect these traders’ future demand, then feedback traders will persistently want to create long and short positions. A related question that will be addressed is the relative dominance of one group of feedback traders over the other in inducing negative or positive autocorrelation. While the empirical financial literature is abundant with evidence of the impact of feedback traders on autocorrelation for the stock market, little research has been done for the foreign exchange market and this research happens to be mixed. In sum, all these questions will be addressed in this paper via the use of a positive feedback trading model augmented with a generalized autoregressive conditional heteroskedasticity (GARCH) specification for the variance of the foreign exchange returns. The paper derives similar results as the paper by Laopodis (2005) who presented further evidence on the feedback trading of several emerging and developed economies. The model will be applied to 17 countries, ten emerging and seven developed, for the period 1990–2003. The insights from this paper may be helpful to investors and policymakers because of the impact that feedback trading can have on a currency’s sustainability (and credibility), volatility, risk exposure and foreign exchange policy. These issues are more relevant to emerging economies in view of their efforts to liberalize their financial markets and make them more integrated with the international financial markets. Besides, given that integrated markets increase the volatility of currency markets and feedback trading, additionally, may exacerbate this volatility, market participants can be more sensitive to these currencies compared to other currencies. This increased sensitivity, in turn, may affect a country’s foreign exchange policy and change the risk/return characteristics of those currencies. The remainder of the paper obeys the following order. The next section contains the theoretical model considerations and then derives some testable hypotheses. It also includes the data sources and construction and presents some preliminary statistical investigation. The third section presents the main empirical findings and discusses them at length, while the last section summarizes and concludes the study with some policy implications.",3
32,3,Journal of Economics and Finance,07 November 2007,https://link.springer.com/article/10.1007/s12197-007-9001-7,The long-run performance of diversifying firms,July 2008,David C. Hyland,,,Male,Unknown,Unknown,Male,"The diversification literature presents a puzzle. While diversified firms trade at a discount (Lang and Stulz 1994; Berger and Ofek 1995; Servaes 1996), firms that announce diversifying events typically have non-negative announcement returns when they first diversify (e.g., Jensen and Ruback 1983; Bradley et al. 1988; Chevalier 1999). In addition several recent studies question whether diversification is a value destroying event.Footnote 1 It is possible that firms have negative performance following diversifying events which would help to explain the diversification discount. We test the hypothesis that diversified firms trade at a discount due to negative long-run abnormal performance following diversification. Various arguments have been offered to explain why diversification may affect firm value. Lewellen (1971) argues that diversification may increase firm value, suggesting that conglomerates have greater debt capacity due to a portfolio effect. Others argue that informational asymmetries in capital markets may induce firms to develop internal capital markets through diversification. Amihud and Lev (1981) suggest that managers may diversify in order to protect the value of their human capital, and Jensen (1986) argues that firms diversify in order to protect the private interests of managers. More recently, financial economists have attempted to explain corporate refocusing by arguing that diversification may reduce firm value. Subrahmanyam and Titman (1999) suggest that the movement toward greater focus has been motivated by a decline in the benefit of maintaining an internal capital market. Shleifer and Vishny (1991) posit that a relaxation of antitrust policies during the 1980’s allows firms to operate on a larger scale in a single business line, thereby favoring a movement toward greater focus. Rajan et al. (2000) find that diversified firms misallocate investment funds toward less efficient divisions. Other researchers posit that a reduction in principal-agent conflicts within firms may foster a return to greater corporate focus. Given the various and sometimes contradictory explanations regarding the effect of diversification on firm value, it is not surprising that we have observed large scale movements toward greater diversification at certain times and towards greater focus at other times over the past three decades. For example, a large shift from diversification to focus was documented in the academic literature as early as 1990.Footnote 2 Morck et al. (1990) report that unrelated acquisitions were associated with negative announcement period abnormal returns during the 1980’s. Empirical research has failed to produce definitive results. Short-horizon event studies report mixed results. Studies examining the valuation impact of mergers suggest that diversification is not a value-destroying event. Yet studies of corporate refocusing activity and studies imputing multi-segment firm value from single-segment proxies suggest that diversification is indeed value destroying. Seemingly contradictory results are even found within a given sample. For example, Hyland and Diltz (2002) report statistically significant positive two-day diversification announcement abnormal returns, yet probit results from their sample imply that diversification is an agency-driven process. Because there is still no consensus in the literature regarding the valuation consequences of corporate diversification, we believe that it is important to continue to carefully evaluate the performance of diversifying firms using a variety of samples and empirical methodologies. Accordingly, we examine the monthly returns from a sample of firms over 36 and 60 month intervals following a diversifying event. In addition to full sample results, we examine sub-samples based upon subsequent control events. This allows us to examine performance based on subsequent events and also allows us to get a sample of firms that diversify and remain diversified. Based upon our analysis of ex post monthly returns, we find evidence that firm size has an impact on the value long-run impact of diversification. We find that large firms have positive abnormal returns, indicating that diversification is value enhancing for them. We find that small firms have negative abnormal returns, indicating that diversification is value destroying for them. The paper is organized as follows. Section 2 describes relevant literature. Section 3 describes our sample. Section 4 describes the methods employed in the paper. Our results and conclusions appear in Sections 5 and 6, respectively.",4
32,4,Journal of Economics and Finance,30 April 2008,https://link.springer.com/article/10.1007/s12197-008-9030-x,How to value a life,October 2008,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
32,4,Journal of Economics and Finance,23 July 2008,https://link.springer.com/article/10.1007/s12197-008-9049-z,Introduction to the special issue in honor of Richard Abel Musgrave,October 2008,Attiat F. Ott,Richard J. Cebula,,Unknown,Male,Unknown,Male,,1
32,4,Journal of Economics and Finance,17 June 2008,https://link.springer.com/article/10.1007/s12197-008-9043-5,A tribute to Richard Abel Musgrave,October 2008,Attiat F. Ott,Robert M. Solow,Paul A. Samuelson,Unknown,Male,Male,Male,,2
32,4,Journal of Economics and Finance,30 July 2008,https://link.springer.com/article/10.1007/s12197-008-9044-4,Public finance and three branch model,October 2008,Richard A. Musgrave,,,Male,Unknown,Unknown,Male,"My fascination with public finance dates back to 1930, my first year of university study at Munich. This was followed by 2 years at Heidelberg, where, to my good fortune, Alfred Weber’s Institut fur Sozialwissenschaft was engaged in a lively debate over how the field of public finance should be structured and how its role should be defined. On one side, there was the tradition of Wagner and Stein, leaders of Germany’s Historical School in the 1880s and 1890s, with central focus on the role of the state as chooser and provider of public goods—a framework distinct from that of market economics. On the other hand, there was the Austrian School of that period, linking the efficient provision of public goods to consumer preference. A first step was thus taken to join public and market economics in a common framework. But as Wicksell (1886) soon, noted, this left a major gap. Given the peculiar nature of public goods, consumers would not reveal their preferences by bidding in the market, so that government is needed to overcome market failure. Wicksell suggested that tax-expenditure voting be used to secure preference revelation, analogous to bidding for private goods in the market. The debate was further enriched with the emergence of fiscal sociology, offered from both a Marxist and Schumpeterian perspective. The romantic view of the state, dating back to early 1800s also reappeared, foreboding the coming of Nazi ideology. When coming to the United States in 1933 and beginning graduate work at Harvard, I thus brought with me a rich legacy of continental perspectives, a body of thought largely unknown to English language authors. To be sure, their own classics from Adam Smith and John Stuart Mill on were also aware of the nature of public goods which called for public provision and finance. But there was less concern with the “nature” of the public good than I had experienced in the continental tradition. With Ricardo, the focus of British fiscal analysis turned to tax incidence and its interference with the efficiency of the market. Pigou (1928) returned to a broader perspective, offering a vision of public finance based on welfare economics. As sketched in his introductory chapter, all economic activity, whether public or private, should be designed to maximize welfare. Public expenditures should be extended to the point of equating the marginal benefit derived from public and from private outlays, thereby maximizing total social welfare. By the same token, as developed in the later chapters of Pigou (1928), taxation should be distributed so as to equate marginal sacrifice, and hence minimize total sacrifice. The individualistic and utilitarian framework of the Austrians was thus broadened into a comprehensive analysis of welfare maximization. Wicksell’s concern with preference was set aside and replaced by postulating an omniscient referee to whom consumer preferences are known. In the 1930s, a new dimension was added to public finance. Based on the Keynesian model of income determination with its focus on aggregate demand, fiscal policy, by setting the level of expenditure, revenue and budgetary balance, emerged as the key instrument of anti-depression policy. Previously, concerned with the microeconomics of efficient resource use in a full employment economy, public finance became a major tool of macro policy. In that decade, the traditional focus of public finance also gained from the search for an objectively-based social welfare function, a concept which would prove essential to setting the optimal mix of private and public goods, and to dealing with fiscal issues of distribution. Next, attention returned to the theory of public goods. Though an old theme, Samuelson’s rigorous analysis of public goods in a general equilibrium setting (Samuelson 1954) captured the attention of a wide range of theorists, and soon became the center of fiscal theory. Wicksell’s concern with how to secure preference revelation was noted but set aside as unmanageable by economic analysis. Implementation of budget choice was again left to an omniscient referee. Uneasy with that assumption, cost-benefit analysis emerged as a tool for efficient budget planning and public choice emerged as new field of analysis. A new perspective soon followed. Where the traditional spirit of welfare-economic-based fiscal theory had postulated an efficient manager wishing to correct market failure, attention now turned to public officials seeking to serve their self-interest. A presumption of excess budgets was followed, and what traditionally has been a benevolent (if complex and imperfect) view of government was replaced by a critical approach. Much may be added regarding further additions to fiscal theory which have altered and broadened its perspective. These include fiscal federalism, optimal taxation, equity in distribution, a growth-oriented model of macro theory and the public sector in an international context. Their impact on fiscal theory is still with us and need not be recounted here.",6
32,4,Journal of Economics and Finance,10 June 2008,https://link.springer.com/article/10.1007/s12197-008-9046-2,Comments on two Musgravian concepts,October 2008,Peggy B. Musgrave,,,Female,Unknown,Unknown,Female,"The concept of the three-branch model of the government budget was first introduced in Musgrave’s 1959 “Theory of Public Finance”Footnote 1. It is based on three propositions: first, that the budget, together with government regulations, represent the place of the public sector in the total economy; second, that the economic functions of government as implemented by the budgetary items of expenditures, taxes and regulationsFootnote 2, may be classified as allocation, distribution and stabilization; and thirdly, that by normative standards the budget should be arranged by assigning these functions to three separate branches. This concept of the budget as fulfilling the three fiscal functions of government serves to rationalize and categorize government economic operations and many students of introductory public finance have found this tripartite division a useful way of thinking about the economics of the public sector. However it offers a normative division of the budget against which the items of a real-world budget may be judged. Real-world practice is not as tidy as this concept requires, and each tax and expenditure item may serve (or disserve) more than one function, not to mention the consequences that are unintended. I have had to explain this to students from around the world who email to ask me in what branch particular items in their country’s budgets should be placed. The concept was introduced at a time when little attention was given to the impact on the U.S. budget of U.S. economic relations with the rest of the world. Little allowance was made for the foreign sector in the three-branch model. Our undergraduate textbook “The Theory and Practice of Public Finance” 5th editionFootnote 3 made only a passing reference to the inclusion of the international sector but without giving specifics. In fact official statistics have yet to fully account for the relationship between the domestic and foreign sectors in aggregate measures of the economy. Until recent years the measure of the entire economy most commonly referred to was gross national product (GNP), a misnomer since it allowed for the production of U.S.-owned investments abroad only to the extent of the returned income while including the product of foreign-owned investments within the U.S. and excluding only their remitted income. Now the statistic GNP is rarely used having been supplanted by gross domestic product (GDP), a measure of all goods and services produced within the geographical bounds of the U.S. (see Table 1, lines 1 and 2). As the acronym implies, GDP excludes entirely the product of U.S. investments abroad, since it equals GNP less income receipts from the rest of the world and plus income payments to the rest of the world. While GDP serves a useful purpose as a measure of all productive activity taking place within the geographic bounds of the United States, there remains no good measure of the aggregate production of goods and services attributable to U.S. owned assets in both the U.S. and abroad but excluding production by foreign investors in the U.S., in other words, a more meaningful measure of U.S. national product.
 Table 1 presents a few indicators of the economic involvement of the U.S. with the rest of the world, and how these connections have been expanding. Between 1980 and 2006, exports have grown from 11.6 to 12.5% of net domestic product (NDP) while imports have grown from 12.4 to 19.0% of NDP. The flow of U.S. direct investment abroad has exhibited even stronger growth both in absolute and relative terms. As a percent of gross private domestic investment in the U.S. it has increased from 10.8% in 1980 to 14.5% in 2006, while foreign direct investment in the U.S. in 2006 nearly matched the direct investment outflow at 14.3% of gross private domestic investment in the U.S., a ratio which rose from 3.5% in 1980. The position of U.S.-owned fixed assets abroad as a percentage of the stock of private fixed non-residential investment in the U.S., has risen threefold between 1980 and 2006 and that 2006 ratio is matched by that for the position of foreign direct investment in the U.S. The combination of accelerating net imports and net outgoing investment has left a rising deficit in the balance of payments on current and direct investment account which is now offset by a rising net inflow of foreign financial claims. This increasing financial indebtedness to the rest of the world also signals the rising globalization of the U.S. economy. This increasing participation in the international economy may be expected to have an impact on the fiscal functions of government and in turn on the federal budget. The balance of payments includes on government account such items as grants to developing counties, taxes on dividends and interest accruing to foreigners or those earned on U.S. owned assets abroad, expenditures made to foreign countries for services provided the government, citizenship fees, tariffs, official reserves and reserve assets and the list goes on. All these balance of payments items on government account also appear in the budget but the budget also contains items such as certain taxes and duties which have an indirect influence on U.S.–foreign economic relations though they may not appear in the balance of payments. This increasing globalization of the world’s economies, and with it the multiple economic policy issues which globalization arouses, it seems clear that the foreign sector should be explicitly incorporated into the three-branch model. The next question is, how might the model be adjusted to allow for what might be termed the “foreign sector”? One approach might divide each branch into domestic and foreign sub-branches as follows:
 This approach implies a close policy relationship between domestic and foreign sub-branches which are together combined in the three main branches. Many years ago as an idealistic Cambridge undergraduate I took part in a debate for and against World Federation in which I spoke in support. Now taking a more realistic view that states wish to retain their sovereignty, one would expect there to be an interest in a primary separation of the budget into their domestic and foreign branches. With that in mind, a three-branch budget might be shown as the following:
 However since the “3-branch budget” is a normative concept and teaching tool, I leave it to the reader to make the choice.",4
32,4,Journal of Economics and Finance,18 July 2008,https://link.springer.com/article/10.1007/s12197-008-9055-1,"Musgrave’s vision of the public sector: the complex relationship between individual, society and state in public good theory",October 2008,Karl E. Case,,,Male,Unknown,Unknown,Male,"What many find attractive about economics as a discipline is that it has a generally accepted structure for dealing with the issues. There is a dominant paradigm which most of us accept. It is what guides our work whether we are aware of it or not. Richard Musgrave was a major architect of that paradigm. His work attacked head on the most important theoretical questions in the discipline: What is our purpose? What should we do to make the system function better? What is the proper role of government? His career was a reflection of his struggle to answer big questions and to apply the answers. Economics texts all begin with a definition of the discipline. Case and Fair (2009) settle on “economics is the study of how individuals and societies choose to use the resources that nature and previous generations have provided.” Most texts make a further distinction, that between positive and normative economics. While a positive or descriptive economics can be at least conceptually value free, normative economics requires a definition of the criteria that determine what constitutes a good outcome. “How does the system operate?” versus “how should the system operate?”",7
32,4,Journal of Economics and Finance,10 July 2008,https://link.springer.com/article/10.1007/s12197-008-9047-1,The public economics of self control,October 2008,Sang Hoo Bae,Attiat F. Ott,,,Unknown,Unknown,Mix,,
32,4,Journal of Economics and Finance,18 June 2008,https://link.springer.com/article/10.1007/s12197-008-9040-8,The relative decline of a Musgrave ‘Merit Good:’ the case of public support of flagship public universities,October 2008,James V. Koch,,,Male,Unknown,Unknown,Male,"There are two reasons most commonly cited why under supply and under consumption occur for merit goods. First, external economies exist that are not taken into account by individual decision makers. Social benefits exceed private benefits at the margin in this version of a merit good. For example, the social benefits attached to an individual receiving a diphtheria shot exceed the private benefits to him. One individual is helped by another individual’s diphtheria shot even if he isn’t aware of that fact. Left to their own devices, individuals would under consume diphtheria shots. Hence, diphtheria shots are a merit good that in this case many would argue should be supplied as a public good. The second major reason speaks to Odysseus’ situation. What was good for Odysseus in the short-run was not good for him in the long-run because he would be myopic at a critical decision-making point in time. Strotz’s classic 1956 article, “Myopia and Inconsistency in Dynamic Utility Maximization” dealt with such circumstances in detail and presaged Musgrave’s merit good contribution, as did much earlier work by Pigou (1920). Strotz asked, as Musgrave was soon to do, “To the extent that consumer sovereignty is one of our values, ought we to allow people to behave imprudently?” (Strotz 1956, 179). Odysseus understood beforehand that when he actually confronted the seductive Sirens, his behavior would be inconsistent with his long-run welfare. Faced with this reality, he could either pre-commit to actions that he could not later change (which he did), or, following Strotz, he could have modified his original plan to take account of his future disobedience (which he also did). In general, these two options are available to any individual who believes she may suffer from myopia at critical decision-making times. But, what if Odysseus did not recognize his likely future myopia? Then, argued Pigou, state intervention is required. Musgrave has argued in a similar vein. Strotz was not so sanguine about this approach—“The individual probably can do as good a job as the state or any other agency in determining allocations for himself as of future dates, provided the future dates include none which are proximate.”(179). However, it is not clear that Strotz believed that pre-commitment always will be a viable option when the individual decision-maker has no sense of personal myopia about a forthcoming decision-making process.",5
32,4,Journal of Economics and Finance,24 April 2008,https://link.springer.com/article/10.1007/s12197-008-9031-9,State-imposed solutions to negative externalities: employment impact of pollution abatement policy,October 2008,Richard J. Cebula,Jay Carmichael,Holly Meads,Male,Male,Female,Mix,,
32,4,Journal of Economics and Finance,11 June 2008,https://link.springer.com/article/10.1007/s12197-008-9045-3,“A reappraisal of social security financing”—revisited,October 2008,Alicia H. Munnell,,,Female,Unknown,Unknown,Female,"Social Security’s financing problems in the first decade of the 21st century are both similar to and different from what they were when Musgrave wrote his article. At that time, the stagflation of the 1970s, a technical fault in the design of the program, and a recession created an immediate cash flow crisis as well as a long-term shortfall. This time, there is no immediate danger of being unable to pay benefits, but the system faces a long-term deficit of roughly the same magnitude as it did in the early 1980s. The following sections contrast the short-term and long-term financing outlook in the early 1980s and today. Each year the Trustees of the Social Security system provide both short-term and 75-year projections of the system’s finances under three sets of assumptions—high cost, low cost, and intermediate. The short-run financial health of the system reflects its ability to pay scheduled benefits in full and is generally measured in terms of the trust fund ratio. This ratio is defined as assets at the beginning of the year as a percentage of costs during the year. Even if the system were operating on a complete pay-as-you-go basis, it would need to hold some assets to serve as a buffer in the event that, say, an economic recession caused revenues suddenly to decline and fall short of outlays. If the system should experience recurring shortfalls, the trust fund would allow time for the enactment and implementation of legislation to restore balance. The short-range definition of financial adequacy involves the following test: (1) if the trust fund ratio is 100% at the beginning of the period, it must be projected to remain at least at that level throughout the 10-year projection period; or (2) if it is below 100% at the beginning, it must be projected to rise to 100% within 5 years and stay at least at that level for the remainder of the 10-year period. While having such a precise test is helpful, it is not necessary for discerning the differences between the situation in the early 1980s and the situation today. In 1982, the trust fund ratio was 15% and projected to fall below zero quickly (see Fig. 1). In 2008, the trust fund ratio is 392% and projected to increase over the next 10 years. The 1982 projections meant that the system was not likely to meet its benefit commitments within a couple of years, whereas today ample reserves ensure full payments for decades. These different short-run situations create very different political dynamics. An imminent cash flow crisis forces Congressional action, while increasing short-run resources enable Congress to postpone remedial action. That is unfortunate, because the long-run deficit facing the system today is very similar to that in 1982, and it needs attention.
 Estimated OASDI trust fund ratio 1980–1986 under three alternative sets of assumptions. Source: U.S. Social Security Administration (1982) The long-run outlook for Social Security extends over a 75-year period. In addition to trust fund ratios, the Trustees present data on annual cash flows—the difference between the cost rate and the income rate—and summary measures, such as the actuarial balance. The cost for calculating the actuarial balance includes the cost of reaching a target trust fund of 100% of annual cost at the end of the period as well as the annual cost rates. Social Security is financed primarily on a pay-as-you-go basis, so—for any given level of benefits—costs closely reflect the demographics. In 1982, as today, the Trustees projected that, under the intermediate assumptions, the ratio of beneficiaries to workers was going to increase from about 31 to about 50 per hundred workers by 2060. An increase in the ratio of retirees to workers raises the cost of the program expressed as a percent of taxable payrolls. The rising cost rate as projected in the 2008 Trustees Report is shown in Fig. 2.
 Projected Social Security revenue and benefit rates, 1990–2085, (as a percent of taxable payroll). Source: U.S. Social Security Administration (2008a) As shown in Fig. 2, rising costs combined with a relatively stable income rate (produced by a stable payroll tax plus rising income from the income tax on benefits, a small portion of program revenues) generate a long-run deficit. The projected long-run deficit as of 2008 is 1.7% of taxable payrolls. The easiest way to interpret the 75-year deficit is in terms of the size of the tax increase required to restore solvency. That is, if the payroll tax rate were raised immediately by roughly 1.70 percentage points—0.85 percentage point each for the employee and the employer—the government would be able to pay the current package of benefits for everyone who reaches retirement age for the next 75 years. The 75-year deficit projected in 1982 was 1.8% of taxable payrolls, very close to the 2008 number. While the 75-year deficits in 1982 and 2008 are quite similar, the relationship between the cost rate and the tax rate differs in the two periods. For the last 25 years, the tax rate has exceeded the cost rate, and Social Security has been running cash flow surpluses. These surpluses are the result of reforms enacted in 1983, which are discussed below, and will continue until 2017. In 2017, the cost rate starts to exceed the income rate and Social Security will have to tap a portion of the interest on trust fund assets to cover benefits. And in 2027, taxes and interest will fall short of annual benefit payments, so the government will be required to draw down trust fund assets to meet benefit commitments. The trust funds will be exhausted in 2041 (see Fig. 3). Some describe the system as “bankrupt” after 2041, but that is incorrect. Tax revenues continue rolling in, so the system will still have enough revenue to pay 78% (75% by 2082) of currently legislated benefits.
 OASDI Trust Fund Ratio 1970–2041. Source: U.S. Social Security Administration (2008a) The similarity of the 75-year deficit in 1982 and 2008 is quite surprising, given the 1983 reforms. The imminent cash flow crisis and the long-term deficit projected in the early 1980s led to the formation of the National Commission on Social Security Reform, headed by Alan Greenspan. The Commission’s 1983 report presented a series of proposals, endorsed by either the whole Commission or a majority of its members, that were designed to restore solvency over the 75-year horizon (National Commission on Social Security Reform 1983). The key solvency reforms that Congress enacted in 1983 included:
 Increased funding
 Accelerate the introduction of scheduled future tax increases. Increase the payroll tax paid by the self-employed to equal the total employer–employee tax on wage-and-salary workers. Extend coverage to non-profit and new Federal government workers. Decreased benefits
 Cut future benefits by increasing the Full Retirement Age from 65, for those reaching age 62 prior to the year 2000, to age 67 for those who reach 62 in 2022 or after. Subject half the benefits of higher-income beneficiaries to income taxation, with the proceeds returned to the Social Security program.Footnote 1
 When Congress enacted the Greenspan Commission recommendations, the Trustees reported that the Social Security system was in balance for the next 75 years. Almost immediately, however, deficits began to appear, and they increased markedly in the early 1990s (see Fig. 4).
 Social Security’s 75-year deficit as a percent of taxable payrolls, 1983–2008. Source: U.S. Social Security Administration (2008a) Table 1 shows the reasons for these emerging deficits. The discrepancy that needs to be explained is 1.72% of taxable payrolls, since the 1983 Trustees Report projected a 75-year actuarial surplus of 0.02% and the 2008 Report projects a deficit of 1.70%. Leading the list is the impact of changing the valuation period. That is, the 1983 Report looked at the system’s finances over the period 1983–2057; the projection period for the 2008 Report is 2008–2082. Each time the valuation period moves out 1 year, it picks up a year with a large negative balance. That is the reason that policymakers now insist on looking beyond the 75-year projection period when considering ways to restore solvency.
 Persistent increases in disability rolls and the change in methods of analysis used by the actuaries also contributed to the increase in the deficit. Another contributor has been a worsening of economic assumptions—primarily a decline in assumed productivity growth. Partially offsetting the negative factors has been a reduction in the actuarial deficit due to changes in demographic assumptions—primarily higher mortality for women. In addition, Social Security changed the way in which it projects undocumented and temporary legal immigration. The new method resulted in a larger number of working-age immigrants—who pay payroll taxes—relative to the number of older immigrants, who will be eligible to receive Social Security retiree benefits. As noted above, Congress could eliminate the 75-year deficit by raising the payroll tax by 1.7 percentage points, split evenly between employers and employees. But a lasting fix would require additional changes. Solutions that focus just on the next 75 years typically involve the build up of Trust Fund assets in the near term and the sale of those assets to pay benefits in the out years. Since the trust funds have no further bonds to sell in the 76th year, the program is suddenly short of money. Lasting solvency would require either a pay-as-you-go system with substantially higher payroll tax rates/lower benefits or the build up of a trust fund larger than that required for 75-year solvency, the returns from which could cover the difference between program costs and revenues. Realistically, eliminating the 75-year shortfall should probably be viewed as the first step towards long-run solvency. But it is much harder to gather the political will today for even a 75-year fix than it was in 1982, because the system’s long-run deficit is not accompanied by an imminent cash flow crisis. The tendency is to delay. And the longer the delay, the bigger and more abrupt the required changes will be, and the longer American workers will feel insecure about the benefits they will receive from Social Security—the backbone of the American retirement income system.",
32,4,Journal of Economics and Finance,15 May 2008,https://link.springer.com/article/10.1007/s12197-008-9036-4,The behavioral justification for public pensions: a survey,October 2008,T. Scott Findley,Frank N. Caliendo,,Unknown,Male,Unknown,Male,"Unfunded social security programs often embody the largest public transfer in most developed economies, and such programs are generally justified on grounds that people save too little for retirement.Footnote 1 But why do individuals fail to save adequately for retirement, and why is government intervention needed as a remedy? Musgrave (1968) illustrates the behavioral justification for a social security program with three allegorical characters:  Now let Calvin and Homer, who are prudent, provide for their old age, while Jack lives it up and does not. Calvin and Homer, knowing that they will have to provide for the aged Jack, will call for a social insurance plan which will force Jack himself to make his own provision and thus relieve them of the burden. Note that the role of social insurance in this case is not technically an insurance function at all...The objective is not one of spreading risk but of forestalling the need for transfers from the prudent to the feckless. The essential function of social insurance, in this case, is compulsory saving. (26)  In addressing the compulsory nature of social security and the partial removal of one’s private decision to save, Musgrave and Musgrave (1984) furthermore state, “the compulsory approach may be viewed as a paternalistic decision by society to protect the imprudent against starvation in old age” (p.723). Individuals like Musgrave’s feckless Jack are forced to save via a public pension program in order to prevent old-age destitution and the demand for state welfare that would otherwise have to be financed by those who adequately save.Footnote 2
 In a similar style, Munnell (1977) states:  People’s failure to provide adequately for their retirement needs can often be attributed to a myopic view of the trade-off between present and future consumption...Social security can be seen as a way of forcing workers to set aside a certain amount of income for retirement consumption and ensuring that such provisions will be adequate, thus preventing the burden of support from falling on others. (7–8)  Kotlikoff et al. (1982) likewise cite myopia as the main justification for social security, and they state “while our Social Security and private pension systems perform a number of functions in the area of income redistribution and insurance, their primary function is to force individuals to save for their old age” (p.1056). Feldstein (1985) more emphatically states, “the principal rationale for such mandatory [social security] programs is that some individuals lack the foresight to save for their retirement years” (p.303). And lastly, Aaron (1999) states, “by forcing people to curtail current consumption when young and providing for pensions at a later age, social insurance ameliorates the adverse consequences of myopia” (p.55). There seems to be consensus that social security is primarily justified on grounds that people do not personally save appropriate amounts because they are shortsighted and they lack the ability to fully internalize during the working years their consumption needs for retirement.Footnote 3
 Some significant research questions logically follow in response to the justification that individuals are shortsighted and myopic. Namely, how myopic do people need to be in order to benefit from participation in a social security program? And what is the optimal level of social security benefits provided by an unfunded system when individuals are myopic? Despite the fact that social security benefits represent the largest public transfer in most developed economies, and despite the fact that the principal justification for such programs relies on a foundation that individuals are myopic and lack the foresight to adequately save for retirement, the existing literature in economics that appropriately addresses these questions is largely inconclusive with policy implications ranging from a massive increase in the size of the program to complete abolition of the program. The problem facing this literature is that the ultimate policy conclusion is often heavily driven by preference parameters that are not observable, and as Hansen and Heckman (1996) state “it is simply not true that there is a large shelf of [precise] micro estimates” (p.100) for these parameters. There is also very little consensus among economists about the range of preference parameters that are reasonable, and general equilibrium models provide the useful insight that preference parameters are difficult to identify separately since there is often a very large feasible parameter space that can hit key targets in the macro data (Caliendo and Gahramanov 2007; Feigenbaum 2008b). On the other hand, this literature is quite successful in carefully demonstrating just how myopic people need to be in order to benefit from participation in a pay-as-you-go social security program even though it is difficult to draw precise policy recommendations. The objective of the present paper is to survey the body of existing studies that assess the validity of the myopia justification for unfunded social security programs, and to examine issues for future work in this area. Additionally, this paper has been written in a basic, straightforward style that may prove useful for researchers and students who want to familiarize themselves with a modern version of the life-cycle/permanent-income model.",14
32,4,Journal of Economics and Finance,06 May 2008,https://link.springer.com/article/10.1007/s12197-008-9032-8,Musgrave’s “target saver” theory: Implications for macroeconomic stability and economic policy effectiveness”,October 2008,Richard J. Cebula,Shyam Menon,,Male,Male,Unknown,Male,"Among the many interesting and valuable contributions made to the field of Public Finance by Richard Musgrave, one of the more unique and intriguing is that of the “target saver” (Musgrave 1959, pp. 264–5). The target saver refers to a household (or individual) that saves in the present for a very specific goal, namely, to fund a target level of consumption in the future. In this framework, should the interest rate on current household saving increase, then the household can actually save less in the present (and thus spend more in the present) and still achieve its target level of future consumption. This line of reasoning leads to the logical deduction that the level of current household saving (S) for the target saver is actually inversely related to the current interest rate (i), i.e., S

i
 < 0, which is of course in direct contradiction to the “conventional wisdom,” which posits that household saving is an increasing function of the interest rate, i.e., S

i
 > 0 (cf. Metzler 1951). Alternatively stated, target saving behavior implies that current household consumption purchases of new goods and services (C) are an increasing function of the current interest rate: C

i
 > 0. Within a standard life-cycle model, a world with target savers would seem quite plausible. Consider, e.g., the case of a young household, saving for the down payment on a new home and/or for the college education of their offspring. Alternatively, consider the case of a middle-age household saving for its retirement. Moreover, empirical support for an inverse relationship between aggregate household saving and the rate of interest, or, equivalently, for a positive relationship between aggregate household consumption and the current interest rate, was obtained some years after Musgrave’s (1959) introduction of the target saver by Weber (1970, 1975). Within this context of theoretical plausibility and empirical reality, the present study investigates in theoretical terms the potential impact of the target saver in the aggregate for (a) macroeconomic stability on the one hand and (b) economic (fiscal and monetary) policy effectiveness on the other hand. The issue of macroeconomic stability under conditions of households in the aggregate on balance manifesting target-saver behavior within an IS-LM framework is considered in the next section of this study. The potential implications of the target saver in the aggregate for monetary policy effectiveness and for fiscal policy effectiveness are considered in the subsequent section of this study. Closing remarks are provided in the “Conclusion”.",
33,1,Journal of Economics and Finance,29 November 2007,https://link.springer.com/article/10.1007/s12197-007-9020-4,Do liquidity induced changes in aggregate dividends signal aggregate future earnings growth?,January 2009,Christi Wann,D. Michael Long,,Female,Unknown,Unknown,Female,"In a recent survey of 384 financial executives, Brav et al. (2004) confirm Lintner’s (1956) observation that managers only increase dividends when strong earnings are sustainable in the future. However, extant empirical literature provides limited evidence that dividend changes signal future earnings growth at the firm level. Nissim and Ziv (2001) find that dividend increases signal future abnormal profits. Conversely, Benartzi et al. (1997), and subsequently, Benartzi et al. (2003) find no evidence that dividend changes signal future earnings growth. The lack of consistent evidence between theory and corporate practice is puzzling. Although Miller and Modigliani (1961) argue that there is no optimal dividend policy at the firm level, this does not imply that there is no optimal dividend level at the macroeconomic level. Marsh and Merton (1987) find that aggregate dividends display systematic time series behavior, casting doubt on the ability of firm-specific dividend behavior to wholly explain the dividend puzzle. The relationship between aggregate dividends and aggregate earnings may actually be stronger than firm-level relationships if aggregation filters out firm-specific earnings information and signifies macroeconomic trends. This is further strengthened by Marsh and Merton’s (1987) suggestion that firms consider industry payout ratios when choosing a target payout ratio. Prior research has paid less attention to aggregate data than firm-level data. Therefore, we expand the current research by utilizing macroeconomic data provided by the Federal Reserve Statistical Releases. In addition, other research largely ignores the effect that underlying economic stimuli may have on aggregate changes in dividend payout policy and subsequent future earnings growth. For example, changes in aggregate cash balances (liquidity shocks) may help provide a context in understanding the relationship between changes in payout policy and changes in future earnings growth. In fact, we find that aggregate payout deviations from Lintner’s (1956) long-run target ratio following a liquidity shock signal aggregate future earnings growth. Thus, the purpose of this paper is to provide new evidence concerning the relationship between changes in dividends and future earnings. In this process, we make several empirical contributions. First, we extend prior research by investigating the role that a latent economic variable, such as increases in excess cash balances (liquidity shocks), may have in relating payout ratio to changes in future earnings growth. For example, we find increases in aggregate payout ratios, if induced by positive liquidity shocks, predict higher aggregate future earnings growth. Second, to the best of our knowledge, we are the first to use the macroeconomic data supplied by the Federal Reserve to reexamine the role that changes in aggregate dividend levels may have in signaling changes in aggregate future earnings. In addition, we present further evidence of Lintner’s (1956) “ratchet effect”. Simply stated, this effect suggests firms are reluctant to cut dividends, and will only increase dividends if supported by higher expected earnings. If true, then aggregate dividends need to grow when the current payout ratio is below the aggregate long-run target payout, and remain unchanged when payout is above the target. Consequently, a long-run target payout ratio is maintained if earnings grow when the current payout ratio is higher than target, and remain unchanged when payout ratios are lower than target. Lastly, we provide additional support for a long-run aggregate target payout ratio.",3
33,1,Journal of Economics and Finance,28 May 2008,https://link.springer.com/article/10.1007/s12197-008-9027-5,A multivariate unobserved component analysis of US housing market,January 2009,Mohamadou L. Fadiga,Yongsheng Wang,,Male,Unknown,Unknown,Male,"There is a great concern about the deterioration of the US housing market because of its potential impact on the US economy. This is particularly pronounced in areas such as part of California, Florida, Arizona, Washington DC, and Massachusetts, which experienced a strong housing boom in the last five years. The Office of Federal Housing Enterprise Oversight housing price index indicates that the US housing prices, on average, increased by 58% between 2001 and 2005. During the boom, high prices lead many to purchase homes requiring longer commuting time, including areas across regional boundaries where real estates were perceived more affordable. Stevenson (2004) and Oikarinen (2006) discuss how changes in property prices in one place spillover into cross-border and surrounding areas. The appreciation of property value spreads across regions thereby forcing the local housing markets to deviate from their original market fundamentals. Moreover, a large amount of “speculative” funds from speculators and average people such as retirees, who were looking for a second home, moved from one area to another. This added pressure to the housing markets and connected markets across regions in an unusual way and partly contributed to higher level of inventory and falling housing prices across regions. Clearly, there are major interactions between US regions, which may or may not be observable. The unobservable factors are embedded in the regional house price trends and cycles whose evolutions are shaped by economic phenomena such as local economic conditions and national monetary policies. Identifying these unobserved components is the main goal of this study. In that regard, we propose a multivariate structural time-series approach, which is a departure from the traditional approaches used in studies with similar focus. Those approaches generally analyze each region individually when applying a state-space approach or use systematic methods such as vector autoregressive (VAR) models, which provide no information with respect to the underlying unobserved components. While there is a well established hypothesis about the prevailing role of local factors as principal determinants of real estate market dynamics, this study argues that the behaviors of house prices are also shaped by broader economic phenomena stemming from national policies. The effects of these policies are embedded in the trend and cycle components of housing prices. These unobservable components exhibit some degree of communality between them not captured by modeling the regions individually or by using VAR models. House prices and housing market have been widely studied at the city, metropolitan statistical area (MSA), regional, and national level. Jones et al. (2004) and Clayton (1996) focus on markets within a city. Jones et al. (2004) investigates the role of submarkets in analyzing intra-city housing market dynamics in Glasgow, UK. The study highlights the relationship between the observed migration patterns within an urban area and the identified submarkets evidenced by high degrees of self-containment, indicating that movements in one submarket have limited impact on another. Clayton (1996) study the single-detached housing prices in the city of Vancouver, British Columbia, using a forward-looking rational expectation model and identified a transitory deviation of housing prices from market fundamentals. While the model does not capture the two real estate booms in Vancouver, it depicts the evolution of price in less volatile periods and illustrates the temporary deviation of housing prices from supply and demand fundamentals. Studies at the MSA level include Miller and Peng (2006) which explore house price volatility in the US MSA using generalized autoregressive conditional heteroskedasticity (GARCH) and Panel VAR models. The study finds time-varying volatilities in 17% of the 277 MSA studied. Jud and Winkler (2002) investigate house price appreciation in 130 US metropolitan areas using cross-sectional and fixed-effects panel models to show the role of location in price appreciation. The study showed that location, population growth, real increase in income, after-tax mortgage rate, and construction cost lead to price appreciation. Some studies used more aggregated data at the regional and/or national level. For example, Baffoe-Bonnie (1998) study regional and national housing market responses to shocks in macroeconomic aggregates based on impulse response functions derived from VAR model. While the study establishes that house prices respond to employment growth and national mortgage rates, it also recognizes the limitations of using aggregate economic variables to explain the fluctuations of real estate values. Guirguis et al. (2005) shows the merits of rolling GARCH and Kalman filter with an Autoregressive representation in forecast US national housing prices. This procedure accounts for the sub-sample parameter instability, which improves its forecasting accuracy. In general, the above studies investigate housing markets for different areas without consideration of common movements among them. The hidden linkage among regional markets could be a key element to help craft sound policy decisions. As for the procedure, some studies analyze the dynamic of price variability, but do not account for the underlying unobserved heterogeneity that shapes the evolution of housing prices. A multivariate unobserved component model circumvents these procedural limitations. It is based on a state-space model with the diffuse Kalman filtering algorithm, which can identify, filter, and estimate the unobserved components while assessing their shared features.",27
33,1,Journal of Economics and Finance,20 December 2007,https://link.springer.com/article/10.1007/s12197-007-9021-3,Agency problems in tracking stock and minority carve-out decisions: Explaining the discrepancy in short- and long-term performances,January 2009,Wei He,Tarun K. Mukherjee,Peihwang Wei,,Male,Unknown,Mix,,
33,1,Journal of Economics and Finance,16 July 2008,https://link.springer.com/article/10.1007/s12197-008-9053-3,The permanent income hypothesis in five major industrial countries: a multivariate trend-cycle decomposition test,January 2009,Hassan Shirvani,Barry Wilbratte,,Male,Male,Unknown,Male,"Since its inception, the permanent income hypothesis (Friedman 1957) has been the subject of extensive empirical research. Over time, the methods employed to test the hypothesis have evolved to incorporate the latest advances in data analysis and statistical techniques, such as the use of panel data and modern time series econometrics. With few exceptions, however, the general thrust of the empirical results, mostly undertaken in the context of individual countries, has been to reject the empirical validity of the hypothesis. In particular, and concentrating on more recent studies, Hall (1978), using aggregate time series data, fails to find empirical support for the joint hypothesis of permanent income and rational expectations, while Sargent (1978), using disaggregated panel data, reaches the opposite conclusion. However, Flavin (1981), upon making certain refinements to Sargent’s methodology, finds consumption to be excessively sensitive to current income, thus lending further support to Hall’s findings. These negative findings against the permanent income hypothesis have since been corroborated by other researchers, using different theoretical refinements and estimation methods (Campbell 1987; West 1988; Deaton 1988; Campbell and Deaton 1989; Campbell and Mankiw 1990; Ziliak 1998; Morley 2005). A key underpinning of the permanent income hypothesis is the assertion that both consumption and income can be conceptually divided into their unobservable and independent permanent and transitory components. In addition, the hypothesis asserts that the permanent components of consumption and income are statistically related, while the transitory components are unrelated or only marginally related. In this light, most of the negative evidence mentioned above can be interpreted as a finding of a significant relationship between the transitory components of consumption and income. Alternatively, as stated earlier, the unfavorable evidence can be interpreted as an indication that current consumption is too sensitive to current income to be explainable in terms of the permanent income hypothesis. Given the unfavorable empirical findings with respect to the permanent income hypothesis, other researchers have developed alternative models of consumption behavior. One such model is the buffer stock model, in which consumers engage in precautionary saving in anticipation of “bad events” such as unemployment, the result of which is that saving responds positively to positive growth of current income, in contrast to the implications of the permanent income hypothesis (Carroll et al. 1992, Carroll 1994). A closely related model is the liquidity constraint model, in which households wish to consume from permanent income but are constrained by inability to borrow, the consequence of which is that consumption tracks current income more closely than is implied by the permanent income hypothesis (Deaton 1991). Finally, Campbell and Mankiw (1989, 1990) offer a model of consumer behavior in which rule-of-thumb consumers always spend a fixed fraction of their current income. A review of the existing evidence on these alternative hypotheses, however, fails to indicate a consensus regarding their empirical validity, although the evidence is weighted toward rejection of the empirical implications of permanent income hypothesis (Campbell and Mankiw 1991; Carroll and Weil 1994; Attanasio and Weber 1995; Browning and Crossley 2001; Gourinchas and Parker 2002; Gross and Souleles 2002; Fuchs-Schundeln 2006). The purpose of this paper is to revisit the empirical validity of the permanent income hypothesis in the context of five major industrial countries through a method which has not previously been used to test the hypothesis and improves upon the previous methodologies. First, drawing on recent advances in time series analysis, we provide a new multivariate decomposition of consumption and income into their permanent and transitory components. This approach builds on the time series properties of the underlying data, as well as on the possible presence of common trends and cycles among the consumption and income data for the countries in the sample. By thus considering the potential interactions among consumption and income of a number of industrial countries, this approach offers a more efficient estimate of the unobservable components of consumption and income for each country, hence facilitating a more robust test of the permanent income hypothesis. Our test results, reported later in the paper, indicate that, for each country in the sample, the permanent components of consumption and income are indeed related, while the transitory components are unrelated, both findings being consistent with the predictions of the permanent income hypothesis. In light of the substantial body of evidence refuting the permanent income hypothesis, we realize that our findings that the transitory components of consumption and income are uncorrelated run counter to evidence in the literature that there is some correlation between the transitory components. However, the findings of correlation between the transitory components of consumption and income may be attributable to the fact that the earlier literature is conducted in a country-by-country framework, thereby deriving estimates of the transitory components which may be tainted by shocks originating outside the sample country. In contrast, our multilateral methodology is capable of eliminating possible interaction between the five sample countries, thus arriving at more accurate estimates of the transitory components. Our investigation is intended to raise questions regarding the hypothesis and not to overturn the existing evidence. In addition, this paper focuses on the issue of whether the implications of the hypothesis hold at the macroeconomic level rather than with respect to disaggregated data. This paper is organized as follows. Section 2 presents the theoretical model. Section 3 discusses the estimation methodology. Section 4 presents the empirical results. Section 5 concludes.",1
33,1,Journal of Economics and Finance,30 November 2007,https://link.springer.com/article/10.1007/s12197-007-9017-z,How useful are signals? A micro-structure analysis,January 2009,Michele O’Neill,Judith Swisher,,Female,Female,Unknown,Female,"Researchers frequently view self-tender offers as signals by managers regarding firm value. Although self-tender offers have been widely studied, little is known about the manner in which the information is incorporated through trading around the offers. We investigate the effect of the offer announcements on the changes in the degree of information asymmetry in the trading process. We also analyze the role that firm and offer characteristics play in those changes. Our research contributes to the literature that addresses how information is processed by focusing on a specific information event, self-tender offers. Our findings also hold implications for firms that make self-tender offers. In general, we show that the degree of informed trading (and hence, the asymmetric information cost) falls during a self-tender offer, then increases following the expiration. Moreover, we investigate whether changes in the degree of information asymmetry persist following expiration for firms that make self-tender offers. We find no significant change in the degree of information asymmetry from pre- to post-offer for the sample as a whole. However, cross-sectional analyses show that certain offer and firm characteristics are related to changes in information asymmetry around the offers. In general, we find that the percentage of shares repurchased and the repurchase premium affect the changes in information asymmetry around the offers. As for various firm characteristics, our analyses show that the pre-offer level of information asymmetry, the number of institutional investors, the percentage of block trades, pre-offer trading volume, and firm risk are all related to changes in the degree of informed trading around the offers. Our cross-sectional analyses also show changes in information asymmetry persist 1 month after expiration of the offers for certain types of firms. We find that changes from pre-offer to post-offer are related to the levels of pre-offer informed trading and risk. Our results are important to both firms making self-tender offers and to investors in those firms. By analyzing changes in information asymmetry around the offers and how those changes are related to specific characteristics, firms and investors can better understand and anticipate the outcomes of the offers. The remainder of the paper is organized as follows. The next section reviews existing research and develops our hypotheses. The third section describes the data and presents descriptive statistics. The fourth section presents the analyses and provides results. The fifth section summarizes our findings.",
33,1,Journal of Economics and Finance,23 October 2008,https://link.springer.com/article/10.1007/s12197-008-9067-x,Equilibrium exchange rates in oil-exporting countries,January 2009,Iikka Korhonen,Tuuli Juurikkala,,Male,Female,Unknown,Mix,,
33,1,Journal of Economics and Finance,25 January 2008,https://link.springer.com/article/10.1007/s12197-007-9022-2,On understanding mutual fund terminations,January 2009,Qiang Bu,Nelson Lacey,,,Male,Unknown,Mix,,
33,1,Journal of Economics and Finance,20 September 2008,https://link.springer.com/article/10.1007/s12197-008-9062-2,Inter-industry differences in layoff announcement effects for financial institutions,January 2009,Julie A. B. Cagle,Amit Sen,James E. Pawlukiewicz,Female,Male,Male,Mix,,
33,2,Journal of Economics and Finance,21 June 2008,https://link.springer.com/article/10.1007/s12197-008-9038-2,The relation between the equity risk premium and the bond maturity premium in the UK: 1900–2006,April 2009,Angelos Kanas,,,Male,Unknown,Unknown,Male,"Modeling the equity risk premium and determining the variables which predict it are fundamental research topics in financial economics. Empirical studies have found that financial variables such as short and long term interest rates and term structure variables have predictive power against the equity premium (Fama and French 1988, 1989; Campbell and Hamao 1989; Shah and Wadhwani 1990). Special attention has been given to the role that government bond yields play in equity premium determination, as they theoretically depend on many state variables which drive expected stock returns (Campbell 1987; Boudoukh et al. 1997). In addition, Fama and French (1989, 1993) have provided strong evidence that stock and bond markets are far from being segmented, suggesting that joint modeling of stock and bond excess returns is required to uncover the interaction between the markets. UK evidence suggests that there exists a bivariate predictability relation between excess equity returns and government bond yields. Clare et al. (1994), Black and Fraser (1995), and Fraser (1995) found that UK excess returns can be predicted by government bond yields, and Lekkos and Milas (2004) have found that the UK equity premium has predictive power for UK government bond maturity premium.Footnote 1
 The present paper addresses the joint dynamics of the equity risk premium and the government bond maturity premium for the UK for a period spanning more than a century. As discussed in Dimson et al. (2003), the bond maturity premium is the premium investors require for holding long-term government bonds instead of short-term government bonds, and is interpreted as the required compensation for the uncertainty about the real interest rate, inflation or both. Using the rich data set of Dimson et al. (2003; DMS), we consider the period 1900–2006 and seek to assess if there is a predictability relation between the equity premium and the maturity premium for the UK. We model the joint dynamics of the two variables using a vector autoregressive (VAR) framework which allows for regime switching, in order to uncover regime-dependent predictability and time variation in the joint dynamics of the variables. Our results contribute to the existing literature in various ways. First, we document evidence that there is a nonlinear, regime switching, relation between the equity premium and the maturity premium for the UK. A linear relation is ruled out, in line with Boudoukh et al. (1997) who documented evidence of nonlinearities for the USA. We identify a regime in which both variables are characterized by low volatility and another regime in which they are characterized by high volatility. The occurrence of the high volatility regime chronologically coincides with major changes in the value of the British pound exchange rate. The low volatility regime is not an absorbing regime and thus, there is a clear distinction between the two regimes with asset allocation implications. Second, regarding predictability, it is found that the lagged maturity premium is related with the equity premium only in the low volatility regime, thereby indicating regime-dependent predictability of the equity premium. Importantly, the predictability relation is positive, in line with previous evidence for the USA (Campbell 1987; Fama and French 1989; Chen 1991). This result is robust to an alternative definition of the equity risk premium in terms of the long term government yield (instead of the Treasury bill yield), and to the inclusion of the real interest rate and the real growth rate in the regime switching VAR as control variables. The regime-dependent predictability result can be interpreted through the link between stock market volatility and the business cycle documented by Hamilton and Lin (1996), and the link between the term structure and the business cycle documented by Harvey (1988), and Estrella and Hardouvelis (1991). When the stock market is in the low volatility regime, an increase in the maturity premium creates recessionary expectations (Estrella and Hardouvelis 1991) which in turn, entail an additional risk factor for the stock market. This factor is priced and is mirrored upon the higher equity premium, thereby producing a positive relation between the maturity premium and the equity premium in this regime. When the stock market is in the high volatility regime (associated with recessions, according to Hamilton and Lin 1996), an increase in the maturity premium does not create an additional risk factor as the market has already incorporated the negative effects from being in this regime. Third, the time variation in the predictability of the UK equity premium has important economic implications. In line with recent literature (Barberis 2000; Brandt 1999; Campbell and Viceira 1999; Kandel and Stambaugh 1996; Guidolin and Timmermann 2005), it is optimal for investors, faced with time variation in investment opportunities, to vary portfolio allocations as a function of predictor variables and as a function of time horizon. Our results on regime-dependent predictability of the equity premium can be interpreted as evidence of regime-dependent asset allocation decisions by investors. Finally, we document evidence of predictability from the lagged equity premium to the maturity premium. The relation is negative, in contrast to Fama and Schwert (1977) for the USA but in line with Lekkos and Milas (2004) for the UK, and can be interpreted through a portfolio flows argument within linked equity and bond markets: In periods of poor equity market performance (negative equity premium), funds are diverted from the equity market to the bond market driving interest rates lower and thus bond returns higher, and vice versa. The finding of bidirectional regime-dependent predictability (from the lagged equity premium to the maturity premium, and from the lagged maturity premium to the equity premium only in the low volatility regime) justifies our departure from a univariate framework for modeling the equity premium and the focus on the joint determination of the equity premium and the maturity premium dynamics within a regime switching VAR. The remainder of the paper is as follows. The next section discusses of the data set and provides descriptive statistics. Section 3 describes the econometric model. Section 4 discusses the empirical results and provides evidence of robustness of results. Section 5 concludes.",
33,2,Journal of Economics and Finance,25 June 2008,https://link.springer.com/article/10.1007/s12197-008-9048-0,An examination of the performance and prospects for the future of internet-primary banks,April 2009,Ken B. Cyree,Natalya Delcoure,Ross Dickens,Male,Female,Male,Mix,,
33,2,Journal of Economics and Finance,28 August 2008,https://link.springer.com/article/10.1007/s12197-008-9061-3,Specialization of state sectoral employment,April 2009,Edward Nissan,George Carter,,Male,Male,Unknown,Male,"In his essay on economic diversity, Desrochers (2001) provides a summary of the views of a substantial number of scholars and practitioners. The views at times conflict in providing explanations for the differing levels of specialization in regions. The geographer’s view of spatial agglomeration is of interest in that a large portion of output is concentrated in core regions around the world and that similar industries locate in particular places with the additional observation that these patterns seem to prevail over time. Cutler et al. (2003) warn about the process of aggregation that could lead to biases. Lim (2004) observes that states are arbitrary units for the relative importance of specialization and diversity. Instead, the US economy is a collection of metropolitan economies linked by a national system. For instance, knowledge spillovers that could lead to specialization are locally bounded and easily identified in metropolitan areas where people are concentrated in small geographic spaces. Knowledge can then be transmitted easily among them. For this reason, Lim chose 313 MSAs, covering 836 counties among the 3,141 counties in the USA, for his study on the process of knowledge spillovers and its relationship to the industrial characteristics of the neighboring metropolitan areas. Simon and Nardinelli (2002) concur with Lim’s views in their study on the rise of American cities between 1900 and 1990, in that cities that initially start with a relatively large proportion of knowledgeable people grow faster than those otherwise. The reasoning advanced is that spillovers are naturally limited geographically to cities and that knowledge is most productive within the confines where it is acquired. The paper will proceed after this introductory section with a review of literature section which provides credence for research on employment diversity at the state level. Sections concerned with measurement and results follow. The final section provides conclusions.",
33,2,Journal of Economics and Finance,10 June 2008,https://link.springer.com/article/10.1007/s12197-008-9037-3,"Entrenched management, capital structure changes and firm value",April 2009,Leonard L. Lundstrum,,,Male,Unknown,Unknown,Male,"A number of investigators, including Berger et al. (1997) and Garvey and Hanka (1999) report that managerial entrenchment affects the firm’s choice of leverage. Theory implies that debt constrains management discretion by either reducing the manager’s bargaining power (Noe and Rebello 1996) or by reducing discretion over spending (Stulz 1990). Zwiebel (1996) argues that entrenched managers use their influence to lower debt levels to the point that capital structure maximizes empire building subject to sufficient efficiency to prevent a takeover. Agency costs will be incurred at the time of security issuance if the manager makes a sub-optimal security issuance choice. Jensen and Meckling (1976) find that agency costs are decreasing in managerial share ownership. Yet Stulz (1988) argues that agency costs are not necessarily monotonically decreasing in managerial share ownership as there exists an “entrenchment” range over which the manager’s ability to deter takeovers dominates the “incentive” effect of managerial share ownership. We examine the relation between managerial share ownership and the likelihood, magnitude, and information content of security issues. When negotiating debt covenants, creditors appear to respond to the fraction of shares held by the CEO. Begley and Feltham (1999) find that the number of covenants in debt contracts is increasing in the fraction of the firm’s shares held by the CEO. This evidence suggests that managerial share ownership may be important to understanding the agency problems associated with security issuance. The evidence that creditors demand more covenants for firms with high managerial share ownership suggests that the creditor’s concerns about agency conflicts are heightened when managerial share ownership is high. Share ownership appears to not only affect creditor demands but also the leverage choice. Friend and Lang (1988), Moh’d et al. (1998) and Nam et al. (2003) all conclude that leverage is decreasing in managerial share ownership. We examine two important extensions of these lines of inquiry. First, while theory implies that managerial share ownership affects the agency costs associated with issuance, security issuance announcement returns have yet to be tied empirically to managerial share ownership, with the limited exception of Limpaphayom and Ngamwutikul (2004) in the case of seasoned equity offerings. Friend and Lang (1988), Nam et al. (2003), and Moh’d et al. (1998) all argue that their results indicate that agency problems are an important determinant of capital structure. While none of these investigators examine the valuation effects of these agency problems while controlling for outside blockholder share ownership, we do. Second, none of the aforementioned papers reports whether the relationship between managerial share ownership and leverage change is monotonic over the range of managerial share ownership. These two gaps in the literature are addressed. Over an intermediate range of managerial share ownership, firms issue significantly less debt than when managerial ownership is extreme. When managerial share ownership is large and blockholder ownership is small, the firm experiences smaller announcement effects from security issues. The change in leverage is defined here as the average debt-to-asset ratio for the 2 years prior to the announcement date less the average debt-to-assets ratio for the 2 year ends subsequent to the announcement date. The debt-to-assets ratio is measured on a market value basis. Holderness (2003) articulates the differential incentives arising from share ownership of outside blockholders versus managers. Our findings with respect to outside blockholders mitigating agency costs of security issuance are consistent with that of Chen and Yur-Austin (2007) who report that outside blockholders help mitigate managerial extravagance with regard to discretionary spending, but our results are in conflict with the Singh and Davidson (2003) finding that outside block ownership offers limited reduction in agency costs. The balance of this paper is laid out as follows: the model is presented in the next section; the sample and descriptive statistics are discussed in the third section; the fourth section presents the empirical specification and results; the conclusion follows.",9
33,2,Journal of Economics and Finance,10 July 2008,https://link.springer.com/article/10.1007/s12197-008-9051-5,The Feldstein–Horioka puzzle in Latin American and Caribbean countries: a panel cointegration analysis,April 2009,N. R. Vasudeva Murthy,,,Unknown,Unknown,Unknown,Unknown,,
33,2,Journal of Economics and Finance,02 August 2008,https://link.springer.com/article/10.1007/s12197-008-9057-z,Convertibles and milestones in staged financing,April 2009,Lanfang Wang,Susheng Wang,,Unknown,Unknown,Unknown,Unknown,,
33,3,Journal of Economics and Finance,12 June 2008,https://link.springer.com/article/10.1007/s12197-008-9035-5,Market reactions to announcements to expense options,July 2009,Larry J. Prather,Ting-Heng Chu,Paul E. Bayes,Male,Unknown,Male,Male,"The Efficient Market Hypothesis (EMH) posits that investors are rational, fully informed utility maximizers, and that they can process complex accounting information presented in a firm’s financial statements. Moreover, in efficient markets, stock prices quickly adjust to reflect all available information. Numerous studies have assessed the impact of accounting changes on a firm’s stock value and concluded markets can properly assess accounting information (e.g., Ball 1972; Abdul-Khalik and McKeown 1978; Holthausen 1981; Harrison and Grudnitski 1987). Dhaliwal (1986) examined footnote disclosure of non-funded, vested, pension liabilities, and reported that the market is capable of using information that does not appear on the face of the balance sheetFootnote 1. Finally, Sahlman (2002) and Huson et al. (2001) believe that rational investors consider fully diluted cash flow per share in valuation computations. However, Aboody et al. (2004b) and Elayan et al. (2005) question the transparency of the accounting treatment of employee stock options and Asness (2004) asserts that investors may be confused about the economic impact of the accounting treatment of employee stock options. Accounting disclosure of stock options has been a consequential issue since the early 1970s and recently there has been an effort to recognize explicitly employee stock options as an expense. While previous pronouncements, APB No. 25 (1972) and FASB No. 123 (1995), required footnote disclosure or expense recognition, the FASB issued an exposure draft concerning Accounting for Stock-Based Compensation-Transition and Disclosure (FASB 2002) that provides “alternative methods of transition to the fair value method of accounting for stock-based employee compensation.” The popular press has given significant attention to the issue of mandatory expense recognition of options. While the CFA Institute favors expense recognition, others oppose mandatory expensing and the opposition has been strong. Asness (2004) provides a summary of the arguments against expense recognition which include: (1) “options have no value when issued and should be expensed only when exercised (if at all)”; (2) “option values are difficult to calculate”; (3) “this stuff is already reported in the footnotes”; (4) “issuing options is a capital structure/balance sheet transaction, not an income statement transaction”; (5) “options are worth less than their market prices to executives because the executives are not diversified”; (6) “mandatory expensing of options will destroy tech companies”; (7) “it will hurt the little guy, who won’t get options”; (8) “options are an expense only if and when companies repurchase shares and thus spend cash at the time of exercise to prevent dilution”; (9) “the effect of options should be recognized by using fully diluted shares, not showing an expense”; (10) “if companies have to move options to the top line, companies will lie about them more and analysts will ‘go pro forma’ and ignore them”; (11) “the market is efficient, so expensing does not matter because the information must be in prices already.” Elayan et al. (2005) and Lang (2004) discuss academic research related to the issue of expensing options. Clearly, the accounting treatment of stock options is as complex as it is controversial. Moreover, Asness (2004) asserts that people may be confused about the economic impact of a change from footnote disclosure to expense recognition and Hirschleifer and Teoh (2002) believe that investors have limited attention and expensing options helps investors focus. Aboody et al. (2004a) and Bell et al. (2002) investigate the relation between SFAS No. 123 stock option expense and share price. Aboody et al. (2004a) find that the expense of stock options, as measured by SFAS No. 123, is reflected in market valuation assessments. These assessments result in a significant negative relation between stock option expense, as defined by SFAS No. 123, and equity price after controlling for net income, equity book value, and expected earnings growth. However, Bell et al. (2002) find that a positive association exists between equity share price and SFAS No. 123 expense for 85 profitable computer software firms. Aboody et al. (2004b) focus on 155 firms that voluntarily began recognizing options expense during 2002 and early 2003 and conclude that the likelihood of expense recognition is directly related to capital market activity, private incentives of top management and board members, political costs, and information asymmetry. Thus, they provide insight on why firms may choose to recognize options expense. Additionally, they briefly examine event day reactions to the announcements of a change in accounting treatment and report that the 32 early announcing firms exhibited a statistically significant positive reaction of 1.25%. However, the 116 firms that announced in August 2002 and later did not exhibit a statistically significant reaction. This result was interpreted as early announcements being positive signals of a firm’s financial prospects whereas later announcements being “no news” because investors believed that the change would become a requirement. However, this research was not intended to provide a robust examination of announcements effects. Elayan et al. (2005) sample 140 firms from July 8, 2003, through May 15, 2003, to examine announcement effects. Their event study uses the earliest date listed on Factiva.com on which a firm indicates that it is likely to expense options rather than the official announcement date. Abnormal returns against the CRSP value-weighted (VW) index reveal that announcing firms exhibit positive abnormal returns on event days −1 through +3. They report that: (1) prices increase for firms that announce a change in accounting treatment to expense options; (2) prices decrease for industry/size/performance matched firms that did not announce a change; (3) the magnitude of the reaction depends on agency costs, the magnitude of option expenses, and financial reporting costs; (4) the reaction is not affected by contracting costs (debt covenants), growth opportunities, or potential political repercussions; and (5) the change in accounting treatment is not a reliable signal of future operating performance. They interpret the results as the market rewarding announcing firms for their honesty and they believe that the reaction is related to transparent reporting. However, they admit puzzlement regarding the long lasting abnormal returns. The reaction is as puzzling as their long-lasting effects. If footnote disclosure is adequate, investors could determine the dilution of future earnings and compute expected future free-cash-flows per share. Thus, there should be no stock price reaction to an announcement of a voluntary change in accounting treatment from footnote disclosure to expense recognition of employee stock option grants. This is because free-cash-flows per fully diluted share do not change.Footnote 2 However, the arguments against a mandatory change in accounting regulations that would require firms to recognize employee stock option grants as an expense (summarized in Asness 2004) suggest that some investors are confused by the economic impact of a change in accounting treatment from footnote disclosure to expense recognition. If confusion exists, it could lead to stock price volatility on the announcement of a voluntary change in the accounting treatment of employee stock option grants and potentially explain the long lasting abnormal returns found in Elayan et al. (2005). To examine the stock price reaction of firms that switch from footnote disclosure to expense recognition we use data from 241 announcing firms during the period January 2002, through December 2004. We use event-study methodology to generate abnormal event-period returns and test those returns for statistical significance using parametric and non-parametric tests. Our study differs from Aboody et al. (2004b) and Elayan et al. (2005) in several ways. First, we use a larger sample over a longer period. Secondly, we examine the abnormal returns surrounding the official announcement date. Next, we use numerous robustness tests that include various beta estimation methods, various indices, parametric and non-parametric tests, and various model specifications. While Aboody et al. (2004b) focus on why firms voluntarily began recognizing options expense, we explore the reaction to the announcement during an 11-day event window around the announcement day. Our study differs from Elayan et al. (2005) because we use different proxy variables that are related to information asymmetry to ascertain the root cause of a reaction. Our extension of the literature is motivated by the controversy over proposed changes to the accounting treatment of options, the complex nature of option valuation, and the complexity of providing investors with sufficient detail to ascertain the effect of options on equity valuation. We make three contributions to the literature; first, we scrutinize the announcement effect on the stock price of firms that voluntarily announce a change in their accounting treatment of employee stock options from footnote disclosure to expense recognition. Our empirical results reveal statistically negative price changes on the announcement followed by statistically positive price changes. Because our results reveal significant stock price volatility during the announcement period, our second contribution is the investigation of the cause of this volatility. We propose the learning, sophisticated investor, neglected firm, and firm size hypotheses to explain the results and find varying degrees of support for the firm size, sophisticated investor, and learning hypotheses. These findings may be attributed to different clienteles’ perceptions of accounting transparency or confusion on the part of some clienteles. Our final contribution is that we show that while significant reactions in average abnormal returns occur during the announcement period, cumulative abnormal returns reveal that the change in accounting treatment has a neutral impact on stock prices. The following section provides background on the accounting treatment of employee stock option grants and develops our primary hypothesis. Section 3 describes our data and methodology and Section 4 discusses our empirical results. The final section concludes.",1
33,3,Journal of Economics and Finance,28 August 2008,https://link.springer.com/article/10.1007/s12197-008-9060-4,Economic freedom: a comparative study,July 2009,Rolando F. Peláez,,,Male,Unknown,Unknown,Male,"Mainstream economics has neglected the role of institutions and culture, instead emphasizing the roles of capital, labor, and technology as determinants of growth. However, Friedman (1982), Friedman and Friedman (1980, 1998), North (2000, 2005), Bauer (1957, 1978, 1981, 1984), Sen (1997, 1999, 2003), de Soto (2000), and Scully (1988, 2002), among others, have underscored the importance of economic freedom in determining the wealth of nations. This paper attempts to identify the idiosyncrasies that limit economic freedom in five groups of countries. The study uses data from the 2007 Index of Economic Freedom published jointly by the Heritage Foundation/WSJ. Empirical work in this area requires measures of the extent to which individuals may freely engage in economic transactions. Freedom’s House 1983 annual report was an early attempt to measure economic freedom. Following this pioneering work, the Fraser Institute and the Heritage Foundation began to publish annual reports on the state of economic freedom in the world. Hundreds of journal articles have used data from Economic Freedom of the World, and from the Index of Economic Freedom, published by the Fraser Institute, and the Heritage Foundation, respectively. With minor exceptions, the country rankings from these two reports are similar. Economic freedom may be defined as “the absence of government coercion or constraint on the production, distribution, or consumption of goods and services beyond the extent necessary for citizens to protect and maintain liberty itself” (2006 Index of Economic Freedom). Practices that reduce freedom include, among others: violating property rights, preventing people from transacting voluntarily inside and outside a nation’s borders, wage and price controls, restrictions on financial activities, high tax rates, inflation, regulatory burdens on business, and corruption in the judiciary, customs service, and government. A composite measure of economic freedom quantifies the extent to which people are free to work, produce, trade, consume, save, invest, and have secure rights to property. The 2007 Index provides data on ten factors or measures of freedom listed below.
 Business freedom: measures the ability to create, operate, and close an enterprise quickly and easily. Trade freedom: measures the absence of tariff and non-tariff barriers that affect international trade in goods and services. Monetary freedom: combines a measure of price stability with an assessment of price controls. Both inflation and price controls distort market activity. Price stability as an expression of free markets is the ideal state. Freedom from government: varies inversely with the level of government spending as a proportion of GDP. Fiscal freedom: measures the confiscation of income and wealth via taxation. It accounts for the tax burden, in terms of the top tax rate on income (individual and corporate separately), and the proportion of tax revenue to GDP. Property rights: assesses the ability of individuals to accumulate private property, secured by laws fully enforced by the state. Investment freedom: assesses the free flow of capital, especially foreign capital. Financial freedom: measures the independence of banks and other financial institutions from government control. State ownership of banks is a burden. Freedom from corruption: assesses the presence of corruption in business, it also includes governmental, legal, judicial, and administrative corruption. Labor freedom: measures the ability of workers and businesses to interact without restriction by the state. Previous editions of the Index graded each factor on a scale of one to five—lower numbers indicating greater freedom. However, beginning in 2007 each factor varies from zero to 100—high scores signaling greater freedom. The overall freedom score for each country is the arithmetic mean of the scores on the ten aforementioned factors. Based on the overall score, the Index distinguishes between five categories of countries as shown in columns one and two of Table 1.
",6
33,3,Journal of Economics and Finance,30 April 2008,https://link.springer.com/article/10.1007/s12197-008-9028-4,Deposit rate sensitivity of credit union shares,July 2009,Liliana V. Stern,Steve Swidler,Christoph Hinkelmann,Female,Male,Male,Mix,,
33,3,Journal of Economics and Finance,06 August 2008,https://link.springer.com/article/10.1007/s12197-008-9059-x,US elections and monthly stock market returns,July 2009,Steven T. Jones,Kevin Banning,,Male,Male,Unknown,Male,"Prior literature regarding the relationship between political happenings and US stock returns can be broadly divided into three categories. First, there is the widespread belief that the stock market prefers Republican presidents to Democrats (the “party effect”). Prior research does not support this belief, and indeed often comes to exactly the opposite conclusion. Huang (1985) found that annual rates of return on stocks from 1929–1980 were better under Democratic presidents than under Republicans. Siegel (2002) found similar results over the 1888–2001 period. Johnson et al. (1999) studied the period of 1929–1996; they found no partisan difference in the returns of the Standard & Poor’s 500 Index, but found a pronounced party effect in favor of the Democrats when examining an index of small stocks. In examining British elections, Hudson et al. (1998) found no difference in the performance of the Financial Times 30 Index between Tory and Labour governments. The second effect, related to the first in that the subject of interest is the president’s political party, tests the market’s immediate reaction to the election of a Democratic or Republican president (the “election reaction”). Here, empirical results line up far more closely with the popular wisdom: the stock market, on average, reacts more positively to the election of a Republican president than to the election of a Democrat. Studies whose results are largely consistent with this hypothesis include Homaifar et al. (1988), Siegel (2002), Riley and Luksetich (1980), and Niederhoffer et al. (1970). The third effect relates to the belief that stock market returns tend to be higher during the last 2 years of any given 4-year presidential term, as opposed to the first 2 years; this is known as the “second-half” effect. Previous evidence has mostly supported this belief. For example, Stovall (1992) examined the annual percentage change in the Dow Jones Industrial Average from 1901 to 1991, and found that equities performed best in the last 2 years of a president’s term. Other studies with findings broadly supportive of the second-half effect include Siegel (2002), Allvine and O’Neill (1980), Huang (1985), and Johnson et al. (1999). The notion of a second-half effect apparently does not apply to England, where Hudson et al. (1998) reported no difference in stock returns over the terms of the various British governments. This paper involves both an attempt to replicate the findings of prior studies, and an effort to look at a broader range of issues than most of these studies have considered. For instance, we test the relationship of market returns not only to which party holds the White House, but also to majority status in the two houses of Congress. In addition, we examine whether the results of our tests change when we control for two key macroeconomic variables: real GDP growth and inflation. We also examine the potential impact of various combinations of control. For instance, does the stock market perform better under the combination of a Democratic President and a Republican Congress, or under the opposite combination? Regarding the issue of election cycles, we test the second-half effect, but also add tests regarding the number of terms that the same party has controlled the presidency. The remainder of the paper is organized as follows. First, we provide details regarding our data and methodology. Second, we itemize the results of our various tests. A final section concludes the paper.",32
33,3,Journal of Economics and Finance,04 April 2008,https://link.springer.com/article/10.1007/s12197-008-9026-6,"Structural changes, market growth and productivity gains of the US real estate investment trusts in the 1990s",July 2009,John C. Topuz,Ihsan Isik,,Male,Male,Unknown,Male,"The growth of Real Estate Investment Trusts (REITs) in the 1990s has been exceptional (Ambrose et al. 2000; Block 2002; Topuz et al. 2005; Ling and Archer 2005). Upon the introduction of many important regulatory reforms and favorable market conditions after 1993, the industry has witnessed significant consolidation and historical surge in assets and market capitalization, with strong implications on REITs productive performance. Using a panel data set from the period 1989 to 1999, this paper utilizes the methodology of Malmquist index theory to explore the effects of such structural and regulatory developments on the productivity and efficiency growth of REITs. “Everybody loves REITs” lately. In effect, these firms are real estate mutual funds that invest money (obtained through the sale of its shares to investors) in residential or commercial properties and earn primarily rent revenue (equity REITs), invest in property mortgages and earn principally interest revenue (mortgage REITs) or combine both investment strategies for shareholders (hybrid REITs). REITs have provided small investors with an opportunity to buy skyscrapers, shopping malls, hotels, restaurants and apartment buildings, without incurring the hassles of direct property ownership. With the all right essentials—consistent and remarkable profits, low volatility, impressive dividends, enhanced liquidity, low correlation with other investment classes, and most importantly professional management, REITs have become a part of every serious investor’s diversified portfolio in recent years. However, it took REITs about five decades to mature into solid citizens of the investment world. In 1960, the US Congress under President Eisenhower created these new institutions to promote real estate development by increasing public participation. The general public responded very favorably to this innovation and REITs were an enormous success in the late 1960s. However, after many mortgage REITs failed during the recession of 1973–1974, markets soon became disenchanted and lost interest in the entire REIT industry. The 1980s, characterized by the escalation of prices, overbuilding and poor performance, did not turn things around for REITs either. During the decade, the major blow came through the Tax Reform Act of 1986 that took away the tax-shelter advantage of REITs.Footnote 1 By the late 1980s, real estate sector was in “big trouble” (Block 2002). Especially after 1993, numerous regulatory reforms have been launched to restore the attractiveness of REITs and boost their operational/managerial performance. Of these, the 1993 Revenue Reconciliation Act is the landmark and arguably most influential one for the industry (Topuz et al. 2005; Ling and Archer 2005). The original legislation of 1960 had exempted these special purpose trusts from corporate income tax if they met certain criterion, some of which later resulted in unintended consequences. To ensure diversified ownership, the original act had stated that a REIT’s income would be taxable if five or fewer individuals owned more than 50%. This code made it difficult for large investors, like pension funds, to participate in REITs, hampering the flow of funds necessary for capital expenditures and expansions. The problem was partially solved by the creation of the Umbrella Partnership REIT (UPREITs) structure in 1992, which allowed investors to hold shares in the umbrella structure rather than in the trust’s real estate.Footnote 2 However, it was the adoption of the Revenue Reconciliation Act in 1993, which has stimulated institutional investors such as pension funds to make large dollar investments in the REIT industry.Footnote 3
 As a reflection of favorable regulatory and business environment, the REIT market has exploded during the 1990s. Most notably, the 1993 Act broadly fostered the acceptance of REITs by mutual funds and other institutional investors, especially pension funds, paving the way for historical injection of funds into the industry. According to the statistics of the National Association of Real Estate Investment Trusts (NAREIT), the industry’s leading trade organization, the industry has grown from 119 REITs with a market capitalization of $8.7 billion in 1990, to 203 REITs with a capitalization of $125 billion by the end of 1999, a 71% increase in the number of REITs and a stunning 1,322% increase in capitalization (see Table 1). The growth is more salient for equity REITs, with 188% increase in number and 2,030% in capitalization. This also manifests the trend of increasing scale in the REITs industry. Further evidence of the rise in the size and stature of the REIT market is the addition of the first REIT, Equity Office Properties, to the S&P 500 (in place of Texaco, Inc.) at the end of the decade.Footnote 4 According to industry experts, the inclusion of REITs in the S&P 500 is a significant step that underlines the spectacular growth of the real estate market in the 1990s and its increasing role in our financial system (Ling and Archer 2005).
 The regulatory and structural changes after 1993 that have substantially altered the environment in which the REITs operate may have also resulted in notable impacts on their operating performance. As mentioned, REITs have sought ways for rapid growth and economies of scale. However, as with any business, a key to successful performance for a REIT during expansion lies with the strength of its management. As firms grow very rapidly, managers usually do not have sufficient expertise and time to handle all operational details. Furthermore, operating overhead increases with faster growth. Isik and Hassan (2003c) provide evidence from the banking industry that asset growth rate is significantly and negatively associated with various measures of operational efficiency. Thus, many expanding REITs in the 1990s might have also outgrown their existing management skills. In addition, due to favorable market trends in real estate during the mid-1990s, investors poured money into REITs at unprecedented pace (Ambrose and Linneman 1998). If REITs had grown as a result of a rise in the market demand for real estate services, then greater demand might have allowed (tolerated) less efficient operations - at least in the short run. Furthermore, the business strategy of REITs during the 1990s was basically “growth through acquisitions”, which resulted in a large wave of M&As. Vogel (1997) argues that the consolidation process observed in the 1990s is based on external, not efficiency related, factors. Evidently, Campbell et al. (1998) and Ambrose et al. (2000) found that these external growth attempts have not yielded positive returns or economies of scale for the REITs, suggesting the possible presence of self-dealing in many of these mergers. Therefore, the short-run outcomes of consolidation in the industry during the 1990s might have been unfavorable since they required a longer period to digest the newly acquired trusts and properly manage the transition period. One important empirical inquiry to address is thus whether REIT productivity and efficiency has been affected in response to the regulatory and structural developments after 1993. If the remarkable REIT growth were achieved on the basis of the same or less productive resources, one should expect clear progress in operational performance. In other words, if REIT outputs and assets run faster “north” than did factor inputs during the decade, we should observe significant improvements in both the productivity and efficiency of these new-born institutions. Moreover, if most of the M&As that took place during the decade resulted in scale economies, we should also detect an overall improvement in resource management in the industry. Yet, to the best of our knowledge, no empirical work has studied the impact of the changes that took place after 1993 on the productivity, technology and efficiency growth of the REITs. The purpose of this study is to examine the developments in the productivity as well as efficiency of the REITs using the nonparametric Malmquist Index method. Our technique allows us to consider in a dynamic setting both productivity growth at the frontier and the spread of productivity levels, as well as the diffusion of technology across the REIT industry. Bers and Springer (1997) and Linneman (1997) expected REITs to continue to grow in size and scale and become more competitive, with strong implications for scale economies/diseconomies in the industry. Hence, we also analyze the changes in the returns to scale of REITs during the period in order to see whether there are scale advantages to exploit by expanding or contracting operations. Our empirical results indicate that the REIT industry has experienced productivity losses and contractions in their production frontier during this more liberal era. However, had it not been the “astonishing” efforts of inefficient REITs to catch up with the leading firms, the productivity fall would have been greater. Most of the overall efficiency gains were predominantly due to the march of typical REITs towards the optimum scale (scale efficiency increases) rather than efficient utilization of resources by management (technical efficiency rises). In addition, upon the launch of real estate reforms, we observed some improvements in production performance, especially in the “catching up effect” of the average REITs, probably due to the new regulatory and business environment that have broadened the opportunity set of services for all kinds of firms. Our analysis of returns to scale suggests that the times when efficiency gains were possible by just acquiring assets are over, as most of the REITs began to suffer from excessive size by the end of the decade. It appears that the “hot money” that fed the frenzy of asset acquisitions generously during the decade has left the industry, and the future viability and competitiveness of REITs will be increasingly dependent upon how well they are managed and how efficiently they are run. “Section 2” provides literature review. We introduce the Malmquist methodology in “Section 3”. “Section 4” discusses data and empirical design. In “Section 5”, we examine the Malmquist index and its subcomponents along with some robustness checks. In “Section 6”, we present our conclusions.",12
33,3,Journal of Economics and Finance,11 June 2008,https://link.springer.com/article/10.1007/s12197-008-9042-6,Foreign direct investment and its impact on the Thai economy: the role of financial development,July 2009,James B. Ang,,,Male,Unknown,Unknown,Male,"The literature on foreign direct investment (henceforth, FDI) suggests that inflows of FDI can exert a positive influence on economic growth through the transfer of new technology and spillover efficiency. However, such a positive impact does not occur automatically, but rather, it depends on the absorptive capacity of the recipient country. Several studies that discuss the importance of absorptive capacity and the success of FDI focus only on human capital and trade regime (e.g., Balasubramanyam et al. 1996; Borensztein et al. 1998; Xu 2000; Kohpaiboon 2003). Only in recent years, some attention has been given to study the role of finance in the relationship between FDI and economic growth. Using a cross-country framework, these studies show that the impact of FDI on economic growth depends on the level of financial development in the host country (see Hermes and Lensink 2003; Alfaro et al. 2004). The results of these cross-country studies suggest that countries with better-developed financial systems tend to benefit more from FDI. However, so far there has been no case study evidence documented. Case studies are particularly useful in disentangling the complexity of the financial environments and economic histories of each individual country. By analyzing case studies, the econometric findings of this project can be related to the prevailing institutional structure, and therefore inform academic as well as policy debate. Hence, this paper aims to complement the above studies, and enrich the literature by providing further evidence using Thailand as a case study. Specifically, we examine how financial development affects the FDI–growth relationship for Thailand over the period 1970–2004. The remainder of the paper is organised as follows. Section 2 sets out the analytical framework. Section 3 describes the empirical model and data sources. Section 4 explains the econometric techniques used in this study. The results are discussed and presented in Section 5, and the last section concludes.",30
33,3,Journal of Economics and Finance,11 February 2009,https://link.springer.com/article/10.1007/s12197-009-9077-3,The Obama effect,July 2009,Dennis Halcoussis,Anton D. Lowenberg,G. Michael Phillips,Male,Male,Unknown,Male,"During the U.S. presidential election campaign of 2008, the conventional wisdom expressed by many political commentators, and a recurring theme in media reports on the election, was that Barack Obama’s lead in the opinion polls was due to the financial collapse precipitated by the sub-prime mortgage crisis, the plunging stock market and forecasts of a severe recession. Because these economic difficulties had arisen during a Republican administration, it was perhaps inevitable that voters would blame the Republicans and turn to the Democratic candidate for deliverance from further disaster. This view follows plausibly from the well-accepted dictum that individuals tend to “vote their pocketbooks.” As evidence for the positive impact of the economic crisis on Obama’s performance, most reports cite opinion poll results indicating that a majority of respondents believed that Republicans were responsible for the crisis, that Obama would be more effective in dealing with the economy than would his Republican opponent, John McCain, and that this belief was especially strong among those potential voters who thought that the economy was the issue of foremost concern or importance in the election. Thus, for example, according to a September 2008 CNN/Opinion Research Corp. poll, 47% of registered voters believed that Republicans were primarily to blame for the problems confronting financial institutions and the stock market, while only 24% held Democrats responsible.Footnote 1 A Washington Post-ABC News national poll, also conducted in September 2008, found that, among likely voters, Obama held a 53–39% lead as the candidate most trusted to handle the economy and Obama led by 13 points in perceived ability to deal with the failure of major financial institutions.Footnote 2 Among voters who prioritized the economy, Obama had an almost two-to-one lead over McCain.Footnote 3 A Quinnipiac poll released on October 1, 2008 showed 46% of those surveyed in Pennsylvania viewed Obama as more able to address the problems of the economy, compared to 37% for McCain.Footnote 4 A Wall Street Journal/NBC News poll, conducted on October 4–5, 2008, reported that approximately one in three voters were “more reassured” by Obama’s handling of the economic crisis versus just 25% for McCain.Footnote 5 By early October 2008, it became clear that Obama was the favorite to win the election, apparently in large measure thanks to the financial crisis.Footnote 6
 However, while Obama’s lead solidified in late September and early October, the crisis arising from financial institutions’ exposure to securitized packages of sub-prime real-estate loans dated at least to the government-brokered takeover of Bear Stearns in March 2008, the failure of IndyMac Bank in July and the Treasury seizure of Fannie Mae and Freddie Mac on September 7, 2008.Footnote 7 If concerns about the financial crisis were the main cause of Obama’s successes, his dominance of the opinion polls should have shown up earlier than it did. On the other hand, there is a remarkable coincidence of timing between Obama’s electoral prospects and the crash of the stock market. The stock market commenced its dramatic 1-week plunge on October 6, shortly after most of the polls started to suggest that Obama was likely to win the election. In fact, we hypothesize that the expectation of an Obama victory was a significant factor contributing to the drop in stock prices. Our hypothesis is based on the fact that stock prices reflect the present value of the expected stream of firms’ future profits, together with the fact that many of the proposals included in Obama’s electoral platform could reasonably be expected to have a negative effect on firms’ profitability. For example, Obama’s platform called for an increase in the marginal tax rate on the most productive workers of more than ten percentage points. His plan would also raise taxes on capital gains and dividends to 20%, compared to a 15% rate proposed by McCain.Footnote 8 According to Ohanian’s (2008) estimate, this five percentage-point difference in overall capital income taxation in the long run is equivalent to a difference in the nation’s capital stock of about 18%, which would result in a 6% decrease in real GDP and the average real wage rate over a 25 year period. Other policies proposed by Obama that would be anticipated to have a negative effect on firms’ profits include higher taxes on oil companies’ “windfall profits” and on multinationals’ overseas earnings, increases in the minimum wage, changes in labor relations law making it easier for unions to organize strikes, price controls in the pharmaceutical industry, and virtual nationalization of the health insurance industry.Footnote 9 Since markets are forward-looking, any increase in the probability of Obama winning the election would, as Newman (2008) points out, cause an immediate expectation of reduced future profits and a consequent fall in stock prices. And indeed, in the week of October 6, 2008, the Dow Jones Industrial Average fell by 1,874 points, or 18%, in its worst weekly decline ever on both a point and percentage basis; the S&P 500 fell by more than 20%. On November 5, the Dow Jones average decreased by 486.01 points, or 5.05%, the worst percentage loss in history for the day after a presidential election.Footnote 10
 In the next section we test the hypothesis that the stock market crash was, at least in part, driven by the market’s expectation that Obama would win the election. We use the S&P 500 index as a measure of stock prices and, as a gauge of daily public opinion, the 2008 U.S. Presidential Election Markets.Footnote 11 The latter are real-money futures markets, operated by the Iowa Electronic Markets of the University of Iowa, in which contract payoffs are determined by the popular vote cast in the 2008 election.Footnote 12 Our results indicate a negative correlation between stock prices and lagged increases in expected Democratic electoral performance, although the correlation is minor in comparison to total variation in stock prices. There are, of course, many factors that might be considered in a model of stock market performance besides expected electoral outcomes.Footnote 13 In particular, in October 2008 the market was clearly impacted by the uncertainty following the collapse of major financial institutions around the world and the threat of a severe and prolonged recession. Nevertheless, our findings offer some support for the argument that the economic policies proposed by Obama, along with the growing likelihood of an Obama victory, contributed to the fall in stock prices.",9
33,4,Journal of Economics and Finance,04 June 2008,https://link.springer.com/article/10.1007/s12197-008-9029-3,Spillover effects on government bond yields in euro zone. Does full financial integration exist in European government bond markets?,October 2009,Faruk Balli,,,Male,Unknown,Unknown,Male,"Recently, international portfolio holders began to focus on other factors instead of standard factors-default (credit) risk, exchange rate risk, or liquidity premium risk factors-when they were holding euro denominated government bonds. The main reason behind the change in investors’ decision is the recent adjustments in the formation of euro bond markets. Why does remarkable change take place in European markets while we can not observe such a change in the rest of the Organisation for Economic Co-operation and Development (OECD) bond markets? The fundamentally transformed structure of the European bond markets has been triggered by the Maastricht treaty. In the content of the treaty, the prospective monetary union [European monetary union (EMU)] member aimed to satisfy certain fiscal and macroeconomic standards to converge upon each other economically and financially by the beginning of the currency union. To quickly summarize the criteria, the members should have public debt levels below the Maastricht ceiling of 60% of their gross domestic product (GDP), plus the budget deficits of central governments should not exceed 3% of GDP. Added to these, member countries were supposed to hold annual domestic inflation rates under 2% per annum. Expectedly, the outcome of these policies were observed as more integrated financial markets since the mid 1990s in the euro region. Besides, both elimination of exchange rate risks and the significant decreases in intra-euro market frictions like trading costs, brokerage commission transaction fees, and taxes have led to a more integrated financial environment since the beginning of monetary union. In the first point of view, accelerated financial integration among euro bond markets has been widely expected, since the macroeconomic and fiscal indicators have shown incredible improvement for the “higher risk”Footnote 1 euro markets, creating a potential for those members to converge with “lower risk” members in terms of bond returns. Figure 2a and b present government bond yield differentials among euro members for a time interval including the pre and post-euro era. As illustrated in the figures, there has been a considerable decline in government bond yield spreads since the second half of the last decade. It might be concluded that in the wake of monetary union, bond yield convergence has been exhibited for the countries experiencing higher credit(default) risk premia for their sovereign debts compared to the “low risk” euro members.Footnote 2
 Current account to GDP ratio of EMU members 
a 10 year benchmark Government Bond Yield Differentials of Euro Members for January 1999–December 2004. b 10 year benchmark Government Bond Yield Differentials of Euro Members for years between 1990 and 2004 Although there has been such a convergence among euro area government bond yields, yield differentials across euro bond markets have not been wiped out completely. One possibly expect that residual spreads would definitely reflect the differences in credit standings of euro markets, since the markets are not perfectly homogenous even after the inception of monetary union. The Stability and Growth Pact and European fiscal frameworks appear insufficient to guarantee that all member states have the same creditworthiness from the market point of view, even though the Maastricht Treaty has forced the members to be in similar fiscal positions.Footnote 3 In that case, euro sovereign bond yields would be counted as an important indicator of market sensitivity to domestic fiscal exposure, since higher bond yields generally result from higher government debt costs or higher current account deficits that require tighter market discipline on national governments’ fiscal policies. From the portfolio holders’ point of view, it is usually expected that those bonds tend to have higher credit risk premium. In Fig. 4, it is shown that Belgium, Greece, and Italy have experienced higher debt service costs compared to other euro members. Similarly, Portugal, Italy and Greece have been experiencing higher current account deficits compared to the rest of the EMU members. Even though the current account deficit/surpluses and government service debt costs are very important indicators for measuring market sensitivity to the credit risk of government bond returns, and these measurement are in higher levels for those markets, the sovereign bond yields do not reflect such risk premia particularly after 1999.Footnote 4
 10-years average bond yield differentials and credit risk for years 1999–2004 Debt to GDP ratios of EMU members Particularly with start of the monetary union, the default risk indicators have very little power to explain the yield spreads in the region. Theoretically, higher debt service holders’ borrowing tends to have higher risk premia compared to those holding lower debt service, but euro government bond yields do not have such patterns robustly. A sufficient explanation for why the default risk premium cannot be rationally observed in the euro denominated government bond yields could be that there is a widely shared belief that even if the government of any EMU member country threatened to not pay its debt, it would be bailed out by the collective EMU governments. This unspoken belief makes foreign portfolio holders consider the debt instruments of all EMU member governments as bearing the same default risk. Under this implicit bail-out commitment, the market will treat all instruments as being of equivalent default risk.Footnote 5
 Concentrating on financial asset pricing models, the recent literature finds strong evidence that idiosyncratic properties of each market or disparate fiscal positions could straightforwardly explain the yield spreads of government bonds between those markets. Nevertheless, after the start of monetary union, the picture is not the same for euro bond markets. Fiscal and other sets of indicators could barely help us explain the differentials in euro bond markets. In other words, local factors are not as effective in explaining the yield spreads as in the periods before the start of euro. At this time, these findings might indicate a full financial market integration among members, since one can claim that financial asset prices no longer depend on local risk factors. The main contribution of the paper comes out at this point. While we control the specific market risk factors such as default risk, liquidity risk or maturity risk, global shocks influence the government bond yield differentials among euro markets differently and cause distortions among bond yield differentials. These findings document that for euro bond markets, global factors play more important role in explaining the bond yield differentials than the specific market risk does. The disappearance of default (credit) risk premium on euro region government bond returns is associated with increasing importance of the global risk and international risk factors on those bonds. In this case, we can barely talk about full market integration given that the global shocks in the bond yields creates the deviation in the yield differentials. We modeled the financial integration by considering both the local and global factors and concluded that financial integration is not perfectly realized in euro bond markets. Secondly, for the first time in the literature – up to now, the volatility of the yield spreads were not modeled, according to our investigation of the literature – we model the volatility of euro originated government bond yield differentials by using the Multivariate GARCH model to control the effect of global shocks. We find that not only the yield differentials but also the volatility of the yield differentials are affected by the global shocks, proving that full financial integration of euro bond markets does not exist yet. The paper is arranged as follows. In Section 2, we perform a detailed literature survey, concentrating on both the government bond markets’ integration and the effect of global shocks through macroeconomic announcements on the markets. We discuss data issues and variable definitions in Section 3. The next section contains the empirical study explaining the yield differentials by employing global risk factors such as macroeconomic announcements and other international risk factors. Section 5 presents the empirical results of the Multivariate GARCH model. The next section contains the robustness checks for the empirical models. The final section offers concluding remarks.",27
33,4,Journal of Economics and Finance,06 August 2008,https://link.springer.com/article/10.1007/s12197-008-9058-y,Multiple shifts and fractional integration in the US and UK unemployment rates,October 2009,Guglielmo Maria Caporale,Luis A. Gil-Alana,,Male,Male,Unknown,Male,"The stationary or nonstationary nature of the unemployment rate has been the study of numerous papers in recent years. On the one hand, the hysteresis approach to unemployment suggests that unemployment is a nonstationary highly persistent variable, since fluctuations in the natural rate of unemployment [the nonaccelerating inflation rate of unemployment (NAIRU)] are permanent. The standard econometric approach for modelling this type of behaviour relies on a unit root specification. On the other hand, the rejection of the unit root hypothesis supports reversion to a natural rate. The slow reversion often found is normally referred to as hysteresis. Blanchard and Summers (1986, 1987) define it as: “a case where the degree of dependence is very high, where the sum of coefficients is close but not necessarily equal to one"". This high persistence of shocks is a feature, among others, of “insider” models (see Lindbeck and Snower 1988), or of models in which fixed and sunk costs make current unemployment a function of past labour demand (see Cross 1994, 1995). In “structuralist” models (see Phelps 1994) the natural rate is “endogenised”: as in NAIRU models, unemployment is viewed as having an equilibrium level to which it generally reverts when hit by shocks, but it is also thought to be subject to infrequent structural breaks, resulting from changes in economic fundamentals, which affect the equilibrium itself. Hence the unemployment series should be stationary (or, more generally, mean-reverting) provided one allows for breaks. The standard approach which uses classical unit root tests [e.g. Dickey and Fuller 1979 (ADF); Phillips and Perron 1988 (PP); Kwiatkowski et al. 1992 (KPSS); or some of their recent developments, i.e., Elliot et al. 1996; Ng and Perron 2001, etc.] for testing the above theories of unemployment has two important limitations. First, it only considers integer values for the orders of integration (1 in the case of unit roots, and 0 for stationarity), but it does not allow for fractional alternatives, where the fractional parameter d can take any value in the interval [0, 1] or even above 1. Besides, these methods do not allow the rate to which reversion occurs (the natural rate) to shift, and are therefore inadequate to describe the pattern observed in unemployment rates. Note that under hysteresis (or persistence—see, e.g., Blanchard and Summers 1986, 1987; Cross 1987) the order of integration (denoted by d) should be equal to or at least close to 1, implying that the effects of the shocks are permanent (if d = 1) or at least highly persistent (if d < 1 though close to 1), whilst infrequent breaks would give support to the structuralist view (Phelps 1994). On the other hand, a value of d close to 0 would favour NAIRU theories (see, e.g., Friedman 1968; Phelps 1967). In this paper, we deal with the two above-mentioned problems (i.e., not considering fractional alternatives and/or mean shifts) by means of a procedure that enables us to consider both fractional orders of integration and multiple mean shifts. By using fractional orders of integration, we allow for a much richer degree of flexibility in the dynamic behaviour of the series. Thus, if the order of integration (denoted by d) is higher than 0 but smaller than 0.5, the series still will be covariance stationary, though the effects of shocks will take longer to disappear than in the case of d = 0. In the latter case, the series is said to be “short memory” and the effects will disappear fairly soon, according to an exponential decay, if, for example, the observations are autoregressive. On the other hand, if d > 0, the series is said to be “long memory” and the decay is hyperbolic. Also, if d belongs to the interval [0.5, 1], the series is no longer covariance stationary but is still mean reverting, with shocks disappearing in the very long run. Further, it is well known that standard procedures for nonstationary unit roots (ADF, PP, KPSS, etc.) have extremely low power if the alternatives are of a fractional form (Diebold and Rudebusch 1991; Hassler and Wolters 1994; Lee and Schmidt 1996, etc.). The existence of different regime shifts in time series has been found in recent years to be crucial in the determination of the long-memory parameter (see, e.g., Diebold and Inoue 2001; Gourieroux and Jasiak 2001; Granger and Hyung 2004; etc.), as overlooking breaks may lead to spurious findings of long memory. For instance, Lobato and Savin (1998) argue that structural breaks may be responsible for the long memory in return volatility processes, and Engle and Smith (1999) investigated the relationship between structural breaks and long memory using a simple model where the data generating process consists of a mean process and a stationary error. Other authors such as Diebold and Inoue (2001) and Granger and Hyung (2004) consider that structural breaks and long memory are highly related and that it is difficult to distinguish one from another. The testing procedure described in Section 2 enables us to consider unit and fractional orders of integration and has standard null and local limit distributions. This is a distinguishing feature of the tests compared with other methods for testing, such as unit root tests, where the limit distribution is non-standard, in the sense that critical values have to be calculated numerically in a case by case simulation study. Moreover, this standard limit distribution holds regardless of the inclusion or non-inclusion of deterministic components and thus is not affected by the inclusion of multiple mean shifts, as will be the case in the present study. We focus on the US and the UK in particular since several studies (e.g., Alogoskoufis and Manning 1988) have found that US unemployment typically displays lower persistence than in European countries such as the UK. The latter is a particularly interesting case because of the mixed evidence on whether persistence has decreased or increased there since the early 1980s when labour market reforms (aimed at eliminating rigidities) were implemented by the Conservative government led at the time by Mrs. Thatcher. Contrary to what one would expect, some authors, such as Blanchflower and Freeman (1994), have reported a slower transition from unemployment to employment in the Thatcher years. The outline of the paper is as follows: In Section 2 we briefly describe the procedure employed here. Section 3 applies this method to examine the orders of integration of the UK and US unemployment rates allowing for multiple shifts. Section 4 contains some concluding comments.",5
33,4,Journal of Economics and Finance,02 April 2008,https://link.springer.com/article/10.1007/s12197-008-9025-7,Does time have value? An empirical examination of the put option embedded in refundable U.S. air fares,October 2009,Ray R. Sturm,Drew B. Winters,,,Male,Unknown,Mix,,
33,4,Journal of Economics and Finance,23 July 2008,https://link.springer.com/article/10.1007/s12197-008-9041-7,"Real exchange rate, stationarity, and economic fundamentals",October 2009,Angelos Kanas,,,Male,Unknown,Unknown,Male,"This paper examines the issue of whether the US/UK real exchange rate is stationary or not, using a new econometric framework and monthly data over the period 1921–2002. We consider an extension of the Augmented Dickey Fuller (ADF) test which allows for stochastic switching across regimes; switching is allowed to depend on theory-implied economic fundamental variables such as the real interest rate differential, the real output differential, and the volatility of the nominal exchange rate. We find strong evidence of stochastic switching across two regimes, and regime (non)stationarity: one regime corresponds to epochs during which the real exchange rate is stationary and PPP holds as a long-run equilibrium theory, which we refer to as the ‘stationary’ or the ‘equilibrium’ regime. The other regime corresponds to epochs during which the real exchange rate is non-stationary and PPP does not hold, which we refer to as the ‘non-stationary’ or ‘disequilibrium’ regime. The probability of transition from the disequilibrium to the equilibrium regime is found to be dependent upon the real interest rate differential and the volatility of the nominal exchange rate, whilst the real output differential does not affect the transition probability. The results indicate that the US/UK real exchange rate is characterized by a stochastic unit root: its dynamics are different across different periods and switch stochastically from a stationary process to a random walk process. This is in line with Genberg (1978), Edison (1987), and Engel and Kim (2001) who argued that the data generation process for the US/UK real exchange rate may be changing over time. Furthermore, this finding may explain the sample-dependence in the results of empirical studies on the US/UK real exchange rate and thus, bridge the gap between studies which found evidence of stationarity and other studies which found evidence of non-stationarity over different sample periods (Lothian and Taylor 1996; Glen 1992; Adler and Lehmann 1983; Corbae and Ouliaris 1991). The result that the nominal exchange rate volatility is significant in affecting the probability of this switch is in line with the literature suggesting that PPP may hold better during periods characterized by fixed or pegged (less volatile) exchange rates (Genberg 1978; Bleaney et al. 1999; McNown and Wallace 1989; Liu 1992). The finding that the real interest rate differential is also important in the transition probability is in line with Dumas (1992), and implies that it should be considered as a determinant variable of the data generation process of the real exchange rate. With regard to the theoretical literature on PPP, this paper belongs to the strand of literature suggesting that the real exchange rate dynamics should be modeled within a multivariate, rather than a univariate, framework by incorporating economic fundamentals (Goswami et al. 2004). Dumas (1992), using a general equilibrium real/financial model, concluded that the expected real exchange rate change depends on the real interest rate differential. An alternative theoretical approach, which explains the real exchange rate behavior in terms of the real interest rate differential, emphasized monetary aspects and sticky prices (Dornsbusch 1976; Frenkel 1976). In contrast to this approach, other researchers (Clarida and Gali 1994; Meese and Rogoff 1988; Davis and Miller 1996), in exploring real exchange rate dynamics, emphasized that shocks arising from the economy’s real side, as opposed to the monetary side, should receive attention. Finally, another strand of theoretical literature, in explaining the causes of PPP’s failure, paid attention to the merits of fixed versus floating exchange rate systems as reflected in the volatility of the nominal exchange rate. In general, poor performance of PPP has been attributed to highly volatile nominal exchange rates (Giovannini 1988; Marston 1990; Knetter 1993), and vice versa (McNown and Wallace 1989; Engel and Rogers 2001). With regard to the empirical literature on PPP, the present paper belongs to the strand of literature referring to stochastic unit root processes introduced in Granger and Swanson (1997) and Leybourne et al. (1996), and further elaborated in Rahbek and Shephard (2002). In the stochastic unit root approach, there are epochs of stationarity and epochs of non-stationarity. Applied to PPP, this approach corresponds to the idea that there are periods of disequilibrium, during which PPP does not hold, and periods of equilibrium, during which PPP holds. Neither regime lasts forever. For instance, during the disequilibrium regime, and according to Dumas (1992), a large real interest rate differential, representing a PPP deviation, is an indication of strong expected reversion in the deviation. This suggests that the disequilibrium regime may switch to the equilibrium regime, and vice versa, in a stochastic manner. An interesting empirical issue that arises from this, is to test for stochastic switch and regime-dependent stationarity, and to assess the importance of monetary or real sector variables in affecting the transition probability. The present paper is an extension of the related strand of literature on nonlinear unit root tests to assess PPP’s validity.Footnote 1 Nonlinearity in ADF tests is usually incorporated by allowing the autoregressive parameter to vary. In the empirical literature, there has been an extensive use of Threshold autoregressive (TAR) models (Michael et al. 1997; Taylor et al. 2001; Sollis et al. 2002; Sarno and Taylor 2002), with thresholds being justified by transaction costs. Such studies allow for switching around a threshold real exchange rate and, as such, they adopt a univariate framework which is not consistent with the literature emphasizing a multivariate framework. In addition, Pippenger and Goering (1993) and Johansson (2001) conclude that the existence of transaction costs significantly diminishes the power of unit root tests based on thresholds. This study adopts an extension of the stochastic nonlinear ADF (Hall et al. 1997), within a multivariate framework. It is multivariate in the sense that the transition probabilities are allowed to be governed not by threshold effects but by the real interest differential, the real output differential, and the nominal exchange rate volatility. Importantly, exploring the role of these variables in explaining regime transition is different from incorporating them directly into the model as factors. This is so as, in the present approach, there is no need to specify a data-generating process of these variables, which are treated as exogenous to the model as part of the information set. Thus, the open question of whether the real interest differential and real output differential are stationary or non-stationary does not arise here. The non-requirement of the stationarity property is an important advantage of the current approach, as analyzed by Ng and Perron (1997), and further discussed by Serletis and Gogas (2004), Coe and Serletis (2002), and Serletis and Zimonopoulos (1997). The remainder of the paper is structured as follows. The next section outlines the econometric methodology and the theoretical underpinnings justifying the selection of economic fundamental variables. The third section presents the empirical findings. Finally, the fourth section concludes.",3
33,4,Journal of Economics and Finance,26 July 2008,https://link.springer.com/article/10.1007/s12197-008-9054-2,Predicting stock splits with the help of firm-specific experiences,October 2009,Kevin Krieger,David R. Peterson,,Male,Male,Unknown,Male,"Stock splits have been a popular topic throughout finance literature for many years. A stock split is theoretically meaningless. Stockholders receive additional shares in a firm, but their decreased value should perfectly offset their increased number. Nevertheless, stock splits still command much attention from academics. A transaction that should have no effect on firm value provides a near-perfect window through which to view the efficiencies of the market. We take an alternative approach to the topic of splits. Because there is considerable evidence that stock splits are accompanied by immediate and/or long run abnormal returns, it may be beneficial to predict stock splits before they occur. There are a number of variables that may be used to predict stock splits. Firms with greater share price or earnings growth over recent years may be natural candidates to split. In the limited prediction literature, these variables are commonly employed. We look beyond the natural quantitative measures typically linked to split predictions to see if firms’ split experiences influence their future decisions to split. Specifically, we explore whether firms that enjoy significant abnormal returns following a previous split, or firms from industries that have recently had favorable stock price responses to splits, are more apt to split. Our goal is to develop a better model to predict splits by focusing on firms that have split before, including their past split returns and recent industry split returns, and to examine the profitability implications of such a model. Fama et al. (1969) claim that stock splits affect stock prices solely because of their association with anticipated future dividends. Grinblatt et al. (1984), however, take issue with Fama et al.’s use of monthly data and the contamination of their sample by confounding events. Grinblatt et al. find significant announcement period abnormal returns that are unrelated to cash dividend policy. They contend, the split may be a signal of higher future firm cash flows. One motivation for splits may be that investors benefit from shares trading in a certain price range. Baker and Gallagher (1980) survey CFOs and 94% profess to splitting, at least in part, to reach an optimal trading range. Lakonishok and Lev (1987) and McNichols and Dravid (1989) both believe there are costs to splitting and that firms do target a trading range. Ikenberry et al. (1996) note that signaling and trading range hypotheses need not be mutually exclusive. Managers may want to push their shares to a desirable trading range, but will only do so when they are relatively confident in the firm’s future prospects. Ikenberry, Rankine, and Stice find abnormal returns of 3.38% around the announcement date, and 7.93% (12.15%) for the 1 (3) year following the announcement of a stock split. Desai and Jain (1997) find similar results. Nayak and Prabhala (2001) find that 46% of the split effect is due to increased dividends, leaving a significant portion to signaling or other causes. Ikenberry and Ramnath (2002) see an abnormal upward drift of about 9% for 1 year after the split in the 1986 to 1997 period. The overall evidence indicates positive abnormal returns are typically associated with stock splits. The profitable trading opportunities associated with stock splits have led to limited studies attempting to predict their announcements. Ezell and Rubiales (1975) match 875 split firms with randomly selected firms. They employ a number of predictors and a discriminant scoring system to distinguish between the “split” and “non-split” firms. The authors hypothesize that those firms with a previous split are more likely to split again, and accordingly, they include an independent variable denoting whether the firm had previously split. They do not, however, consider return performance following the previous splits. Ezell and Rubiales correctly identify the split firm from each pair approximately 72% of the time, significantly higher than 50%, but not an overwhelmingly successful rate considering that the non-split firms are chosen at random.Footnote 1 Growth in share price (relative to the market), growth in earnings, and the retained earnings level of the firm are the most useful indicators. While the effect of previous splits is insignificant, Ezell and Rubiales’ confirmation of some predictability provides an early indication that historical data may be useful in split forecasting.Footnote 2
 Hwang et al. (2005) use a probit modeling technique to predict when stocks might split. They use shares outstanding, stock price, returns, book value, and dividend and earnings growth rates to predict splits, and find all variables to be statistically significant in most regressions. They are able to predict 30% of splits in a 1 year horizon, but also find that abnormal returns cannot be obtained through various split-motivated trading strategies, consistent with market efficiency. Firms that meet the quantitative characteristics usually exhibited by “splitters”, but do not split, often have negative abnormal returns in the following months. Firms that do split have positive abnormal returns when the split is a surprise (not forecasted based on the quantitative measures), but not when the split was predicted. While the study by Hwang et al. (2005) makes a useful contribution, they do not include variables which may indicate an increased or decreased likelihood of a split due to a firm’s unique experience. Firms that enjoyed large abnormal returns subsequent to a previous split may be more apt to split again. A company whose competitors have undergone “rewarding” splits may be more likely to attempt a split as well. We contend that split forecasting ability may improve if data is included incorporating such firm-specific situations. Dilotte and Manuel (1996) look at firms that split their shares at least twice between 1970 and 1988. They find that the market responds more positively (negatively) at the second split announcement of firms that underwent high (low) earnings growth following their first split. Moreover, if a firm surprises the market by posting high (low) earnings growth following a second split after disappointing (pleasing) the market in the first split, the market reacts to such news with more extreme positive (negative) abnormal returns. Dilotte and Manuel (1996) note the market observes the performance associated with previous splits of a firm and reevaluates share price, after a second split, in concordance with the expectations arising from the original split. If this is the case, we also expect that firms themselves might consider their previous split history and may engage (fail to engage) in a subsequent split, in part, due to an earlier positive (negative) market reaction. We develop an annual stock split forecasting model including, as independent variables, both the stock price response to prior splits and the stock price performance of other firms in the potential splitting firm’s industry following splits. We are able to predict splits with a greater degree of accuracy than prior studies. Furthermore, we provide evidence that our prediction model leads to significantly greater returns in the next year for firms predicted to split as compared to those predicted not to split. Thus, in contrast to earlier findings, we conclude that the market is inefficient with respect to the predictability of stock splits. We develop the paper as follows. In section 2 we discuss the data, and in section 3 we develop the methodology to predict splits. We present the prediction results in section 4 and the stock return results in section 5. We conclude in section 6.",2
33,4,Journal of Economics and Finance,18 June 2008,https://link.springer.com/article/10.1007/s12197-008-9039-1,"Age, investing horizon and asset allocation",October 2009,James P. Dow Jr.,,,Male,Unknown,Unknown,Male,"One of the basic questions in household finance is how age should affect the allocation of household assets. Standard financial advice is that the share of assets held as stock should diminish as the investor gets older. Reasons given for this include the possibility that labor income allows investors to adjust to poor investment results (Bodie, Merton and Samuelson 1992) and also the presence of negative serial correlation in stocks over the long run could cause investors with longer investment horizons to face less risk (Cochrane 1999). However, in contrast to standard advice, empirical evidence suggests that the fraction of assets held as stock increases as investors get older. One difficulty in judging the importance of this result is that the theoretical models make predictions based on investment horizon while the empirical research uses investor age. While it is plausible that younger investors have longer investment horizons, it may not necessarily be the case. For example, younger investors may be saving for a house or for college for their children giving them relatively short investment horizons. In addition, as individuals get older they may become more financially sophisticated, and there is strong evidence that education, and so presumably financial sophistication, affects asset allocation. This paper investigates this issue using data from the Survey of Consumer Finances (SCF). The SCF asks households both what their investment horizon is and why they are investing, which can be used to separate out the effects of investment horizon and age. The survey also asks about various investment and employment activities, which can be used as proxies for financial sophistication. This paper shows that age is not simply related to reported investment horizon. Age and investment horizon are almost uncorrelated in the data set as both older and younger households tend to have shorter investment horizons. This affects how regressions of stockholding on age are interpreted. Studies that are testing the effect of investment horizon on stockholding by using age as a proxy are probably testing a different relationship instead. To investigate this, the paper regresses stockholding on age along with a number of proxies for investment horizon and financial sophistication. The inclusion of these variables substantially reduces the magnitude and statistical significance of the coefficient on age. The coefficients on the variables for investment horizon and financial sophistication are generally of the expected sign. This regression shows that the empirical evidence is not necessarily inconsistent with standard financial advice. Basic financial theory (e.g. Samuelson 1969) suggests that expected lifetime should not matter for the portfolio decisions of individuals. However, this contrasts markedly with traditional investment advice, that as individuals get older they should reduce their holdings of risky assets. Furthermore, within the category of risky assets, it is recommended that investors shift their holdings to less risky investments, such as from stocks to bonds. Both ideas contrast with the empirical evidence that tends to find stockholding increase with age, at least up to a point. Two broad approaches have been taken in response to these contradictions. First, some have argued theoretically that age should affect stockholding, either positively or negatively. Second, it may be that age is not a good proxy for investment horizon in empirical studies. In the public sphere, the theoretical justification often given for young people holding more stock is long-run negative autocorrelation in stock prices (Siegel and Hoban 1991; Cochrane 1999). Stocks are relatively less risky compared with bonds over longer horizons making them more appropriate for young investors with long investment horizons. Academic arguments have tended to focus on the interactions between labor markets and investment returns. Bodie, Merton and Samuelson (1992) point out the labor income can allow individuals flexibility in responding to poor investments results—if stock market returns are low, individuals can compensate by working more. Viciera (2001) offers an alternate implication for labor income. Risk-free labor income is in effect a “forced” investment in a risk-free asset, leading individuals to take a riskier position with their financial wealth. As individuals age, the lifetime importance of labor income diminishes and (to the extent that risk-free pension income is lower than labor income) should cause individuals to reduce the risk of their financial portfolio. In contrast, Benzoni et al. (2007) argue that labor income is cointegrated with dividend payments, so that labor income is a stock-like investment, which makes young households want to hold less of their financial wealth as stock. From simulations, they find that optimal stock holding should have a “hump shape”, first increasing with age and then decreasing with age. Cvitanic et al. (2006) also find a hump-shaped pattern for stockholding in a continuous time model of portfolio choice with labor flexibility. Heaton and Lucas (2000a, b) point out that certain kinds of jobs may have higher labor risk and so this would lead individuals with those jobs to invest less in stock. Most empirical studies of portfolio decisions include age as an explanatory variable, with the assumption that older individuals have a shorter life expectancy and investing horizon. Broadly, studies find that stockholding increases with age until near retirement. After that, results tend to be inconclusive. Cohn et al. (1975), Kullmann and Siegel (2003) and Wang and Hanna (1997) find that stockholding increases with age. Riley and Chow (1992) find that it increases with age until age 65. The evidence from Bertaut Star-McCleur suggests that being older and younger reduces stock holding compared with middle-age individuals. Ameriks and Zeldes (2001) find a similar hump shape. Shum and Faig (2006) use SCF data find a hump-shaped pattern for age. They add in dummy variables for eight different reported savings motives. They find that households with retirement as a motive (and presumably a longer investment horizon) tend to hold a larger share of their wealth as stock while those who are investing to purchase a home (presumably with a shorter investment horizon) hold less wealth in stock. Several studies have looked at how health, along with a number of other factors including age, affects the stockholding decisions of older individuals. Rosen and Wu (2004) generally find conflicting or insignificant results for age. Berkowitz and Qiu (2003) also find conflicting and insignificant results for married couples but age decreases stockholding for singles. Edwards (2003) also finds that stockholding decreases with age for this age group. Of course, there are a number of factors that age can proxy for, which can effect the interpretation of these studies. Most important may be “cohort” effects. With improvements in financial technology, the cost of investing in non-bank assets has fallen. Younger individuals may have grown up in an environment more conducive to investing in stock. Also, wealth increases with age, and for some utility functions, higher wealth implies lower risk aversion and so greater stockholding. And if there are transactions costs associated with becoming a stock holder or managing more sophisticated financial instruments, greater wealth could make individuals more likely to hold stock. Alternatively, if financial sophistication is developed by experience, in addition to formal education, older individuals may have accumulated more familiarity with financial products and may be more willing to invest in risky assets. Most studies generally include controls for wealth and education. Both higher wealth and greater education are found to increases stockholding. The importance of education suggests that financial sophistication may be important for the portfolio allocation decision. This paper uses the most recent (2004) round of the SCF. The SCF is performed triennially by the Federal Reserve and focuses on obtaining detailed financial information from the respondents. The manner in which the data is collected and reported has several special features that must be taken into account when performing statistical analysis. First, the survey oversamples wealthy households. The reason for this is certain financial decisions of interest are much more likely to be made by wealthier households and so enough households of this type need to be included in the sample to make the results meaningful. Because of this, averages calculated from the data do not reflect population averages. A second feature of the data set is that missing values are replaced by “imputed values”. The Federal Reserve uses a multiple imputation procedure (Montalto and Sung 1996), which creates five observations from each initial observation. This makes the size of the data set look five times larger then it actually is, making the standard errors smaller then they really are. The suggested procedure for dealing with this comes from Montalto and Sung (1996), which is to run five separate regressions with each imputed data set and then determine the appropriate average values of the standard errors and coefficients.",8
34,1,Journal of Economics and Finance,28 August 2008,https://link.springer.com/article/10.1007/s12197-008-9033-7,Mad Money stock recommendations: market reaction and performance,January 2010,Terrill R. Keasler,Chris R. McNeil,,Male,,Unknown,Mix,,
34,1,Journal of Economics and Finance,24 December 2008,https://link.springer.com/article/10.1007/s12197-008-9070-2,Income smoothing and foreign asset holdings,January 2010,Faruk Balli,Rosmy J. Louis,Mohammad Osman,Male,Unknown,Male,Male,"In this paper, we re-examine the ties between international portfolio allocation and income smoothing. We propose a revised approach of measuring income smoothing via foreign asset holdings that focuses on factor income inflows as opposed to the commonly used net factor income in the literature mainly proposed by Sørensen et al. (2007). A compelling reason for purely concentrating on factor income inflows is mainly that during recession periods wages, interest, and profits tend to be lower and this may entail a reduction in outflows, consequently an overestimation of net foreign factor income and its impact on income smoothing via international asset holdings. Using net factor income in our views carries the potential drawback of producing higher (lower) smoothing than normal during recession (expansion). The factor income inflow by contrast does not suffer from this shortcoming. Its movement or magnitude does not necessarily synchronize with fluctuations in domestic output. Since this paper’s primary concern is to construct the strong tie with the international portfolio allocation and international income smoothing, it is less likely to add factor outflows to get a valid estimation of income smoothing via domestic investors’ international portfolio allocation. Macroeconomic models are built on the central assumption that economic agents are either rational or near-rational. Grubel (1968) explains investors’ rationale for holding internationally diversified portfolio by looking at the mean-variance of both portfolios with purely domestic assets and portfolios with a combination of domestic and foreign assets. He shows that the mean-variance of the latter is smaller than the former. Lewis (1999) substantiates Grubel’s main findings by providing both theoretical foundation and empirical evidence. However, French and Poterba (1991) and Tesar and Werner (1995) have observed that investors in high income countries do not hold foreign financial assets as much as they should optimally. A large portion of their financial assets are from the domestic market, a behavior that is known as “home bias”. Nevertheless, over the last decade, capital market integration has grown tremendously leading to higher volumes of international assets trading across borders. This has led to a downward trend in home bias levels, in particular among high-income OECD members. In aggregate level data, Sørensen et al. (2007) have recently shown that there is a strong connection between the volume of cross-border assets holding and income smoothing. More intuitively, this implies that the more internationally diversified an investor’s portfolio is the higher possibility to smooth income as they are able to switch income from the foreign markets to the domestic market to keep their levels of consumption relatively stable over time at home.",3
34,1,Journal of Economics and Finance,01 November 2007,https://link.springer.com/article/10.1007/s12197-007-9015-1,Market undervaluation of risky convertible offerings: Evidence from the airline industry,January 2010,Vitaly S. Guzhva,Kseniya Beltsova,Vladimir V. Golubev,Male,Female,Male,Mix,,
34,1,Journal of Economics and Finance,11 October 2008,https://link.springer.com/article/10.1007/s12197-008-9066-y,"Academic content, research productivity, and tenure",January 2010,Jorge Brusa,Michael Carter,George E. Heilman,Male,Male,Male,Male,"The relationship between the publication records of graduates and the tenure process has been a topic of interest in many different fields. For example, Christensen et al. (2002) report that Accounting graduates publish on average 1.95 research articles during the first 7 years of employment. Denis et al. (2006) indicate that 11% of graduates in Information System (IS) had published 1or 2 articles in the 20 elite IS journals within 6 years of graduation, while only 2% of the IS graduates had 3 or more top articles in the same 6 year period. In the field of Economics, Hamermesh et al. (1982) indicate that not only the quantities, but also the qualities of articles published are important factors for compensation, promotion and tenure. Davis et al. (2001) find that, while publishing success depends on the quality of the graduate school attended and the type of the employer, research output begins to decline after tenure. Bess (1998), however, finds no reduction in research productivity after tenure, although Oster and Hamermesh (1998) do indicate that publications in leading journals decline sharply with age. In the field of Finance, Zivney and Bertin (1992) report that the top 10% of the graduates will have approximately 6 publications by the time of the tenure decision while those in the 50th percentile will have 2 publications by the same time. Fishe (1998) extends from the previous study and investigates the research standards required to become a full professor of Finance. His results show that full professors in top-ranked programs place an average of 1/3 of their articles in top finance journals while full professors at lower-ranked programs place only about 1/6 of their articles in the same journals. Griffiths and Winters (2005) reach similar conclusions. Researchers in several business disciplines, including Finance, have looked for patterns and other institutional factors that may help explain variations in pre- and post-tenure publication records (Hasselback et al. 2000; Powers et al. 1998; Williamson and Cable 2003). One common finding across all business areas is that the prestige of a graduate’s academic origin and affiliation significantly affects the placement of his/her research (Fogarty and Ruhl 1997; Long et al. 1998; Williamson and Cable 2003). For example, Chen and Huang (2007) find that, researchers in elite programs in Finance co-author more articles with people from non-elite programs than from within elites. While the prestige effect has a significant influence on publication record, other influential factors have also been identified. For example, Brusa et al. (2006) show that the academic content of a Finance program has a significant impact on the publication records of its graduates. Specifically, their results indicate that the number of courses in Investments, Macroeconomics, Microeconomics, Continuous Time Finance, and Stochastic Calculus has a positive influence on measures of research productivity. Their results also reveal that, while the majority of programs require courses in Investments (94.64% of programs), Microeconomics (94.64% of programs) and Macroeconomics (82.14% of the programs) only a few require courses in Continuous Time Finance (17.86% of programs) and Stochastic Calculus (35.71% the programs). The significant influence of courses and the different program requirements found by Brusa et al. (2006) trigger an interesting question: At the time of the tenure decision, can we observe differences in research productivity between graduates from the more mathematically oriented programs (those requiring classes in Continuous Time Finance and/or Stochastic Calculus) and graduates from programs without these requirements? Students graduating from mathematically oriented programs may have a different level of knowledge that provides the basis for a different level of productivity. In other words, programs that are more mathematically oriented may provide students with additional knowledge and technical skills that establish a foundation for higher levels of research productivity. The main objective of the investigation is to examine this issue. More precisely, the goals of this study are to: (1) examine the publication records of the graduates from these programs on a yearly basis, (2) determine the differences among the publication records of the graduates by the time of the tenure process, and (3) investigate whether differences in programs’ academic content explain differences in publication records at the time of the tenure process. To attain these goals, we investigate the publication records of graduates from 56 doctoral programs in Finance. Twenty-five of these programs require Continuous Time Finance and/or Stochastic Calculus and 31 programs do not. We assume that programs requiring Continuous Time Finance and/or Stochastic Calculus are more mathematically oriented than programs without these requirements. We hypothesize that the more mathematically oriented programs may provide students with additional knowledge and technical skills that contribute to a higher level of research productivity and different records of publication by the time of tenure. To test this hypothesis and assess the influence of program content on research productivity, we examine the publications of 888 individuals who graduated from doctoral programs in Finance between 1990 and 2004. The results indicate that when compared to their counterparts, the graduates from programs requiring more mathematically oriented courses exhibit more publications in the leading Finance journals by the time of tenure. Also, the results show that the academic content required by the programs has a significant influence on the publication records of the graduates by the time of tenure. The results of these investigations would be of interest to different members of academia. For example, the results could be relevant to directors of doctoral programs, future doctoral students, and administrators in charge of recruitment, promotion, and tenure. The remainder of the paper is organized as follows. In Section 2 we examine the research productivity of the graduates and in Section 3 we compare their performance during the mid-time tenure review process and the tenure process. In Section 4 we examine the influence of the academic content offered by the programs on the research productivity of the graduates by the time of tenure. In Section 5 we conclude and summarize the paper.",2
34,1,Journal of Economics and Finance,10 October 2008,https://link.springer.com/article/10.1007/s12197-008-9065-z,The long-term success of M&A in the automotive supply industry: determinants of capital market performance,January 2010,Jan-Peer Laabs,Dirk Schiereck,,Unknown,Male,Unknown,Male,"Over the last decades, wealth creation through mergers and acquisitions (M&A) has been extensively discussed in empirical finance research. With a variety of different approaches and foci, authors focusing on short-term announcement effects unambiguously conclude that mergers and acquisitions seem to create value. However, as this short-term value creation potential is mostly attributed to the shareholders of the target companies (Bradley et al. 1988, p. 31), a closer look at the returns to acquiring firms reveals a different pattern: While short-term announcement returns for acquiring companies average at around zero (Bruner 2002), long-term post-merger returns even indicate significant value losses on the acquirer side.Footnote 1 In the light of on-going merger activity and consolidation, these negative reactions pose a challenge to the management of merging companies and call for a comprehensive list of determinants for the direction and magnitude of the experienced value effect. Towards the turn of the last century, many industries including the automotive supply industry are facing increasing merger activity. The pressure to produce better equipped and less expensive automobiles has created a growing trend towards specialization and internationalization of the industry. While some product ranges such as braking components are now dominated by very few players, acquisitions and foreign direct investments have also led to geographical expansion of players across borders and across continents (Sadler 1999). In the light of these specific industry characteristics, previous research in the automotive supply industry has shown that acquiring companies are able to realize significant positive short-term returns as an expression of the global synergy and efficiency potential underlying the transactions (Mentz and Schiereck 2006). However, the question, whether acquirers are able to sustain these exceptional positive returns beyond a short-term announcement window, remains open. Due to various methodological difficulties associated with studies on long-term abnormal performance, evidence from other industries remains scarce or narrows its focus to either one of the time horizons (short- vs. long-term) or a single deal characteristic (method of payment, cross-border). In contrast to previous research, our study determines the short- and long-term performance of acquiring firms in the automotive supply industry, uses a combination of Buy-and-Hold-Abnormal-Returns (BHARs) and expected returns based on the Fama-French-3-Factor model to determine statistically reliable indications of long-term performance, and analyses a comprehensive list of deal and acquirer characteristics for their respective impact on the long-term wealth effect. The objective of this study is twofold: After updating previously published announcement effects on acquirers in the automotive supply industry, the short-term perspective is expanded by long-term abnormal returns based on event time (BHARs) and calendar time (Fama-French 3-Factor model). Then the observed long-term return patterns are examined to detect and categorize determinant variables. The main focus lies in determining an effect based on differences in geography, product range, transaction size, and bidder experience. The remainder of this paper is organized as follows: Section 2 provides a brief overview of the relevant literature and outlines the derived hypotheses for the short- and long-term value creation effects on acquiring companies in the automotive supply industry. Section 3 presents the applied methodology as well as the sample selection procedure. The following Section 4 contains the empirical results including the overall short-term announcement effects and the long-term performance as well as the analyses concerning different value drivers. Section 5 complements the results with a cross-sectional analysis before Section 6 summarizes the findings and concludes.",18
34,1,Journal of Economics and Finance,16 December 2008,https://link.springer.com/article/10.1007/s12197-008-9069-8,A note on the relation between the equity risk premium and the term structure,January 2010,Angelos Kanas,,,Male,Unknown,Unknown,Male,"The equity risk premium is linked with the term structure through the business cycle, with high expected equity premia and upward sloping term structures both being associated with recessions, and small premia and inverted term structures with expansions (Fama and French 1989; Harvey 1988; Estrella and Hardouvelis 1991). Empirical evidence on the relation between the two variables has focused mainly on linear models (Fama and French 1989; Campbell 1987; Clare and Thomas 1994; Shah and Wadhwani 1990), and contended that expected stock returns are positively related to bond maturity spreads. In addition, Harvey (1988) has emphasized the asymmetric role of upward versus downward sloping term structures. Departing from linearity, Boudoukh et al. (1997) documented for the USA that the relation between the equity premium and the slope of the term structure is nonlinear. The present paper examines the relation between the equity premium and the slope of the term structure by introducing the specific type of nonlinearity which arises from regime switching in equity premium dynamics. Different regimes in equity premium are justified on the following grounds. Firstly, business cycles are linked with stock market fluctuations and thus, may trigger regime switching in equity premium dynamics. Second, Whitelaw (2000) has shown that consumption growth is characterized by different regimes with implications on equity premium dynamics through its dependence on the marginal rate of consumption substitution. Finally, there is extensive direct evidence supporting the existence of regime switching in equity premium (Guidolin and Timmermann 2005; Meenagh, et al. 2007). Using a data set spanning the period 1900–2006 for the USA, the UK, and Japan, we model the relation between the equity premium and the slope of the term structure by incorporating volatility regime switching in the equity premium, and by allowing for asymmetric effects between positive and negative term structures within each regime. For all three countries, we document that asymmetric effects upon the equity premium are uncovered only if regime switching is allowed in equity premium dynamics. Predictive power for the equity premium arises only from the positively sloped term structure and only in the low volatility regime of the equity premium, suggesting an asymmetric regime-dependent relation between the two variables.",1
34,1,Journal of Economics and Finance,26 September 2008,https://link.springer.com/article/10.1007/s12197-008-9063-1,Are day traders bias free?—evidence from internet stock message boards,January 2010,Ying Zhang,Peggy E. Swanson,,,Female,Unknown,Mix,,
34,2,Journal of Economics and Finance,11 July 2009,https://link.springer.com/article/10.1007/s12197-009-9089-z,"Open interest, volume, and volatility: evidence from Taiwan futures markets",April 2010,Stéphane M. Yen,Ming-Hsiang Chen,,,Unknown,Unknown,Mix,,
34,2,Journal of Economics and Finance,09 July 2009,https://link.springer.com/article/10.1007/s12197-009-9093-3,Unemployment and compensating wages: an analysis of shift work,April 2010,Ronald DeBeaumont,Christian Nsiah,,Male,Male,Unknown,Male,"Labor market theory suggests workers may require a wage premium to accept jobs with unpleasant characteristics. The theory of compensating wages has been tested with respect to a number of job attributes, the results of which have been mixed. There is a significant amount of evidence in support of compensating wages based on risk of injury or death on the job. For some examples, see Viscusi and Aldy (2003), Black and Kniesner (2003), and, for gender specific risk, Hersch (1998). Additionally, compensating wages have been found for other negative job attributes such as stress [French and Dunlap (1998)], noisy work environment [Daniel and Sofer (1998)], and risk of job loss [Moretti (2000) and Magnani (2002)]. As pointed out in Rosen (1986), the theory of compensating wages rests on the assumption that the number of individuals who are indifferent to a particular job characteristic must not be large enough to negate the need for a wage premium. All else constant, employers in regions where unemployment is relatively high should be able to choose from a larger labor pool, potentially reducing the need to offer a compensating wage. There is evidence that employers are more selective during periods of high unemployment. For instance, Kandil and Woods (2002) find that the earnings of women relative to men improve during times when unemployment is low. Boushey (2002) provides similar results for black workers, and Hotchkiss and Pitts (2007) find the labor market intermittency penalty rises during times of labor market weakness. These studies refer to worker characteristics that businesses may find relatively less appealing, where compensating wages arise for job characteristics that workers find undesirable. As unemployment increases, however, there would be fewer opportunities to choose from, and thus workers would presumably be less picky. The goal of this study is to investigate the role that unemployment plays in determining compensating wages. Although previous research typically adjusts for region and occupation, no study specifically tests the impact the local unemployment rate has on compensating wages. This is of potential importance, however, as high unemployment regions may differ significantly in the relative magnitude of compensating wages. If so, workers in weak labor markets would find it more difficult to supplement their pay by accepting otherwise undesirable jobs. With respect to wage premiums for night shifts, we indeed find that local labor market conditions matter. Specifically, as unemployment increases, the wage advantage of working a night shift declines.",2
34,2,Journal of Economics and Finance,26 July 2008,https://link.springer.com/article/10.1007/s12197-008-9052-4,An analysis of Australian exchange traded options and warrants,April 2010,William J. Bertin,Paul Fowler,Laurie Prather,Male,Male,Female,Mix,,
34,2,Journal of Economics and Finance,03 June 2009,https://link.springer.com/article/10.1007/s12197-009-9088-0,Distribution of income and expenditures across nations,April 2010,Edward Nissan,Farhang Niroomand,,Male,Unknown,Unknown,Male,"According to Dean (1978), the theory of value and its associated theory of distribution are key features of any methodological or philosophical approach in economics. But it was Karl Marx who conceived the idea of surplus value that emerges from the process of production. It is the difference, according to Mandel (1968a), between the product of labor and its cost. Labor produces surplus above its wages. So, in an exchange economy such as the capitalist system where population is divided into owners of capital and sellers of labor, labor becomes alienated because of exploitation, resulting in revolution, with the ultimate goal of a communist system. Marxism spawned a great variety of ideas that seem plausible to correct for the exploitation of labor. This article intends to explore one of them, state planning. The paper will proceed after this introductory section with Section 2 undertaking a discussion of the rise of the New Left, (radial economics), followed by Section 3, which undertakes state planning. The final section provides conclusions.",
34,2,Journal of Economics and Finance,16 September 2009,https://link.springer.com/article/10.1007/s12197-009-9104-4,Covariance estimation: do new methods outperform old ones?,April 2010,Lan Liu,Hao Lin,,,,Unknown,Mix,,
34,2,Journal of Economics and Finance,09 May 2009,https://link.springer.com/article/10.1007/s12197-009-9085-3,Do Wall Street economists believe in Okun’s Law and the Taylor Rule?,April 2010,Karlyn Mitchell,Douglas K. Pearce,,Female,Male,Unknown,Mix,,
34,2,Journal of Economics and Finance,25 February 2009,https://link.springer.com/article/10.1007/s12197-009-9080-8,Openness and the speed of adjustment in the money market,April 2010,Sahar Bahmani,,,Female,Unknown,Unknown,Female,"In an effort to fight inflation or recession, central banks manipulate the money supply. Perhaps, the easiest way to understand the relation between money, income and price level is through the Quantity Theory of Money, which is widely used in conducting monetary policy. Given a stable velocity, the Quantity Theory implies that an increase in money supply is usually transmitted to an increase in price level, or the level of output, or a combination of both. One major question that needs to be addressed is at what speed these macro variables adjust toward their long-run equilibrium values and whether openness can influence the adjustment speed. Obviously, any policy that hinders the adjustment process of each variable will have an adverse impact on the speed of adjustment. On the other hand, any policy that facilitates the adjustment process will have a positive effect. The United Nations frequently talks about economic sanctions against countries that violate international law or norm of agreement. Often, sanctions are expected to curtail domestic production and slow down the adjustment of output or income, thereby hurting the country against which the sanctions are imposed. Thus, the opposite policy of more openness and less restriction that allows access to foreign resources, especially foreign inputs and technology, is expected to boost the speed of adjustment of the production process. Thus, our conjecture in this paper is to test the hypothesis that the adjustment speed and economic openness are directly related. For this purpose, we introduce our testable model and methodology in Section 2. The empirical results supporting the main theme of this paper are reported and discussed in Section 3. Section 4 summarizes the findings and concludes. Data definition and sources are provided in an Appendix.",
34,3,Journal of Economics and Finance,09 August 2008,https://link.springer.com/article/10.1007/s12197-008-9056-0,The economic profitability of pre-IPO earnings management and IPO underperformance,July 2010,Yan Xiong,Haiyan Zhou,Sanjay Varshney,Male,Unknown,Male,Male,"Earnings management has long been a concern of academics. Management’s use of judgment in financial reporting has both its benefits and costs. It is beneficial to financial statement users if the reporting methods and policies selected by managers reflect the firm’s intrinsic value. In this case, managers utilize their knowledge about their own business to increase the effectiveness of financial reporting as a form of communication. However, the judgment allowed in financial reporting can also create opportunities for managers to choose reporting methods and policies that mislead financial statement users. In recent years, especially in regards to issues surrounding the financial reporting of publicly traded firms, earnings management has received tremendous attention by both the popular press and accounting regulatory agencies. To address this concern, the SEC formed an earnings management task force in 1998 to trace firms that use creative accounting methods to manipulate their earnings.Footnote 1 In addition, accounting academia has been asked by the SEC to provide information on whether existing disclosure requirements are useful in controlling earnings management and whether additional standards are needed to control for earnings management.Footnote 2
 To get information necessary for effective standard setting on earnings management, it is essential for standard setters to identify firms that have the opportunities and incentives to manage earnings and then determine whether the consequence of earnings management is serious enough to set new reporting standards. The purpose of this study is to test whether investors are able to use their knowledge of the IPO underperformance anomaly combined with the estimates of pre-IPO earnings management to systematically outperform the market. There are two anomalies that are related to IPO process. Many studies have examined the IPO underpricing and IPO underperformance phenomena. The IPO underpricing refers to the fact that IPOs’ initial offer prices are consistently lower than their end-of-first-day market prices while the IPO long-run underperformance refers to the fact that IPO firms’ long-run stock returns, measured for 3 to 6 years after the initial offerings, are significantly less than those of a matched sample of non-IPO firms (Ritter 1991). Prior literature treats the IPO underpricing and IPO underperformance anomalies as two separate phenomena and attempts to use different theories to explain them. A number of explanations have been offered for IPO underpricing (Baron 1982; Ritter 1984; Titman and Trueman 1986; Tinic 1988). However, with a few exceptions, these hypotheses have been dismissed due to either conflicting empirical evidence or the lack of sound theoretical foundations (Lowry and Murphy 2006; Drucker and Puri 2005; Xiong et al. 2005). The four hypotheses listed below are the prevailing hypotheses that provide reasonable and empirically testable explanations for IPO underpricing: asymmetric-information hypothesis (Baron 1982), monopsony-power hypothesis (Ritter 1984), the signaling hypothesis (Titman and Trueman 1986), and the lawsuit avoidance hypothesis (Tinic 1988). Additional hypotheses of IPO underpricing tend to be merely special cases of these four. While some support has been indicated for each of the hypotheses, all four have also been widely criticized and no one hypothesis is universally accepted. It is likely that these hypotheses are not mutually exclusive, thus explaining the partial support for all. Thus, the underpricing of IPOs remains an unexplained anomaly. One interesting commonality among these theories is that all of them assume that IPO offer prices are purposely set below the underlying intrinsic value. None of these theories consider the possibility that the initial offer prices may be correct and that the first-day run ups are due to systematic short-term overvaluation by the secondary market. A recent study (Xiong et al. 2005) that proposes and tests an alternative explanation for this phenomenon, a hypothesis based on the role of earnings management as an explanation of IPO underpricing. Contrary to current beliefs, Xiong et al. (2005) find that IPOs are not initially underpriced, but that instead, the commonly observed short-term run up in price is the result of unsophisticated investors in the secondary market being temporarily fooled by the pre-IPO earnings management. The IPO underpeformance anomaly appears more consistent with this short-term overvaluation theory than other underpricing theories found in the extant literature. This may be due to the facts that the market could only be temporarily fooled and that upward managed earnings have to be reversed in the future periods. One major advantage of this short-term overvaluation theory is that it is able to provide consistent explanations for both phenomena. Many studies have examined the IPO long-run underperformance phenomenon. Ritter (1991) conjectures that this phenomenon is the result of investors’ overoptimism about the earnings potential of young growth companies. Teoh et al. (1998a) explore whether earnings management is a possible source for this overoptimism. They find that there is a negative association between the IPO firms’ earnings management and their subsequent stock returns over 3 years. More significantly, the studies demonstrate that at-issue discretionary accruals reverse in subsequent years and therefore, could be used to predict the cross-sectional variation in the post-issue underperformance of IPO firms. While prior studies demonstrate general IPO underperformance and IPO underpricing phenomena, no study has attempted to determine whether these patterns can be used to generate an actual trading strategy that will significantly outperform the market. The primary focus of earnings management research to date has been on detecting whether and when earnings management takes place (Healy and Wahlen 1999). Healy and Wahlen (1999) suggest that the area of earnings management research would be more fruitful if it provided evidence on the impact of earnings management on the capital markets. The current study’s investigation of the impact and magnitude of earnings management on IPO firms’ long-term stock performance provides relevant valuable information on this issue. The results of our study provide information on both the prevalence of earnings management and its impact on the capital markets under circumstances in which the market can likely be, at least temporarily, fooled. Thus, our results help the regulators gauge the effectiveness of current disclosure regulations and take further measures to protect the public interest in the market. This study tests the market performance of a zero-investment trading strategy based on the knowledge of IPO underperformance and estimates of pre-IPO earnings management. Our study is different from other studies in the literature in (1) implementing a zero-investment trading strategy based on the IPO underperformance phenomenon and the estimates of pre-IPO earnings management and (2) controlling for the constraints on short-selling imposed by the Federal Reserve Board under the Securities Exchange Act of 1934. This study contributes to the finance and accounting literature by providing empirical evidence on market performance of a trading strategy related to earnings management in the IPO market, that is, whether investors’ perception of earnings management in pre-IPO period is of a magnitude by which abnormal returns can be earned. In addition, the study extends the current literature by examining a trading strategy based on the IPO underperformance, which relate investors’ perception of IPO underperformance to the market performance of trading behaviors. This study has important implications for standard setters in determining whether new standards and/or additional required disclosures are needed to control for earnings management in the prior- IPO period.Footnote 3 Accounting standard setters may also find this study useful for evaluating the level of flexibility they should allow in accounting standards which provide rooms for corporate managers of IPO firms to manipulate reported earnings. Furthermore, this study provides relevant information for investors. As a group, investors have expressed concerns about firms’ use of earnings management. IPO firms are likely candidates to employ earnings management because of the information asymmetry between investors and issuers prior to their public offerings. Investors in the IPO market, concerned about the relevance of earnings for determining a new security’s value, can use the evidence from this study to better evaluate firms’ pre-IPO reported earnings. By identifying specific relationships between earnings management and IPO initial price changes and possibly developing trading strategies to capitalize on this knowledge, investors should be better able to make informed decisions regarding the earnings reported in IPO prospectuses. Finally, this study provides evidence to market regulators such as the SEC in addressing their concerns over earnings management. The SEC is currently examining new disclosure requirements and has also formed an earnings management task force to regulate earnings management. To achieve their goals, the SEC must determine the level of discretion that managers should be allowed to exercise in financial reporting. One of the Information that SEC must rely on when making its decision is the impact of earnings management on stock markets (Healy and Wahlen 1999). The current study’s investigation of the impact and magnitude of earnings management on IPO firms’ long-term stock performance provides relevant valuable information on this issue. The results of our study provide information on both the prevalence of earnings management and its impact on the capital markets under circumstances in which the market can likely be, at least temporarily, fooled. Thus, our results help the regulators gauge the effectiveness of current disclosure regulations and take further measures to protect the public interest.",7
34,3,Journal of Economics and Finance,16 July 2008,https://link.springer.com/article/10.1007/s12197-008-9050-6,An investigation of the weekend effect during different market orientations,July 2010,Denis Boudreaux,Spuma Rao,Phillip Fuller,Male,Unknown,Male,Male,"Of the many seasonal anomalies, very few are quite as perplexing and as well researched as the anomaly of positive Friday/negative Monday returns called the weekend effect. The weekend effect occurs when stocks display significantly lower returns from the close on Friday to the close on Monday. It has been found in markets worldwide and has been extensively documented by Fama (1965), Cross (1973), French (1980), Lakonishok and Smidt (1989), and Abraham and Ikenberry (1994). Some empirical research indicates that the negative return on Monday is caused by negative returns occurring on previous trading days. But according to Abraham and Ikenberry, negative returns on Fridays are uncommon; in fact, almost two-thirds of all Fridays produce positive returns. On those Fridays with negative returns, researchers have also found that Monday returns are negative 80% of the time. In contrast, if the Friday returns are positive, then the Monday returns are positive only 50% of the time (Abraham and Ikenberry 1994). The trading patterns of individual investors appear to be at least one of the causes for the weekend anomaly, as Miller (1988) explains in his individual investor hypothesis. Miller attributes the weekend effect to two factors. First, individual investors initiate greater activity on Mondays, after having had time over the weekend to reflect on their needs. Second, the information provided during the week by the brokerage community, which is biased toward buy recommendations, is less available on weekends (Brooks and Hongshil 1997). These two factors inevitably contribute to the higher percentage of sell orders by individuals on Monday mornings. In addition to the individual investor hypothesis, Abraham and Ikenberry suggest that, “[the weekend effect] is substantially the consequence of information released in prior trading sessions, particularly on Friday,” (Abraham and Ikenberry 1994, p. 276). After transaction costs, weekend trading is most profitable in closed end mutual funds after the market declines between 0.5% and 1.5% on Friday (Hsaio and Solt 2007). Consistent with the weekend effect, implied returns from stock index forecasts entered on weekends are significantly lower than those entered on weekdays (Theissen 2007). Brusa, Pu, and Schulman document evidence of a ‘reverse’ weekend effect—whereby Monday returns are significantly positive and they are higher than the returns on other days of the week—over an extended period of eleven years (from 1988 to 1998). They also find that the ‘traditional’ weekend effect and the ‘reverse’ effect are related to firm size in that the ‘traditional’ weekend effect tends to be associated with small firms while the ‘reverse’ weekend effect tends to be associated with large firms. In addition, they find that during the period in which the ‘reverse’ weekend effect is observed, Monday returns for large firms tend to follow previous Friday returns when previous Friday returns are positive, but they do not follow the previous Friday returns when Friday returns are negative. Furthermore, they find that during the period in which the ‘reverse’ weekend effect is observed, Monday returns are positively related to the volume of medium-size and block transactions, but negatively related to the volume of odd-lot transactions (Brusa et al. 2000, 2003a,b, 2005; Gu 2004). A study which investigated the day-of-the-week effects in the pre-holiday returns of the Standard & Poor’s 500 stock index during the period 1930–1999 was based on within-day contrasts and between-day contrasts. There were two major findings. First, the results were consistent with prior research in that there was a strong pre-holiday effect up to 1987, but the pre-holiday effect was greatly diminished after 1987. Second, contrary to that frequently observed in the literature for typical days, there was no evidence of a weekend effect in pre-holiday returns (Keef and Roush 2005). McEwan (2002) suggests a “weekend” effect exists in currency prices. Another study contends that speculative short sales contribute to the weekend effect: the inability to trade over the weekend is likely to cause short sellers to close their speculative positions on Fridays and reestablish new short positions on Mondays causing stock prices to rise on Fridays and fall on Mondays (Chen and Singal 2003). Compton and Kunkel 2000 investigated the feasibility of using individual retirement accounts to exploit well-known calendar anomalies in the financial markets. They find no evidence of either a January effect or a weekend effect which may imply that investors have traded them out of existence. However, they find a significant turn-of-the-month effect in both stocks and bonds and show that investors may be able to enhance the performance of their retirement portfolios. They demonstrate that investors using a turn-of-the-month switching strategy would have outperformed a buy-and-hold strategy in stocks or bonds. Higgins, Howton, and Perfect examined the daily autocorrelation and cross-correlation patterns of IPO returns. Evidence is found that IPO investors make buy and sell decisions on specific IPO securities based more on market returns the previous day than on returns to that security the previous day. However, despite all the research conducted on the weekend effect, no research to date has sought to determine the robustness of the weekend effect during Bear markets versus during all other types of markets. The aim of this study is to investigate the weekend effect, more specifically during Bear markets, and establish whether a pattern exists between percentage of returns, days-of-the-week, and market orientation. A Bear market is described in the literature as a prolonged period or market trend in which investment prices fall, accompanied by widespread pessimism. If the period of falling stock prices is short, such as weeks or a few months and followed a period of rising stock prices, the term most commonly used for this type of activity is a market correction. Bear markets are correlated with recessions, high unemployment rates and the inflation. Non-Bear markets are trading periods where the market is gaining value and stock price changes are mostly positive. The data is expected to show a stronger weekend effect during Bear markets. And, if this assumption proves valid, then it would be safe to classify the weekend effect as a possible market indicator. According to research conducted by Connolly (1989), it is conceivable that weekend effect does not exist at all. In his article, “An Examination of the Robustness of the Weekend Effect,” Connolly suggests that statistical flaws in the regressions testing of the weekend effect and failures to adjust the significance levels for large sample sizes have resulted in a rejection of the null hypothesis. Upon making the required corrections, Connolly concludes that there is very little evidence of the weekend effect after the mid-1970s. This study will therefore reexamine the weekend effect by researching the stock market diffusion process in the Dow Jones Industrial Average (DJIA), the S&P 500, and the NASDAQ. Data for the DJIA and S&P 500 is available beginning on February 2, 1976, and data for the NASDQ is available beginning on October 11, 1984. Data for all three markets was collected until September 13, 2002. The paper will summarize stock market return data and the statistical properties for each of the three indices, followed by a comparison of daily returns in Bear and non-Bear market modes, and a comparison of weekend and non-weekend daily returns while in those modes. The research will continue by dividing the data into different market modes and weekday orientations and comparing each set of data with an assumed daily return of zero. The paper will then conclude with an analysis of the results and ideas for future research.",2
34,3,Journal of Economics and Finance,11 March 2009,https://link.springer.com/article/10.1007/s12197-009-9079-1,Bequest motives and household money demand,July 2010,Jan Tin,,,Male,Unknown,Unknown,Male,"The bequest motive has long been postulated as one of the major motives determining equilibrium levels of saving and money demand. However, its relationship with money demand remains empirically unexamined, especially at the microeconomic level. Since the1970s, the discovery and collapse of the standard short-run aggregate money demand have diverted the attention of most researchers from microeconomic studies in search of a stable aggregate money demand. In recent years, the focus of most studies has shifted from the standard short-run aggregate money demand to the cointegration model of long-run aggregate money demand. One of the consequences is that, under the assumption of a representative economic agent, the effects of the bequest and life-cycle motives on money demand have been assumed away, leaving money balances to be demanded mainly for transactions, precautionary, and speculative purposes. This study examines the empirical relationship between the bequest motive and money demand in general and its interactions with the life-cycle motive in particular by utilizing panel data from the Survey of Income and Program Participation (SIPP). SIPP contains not only longitudinal information on income and demographics but also cross-sectional information on financial assets, amounts of return, and life insurance coverage. The availability of data on money demand enables this study to obtain parametric estimates free from the assumption of a representative economic agent (Gorman 1953), the identification problem (Cooley and LeRoy 1981), or the aggregation problem (Barnett 1980). The availability of cross-sectional data on life insurance coverage enables this study to obtain a measure of the bequest motive, while the availability of human capital and family formation variables enables this study to obtain a proxy for the life-cycle motive. Regression results indicate that a positive relationship exists between money demand and bequest motives. The strength of the relationship, however, is not constant and exhibits a life-cycle behavior during various stages of an individual’s life. Householders with bequest motives save more in monetary assets, especially interest-earning monetary assets, and transfer a larger percentage of their permanent incomes to monetary assets than those without bequest motives. The quantity of interest-earning monetary assets demanded for both bequest and life-cycle purposes initially increases with age, reaches a maximum, and then declines, but no such relationship can be detected for non-interest-earning checking accounts. This paper is organized in the following manner. Section 2 reviews major developments in the literature. Section 3 explains the econometric model of money demand employed in this study. Section 4 discusses the data source and definitions of dependent and independent variables. Section 5 presents empirical results. The final section contains a brief conclusion.",5
34,3,Journal of Economics and Finance,25 December 2008,https://link.springer.com/article/10.1007/s12197-008-9072-0,"Autocorrelation, return horizons, and momentum in stock returns",July 2010,Ming-Shiun Pan,,,Unknown,Unknown,Unknown,Unknown,,
34,3,Journal of Economics and Finance,16 January 2009,https://link.springer.com/article/10.1007/s12197-008-9074-y,Recent evidence on the impact of government budget deficits on the ex ante real interest rate yield on Moody’s Baa-rated corporate bonds,July 2010,Richard J. Cebula,Pablo Cuellar,,Male,Male,Unknown,Male,"In the US, there was a brief experience with federal government budget surpluses during the 1998–2001 period. However, given the 2001 recession, sluggish economic growth since 2001, and budgetary demands involving proposed further income tax cuts on the one hand and the “war on terrorism” in the aftermath of the terrorist attacks on the U.S. on September 11, 2001 on the other hand, the specter of federal government budget deficits, potentially huge ones, has raised its ugly head once again. As Alan Krueger (2003) observes, budget deficits have re-emerged as a major economic concern. The impact of deficits on interest rates has been studied extensively (Al-Saji 1992, 1993; Barth et al. 1984, 1985, 1986; Cebula 2005; Cukierman and Meltzer 1983; Feldstein and Eckstein 1970; Findlay 1990; Hoelscher 1983, 1986; Holloway 1988; Johnson 1992; Ostrosky 1990; Saltz 1998; Swamy et al. 1990; Tanzi 1985; Zahid 1988). These studies typically are couched within IS-LM or loanable funds models or variants thereof. Many of these studies find that the government budget deficit acts to raise longer term rates of interest while not significantly affecting shorter term rates of interest. Since capital formation is presumably much more affected by long term than by short term rates, the inference has often been made that budget deficits may lead to ""crowding out"" (Carlson and Spencer 1975; Cebula 1985; Krueger 2003). The interest rate/budget deficit literature has generally focused upon the yields on Treasury bills, Treasury notes, and Treasury bonds, although occasionally the yield on other bonds (such as Moody’s Aaa-rated corporate bonds) has received attention. In recent years, however, the deficit impact on Moody’s Baa-rated bond yields, especially the ex ante real interest rate yield, has received virtually no formal attention in this literature. Accordingly, the purpose of this study is to provide current evidence as to the effect of the U.S. federal budget deficit on the ex ante real interest rate yield on Moody’s Baa-rated corporate bond issues. Arguably the focus on this ex ante real long term interest rate is especially justified because of its relevance to private-sector capital formation decisions. Using seasonally adjusted quarterly data, the study investigates the period 1973.1–2007.4. We begin with 1973.1 because this is approximately the time of the abandonment of the Bretton Woods agreement. Ending the study period with 2007.4 makes this study very current and hence very pertinent. Moreover, using 34 years of quarterly data provides a relatively longer term perspective on the impact of the budget deficit on the ex ante real Moody’s Baa-rated corporate bond interest rate yield. Section 2 provides the framework for the empirical analysis, an open loanable funds model. Section 3 defines the variables in the empirical model and describes the data. Section 4 provides the empirical results, whereas an overview of the study findings is found in Section 5.",13
34,3,Journal of Economics and Finance,18 September 2008,https://link.springer.com/article/10.1007/s12197-008-9034-6,The joint decision to manage earnings through discretionary accruals and asset sales around insider trading: Taiwan evidence,July 2010,Chih-Jen Huang,,,Unknown,Unknown,Unknown,Unknown,,
34,3,Journal of Economics and Finance,10 December 2008,https://link.springer.com/article/10.1007/s12197-008-9071-1,New evidence on shareholder wealth effects in bank mergers during 1980-2000,July 2010,Adel A. Al-Sharkas,M. Kabir Hassan,,Male,Unknown,Unknown,Male,"From 1934 through the 1970s, the number of banks in the United States remained fairly stable. In the late 80s, however, the number of American banks started to decrease significantly. Specifically, between 1980 and 2000, the number of banks declined from 14,404 to 9,214, a decrease of 36%.Footnote 1 The two main causes of such a retreat were bank failures and bank mergers. Statistics show that between 1985 and 1992, failures contributed significantly to the decrease in the number of banks. Still, failures accounted for less than half of the decrease in the number of banks. This trend has become more evident since 1992, where the number of bank failures accounted for less than 15% of the total decline in the number of banks. The remaining part of the reduction can be explained by the growing trend towards larger banks and bank consolidation. The merger mania has yielded considerable research interest in this topic. In spite of a large body of literature, many puzzling questions remain unanswered. For instance, a common finding is that bank mergers do not create value, yet they continue to occur. Empirical evidence indicates that, on average, there is no statistically significant gain in either market performance or operating performance of the combined firm. Moreover, shareholders of target firms gain at the expense of the bidder firms. This has been documented over the course of many studies covering different time periods and across countries, and it is true whether one examines accounting data or the market value of equity. The main objective of this paper is to analyze the effect of bank mergers on bidder, target and combined firms. For this purpose, the analysis is extended to a relatively larger data set (785 mergers) over a long time period (1980–2000). This large data set will provide a better opportunity to measure the full wealth effects from bank mergers. Statistically, it has been shown that with small data sets, one or two unusual mergers can easily influence results. This analysis differs from previous studies of bank mergers in three respects. First, this analysis of bank mergers spans a longer time period, with the advantage of considering more recent bank mergers. Second, this study reveals a detailed analysis of stock market returns pertinent to 785 mergers of U.S banks. Examining combined returns will help to determine the overall economic impact of the merger. In particular, the expected overall contribution of the merger is studied as opposed to only the expected value from either the bidder’s or the target’s standpoint. Third, we introduce two new bank merger event-study models. Previous bank merger event studies utilize the standard market model methodology to find whether bank mergers generate abnormal returns for bidder, target and combined firms. There is a critical assumption in using the traditional event study methodology, including the linearity of the relationship and the independency and homoskedasticity of the stock returns. Hart and Apilado (2002) show the advantage of the use of the GARCH-M model versus the traditional model when employing an event study. The focus of their paper is interstate mergers (state to state). Therefore, this paper constructs an EGARCH model that is superior to the standard market model in the sense that it controls for linearity, homoeskedasticity and correlated error term assumptions of the standard market model. Violation of these assumptions could lead to inefficient estimators in the market model. Moreover, using a sample from the banking industry provides a control for industry-specific factors that could affect returns. The common control in previous event studies is the market return. However, there may be other factors specific to each industry that should be included in order to obtain unbiased results. Unlike previous bank merger literature, this study goes beyond the simple market model. We use a three-factor model that controls for exchange rate and interest rate shocks common to the banking industry. This is an attempt to focus on models used in the bank mergers event study. Hence, not only does this study evaluate bank mergers but it also investigates the results of two new different event study methodologies. The results show that the target bank shareholders experience significant positive abnormal returns while abnormal returns to bidding bank shareholders are significantly negative. For combined firms, cumulative abnormal returns are significantly positive. Comparing the results of both methodologies reveals that the modified market model may overstate the abnormal returns for bidder, target and combined firms. Therefore, we argue that basic conclusions regarding issues such as the wealth impact of bank mergers can depend on the chosen model. Actually, the empirical results suggest that the choice of methodology will affect inference about the magnitude effect of bank mergers. Accordingly, the failure to use accurate models to describe stock returns might be responsible for the mixed and contradictory results reached by earlier studies regarding the impact of bank mergers. Finally, this study estimates cross-sectional regressions of the abnormal returns of target, bidder and combined firms on a number of explanatory variables. In particular, we examine the effects of relative size, relative book-to-market value, method of payment, and the number of bidders on the abnormal returns of bidder, target and combined firms. For bidder returns, the regression results demonstrate that the coefficient on relative size is significantly positive, suggesting that relatively large targets capture a significantly large merger premium. Interestingly, for bidders, abnormal return is negatively related to relative size. The larger the target relative to the bidder, the smaller the CAR will be for the bidder. This result indicates that the bidders fare much worse when target is relatively large. This negative coefficient is consistent with the positive coefficient on relative size in the target returns if the higher returns to target shareholders in relatively large mergers lose bidder shareholders. Moreover, if the market believes the merger is value destroying for bidder, the larger the relative size of the target, the more value will be transferred from the bidder to the target. Empirical evidence suggests target, bidder, and combined firm returns are positively related to cash mergers. As for the number of bidder variable, it is negatively related to bidders’ returns. The negative sign of this variable is consistent with the overpayment hypothesis. Also, there is evidence that diversifying mergers are bad for bidders. However, we fail to find evidence that this variable affects target and combined returns. It is worth noting that the results show that relative book-to-market values are not confirmed as relevant factors. The remainder of this paper proceeds as follows. “Section 2” reviews the literature on bank mergers. “Section 3” describes the data. “Section 4” describes the methodologies followed in this paper. “Section 5” represents the empirical findings. The last section concludes.",
34,3,Journal of Economics and Finance,25 April 2009,https://link.springer.com/article/10.1007/s12197-009-9081-7,The strength and source of asymmetric international diversification,July 2010,Leyuan You,Robert T. Daigler,,Unknown,Male,Unknown,Male,"Numerous studies on international stock diversification support investing globally to reduce the risk of a stock portfolio. However, more recent studies reveal increasing correlations (decreasing diversification benefits) between markets over time, especially during more volatile periods, bear markets, business downturns, and financial crises (for example, see Erb et al. 1994; Longin and Solnik 1995; Shawky et al. 1997; Longin and Solnik 2001; and Schwebach et al. 2002). This increase in correlations over time, as well as the associated asymmetric correlations during different types of markets, raises the question of what statistical characteristics of the underlying return distributions are associated with these asymmetric diversification results and whether behavior rather than economic fundamentals are related to these results. The main purpose of this study is to examine the high and asymmetric correlations during periods of major market movements in a different way, namely by investigating how the characteristics of the market returns are associated with the correlation patterns. We achieve this by employing the most actively traded international stock index futures (SIF) contracts from the recent major bull (late 1990’s) and bear markets (early 2000’s). We find that the resultant asymmetric correlations for the bull market relative to the bear market are (logically) strongly associated with the larger returns of the SIFs distributions. Most surprising, and contrary to previous literature, it is the largest positive returns during bear markets, not the largest negative returns, that have the greater influence on the higher asymmetric correlations that occur during declining markets. This is consistent with a herding instinct of investors as the key factor affecting the higher correlation between international markets, since the alternative of fundamental news of widespread economic recovery is inconsistent with a continuing bear market. Moreover, these results show that knowledge of the characteristics of the underlying returns across markets helps us understand the asymmetric correlation patterns between the bull and bear markets. Consequently, these results should motivate additional research to determine the interrelations of the statistical characteristics of return behavior that are associated with the results found in this paper. Finally, our results show that the level of correlations across countries continues to increase over time, with only the Asian markets now having significantly lower correlations with other markets. Our use of SIF avoids common problems inherent in the practice of employing cash stock indexes to examine diversification, as discussed in the data section of this paper. Consequently, the results should be more consistent and contain fewer biases, at least concerning stale prices and liquidity, compared to previous studies.",5
34,4,Journal of Economics and Finance,12 February 2009,https://link.springer.com/article/10.1007/s12197-009-9078-2,"Patent citations, technology diffusion, and international trade: evidence from Asian countries",October 2010,Shoji Haruna,Naoto Jinji,Xingyuan Zhang,Male,Male,Unknown,Male,"International technology diffusion is important for global economic development because it determines the pace at which the world’s technology frontier can expand over time. As pointed out by Keller (2004), the major channels for technology diffusion across countries include international trade and foreign direct investment (FDI). While the literature on the spillover effects of FDI activity is bountiful,Footnote 1 that on the inflow of foreign technology through trade is less extensive. Coe and Helpman (1995) estimated positive and quantitatively large effects on total factor productivity growth from import-weighted foreign R&D in 22 OECD countries. Similar effects were also found for technology diffusion from highly industrialized countries to 77 less developed countries by Coe et al. (1997). However, the analysis by Keller (1998) casts some doubt on these results. Using randomly created import shares, he showed that they are not essential to obtaining the results of Coe and Helpman (1995). Thus, the import shares used by Coe and Helpman (1995) and Coe et al. (1997) may not be a good option for showing the importance of imports as a vehicle for technology diffusion. The use of patent citations as a proxy for the inflow of foreign technology was pioneered by Jaffe et al. (1993).Footnote 2 They used patent citations as a measure of the extent of technology spillover within the U.S. Jaffe and Trajtenberg (1999) used patent citation data to compare the magnitudes of technology flows across countries and technology fields. Each U.S. patent application has to include information about the “prior art.” The presumption here is that citations to “prior art” are informative of the causal links between the corresponding patents. As stated elsewhere, “⋯citations made may constitute a “paper trail” for spillovers, i.e., the fact that patent B cites patent A may be indicative of knowledge flowing from A to B (Hall et al. 2001, p. 14). Using patent citation data, Sjoholm (1996) investigated whether the import activities of Swedish firms promote the inflow of foreign technology. His results suggest a positive correlation between Swedish patent citations of foreign patents and bilateral trade. MacGarvie (2002) investigated the relationship between the detailed data of French firms’ patent citations and their exports and imports and found that the inventions of exporters and importers are more likely to be affected by foreign technology than those of firms that do not engage in international trade. MacGarvie (2003) argued that the three mechanisms emphasized in the FDI literature, i.e., demonstration effects, labor mobility, and buyer and supplier linkages, may facilitate technology diffusion through exporting and importing. During the last quarter century, newly industrializing economies (NIEs), such as South Korea and Taiwan, have achieved tremendous technological progress and economic growth. With their successful export-oriented development strategies, many South Korean and Taiwanese companies have now graduated from imitators of the technologies developed by the U.S., Japan, and other OECD countries to genuine innovators, especially in various cutting-edge high-tech sectors (Kim 1997 and Trajtenberg 2001). In the last few years, attention has been focused on the so-called BRIC countries, i.e., Brazil, Russia, India, and China, which are emerging as major players in the global economy of the 21st century. Using U.S. patent citation data, Hu and Jaffe (2001) examined the patterns of technology diffusion from the U.S. and Japan to South Korea and Taiwan. Using the citation frequency equation proposed by Jaffe and Trajtenberg (1999), Hu and Jaffe (2001) found that it is much more likely for South Korean patents to cite Japanese patents than U.S. patents, whereas Taiwanese patents cite them more evenly. However, it is not clear from their analysis whether the trading activities of South Korean and Taiwanese firms promote the inflow of foreign technology since their study was focused on only the citations themselves. We take an alternative approach to examine whether international trade plays an important role as a channel of international technology diffusion. We extend the sample used by Hu and Jaffe (2001) to two BRIC countries, i.e., China and India, and use a modified version of the Balassa index proposed by Balassa (1965) to define the extent of trade specialization between these economies and several developed countries from among the U.S., Japan, and the other G7 countries.Footnote 3 To investigate the correlation between learning of foreign technology and trade specialization, we utilize patent citation data from both the USPTO and JPOFootnote 4 as a proxy for technology diffusion. Our results reveal that trade specialization has had a significant impact on technology diffusion in these economies. The present paper contributes to the preceding literature in two respects. First, it examines empirically the role of patent citations pertaining to the four emerging economies. Second, different from some literature on the relation between trade and technology diffusion, we attempt to evaluate the impact of trade structure, i.e., trade specialization, on the technology diffusion. To the best of our knowledge, no empirical investigation of the like has ever been performed. The paper is organized as follows. Section 2 explains how patent citations and trade data have been classified in accordance with the International Standard of Industrial Classification (ISIC) and describes the patterns of patent citations and trade in the sample countries. In Section 3, we describe the econometric specifications of our model and discuss the empirical results, i.e., the correlation between the inflow of foreign technology and international trade. Section 4 is assigned to a summary of our findings and the direction of our future research.",13
34,4,Journal of Economics and Finance,08 January 2009,https://link.springer.com/article/10.1007/s12197-008-9073-z,Modeling the time to an initial public offering: When does the fruit ripen?,October 2010,Yamin Ahmad,Russell Kashian,,Unknown,Male,Unknown,Male,"The financial sector is currently witnessing a focused, but unique transformation. In this transformation, two events occur, often within a limited time frame. The first movement is the conversion of credit unions to mutual savings banks. Following the passage of the Credit Union Membership Act of 1998, the conversion of credit unions to mutual savings banks became easier. The second movement is the demutualization of these recently converted institutions into publicly traded banks. While there is considerable literature regarding the conversion of mutual savings banks to publicly traded banks, this paper seeks to connect the literature regarding the conversion of credit unions and the conversion of mutual savings banks. We use a hazard model to identify some of the characteristics which might affect the conditional probability that a credit union that has converted to mutual status, subsequently issues an initial public offering (IPO). Over the past 10 years, over 20 credit unions in the U.S. have converted to mutual savings banks. The majority of these institutions subsequently converted to stock-owned institutions. This activity, while very limited in terms of number of institutions, is hotly debated. Opponents of this activity say that in almost every case this process is motivated by insiders attempting to garner an economic transfer of wealth. Given the literature regarding the under-pricing of mutual conversions, insiders can anticipate a windfall as directors and top managers of the erstwhile credit union take advantage of the initial offering of stock. This paper pulls together the strands of literature regarding the transformation from credit union to mutual banks to stock owned banks. These three forms of ownership are unique in their structure. Credit unions differ from mutual savings banks in several ways. First, credit unions operate on a “one person one vote”, principle in which each member has an equal vote in electing the board of directors. Mutual savings banks use weighted voting where depositors exercise a number of votes proportional to the dollar amount of deposits they hold at the bank. Credit union membership requires a share savings account, while mutual savings banks customers can either participate as savers, borrowers, or both. Stock ownership differs strongly from these two ownership forms. In a stock owned bank, investors must purchase stock (their ownership is not a function of participation in the operations of the institution). In addition, they hold a divisible claim on the assets of the institution. The conversion of credit unions to mutual status is a development that has the attention of both researchers and the public. One reason is the size of the industry in terms of institutions. There are currently over 8,000 credit unions in the United States. This is more than ten times the number of remaining mutual thrifts. Another reason is the timing of this development. While this effort by credit unions to convert into mutuals is picking up, it is occurring as the effort to demutualize the historic mutuals is past its zenith.Footnote 1 This dichotomy (the intriguing development of conflicting conversions) calls into question the benefit of mutualization in an era of demutualization.
 The subsequent demutualization is an attempt to derive value from the non-specificity in the ownership of mutual savings banks (thrifts). As a result, demutualization has been a popular movement, as evidenced by the hundreds of demutualizations conducted over the past 30 years. When a mutual converts to a publicly traded stock corporation, it once again transforms into a unique form of ownership. The nature of the mutual allows depositors to own the firm through participation in the ordinary business of the thrift; the stock form of ownership allows the investors to own the firm through the purchase of equity. Ultimately, demutualization changes the economic makeup of ownership. It is within this backdrop that we investigate the pattern of duration dependence that exists when converted credit unions demutualize and issue an initial public offering. We begin our analysis by estimating the survivor, hazard and integrated hazard functions nonparametrically using Kaplan-Meier product limit estimators. We use these to examine the pattern of duration dependence that exists in the data and then utilize Cox’s (1975) partial likelihood approach to estimate a proportional hazards model with time varying covariates. We consider various characteristics that have been examined within the literature on bank transformation in order to see if they can help us to understand the observed pattern of duration dependence within the credit union industry. There are four key findings within this paper. First, we find evidence of positive duration dependence in the data we examine, which indicates that the conditional probability a converted credit union issues an IPO, increases, the longer they remain in mutual status. Second, we find that the hazard of an IPO issue increases in two waves. The first wave occurs between 3 1/2 years to 4 years reflecting the increased transition intensity for those who wish to demutualize early. The second spike in the hazard function occurs at approximately 8 years after conversion, where the majority of the institutions in our sample demutualize at that point. Upon estimating the model with Cox’s (1975) semi-parametric partial likelihood approach, we find the probability that a converted institution will issue an IPO is influenced to a large extent by various measures that capture asset quality, in particular through measures that are equity driven. Notably, we find that ROA, total loans to total assets and finally the size of the institution do not appear to influence the conditional probability that they might issue an IPO. The last result appears to run contrary to the existing literature which finds that the size of an institution often plays a role in bank transformation (see for example Berger et al. 1998, or Pagano et al. 1998). That is, we find no evidence that smaller converted credit unions are any less likely to demutualize when compared to large ones. Finally, we also find that institutions with very high or very low values of total equity to total assets are less likely to issue an IPO. The remainder of the paper proceeds as follows. The next section places this paper in the context of the relevant literatures. Section 3 describes the data, outlines the methodology used to estimate the hazard functions and discusses the results from the nonparametric estimation of the hazard function. Section 4 estimates the data using the Cox (1975) partial likelihood model, and examines the various factors that contribute to an IPO issue. Section 5 concludes.",3
34,4,Journal of Economics and Finance,20 May 2009,https://link.springer.com/article/10.1007/s12197-009-9083-5,How well is productivity being priced?,October 2010,Lakshmi Balasubramanyan,Ramesh Mohan,,Female,Male,Unknown,Mix,,
34,4,Journal of Economics and Finance,30 January 2009,https://link.springer.com/article/10.1007/s12197-009-9075-5,Contrarian strategies and investor overreaction under price limits,October 2010,Anchor Y. Lin,Peggy E. Swanson,,Unknown,Female,Unknown,Female,"How do daily price limits in equity markets affect price behavior? Are price limit regulations able to subdue unwarranted extreme investor behavior and reduce price volatility? Do price limits delay price discovery or induce investor overreaction? (Kim and Rhee 1997 and Huang et al. 2001). Proponents of price limits argue that because price limits provide cooling off periods, they prevent overreaction and reduce price volatility. Although government intervention or restrictions may delay price discovery, excessively violent price fluctuations due to speculative trading or market stress are not in investors’ best interests. Stock exchanges in emerging markets such as Mexico, China, Malaysia, Taiwan, Korea, and Thailand regulate daily price movements by imposing price limit rules that set the maximum range of fluctuations allowable within a single trading day and interrupt the trading process when prices hit pre-specified limits. Opponents criticize price limits because they not only delay price discovery for stocks locked in limits but also accelerate price overshooting for stocks approaching limits. Critics argue that the delayed price discovery restricts prices from fully reflecting fundamentals and causes market inefficiency. Kim and Rhee (1997) find that price limits cause spillover price volatility into the subsequent trading day rather than reducing price volatility. When a stock hits and locks in its price limit, trading of the stock is halted but trading volume will be heavier on the following day. Lehmann (1989) and Subrahmanyam (1994) support the argument that price limits help cause a magnet effect in that prices accelerate as they near the limits because traders chase the trend and/or fear illiquidity in trading limit-hitting stocks. This magnet effect drives prices to hit the limits more quickly as traders rush to purchase stocks that are near the price ceiling and rush to sell stocks that are near the price floor. The result is more rather than less price volatility. Past studies indicate that four basic concepts—the cooling—off effect, the delayed price discovery effect, the magnet effect, and the overreaction effect—help explain how price limit rules affect price behavior (See Lehmann 1989; Kim and Rhee 1997; Huang et al. 2001 and Cho et al. 2003). The cooling-off effect argues that price limits provide cooling off periods reducing investor overreaction and decreasing price volatility. The delayed price discovery effect argues that price limits restrict prices from fully reflecting fundamentals. If fundamentals drive prices to their limits but these price limits control price movements, prices are expected to continue the same direction in subsequent trading days. The magnet effect hypothesizes that price limits cause traders to rush to buy and/or sell stocks approaching the price limits because they fear that they will be unable to trade when prices hit the limit. Traders’ eagerness to buy or sell stocks nearing price limits forces prices to hit the limit range faster than stocks not nearing price limits. This is a self perpetuating process. Finally, the overreaction effect argues that, if price limit rules induce investor’s overreaction to news, the overshooting prices of limit-hit stocks will eventually reverse to the fundamental value. Huang et al. (2001) conclude that “the overreaction hypothesis predicts price continuations for the overnight period and price reversals for the subsequent trading day.” It has been argued that if limit-hit behaviors are driven primarily by investors’ overreaction, price limit rules may be effective in providing a cooling-off period for investors to reexamine the limit-hit stocks. Specifically, this study focuses on the investment performance of contrarian behaviors based on the delayed price discovery and the overreaction effect for the Taiwan stock market. If delayed price discovery exists, investors using the trading strategy of buying limit-hit winners or selling limit-hit losers should be able to earn excess profits due to price continuation at subsequent periods. Thus investors are expected to perform well when they buy (sell) stocks that hit the upper (lower) limit in the current period and sell them in the next period. Conversely, if the overreaction effect exists, investors who conduct contrarian trading strategies of buying past losers or selling past winners should be able to earn excess returns due to expected price reversals. Since overreaction leads to price overshooting at the current time and price reversal at subsequent trading periods, investors should perform well to sell (buy) stocks that hit the upper (lower) limit at the current period and buy them back at the next period when the price reverses. The study has several important implications for policymakers and for investors. Policymakers, in deciding whether or not the imposition of price limits is the proper policy to control stock market price volatility, can make better decisions if they understand the impact of price limit rules on price formation and investor behavior. If delayed price discovery exists, price limit rules allow prices to gradually reflect fundamental information at subsequent trading days despite an initial decrease in market efficiency. Thus, price limit rules may provide investors with cooling off periods and reduce price volatility that aids policymakers in achieving policy goals. However, if the overreaction effect exists, price limit rules increase price volatility by causing investors to overact to stocks that are approaching price limits. Thus the goal of price volatility reduction may be not achieved. For investors, an expectation that prices will eventually reverse suggests that contrarian trading strategies will lead to superior returns. Therefore, both policymakers and investors need to be able to identify when and under what condition one of these effects is stronger. Our empirical findings support the overreaction effect. There is evidence of delayed overreaction, indicating price continuations for the overnight period and price reversals for the subsequent trading day, consistent with Huang et al (2001) who investigate price behavior of limit-hit stocks in Taiwan from 1990 to 1996. This paper extends Huang’s study and investigates the relationship between limit-hit behavior and firm characteristics. In addition, we measure the price behaviors of intraday limit-hit stocks and limit-hit-lock stocks as well as investment returns of contrarian strategies for catastrophic event days and during regulation change periods. The evidence suggests that investors have a greater tendency to overreact in small-size, high-turnover, and non-high-tech stocks. Additionally, the price overshooting phenomenon in down-hit stocks is stronger during regulation change periods than during the entire sample period. Lastly, catastrophic events have asymmetric impacts on limit-hit stocks and cause price reversals for up-hit stocks and price continuation for down-hit stocks. The paper is organized as follows: Section 1 is the introduction, Section 2 describes the background of Taiwan’s stock markets, Section 3 reviews related literature, Section 4 describes data and methodology, Section 5 reports and analyzes empirical results, and Section 6 concludes.",4
34,4,Journal of Economics and Finance,14 November 2008,https://link.springer.com/article/10.1007/s12197-008-9068-9,Price discovery in the Indian gold futures market,October 2010,Pantisa Pavabutr,Piyamas Chaihetphon,,Unknown,Unknown,Unknown,Unknown,,
34,4,Journal of Economics and Finance,17 August 2010,https://link.springer.com/article/10.1007/s12197-010-9149-4,Introduction to the symposium on mathematics and economics: some perspectives from the Mathematical Association of America Curriculum Foundation seminar,October 2010,Richard Vogel,James E. Payne,,Male,Male,Unknown,Male,,
34,4,Journal of Economics and Finance,17 August 2010,https://link.springer.com/article/10.1007/s12197-010-9150-y,The state of mathematics education today: what happens in the math classroom,October 2010,Sheldon Gordon,,,Male,Unknown,Unknown,Male,"The long-term effects of this reform movement are hard to assess. The kinds of conceptual problems that typified the calculus reform textbooks have been picked up and incorporated into virtually all of the “traditional” textbooks. So, one can certainly conclude that the initiative has shifted the entire calculus enterprise in the direction of reform. In the process, however, all but one of the reform textbooks that grew out of the movement have been dropped by the commercial publishers, whose focus has been increasingly on publishing only blockbuster textbooks that would appeal to all potential adopters. Unfortunately, the new material incorporated into these books tends to be add-ons at the end of sections or the end of problem sets and few people actually have the time or inclination to get to them. They do not fit easily into a course that may be focused on developing algebraic skills and covering every possible traditional topic. So, one could also argue that, like the traditional land war in Asia, every invading army has been swallowed up by the sheer size of the enterprise. One significant aspect of this has to do with the use of technology. Certainly, in high school, the use of graphing calculators for mathematics education has become ubiquitous. A student literally cannot hope to do well on any of the high-stakes tests, such as the SAT or the Advanced Placement test in calculus, without using a graphing calculator. Many schools now integrate their use as early as middle school, so almost all college-bound students have used them for years throughout their mathematics education. The various college-level mathematics professional societies conduct an extensive survey to ascertain the state of mathematics education every 5 years. In the most recent (Lutzer et al. 2007) survey, it was found that only about 50% of all students in college calculus are allowed to use technology. But, without technology, most of the other reform efforts become impossible to implement, so the implication is that the reform movement still has a long way to go. In a related direction, as has been reported in the introductory article in this Symposium, the Mathematical Association of America’s (MAA) committee on Curriculum Renewal Across the First Two Years (CRAFTY) conducted the first round of its Curriculum Foundations project several years ago. In this project, leading educators from 17 disciplines, primarily those that traditionally have been considered highly quantitative, were brought together to discuss and prepare recommendations to the mathematics community on the mathematical needs of their students today. In reading the recommendations (Ganter and Barker 2004), it is amazing to see the degree of uniformity that exists about those needs. In particular, virtually every discipline recommends:
 There should be a very strong emphasis on problem solving. There should be a very strong emphasis on mathematical modeling. Conceptual understanding is more important than skill development. The development of critical thinking and reasoning skills is essential. The use of technology, especially spreadsheets, is extremely important. A major emphasis should be on the development of communication skills (both written and oral). There should be a much greater emphasis on probability and statistics. There should be greater cooperation between mathematics and the other fields. Furthermore, these are precisely the same sentiments expressed by the economists who participated in the Curriculum Foundations workshop on Mathematics and Economics. (Very similar recommendations also came out of the other workshops in the second round of the CF project.) Despite this convergence of opinion among most of the disciplines that send us students, the large majority of mathematics departments has not particularly listened to or, at least, has not taken the recommendations to heart. Most are still offering courses with the same philosophy (primarily preparing students for the next math course) that they have had for decades. In large measure, this can probably be ascribed to doing what is familiar and comfortable. It can also likely be attributed to the fact that most mathematicians are not all that familiar with the recommendations that have been coming out of the CF project or perhaps to a belief that those recommendations do not apply in their institution since the other departments on campus are not making a case for change locally. (Sometimes, though, the issue can be one of words having different meanings in different disciplines. For instance, many physicists and engineers have criticized mathematics courses as being too theoretical. Mathematicians interpret this as meaning that we spend too much time on theorems and proofs, but the practitioners really mean that the focus in the math classes is primarily on contrived, non-contextual problems rather than real-world applications. In any dialog between departments, very specific examples of problems are essential; certainly this is much more likely to be effective than phrases that can be interpreted differently.) As a matter of fact, CRAFTY’s picture of how the CF reports can best be utilized locally is for the partner disciplines to use them as a vehicle for opening discussions with the mathematics department. We suspect that that would be a far more effective approach than expecting some members of the mathematics department to lobby the rest of the department to make such changes or even to approach the other departments.",1
34,4,Journal of Economics and Finance,31 August 2010,https://link.springer.com/article/10.1007/s12197-010-9154-7,Problem-based learning: merging of economics and mathematics,October 2010,Rae Jean B. Goodman,,,,Unknown,Unknown,Mix,,
34,4,Journal of Economics and Finance,31 August 2010,https://link.springer.com/article/10.1007/s12197-010-9155-6,Teaching across disciplines: using collaborative instruction in undergraduate education,October 2010,Roberta Schroder,,,Female,Unknown,Unknown,Female,"Collaborative learning environments, including learning communities, attempt to pair two courses so that there are coordinated and cohesive components associated with the course curriculum. The main strategy involves two instructors teaching two related, yet distinctively different, courses while maintaining a meaningful and effective connection between the two curricula. Collaborative learning models have been discussed in the academic literature for decades (Gabelnick et al. 1990). Much research suggests that there are substantial benefits to both faculty and students in a learning community.(Killacky et al. 2002; Gabelnick et al. 1990) This is particularly true of undergraduates who have been identified as at-risk and are derived from under-served populations. Although not exclusively, community colleges often serve these populations (Gabelnick et al. 1990; Wilmer 2009). Literature shows that traditional student populations at both the undergraduate and graduate levels world-wide have shown promising results in the areas of student achievement, engagement and retention (Tinto and Engstrom 2002; Smith and MacGregor 2009). There are also challenges associated with learning communities that should be addressed in the early stages of course development.",3
35,1,Journal of Economics and Finance,03 March 2009,https://link.springer.com/article/10.1007/s12197-009-9076-4,Do size and unobservable company factors explain stock price reversals?,January 2011,Robert Cressy,Hisham Farag,,Male,Male,Unknown,Male,"Extant studies of price reversal have either used time series analysis (e.g. Debondt and Thaler 1985) or cross-sectional analysis (e.g. Bremer and Sweeney (1991), Cox and Peterson (1994)) to explain the tendency for investors to overreact to new information and subsequently to correct their judgements. Both methods have been shown to possess explanatory power. However, a model which combines the two dimensions may prove superior to either alone by controlling for biases introduced by ignoring the second dimension (Hsiao 2004). Secondly, with the exception of Diether et al (2008) no existing study allows for the existence of unobservable effects that may explain investors’ reactions to dramatic price changes. Extant studies of which we are aware employ observable magnitudes to explain reversal phenomena, e.g. variations in initial CARs and firm size. Unobservable factors may however be important and again ignoring them will be likely to bias results. Empirically, such factors may include temporary phenomena affecting all firms lly (e.g. changes in investor risk aversion) or permanent factors specific to the firm. Thirdly, whilst emerging market studies are growing in popularity, no study appears to have examined price reversal phenomena specifically in the Egyptian stock market, one of the leading emerging markets in the MENAFootnote 1 region. There is thus a gap in the literature and one which we fill in the present paper. Here we use panel data methods to analyse investor reactions to dramatic 1-day price falls on the Egyptian stock market and allow for both unobservable fixed time (period) and cross-sectional (company) effects. Daily price data from a sample of 20 companies experiencing dramatic 1-day price falls in the period 2004–2007 reveal strong evidence of price reversal. Temporary, unobservable time-specific phenomena common to all companies, together with permanent, unobservable company-specific factors are found to be important determinants of these reversals. In line with the literature, firm size is found to be negatively correlated with post-event abnormal returns to the stock. This is consistent with the argument that small firms have a greater tendency to price-reverse because of their greater informational opacity. However, permanent unobservable firm differences are found to explain a very significant proportion of the remaining variation. We demonstrate that for a given size of firm about 60% of the post-event variation in stock prices can be explained by permanent firm-specific effects.Footnote 2 We find that whilst the average return to a strategy of buying Losers is relatively small, taking into account positive and permanent firm-specific information makes potential gains from a selection strategy among Losers large enough to outweigh any effects of the bid-ask bounce that may be present in the data. The rest of the paper is organised as follows. Section 2 presents a survey of the literature. Section 3 summarises the existing theories and provides a new hypothesis to be tested in the paper. Section 4 briefly discusses the dataset employed and is followed by Section 5 which provides details of the econometric modelling. Section 6 reports the empirical results and a final section summarises and concludes.",4
35,1,Journal of Economics and Finance,14 October 2009,https://link.springer.com/article/10.1007/s12197-009-9105-3,Some causes of interstate differences in community bank performance,January 2011,Albert E. DePrince Jr.,William F. Ford,Pamela D. Morris,Male,Male,Female,Mix,,
35,1,Journal of Economics and Finance,30 April 2009,https://link.springer.com/article/10.1007/s12197-009-9082-6,Can overconfidence explain the consumption hump?,January 2011,Shantanu Bagchi,,,Unknown,Unknown,Unknown,Unknown,,
35,1,Journal of Economics and Finance,03 December 2009,https://link.springer.com/article/10.1007/s12197-009-9115-1,News and noise: do investors react to stock split announcements differently during periods of high and low market volatility?,January 2011,Steve Johnson,Robert Stretcher,,Male,Male,Unknown,Male,"The finance literature yields considerable evidence that positive cumulative abnormal returns (CAR’s hereafter) follow stock split announcements. Some articles credit informational signaling for the positive CAR result, while others argue a liquidity effect; that by going from a higher price to a lower price, a firm’s shares become more liquid because of a smaller denomination unit (e.g., Brennan and Hughes 1991; Conroy and Harris 1999; Ikenberry et al 1996). Three stock-split event studies are particularly relevant to our paper. Fama, et.al. (1969) found that positive abnormal returns occur in the month of the announcement, and that abnormal returns were essentially zero in the months following the announcement month. NYSE and Amex firms yielded additional knowledge in a study by Ikenberry, et. al. (1996). The study concluded that a significant positive abnormal return occurred in a five-day window surrounding 2-for-1stock split announcements during the period 1975–1990. It also suggested relationships of abnormal returns with the post-split price, market capitalization, and the book-to- market ratio. Recently, Chern et. al. (2008) showed that abnormal returns surrounding splits were significantly lower for optioned versus non optioned stocks, supporting the idea that the information environment has a profound effect on the price reactions around split announcements. Perhaps the most compelling encouragement for this study, however, was the decline in announcement period abnormal returns over certain time segments identified by Ikenberry, et. al. (1996): the periods from 1975–1980, 1981–1985, and 1986–1990. If, indeed, the signaling argument is valid, it follows that certain market conditions may magnify or obscure the hand-waving that a firm tries to affect in order to gain attention. Previous research, including Lang and Lundholm (1993), conjectures that high volatility may be a proxy for the information environment. Docking and Koch (2005) in their study of dividend change announcements, find that announcements to lower dividends are followed by a greater decrease in stock price during periods of high volatility and high market returns. Since there is evidence that different information environments affect excess stock returns around stock split announcements (Chern et al 2008), stock price volatility can be used as a measure of the information environment (Lang and Lundholm 1993), and stock market volatility does have an effect on excess stock returns around another corporate announcement, dividend changes (Docking and Koch 2005), our intent is to provide further insight by separately examining the price impact of stock splits during different volatility regimes by using the VIX volatility index.",2
35,1,Journal of Economics and Finance,08 July 2009,https://link.springer.com/article/10.1007/s12197-009-9094-2,The sub-prime mortgage crisis and the changing value of cash,January 2011,Frederick Adjei,,,Male,Unknown,Unknown,Male,"Faulkender and Wang (2006) study the cross-sectional differences in the marginal value of corporate cash holdings owing to variations in corporate financial policy and discover that the marginal value of cash decreases with larger cash holdings, higher debt, and as firms choose dividends rather than repurchases to distribute cash. However, this study does not control for exogenous financial shocks such as recessions or a credit crunch. In such periods, extra cash holdings may serve to reduce the shock of decreased external funding or reduced cash flows from operations, enabling firms to maintain their performance and increase shareholder wealth. Consequently, shareholders may value an increase in cash during financial crises even if the firm already has large cash holdings. In this paper, the central questions we answer are whether shareholders place more value on an extra dollar of corporate cash holdings during a credit crunch than before the crisis, and what financial characteristics influence that change in value. If shareholders perceive that the complexity in obtaining external funding has increased due to the financial crisis and firms will have to give up value-increasing investments, then a dollar of cash may be worth more than it was before the crisis. Conversely, if shareholders perceive that extra cash will increase agency problems, even in a financial crisis, then the marginal value of corporate cash holdings may decrease. This study is important because the marginal value of cash in a financial crisis, although essential to firm valuation, has not been examined. While the extant literature extensively attempts to determine the marginal value of debt (Eckbo (1986), Kemsley and Nissim (2005), Eberhart (2005), and Hennessy and Whited (2005)), the marginal value of cash has received little attention in the literature (Pinkowitz and Williamson (2004) and Faulkender and Wang (2006)). This is surprising in view of the fact that corporate liquidity frictions have important implications for firm performance and survival especially during a financial crisis. First, corporate liquidity allows firms to maintain their investments and operations without external funds, and hence avoid both transaction costs on external funding and the associated information asymmetry costs. Additionally, corporate liquidity reduces the probability of incurring financial distress costs in times of low cash inflows since there will be cash available to service obligatory debt payments. Moreover, excess cash may be invested in negative NPV projects and destroy shareholder wealth (Jensen and Meckling 1976). The effects of the aforementioned corporate liquidity frictions may be magnified during a financial crisis and hence, the value implications of cash reserves in a financial crisis require further understanding. Our findings are the following. We find that the value of an additional dollar of cash is significantly lower during the crisis period than during the pre-crisis period. Additionally, in the pre-crisis period, we find that as firms’ cash levels increase the marginal value of cash decreases, consistent with the findings of Faulkender and Wang (2006). The sub-prime mortgage crisis of 2007 became evident with the depreciation of the US dollar, the decrease in corporate performance and corporate liquidity, and the unprecedented home foreclosure rates. The severity of this crisis offers a unique opportunity to examine the impact of corporate cash holdings on firm value during financial crises. We begin by identifying the commencement of the sub-prime mortgage crisis. Next, we examine the change in the value of an extra dollar of cash holdings from the pre-crisis period to the crisis period. We also investigate the impact of firm financial characteristics on the change in value.",3
35,1,Journal of Economics and Finance,24 September 2009,https://link.springer.com/article/10.1007/s12197-009-9107-1,Golden eggs versus plastic eggs: hyperbolic preferences and the persistence of debit,January 2011,Amanda Swift King,John T. King,,Female,Male,Unknown,Mix,,
35,1,Journal of Economics and Finance,01 October 2009,https://link.springer.com/article/10.1007/s12197-009-9109-z,Board independence and market reactions around news of stock option backdating,January 2011,Zahid Iqbal,Kun Wang,Sewon O,Male,,Unknown,Mix,,
35,1,Journal of Economics and Finance,24 November 2009,https://link.springer.com/article/10.1007/s12197-009-9113-3,Regional information and market efficiency: the case of spread betting in United States college football,January 2011,Daniel D. Kuester,Shane Sanders,,Male,Male,Unknown,Male,"United States college football is a physically strenuous sport played almost exclusively outdoors. Further, many teams of the modern era play a multi-regional non-conference schedule such that regional climate has a potentially large effect upon game outcomes. Indeed, given these characteristics, college football is an ideal sport through which to test whether regional climate has any bearing upon the efficiency of betting markets. In other words, are the effects of regional climate well understood by bettors to the extent that betting markets are equally predictive of game outcomes in intraregional games as in various classes of interregional games? The answer to this question has broader implications in terms of whether markets utilize regional information to produce efficient outcomes. There is a wealth of research on the efficiency of sports betting markets. Such markets are popular in testing for efficiency because the value of each market outcome therein is readily observable. Sauer (1998) provides a comprehensive review of the earlier literature in this field. Specific to this paper, Sauer notes that Paul et al. (2003) find evidence that non-favored teams win more often than betting markets predict in college football, as do Paul and Weinbach (2005) in the case of college basketball. Paul et al. (2004) find evidence of bias in totals betting markets. Vergin and Scriabin (1978) show that particular betting strategies can be used to profitably exploit inefficiencies in NFL betting markets. Gray and Gray (1997) use a probit analysis to test the overall efficiency of the NFL spread betting market, finding evidence that this market contains both statistical inefficiencies (i.e., outcomes that are significantly different from the expected outcome when taking into account observable information) and economic inefficiencies (i.e., outcomes that represent a statistically significant profit opportunity when taking into account observable information). The paper further suggests the use of exogenous variables, such as expected game weather, to further test for NFL betting market inefficiencies. Hausch et al. (1981) and Asch et al. (1982), among others, show an undervaluing of favored horses in the market for racetrack betting. However, it is not clear that this inefficiency is sufficient to constitute a profitable betting strategy. Conversely, Woodland and Woodland (1994) show an undervaluing of non-favored teams in major league baseball games. Again, it is unclear that this bias constitutes a profitable betting strategy. Sapra (2008) finds that bettors overreact to an NFL team’s recent performance against the spread. This may be due to (very) small sample inference on the part of present bettors. Perhaps closer in focus to the present study, Borghesi (2007) finds that the NFL totals betting market does not accurately account for the degree to which wind, heat, and rain decrease a game’s point total. He further presents a (partly weather-related) betting strategy that proves to be economically profitable. To evaluate the basic research question, we consider the set of games between NCAA Division I-A teams during the seven seasons from 2000 through 2006.Footnote 1 These games are divided into four classifications: games in which a team from an arid or semi-arid region hosted a team from a humid or semi-humid region, games in which a team from an arid or semi-arid region hosted a team from a like region, games in which a team from a humid or semi-humid region hosted a team from an arid or semi-arid region, and games in which a team from a humid or semi-humid region hosted a team from a like region. Within the study, any region receiving an average annual rainfall of more than thirty-two inches according to www.weather.com is considered to be humid or semi-humid, whereas all other regions are classified as arid or semi-arid.Footnote 2 This arbitrary cut-off point, while perhaps a bit expansive in its definition of arid and semi-arid regions, allows us to meaningfully account for distinctions in climate aridity (humidity) within the United States while maintaining a healthy number of observations over each set of games. The aridity (humidity) of a region is correlated with climatic characteristics other than the moisture level of the air. For example, arid regions tend to be cooler because cool air is capable of holding less water vapor. As cooler temperatures are observed at higher altitudes, arid regions are also more likely to rest at higher elevations than are humid regions. Thus, aridity (humidity) of a climate gives us information not only about the moisture level of the air in a climate region but also about the expected temperature and atmospheric pressure within the region. Such factors, in turn, certainly bear upon a college football player’s ability to remain hydrated, his muscular responsiveness, and his ability to oxygenate the blood. Further, most games in which an arid region team faces a humid region team take place prior to the conference season, when interregional differences in climate and aridity are pronounced. Athletes may find it more difficult to perform in a climate to which they are not acclimated. Commenting on his second place finish in the 1968 Olympic 1500 meter race in Mexico City, Jim Ryun said, “We had thought that 3:39 would win and I ran under that. I considered it like winning a gold medal; I had done my very best and I still believe I would have won at sea level” (Maule 1981). Ryun lost the 1,500 m final to a Kenyan runner, Kip Keino, who was well acclimated to high-altitude conditions. The race marked Ryun’s first defeat at either the 1,500 m distance or the one mile distance in more than three years. Elite (middle) distance runners today typically spend a great deal more time acclimating to the climates of important races than did their predecessors, a revelation of the performance benefit of such activities. However, college football players, faced with weekday practices, weekday academic courses, and a new game each weekend, have little time to acclimate should they leave their home region to play a game. Thus, it is quite plausible to posit that exposure to new climate influences college player performance. The present paper questions whether the extent of this potential influence is well understood in betting markets or whether it constitutes a source of market inefficiency. If certain regional climate effects are not well understood in a market sense, we would expect betting markets for (certain) interregional games to be less efficient than betting markets for intraregional games. In this controlled experimental manner, we can potentially test for the presence of regional climate effects upon betting market efficiency as distinct from the potential effects of home field advantage and travel. As we are testing for market efficiency with respect to a single characteristic (i.e., climate aridity), we eschew the probit analysis used in some papers that test the effectiveness of a multivariable betting strategy. We instead use hypothesis testing, where the number of (against-the-spread wins) over a set of games is tested as a binomial random variable with characteristics (under the null hypothesis) that would arise from an efficient spread betting market. Spread betting markets allow participants to wager upon whether a favored team will win by at least a given margin, known as the point spread. In such markets, a spread is set by a bookmaker and subsequently moves with the gravity of wagers in the market. Given the binomial nature of a spread bet outcome (i.e., win or lose), an efficient spread betting market will feature a final pre-game spread that constitutes the median expected game outcome (i.e., a spread that is “covered” or met by the favored team half of the time). It is important to note that a spread bettor need not rely on an inefficient point spread to earn an accounting profit in the long-run. Sauer (1998) discusses this point in a model of heterogeneous bettors. As noted previously, an efficient spread is one that is covered 50% of the time. A risk-minimizing spread is one that divides money evenly on each side of a spread bet so as to minimize the bookmaker’s risk. A risk-minimizing spread might take into account bias on the part of some bettors such that an unbiased bettor can earn an accounting profit even in the presence of efficient market outcomes. This paper shows that spread betting markets for certain types of interregional NCAA Division I-A football games are inefficient. Using point spreads and game results over a seven year span from 2000 to 2006, we find that teams in arid regions “win” against the spread (i.e., cover the spread when favored or prevent the opponent from covering the spread when not favored) in 56.64% of games in which they host a team from a humid region. Appropriately treating number of against-the-spread “wins” as a binomially distributed variable, a hypothesis test upon this sample information leads us to conclude that spread betting markets do not efficiently account for difficulties faced by humid region teams when traveling to and competing in arid and semi-arid regions. Moreover, we find that placing a spread bet on an arid region team when it hosts a humid region team constitutes a statistically significant profit opportunity within the seven season sample given standard bookmaker fees. These two results, known respectively as weak form and strong form market inefficiency, indicate that climate aridity is an observed characteristic for which college football betting markets do not accurately control. As previous research attests, it is quite rare to find strong form market inefficiency when controlling for a single variable. Most strong form inefficiencies are found to be the result of an elaborate, multivariable betting strategy. Therefore, in relation to past research, the effect of climate aridity upon college football spread betting market efficiency is dramatic. From the seven season sample, there is no evidence that the spread betting market is inefficient in other circumstances of intraregional or interregional competition. That is to say, there is no evidence of betting market inefficiency when an arid region team travels to a humid region for competition, when an arid region team travels to a like region, or when a humid region team travels to a like region. To explain this set of results, we posit that arid regions present unique performance difficulties for the non-acclimated athlete, the extent of which are not well understood in betting markets. This statistical result establishes a heretofore undocumented inefficiency in the college football point-spread betting market. Further, it provides support for the more general hypothesis that market information pertaining to a specific region may be something of an experience good. That is to say, such information may not be used in a way that leads to market efficiency if some market actors are unable to “experience” (or directly observe) said conditions.",6
35,2,Journal of Economics and Finance,28 May 2009,https://link.springer.com/article/10.1007/s12197-009-9086-2,A Bayesian analysis of market information linkages among NAFTA countries using a multivariate stochastic volatility model,April 2011,Petra Fleischer,Ross Maller,Gernot Müller,Female,Male,Male,Mix,,
35,2,Journal of Economics and Finance,12 May 2009,https://link.springer.com/article/10.1007/s12197-009-9084-4,On the financial characteristics of firms that initiated new dividends during a period of economic recession and financial market turmoil,April 2011,Bruce C. Payne,,,Male,Unknown,Unknown,Male,"Dividend announcements have been of great interest to investors, investment counselors and financial scholars for years. They are said to have informational content concerning the value of the firm, and the relationship between dividend announcements and the value of the firm has long been established by early studies in finance (Lizenbunger and Ramaswamy 1982; Miller and Scholes 1978; and Venkatesh (1989). Further, models to predict changes in the firm’s value as a result of those announcements have been developed by Lintner (1956), Fama and Babiak (1968), and Kolb (1981). Payne et al. (Payne et al., 1992) found a unique profile of financial characteristics for those firms that initiated new dividends. That work however, ignored the macroeconomic background in which those announcements were made. The initiation of new dividends and increases in dividend payout ratios occur infrequently because once initiated it is expected by most investors that the new dividends will be maintained, and financial signaling theory would lead investors to conclude that the initiation of new dividends is an indication that the firm expects increased cash flows in the future. Consequently, announcements of new dividend initiations generally have a positive effect on stock prices. Obviously, dividends do not have to be paid, but unless the initiation is identified beforehand as a special dividend resulting from unanticipated cash inflows, it is difficult to reverse the action without having an adverse effect on the value of the firm. Kohers and Caton (2003) found that dividend omissions have significantly negative effects on firms across industries. Wansley and Lane (1987) found that to the extent that the expectation of maintaining the newly announced is true, the initiation of new dividends add to fixed financial costs and thus, to the financial leverage (financial risk) of the firm.",7
35,2,Journal of Economics and Finance,25 June 2009,https://link.springer.com/article/10.1007/s12197-009-9092-4,The wisdom of the few or the wisdom of the many? An indirect test of the marginal trader hypothesis,April 2011,Calvin Blackwell,Robert Pickford,,Male,Male,Unknown,Male,"In 2002 Vernon Smith was awarded the Nobel Prize in Economics for “having established laboratory experiments as a tool of empirical economic analysis” (Nobelprize.org 2002). Although not mentioned in the Nobel Prize announcement, one of Smith’s lasting contributions to economics has been to provide empirical evidence regarding how markets work. In particular, his research on market equilibrium shows how relatively simple markets with a paucity of information can reach a Pareto efficient outcome. In a 1982 paper, Smith summarizes his research program on market equilibrium and links his research to what he calls the “Hayek Hypothesis,” namely that “Strict privacy together with the trading rules of an institution are sufficient to produce competitive market outcomes at or near 100% efficiency.” (p. 167) The research program on how the institutional details of a market impact its equilibrium properties continues to this day. Although Smith and many others have investigated extensively how the institution affects the outcome, one area of market research that is still relatively unexplored is the precise way in which traders reach an equilibrium in a market. To understand what we mean by this statement, an analogy may be helpful. William Harvey, a 17th century physician, is credited for discovering the basic workings of the pulmonary system. He showed, for example, that the heart propels the blood throughout the body, from arteries to veins and back to the heart. Nonetheless, he was unable to describe exactly how blood moves from the arteries back to the veins (Boorstin 1983). This fact didn’t make his theory wrong, it simply made it incomplete. Current economic models of markets are like Harvey’s model of the pulmonary system — basically correct, but lacking fine details. That is, economists know a great deal about markets and market equilibria, but relatively little about the process by which markets generate equilibria. Walras’ (1954) analysis of all markets clearing through the use of a single auctioneer, although a valuable theoretical tool, has long been recognized as an inaccurate description of the market clearing process. In fact, the price generation process at its most basic level of analysis is still quite mysterious to economists. How do individual buyers and sellers meet in the market place and determine a price? We know this happens, and we know a lot about what causes price to change, but we know very little about the actual equilibration process. This topic has been investigated by a number of researchers and using a variety of methodologies, but still remains substantially unknown. Theorists have looked at the negotiating problem for a long time, but have made only a little progress. Economists have no models of how people behave in an unstructured bargaining environment, although there has been progress made in structured bargaining environments. For example, Rubinstein (1982) analyzes a situation in which two players make sequential offers as to how to split a pie, and Smith (1991) contains a wealth of information on experimental economics’ investigations of the double auction mechanism. Unfortunately, there is still much economists do not know about the market clearing process. One theory regarding how markets reach equilibrium is the Marginal Trader Hypothesis, which posits that a small group of relatively well-informed traders move the market to the equilibrium price and help to keep the price at the “correct” level. Forsythe et al. (1992) invoke this hypothesis to help explain the remarkable accuracy of the Iowa Political Stock Market. This market’s predictions for the vote share of the candidates in the 1988 and 1992 U.S. presidential elections were more accurate than any poll. The Marginal Trader Hypothesis (MTH) has a long history in economics, and has the advantage of “feeling” correct. Even so, the MTH has received some criticism. Brüggelambert (2004) provides evidence against the MTH, and Surowiecki (2004) also argues against this hypothesis. However, the MTH appears not to have been rigorously examined. In this paper we use a unique data set to attempt to draw some conclusions about the MTH. We compare the results of two similar decision tasks whose institutional details are slightly different in order to determine if marginal traders are the cause of the market’s accuracy. Specifically, we compare the aggregate performance of the Iowa Electronic Markets to the performance of college students on a simple predictive task — guessing the class average on an exam. Because no trading is involved in the average-guessing task, the aggregate accuracy on this task cannot be due to the effects of marginal traders or the MTH. We argue that the overall accuracy of both the electronic markets and the average-guessing task are broadly the same, and then examine certain characteristics of “marginal traders” and compare them across the two institutions. We find little difference across institutions, implying that the MTH may not be the explanation for the market’s accuracy. The outline of this paper is as follows. First we will describe the Iowa Electronic Markets, the results of those markets and how those results support the Marginal Trader Hypothesis (MTH). In the next section we will provide some arguments against the MTH. In the third section of the paper we will precisely outline how we will attempt to test the MTH. In the fourth section of the paper we will describe our own decision task, including its similarities and differences from the task traders make in the Iowa Electronic Markets. The fifth section will present data showing that performance on the two tasks is similar, while in the sixth section we use our data to test the MTH. Section seven concludes the paper.",5
35,2,Journal of Economics and Finance,27 June 2009,https://link.springer.com/article/10.1007/s12197-009-9090-6,The exchange traded funds’ pricing deviation: analysis and forecasts,April 2011,Richard A. DeFusco,Stoyu I. Ivanov,Gordon V. Karels,Male,Male,Male,Male,"In this paper, we study the pricing deviations of the three most liquid Exchange Traded Funds (ETFs) from the price of the underlying index. We examine Spider (the S&P500 tracking ETF); Diamonds (the Dow Jones Industrial Average tracking ETF); and Cubes (the NASDAQ 100 tracking ETF) and find that their price deviation is stationary and predictable. Gastineau (2004) finds underperformance by ETFs when compared to index mutual funds (in terms of tracking error) and points out that the reason for the weakness is the formal specification of the ETF management policies; namely, the timing of declaration of creations or redemptions by investors. The presumption in the investment community is that ETFs, due to their nature of not requiring significant management involvement in the day-to-day running of the ETF portfolio, employ computers to monitor and clear any mispricing of the ETF. We show that the policies to adjust the price imbalances via creation and redemption of ETF units are effective, but the price deviation is predictable and nonzero. The pricing deviation is specific for ETFs and does not exist in index mutual funds. Therefore, it can be considered an extra cost of trading and holding ETFs, in addition to the explicit transaction costs, such as brokerage and maintenance fees and bid-ask spread. The reason for the predictability of pricing deviation stems from the stationarity. The reasons for the pricing deviation being nonzero are the specific price discovery processes and dividend accumulation and distribution for this asset class. We conduct a study of the ETF pricing deviations and provide a new metric for the performance of ETF literature. Based on the extant research of the informative usefulness of security baskets, and of certain ETFs in particular, and the lack of comprehensive and general study of the pricing deviation of ETFs, it appears that there is a strong theoretical and empirical groundwork to justify extended research in this field.",27
35,2,Journal of Economics and Finance,24 July 2009,https://link.springer.com/article/10.1007/s12197-009-9096-0,Timing of price clustering and trader behavior in the foreign exchange market: evidence from Taiwan,April 2011,Hao-Chen Liu,,,Unknown,Unknown,Unknown,Unknown,,
35,2,Journal of Economics and Finance,25 July 2009,https://link.springer.com/article/10.1007/s12197-009-9095-1,Sarbanes-Oxley wealth effects: focus on technology firms,April 2011,Aigbe Akhigbe,Anna D. Martin,Melinda Newman,Unknown,Female,Female,Female,"The start of the new millennium found the technology sector on the verge of a market bubble burst, and firms across multiple sectors plagued with the outbreak of accounting scandals. Prompted by such historic occurrences, the landmark Sarbanes-Oxley Act of 2002 was passed and applied to all firms listed in the U.S. with reporting obligations to the U.S Securities and Exchange Commission. It may be that enforcement of legislated structures will have different implications for firms with differing characteristics. For example, shareholders of firms with high information asymmetry and resultant discounted equity prices may benefit from the Act”s requirements to improve transparency. Alternatively, a requirement for independent board and/or board committee members may not be an efficient governance structure for firms with high information asymmetry (e.g., Linck et al., 2005). Because technology firms have unique characteristics, greater information asymmetry and greater litigation exposure, we argue technology firms are differentially affected by the passage of the SOX legislation. While previous studies examine the wealth effects of SOX passage (Akhigbe and Martin, 2006; Chhaochharia and Grinstein, 2007; Li, 2007; Litvak, 2007; Zhang, 2007; Li et al. 2008), none have separately examined the effects on technology firms. Thus, one purpose of this study is to examine the wealth effects of events leading to the passage of SOX for all firms and ascertain whether the effects are sensitive to measures of governance and information asymmetry. We hypothesize that the valuation effects from the passage of SOX depend on investors” expectations of the net effects of 1) compliance costs, with relatively high costs projected for small and well-governed firms, 2) stronger governance, with relatively strong benefits expected for firms with less independent boards, weaker shareholder rights, and weaker external monitoring, 3) reductions in information asymmetry, which may particularly benefit those firms that are high-growth, that have high stock price volatility, and that invest heavily in research and development (R&D), and 4) costs associated with reductions in risk taking, which may also be expected for firms with high information asymmetry. Across our full sample of 1,158 firms, we find that wealth effects are less favorable for firms that likely will incur high compliance costs (e.g., small firms and firms with independent boards) and more favorable for firms that are expected to benefit from improved governance (e.g., firms with less independent boards and highly-levered firms) and improved transparency (e.g., firms with high growth expectations, R&D investment, and stock price volatility). These results confirm and extend the previous work of Akhigbe and Martin (2006), Chhaochharia and Grinstein (2007), and Litvak (2007). The second and main purpose of this study is to focus on the wealth effects of SOX for technology firms due to their unique characteristics. First, firms in technology-intensive industries tend to be high growth, R&D intensive firms with greater stock price volatility. This suggests that technology firms may have higher levels of information asymmetry, on average. Thus, technology firms may benefit to a greater extent from SOX passage if the increased transparency required by SOX reduces information asymmetry. Second, the increased culpability of management and allowances for litigation within SOX may especially influence technology firms to curb risk taking, as they tend to be more susceptible to litigation (Kasznik and Lev, 1995; Johnson et al., 2001). Additionally, Linck et al. (2005) suggest that small, less independent boards may be the optimal form of governance for firms with high information asymmetry. Because technology firms tend to invest in innovative projects that likely require proprietary knowledge, a flexible board structure with discretionary power may be particularly beneficial for these firms. Thus, technology firms may experience less favorable or negative wealth effects from SOX passage if the Act”s increased liability and uniform governance requirements impose significant costs on these firms. Overall, we expect the passage of SOX to invoke a differential wealth response in technology firms. Our analyses show that the passage of SOX did lead to significantly different wealth effects for technology firms. That is, the portfolio of 218 technology firms experienced positive wealth effects whereas the portfolio of 940 non-technology firms experienced negative wealth effects in response to events indicating stringent reform legislation. Furthermore, we find that board independence, growth expectations, and R&D investment are sources of the differential response for technology firms.",1
35,3,Journal of Economics and Finance,13 August 2009,https://link.springer.com/article/10.1007/s12197-009-9098-y,Business cycle and aggregate industry mergers,July 2011,Srdan Komlenovic,Abdullah Mamun,Dev Mishra,Unknown,Male,Unknown,Male,"Compared with direct investment, mergers are a more efficient means of expanding shortage capacity resulting from an increase in industry demand (Becketti 1986) and of reducing excess capacity resulting from a decline in industry demand (Jensen 1993). Furthermore, Shleifer and Vishny (1992) conjecture that firms are credit constrained during recessions and are thus unable to pay fundamental value to targets, and vice versa, during a boom. These arguments together imply that mergers can be pro-cyclical. However, there is a paucity of empirical studies on the pro-cyclicality of aggregate merger activities—exceptions are early merger studies that provide inconclusive evidence. Early studies (Nelson 1959, 1966; Weston 1961; Gort 1969; Melicher et al. 1983; and Becketti 1986) that investigated the role of underlying economic activity on aggregate mergers largely ignored industry-level factors.Footnote 1 Recent merger studies (Mitchell and Mulherin 1996; Andrade et al. 2001; Andrade and Stafford 2004; and Harford 2005), however, have added to our understanding of the determinants of industry-level merger activity and the reasons behind the uneven distribution of aggregate mergers over time. However, these studies largely ignore the role of economic activity (macroeconomic factors) in general and business cycle in particular on industry-level merger activity. Among recent studies, Maksimovic and Phillips (2001) provide weak evidence that aggregate mergers are pro-cyclical. Mitchell and Mulherin (1996), who examine industry-level aggregate mergers, suggest that “…a fruitful research design would consider the joint effect of macroeconomic and industry-level factors in modeling the behavior of takeovers over time” (p. 195). However, none of the existing studies, to the best of our knowledge, investigates the pro-cyclicality of mergers at the industry-level. This study fills this gap by empirically examining the relationship between industry-level aggregate mergers and business cycle while controlling for industry-level determinants of merger activity. This is the first study to recognize and control for industry effects while examining the effect of business cycle in merger activity. Likewise, anecdotal evidence suggests that mergers within (i.e., related) and outside (i.e., unrelated) an industry may have different motivations. For example, a related merger may aim to build economies of scale and/or build capacity instantly in order to meet increased aggregate demand. On the other hand, an unrelated merger may be motivated to diversify a firm’s business risk, take advantage of economies of scope, and/or respond to decreased aggregate demand (e.g., due to saturation) in the acquirer’s industry. We identify each merger according to the acquirer’s and target’s industries using the 48 industry groups to classify mergers into related (e.g., horizontal) and unrelated industries according to Fama and French (1997). This classification allows us to investigate i) the relationship between business cycle and aggregate mergers at the industry-level while controlling for other factors and ii) the role of business cycle in related and unrelated mergers separately. We capture the level of economic activity using the Chicago Fed National Activity Index (CFNAI). The CFNAI is made up of 85 different business/economic series representing the current level of economic activity. Accordingly, it provides information on the status of economic activity that a firm’s management may use in adjusting its strategies in response to a change in economic activity (e.g., expand or contract production). The use of CFNAI is a departure from the existing literature, which primarily relies on National Bureau of Economic Research (NBER) business cycle dates, or a single series (e.g., industrial production by McQueen and Roley 1993). The advantage of using the CFNAI data over NBER business cycle dates is that the NBER dates provide the status of the economy with substantial lag (e.g., it may take several years before a stage of the economy is made public), meaning that such information will have little value for managerial decision-making. We separately examine the effect of business cycle on related and unrelated mergers. In the univariate tests, we find that industry-level merger activity is pro-cyclical. This result is practically similar for both related and unrelated mergers. Furthermore, in the panel Tobit tests, we find robust evidence that industry-level mergers are pro-cyclical. This evidence holds after controlling for macroeconomic and industry-level factors that are likely to affect merger activity, e.g., interest rates, stock market returns, industry-level capacity utilization, industry concentration, industry-level cash holdings, industry-level market valuation (market to book), and industry-level leverage (debt to equity). All of these control variables maintain theoretically (empirically) expected relationships with aggregate mergers. We also control for proxies of neoclassical theory (e.g., industry shocks and deregulation) and behavioral theory (e.g., book to market and the desperation of Tobin’s Q). We find that proxies of both neoclassical and behavioral theories explain merger activity in general; however, the pro-cyclicality of mergers is not fully captured by any of these theories. We offer several innovations over the existing literature on aggregate mergers. First, by studying industry-level aggregate mergers, we show that although merger intensity varies by industry, mergers are pro-cyclical. Second, we show that proxies of behavioral and neo-classical theories of mergers significantly explain industry-level mergers; this is consistent with the past literature on industry-level aggregate mergers. However, these proxies do not capture all the variation of industry-level mergers over time. Our proxy of business cycle captures a large part of this unexplained variation. For example, a 1.5 standard deviation increase in the proxy of business cycle on average increases related merger activity (measured as a percentage of industry assets) between 0.049% and 0.110%. On the other hand, the same variation in the proxy of business cycle increases unrelated merger activity between 0.124% and 0.232%. Third, we use the most comprehensive proxy of business cycle that relies on current information on economic activities available at that particular time. The contemporaneous nature of this business cycle proxy makes it better suited for use by managers in making strategic decisions (e.g., expansion and contraction of business units). Finally, yet importantly, by using this most comprehensive proxy of business cycle and by breaking down mergers by industry for the longest possible period, we provide more conclusive evidence that mergers are pro-cyclical. This evidence helps to shed a positive light on the existing mixed evidence on the effect of business cycle on aggregate merger activity.",19
35,3,Journal of Economics and Finance,07 November 2009,https://link.springer.com/article/10.1007/s12197-009-9110-6,Temporary open market operation on MBS repos: any foreshadowing of the financial crisis of 2008?,July 2011,Ozgur (Ozzy) Akay,Drew B. Winters,,Unknown,Male,Unknown,Male,"The Federal Reserve interacts with the market on a daily basis through open market operations to manage the money supply. This paper examines those interactions across the decade of the 2000s in advance of the financial crisis of 2008. The goal of the paper is to determine if anomalous open market operations could have provided the Federal Reserve with early warning of the impending crisis. The Federal Reserve Act of 1913 created the Federal Reserve System. The Act specifies four goals for the Federal Reserve: (1) create a monetary authority to control the money supply, (2) create a lender of last resort to provide liquidity in times of crisis, (3) create an efficient nationwide check clearing system, and (4) create a bank supervision system.Footnote 1 This paper relates to the monetary authority role of the Federal Reserve. The Federal Reserve has three monetary policy tools: (1) reserve requirements, (2) discount window rate, and (3) open market operations. The Board of Governors of the Federal Reserve System is responsible for reserve requirements and the discount window rate. The Federal Open Market Committee (FOMC) is responsible for open market operations Once the policy objectives are set by the FOMC, the Domestic Trading Desk at the Federal Reserve Bank of New York (desk) implements daily trading activities.Footnote 2 On a daily basis the desk must decide whether to: (1) add reserves, (2) drain reserves, or (3) abstain from trading. When the desk decides to change the money supply (change reserves), it has two choices for its open market operations: (1) outright operations or (2) temporary operations. Outright operations are the purchase or sale of treasury securities. Temporary operations are transactions in repurchase agreements (repos) and reverse repurchase agreements (reverse repos). Repos are the perfect security for temporary operations because repos automatically unwind at maturity. This paper examines temporary open market operations. The desk interacts with the market on a daily basis as it conducts open market operations.Footnote 3 Each morning the desk gathers market information to determine if open market operations are required to achieve current monetary policy objectives. At about 9:30AM the desk announces that day’s operations and conducts the necessary trading with primary dealers. A typical day has trading of $5 billion to $10 billion, which requires less than 10 min to complete.Footnote 4 Through this daily interaction with the market the Fed has an opportunity to observe when the market is uncomfortable or tight. These observations could provide the Fed an early warning of changing market conditions (in this paper, the current financial crisis). This paper examines market yields on temporary open market operations to determine if these yields provide signals of the impending financial crisis. Specifically, we examine market yields from daily temporary open market operations from 7/7/2000 and 12/31/2008 for large spikes in yields. In a related paper, Cecchetti (2009) analyzes the spread between the U.S. agency and Treasury security yields during the financial crisis of 2007–08, specifically between January 2007 and May 2008. He documents that the spread widens starting in August 2007 and reaches 90 basis points in May 2008. He attributes the increase in the spread mainly to the decreasing yields on Treasury securities caused by increasing demand for these securities, a phenomenon named “flight to quality” in the financial literature. In another closely related paper, Fleming et al. (2009) uses the data on repurchase agreements conducted as a part of open market operations to analyze the response of the spread between the rates of Treasury and mortgage-backed securities to the liquidity enhancement program of the Fed, known as the Term Securities Lending Facility (TSLF).Footnote 5 They document that the spread between the repo rates of Treasury securities and agency and mortgage-backed securities narrows after the Fed provides liquidity to the financial markets through TSLF. They also attribute the narrowing in the spread to the changes in the Treasury repo rates, rather than changes in the rates of the agency and mortgage-backed securities. Different from these two papers, we look at the events before the financial crisis to explore whether there were any signals of the impending crisis. We identify four anomalous spikes in market yields on repos on mortgage-backed securities (MBS). Next, following Fleming et al. (2009), we adopt an event study approach and conduct a detailed analysis of these four events to determine if anything surrounding these events should have signaled the impending financial crisis to the Fed. We find no evidence of signals of the impending financial crisis. Instead, each spike is simply an abnormally high amount of securities offered to the Fed for 1 day on four separate isolated events.",
35,3,Journal of Economics and Finance,10 October 2009,https://link.springer.com/article/10.1007/s12197-009-9108-0,Minimax price bounds in incomplete markets,July 2011,Unyong Pyo,,,Unknown,Unknown,Unknown,Unknown,,
35,3,Journal of Economics and Finance,01 April 2010,https://link.springer.com/article/10.1007/s12197-010-9124-0,Mean reversion and long memory in African stock market prices,July 2011,Emmanuel Anoruo,Luis A. Gil-Alana,,Male,Male,Unknown,Male,"Several authors have found evidence of mean reversion in stock market prices (see, for example, Poterba and Summers 1988 and Fama and French 1988). The standard econometric approach to settle this issue empirically relies on establishing the order of integration of the series by carrying out unit root tests. Standard methods are the ADF-test (Dickey and Fuller 1979), PP (Phillips and Perron 1988), KPSS (Kwiatkowski et al. 1992), etc. According to these methods, the series is either nonstationary I(1) or stationary I(0). In the former case, that includes the random walk as a particular case, shocks are of a permanent nature while in the latter case shocks are transitory and therefore, mean reverting, disappearing in the long run. More recently, the possibility of fractional orders of integration with a slow hyperbolic rate of decay has also been taken into account. Thus, the number of differences required to render a series stationary I(0) may not necessarily be an integer value (usually 1) and may be a value between 0 and 1, or even above 1. This approach has been widely employed to analyze financial data in developed countries. Examples are the papers of Crato (1994), Cheung and Lai (1995), Barkoulas and Baum (1996), Barkoulas et al. (2000), Sadique and Silvapulle (2001), Henry (2002), Tolvi (2003) and Gil-Alana (2006) among many others. In this context, if stock market prices are I(0) they are mean reverting, with shocks disappearing relatively fast; if they are fractionally integrated (I(d)) with d above 0 but smaller than 1 (i.e. 0 < d < 1), prices still display mean reversion though the adjustment takes a longer time, in fact, longer as higher is the value of d. On the other hand, if stock prices are I(1) no mean reversion is obtained and the same happens if the series is I(d) with d > 1.Footnote 1
 This paper examines the existence of mean reversion in the stock market prices in several African countries, namely, Egypt, Morocco, Tunisia, Nigeria, Mauritius, Kenya, South Africa, Zimbabwe, Botswana and Namibia, by means of fractionally integrated techniques. Like developed financial markets, African emerging capital markets provide investors with opportunities to diversify their portfolios. In addition, the macroeconomic dynamics that govern stock market returns in African economies are unarguably different from those of developed countries. Capital markets in emerging economies tend to be underdeveloped and most likely inefficient. Market thinness and non-synchronous trading are among other factors that may affect stock returns in emerging capital markets of Africa. Harvey (1995) points out that emerging capital markets tend to have higher expected returns and display more volatility than those of developed capital markets. We examine these markets by means of long range dependence techniques. To the best of our knowledge there are no applications involving these techniques in African countries. The structure of the paper is as follows: Section 2 briefly reviews the literature on stock market prices in developing countries. Section 3 presents the data and the empirical results based on fractional integration, extending also the model to allow for a structural break in the data. Section 4 contains some concluding comments.",14
35,3,Journal of Economics and Finance,30 July 2009,https://link.springer.com/article/10.1007/s12197-009-9097-z,Jump risk and cross section of stock returns: evidence from China’s stock market,July 2011,Haigang Zhou,John Qi Zhu,,Unknown,Male,Unknown,Male,"The theoretical importance of incorporating sudden, infrequent, but often drastic price movements into asset pricing models has long been emphasized in the finance literature since Press (1967), Merton (1976), and Cox and Ross (1976). Previous studies have demonstrated the substantial impact of jumps in the field of portfolio and risk management, option and bond pricing, and hedging in various financial markets.Footnote 1 There is, however, surprisingly sparse theoretical or empirical effort on understanding the dynamic relation of jump risk to cross-sectional expected stock returns. We follow a stylized SDF (Stochastic Discount Factor)-based diffusion-jump model to examine the testable implications for the cross sectional relation between expected excess returns and jump intensities. The latter is a proxy of firm’s exposure to jump risk and defined as the monthly sum of absolute daily jump returns. Using high frequency trading data from China, we provide empirical evidence from an emerging market that is complementary to the existing literature on the topic predominantly resting on the U.S. market. In this paper, we apply the continuous-time modeling framework of Yan (2008) to empirically test the systematicity of jump risk and to disentangle the risk premium borne from jumps. The model posits the stochastic discount factor and stock prices follow correlated diffusion-jump processes composed of a Brownian component and a Poisson component. The expected excess stock return compensates for two sources of systematic risks: a diffusive component due to the covariance between the Brownian motions of the SDF and of the stock price, and a jump component when the Poisson processes of the SDF and of the stock price are dependent. The expected excess stock return is solely determined by the conventional market β if and only if jump risk is unsystematic (that is, the Poisson processes of the SDF and of the stock prices are uncorrelated), otherwise part of the cross-sectional variation in the expected excess returns can be predicted by jump characteristics of stocks. One implication of the model is that, if the average jump size of SDF is positive, the expected excess return is monotonically decreasing in the jump intensity of the stock, ceteris paribus. We further test the profitability of a simple trading strategy based on this cross-sectional pattern. The accurate detection of jumps and concrete estimation of their sizes are crucial to measure the jump intensity, a key element in our empirical analysis. A flurry of studies have proposed various methods to detect jump arrivals and to measure realized jump sizes.Footnote 2 Among them, we adopt a non-parametric approach suggested by Lee and Mykland (2008). Their detection technique can precisely identify the intervals over which jumps occur and allow for multiple occurrence of jumps in a trading day. Moreover, for the purpose of obtaining a joint measure of jump frequency and jump size, the Lee and Mykland method is more straightforward and intuitive compared to approaches developed in other studies. We employ the Lee and Mykland jump detection technique to high frequency trading data for each A Share stock traded on the Shanghai Stock Exchange from January 2003 to September 2008. For each stock, we identify the intra-day intervals over which jumps arrive, calculate jump returns over the intervals, and define monthly accumulative absolute jump returns a proxy for the jump intensity that embodies the information of both average jump size and jump frequency of the stock. We then form decile (or quintile) portfolios sorted on the jump intensity and hold them for one month. The Fama-MacBeth test presents strong evidence in favor of the existence of systematic jump risk in stock portfolios and confirms our conjuncture that expected excess stock returns are monotonically decreasing in jump intensity. A zero-cost portfolio exploiting the return spreads between the top and bottom decile portfolios could earn an annualized return as high as 24% with an annualized Sharpe ratio of 1.67. The results are robust to a set of firm characteristic variables, including market risk, past and idiosyncratic returns, size, book-to-market ratio, lagged volatility, turnover ratio, and illiquidity. The rest of the paper proceeds as follows. In Section 2, we present the theoretical model, derive the testable implications, and briefly describe the method of detecting jumps and estimating jump sizes. Section 3 details the construction of variables and testing portfolios. We present empirical results in Section 4 and conclude in Section 5.",9
35,3,Journal of Economics and Finance,02 September 2009,https://link.springer.com/article/10.1007/s12197-009-9100-8,Monday returns and asset pricing,July 2011,Jorge Brusa,Wayne Y. Lee,Pu Liu,Male,Male,,Mix,,
35,3,Journal of Economics and Finance,29 September 2009,https://link.springer.com/article/10.1007/s12197-009-9106-2,Financial development and economic growth in Vietnam,July 2011,Sajid Anwar,Lan Phi Nguyen,,Male,,Unknown,Mix,,
35,3,Journal of Economics and Finance,22 April 2010,https://link.springer.com/article/10.1007/s12197-010-9134-y,Bank loan commitments and Material Adverse Change clause,July 2011,Chung Baek,Jongwook Reem,Thomas A. Jackman,,Unknown,Male,Mix,,
35,4,Journal of Economics and Finance,16 April 2010,https://link.springer.com/article/10.1007/s12197-010-9127-x,Dangers of commitment under rational expectations,October 2011,George A. Waters,,,Male,Unknown,Unknown,Male,"Commitment by a monetary policymaker is effective due to its impact on public expectations. Therefore, results about the potential benefits and dangers of commitment could be sensitive to assumptions about the policymaker’s knowledge of and the formation of expectations. Waters (2009) studies the gains to commitment in a New Keynesian frameworkFootnote 1 with forward looking interest rate rules where public expectations are formed by least squares learning, and shows that the modified commitment setting advocated by Blake (2001) and Jensen and McCallum (2002) may not be optimal under learning if there are errors in the policy rule. However, one might suspect that this result is independent of learning and is driven by the presence of the errors in the policy rule. The present paper refines these results by studying the same class of interest rate rules under rational expectations. Interest rate rules that respond to public expectations are studied in Evans and Honkapohja (2003, 2006), who show that such rules have the desirable features of determinacy and expectational stabilityFootnote 2 under both discretion and commitment. Waters (2009) extends their work, reporting determinacy and expectational stability results for a range of commitment levels and simulating the model under least squares learning to find the optimal level of commitment. CommitmentFootnote 3 implies that policy must be history dependent, as emphasized by Woodford (1999). The policymaker consistently sets the target rate in response to lagged variables, output in the present context, so that public expectations must take them into account as well. A higher level of commitment corresponds to a larger coefficient on lagged output in the interest rate rule. Without errors in the policy rule, the behavior of the model under learning and rational expectations is virtually identical. Modified commitment is optimal, but the performance of the policy rule deteriorates rapidly for greater levels of commitment, particularly for a policymaker concerned about output stabilization, as opposed to inflation stabilization. A reasonable conjecture is that the combination of this asymmetry around modified commitment in the policymaker’s losses and the policy rule errors implies that modified commitment is not optimal. However, a primary finding of this work is that modified commitment remains optimal under rational expectations, which implies that learning is a necessary condition for the optimality of lesser degrees of commitment. The asymmetry of the losses for greater levels of commitment does play a role if the policymaker is uncertain about the true parameter values. Simulation with errors in the policy rule due to errors in the estimate of parameter values shows that modified commitment need not be optimal, even under rational expectations. Therefore, a policymaker who adopts history dependent rules must have a high degree of confidence in their model of the economy.",2
35,4,Journal of Economics and Finance,16 June 2009,https://link.springer.com/article/10.1007/s12197-009-9087-1,Market impact of international sporting and cultural events,October 2011,António Miguel Martins,Ana Paula Serra,,Male,Female,Unknown,Mix,,
35,4,Journal of Economics and Finance,03 September 2009,https://link.springer.com/article/10.1007/s12197-009-9102-6,Predictability of the U.S. Dollar Index using a U.S. export and import price index-based relative PPP model,October 2011,Axel Grossmann,Marc W. Simpson,,Male,Male,Unknown,Male,"Multinational corporations and international investors alike have long been interested in either speculating with or hedging foreign currency exposures. While many speculators or hedgers are interested in the specific bilateral exchange rates between two currencies, others may be interested, broadly, in the general level of the U.S. Dollar. The general level of the U.S. Dollar is also of interest to policymakers, as it influences the overall level of economic activity, the competitiveness of U.S. businesses, and the prices that U.S. consumers pay. All of these market participants and policymakers would find it useful to have a guide to the general equilibrium level of the Dollar that perhaps could be used in predicting future changes in the Dollar’s value. Although the last two and a half decades have produced innumerable attempts to understand the underlying behavior of exchange rates, the academic literature is replete with studies that demonstrate the inability of economic models to outperform a random walk in forecasting exchange rates (e.g. Meese and Rogoff 1983; Mark 1995; MacDonald and Marsh 1997; Kilian 1999; Mark and Sul 2001; Qi and Wu 2003; Xu 2003; Nucci 2003). Morey and Simpson (2001a), however, demonstrate that a relative purchasing power parity (PPP)-based model, first proposed by Hakkio (1992), could be useful in predicting the direction of change in several bilateral exchange rates. Given the importance of the U.S. Dollar Index for investors and policymakers the aim of this paper is to: 1) construct a relative PPP-based equilibrium index, or equilibrium exchange rate (EER) for the U.S. Dollar over the post-Bretton Woods period (1973 to 2005) that could allow agents to determine the extent to which the nominal U.S. Dollar is over- or undervalued; 2) to investigate, using two different methodologies, if deviations from the constructed EER provide any information about the future path of the U.S. Dollar. First, following the approach by Hakkio (1992) and Morey and Simpson (2001a), probabilities of convergence of the nominal U.S. Dollar Index towards its constructed EER are calculated. Probabilities higher than 50% would indicate that the constructed EER provides investors with some information about the future change of the U.S. Dollar. Second, a linear recursive forecasting technique is utilized to determine the horizon over which such a relative PPP-based model is able to outperform a pure random walk in forecasting the nominal U.S. Dollar Index out-of-sample. The paper provides the following interesting results. First, the constructed EER seems to perform well as a model of the equilibrium level of the U.S. Dollar index as we find half-lives of less than 0.60 years for the early post-Plaza Accord period. Second, the constructed relative PPP-based EER is successful in predicting the direction of change in the nominal U.S. Dollar Index up to 68.4% of the time, which indicates that the EER is able to provide investors with some information about the future movement of the value of the U.S. Dollar. Finally, we provide evidence that a simple, linear, recursive technique that uses the EER is able to statistically significantly outperform the random walk in predicating the value of the U.S. Dollar index over terms of less than four months.",4
35,4,Journal of Economics and Finance,26 November 2009,https://link.springer.com/article/10.1007/s12197-009-9111-5,The patterns of cross-border portfolio investments in the GCC region: do institutional quality and the number of expatriates play a role?,October 2011,Faruk Balli,Rosmy J. Louis,Mohammad Osman,Male,Unknown,Male,Male,"The objective of this paper is to shed lights on the determinants of portfolio investments to Gulf Cooperation Council (hereafter GCC) countries by investigating the role played by market forces, labor in-migration, cultural affinities, and institutional quality. The GCC is an interesting case begging for in-depth understanding of capital inflows for many reasons. Following the oil crisis of the 1970s and early 1980s, the GCC had seen the largest increase in oil and gas export revenues. Although part of this money was invested at home to build infrastructure and develop the agricultural sector, a large portion was also allocated to finance investment in the United States and Europe and heavy weaponry. These investments took place to respond to two urgencies at the time: the need for protection against political instability and conflicts in the region and the need to reduce reliance on oil revenues. During the 1970s and the 1980s when war was raging in the Middle-East, it was inconceivable for the rather risk-averse investors to allocate their portfolio in the region since the people who live there and the governments themselves were expatriating capital in the hope of not losing it all at once in a conflict. Moreover, in most of these countries, the law did not permit foreigners to own properties and have their own businesses without a local sponsor. Although massive investments of GCC oil revenues in US and European markets have solidified political relationships with the West and have provided the much needed protection to keep the political landscape intact, domestic macroeconomic structural reforms did not deliver the good as anticipated. It did not take long for these countries (mostly Saudi Arabia) to realize that supports to the agricultural sector for example out of oil revenues were not the best usage of financial resources when oil prices tumbled in 1983 and thereafter (Hourani et al. 2004). However, a series of unfortunate events that took place in the 1990s and 2000s has brought to the GCC the opportunity to correct the mistakes of the past as oil and gas prices were climbing again to reach record levels in 2000 and thereafter. Also, these events have changed the patterns of portfolio investments to GCC countries. With the September-11 attack on US soil, many individuals of Arab descent who had investments in the US and other western countries were looking for a safe haven as fear was mounting that these countries might freeze their capital alleging that they have ties to terrorist organizations. No other countries in the Middle-East and Africa represented a better alternative than the GCC region. The invasion of Iraq in 2003 and the rise of extremism giving rise to a halt in the production of oil in Iraq and a reduction in the world supply of crude oil and natural gas have also benefited the GCC as the price of oil had reached levels never seen before. With the new windfall of oil and gas revenues, the GCC has this time adopted a different strategy by investing in industries in which they can build a competitive advantage. These include petrochemical, banking and financial services, airline, tourism, real estate, telecommunications, steel, and transportation, to cite just a few (Fasano and Iqbal 2003). In terms of portfolio investment allocation, a wider range of countries in the Middle-East, North Africa, and Asia have received their fair share while the US and Europe continue, due to political reasons, to receive the largest portion (World Bank 2005). What this new strategy seems to have created is a reciprocal and trustworthy partnership that also attracts capital from investors of these regions and the rest of the world to the GCC. Since governments are majority shareholders and exercise control in most of the major companies, the overly risk-averse investors are reassured that their investments are in good hands since governments are less likely to go bankrupt. As a result, initial public offerings (IPOs) by companies are often oversubscribed, reflecting investors’ drives for a share of previously wholly-owned companies by nationals and newly-formed corporations. Since the GCC stock markets are fairly new, share prices are initially offered at a bargain and returns are fairly substantial. Bley and Chen (2006) report that, in 2004 alone, foreign portfolio holders collected between 150 and 170 billion US dollars in profits from the GCC countries. Along with the massive investment brought about by rising oil revenues, fiscal disciplines coupled with low interest rates (on average between 3 and 4% in some cases) have also helped the GCC in creating macroeconomic conditions amenable to an orderly dynamic business environment. Translation costs and uncertainty about capital repatriation are minimal due to these countries’ long ties with the US dollar as their anchor currency. Reforms to further enhance benefits from technology transfer and massive capital inflows have been initiated in terms of new laws to protect property rights, combat corruption, and to ease ownership restrictions and immigration. These measures contrast squarely with the external environment in the Middle-East, Asia, and Africa where corruption, political upheaval, and poverty hinder economic development. The misfortune of the region has offered to the GCC a pool of cheap labor to capitalize on as they expand ambitious projects of infrastructure. The GCC’s commitment to foster both vertical and horizontal capital inflows as an important engine of their economies can be seen as a smart move as they are moving away from oil dependency to create industry and service-based economies capable of rivaling other economies in the international markets. The combined outcomes of this overall dynamic are that the GCC markets have become more and more important over the years for investors seeking higher returns and workers seeking better opportunities and for countries seeking investment projects.Footnote 1
 There have been some previous attempts at documenting the determinants of capital inflows to the Middle-East and North Africa. For example, in their study of international portfolio allocation, Lagoarde-Segot and Lucey (2007) find that risk diversification and the drive for higher profits explain investors’ inclination for the region. Sadik and Bolbol (2001) provide evidence of linkages between capital flows/foreign direct investment (FDI) and technology spillovers. They have shown that contributions to technology and productivity can emanate from FDI as well as other forms of capital formation. Mina’s (2007) examination of the location determinants of FDI inflows to the GCC could not find that FDI is stimulated by the production, prices, and proven reserves of oil. He instead found that relative oil utilization encourages FDI. These contributions without a doubt have deepened our understanding of portfolio inflows to the region in general and to the GCC in particular. However, the picture is still incomplete when a number of important factors such as investors’ preferences, regional and home biasness, labor in-migration, and relative institutional quality are being overlooked or not being compared to other successful regional blocs. In these respects, the present paper complements the existing literature in several aspects. It is to our knowledge, a primer for the GCC if not the whole Middle-East and North-Africa regions. We therefore argue that external environment, economic fundamentals, countries’ specific characteristics, economic ties between source and host countries, institutional quality of the host country relative to other potential host countries, the level of real income of source countries and the financial openness of the host country all can potentially explain international portfolio inflows to GCC countries. Most importantly, it is well known that factors such as distance, financial innovations, capital mobility, and the number of nationals from the source countries residing or working in the host country tend to strengthen economic ties amongst nations. For example, an Indian investor would find it easier to invest in, export and/or import to the GCC where Indians constitute the majority of the population of workers in comparison to other non-GCC countries. Therefore, the interesting question that we raise comes quite naturally: Can institutional quality, the number of expatriates, economic linkages between hosts and source countries and/or socio-cultural affinities explain the patterns of cross-border portfolio investments in the GCC region? We investigate the patterns of portfolio allocation to GCC financial markets using panel data analysis and compare the findings with those obtained for the OECD countries. Our results show that, in line with earlier contributions to the literature, bilateral factors such as trade volumes and the source country’s share of world market capitalization play a significant role in portfolio allocation to the GCC region. There is also a strong portfolio “GCC bias”. That is, a large share of the GCC investment comes from GCC members themselves. This bias is similar in nature to the portfolio Euro bias also observed.Footnote 2 It is the notable consequence of not only the high level of financial and economic integration that characterizes the GCC countries but also a reflection the ripple effect of post-September-11 reactions. We find that institutional quality is statistically significant in all estimated models indicating that investors’ perception about the quality of institutions is an important factor in portfolio allocation. The GCC receives a larger chunk of foreign portfolio investment because it has higher institutional quality relative to other competing countries in the region. Additionally, we explore the effect of labor in-migration to the GCC on capital inflows. We find that the greater the number of expatriates originated from a given source country living in the GCC the higher the volume of portfolio inflows from that country to the GCC. This can be explained by the unique structure of the GCC members where more than 80% of their population are non-resident aliens. The resulting effects are higher volume of goods and services traded (frequent trips back home), higher volume of remittances transferred and lower transaction costs. The remainder of the paper is organized as follows. Section 2 presents a multi-market portfolio model relating international portfolio allocations with bilateral linkages. Section 3 describes the data set and the construction of some of the key variables of interest. Section 4 presents the empirical findings and analyzes the determinants of cross border asset holdings. Section 5 presents the robustness check and Section 6 concludes the paper.",15
35,4,Journal of Economics and Finance,02 September 2009,https://link.springer.com/article/10.1007/s12197-009-9103-5,Dynamics of floating exchange rate: how important are capital flows relative to macroeconomic fundamentals?,October 2011,Wei Sun,Lian An,,,,Unknown,Mix,,
35,4,Journal of Economics and Finance,10 June 2010,https://link.springer.com/article/10.1007/s12197-010-9137-8,Credit ratings and long-term IPO performance,October 2011,Kam C. Chan,Yung Ling Lo,,Male,,Unknown,Mix,,
35,4,Journal of Economics and Finance,08 January 2011,https://link.springer.com/article/10.1007/s12197-010-9168-1,Will export taxes replace VERs?,October 2011,David Franck,Nadeem Naqvi,,Male,Male,Unknown,Male,"It is not uncommon for the heads of state to visit each other and announce at the end of the trip that the two countries have reached yet another bilateral trade agreement. The details of its implementation are typically left for future determination by working groups of these nations. The devil, though, lies in the detail. Protectionist tendencies do not simply disappear with the agreement; rather they are often redirected to other price or quantity restraints outside the agreement typically with less obvious effects. Visible quotas morph into less easily observable voluntary export restraints (VERs). Import tariffs are replaced with costly bureaucratic environmental standards. This is due to the fact that successive rounds of GATT/WTO negotiations, including the eight-year-old Doha round, have ostensibly eliminated tariffs, subsidies and the like, as evidenced by the ongoing dispute between the U.S.A and the European Union on the matter of subsidies related to Boeing and Air Bus, in 2010. But are they simply being replaced with other types of trade distorting policies?Footnote 1 As a consequence, interest in export taxes has gained new favor. Voluntary export restraints (VERs) in international trade are sometimes modeled as quantity constraints, with restriction-induced rents accruing to the exporting country.Footnote 2 On other occasions, they are modeled as export taxes in the exporting country.Footnote 3 The presumption is these two policies are equivalent. Whereas this equivalence may be true from the point of view of the importing country, we demonstrate that it does not hold for the exporting country. In particular, if the exporting country is a net recipient of real purchasing power, it prefers, in terms of real income gains, limiting its exports by export taxes rather than by quantity constraints, but if it experiences a net outflow of real purchasing power, it will implement quantity constraints on its exports.
 The significance of this result arises from, among other things, different real income effects of a transfer in the exporting country under the two alternative foreign trade policies. Suppose, for example, China is asked to reduce its textile exports to the United States. To accomplish this, China could place quantity restraints on its exports, or tax its exports. Which of these two alternative policies is employed does not matter to the United States, but from China’s point of view, they have different real income consequences. In particular, we demonstrate that the shadow prices of foreign exchange in the exporting country are different under the two alternative policies. Our remarks should not be construed to mean that it is incorrect to model VERs as export taxes or as quantity constraints. We merely point out one set of circumstances in which the exporting country will prefer export taxes, and another circumstance in which it will prefer quantity constraints. Thus the presumed equivalence between export taxes and VERs is, in this sense, strictly invalid. In Section 2, we examine VERs as quantity constraints. In Section 3, we investigate VERs as taxes on exports in the exporting country. Section 4 contains some concluding remarks. Our investigation is conducted using the GDP function approach. This has the advantage of embracing considerable generality in the production structure of the economy under consideration. For example, we place no restriction on the number of commodities or factors other than the there must be at least as many factors as there are final goods, so as to avoid flats in the production possibility hyper-surface, to eliminate the possibility of multiple equilibria. Intermediate goods and joint production is also accommodated by the GDP function approach.Footnote 4
",
36,1,Journal of Economics and Finance,15 August 2009,https://link.springer.com/article/10.1007/s12197-009-9099-x,Analysing contagion and bailout effects with copulae,January 2012,Gregor N. F. Weiß,,,Male,Unknown,Unknown,Male,"Contagion effects between banks have been a field of research since the 1930s when bank failures occurred in a domino-like fashion (see e.g. Calomiris and Mason 1997, for a study of bank failures during the Great Depression). In the context of bank contagion, one usually distinguishes between bank runs and bank panics with the former being confined to one specific bank and the latter being an irrational and indiscriminate withdrawal of deposits from all banks (Bhattacharya and Thakor 1993; Kaufman 1994, describes this irrational form of a bank panic as pure contagion) . More generally, bank contagion can also be defined as a transmission of information within the banking industry (see e.g. Gorton 1985; Bessler and Nohel 2000; Akhigbe and Madura 2001). Aharony and Swary (1983) define noisy (or firm-specific) bank contagion as an adverse effect of a bank failure on banks due to correlations between banks whereas pure contagion is caused by problems which are uncorrelated across banks. The existence of these contagion effects in banking is often explained by the presence of information asymmetries between banks and its stakeholders. As a single bank usually shares certain characteristics with it’s competitors (e.g. a similar customer base, credit portfolio or syndicated corporate loans), adverse effects on the bank could also implicate adverse effects on other banks and possibly other industries. Due to information asymmetries between banks and stakeholders, however, the latter might not be able to distinguish between affected and unaffected banks withdrawing deposits and repricing stocks indiscriminantly (Bessler and Nohel 2000). It is this danger of an irrational bank panic (possibly leading to a systemic risk) causing considerable costs to the financial sector that is often named as a justification for a state’s involvement as a lender of last resort and the necessity for regulation in banking (see e.g. James 1991; Goodhart and Huang 2005). Methodically, contagion effects in banking have often been studied by computing abnormal stock returns (see e.g. Akhigbe and Madura 2001; Gropp and Moerman 2004; Kabir and Hassan 2005). In these studies, contagion is presumed to be present if negative abnormal returns or increased volatility can be detected in the post-crisis period after the event that is supposed to be causing the bank panic. In addition to this, some authors have tried to use extreme value theory to estimate the number of co-exceedances (i.e. the number of joint occurrences of extreme events in the left tail of a bivariate series) in order to isolate contagion effects across banks (Gropp and Moerman 2004; Gropp and Vesala 2004). Simultaneously to the analysis of bank contagion, a different branch of research has concentrated on analysing contagion effects between financial markets in times of crisis (like e.g. in the Asian crisis of 1997). In this branch of research, early works concentrated on studying correlations between stock market indices (see e.g. Forbes and Rigobon 2002) whereas recent work has focused on substituting a correlation-based analysis by a more general copula-based approach (see e.g. Rodriguez 2007; Chen and Poon 2007). The aim of this paper is to propose a new methodical framework for the analysis of bank contagion. By combining a market model from classical event studies and copula methodology, the framework proposed in this paper allows for an analysis of bank contagion that directly assesses the changes of dependencies between banks instead of proxying contagion via abnormal returns. Furthermore, in the empirical part of this paper, I focus on detecting contagion effects and effects by bailout announcements for the near-collapse of German Deutsche Industriebank IKB AG (IKB) as a result of the subprime crisis and several announcements during Japan’s banking crisis in the 1990s. Although several previous studies have focused on contagion effects in banking (see e.g. De Bandt and Hartmann 2001, for a comprehensive overview of empirical studies), the effects of a state’s involvement as a lender of last resort on banking contagion have only scarcely been analysed in empirical studies. To the best knowledge of the author, this paper is the first one to analyse changes in the dependence structure of banks around bailout announcements. More precisely, the empirical study given in this paper tries to answer two questions: Firstly, did announcements of isolated crises at certain banks lead to contagion effects across the respective banking sector? Secondly, did the rescue efforts of the state of central banks as a lender of last resort limit or reverse these contagion effects? To answer these questions, abnormal stock returns are computed in a first step from a market model for all listed German and Japanese banks available from Thomson Financial Datastream. In a second step, contagion between banks is parameterised by two concepts based on copulae: Firstly, a convex combination of parametric copulae with different tail dependence characteristics is fitted to the abnormal returns with contagion being indicated by an increase in the coefficient of the lower tail dependent Clayton copula. Secondly, the extension of the well-known bivariate tail dependence coefficient to multivariate copulae proposed by Schmid and Schmidt (2007) is computed for the data to examine changes directly in the coefficient between announcements dates. The contributions of this article are numerous. Firstly, this paper extends the ongoing work on the empirical analysis of bank contagion by examining the success of bailouts in reversing contagion effects. By combining a market model and state-of-the-art copula methodology, this paper presents a new framework for directly assessing the impact of contagion and bailouts on the dependencies between banks. Furthermore, the results in the empirical study show that contagion could be observed in both samples after negative announcements of selected banks. In addition to this, the state’s announcements of a bailout did not simply reverse the changes in the dependence structure but led to a persistent shift from lower tail dependence to tail independence in the respective banking sector indicating that bailout announcements are successful in decreasing the probability of extreme joint downward movements of returns while leaving the probability of extreme joint upward movements unchanged. The remainder of this article is structured as follows. Section 2 discusses the theory on contagion effects and bailouts. In Section 3, the methodology and model specifications are described. Section 4 exhibits the data and presents the empirical findings. Concluding remarks are given in Section 5.",8
36,1,Journal of Economics and Finance,26 August 2009,https://link.springer.com/article/10.1007/s12197-009-9101-7,The interaction of corporate dividend policy and capital structure decisions under differential tax regimes,January 2012,Ufuk Ince,James E. Owers,,Male,Male,Unknown,Male,"Almost half a century after the seminal MM contributions (Modigliani and Miller 1958; Miller and Modigliani 1961), capital structure and dividend policy continue to be topics of great interest in the academic literature and for industry practitioners. In this paper we aim to contribute to the understanding of capital structure, dividend policy, and their interaction. We develop a model in which firm value is determined simultaneously by dividend payout and financial leverage. The model explicitly accommodates four different income tax rates: corporate income, personal interest, cash dividends, and capital gains. We apply the model to ten different U.S. tax regimes since 1979 and generate several testable predictions. Two key findings relate to the effect of tax rate differentials on the relationship between dividend payout, financial leverage, and firm value. When the dividend tax rate exceeds the capital gains tax rate, dividend payout can partially offset value-enhancing effects of leverage. When the two rates are close, dividend payout loses its moderating influence. Using the S&P 1500 universe, we obtain empirical results that are consistent with the model’s predictions. Table 1 shows that, there has been a significant variation in the tax rates over the past three decades. Under the tax code that was in effect until 2003, the personal tax rates applicable to interest and dividend income streams were essentially identical. In contrast, the marginal tax rates for dividend and capital gains portions of the equity income were typically significantly different. Recognizing this divergence, in our model we partition the income on equity into its two components, to which distinctly different tax rates apply. This enables the derivation of an “extended” Miller model that incorporates the effects of both leverage and dividend payout under one unified analytical framework. In particular, we obtain an equation that adds a third term (dividend payout) to the Miller (1977) expression. Examining the model and its comparative statics, we observe that dividend payout is an important factor in determining firm value above and beyond the effect of leverage. In the Modigliani and Miller (1958, 1963) and Miller (1977) models, which do not formally incorporate the dividend payout and focus only on leverage, the valuation effect of leverage is typically positive. However, when dividend payout is allowed to play a role, the value effect of leverage is not as straightforward. The tax rates on different income streams and their relative values determine the exact nature of the influence of dividend payout on firm value.
 Using our model, we then investigate how varying tax regimes during the period from 1979 to 2002 affected firm value and their implications for dividend policy and capital structure decisions. In particular, during the late 1980s and early 1990s, when all four tax rates were within a few percentage points of each other, the effect of payout is negligible. However, when the spread between capital gains and dividend tax rates becomes significant (such as in periods I and III in Fig. 1), the dividend payout term is influential in moderating, and at times even reversing, the positive effect of leverage.
 Top marginal tax rates over the last three decades. The figure depicts the variation in the marginal federal tax rates applicable to four types of income: personal tax rate on interest, τ

pi
; personal tax rate on dividends, τ

pd
; personal tax rate on capital gains, τ

pg
; and corporate income tax rate, τ

c
. The four tax rates varied significantly during the three decades from 1979 to 2008, both in absolute levels and in relation to each other. Period II from 1988 to 1992 stands out in terms of the narrow band within which all four rates fell. Periods I and III are characterized by rates that were spread over a wider numerical range, with all Period III rates (except τ

pg
) at significantly lower levels than those during Period I. The personal tax rate on dividend income declined the most, from 70% to 15%, erasing the tax advantage for capital gains income by 2003. The historically low tax rates during the period from 2003 to 2008 are prescribed by the Jobs and Growth Tax Relief Reconciliation Act (JGTRR). This period is characterized by only two tax rate levels, where the marginal tax rates on corporate and personal income both are 35% and the personal tax rates on dividends and capital gains are both 15% To reveal whether the intuition and predictions gleaned from the analysis of the three-term model are consistent with the evidence, we conduct an empirical analysis using companies in the S&P 1500 index. We find that the influence of the payout ratio was strongest in periods I and III when dividend and capital gains tax rates were far apart, and much weaker in period II when they were similar, confirming the predictions of the model. The main contribution of this study is the development of a concise model that reveals the conditions under which dividend policy is irrelevant (as in Miller and Modigliani 1961), or on the contrary, is an important factor to consider. Recent studies have provided strong evidence for the relevance of dividends. DeAngelo and DeAngelo (2006) reexamine the Miller and Modigliani (1961) result and conclude that, contrary to that celebrated result, dividends are not irrelevant. Similarly, Auerbach and Hassett (2006, 2007) study the wealth effects of various key events that led to the enactment of the Jobs and Growth Tax Relief Reconciliation Act of 2003 (JGTRRA), and show that the level of dividend payout is an important determinant of the change in firm value. Except the five-year period during 1988–1992, dividend and capital gains income streams were historically taxed at significantly different rates. Our model implies that during those years when there was a tax bias in favor of capital gains, dividend policy did have valuation implications. Under the provisions of JGTRRA, the rate difference between the two types of equity income streams converged in 2003 for the first time after 11 years. As a result of this convergence, our model indicates that firm values are once again payout-insensitive. As of August 2009, the U.S. Congress has not yet acted to extend the JGTRRA dividend taxation provisions. Therefore, it is probable that in 2010 the sunset provisions of JGTRRA will take effect and the tax rates will revert back to those during the pre-2003 regime. Dividend policy will once again interact with financial leverage to affect firm value. Given the wide historical variation in the tax rates on the two types of equity income streams, measuring the moderating influence of dividend payout policy on the leverage/firm value relationship is especially important.",3
36,1,Journal of Economics and Finance,27 August 2009,https://link.springer.com/article/10.1007/s12197-009-9091-5,Financing professional partnerships,January 2012,Linus Wilson,,,Male,Unknown,Unknown,Male,"This paper considers the economic rationale for professional service firms to adopt a corporate or partnership form of organization. The professional service firm can adopt a more traditional employment relationship in which control and residual claims are separated from employment contracts. It is well known,Footnote 1 that capital structure does not affect the hiring decisions of traditional firms where residual rights of cash flow and control are not exclusively bundled with employment. This standard result is upheld here. In contrast, this paper shows how financial structure and net-debt levels, in particular, affect the behavior of professional service partnerships. (Net-debt is defined as debt obligations minus cash on hand.) The professional service firm that adopts a partnership structure chooses to tie employment to equity stakes and control rights. Here we argue that partnerships can signal their hiring intentions through capital structure. Yet, the magnitude of financial frictions will play a large role in a professional service firm’s decision to adopt a partnership or corporate structure. This paper builds on an idea of Levin and Tadelis (2005). That paper argues that, when clients only detect the average ability of employees a small fraction of the time, the profit share-maximizing partnership will be more profitable and supply higher quality than the profit maximizing corporation. Yet, for high levels of market monitoring, Levin and Tadelis (2005) argue that the corporation is more profitable. This observation is used to explain why partnerships are clustered in the relatively opaque professional services where clients find it difficult to judge the quality of the firm’s employees. Unlike that study, here we argue that partnerships can always achieve the full monopoly profits when capital structure is transparent to potential clients. When all clients can observe these capital structure changes, they can infer the partnership’s hiring decision from the partnership’s choice of net-debt. Therefore, even if clients cannot observe the quality of the firm’s professionals directly, they can infer employee ability from the partnership’s fully informative capital structure signals. When some clients are uniformed about the abilities of the professionals they employ, corporations may deviate from the profit maximizing output and quality level. Further, net-debt levels, capital structure, does not affect the corporation’s hiring decision. Therefore, the corporation cannot credibly signal its hiring intentions to uninformed clients by its choice of capital structure. Ward (1958) first discussed the importance of debt in a partnership’s hiring practices. Ward (1958) found that debt obligations could affect the hiring decision in a firm which compensated workers with equal profit shares. Thus, even if the debt played no productive role in terms of paying for new investment, this obligation could affect the output of the firm by influencing the firm’s hiring decision. This is in contrast to the standard result in which a firm that pays workers the exogenously determined wage. In those firms, as we will show, capital structure has no direct effect on the firm’s hiring decision. Yet, Ward (1958) did not envision workers themselves adjusting debt levels to increase their individual and joint consumption. That paper only envisioned a social planner adjusting debt levels at a macroeconomic level. In contrast to Ward (1958), which never envisioned a need for outside equity stakes in a static setting, the present paper has a role for non-voting equity. This essay argues that, when the partnership is too large with existing levels of fixed costs, it should sell non-voting equity claims to pay down its fixed obligations. When the partnership is too small, it can sell debt to increase its fixed costs and expand its equilibrium size. The solution in Ward (1958) involved having debt obligations equal to total profits. With uninformed clients, as in Levin and Tadelis (2005), such a prescription would lead the professional partnership, which is a type of worker cooperative, to hire too many professionals. Therefore, unlike Ward (1958), the present paper has a role for both outside equity and debt obligations in a static model. This paper finds that transparent capital structure and frictionless financial markets unambiguously make the partnership form more profitable than the corporate form. If clients can readily observe capital structure changes, then they can infer the optimal hiring decisions of the partnership for the observed level of net-debt. This transparency will encourage the partnership to only take on a level of debt or cash on hand that would induce the partnership to hire at the full-information, profit maximizing level. Financial transparency allows the partnership to be more profitable and more selective than the corporation. Financial frictions would only serve to diminish the profitability of the partnership when it has transparent finances. With transparent finances and financial frictions, we are able to support the dichotomy advanced by Levin and Tadelis (2005). That is, the present paper supports Levin and Tadelis (2005)’s proposition that the partnership organizational form will be the preferred business structure when many clients are uninformed. Likewise, the corporate form will be the more preferred mode of organization when most clients directly observe the quality of the professionals that they hire. This support for Levin and Tadelis (2005, p. 142)’s “central comparative static result” only applies when finances are transparent and financial adjustments are costly. There are several recent papers that attempt to explain the weaknesses and strengths of professional partnerships. Huddart and Liang (2003) and Huddart and Liang (2005) explore how free rider problems in the monitoring of the effort of partners affect the partnership’s size. The former study discusses how exogenous variation in the partner’s ability and risk tolerance affects their willingness to monitor and stay with the firm. In contrast, Huddart and Liang (2005) explores the endogenous design of control systems when partners have identical characteristics. The latter paper argues that either a partnership is small and all partners engage in production and monitoring or the partnership is large (like a Big Four accounting firm) and some partners specialize in monitoring and some engage in client service. In Morrison and Wilhelm (2004) free rider problems in the mentoring of associates are the justification for professional partnerships. In that paper, partners mentor so that they will be able to sell their shares in the firm to the next generation of partners. Morrison and Wilhelm (2008) argues that investment banks went public as the development of human capital became less important, relative to the raising and management of financial capital. (Bar-Isaac 2007) also discusses the virtues of mentoring, but that paper says that mentoring can solve the moral hazard problems of the firm’s more senior members. That paper argues that partners with good reputations have little incentive to exert effort unless they are residual claimants on part of the future reputations of their associates. Garicano and Santos (2004) says that profit sharing in partnerships gives partners incentives to make efficient referrals to their customers. This present paper builds on the idea of Levin and Tadelis (2005) that the partnership is a quality commitment that resolves the problem of over-hiring. Unlike these recent papers, the present paper argues that net-debt levels play a prominent role in the partnership’s ability to profitably serve its clients. When net-debt levels are transparent to clients, partners can increase their individual and joint consumption by adjusting net-debt levels. This paper is also closely related to Wilson (2008). In contrast to Wilson (2008) net-debt levels in the present paper are transparent. Wilson (2008) explores the case where net-debt levels are not observed by clients. In that paper, the partnership behaves very much like the corporation because the partnership cannot credibly signal its hiring intentions because its capital structure choice is secret. The timing of the game is represented in Fig. 1. In period 0, nature selects the fraction of clients who are informed about the average quality of professionals in the firm. This choice is common knowledge. In period 1, the founders of the firm choose to organize as a corporation or a partnership. This choice is also common knowledge and is observed by all potential clients. In period 2, the partnership or corporation makes a capital structure choice that is also observed by all clients. In period 3, the professional service firm makes a secret hiring choice that is only observed by a fraction, μ, of the clients. Finally, in period 4, clients bid competitively for professional service. In this last period, services are performed and wages, debts, and profits are paid.
 The sequence of events We will introduce the model in Section 2. Then, we will begin solving the model by moving backwards through the game. We discuss the bidding strategies of the potential clients in period 4 at the end of Section 2. In Section 3, we discuss the hiring strategies of the professional service firm. In Section 4, we discuss the period 2 capital structure decision and finally the firm’s choice of organizational form. In Section 5, we conclude.",1
36,1,Journal of Economics and Finance,26 November 2009,https://link.springer.com/article/10.1007/s12197-009-9114-2,Which implied volatility provides the best measure of future volatility?,January 2012,Guan Jun Wang,Pierre Yourougou,Yue Dong Wang,,Male,,Mix,,
36,1,Journal of Economics and Finance,27 November 2009,https://link.springer.com/article/10.1007/s12197-009-9116-0,Speculating on presidential success: exploring the link between the price–earnings ratio and approval ratings,January 2012,Tomasz Piotr Wisniewski,Geoffrey Lightfoot,Simon Lilley,Male,Male,Male,Male,"Stock markets and politics are enduring staples of dinner-party conversations, but surprisingly little is known about the interaction between the two. The studies that have been written so far have tended to be from the perspective of a trader and within these two distinct streams of thought can be identified. The first concentrates on the political business cycle, which was introduced by Nordhaus (1975: 187), who suggested that ‘within an incumbent’s term in office there is a predictable pattern of policy, starting with relative austerity in early years and ending with the potlatch right before elections.’ Herbst and Slinkman (1984) and Booth and Booth (2003) developed the market ramifications of this insight to demonstrate the existence of a 48 month cycle within stock movements. The second theme examines the profitability of different investment strategies under Democratic and Republican administrations. Notable here are Hensel and Ziemba’s (1995) intriguing conclusion that a ‘politically correct’ strategyFootnote 1 would outperform a range of other investment approaches and Santa-Clara and Valkanov’s (2003) finding that returns are robustly higher under the Democrats.Footnote 2 These studies, then, are primarily oriented to informing trading practice. In this paper, we change perspective to examine how policy makers might derive information from financial market performance—or more precisely, expectations of such—particularly with regard to the ways in which such expectations relate to approval of an incumbent president. Exploration of the relationship between electoral preferences and a variety of non-financial economic indicators has considerable history (Lewis-Beck and Stegmaier 2000). Studies have integrated a range of different measures of economic well-being, some utilizing directly observable macroeconomic variables such as the unemployment rate, the rate of inflation or both (e.g. Kenski 1977; Kernell 1978; Golden and Poterba 1980; Ostrom and Simon 1985), whilst others draw on opinion surveys, such as the Michigan Survey of Consumer Attitudes and Behavior (e.g. MacKuen et al. 1992; Erikson et al. 2000). The latter two studies cited are perhaps the most controversial within this tradition. In attempting to establish whether the electorate tends to look back at past or forward to future predicted economic performance the authors mobilize a striking metaphorical illumination, the contrast between ‘Peasants’ and ‘Bankers’. Peasants are apparently those who are retrospective in their judgmental orientation and focused parochially upon the reflection of change in their own ‘pocketbooks’, whilst Bankers are seen to be prospective and broader in their orientation, looking ‘sociotropically’ at the economy as a whole. The controversy turns on a number of points. Most immediately there is the question of whether looking forward or looking back predominates in judging of the president. The latter position is most vividly captured by Key’s (1964: 568) conception of the electorate as a ‘rational god of vengeance and reward’, whilst the former is most clearly captured in MacKuen et al.’s (1992) original article on the subject. The controversy seems far from settled in any of its dimensions. As Lewis-Beck and Stegmaier (2000: 188) pithily note: ‘leading scholars, looking at essentially the same data, and following model specifications that are conceptually similar, arrive at estimates that yield opposite conclusions.’ Attention has since focused on both more nuanced attempts to grasp the extent of the two possible orientations and, more intriguingly, on the methodological difficulties of successfully separating them, given the data available (see, for example, Clarke and Stewart 1994; Norpoth 1996). Vital here has been the issue of the possible multicollinearity of the prospective and retrospective perceptions of economic performance that constitute a central part of the data drawn upon from the Michigan survey. Measures of economic prospects and retrospections are garnered from the Michigan survey by a set of questions that ask how the respondent perceives both her own situation or that of business conditions, both via categorical judgment (good vs. bad) and in a more relative sense (better vs. worse) over a number of time periods. The long-term categorical judgment of the economy as a whole is seen by Erikson et al. (2000: 301) to be the most effective correlate of approval. However, Lewis-Beck and Stegmaier (2000: 188), following Norpoth (1996), suggest that in time series derived from the data drawn from the Michigan survey ‘multicollinearity may be rendering coefficients unstable, subject to serious change from one specification to the next’ (see also, Greene 2000: 257). In this paper, we are uniquely able to shed more light on and develop fruitfully the ‘Bankers/Peasants’ distinction for two key reasons. Firstly, only one of our variables—the Price–Earnings ratio—is (in part at least) prospective in orientation and it is not highly inter-correlated with our measures of retrospection. And secondly, this measure of economic expectation is in large part moved by those who we can assume to be informed and who have financial interest in the accuracy of their predictions. Specifically, we demonstrate a robust correlative relationship between the aggregate P/E ratio and Gallup polling presidential approval ratings. Doing so enables us to draw upon the enhanced informational content seen to derive from knowledgeable, financially interested participation in a market, as opposed to mere expression of opinion (see, for example, Arrow et al. 2008). The extent of this financial interest in the stock market is indexed both by the approximately 18% of US families who directly hold shares and the more than half of them who are either direct or indirect shareholders, the latter primarily via their ownership of mutual funds and/or pension plans (Federal Reserve Board, Survey of Consumer Finances 2007, see particularly Tables 5 and 6). One could also point to the 850,000 plus individuals employed in the US securities industry in 2007 (SIFMA 2008:16) and the huge extent of activity on the market. With several million trades, representing 3.4 billion shares, executed on a typical single day on the New York Stock ExchangeFootnote 3 alone, an ongoing incorporation of an expansive range of shifting informed opinion can rapidly take place. As MacKuen et al. (1996: 795, see also Erikson et al. 2000: 299) point out: ‘people will process information more slowly than financial markets’. Erikson et al. (2000: 298), in their re-visitation of the Bankers and Peasants distinction, justify their hypothesis that the electorate can be, and is, informed by expectations of the future performance of the economy by noting that individual, economically active, citizens will find it ‘worthwhile to anticipate inflation, interest rates, and their own income stream.’ They contend that everyday interaction facilitates the transfer of information ‘from sophisticated analysts, through the mass media, and down to the street level’ and that the consensus street forecast is roughly the same as the opinion of an average expert (Erikson et al. 2000: 299). Our approach is in sympathy with this line of reasoning, but rather than trying to discern the impact of the expert ‘signal’ on approval via attending to its reflection in aggregate perceptions on the economy, we prefer to hear it from the horse’s mouth. The relationship between approval and one of the likely key objective underpinnings of that shifting expert signal produced by ‘sophisticated analysts’—the Price–Earnings ratio—is examined directly. And in doing so, we discover a relationship that appears to be substantive and robust. Although clearly there is no complete identity between the two populations of investors and electors it is reasonable to assume that in coming to their respective opinions on the state of and prospects for the economy they are mutually influential. Not only are electors likely to be in part informed about ‘the economy’ by the ways in which discussion by and for investors on such takes place. Investors themselves are likely also to be endlessly trying to ascertain how the wider population will respond to perceived economic circumstances and prospects, given the extent of influence of consumer sentiment upon the performance of ‘the economy’. The remainder of the paper is organized as follows. In the next section we elaborate on our choice of prospective explanatory variable, the P/E ratio. In the third section we go on to discuss the control variables incorporated into our model, exploring both their construction and the sources of data from which they derive. The fourth section presents and discusses our empirical results before we conclude with an assessment of their possible implications.",9
36,1,Journal of Economics and Finance,11 December 2009,https://link.springer.com/article/10.1007/s12197-009-9112-4,Sportsbook pricing and the behavioral biases of bettors in the NHL,January 2012,Rodney J. Paul,Andrew P. Weinbach,,Male,Male,Unknown,Male,"The NHL betting data contains information from three full seasons of the hockey betting market, following the lost lockout season of 2004–05. In these years, 2005–06 to 2007–08, the dominant form of hockey betting changed to strictly odds (like baseball), as opposed to the previously popular “Canadian Line” where there existed a pointspread (typically a half-goal) and odds. Due to the addition of a shootout and the elimination of ties in hockey, the odds wager became the most popular and common form of hockey betting, as each game now concludes with a winning and losing team. Sports Insights presents combined data from four sportsbooks to show the percentage of bets on the favorite and underdog for its subscribers. The four on-line sportsbooks are BetUS.com, CaribSports.com, SportBet.com, and Sportsbook.com. Data were available for each game played in the three seasons, where odds were posted. The raw data set includes information on playoff games, all-star games, preseason games, etc. For the purposes of this study, however, only regular season games were included. Given the importance of the home/road distinction found in studies of sports which use odds betting (specifically baseball and hockey) in Gandar et al. (2002) and Gandar et al. (2004), we decided to initially split the data into home favorites (road underdogs) and road favorites (home underdogs) to examine the actions of bettors and observe results of various betting strategies. If sportsbooks were basing odds off of the flow of dollars bet and setting prices to balance the betting dollars, under the assumptions of the traditional models of sportsbook behavior, the odds should reflect the percentage of bets received on the favorite and the underdog. The easiest place to begin with these detailed data is to plot the results. To illustrate the possible preferences of bettors and betting strategy results, we have arranged the data from the biggest favorites to the smallest favorites for the groupings of home favorites and road favorites. Given the availability of the game odds set by the sportsbook, the percentages bet on favorites and underdogs, and actual game results, we decided to plot all data side-by-side in terms of betting percentages. In terms of the favorite, the percentage of bets which the favorite should have attracted to balance the book (based on odds), the actual percentage bet on the favorite, and the actual win percentage of a strategy of betting the favorite are presented in Table 1. The data are arranged in terms of the percentage of unit bets which would constitute a proportionally balanced book, based on actual sportsbook odds, organized from highest percentage (biggest favorites) to lowest percentage (smallest favorites) from left-to-right. To clearly illustrate the tendencies and results, the actual betting percentages on favorites received by the sportsbook and the winning percentage of the favorite are shown as 25-game and 100-game moving averages, respectively. This plot allows an easy visual of this market.
 For home favorites, the data appear somewhat closely grouped, although the expected percentages bet on the home favorite are generally slightly higher than the actual percentages bet on the home favorite. In addition, the moving average of the actual win percentage of the favorite appears to at least be within range of both other values. The road favorites (home underdogs) tell a different story. As in Table 1 above, the expected percentage bet on the favorite based on the odds (to balance the book), the actual percentage bet on the road favorite, and the favorite win percentages are plotted together in Table 2.
 In Table 2, it appears the expected percentage of bets (based on posted odds) on the favorite and the actual favorite win percentage appears to map rather closely together. On the other hand, the actual percentage bet on the road favorite exceeds the expected percentage bet (based on sportsbook odds) across the sample of road favorites. Visual evidence suggests odds are not set by the sportsbook based on the percentage wagered by bettors, as bettors seem to overestimate the odds of road favorites winning. It appears bettors prefer road favorites by a large margin, but this is not captured by the sportsbook odds, which, likely not coincidently, tend to map closer to actual favorite win percentages. The following sections present formal tests to explore the notion of bettor preferences, sportsbook pricing, and returns to various betting strategies. The premise of the traditional models of sportsbook behavior, in terms of pointspreads, was sportsbooks were attempting to attract even betting dollars on both sides of the proposition. If achieved, this position would clear the market and would allow the sportsbook to capture its commission on losing bets without risk. Extending this idea into odds wagering, such as hockey and baseball, sportsbooks were assumed to be attracting betting dollars proportionally with respect to odds. If sportsbooks are not setting odds based on the betting percentages received on favorites and underdogs, they are not pricing to balance the book. If not pricing to balance the book, findings of market efficiency based on the actions of bettors in gambling markets are quite suspect. Bettors in the aggregate may not be revealing “The Wisdom of Crowds”, but may actually be quite biased, while the sportsbook may be setting the price (odds) for other purposes. To test the null hypothesis that sportsbooks set odds to balance the book, we test the following simple regression model:
 The dependent variable is the actual percentage bet on the favorite by bettors from the Sports Insights data. The independent variable is the percentage of unit bets on the favorite required to produce a proportionally balanced book, based on the posted sportsbook odds. If sportsbooks are setting prices to balance the book, β0 = 0 and β1 = 1. Therefore, a simple F-test for this null hypothesis is tested for the groups of home favorites and road favorites. Coefficients and t-statistics for the intercept and independent variable are presented along with the F-statistic for the null hypothesis of balanced book behavior by sportsbook in Tables 3 (home favorites) and 4 (road favorites).
 The null hypothesis that sportsbooks set prices to balance the book is rejected for both samples. The actual percentage of bets on the favorite is not one-to-one with the odds set by the sportsbook. Within the relevant range within the sample, home favorites receive a slightly lower betting percentage than expected under the posted odds. For the sample of road favorites, as clearly illustrated in the graph in the previous section, road favorites attract higher betting percentages than expected under the set odds. Overall, the null hypothesis that sportsbooks set odds (prices) to balance the book can be rejected. Sportsbooks do not set odds based solely on the actions of market participants. The alternative hypothesis here is not that sportsbooks set prices to maximize profits based on bettor misperceptions (as seen in Levitt (2004)), but simply that prices are not being set to balance the book. How and why sportsbooks are pricing the NHL market requires additional tests to determine if they are exploiting known bettor biases or pricing as a forecast to capture commissions in a long-run repeated game and discourage entry by informed bettors. To determine if sportsbooks are truly pricing to maximize profits, as suggested by Levitt (2004), we examine returns to simple betting strategies. Examination of returns to simple betting strategies allows us to determine if the pricing by sportsbooks is efficient and if the sportsbook earns higher returns by pricing through some mechanism other than balancing the book. To begin, we simply calculate returns to betting strategies based on simple rules of wagering on all favorites or all underdogs for the entire sample and at various thresholds. As in the previous section, the sample is split into home favorites and road favorites. Table 5 presents the results for home favorites (road underdogs) and Table 6 presents the results for road favorites (home underdogs).
 Table 5 shows a simple strategy of wagering on the underdog earns positive but not significant returns. Betting all road underdogs earns nearly two cents (0.0198) per dollar wagered and betting on all road underdogs where favorite odds are −200 or greater earns nearly six cents (0.0591) per dollar wagered. As in many other sports, a strategy of wagering on home favorites earns lower returns than wagering on road underdogs. It appears that sportsbooks in our sample shade the odds higher than the market clearing price would imply, perhaps guarding against being overly lopsided on certain favorites. This slightly higher price set by the sportsbooks allows for contrarian bettors, those that prefer underdogs, to earn slightly positive returns, but these returns are not found to be statistically significant. In the previous section it was shown bettors clearly prefer road favorites, placing an extremely high percentage of wagers on these teams. Despite this clear behavioral bias, the sportsbook does not appear to set prices to exploit these biases (or price to maximize profits in terms of Levitt (2004)). Prices appear to be set much closer to true probabilities on outcomes of games. A simple strategy of betting against popular public sentiment, wagering on the home underdog, does not earn positive profits. This strategy loses more than three cents (−0.0332) per dollar wagered, posting higher losses than a simple strategy of betting on the road favorite (−0.0137). Bigger road favorites, for instance those with favorite odds of −150 or greater, actually earn positive (but insignificant) returns (0.0311) in this small sample. Although bettors have a clear behavioral bias for road favorites, sportsbooks appear to price more as a forecast of game outcomes rather than post higher prices on road favorites. This result is similar to the sides and totals markets for the NBA (Paul and Weinbach 2008). Given the NHL market is even smaller than the daily market for the NBA, sportsbooks could be pricing as a forecast to discourage entry into this market by informed bettors, who may easily recognize the behavioral biases of the hockey betting public. This action prevents losses to “wiseguys” and still earns the sportsbook its commission on losing bets in the long-run. Given that betting behavior is likely to be a repeated game, as most bettors enjoy the act of betting on sporting events (the “consumption” element of betting), the sportsbook may actually earn higher profits by earning their commission on losing bets over a season (or many seasons) as opposed to pricing to exploit known biases, where recreational bettors may lose their bankroll for gambling quickly, and may not re-enter the market. Given the availability of the betting percentages, determination of returns from other simple betting strategies are possible. One angle we examined were cases where the actual percentage bet was significantly higher or lower than the percentage bet suggested by the actual betting market odds. If odds were truly set to balance the book, with the assumption of equal expected returns to bets on the favorite and bets on the underdog, the betting odds can be used to calculate the percentage of unit bets on the favorite and underdog that would offer proportional balance to the sportsbook. Given this information and the actual betting percentage, it is possible to isolate games where the public has a higher percentage bet on one side than would be implied by the odds (or a lower percentage bet than implied by the odds). This allows for an investigation of games involving informed betting, where the public is exploiting a weak line posted by the sportsbook or games involving uninformed betting, where the public overestimates the probability of a given team winning. If informed bettors truly exist and are betting to exploit incorrect betting lines, a betting strategy of wagering with the money should lead to profits. When the betting is uninformed, however, a betting strategy of wagering against the public money should lead to profits. Given public preferences for favorites, our initial notion (based on previous studies) was that it was likely that higher percentages of betting dollars on favorites would likely constitute uninformed betting action, while higher percentages of betting dollars on underdogs could indicate informed “smart money” plays. Table 7 presents the results for home favorites and Table 8 presents the results for road favorites.
 Not surprisingly, given the chart in the previous section, betting against the public when they overbet (compared to projected percentages based on posted odds) home favorites is found to be profitable. From Table 7, in all games where the percentage bet on the favorite exceeds the expected percentage bet on the favorite as implied by the odds, a strategy of wagering on underdogs earns more than seven cents (0.0711) per dollar bet, while wagering on these popular favorites loses nearly eight cents (−0.0795) per dollar bet. These returns reject the null of a fair bet (returns are equal to expected returns—given the commission charged on bets) and also rejects the null of no profitability (returns are greater than zero) at the ten percent level. In games where the public clearly favors the home favorite, yet the actual odds on the favorite are lower than the true price which would balance the book, wagering against public sentiment is profitable. In relation to games where road underdogs receive a higher percentage of bets than implied by the odds, little in the way of profitability is found. For all games which meet this criteria, wagering with the public (betting on the underdog) loses slightly less than one cent (−0.0078) per dollar bet, while wagering against the public (betting on the favorite) loses more than two cents (−0.0244) per dollar bet. With road underdogs, finding a higher percentage of betting dollars on this side of the proposition does not imply “smart money” as it does not earn positive returns. For the sample of road favorites (home underdogs), as presented in Table 8, statistically significant returns are not found in any of the wagering strategies. Positive returns are found for contrarian bettors in both cases (situations where more money is wagered on the road favorite than implied by the sportsbook odds and the case where more money is wagered on the home underdog than implied by the sportsbook odds) presented in Table 8. Given the sample size available to test these strategies, however, significance is not found. These results imply that the sportsbook does earn some additional profits when the public bets games in a different proportion than those implied by the odds. This is not necessarily evidence, however, of pricing to exploit known bettor biases in all cases. In general, however, It does appear that bettors who follow a contrarian strategy, essentially wagering on the same side as the sportsbook (due to their non-proportional betting action compared to the odds), outperform those who wager with public sentiment.",10
36,1,Journal of Economics and Finance,13 April 2010,https://link.springer.com/article/10.1007/s12197-010-9129-8,The nexus between exchange rates and stock markets: evidence from the euro-dollar rate and composite European stock indices using rolling analysis,January 2012,Christos Kollias,Nikolaos Mylonidis,Suzanna-Maria Paleologou,Male,Male,Unknown,Male,"Given that both foreign exchange rates and stock prices are of vital importance to the development of a country’s economy, they have attracted considerable attention in the relevant literature, both empirical and theoretical. Theoretical links between stock prices and exchange rates have taken three forms. First, unidirectional causality from exchange rates to stock prices is posited by the goods market hypothesis (Dornbusch and Fischer 1980) which suggests that changes in exchange rates affect the international competitiveness of local firms and hence their earnings and share prices. However, the sign of the impact of exchange rate fluctuations on stock markets cannot be easily predetermined. For example, in an export-oriented economy, domestic currency depreciation makes local firms more competitive, leading to an increase in their exports and earnings. Since stock prices are determined by the present values of future cash flows of firms, currency depreciation is expected to raise domestic share prices. However, the reverse relationship may also hold in the case of local firms utilising imported inputs in their production. In this case, domestic currency depreciation is expected to raise the cost of production, thereby reducing firms’ sales and profits. This in turn might lead to a fall in their stock prices. Second, the reverse causality (from share prices to exchange rates) is postulated by the portfolio balance models of exchange rate determination (Frankel 1983). According to these models, exchange rates play the role of balancing the demand for and supply of assets. For example, a blooming stock market leads investors (local and foreign) to raise demand for domestic assets and currency. These shifts in demand and supply of currencies cause domestic currency appreciation. A similar unidirectional causality is induced by changes in domestic wealth (Gavin 1989). Consider the previous example of a blooming stock market. An increase in domestic wealth, due to a rise in domestic asset prices, leads to higher domestic money demand and interest rates. This again leads to appreciation of domestic currency by attracting foreign capital. Finally, a weak or no association between stock prices and exchange rates is suggested by the asset market models (Frenkel 1976). These models treat exchange rate to be the price of an asset the fundamental characteristic of which is that its present value is largely influenced by its expected rate of return. Since developments of stock prices and exchange rates may be driven by different factors, the asset market approach advocates the absence of any linkage between stock prices and exchange rates.Footnote 1
 The existing empirical evidence also remains ambiguous and has yielded conflicting results regarding the interdependence between exchange rates and stock markets. Cointegration analysis and Granger causality tests have been extensively employed to explore this relationship. Cointegration analysis generally indicates the absence of any significant long-run relationship between the two markets (see inter alia Bhandari and Genberg 1989; Bahmani-Oskooee and Sohrabian 1992; Granger et al. 2000; Nieh and Lee 2001; Smyth and Nandha 2003; Morales 2009).Footnote 2 Granger causality tests provide mixed evidence on the short-run dynamics of the exchange rate—stock market relation. Hatemi-J and Irandoust (2002) report unidirectional causality running from stock prices to exchange rates in Sweden. Smyth and Nandha (2003) find that exchange rates drive stock prices in India and Sri Lanka. Wu (2000) and Morales (2009) find similar unidirectional causality for Singapore and eastern European countries, respectively. Finally, Ajayi and Mougoué (1996) report significant feedback relations for the big eight industrial economies: the USA, Canada, France, Germany, Italy, Japan, the Netherlands and the UK. On the whole, the direction of causality seems to be controversial and the observed differences may be attributed to country-specific factors (such as the trade size, degree of internationalisation, exchange rate regimes, degree of liberalisation, capital controls, etc.). In order to account for these factors, some researchers investigate the impact of capital market liberalisation, exchange rate regimes and financial crisis in emerging markets as potential sources of instability in the exchange rate—stock market linkages (Granger et al. 2000; Pan et al. 2007). Their findings indicate that the relationship and causality between the two markets varies over time and that it crucially depends on whether the stock markets are operating under normal of stressful conditions. Some analysts further explore the time-varying nature of this relationship using more advanced econometric techniques. Tastan (2006) employs a bivariate GARCH to examine the relation between the euro–dollar exchange rate and two U.S. equity indices and infers that the conditional correlation varies significantly over time. Harmantzis and Miao (2009) reach similar conclusions for a sample of ten industrialised countries. Using the copula approach, they find that the pattern between exchange and equity markets changes with the passage of time and, especially, around periods of global financial turmoil. In this paper, we contend that explicitly accounting for time variation in the exchange rate—stock market relation is important. The purpose of the paper is to investigate the nexus between the euro-dollar exchange rate and two composite European stock indices (FTSE Eurotop 300 and FTSE eTX All-Share Index) for the period 02/01/2002 to 31/12/2008 (daily observations). It departs from the relevant empirical literature thus far in that it employs a new data set and a battery of rolling tests (unit root, cointegration and Granger causality tests); a methodology that, to the best of our knowledge, has not been used before to tackle the issue at hand.Footnote 3 This approach allows for the emergence of a clearer picture of the possible dynamic linkages between exchange rates and stock prices since, although the sample size remains unchanged, the sample period moves ahead by one observation at a time. Therefore, the observed test statistics at every stage reflect the variation in the interrelationship between the two markets due to new information. Consequently, by rolling analysis, one may detect changes in causality, that is structural breaks in the linkages. This is particularly important for the purposes of the present study. Should we use the full sample period, parameter estimates after the structural break (or breaks) would stem from data mixed with two (or even more) regime shifts and thus may be imprecise or even invalid. Rolling causality analysis, however, ensures that the effects of such regime shifts are restricted to specific event periods and do not contaminate the overall picture.Footnote 4
",23
36,1,Journal of Economics and Finance,29 May 2010,https://link.springer.com/article/10.1007/s12197-010-9135-x,Revisiting the forward—spot relation: an application of the nonparametric long-run correlation coefficient,January 2012,Angelos Kanas,Christos Ioannidis,,Male,Male,Unknown,Male,"One of the most important empirical issues in international finance is testing whether the forward exchange rate contains useful information about the future spot exchange rate. Under the assumption of risk neutral efficient market hypothesis, the forward rate should be an unbiased (and optimum) predictor of the future spot exchange rates. Early empirical studies, using regression analysis, brought out a ‘puzzle’ regarding the ability of the forward rate to predict the future spot. Such studies include Bilson (1981), Cumby and Obstfeld (1984), Fama (1984), and Froot and Frankel (1989).Footnote 1 More recently, several studies, using cointegration methods, led to inconclusive results which depend on the parametric modelling of cointegration. Such studies include: Hakkio and Rush (1989), Barnhart and Szakmary (1991), Naka and Whitney (1995), Hai et al. (1997), Norrbin and Reffett (1996), Newbold et al. (1998), Clarida and Taylor (1997), Barnhart et al. (1998), and Luintel and Paydal (1998). This study illustrates that the correlation between the forward rate and the future spot, and thus the ability of the forward to predict the spot, is indicative of the confidence the market places on the current value of the spot rate. In periods of ‘major events’ (such as crises or the Introduction of Euro), this relationship becomes very pronounced compared to periods of relative stability. In this setting, the study aims at testing the ability of the forward rate to predict future spot rate using a nonparametric measure of the linear association between spot and forward, namely the nonparametric long-run correlation coefficient. This approach has several advantages over the approaches used in previous studies. Firstly, it is nonparametric in the sense that it is not limited by model specification including the correct choice of the number of lags. Secondly, the long-run correlation coefficient is not different from an infinite return horizon correlation coefficient. It is supposed to capture the full effect of permanent innovations. Moreover, it would represent a relevant correlation concept for traders or investors that do not have finite horizons in mind when evaluating return correlations. Finally, we employ the long-run correlation estimator developed by Albuquerque (2001), which makes optimal use of information that results from the employment of lead-lag (alignment) criteria between the spot and the forward. The long-run correlation coefficient is applied to test the linear association of the spot and forward £/DM rate over 1992 when two major events happened, namely the May 1992 British General Elections and the September 1992 ERM devaluation of the Pound. In addition, it is applied to the spot and forward rates between the French Franc (FF) and DM (FF/DM), the Belgian Franc (BEF) and DM (BEF/DM), the Austrian Schilling (AT) and the DM (AT/DM), and between the Netherlands Guilder (NLG) and DM (NLG/DM) up to the Introduction of Euro in 1999. We find evidence of statistically significant long-run correlation between the forward and the spot rates in anticipation of these major events. The results suggest that when currency markets anticipate a major event, the forward rate does reflect important information for the future spot exchange rate. In the absence, however, of such events, the long-run correlation between the spot and the forward rate is rather weak, and in line with the results of previous studies suggesting inability of the forward to predict the future spot. The paper is organized as follows; Section 2 provides a discussion of the statistical problems that are inherent in previous methodologies. Section 3 develops the theoretical background regarding the relationship of the spot and forward rates. The choice of the estimator, its properties and data are reviewed in Section 4. Our results are presented in Section 5. Finally our conclusions are presented in Section 6.",
36,1,Journal of Economics and Finance,18 June 2010,https://link.springer.com/article/10.1007/s12197-010-9138-7,The value of the option to preserve farm real estate,January 2012,Jeffrey R. Stokes,,,Male,Unknown,Unknown,Male,"As of April 2007, 19 states had authorized state-level Purchase of Agricultural Conservation Easement (PACE) programs whereby easements or restrictions have been acquired on 1.6 million acres of farm real estate at a cost of over $2 billion (American Farmland Trust 2007).Footnote 1 Given the population concentration in the Mid-Atlantic region, it is not surprising that States such as Pennsylvania and Maryland are at the forefront of PACE programs. Through the Pennsylvania Agricultural Conservation Easement Purchase Program over 3,000 easements/restrictions have been acquired on over 344,000 acres of farmland at a cost of about $536 million since 1988 (American Farmland Trust 2007). Similarly, through the Maryland Agricultural Land Preservation Foundation and the Rural Legacy programs, over 2,100 easements/restrictions have been acquired on nearly 300,000 acres of farmland at a cost of about $387 million (American Farmland Trust 2007). With over 2,000 outstanding applications in Pennsylvania alone (and over 5,000 in the U.S.), it is clear that the demand for PACE programs to preserve farm real estate and control development is not likely to abate any time soon. Conservation easements consists of deed restrictions that landowners voluntarily place on their land for any number of reasons including the protection of resources such as farm real estate. Landowners placing a conservation easement on their farm real estate retain all their private property rights including the right to use their land for farming or ranching. However, the easement is designed to keep farm real estate available for production and therefore permanently precludes development. Obviously, the sale of an easement by a landowner necessitates a determination of its value. While research such as Munneke and Trefzger (1998) offers a viable approach to determining the value of an easement, an alternative is a real option or contingent claims framework. This is because, similar to a financial option, the landowner possesses the right but not the obligation to develop farmland and this real (perpetual) option has value. Williams (1991) was probably first to formally value the right to develop real estate as a real option. Subsequent work by Quigg (1993), Capozza and Li (1994, 2001, 2002), Holland et al. (2000), Schatzki (2003) and others have sought to empirically validate the real option model for land development. More recently, Towe et al. (2008) suggest a simple real option model may be appropriate for examining the timing of land conversions in the presence of PACE programs, but the authors do not take up the issue of valuing the landowner’s option to preserve. Yet the timing of land conversion is inextricably linked to valuation since value is what drives the timing decision suggesting that valuing the preservation option is a necessary first step. Like the landowner’s development option, in areas where PACE programs are available, an agricultural conservation easement is essentially a derivative security that can be valued using contemporary option theory. At least two features of conservation easements make their valuation potentially different than the valuation of development options. First, because the real estate remains in its agricultural use after preservation, the landowner retains the right to the income stream that the real estate produces after exercising the option. This income stream is forgone (or exchanged) upon exercising the development option. Second, the landowner receives the difference between the market value of farm real estate and its agricultural use value upon exercising a preservation option. When exercising a development option the landowner receives the difference between the value of developed real estate and the cost to development it. The primary contributions of this paper are twofold. First, a contingent claims model is developed for the valuation of a preservation option for farm real estate. The model is based on the research cited above for development options, but heretofore represents the first attempt to analytically value preservation options in a similar manner. The second contribution of the paper is the reporting of the analytic and numerical results of the model solution where many interesting policy results emerge. Key among these results is the fact that the landowner’s option to delay preservation can have significant value. So much so that it is likely that systematic undervaluation of easements occurs regularly in practice. In addition, technological progress that reduces the volatility in the agricultural use value of farm real estate and farm policy designed to support and stabilize farm incomes may both make preservation less likely to occur. The paper is organized as follows: in Section 2 we discuss and specify the underlying sources of uncertainty affecting the valuation of the landowner’s preservation option, specify the option valuation model, and present an analytic solution for the preservation option valuation model. In Section 3, we present a detailed analytic and numerical analysis of the solution from Section 2 while in Section 4, we suggest a simple extension of the model for capturing the landowner’s option to develop farm real estate. Section 5 concludes.",1
36,1,Journal of Economics and Finance,10 July 2010,https://link.springer.com/article/10.1007/s12197-010-9140-0,Debt dependence and corporate performance in a financial crisis: evidence from the sub-prime mortgage crisis,January 2012,Frederick Adjei,,,Male,Unknown,Unknown,Male,"Financial markets and intermediaries are generally thought to mitigate adverse selection and moral hazard problems that can make raising external funds expensive for firms. Debt dependent firms should benefit disproportionately more from well-developed financial markets and intermediaries owing to the ease of obtaining external funds in such markets (see Braun and Larrain 2005). Conversely, a credit crunch, such as the sub-prime mortgage crisis, should have a disproportionately negative impact on debt dependent firms due to the reduced credit availability and increased difficulty of obtaining external funds. There exists an extensive literature on the impact of the frictions of debt dependence on corporate performance during a financial crisis. However, these studies are either at the country level (Rajan and Zingales 1998) or the industry level (Braun and Larrain 2005; Kroszner et al. 2007; Dell’Ariccia et al. 2004) but not at the firm level. This is a non-trivial oversight. Given that differences in firm financial characteristics partly explain differences corporate performance among firms, it is imperative that we examine the relationship at the firm level. Knowledge of the performance of debt dependent firms in a financial crisis could help corporate executives prepare better for such crises and help investors assess debt dependent firms more accurately during a financial crisis. A case of a financial crisis is the subprime mortgage crisis. The severity of this crisis presents a unique opportunity to better comprehend financial crises and their differential impact on corporate performance. In this study, we investigate the effect of debt dependence on the differential impact of the sub-prime mortgage crisis on corporate performance of non-financial firms. We identify the beginning of the sub-prime mortgage crisis and investigate the effect of debt dependence on changes, from pre-crisis to crisis levels, in corporate performance. Our findings are the following. We find that the higher the debt dependence, the greater the impact of the sub-prime mortgage crisis in decreasing corporate performance. Additionally, for high-debt firms, we find that the higher the new debt borrowed during the crisis period, the greater the decline in corporate performance. However, for low debt firms in the crisis period, we find no significant relationship between new debt borrowed and corporate performance.",3
36,1,Journal of Economics and Finance,09 November 2010,https://link.springer.com/article/10.1007/s12197-010-9161-8,Social identity and schooling inequality,January 2012,Edward Nissan,George Carter,,Male,Male,Unknown,Male,"Chen and Li (2009) contend that social identity defines an individual’s sense of membership in a social group. Group identity is considered to be central in understanding social psychology, sociology, anthropology, and political science which can help explain ethnic and racial conflicts and the formation of human capital. An important tenant of social identity theory is the association of a person with certain groups. According to Holzer and Neumark (2000), affirmative action measures were taken to promote education and employment opportunities for citizens (women, minorities) subject to discrimination. In essence, the measures of affirmative actions were designed to erase differences in social identity especially due to financial affluence. Bell and Wray (2004) consider the war on poverty begun in 1964 an attempt to improve the quality of workers. Educational programs were devised to make the jobless more skillful; yet, according to Bell and Wray, long-term unemployment among disadvantaged workers has been on the increase, and poverty rates are high. Seguino (2005), recognizing that economic inequality between and within countries widened in the last decades, says the effect of inequality on growth is inconclusive. The effects of inequality can enhance or reduce growth. Economic structure and institutional environment of an economy determines whether growth is enhanced or reduced. For instance, gender and ethnic inequalities could induce growth in the short to medium terms. This is so because inequality can produce positive demand-side effects. Goldin et al. (2006) tackle the issue of gender gap in college attendance. In 1947 the ratio of men to women was 2.3 to 1.00. In 1960 the ratio was 1.55 to 1.00. By 1980, the gender gap in enrollment was gone, and in 2003 the ratio reversed in favor of women 1.35 to 1.00. The sources of this reversal are numerous, and include improvement in achievement test scores in high school and continuing into college. One reason for this phenomenon is that behavioral problems are more pronounced in boys than in girls. Added to this, barriers for girls to higher education were eliminated. The result is that the wage premium for college is higher for women than men. The factors that lend to income inequality are numerous, according to Kassa (2006), who explains that there are direct and indirect factors included in a typical study. Attention was given to economic and technological development. Included also are population, educational level, education inequality, urbanization, structure of the population, and composition of households. At times, factors included are the share of the government, the share of the private sector, and democratization. Broadly, the factors can be aggregated into groups that include economic and technological development, resources and their distribution, demographic factors, political factors, cultural factors, and macroeconomic factors. Goldsmith et al. (2006) set out to investigate the workplace experience for explanations of differences in wages. Samples were chosen to determine characteristics of workplace experience. The full samples by ethnicity (blacks, whites, Latin) and by gender (male, female) were defined by 38 indicator variables. The purpose of their analysis is to determine some socio-economic explanations for the wage gaps. Their results support the propositions that employers offer greater rewards to whites for similar levels of experience, and yet, racial/ethnic returns differences for tenure and seniority tend to be smaller with the current employer. In a similar manner, this paper intends to probe the question whether socio-economic identity explains the educational gaps. Urquiola and Verhoogen (2009) henceforth (UV) investigate schools’ choices of class size and households’ choices of student outcomes. They recognize social identity, perhaps as consequences of income and extreme wealth inequality as pointed out earlier, as important factors determining students’ accomplishments. They chose for their analysis Chile’s liberalized education market to find out whether schools adjust prices or classroom enrollments in order to handle rising numbers of students. The UV finding was a sorting of households into schools of different qualities, thus generating discontinuities among household characteristics as an example of social identity. The focus of UV was to examine the choices of class size to estimate their effects on student outcomes. The model is built subject to class-size cap and integer constraint on the number of classrooms. UV explains that the model draws on two sources of information. The first is from the Chilean Ministry of Education which calculates average class sizes in each grade. The second draws on Chile’s standardized testing program which tracks the mathematics and language test scores in the schools as well as monthly income and parental schooling. In the process, UV provided descriptive statistics on which their economic models are based. The descriptive statistics are a rich and valuable source for conducting further research, which is the intention of this paper. The remainder of the paper is organized as follows. Section 2 provides literature review and Section 3 provides the description of the data and methodology. Section 4 presents results. Section 5 concludes.",
36,1,Journal of Economics and Finance,18 November 2010,https://link.springer.com/article/10.1007/s12197-010-9164-5,The effect of the Kyoto Protocol on carbon dioxide emissions,January 2012,Risa Kumazawa,Michael S. Callaghan,,Female,Male,Unknown,Mix,,
36,1,Journal of Economics and Finance,26 November 2010,https://link.springer.com/article/10.1007/s12197-010-9165-4,The housing bubble in real-time: the end of innocence,January 2012,Rolando F. Peláez,,,Male,Unknown,Unknown,Male,"The conventional view holds that bubbles are only recognizable ex post; otherwise, they would not occur (Kindleberger and Aliber 2005). However, Reinhart and Rogoff (2009), Borio and Lowe (2002), and Shiller (2000, 2003), document cases in the early stages of financial bubbles when deviations from empirical regularities signaled the looming disaster. It is extremely important to develop empirical methodologies capable of identifying asset price booms before they mature into full-blown bubbles. This paper shows that an early warning system was available that could have probably averted the greatest destruction of wealth in history. The data and analytical techniques were available in real time as the bubble inflated. As early as 2001-02 the relationship between the growth rates of house prices and of income deviated sharply from the historical pattern of more than 25 years. Moreover, the quarterly growth rates of the house price index and of per capita disposable personal income are cointegrated. Therefore, as the disequilibrium grew the more certain and violent the correction had to be. Tragically, we did not have to wait until 2008 to know the denouement. The empirical findings cannot be reconciled with the rational expectations hypothesis or the efficient markets hypothesis (EMH). By 2003, the disequilibrium was unprecedented, yet it would grow for another 4 years of delirious speculative fever. It is important to show that the crisis was not a random event like an unpredictable 100-year flood; instead, as the disequilibrium grew the more predictable was the disaster. The bubble swept into the graveyard of ideas the last remaining illusion about financial market macro-efficiency, vindicating Samuelson’s (1998) dictum. The plan of the work is as follows. Section 1 describes the data and its sources. Section 2 describes a basic structural time series model. Sections 3 shows the smoothed growth rates that market participants could have used to formulate expectations about the growth of house prices in real time. Section 4 shows that the growth rates of the house price index and of disposable personal income per capita are cointegrated. The final section concludes.",6
36,1,Journal of Economics and Finance,06 April 2011,https://link.springer.com/article/10.1007/s12197-011-9176-9,GDP trend deviations and the yield spread: the case of eight E.U. countries,January 2012,Periklis Gogas,Ioannis Pragidis,,Male,Male,Unknown,Male,"The yield curve, measuring the difference between short and long term interest rates, has been at the center of recession forecasting. The theoretical justification of this line of work is that since short term interest rates are instruments of monetary policy, and long term interest rates reflect market’s expectations on future economic conditions, the difference between short and long term interest rates may contain useful information to policy makers and other individuals for the corresponding time frame. Furthermore, when the yield curve is upward slopping during recessions, it indicates that there are expectations for future economic upturn. On the other hand, just before recessions, the yield curve flattens or even inverts. De Graeve et al. (2009) explain the predictive power of U.S bond yield curve through demand shocks, wage markup shocks and the investments shocks. There are two major branches of empirical work in this area: first, simple OLS estimation where researchers try to predict future economic activity and second, probit models that are used to forecast upcoming recessions. The main objective of these two classes of papers is to accommodate the fluctuations of future economic activity taking into account the information that is included in the yield curve and is independent of the exercised monetary policy. According to the influential paper in this line of research by Estrella and Mishkin (1997), the short end of the yield curve can be affected by the European Central Bank or the Federal Reserve or any other central bank, but the long end will be determined by many other considerations, including long term expectations of inflation and future real economic activity. In their paper, after taking into account monetary policy conducted in four major European countries (France, Germany, Italy and the U.K), Estrella and Mishkin (1997) show that the term structure spread has significant predictive power for both real activity and inflation. Bonser-Neal and Morley (1997), after examining eleven developed economies, found that the yield spread is a good predictive instrument for future economic activity. In the same vein, Venetis et al. (2003) reached the same conclusion, as did Hamilton and Kim (2002). On the other hand, Kim and Limpaphayom (1997) tested Japan and found evidence that the expected short term interest rate is the only source of predictability for Japan, and not the term premium. Ang et al. (2006), after modeling regressor endogeneity and using data for the period from 1952 to 2001, conclude that the short term interest rate has more predictive power than any term spread. They confirm their finding by forecasting GDP out of sample. Bordo and Haubrich (2008) examine the predictive power of the yield curve in U.S.A over the period from 1875 to 1997. They find that real growth can be predicted more accurately using both the level and the slope of the yield curve. In the same vein Fisher and Felmingham (1998) argue that Australian consumption growth can be predicted by the yield curve. There is also a class of papers that uses probit models to forecast recessions. Wright (2006), using as explanatory variables the Federal Reserve funds rate and the term spread, forecasts recessions six quarters ahead for the U.S. economy. Chauvet and Potter (2001) propose out-of-sample forecasting using standard probabilities and “hitting probabilities” of recession that take into account the length of the business cycle phases. They found that standard probit specification that does not consider the presence of autocorrelated errors and that has time varying parameters due to existence of multiple breakpoints tends to over-predict recession results. In their paper, Estrella et al. (2003) use recent econometric techniques for break-testing to examine whether the empirical relationships are in fact stable. They find that models that predict real activity are somewhat more stable than those that predict inflation, and that binary models are more stable than continuous models. Feitosa and Tabak (2007), for the case of Brazil, find that the spread possesses information which is not totally explained by the monetary policy. This paper, following the line of previous work using probit models, concentrates on the predictive power of the yield spread in the context of the European Union. To the best of our knowledge, no such analysis has been done yet for the E.U. Furthermore, as a dependent variable, we use the business cycle instead of the commonly used GDP, and a recession in this paper is defined as a deviation of the business cycle below thelong-run trend. We also augment the model with non-monetary policy variables as well, such as the rate of unemployment and the corresponding stock exchange indices in an effort to improve the predictive power of the model. The rest of the paper is organized as follows. In the next section, we describe the data used. We then discuss the methodology and present the empirical results, and finally, in the last section we draw the conclusions for this study.",3
36,1,Journal of Economics and Finance,26 April 2011,https://link.springer.com/article/10.1007/s12197-011-9181-z,Migration and public policies: a further empirical analysis,January 2012,Richard J. Cebula,Usha Nair-Reichert,,Male,Female,Unknown,Mix,,
36,1,Journal of Economics and Finance,11 August 2011,https://link.springer.com/article/10.1007/s12197-011-9200-0,Relating economic infrastructure indexes to investor protection for selected emerging economies,January 2012,Farhang Niroomand,Edward Nissan,,Unknown,Male,Unknown,Male,"Shleifer (2009) contends that economists have remained at odds in assessing economic progress in the last 25 years. Two divergent views are captured by two recent books that Shleifer reviews. The first book is a collection of articles edited by Balcerowicz and Fisher (2006). The second book is by Stiglitz et al (2006). While the first book comes out on the side of free market policies, the second book disagrees. Shleifer provides first-hand facts of economic and social development during the period 1980 to 2005. Inflation-adjusted per capita world wide rose from $5,400 in 1980 to $8,500 in 2005. Life expectancy grew and mortality and poverty fell. More countries became democratic with wide acceptance of free market policies. Shleifer enumerates the events of the reforms in the 1980s in China, and the elections of Margaret Thatcher in Britain and Ronald Reagan in the United States as important to mark the beginning of the economic progress during this period. Shleifer’s summary of the views outlined in the first book is that open economies in stable macroeconomic environments with protection of property rights ensure rapid economic growth. In the words of Shleifer (page 126), “Milton Friedman would have put it better, but with the same idea.” His summary of the views of the second book is that the economic world is gloomy. The book is a critique of free-market policies, with proposals for alternatives. Among the alternatives are state ownership and extensive regulations with emphases on virtues of inflation and capital controls. Shleifer’s personal views regarding the quarter century under consideration is that some economies that embraced capitalism after moving away from socialism began to grow after initial setbacks. Asian economies grew rapidly, yet faced difficulties in the 1990s. Even though South America adopted budget discipline and privatization in the 1980s and 1990s, their economies did not realize rapid growth because of overbearing taxation and regulation. The experience the period taught is that economic and political disorder along with obsolete human capital can hinder economic turnaround. In short, the central challenges are human capital, predatory regulation and tax policies, especially in Africa and Latin America. Free-market policies with support from governments can deliver growth and prosperity. Henry (2007), in his review of a large number of publications, concurs with the personal opinions of Shleifer in that liberalization, especially capital, was effective on the cost of capital, investment and economic growth. Henry used emerging market economies for his assessment of the impact of liberalization of their stock market reforms as an important shift for capital account openness. The 17 economies that will be the subject of this paper introduced stabilization programs, trade liberalization and privatization sometime between 1986 and 1995. The question Henry tries to answer is why return to capital in developing economies, in spite of liberalization, is smaller than in developed countries. The classical answer to this question is differences in capital-to-labor ratios across countries due to the level of total factor productivity. Typically, total factor productivity is an index measure of technology or ideas. Henry proclaims that given a stock of technology, total factor productivity measures the efficiency with which an economy transforms capital and labor into output. Included also in total factor productivity are weak institutions and inappropriate government regulation that may distort decision making by reducing the total factor productivity. The return to capital in emerging economies should be higher than in rich economies unless emerging market governments encourage capital accumulation, invention and technology and skill acquisition. For purposes of illustration, Henry groups the economic institutions into what he calls economic infrastructure and displays for the purpose of ranking a selection of emerging economies by four frequently employed measures. The four quality-of-economic-institutions indexes are the measure of social structure by Hall and Jones (1999), the Index of Economic Freedom by Heritage House (2006), the Doing Business Index by World Bank (2006) and the Global Competitiveness Index by World Economic Forum (2006). The rating of the economies as reported by Henry (page 912) is reproduced in the paper in Appendix A. When investors perceive that investment protection is weak, they will stay away. Five measures for investor protection were employed. They are the rule of law, efficiency of the judicial system, contract repudiation, expropriation risk, and the accounting system, incorporated from La Porta et al (1998), reported by Henry (page 913) and reproduced in this paper as Appendix B. Henry employed the infrastructure indexes and investor protection to elicit empirical prediction that the impact of liberalization should be stronger investor protection. Henry found when comparing the emerging economies average with the G-7 average for the economic infrastructure (See Appendix A) that the G-7 scores (the lower the better) for the four indexes were 14, 14, 22 and 15, as compared to the emerging market scores of 63, 68, 83 and 59. Henry concluded that the economic infrastructure of emerging markets is weaker than that of developed countries. Similarly for the investor protection (See Appendix B), the scores for the G-7 for the five variables (the higher the better), were 9.1, 9.2, 9.2, 9.5 and 6.4 out of a maximum of 10. The scores for the developing economies were 4.8, 6.2, 6.1, 6.8 and 5.7. The purpose of this paper is to expand the analysis by probing further the data displayed in Appendix A and Appendix B. This is done by comparing the countries for their average ranks and relating by regression methodology the economic infrastructure indexes as dependent variables to investor protection variables jointly and singly as independent variables. It is expected that the regression coefficients will be negative. Note that the accounting standards indicator variable in Appendix B will not be used in the regression because of many missing values. The paper, after this introductory section, is composed of sections 2, 3, 4 and 5.",1
36,2,Journal of Economics and Finance,08 January 2010,https://link.springer.com/article/10.1007/s12197-009-9117-z,The impact of security concentration on adverse selection costs and liquidity: an examination of exchange traded funds,April 2012,Kenneth Small,James Wansley,Matthew Hood,Male,Male,Male,Male,"In this work we explore the relationship between adverse selection costs, as a percentage of the bid-ask spread, and liquidity for a sample of basket securities, namely Exchange Traded Funds (ETFs), and we compare these relationships to a matched sample of equities. Prior theoretical works predict lower adverse selection costs in basket securities relative to individual equities (Subrahmanyam (1991), Gorton and Pennacchi (1993), and Kumar and Seppi (1994)). While we present the first test comparing adverse selection costs for ETFs and equities, empirical tests comparing mutual funds and equities have generally borne out these predictions, although the differences in adverse selection component of the spread have typically been smaller than expected. Neal and Wheatley (1998) estimate the adverse selection component of the bid-ask spread for 17 mutual funds and a control sample of 17 common stocks. They find the Glosten and Harris (1988) model estimates averaged 19% for the funds and 34% for the control stocks and that the George et al. (1991) model estimates averaged 52% and 65% for the mutual funds and control stocks, respectively. The small difference between estimates of the adverse selection component of the spread for equities and closed-end mutual funds present a problem for Neal and Wheatley. They state: “Adverse selection arises primarily from factors other than a firm’s current liquidation value” (p.123), and they also suggest that the adverse selection spread decomposition models may be mis-specified. We test whether their findings may also be related to common factors in the portfolio of underlying securities, and we find evidence that adverse selection costs decrease as the number of equities held in the underlying ETF increases and does not increase as the concentration (using a Herfindahl index) among the securities increases. We estimate measures of liquidity and the adverse selection component of the bid-ask spread and test for determinants of the spread component for exchange traded funds. Based on the prior theoretical work and empirical findings comparing adverse selection components of mutual funds to individual equities we expect to find lower adverse selection costs for ETFs than for matched equities. Following Gorton and Pennacchi (1993), one possible reason for these differences is that informed agents may prefer to trade industry concentrated basket securities to avoid detection by regulatory agents or uninformed traders who might be monitoring their trading activities in the underlying securities. Finally, we explore the determinants of liquidity and the adverse selection component of the spread, focusing on differences in portfolio construction and concentration. Why do some basket securities rank as the most traded instruments in the U.S. market (e.g., QQQQ) and some are in the lowest quartile of trading volume (e.g., MTK), even though they may be concentrated in the same industry or hold common securities?Footnote 1 Some ETFs hold as few as 11 securities and some hold over 2000. Does the addition of securities diversify away adverse selections costs? To explore these issues, we examine the adverse selection costs and liquidity of ETFs versus a matched sample of equity securities. We also explore factors that contribute to basket security liquidity and adverse selection costs. Neal and Wheatley (1998) and Chen et al. (2002) examined adverse selection costs in closed-end mutual funds, but we focus on ETFs because of their unique structure. ETFs trade intra-day and earn returns that are very similar to those of their underlying portfolio of securities. Unlike many closed-end mutual funds, which often trade at a discount or premium to net asset value, exchange traded funds are easily created and redeemed. This process reduces the difference between the price of the ETF and its net asset value. The elimination of the premium or discount also reduces investor uncertainty regarding the future value of the security. By focusing on ETFs we remove any noise that premiums and discounts introduced in previous studies. Our results indicate that exchange traded funds have significantly lower adverse selection costs than a matched set of equities, regardless of the model used to estimate adverse selection costs. We find Lin et al. (1995) ETF adverse selection costs, as a percentage of the bid-ask spread, to be 19.7% for ETFs and 34.3% for a matched sample of equities; these percentages are 29.6% and 72.6% using the George et al. (1991) model, and they are 18.1% and 44.1% using the Glosten and Harris (1988) model. In a multivariate framework, ETFs also have significantly lower adverse selection costs than the sample of equities. We also document significantly higher levels of quoted dollar depth for ETFs compared with matched equities. Actually, the ETFs have quoted dollar depth 35 times larger than the sample of equity securities. However, the ETFs also have higher effective and quoted spreads than the matched sample of equities. When considering spreads and depth in a unified framework, ETF liquidity is significantly greater than that of a matched sample of securities. An extended liquidity and adverse selection analysis indicates that sector concentrated ETFs do not exhibit lower levels of adverse selection costs or decreased liquidity. In addition, we find no evidence that the concentration among the equities held in the underlying portfolio of a basket security has an impact on adverse selection costs or liquidity. We do find evidence that the number of securities held in the basket has a significant impact on liquidity and adverse selection costs. As the number of securities held in the underlying portfolio increases, adverse selection costs decrease and liquidity increases.",2
36,2,Journal of Economics and Finance,09 January 2010,https://link.springer.com/article/10.1007/s12197-009-9118-y,Stock market return and volatility: day-of-the-week effect,April 2012,M. Hakan Berument,Nukhet Dogan,,Unknown,Unknown,Unknown,Unknown,,
36,2,Journal of Economics and Finance,20 January 2010,https://link.springer.com/article/10.1007/s12197-009-9120-4,A model of promotion and relegation in league sports,April 2012,John Jasina,Kurt Rotthoff,,Male,Male,Unknown,Male,"Most North American sports leagues are closed leagues that operate with a fixed set of teams every season. This differs from other leagues throughout the world that have open leagues that practice promotion, or a team from a lower division being promoted to a higher league, and relegation, where the lowest teams of a given division are demoted to a lower division. The difference in league structure means teams make choices concerning inter-season strategies, investment in players, and investment in revenue generating activities differently. All of the major sports leagues in North America, Major League Baseball (MLB), National Basketball Association (NBA), National Football League (NFL), the National Hockey League (NHL), and Major League Soccer (MLS) are closed leagues. Entry or expansion of a closed league is rare, granted only by the approval of existing team owners and is typically accompanied by a large fee. Since 1995 four teams have entered the NFL. The Carolina Panthers and Jacksonville Jaguars began play in 1995 and the Houston Texans, who began play in 2002, were allowed to enter the NFL after a $700 million payment to the league. The fourth team, the Cleveland Browns, was allowed to re-enter the league after the existing franchise left town for Baltimore. Outside of North America most leagues are open leagues. Open leagues are set up with several hierarchical divisions and use a system of promotion and relegation. The primary determinant of promotion and relegation is on field success.Footnote 1 At the end of each season the worst performing teams in the upper divisions are relegated, or demoted, to a lower division and the best performing teams in lower divisions are promoted to higher divisions. The English Football League is divided into four hierarchical divisions. Below the Football League are several other smaller leagues. For detailed descriptions of the English Football League and historical facts see Noll (2002) and Szymanski and Valletti (2005). The primary difference between an open and closed league is the ability for new teams to enter the existing league. As stated above, entry into a closed league requires the prior approval of the existing team owners and a large fee. A closed league will expand if there are profit maximizing opportunities in cities that do not currently have a professional team or to prevent entry by a competing league. Most likely a competing league would choose locations that were unexploited by an existing league, thus the existing league must choose to expand into unexploited markets in order to contest the other league. Promotion and relegation serves as a means of entry into the open leagues. Any person could start his or her own team, begin competing at the bottom of the league and gain promotion to the major league over time. Entry could also be achieved by purchasing an existing minor league team and hire quality players and coaches to achieve the same result. Entry in an open league does not require approval by a franchise fee or existing team owners. Because closed league teams don’t face free entry, there is less competition for television contracts and ticket prices. This allows teams to not only extract monopoly rents from these contracts, but they can also extract public subsidies for stadiums and facilities (Coates and Humphreys 2003; Jasina and Rotthoff 2008) because of the scarcity of teams. The league, utilizing its monopoly powers, can also impose blackout rules on television coverage, which forces fans to purchase tickets to the event before the league broadcasts the event in local markets. This behavior is well documented in the literature (Noll and Zimbalist 1997; Quirk and Fort 1992, 1999). In open leagues it is conceivable that a city could have more than one team competing in the same sport and even in the same league. London has several teams competing in the top two tiers of the English Football League. The presence of alternatives keeps ticket prices below monopoly levels, meaning the only monopoly powers a team can have, in open leagues, is a name brand monopoly. The presence of alternative teams also reduces the incentive for cities to pay subsidies to teams because a club’s threat of relocation is limited since there is a credible threat of entry from a new team. One example of the open league system is the history of Rushden and Diamonds in English soccer. In 1992 two teams, Rushden Town and Irthlingborough Diamonds merged to form Rushden and Diamonds. Prior to the merger, Rushden was relegated to the Southern League Midland Division because their facilities were deemed unfit for Premier Division football, and Diamonds competed in the United Counties League. The merged team was able to reach Football League status after 9 years. Another advantage of promotion and relegation is that it eliminates meaningless games that are prevalent in North American sports where it becomes clear at some point during the season which teams will and will not compete for the championship that year. The teams who finish worst are rewarded with the best picks of next year’s draft. In addition to this benefit, there are no penalties for coming in last. Teams actually go out of their way to lose games (Taylor and Trogdon 2002) because of these incentives and these meaningless games are less competitive and less desirable to fans. In open leagues teams compete for the right to be promoted, but also to avoid relegation. Teams not in contention for the league championship have the incentive to field the best quality team they can, all season long, to prevent relegation. Adopting a model from Szymanski and Ross (2000), it can be shown that teams tend to spend more on player talent in an open league than in a closed league. The increased spending on player talent means that teams in an open league will be less profitable than teams in a closed league. This paper shows that the Szymanski and Ross model implies that aggregate spending is higher for an open league, relative to the closed league, and aggregate profit is lower. A relegated team will have to play in a lower division which has lower revenue generating potential. Because of this, teams will spend more on player talent to avoid relegation. The prospect of promotion to a higher division, with more revenue generating potential, also causes teams in the lower division to spend more than they would in a closed league. These results are consistent with Noll (2002), where Noll concludes that, holding all else constant, leagues that practice promotion and relegation will have stronger teams than leagues that are closed. Major League Soccer (MLS), the top soccer league in the United States, will always be weaker than teams from the top leagues in Europe because the MLS is a closed league. Szymanski and Ross (2000) consider a case with two large market teams and two small market teams and find that the difference between the strength of the two markets determines the total effort in the league. As the difference between the markets increases, total league effort increases, but they were unable to generalize any results for their model. Szymanski and Valletti (2005) take the Szymanski and Ross (2000) model a step further by solving and generalizing the model for the n-team case. Our paper continues the generalization of this model by allowing the contrast of a closed system and an open system. We solve for the equilibrium levels of spending on player talent as a function of league size and the number of teams being promoted and relegated. The next section will set up the models for both the open and closed leagues, followed by the analysis of these models. The last section will conclude.",9
36,2,Journal of Economics and Finance,29 January 2010,https://link.springer.com/article/10.1007/s12197-010-9121-3,On the feasibility of monetary union among Gulf Cooperation Council (GCC) countries: does the symmetry of shocks extend to the non-oil sector?,April 2012,Rosmy Jean Louis,Faruk Balli,Mohamed Osman,Unknown,Male,Male,Male,"In this paper we address two key empirical questions. First, to what extent do the non-oil sectors of the GCC countries, namely Bahrain, Kuwait, Qatar, Oman, Saudi Arabia, and the United Arab Emirates (UAE), satisfy the prerequisite of common shocks for monetary union?Footnote 1 Second, does the degree of shocks symmetry or asymmetry between the GCC countries and the United States (US) and/or the three largest European economies (France, Germany, and Italy) warrant the choice of either the US dollar or the Euro or a combination of the two as the anchor for the newly proposed single currency? This paper is motivated by the upcoming signing of a monetary union by GCC countries and the issuance of a single currency, which is to be pegged to the US dollar. Also, since the recent decline in the value of the dollar relative to the Euro and other major currencies has rekindled the debate on the merit of the dollar as a solid anchor; we are particularly interested in determining how suitable of an alternative the Euro or a basket could be for these countries. There is an abundant literature on the choice of exchange rate regimes and the dollarization of economies. Most notably is the seminal paper of Mundell (1961) on optimum currency areas (OCAs) along with subsequent works by McKinnon (1963), Kenen (1969), and Tower and Willet (1976) that stress the importance of relative economic sizes, labor mobility, degree of openness, trade concentration, and similarity of shocks. These contributions constitute the workhorse for assessing the suitability of fixed, flexible exchange rate regimes, and prospective monetary unions. The determination of the degree of symmetry between shocks across countries has been thus far the most popular criterion used in empirical works to evaluate OCAs. According to this approach, one needs to examine whether AD and AS shocks are correlated across member countries to conclude on the desirability of monetary union, ceteris paribus. In this paper, not only are we interested in the suitability of either the US dollar or the Euro, or an index based on the two currencies as the principal anchor for the new GCC currency, but we also examine the symmetry of shocks both across and between member countries and possible anchor countries. The debate on whether fixed regimes are better than floating regimes, vice-versa, is a very old one. Under a fixed exchange rate regime, monetary policy is imported and provided that fiscal and monetary disciplines are in order at home, inflation and output tend to be stable. The ensuing costs are forgone potential seignorage revenue and the inability to respond to asymmetric shocks, among others. Under flexible exchange rate systems, countries usually experience higher inflation and lower growth but are better equipped to respond to economic shocks since the conduct of monetary policy rests with their central banks. However, the theory of OCAs is clear on its prescriptions regarding the choice of exchange rate regime. Countries that are subjected to idiosyncratic shocks are better off in retaining monetary policy independence while those that are subjected to symmetric shocks may opt for a fixed exchange rate system. In this regard, it is the symmetry of shocks that dictates whether the Euro or the dollar or a basket is the suitable anchor to peg the new currency to. Following the work of Bayoumi and Eichengreen (1994), output and prices are two key macroeconomic variables whose dynamics have been intensely scrutinized to gauge AD and AS shocks. However, in the case of GCC countries, it is well-known that output is heavily dichotomized. About 54% of total output for the group comes from the non-oil sector while the oil sector contributes the remaining 46%. An important feature of these economies is that they already face symmetric shocks in the oil sector which partially justifies the undertaking of currency unification and eventually the pegging of that currency to the US dollar. Since oil output is traded in US dollar in the international market, they all react similarly to shocks tributary to that sector. Whether they themselves produce those shocks by curtailing production via the Organization of Petroleum Exporting Countries (OPEC) or respond to those shocks emanating from increase or decrease in demand. Recently, there have been renewed efforts from GCC governments to reduce their reliance on the oil sector through reinvestment of oil revenues in infrastructure, tourism, construction, and other services in an attempt to further diversify their economies. In essence, the fundamental issue we address in this paper is whether the growing importance of the non-oil sector for these economies likely to impose substantial adjustment costs if their responses to shocks are not synchronized. The case of the GCC countries is peculiar. All member countries but Kuwait have had their currencies pegged to the US dollar since the 1970s. With the apparent downfall of the US dollar as a reserve currency and the rise in inflation, there have been calls for a revaluation of individual currencies with respect to the dollar, and in some cases, for even a de-peg of the currency to the US dollar in favor of either the Euro or a basket peg. Since the GCC countries plan to achieve a monetary union by 2010 and peg their new currency to the US dollar, it is imperative to test whether GCC countries and the US are subjected to similar macroeconomic shocks, notwithstanding, it is monetary policy from the US that will shape the path of non-oil output and prices in these countries as an economic bloc. Since the Euro or a basket peg with major currencies such as the Euro and the dollar has been suggested as alternatives, it is a natural extension while we are extracting the shocks to look into the suitability of these alternative anchors. In view that the Euro would play the same role as the dollar for the GCC countries, we investigate whether shocks from the GCC countries and the core European economies are symmetric. Our hypothesis is that if both demand and supply shocks are symmetric between the GCC countries and the US (core European countries), then the US dollar (Euro) is qualified as the suitable anchor. However, if, say, supply (demand) shocks are symmetric between the GCC countries and the US but demand (supply) shocks are symmetric between the GCC countries and the core European countries, then it may make sense for a basket with these two major currencies to be the appropriate anchor. The choice of an anchor here is only guided by the underlying principle that the costs of forming a currency union tend to be relatively small when shocks are synchronized across countries. In this vein, it is preferable for a country to adopt the currency of another country with which they share at least one common shock as opposed to none, of course, other factors such as political and cultural affinities remain the same. The approach taken in this paper is in line with previous works in the literature (Bayoumi and Eichengreen 1994; Horvath and Rátfai 2004) that employ bivariate structural vector autoregression (SVAR) of output growth and inflation identified with long-run restrictions à la Blanchard and Quah (1989). We compute these variables using data on non-oil output, real GDP, and GDP deflator for the period 1970–2006 from the United Nations Statistical Databases—National Accounts Main Aggregates. In the case of GCC countries, our SVARs contain non-oil GDP growth and inflation. For the prospective anchor countries, we use data on real GDP, since we have no compelling reason to disaggregate their output into oil and non-oil components. Non-oil GDP can be seen as a proxy for industrial production, which is a subcomponent of real GDP. Therefore, there is no great loss of information from calculating the correlation between shocks originating from SVAR with non-oil output growth and those emerging from real GDP growth. We use the long-run restriction that only supply shocks can have long-term effects on non-oil output (output) to identify our model though it may be quite possible that “positive shocks to the oil sector may spill over substantially to the non-oil sector through for example increased spending on education and infrastructure.”Footnote 2 Our results show at the 5% significance level that: (a) although demand as well as supply shocks are symmetric for the core European countries, these shocks are mostly asymmetric with shocks affecting the GCC countries; (b) GCC supply shocks to the non-oil sector are asymmetric with US supply shocks; (c) with the exception of the UAE, demand shocks are mostly symmetric between the GCC countries and the US; (d) On average supply shocks to the non-oil sector as well as demand shocks are symmetric across GCC countries, with the latter showing a tighter link. These results conspicuously suggest that there are major adjustment costs involved for GCC countries if they chose to anchor their new currency with the Euro. Despite its continuous decline vis-à-vis other currencies, the US dollar remains an optimum choice since monetary policy from the US can smooth demand shocks both at home and in the GCC countries. The rest of the paper is organized as follows. Section 2 discusses the empirical background. Section 3 presents the underlying theory and the SVAR methodology. Section 4 describes and analyzes the data in details. Section 5 discusses the empirical results and Section 6 concludes the paper.",6
36,2,Journal of Economics and Finance,10 February 2010,https://link.springer.com/article/10.1007/s12197-009-9119-x,A panel data analysis for housing affordability in Taiwan,April 2012,I-Chun Tsai,Chien-Wen Peng,,Unknown,Unknown,Unknown,Unknown,,
36,2,Journal of Economics and Finance,23 February 2010,https://link.springer.com/article/10.1007/s12197-010-9122-2,A vector-autoregression analysis of credit and liquidity factor dynamics in US LIBOR and Euribor swap markets,April 2012,Finbarr Murphy,Bernard Murphy,,Male,Male,Unknown,Male,"US Corporate debt exceeded US$6 Trillion in 2008Footnote 1 and remains one of the largest traded asset classes. Key to understanding and managing the risk of this asset class is the additional risk associated with the debt over and above perceived, default-free paper such as US sovereign debt. The determinants of this additional risk are of particular interest to fixed-income portfolio managers, and have key implications for bank treasurers, proprietary swap traders and indeed government agencies charged with responsibility for the structuring and risk-management of sovereign debt programmes. Our focus in this paper is aimed at understanding the variation in the spreads on default and liquidity risk prone, fixed for floating interest rate swaps. Moreover, the perspective we adopt is very much influenced by the intuition and market-based practices adopted by a proprietary swap trader, or bank treasurer charged with managing the bank’s liquidity requirements in the interbank term deposit and swap markets. Using a vector autoregressive (VAR) analysis, we determine the contribution of both liquidity and credit-default risk-factors to swap spreads using highly-liquid market-traded proxies, the LIBOR–OIS spread and the spread on CDS contracts. The impact of the credit crisis from 2007 is examined and analyzed using both Granger Causality testing and impulse-response analytics. The analysis contributes to the growing literature in this area by providing empirical evidence that liquidity and default risk have considerably stronger effects on swap spreads than previously estimated. Our analysis further  extends the contributions of earlier work (Duffie and Singleton 1997) by providing an inter-temporal perspective on the dynamical interplay between credit-default and liquidity risk-factors, and hence between the credit and money markets. Moreover, given the severity of the credit crisis that commenced in 2007, the relative impacts of credit and liquidity shocks on swap spreads have undoubtedly changed. Based on our use of highly-liquid credit default swap and overnight-index swap data, possible new scenarios are  presented. There is an abundance of literature (Longstaff and Schwartz 1995; Grinblatt 1995; Duffie and Singleton 1997; Duffee 1999; Collin-Dufresne et al. 2001; Elton et al. 2001; Huang and Huang 2003) demonstrating that the difference in yields between corporate bonds and Treasury debt cannot be fully explained by structural models. Most of these articles postulate that liquidity concerns is the second order determinant along with other factors producing the yield spread. Much of this early research was limited by having only bond data available. Soon after Credit Default Swap (CDS) trading began, the role of the CDS as a credit spread determinant was examined (Longstaff et al. 2005; Berndt et al. 2004; Zhu 2006). CDSs provide a very pure measure of default risk by isolating the default probability of the underlying name and the ability of participants to buy and sell credit in liquid, standardized markets. The swap spread in our paper is defined as the difference in basis points between the quoted price of a 2-year fixed-for-floating, spot starting, interest rate swap and the quoted yield on the comparable-maturity Treasury instrument. Treasury instruments are default-free while interbank term-deposit yields include a premium for default and liquidity risk. The increased liquidity of CDSs suggest that at least part of the swap spread can be explained by the spreads on relevant CDSs. Consequently, we build on Duffie and Singleton’s (Duffie and Singleton 1997) work and later work (Huang and Neftci 2003; Longstaff et al. 2005) in this area by including a time series of CDS prices. This work differs from earlier papers in that a liquid, market-quoted liquidity proxy is used as a secondary determinant of the variation in swap spreads. We compare results from a data series covering the credit crisis (2006–2009) with those of previous studies. The application of CDSs as a proxy for credit provides an innovative application of this asset class as a measure of swap spreads. The results have implications for credit derivative traders and bond traders because of the strong correlation between asset types. In addition, the use of a data set that covers a historically volatile period in all markets and a period with exceptionally high spreads provides evidence of changed dynamics between the relative impact of credit and liquidity on swap spreads. This includes the immediate impact of shocks in credit and liquidity to the credit spread and also to the temporal effects of these factors. Finally, the so-called “credit crisis” period is as much about the adverse impact of liquidity risk as it is credit on the normal functioning of the money markets. The interaction of these two factors is examined producing evidence of a significantly strengthening liquidity factor over the period. This is in line with contemporary market intuition on the widely commented absence of liquidity in the money markets during the second half of 2008. The seminal paper dealing with the determinants of swap spreads was presented by Duffie and Singleton (Duffie and Singleton 1997).Footnote 2 Prior to this, it was argued (Grinblatt 1995) that the liquidity difference between one month LIBOR and 1 month Treasury bill yields is enough to explain swap spread movements. Calibrating a one factor model (Vasicek 1977), Grinblatt found that the one month spread explained about 35% to 40% of the variation of swap spreads between two and ten years maturity. Grinblatt assumed credit spreads did not significantly impact on swap spreads and were not included in his model. The bargaining position of swap counterparty’s was investigated (Lang et al. 1998) by analyzing the impact of credit spreads on the swap market. Although their research is limited by the research question, they show that credit risk and procyclical economic status influence the swap spread. D&S97 used four variables in a VAR analysis to capture the impact of liquidity and risk effects. They used the six month Treasury zero yield (TB6),Footnote 3 the spread between the three month repo rate on the 10-year Treasury note and the repo rate on the on-the-run (OTR) Treasury note (REPOSP), the spread between rates on BAA- and AAA-rated commercial paper (CPS) and the spread between the 10-year zero rate implied by the swap and Treasury markets (ZEROSP10). These variables look to examine how the swap spread is affected by itself (ZEROSP10), risk free rates (TB6), liquidity (REPOSP) and credit risk (CPS). They showed that the spreads were influenced by both liquidity and credit factors but the nature of the impact was quite unique. They showed that liquidity effects are short lived and that credit effects are weak initially but increased in significance over several months. Huang & Neftci (Huang and Neftci 2003)Footnote 4 extend D&S97 by introducing daily data on credit spreads, on-the-run (OTR) Treasuries and LIBOR rates. This allowed them to segregate credit spread considerations from liquidity considerations. Specifically, they used five variables; swap spreads (SwapSprd),Footnote 5  6-month LIBOR (LIBOR), modified duration (duration), credit spread (credit) and the slope (slope) of the term structure. D&S97 found that liquidity effects are strong and short lived and positive except for a brief initial period. H&N03 found liquidity has an initial negative influence on swap spreads. Our results agree with those of D&S97 and show a positive, long term influence of liquidity. D&S97 found that credit effects are initially weak but do have a negative persistence over time. In contrast, H&N03 found that credit effects were initially negative but dissipated quickly. Our results showed a strong positive influence of credit shocks on the swap spread. The contrast with D&S97 and H&N03 must be understood in the context of the credit crisis since 2007 which covers the data period. D&S97 argued that their results supposed that in recessionary times, “commercial paper spreads to Treasuries rates tend to narrow” and therefore leads to a narrowing of the spread between 10-year zero rates implied by the swap and Treasury markets. Others (Friedman and Kuttner 1993) make a more decisive argument showing that the difference between the interest rates on commercial paper and Treasury bills typically widens in advance of recessions, and narrows again before recoveries. Our results demonstrate a significant positive impact of a credit “shock” on swap spreads that tends to diminish quickly (after about 10 weeks). The resultant analysis shows that the absolute impact of this shock lags the swap spread by some 3 weeks. The turmoil of the markets during the data period will subject the results to further analysis as the data available grows, yet the unambiguous role of CDS data in swap spread analysis is manifest. The paper is organized as follows. Section 2 overviews credit default swaps, spot-starting LIBOR interest rate swaps and Overnight Indexed Swap derivatives from a pricing, proprietary trading as well as an interest rate hedging cum banking liquidity management perspective. Section 3 explains the Vector Autoregression (VAR) technique along with our likelihood ratio estimation of optimal lag length and analysis of both Granger Causality and impulse-response outputs of the VAR model. Section 4 discusses the data set and provides a market perspective or intuition for the particular choice of variables. Section 5 compares the empirical results against the background of earlier work in this area. Section 6 concludes and comments on the implications of our findings for the macroeconomic policy actions and remedial liquidity interventions of both European and US central banks during the current crisis in global money markets.",1
36,2,Journal of Economics and Finance,25 March 2010,https://link.springer.com/article/10.1007/s12197-010-9123-1,Executive compensation and gender: S&P 1500 listed firms,April 2012,João Paulo Vieito,Walayet A. Khan,,,Unknown,Unknown,Mix,,
36,2,Journal of Economics and Finance,06 April 2010,https://link.springer.com/article/10.1007/s12197-010-9125-z,Long-term reactions to large stock price declines and increases in the European stock market: a note on market efficiency,April 2012,Achim Himmelmann,Dirk Schiereck,Moritz Zschoche,Male,Male,Male,Male,"The well-known efficient market hypothesis (EMH), which postulates that asset prices fully reflect all relevant information, is one of the pillars of modern finance (Ross 2002). The implications of the EMH are that stock prices are not predictable and investors cannot earn abnormal returns. Despite the intuitive appeal of the EMH, numerous anomalies, or apparent contradictions of the hypothesis, have been documented (Fama 1991). Conspicuous among theses anomalies are those that deal with predictable patterns in stock returns following large price increases or large price decreases. Such patterns, should they prove to be significant, would impugn the EMH in its weakest form. In attempting to explain the apparent reversals in stock returns over fairly long holding periods of 3 years or more, De Bondt and Thaler (1985) postulate that the predictable reversal in returns that they document could be due to overreaction on the part of investors. That is, investors overreact driving stock prices too high or too low, then prices move in the opposite direction in order to correct the initial overreaction. In contrast to the return reversals posited by the Overreaction Hypothesis (ORH) of De Bondt and Thaler, other researchers document momentum in short-run returns, which some have suggested could be the result of underreaction on the part of investors (see for example, Lehmann 1990; and Jegadeesh and Titman 1993). While the ORH and the Underreaction Hypothesis (URH) both imply a breakdown in market efficiency, a third hypothesis that leaves market efficiency intact, yet incorporates the apparent anomalous stock returns patterns is the Uncertain Information Hypothesis (UIH) of Brown et al. (1988). The UIH postulates that rational, risk-averse investors, when confronted with new information, the true nature of which is not immediately clear, would be expected to underreact to good news and overreact to bad news. Thus, an ex post examination would find momentum in returns, when the news is good, and return reversals, when the news turns out to be bad. Alternately, it has also been argued that the observed empirical evidence is not a contradiction of the EMH, but rather a failure of the methodologies employed in the studies (Chan 1988; and Zarowin 1989). After adjusting for factors such as changing risk, size, or bid-ask spreads the anomalies disappear. This paper seeks to determine which of the four hypotheses best explains the development of stock prices after large movements. We focus on an environment that has not yet been examined: the cross-country European capital market. To that end, the stocks of the EuroStoxx 50 are investigated over the period from January 1999 through December 2003. In contrast to the existing literature, we follow a broad set of approaches and apply several robustness checks to ensure that possible patterns in stock price are not primarily attributable to chance: 1) we define large price movements based on monthly returns as well as on cumulative daily returns; 2) we adjust these price reactions for market developments; and 3) we use three different return models: the market-adjusted-model, a GARCH-specified market model, and a GARCH-specified model that assumes the alpha term is zero, to analyze the price development over time. The results of our analyses support the EMH. We cannot find any systematic pattern in stock price developments subsequent to large movements. Cumulated Abnormal Returns (CAR) after strong price declines and increases are not significantly different from zero in almost all the cases we analyze. These findings hold true for time spans up to 24 months following the events. Hence, we confirm the reasoning of Fama (1998) that recently documented pricing patterns such as ORH or URH depend primarily on study design and cannot be differentiated from pure-chance events once methodological issues are controlled for. Section 2 builds the foundation by introducing various hypotheses which predict the future development of stock prices. The data and methodology of the empirical study are described in Section 3. The event study findings are discussed in Section 4. In the final section concluding remarks are made and continuative issues are formulated.",13
36,2,Journal of Economics and Finance,15 April 2010,https://link.springer.com/article/10.1007/s12197-010-9130-2,Determinants of the medium of payment used to acquire privately-held targets,April 2012,Jeff Madura,Thanh Ngo,,Male,,Unknown,Mix,,
36,2,Journal of Economics and Finance,20 April 2010,https://link.springer.com/article/10.1007/s12197-010-9132-0,Not paying dividends? A decomposition of the decline in dividend payers,April 2012,Candra S. Chahyadi,Jesus M. Salas,,Unknown,,Unknown,Mix,,
36,2,Journal of Economics and Finance,30 April 2010,https://link.springer.com/article/10.1007/s12197-010-9136-9,On the cointegration of international stock indices,April 2012,Richard Fu,Marco Pagani,,Male,Male,Unknown,Male,"Since Engle and Granger’s (1987) seminal paper, the economic literature on cointegration has grown exponentially. Cointegration tests and error correction models are widely used in the finance literature to examine relationships among asset returns. It is Engle and Granger’s (1987) paper that first formalized the concept of cointegration and developed the bivariate cointegration test. However, even if there is no pairwise cointegration, one cannot conclude that there is no cointegration among the multiple time series. Johansen (1988)’s methodology based on multivariate vector autoregressive (VAR) model has provided researcher necessary tools for multivariate cointegration tests and error corrections. An important application of cointegration tests in finance area is to test market efficiency by examining the long-run relationship among time series of prices of different assets. For example, if the prices in two markets are cointegrated then it would be possible to forecast one from the other, which suggests that markets are either not efficient, or individual markets are efficient but segmented, or market indices are driven by time-varying risk factors. Kasa (1992) investigated national stock indices of five developed countries from 1974 to 1990. He used Johansen’s (1991) likelihood ratio test to examine the cointegration relationship among the five national stock indices. He reported a pronounced cointegration relationship among five national stock indices comprising the US, UK, Japanese, Germany and Canadian markets. In particular, his results demonstrated much stronger evidence in favor of cointegration with long-lag models than for short ones. Finally, Kasa showed that in long-lag specifications, there are four cointegration vectors among the five indices and argued that there was one common stochastic trend exists among the five markets. Kasa’s findings are quite puzzling for two reasons. First, the existence of a cointegration relationship runs counter to the efficient market theory. It implies that in the long run, the stock indices among different countries are closely linked, even though they may temporarily diverge in the short run. As shown in the literature, researchers need only examine past returns of stock indices, which are completely public knowledge, to detect potential cointegration relationships. The existence of the cointegration among these five country stock indices violates the weak form market efficiency unless it can be explained by time varying expected return or complete market segmentation.Footnote 1 Second, if there is one underlying common stochastic trend among major countries’ stock indices, as argued by Kasa, the benefits of international diversification would be greatly diminished. On the contrary, diversification benefits from international investments are well documented in empirical studies over past decades.Footnote 2
 Richards (1995) examined the statistical basis for the rejection of the null hypothesis of no cointegration among five national stock markets in Kasa’s (1992) paper. He concluded that the finding of cointegration relationship is due to the failure to adjust asymptotic critical values to take into account the small number of degrees of freedom embedded in the Johansen’s methodology, especially in a long lag VAR model. He then used two other methodologies similar to those of Fama and French (1988) and DeBondt and Thaler (1985) to investigate whether national stock market returns demonstrate weaker form of long-horizon relative predictability. He found that the national stock indices include a common world component as well as two country-specific components and argued that there are comovements among national stock market returns but no cointegration.Footnote 3
 In this paper, we first show that Kasa’s results are persistent if we use the same methodology in a sample for an extended period from 1970 up to 2003, as well as during different sub-periods. Moreover, using Kasa’s methodology, we show the existence of cointegration even among emerging markets, though somewhat weaker than the cointegration among developed countries. Yet as in Kasa (1992), the findings of cointegration largely depend on using long-lag VAR model specification in the Johansen’s test. Next, we investigate whether the findings of cointegration are driven by size distortions of the Johansen’s test, as documented in previous econometric simulation studies. We employ the Johansen ’s (2002) small sample correction for the likelihood ratio tests. After the correction for small sample, we find that the cointegration in our sample still holds, yet it is much weaker than without correction. Moreover, the findings of cointegration critically depend on long-lag VAR models. In fact, after taking sufficiently long lag model, the null hypothesis of no cointegration is always rejected, as in the tests without small sample correction. These results suggest that Richards’s (1995) argument that Kasa’s results are due to small sample sizes is not likely to be a sufficient explanation of Kasa’s apparent findings of cointegration. In the econometric literature, various simulation studies document that the performance of the Johansen’s test is sensitive to the misspecification of VAR model, which may lead to over-rejection of the null hypothesis of no cointegration.Footnote 4 Both Kasa’s study and ours using Johansens’ test show that long-lag VAR model tend to produce stronger results of cointegration, which might be a spurious finding due to the likelihood of the misspecification of the VAR model. In order to avoid this problem, we implement a new non-parametric cointegration rank test developed by Shintani (2001) to test the cointegration relations in our sample. The distinctive feature of Shintani’s test is that it does not need to pre-specify the lag length as the VAR model requires. Using this non-parametric test enables us to avoid the likely size distortion of the Johansen’s test due to the VAR misspecifcation, which results in over-rejection of no-cointegration null. The empirical results from Shintani’s non-parametric test show that we cannot reject the null hypothesis of no cointegration at conventional significance level. In conclusion, our results are consistent with the argument that Kasa’s findings are likely due to statistical misspecifications caused by size distortions in the VAR type cointegration tests. Kasa’s conclusion of pronounced cointegration relationship based upon long-lag VAR model in the Johansen’s test are not supported by the results of non-parametric test, which is not subject to the size distortion due to the VAR misspecification. The rest of the paper is organized as follows: Section 2 reviews cointegration tests and previous empirical findings. Section 3 presents our empirical results of Johansen’s original cointegration test on a expanded sample, Johansen’s (2002) test of small sample correction, and Shintani’s (2001) non-parametric cointegration rank test. Section 4 concludes the paper.",2
36,2,Journal of Economics and Finance,20 July 2010,https://link.springer.com/article/10.1007/s12197-010-9141-z,"The effects of age, experience and managers upon baseball performance",April 2012,Berna Demiralp,Christopher Colburn,James V. Koch,Female,Male,Male,Mix,,
36,2,Journal of Economics and Finance,27 July 2010,https://link.springer.com/article/10.1007/s12197-010-9146-7,How do quotes and prices evolve around isolated informed trades?,April 2012,A. Can Inci,H. Nejat Seyhun,,Unknown,Unknown,Unknown,Unknown,,
36,2,Journal of Economics and Finance,20 January 2012,https://link.springer.com/article/10.1007/s12197-012-9225-z,Erratum to: The interaction of corporate dividend policy and capital structure decisions under differential tax regimes,April 2012,Ufuk Ince,James E. Owers,,Male,Male,Unknown,Male,,
36,3,Journal of Economics and Finance,18 December 2010,https://link.springer.com/article/10.1007/s12197-010-9166-3,Dutch-auction IPOs: institutional development and underpricing performance,July 2012,Mary Ann Robinson,Richard Robinson,,,Male,Unknown,Mix,,
36,3,Journal of Economics and Finance,06 April 2010,https://link.springer.com/article/10.1007/s12197-010-9126-y,Impact of exchange rate volatility on commodity trade between U.S. and China: is there a third country effect,July 2012,Mohsen Bahmani-Oskooee,Jia Xu,,Male,,Unknown,Mix,,
36,3,Journal of Economics and Finance,16 April 2010,https://link.springer.com/article/10.1007/s12197-010-9128-9,Why do banks acquire non-banks?,July 2012,Maretno A. Harjoto,Ha-Chin Yi,Tosporn Chotigeat,Unknown,Unknown,Unknown,Unknown,,
36,3,Journal of Economics and Finance,17 April 2010,https://link.springer.com/article/10.1007/s12197-010-9133-z,Re-examination of industry effects due to withdrawn mergers,July 2012,Jeff Madura,Thanh Ngo,,Male,,Unknown,Mix,,
36,3,Journal of Economics and Finance,29 June 2010,https://link.springer.com/article/10.1007/s12197-010-9139-6,Explanation for market response to seasoned equity offerings,July 2012,Robert M. Hull,Sungkyu Kwak,Rosemary L. Walker,Male,Unknown,Female,Mix,,
36,3,Journal of Economics and Finance,15 July 2010,https://link.springer.com/article/10.1007/s12197-010-9143-x,Bankruptcy behavior in the NFL: does the overtime structure change the strategy of the game?,July 2012,Kurt W. Rotthoff,,,Male,Unknown,Unknown,Male,"“The Great Overtime Debate” between Phil Simms and Jim Nantz on Showtime Sports’ Inside the NFLFootnote 1 demonstrates the continued debate on the issue of overtime in the National Football League (NFL). The debate is built on the idea that the NFL’s overtime structure doesn’t change the nature of the game. However, analysts on other major sports networks state that NFL teams do their best to avoid overtime while college teams are willing to run out the clock to take the game to overtime. This study adds advanced statistical analysis to the ongoing debate; does the overtime structure change the strategy of the game? The NFL and college football both play four quarters of regulation and both have an overtime structure in place if the game is still tied at the end of regulation. However this is where the similarities end. In college football the overtime rule says each team gets a chance to score from the 25 yard line. Each team takes their chance until one team scores more points with their chance than the other team does. This tit-for-tat strategy allows both teams the opportunity to play offense and score. On the other hand, the NFL says the first team to score in overtime wins the game, which is known as sudden death. In this set-up, if the team that gets the ball first scores, the other team will never have a chance on offense. During “The Great Overtime Debate” (2009) Jim Nantz points out that the original NFL overtime rule was established in 1974. From 1974 to 1978 there were 34 overtime games of which the first team to touch the ball won 48.5% of the games (29% had a first position win). More recently, from 2004 to 2008, 72 games went into overtime. In the more recent sample, the team who touches the ball first wins 61.8% of the games, with 38.9% winning on the first drive. Jim Nantz suggests that this occurs because the NFL has changed rules in the league to open up the offense and increase scoring. Rosen and Wilson (2007) write that “[t]he overtime method used in college football is designed to minimize the importance of the coin toss, unlike the system used by the National Football League (NFL). Their sudden-death overtime system has long been criticized for favoring the team that gets the ball first, implying that the team which wins the coin toss usually wins the game.” If overtime structure causes teams in different leagues to act differently when they approach the possibility of going into overtime, their difference in strategy will be observable. The NFL’s rules creates an environment where the optimal strategy to win the game changes. This change in strategy, and implicitly the overtime success being influenced by something outside of their control (the coin flip), will cause the teams in different leagues to act differently. NFL teams should be more likely to act with risky plays late in the game. This study uses a unique approach to measure the unintended consequences that occur because of the overtime structure. I adopt the financial concept of ‘bankruptcy behavior’ to the scoring patterns of college football and the NFL to see if there is any measurable evidence of increased risk taking in the NFL. I find evidence that NFL teams do employ risky strategies to win games when the score is close in the fourth quarter. The next section discusses the differences in the two leagues followed by an explanation of bankruptcy behavior. Section 4 looks at the data and results with Section 5 concluding.",1
36,3,Journal of Economics and Finance,24 July 2010,https://link.springer.com/article/10.1007/s12197-010-9145-8,An empirical analysis of mean reversion of the S&P 500’s P/E ratios,July 2012,Ralf Becker,Junsoo Lee,Benton E. Gup,Male,Unknown,Male,Male,"Numerous authors have suggested that the price-earnings (P/E) ratio can be used to predict the future movement of stock prices. For example, White (2000), Campbell and Shiller (1988, 2001), and Siegel (2000) argued that the S&P 500 was overvalued in the late 1990s and stock prices would decline. Shen (2000) found that high P/E ratios generally have been followed by slow growth in stock prices in the next decade. Such arguments are based on the belief that P/E ratios are mean-reverting. According to Campbell and Shiller (2001), the mean-reverting property of P/E ratios implies that stock prices will not drift far from their normal nominal price relative to earnings. This implies that P/E ratios tend to remain stable around an historical mean. Given mean-reversion, stock prices will fall if current prices are high as compared to earnings, and vice versa. There are two sources of change contributing to the stability of P/E ratios. One is the adjustment process in stock prices (numerator), and the other is the adjustment in earnings (denominator). Campbell and Shiller (2001) state that adjustments through earnings will be weak. Thus, adjustments to high P/E ratios will come from declining stock prices. Therefore the mean-reverting theory has important implications on the long-run stock market outlook. On the other hand, some authors argue that economic fundamentals and economic structures change continuously, and these changes can have permanent persistent effects on P/E ratios. Accordingly, there is no reason to believe that higher or lower P/E ratios should revert to some historical level. Following this line of reasoning, P/E ratios should be characterized as a stochastic process that will fluctuate without bounds. Thus, P/E ratios are mean averting, and it will be difficult to predict the movement of stock prices from P/E ratios. For example, when the Dow Jones Industrial Average was about 8,700 in the late 1990s, Glassman and Hassett (1999), Elias (1999), and Kadlec (1999) wrote books with titles stating that the Dow would go to 36,000, 40,000, and 100,000 respectively. In March 2009, the Dow was 6547, but at the end of November 2009 the Dow was 10,450. Are P/E ratios mean reverting? To our surprise, the literature has neglected this fundamental question. Instead, many authors assume that price earning ratios are mean reverting. These authors then focus on finding evidence of the relationship between P/E ratios and stock prices. Other authors having different views tend to refute the notion of the mean reverting theory simply due to the reason that the relationship is not supported from the data. However, there is a potential problem with both views, since finding or refuting the relationship between P/E ratios and stock prices involves a certain econometric problem. Most readers may agree that stock prices and earnings data are non-stationary. When variables are nonstationary, standard statistical inference can be spurious. Ferson et al. (2003) point out that there is a spurious regression bias in predictive regressions for stock returns. Pontiff and Schall (1998), Goyal and Welch (2003) and Lee et al. (1999) make similar arguments in the case of the book-to-market ratio and dividend yield of the Dow Jones Index. It is clear that spurious regressions can lead to incorrect inferences when dealing with highly persistent data. This point cannot be over-emphasized. Ferson et al. (2003) note that the unit root hypothesis is not rejected for 361 of the 500 stock return series they considered. If stock returns are highly persistent, it is obvious that stock prices are nonstationary. Whether a time series is mean reverting or not depends on whether it is stationary or nonstationary. If it is nonstationary, there is no tendency to revert to a long-run mean. If P/E ratios are nonstationary, they will not contain predictive information about future stock prices. Stated differently, there will be no statistical evidence to support the belief that high P/E ratios should be followed by slow growth in earnings in order to return to a long-run stable mean. Thus, the question of whether P/E ratios are mean-reverting is of crucial importance to analysts, investors, and academics alike. Thus, it is surprising that a careful analysis of whether P/E ratios are mean-reverting has not been undertaken in the literature. Unit root tests are the appropriate statistical tool to investigate whether a series is stationary or not. The null hypothesis of nonstationarity will only be rejected in favor of the alternative hypothesis of stationarity, if the data provide sufficient statistical evidence for such a conclusion. Other related hypotheses such as a random walk hypothesis or a martingale hypothesis typically assume no serial correlation. It is possible that these hypotheses can be rejected, but the series is still non-stationary with serial correlation. A unit root hypothesis allows for serial correlation, and it is directly related to the underlying property of non-stationarity under general conditions. One important implication is that when P/E ratios are stationary, implying mean reversion of P/E ratios, we can say that there is a long-run cointegration relationship between price and earnings. Cointegration holds when one series is a rational forecast of the other, as in the relationship between spot and forward rates in a foreign exchange market. In the case of the P/E ratios used here, it should be noted that the denominator, E, is measured by past earnings, whereas the numerator, P, represents the present value of expected dividends.Footnote 1 Then, the implied concept of cointegration relates to our main question of whether P/E ratios would reveal valuable information about the movement of future stock prices. In the presence of cointegration, the relationship between the P/E ratio and stock prices is not spurious. If stock prices and earning are cointegrated, then the P/E ratio will be stationary. This article deals empirically with the stationarity of the S&P 500 P/E ratio time series that spans over 100 years. We use empirical techniques that incorporate recent methodological developments in the literature. Our statistical results provide an explanation for the apparent non-stationarity of P/E ratios, while at the same time arguing that the P/E ratio will eventually revert to some long-run mean(s). First, we show that the time-series of monthly P/E ratios is statistically nonstationary (difference-stationary). However, this result is reversed after allowing for multiple structural breaks. When structural changes are controlled for in the testing regression, the unit root null hypothesis is rejected. Overall, we find that the S&P 500 P/E ratio is stationary around multiple structural changes. Simply stated, the P/E ratios fluctuate around different means. This result has an important implication that without controlling for known structural breaks, predictive regressions for stock prices or stock returns can be spurious. Therefore, the price-earning ratio can be used as variable to forecast other variables in the stock market only when structural changes are included in the model. Our results are in line with the results in Rapach and Wohar (2005) and Lee et al. (2006), who find that accurate forecasting and empirical verification of theories can depend critically on understanding the appropriate nature of time-series involving structural changes.Footnote 2 Improper modeling of structural changes can lead to incorrect forecasting performance and incorrect statistical inference. Our findings also support the arguments for mean reversion in Campbell and Shiller (2001), but only when structural changes are allowed. The remainder of this paper is organized as follows. In Section 2, we discuss recent developments in the methods and present our testing methodologies. Section 3 reveals our empirical findings. In Section 4, we examine the issue using regime switching models. Our concluding remarks are in Section 5.",10
36,3,Journal of Economics and Finance,29 July 2010,https://link.springer.com/article/10.1007/s12197-010-9144-9,Currency depreciation effects on ADR returns: evidence from Latin America,July 2012,Omar A. Esqueda,Dave O. Jackson,,Male,Male,Unknown,Male,"U.S. investors have relied on American depository receipts (ADR) as a convenient diversification instrument because of their ready availability, ease of availability compared to the hassles of other international investment alternatives, and the growing diversity of stocks (Arnold et al. 2004). However, notwithstanding the advantages of international diversification via ADRs, several inherent risks remain. First, an element of the international diversification advantage commonly found in the literature, exchange rate risk is clearly more present in ADRs than in U.S. stocks. Further, exchange rate risk is multi-faceted (Bae et al. 2008) and any firm with foreign operations has exchange-rate risk. Depository shares whose underlying stock is denominated in a currency different from the official currency of the host country will have translation exposure, in addition to economic and transaction exposure. Latin American ADRs have all three types of exposure. Since most ADR originators have international exposure in their operations, economic and transaction exposure is extremely common in these firms. However, translation exposure is the ubiquitous exchange rate risk carried by the ADR holder when translating cash flow values from the relevant currency to U.S. dollars. In this paper we analyze the behavior of ADR returns during the 300-day period surrounding various currency crises in the originator’s country. We measure the return reduction for holders of Latin American ADRs during these crises periods. Argentina and Chile each had one major currency depreciation while both Brazil and Mexico each experienced three devaluations during the review period. By analyzing the currency depreciations of these Latin American countries that switched from a pegged to a floating currency we are able to measure the depreciation effect of the underlying currency under varying circumstances. Brazil and Mexico have had two more depreciations after the switch, whereas Argentina has maintained a less volatile exchange rate. Chile on the other hand has preserved a floating exchange rate in recent times and according to our criteria, Chile’s only relevant currency depreciation occurred during 2008. The Latin American countries included in our sample share a high and homogeneous linkage to the U.S dollar in addition to being emerging economies, which in turn facilitate the formulation of various inferences from our results. Moreover, since the countries in our study have similar trading hours, these markets should simultaneously react to various stimuli unlike their Asian and European counterparts. Simultaneous trading hours also has the beneficial effect of reducing the likelihood of lagged effects. After controlling for the underlying stock, exchange rate, and home and U.S. indices, we find in all events that currency depreciations result in statistically and economically significant losses for investors. The results persist after including exchange rate returns as a control variable orthogonalized against the corresponding domestic index. The coefficient of the variable exchange rate risk shows that a depreciation of the domestic currency against the U.S. dollar consistently has a negative effect on ADR returns. These losses are particularly significant when we analyze the period just prior to and after the crises period (−150, +150 days around day 0). Hence, we conclude that currency crises carry substantial losses for ADR holders, in addition to the commonly embedded exchange rate risk. These results have important implications for ADR investors attempting to form a diversified portfolio. Investors must hedge against exchange rate fluctuations, particularly if they have a position with ADRs from countries whose currencies are subject to strong market pressures. ADR prices are determined primarily by the underlying stock, exchange rates, and host country index, in that order. However, factors affecting ADR prices have changed their degree of importance over time, with the contribution of the U.S. stock market index having increased in recent years. Liang and Mougoué (1996) and Fang and Loo (2002) suggest that exchange rate risk in ADRs is diversifiable and does not command a premium. This paper does not explicitly test whether exchange-rate risk is priced. However, it suggests that translation exposure alone can substantially affect ADR prices and hedging against it can bear high transaction costs. The effect on ADR prices of economic and transaction exposure depends on the foreign exchange exposure of the underlying firm. Firms need to hedge accordingly, and if they do, the only losses will be those as a result of translation exposure, and exchange rate risk will be a diversifiable risk. Yet, Loderer and Pichler (2000) find for their sample that firms do not hedge against the long term effect of currency depreciation on firms’ future cash flows. Therefore, ADR holders need to hedge not only against translation exposure, but also against the reduction on the stream of future cash flows for the firm following currency depreciations. On the other hand, since markets are segmented, at least during this time period, ADRs can be effectively used for diversification purposes. In order to investigate the effect of exchange-rate risk on ADR returns, we use seemingly unrelated regressions (SUR) and multivariate regression models (MVRM) in a panel sample of 74 ADRs from Argentina, Brazil, Chile, and Mexico during the period from May 1994 to April 2009. Since there is a concern that firms’ residuals are correlated across individual events (crises), one regression is used for every event, for the firms based in the crises country. Hence, we estimate eight SUR equations which include only the ADRs originated in the respective crises country at the time of the event. The paper is organized in the following order; Section 2 outlines our review of related literature, the data, hypotheses, and econometric technique is presented in Sections 3, 4, and 5 respectively. Section 6 includes a discussion of the results, and Section 7 concludes.",9
36,3,Journal of Economics and Finance,10 August 2010,https://link.springer.com/article/10.1007/s12197-010-9147-6,Sudden equity price declines and the flight-to-safety phenomenon: additional evidence using daily data,July 2012,Joe Brocato,Kenneth L. Smith,,Male,Male,Unknown,Male,"Typically the total returns of stocks and bonds are positively correlated. To some extent this positive return association results from common discounted cash flow characteristics that drive both stock and bond prices during periods of macroeconomic stability. While positive stock/bond return correlations are the norm, a casual reading of the literature presents a wide array of numerically different return correlations (differences in magnitude as well as in sign) for both the U.S. and other developed economies. These varying correlations arise from differences in the time period under investigation and different economic environments existing when the stock and bond price data are generated, the frequency of the observations used in the correlation computation, differences in the nature of the index from which the return calculations are obtained, and many other factors. For example, using annual U.S. stock and U.S. long-term government bond data from Ibbotson and Associates—now published by Morningstar (2007)—we find a long-run return correlation of 0.162 when the time period is 1927 through 2001. Extending the interval from 1927 through 2006 increases the coefficient to 0.190. Excluding the Great Depression and WWII from the Morningstar data series produces a coefficient of 0.231. More recently from 1970 through the end of 1980 Dopfel (2003) reports a U.S. correlation of 0.40. Scruggs and Glabadanidis (2003) find that the conditional correlation between U.S. stock and bond returns varies considerably over the post-WWII period, being negative in the late 1950s and early 1960s but positive since the mid-1960s. Engle (2002) using various ARCH models finds that the correlations in the 1990s to be mostly positive. However, in earlier periods Ilmanen (2003) identifies three episodes of negative U.S. stock/bond correlations (1929–1932, 1956–1965, and 1998–2001). He relates these to unique time-dependent economic conditions. Brocato and Steed (1998) in a study of asset allocation over the business cycle from 1972 through 1993 find the monthly stock/bond total return correlation to be 0.253 over expansions, rising to 0.524 over recessions, and stabilizing to 0.325 when expansion and recession data are pooled. Schwert (1990) finds considerable temporal instability of the stock/bond return correlations in the U.S. economy particularly at turning points in the business cycle. A related research finding is reported by Guidolin and Timmermann (2005) for the U.K. They report stock and bond correlations to be positive and significant during economic expansions but negative for economic downturns. Changing the frequency to daily data and using the S&P 500 index and long-term U.S. government bond data compiled by Thomson DataStream we obtain in this paper a correlation coefficient of 0.095 for the March 1984–October 2006 interval. This positive but materially lower correlation coefficient is due to the use of daily frequency data which contain a large random noise component. The chief conclusion to be drawn from this brief review of the evidence is that while historical stock and bond return correlation coefficients exhibit considerable instability over time depending upon economic and expected factors, observation frequency, and index construction, they are usually positive over longer intervals when macroeconomic stability characterizes the financial environment. For this paper, we formally define the flight-to-safety phenomenon as a sudden and unexpected investor-driven re-allocation from risky equity securities to the safety of default-free U.S. government Treasury bonds. At any point in time, an observed stock/bond correlation coefficient and flight-to-safety behavior bear a close relationship. The purpose here is to quantify the economic and statistical significance of this relationship using daily data. From a traditional asset-pricing perspective, an abrupt decrease in the value of stocks can be explained in terms of sudden changes in perceived capital loss probabilities and in the equilibrium required risk premium for bearing this possible loss. As will be statistically quantified, a daily flight-to-safety reaction in fully functioning securities markets has a predictable impact on long-run stock/bond total return correlations. The term flight-to-safety as used in this discussion should be distinguished from asset re-allocation motivated by strategic versus tactical motives. Pure flight-to-safety behavior is characterized by market participants’ desire to abruptly decrease their portfolio’s exposure to securities bearing increased risk of capital loss. Conversely, strategic asset re-allocation decisions are driven by deliberate longer-run portfolio adjustments aimed at a target mix of securities for a given time line, risk profile, return objective, and expected capital market outlook. Tactical re-allocation decisions are geared to receiving (or avoiding) a short-run abnormal return (or loss) due to a perceived alteration in relative security prices that is expected to be temporary. In distinction to strategic and tactical motives, pure flight behavior is motivated by a fear of sudden and permanent capital loss without regard for any long-run strategic portfolio imbalance that may result or to a perceived short-run tactical excess return gain or loss. Whereas strategic and tactical decisions are based upon some measure of information (either current or expected), a flight-to-safety play is predicated upon a sudden information void regarding what future equilibrium prices and required returns will be. Flight-to-safety actions are examples of very short-run, purely idiopathic investor behavior regarding subsequent relative stock and bond prices. While one can tacitly relate ex post unexpected macroeconomic or political shocks to a sudden move from equity securities as we do in this paper, such behavior is initially motivated by the ex ante (and unknown) consequences of not re-allocating quickly. In an innovative paper, Gulko (2002) presents convincing evidence that rapid and unexpected declines in U.S. equity prices shifts the stock/bond correlation from its normal positive sign to a statistically significant negative sign. His methodology is as follows. Using daily data from 1987 through 2000 Gulko identifies six stock market crashes related to macroeconomic or political events. Gulko defines a “crash” as one where equity prices as measured by the S&P 500 index decline by five percent or more over a one-day (or weekend) trading period. Using an event study-type model he aggregates the data into three sub-groups, a pre- and post-crash set of observations covering ten days prior and subsequent to a crash day. The event period which is approximately two weeks in length is nested between the pre- and post-crash periods. Regressions estimated separately on the three spliced and pooled time series over the 1987 through 2000 period show the stock/bond regression slopes to be positive over the pre- and post-crash periods while the event period regression produces a statistically significant negative slope coefficient. Gulko cites this sign change as evidence of a market-decline-induced flight-to-safety. More formally, Gulko uses the term decoupling in the sense that the stock/bond return correlation temporarily disconnects from its normal positive relationship. The sign change can readily be attributed to perceived increases in the equity risk premium associated with sharp and unexpected declines in equity prices and the desire by investors to preserve capital value by rapid purchases of risk-free government debt securities. In this paper we extend and articulate Gulko’s work on the flight-to-safety phenomenon, decoupling, and the temporary shift in the normally positive stock/bond return correlation. In particular, we investigate four additional avenues of market-induced shifts from stocks to bonds: (1) We do not aggregate the data. Gulko’s methodology pools daily data by splicing pre- and post-crash intervals. Such aggregation masks potential differences between various equity market crashes. Using non-pooled daily data our method brings into sharper relief the stock/bond market interaction both before and after a given crash date. This allows us to relate sudden economic disruptions to specific calendar days and any subsequent daily reversion activity; (2) Preceding our regression results is percentage change analysis. Percentage change analysis gives both the sign and magnitude of stock and bond prices (stated as daily percentages) for crash days and for the subsequent trading days. From these results we obtain indirect evidence on the flow of funds between stock and bond markets as these flows relate to crash events; (3) We carry out pre- and post-event correlation and variance analysis so as to gauge the extent of post-event persistence caused by sudden equity market declines. In this context, correlation persistence has been identified in the literature as a by-product of market shocks (Solnik et al. 1996); (4) We extend the time interval so as to allow an analysis of the flight-to-safety behavior resulting from the September 11, 2001 attack. As we will explain, the September 11 flight response is a delayed temporal analysis since the stock market was closed the week of the attack. The paper is organized as follows. Section 2 describes the data, related summary statistics, and raw stock/bond return correlations. Section 3 identifies the crash dates and associated macroeconomic or political event activity, the daily percentage declines occurring on these dates for both stocks and bonds, and the next-day rate of return responses in these markets. Section 4 describes the regression model and the methodology used. Section 5 presents the regression results. Section 6 compares pre-and post-event return correlations and volatility. Here we relate the crash events to post-event turbulence in the U.S. stock and bond markets. Section 7 concludes the paper.",2
36,3,Journal of Economics and Finance,10 August 2010,https://link.springer.com/article/10.1007/s12197-010-9148-5,An entropy approach to size and variance heterogeneity in U.S. commercial banks,July 2012,Lakshmi Balasubramanyan,Spiro E. Stefanou,Jeffrey R. Stokes,Female,Male,Male,Mix,,
36,3,Journal of Economics and Finance,11 August 2010,https://link.springer.com/article/10.1007/s12197-010-9142-y,Principal component analysis of yield curve movements,July 2012,Joel R. Barber,Mark L. Copper,,Male,Male,Unknown,Male,"An important issue in interest rate modeling is the number and nature of the random factors driving the evolution of the yield curve. A number of studies employing factor analysis and principal component analysis have found that two to three factors explain over 95% of the variation in yield curve changes.Footnote 1 Papers employing principal component analysis are either lacking statistical tests (Barber and Copper 1996; Golub and Tillman 2000; Novosyolov and Satchkov 2008) or the tests are indirect in nature. Indirect tests involved evaluating the performance of principal component analysis in a particular strategy or application. These papers test the performance of principal component analysis applied to hedging bond portfolios (Falkenstein and Hanweck 1997), immunization (Soto 2004), value at risk estimation (Abad and Benito 2007), and interest rate forecasting (Reisman and Zohar 2004). These papers contained no explicit statistical tests of the inherent dimension of the interest rate model. Neither principal component analysis or factor analysis has been used to test the shape of yield curve shifts or their persistence over time. Roughly speaking, the difference between principal component analysis (PCA) and factor analysisFootnote 2 (FA) is that the objective of PCA is to determine a small set of components that best explain the total variation in the sample, whereas the objective of FA is to determine small set of factors that best explain the common variation. In finance it is convenient to think of the underlying variables as securities. With PCA the covariance matrix is approximated by linear combination (or portfolio) of the underlying set of securities. In general, the error cross-correlations will not be zero. With FA the sample covariance matrix is divided into a matrix explained by common factors, and a matrix explained by unique factors. The technique assumes that the error covariance matrix is diagonal.Footnote 3 Because of the assumption that the cross-correlation of the errors is zero, the common factors cannot be represented as a linear combination of the underlying data. So in this sense, the common factors are unobservable, unlike principal components which by definition are combination of the underlying data. Another difference is that with FA the addition of an extra factor will change the factors already present; for example, a two- and three-factor model will not share the same first two factors. PCA seems like a natural choice for yield curve analysis because it is not important that the error matrix have a diagonal form.Footnote 4 Indeed, our focus is in explaining the total variation in the yield curve. The gain in using PCA is multifold: it is computationally simpler and much faster;Footnote 5 the components are observable in the sense they can be constructed as a linear combination of observed data; the explanatory power is higher because the form of the error covariance matrix is not restricted; and the addition of an extra components does not change the components that are already present. Also, unlike maximum likelihood factor analysis, its implementation does not require the assumption that the underlying variables have a joint normal distribution. This paper seeks to fill a gap in the literature by applying statistical tests to the results of principal component analysis of historical yield curve changes. In particular, we test (1) the inherent dimension of historical yield curve changes indicated by significance of eigenvalues of covariance matrix, (2) the practical dimension determined by a variance threshold, (3) the shape of yield curve changes associated with the first principal component, and (4) the persistence of this shape over time.",12
36,3,Journal of Economics and Finance,23 September 2010,https://link.springer.com/article/10.1007/s12197-010-9156-5,Oil prices and exchange rates in oil-exporting countries: evidence from TAR and M-TAR models,July 2012,Hassan Mohammadi,Mohammad R. Jahan-Parvar,,Male,Male,Unknown,Male,"The past few years has witnessed a revival of interest in the effect of high oil prices on the economies of oil-exporting-countries. Most literature on the subject falls into two broadly related categories. One is the ongoing literature on the “curse of natural resources” (Sachs and Warner 2001), which postulates that resource-rich countries tend to under-perform their resource-poor counterparts in terms of economic development and poverty reduction. The other is the literature on “Dutch disease” (Buiter and Purvis 1982; Corden and Neary 1982; Corden 1984) which is primarily concerned with potential macroeconomic effects of the curse, such as the appreciation of the real exchange rate, loss of competitiveness in export oriented sectors, decline in the share of manufacturing industry and a boom in non-tradables in afflicted economies.Footnote 1 As it appears, the appreciation of the real exchange rate serves as the necessary condition for “Dutch disease”.Footnote 2
 Beginning with oil shocks of the early 1970s, a large and growing body of studies has focused on the experience of oil-exporting countries. Early studies include Buiter and Purvis (1982), Bruno and Sachs (1982), Edwards and Aoki (1983), Neary and Van Wijnbergen (1986), and Warr (1986). More recent research includes Fardmanesh (1991), Hutchinson (1994), Brunstad and Dyrstad (1997), Bjorland (1998), Koranchelian (2005), Issa et al. (2008), Zalduendo (2006), Kalcheva and Oomes (2007), Jahan-Parvar and Mohammadi (2009, 2010) and Korhonen and Juurikkala (2009). As it appears, evidence based on traditional tests of cointegration generally supports the Dutch disease hypothesis. For example, Koranchelian (2005) finds strong support for the effect of oil prices on real exchange rates in Algeria. Zalduendo (2006) reports similar results for Venezuela. Issa et al. (2008) find that higher energy prices resulted in an appreciation of Canadian dollar since 1993 when Canada became a net energy exporter. Kalcheva and Oomes (2007) report similar results for Russia with long-run elasticity of real exchange rates with respect to real oil prices around 0.5. Korhonen and Juurikkala (2009) reach similar conclusions using a pooled mean group estimator applied to a panel data set of nine OPEC countries. More recently, Jahan-Parvar and Mohammadi (2010) examine the possibility of Dutch disease in 14 oil-exporting countries by applying the autoregressive distributed lag model of Pesaran et al. (2001). Their findings also support the existence of long-run relations between oil prices and exchange rates. However, estimates of long-run elasticities are meaningful and significant only for four countries of Angola, Indonesia, Nigeria and Russia, and range between 1.813 for Angola to 0.355 for Indonesia. These results cast a shadow of doubt on the extent that real exchange rates respond to oil price shocks.Footnote 3
 A critical assumption underlying previous tests of cointegration is symmetric adjustments of exchange rates in response to positive and negative deviations from equilibrium. However, several factors may contribute to asymmetric responses in real exchange rates to oil prices in oil-exporting countries. As Enders and Dibooglu (2001) point out, official interventions may result in asymmetric adjustments in exchange rates. Under a managed float, monetary authorities might be more willing to tolerate a currency appreciation than depreciation. Also, the mere existence of currency bands may mitigate exchange rate movements unless the level of the band is altered. Similarly, stickiness of nominal prices, especially in the downward direction, provides additional justification for asymmetric adjustments in real exchange rates. Also, as Ewing et al. (2006) point out, asymmetries through heterogeneous expectations about nominal exchange rates by risk-averse market participants, high transactions costs which inhibit or slow the adjustment process; economic and political shocks which may affect oil prices and exchange rates differently; and institutional factors such as OPEC pricing and quota mechanisms may prevent exchange rates from moving in tandem with real oil prices. Putting together, these factors provide sufficient justifications for the possibility of asymmetries in the dynamics of oil price—exchange rate relationship. In fact, a number of studies (Taylor and Taylor 2004; Taylor et al. 2001) have explored that possibility using data from industrial economies. Given asymmetric adjustments, dynamic relations implicit in tests of the Dutch disease hypothesis based on traditional methods of Engle and Granger (1987), Johansen (1988) and Pesaran et al. (2001) are mis-specified. Furthermore, traditional tests of cointegration will have low power (see for example, Pippenger and Goering 1993; Enders and Granger 1998; Enders and Siklos 2001). The proper approach is to allow for asymmetric response by applying threshold autoregressive (TAR) and momentum threshold autoregressive (MTAR) models developed by Enders and Granger (1998), Enders and Siklos (2001) and Enders (2001).Footnote 4
 This paper re-examines the existence of threshold cointegration between real oil prices and real exchange rates in a sample of 13 oil-exporting countries. We show that allowing for threshold adjustments yield results that are much less supportive of the Dutch disease hypothesis. Thus, previous empirical support may reflect a model mis-specification. The paper proceeds as follows: Section 2 introduces the empirical model and discusses the contribution of threshold autoregressive models to the analysis. Section 3 describes data sources and construction of the variables. Section 4 discusses the empirical findings; and Section 5 concludes.",43
36,4,Journal of Economics and Finance,21 April 2010,https://link.springer.com/article/10.1007/s12197-010-9131-1,"Incentive to manipulate earnings and its connection to analysts’ forecasts, trading, and corporate governance",October 2012,Deniz Igan,Marcelo Pinheiro,,,Male,Unknown,Mix,,
36,4,Journal of Economics and Finance,11 September 2010,https://link.springer.com/article/10.1007/s12197-010-9153-8,Information effects of announced stock index additions: evidence from S&P 400,October 2012,Marek Marciniak,,,Male,Unknown,Unknown,Male,"Prior studies on stock additions to S&P indices have offered different explanations of the wealth effects of firms included in an S&P index. However, they have all converged on one finding: Investors respond positively to press announcements of additions to the S&P indices.Footnote 1 What causes this consistency across the earlier research? Some researchers, including Harris and Gurel (1986) and Shleifer (1986), claim that price increases observed for the stocks added to S&P indices result purely from index-fund buying and are devoid of any information effects. Other scholars, for example, Jain (1987) and Cai (2007), contend that addition announcements, preceded by rigorous firm selection process by the S&P index committee, may contain important signals about the included firms that elicit the price effects observed at the announcement. Perhaps the general guidelines for firm selection the S&P committee employs can offer some evidence in support of the latter claim.Footnote 2 Firms eligible for inclusion in an S&P index must be financially viable,Footnote 3 must display adequate liquidity, must trade at a reasonable price,Footnote 4 and must belong to an industry that is important enough to deserve representation in a particular S&P index.Footnote 5 It can be argued that central to firm selection process undertaken by the S&P are financial viability and industry representation. As a result, the implication is that only financially strong firms belonging to industries with future growth potential are considered for inclusion in an S&P index. Consequently, the announcement of a firm’s addition to one of the S&P indices should send a positive signal about the firm and its industry. Although the same rigorous standards are applied to the added firms, the strength of the signal should not be the same across the added firms given the different information revealed about those firms at the announcement. Some information, such as current membership in another S&P index, exchange listing, and reasons for index change, are explicitly included in a press release, while others, such as firm size, are implied, as inclusion in a particular index imposes market cap constraints on firms added to that index. Investors may react differently to firms that have never been part of any S&P index as compared to those that have been members of an S&P index prior to the announcement. Likewise, the market’s response to NYSE-listed firms may vary from that of NASDAQ-traded firms. By the same token, a firm added to an index to replace a firm that is being acquired may elicit a response that differs from the one caused by a firm that takes the place of a firm that is being shifted to a larger-cap index. Further, the market may assess other factors that are not explicitly stated in the press releases at the announcement but which differentiate the firms added to a given index. Investors may respond differently to smaller and larger firms within a given range of market capitalization values. As the S&P bases its selection of firms on financial strength as well as industry representativeness, the implication is that the industry a chosen firm comes from must be assumed to have strong growth prospects. As a result, a favorable signal about the prospects of a given industry implied in the added firms may spill over into peer firms from that same industry at the announcement. If the market extends its positive reaction to the industry added firms represent, a related empirical question is whether the market will react differently to same-industry firms that are not members of a given S&P index and those that have been members of the same index that a new firm is being incorporated into. In the paper I will address the empirical questions posed above in the context of announcements of firm additions to the S&P 400 index. I have chosen the mid-cap S&P index for a number of reasons. First of all, unlike S&P 500 and, to a lesser extent, S&P 600, S&P 400 has received little interest in prior research.Footnote 6 Second, the index includes firms that are big enough to enjoy the visibility of a large-cap firm but still small enough to belong to one distinct industry, which makes it easier to measure intra-industry contagion effects. Third, S&P 400 covers over 7% of the U.S. equities market and is the most widely used index for mid-cap companies.Footnote 7 Fourth, because of its middle position between S&P 600 and S&P 500, it comprises an interesting portfolio of firms that can be small enough to approach the upper market cap bound for S&P 600 and big enough to come close to the lower market cap bound for S&P 500. Fifth, the market capitalization ranges for S&P 400 firms tend to be much narrower than for S&P 500 firms and slightly narrower than for S&P 600 firms.Footnote 8 Sixth, S&P 400 is of strategic importance to added stocks since it includes firms that have successfully gone through the initial stages of growth and can become potential candidates for inclusion in S&P 500. The study is organized as follows. The related literature on additions to S&P indices is reviewed in Section 2. Section 3 develops research hypotheses. Section 4 presents the data and methodology employed to test the hypotheses. The results are discussed in Section 5, and Section 6 concludes the paper.",3
36,4,Journal of Economics and Finance,25 September 2010,https://link.springer.com/article/10.1007/s12197-010-9157-4,Information dynamics effects from major world markets to SAARC nations,October 2012,Vivek Bhargava,Akash Dania,,Male,Male,Unknown,Male,"Following the collapse of the World Trade Organization (WTO) negotiations of July 2006, there has been considerable interest in the issue of regional economic cooperation among academicians and practitioners. Of particular interest have been regional cooperation organizations of the Asian countries, owed partially to the continued economic growth and trade liberalization in Asia, especially in countries such as India and China. On December 8, 1985, Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan and Sri Lanka formally adopted the charter for formation of the South Asian Association for Regional Cooperation (SAARC). At the time of its formation, SAARC had a total population of approximately 1.47 billion people (World Bank 2007). Constituting one-fifth of the entire world population, SAARC became the largest regional cooperation block in the world (Fig. 1).
 
a GDP for Bangladesh, India, Pakistan and Sri Lanka (in billions of dollars). Figure 1a reports Gross Domestic Product (GDP, in billions of U.S. dollars) for Bangladesh (GDP BD), India (GDP IND), Pakistan (GDP PAK) and Sri Lanka (GDP SRL). Data period is between years 1985–2008 and is from International Monetary Fund database (IMF). Year 2008 data is projected figure. b Population for Bangladesh, India, Pakistan and Sri Lanka (in billions of dollars). Figure 1b reports population (in millions) for Bangladesh (PBD), India (PIND), Pakistan (PPK) and Sri Lanka (PSRL). Data period is between years 1985–2008 and is from International Monetary Fund database (IMF). Year 2008 data is projected figure To investigate the underlying economic structure and characteristics of the SAARC region previous studies have focused on topics such as trade pattern of SAARC (Hassan 2001; Clarete et al. 2003; Mohanty and Chaturvedi 2006; Rajapakse and Arunatilake 1997); interdependence of SAARC region currency market (Khan and Zubair 2007); interdependence of macroeconomic structure of SAARC nations (Guru-Gharana 2000); and modeling welfare gains from tariff free trade in SAARC (Jayaraman 1978; Sriniwasan 1994; Blomstrom and Kokko 1997). However, an area of research that has received no attention is whether the equity markets of SAARC region exhibit integration with global equity markets. The principal benefits of regional trading alliance (RTA) are lower transaction cost and harmonization of the legal framework and standards associated within regional trade. As the intra region demand of goods and services increases, the nature of dependence of the equity markets of SAARC can be hypothesized to grow. However, if equity markets reflect primarily local business environment, rather than regional (SAARC wide) conditions, integration is less likely. An understanding of these issues is important because the fragile equity markets of the SAARC region could be vulnerable to spillovers and contagion effects from regional market. Owing to liberalization, elimination of trade barriers and alignment of legal and regulatory infrastructure in SAARC region, it is imperative to understand whether equity markets of the SAARC region are interdependent and, if these equity markets react to volatility in major foreign markets? Answers to these questions regarding equity markets in the SAARC region are important for investors, policymakers and managers in South Asia and other regions especially after the onset of the financial crises of 2008. Moreover, China, Japan, Republic of Korea, Australia and Iran have expressed interest in the membership of SAFTA. The European Economic Community (EEC) and the United Sates have already been granted an observer status to SAARC. Russia is a potential observer candidate. Owing to the presence of a rapidly and economically expanding India, SAARC is a regional block of growing importance. By means of a vector auto regression and GARCH-in-mean analysis on weekly data (between January 1999 and September 2008) of relevant market index returns we achieve several important results. First, the equity market of India exhibits strong interdependence and volatility spillover effect of major foreign markets (i.e. the markets of U.S., U.K. and China). Second, Bangladesh, Pakistan and Sri Lanka exhibit mixed results when analyzing the issue of interdependence and volatility spillover effect of these foreign markets. We also analyze whether the equity markets of Bangladesh, Pakistan and Sri Lanka exhibit interdependence to the Indian equity market (largest among the SAARC nations). We find that only the markets of Pakistan and Sri Lanka exhibit a significant and positive interdependence. Rest of this study is organized as follows: Section 2 presents relevant discussion on economic and financial integration in the SAARC region, Section 3 presents the data and econometric methodology, Section 4 presents the empirical results which are followed by concluding remarks and managerial implications in Section 5.",2
36,4,Journal of Economics and Finance,23 November 2010,https://link.springer.com/article/10.1007/s12197-010-9162-7,A model for risky cash flows and tax shields,October 2012,Howard Qi,Sheen Liu,Dean Johnson,Male,Unknown,Male,Male,"Beginning with Miller and Modigliani (1958, 1963, hereafter MM), the recognition of corporate debt tax shields has been a central component in optimal capital structure, cost of capital and capital budgeting. Even though this is a well-studied topic, there are still many challenges concerning the value of tax shields. There are active ongoing efforts in this area. For example, Liu (2009) proposes a slicing approach to resolve the tax shields issue. Fernández (2004, 2005, 2006), and Cooper and Nyborg (2006) propose different determinations of the discount rate for the annual tax shields. Leland (1994), Leland and Toft (1996), Sarkar and Goukasian (2006), and Sarkar (2008) try to provide close-form solutions that account for convex tax schedule. However, the effort to handle the loss of tax shields when earnings cannot cover debt interest liabilities has not been satisfactory thus far. In previous studies, this issue is either handled with over-simplified assumptions in order to obtain a closed-form solution, or the model derivations are seriously problematic as we explain later. On top these problems, another difficulty with the previous effort is that the previous models with closed-form solutions are technically complicated to implement. For example, they require modeling the unobservable asset value process in a continuous time fashion. In this study, we intend to develop a framework to account for the partial loss of tax shields. Our framework focuses on observable cash flows rather than the unobserved stochastic asset value process, thereby it is easier to implement technically, easier to interpret conceptually and more directly linked to corporate budgeting decisions. Thus, not only do we fill the gap by developing a theoretical framework in the study of tax shields, but we also make it friendlier to practitioners. Specifically, we extend the traditional WACC and APV framework by proposing a model for risky earnings where the uncertain earnings may result in a partial or full loss of debt tax shields. Just as MM (1963) can be thought of as an upper bound on the value of the debt tax shield, the value given here can be thought of as a lower bound by assuming away the presence of carry-forwards and carry-backs and the market for carry-forwards. Our model provides an explanation for the under-leverage puzzle, better estimates of the required equity return, and an explanation for potential risk-seeking behaviors in corporate investment when the firm experiences financial difficulties. To our best knowledge, this is the first study revealing the underlying incentive for the potential risk-seeking behavior other than the well-known asset-substitution problem driven by the threat of looming bankruptcy. Our study provides a closed-form solution for the expected effective tax shields with four empirically testable predictions. First, it predicts that a firm with safer and profitable earnings is prone to using more debt. Second, a firm with more non-debt tax deductibles would use less debt. Third, for financially distressed firm, more volatile earnings may increase the value of expected realizable tax shield. Fourth, under a normal distribution for risky earnings, the phase-transition-like behavior (in the third prediction) occurs when the expected earnings are 50% of the value of the total tax deductibles. The paper is structured as follows. In Section 2, we examine the potential loss of tax shields and construct a model to find its impact on the present value of the debt tax shields and the subsequent cost of capital equations. Sensitivity results are presented to provide insight into the material impact of the loss of tax shields. Section 3 provides empirical predictions and concludes the study.",6
36,4,Journal of Economics and Finance,09 November 2010,https://link.springer.com/article/10.1007/s12197-010-9160-9,Managerial hedging ability and firm risk,October 2012,Lee M. Dunham,,,,Unknown,Unknown,Mix,,
36,4,Journal of Economics and Finance,25 November 2010,https://link.springer.com/article/10.1007/s12197-010-9163-6,Does mandatory disclosure affect subprime lending to minority neighborhoods?,October 2012,Zsuzsa R. Huszár,George H. Lentz,Wei Yu,Female,Male,,Mix,,
36,4,Journal of Economics and Finance,19 October 2010,https://link.springer.com/article/10.1007/s12197-010-9159-2,Recent evidence on determinants of per residential customer electricity consumption in the U.S.: 2001-2005,October 2012,Richard J. Cebula,,,Male,Unknown,Unknown,Male,"The June 22, 2009 issue of National Geographic was devoted entirely to the topic “Energy for Tomorrow.” Among the numerous timely issues considered at length in this publication is an expected massive (“nearly 50 percent”) growth in the demand for energy over the next 21 years and the concurrent potential massive rise in CO2 and other greenhouse emissions that are allegedly heating our planet and putting us on a path that could raise the mean global temperature by as much as six degrees Celsius by 2030 (National Geographic Society 2009, p.7). These concerns appear consistent with those of the International Energy Agency (2008, p. 4), where world primary energy demand is predicted to rise by roughly 1.6 percent annually (on average). Also noted is the critical role of coal being used to generate electricity. For example, it is observed that daily “Some 14,000 tons of coal fuel Utah’s Hunter Power Plant, which feeds the grid in the burgeoning West” (National Geographic Society 2009, p. 9). The advantages of a “Smart Grid, which matches supply to demand, thereby elevating efficiency and diminishing the need for more power plants is also discussed, along with its projected cost for just the U.S. of $400 billion (National Geographic Society 2009, p. 30). Numerous other issues are raised, including how we can do a better job of conservation and whether we can “home-make” energy (National Geographic Society 2009, pp. 52-55). It also is observed that China opens two new coal-fired power plants weekly, with some 80 percent of China’s total electricity generating capacity being coal-fired (National Geographic Society 2009, pp. 66-67). The vaguely-answered and controversial issue of whether it is practical and possible to develop and then deploy clean coal technology also is discussed. Apparently, practical and efficient clean coal technology does not as yet exist; however, this situation is theoretically subject to change, given proposed investments in such new technology (National Geographic Society 2009, p. 28). From a narrower and thus somewhat different perspective, namely, that of the American Society of Civil Engineers (2009, p. 134), it is argued that “The U.S. generation and transmission [of electricity] is at a critical point requiring substantial commitment in new-generation investment to improve efficiencies in the existing generation [of electricity], and investment in new transmission and distribution systems.” The ASCE (2009, p. 134) has noted that electricity demand has increased about 25 percent since 1990, during which time construction of new transmission facilities decreased by roughly 30 percent. Overall, the ASCE (2009, p. 134) observes that “The transmission and distribution system has become congested because growth in electricity demand and investment in new generation facilities have not been matched by investment in new transmission facilities.” Finally, the ASCE (2009, p. 134) warns that massive investment in generation, transmission, and distribution facilities are needed in the next two decades in order to avert a trend towards increasingly serious congestion, shortages, and bottlenecks in electricity consumption in the U.S. Under the appropriate rubric of environmental economics as well as regional economics, an extensive empirical literature concerned with energy consumption has developed during the last few decades. A significant component of this literature is concerned with the consumption of electricity, including the residential consumption thereof (Taylor 1975; Halvorsen 1975, 1978; Garbacz 1983; Dubin and McFadden 1984; Bohi 1984; Dodgson et al. 1990; Branch 1993; Harris and Liu 1993; Sailor and Munoz 1997; Joskow 1997; Yan 1998; Henley and Peirson 1998; Holtedahl and Joutz 2004; Kamerschen and Porter 2004; Halstead et al. 2004; Moral-Carcedo and Vicens-Otero 2005; Palmer and Burtrow 2005; Zachariadis and Pashourtidou 2007; Horowitz 2007; Yuan et al. 2010; Yoo and Lee 2010; Chandran et al. 2010; Raty and Carlsson-Kanyama 2010). The residential consumption of electricity in the U.S. will presumably continue to rise as the population continues to increase, especially if claims of global warming are correct (ASCE 2009; Harris and Liu 1993; Energy Information Administration 2006), making it all the more important for both policymakers and energy firms to understand the factors influencing that consumption. Clearly, a sound knowledge of systematic contemporary determinants of residential electricity consumption is invaluable to policymakers and private sector decision makers in the electricity industry, given the apparent investment and infrastructure challenges facing the U.S. in coming years. Accordingly, the present study seeks to identify key economic factors and other conditions that have influenced the consumption of electricity per residential customer in the U.S. during recent years. Among other things, the study investigates the impact on this measure of residential electricity consumption of the degree to which each state has pursued energy efficiency policies. Thus, the question of whether state energy-efficiency policies work is integrated into the present analysis. Unlike most previous studies, this study uses a state-level panel data set for the period 2001 through 2005. By focusing on this time period, the evidence provided in this study is relatively current. Section 2 of this study provides the initial framework for the analysis, whereas Section 3 provides the P2SLS (panel two-stage least squares) estimates based on that initial framework. P2SLS estimations of expanded versions of the basic model are found in Section 4. Finally, Section 5 of this study summarizes the findings of the study.",5
37,1,Journal of Economics and Finance,05 October 2010,https://link.springer.com/article/10.1007/s12197-010-9158-3,Financial liberalization and firms’ capital structure adjustments evidence from Southeast Asia and South America,January 2013,Rashid Ameer,,,Male,Unknown,Unknown,Male,"Most of the emerging markets in Asia and South America had a legacy of financial markets control.Footnote 1 Not only did the governments control the banking sectors but they also intervened in the capital markets and imposed constraints such as setting the prices of initial share offerings, placing restrictions on price movements in the secondary markets, and limiting the maturity of debt. Only the largest firms were selected for stock market listing, thereby significantly affecting stock market liquidity and financing (Glen and Pinto 1995; Hasnan 2000). During the past two decades, however, the financial markets controls have either been removed or reduced due to the financial liberalization, thereby allowing financial markets of the these countries to emulate the developed financial markets. The liberalization of domestic financial sector should create new financing opportunities for a closed (repressed) economy (Schmukler and Vesperoni 2000). The financial reforms should provide equal opportunities to public and private sectors, such as access to international capital markets and new domestic sources of capital for financing decisions. So far, the dynamic nature of liberalization and its impact on the capital structure of emerging market firms has received little attention in global finance literature. Previous studies have only examined the relationship between a few banking sector and stock markets indicators and the firms’ leverage ratios (see e.g., Harris et al. 1994; Demirguc-kunt and Maksimovic 1996; Schmukler and Vesperoni 2000; Agarwal and Mohtadi 2004). There is no empirical evidence about the capital structure adjustment process of firms in the emerging markets particularly due to liberalization.Footnote 2 Most of the previous capital structure studies (see e.g., Schmukler and Vesperoni 2000; Booth et al. 2001) have used linear models (with or without fixed effects) to analyze the capital structure of firms in the developing countries. We argue that the best way of viewing the question of financial sector regulation and deregulation is not along a single dimension but as involving multidimensional adjustments. Hence, an estimation model that can explain the non-linear effects on the capital structure is required. The main contribution of our work lays in a systematic analysis of the impact of the financial liberalization and legal environment on the firm-level capital structure adjustments, which is a new contribution to the emerging markets’ capital structure literature. Although the measurement of financial liberalization policies is not straightforward and complicated due to their reversible nature but our findings tell the story that financial liberalization reforms carried out by respective governments in the emerging markets deserve credit. The sequencing of financial reforms either in the banking sector or stock markets does matter in creating a desirable environment that impinges upon the financing decisions of the firms. We believe that impact of liberalization in capital structure adjustment process has not been properly explored and therefore, this paper fills an important gap in the capital structure literature.",11
37,1,Journal of Economics and Finance,05 January 2011,https://link.springer.com/article/10.1007/s12197-010-9169-0,"Information markets, product markets and vertical merger",January 2013,Jihui Chen,Qihong Liu,,Unknown,Unknown,Unknown,Unknown,,
37,1,Journal of Economics and Finance,11 January 2011,https://link.springer.com/article/10.1007/s12197-010-9170-7,Uncertainty and risk premium puzzle,January 2013,Heeho Kim,,,Unknown,Unknown,Unknown,Unknown,,
37,1,Journal of Economics and Finance,13 January 2011,https://link.springer.com/article/10.1007/s12197-010-9167-2,Informational externalities of initial public offerings: Does venture capital backing matter?,January 2013,Carmen Cotei,Joseph Farhat,,Female,Male,Unknown,Mix,,
37,1,Journal of Economics and Finance,01 February 2011,https://link.springer.com/article/10.1007/s12197-011-9171-1,Crack spread option pricing with copulas,January 2013,Hemantha S. B. Herath,Pranesh Kumar,Amin H. Amershi,Unknown,Unknown,Male,Male,"Spread options are options on the price difference between two or more traded assets. “Crack” is a term used in refining which refers to the “cracking” of crude oil into other products (for example heating oil or gasoline). A crack spread (also known as a refinery spread) is the simultaneous purchase or sale of crude against the purchase or sale of refined petroleum products. Crack spreads were introduced in October of 1994 on the New York Mercantile Exchange (NYMEX) and are used for managing risk in the oil industry. Crack spread optionsFootnote 1 are puts and calls on the one-to-one ratio between the New York Harbor heating oil futures or New York Harbor unleaded gasoline futures and the Exchange’s light crude oil futures contract. Our article considers the application of copula models to the problem of pricing heating oil–crude oil crack spread options. A heating oil–crude oil crack spread call option struck at a strike price K at maturity is given by \( {\left( {42 \times \left[ {\hbox{HO}} \right] - \left[ {\hbox{CO}} \right] - K} \right)^{ + }} = \max \left\{ {42 \times \left[ {\hbox{HO}} \right] - \left[ {\hbox{CO}} \right] - K,0} \right\} {\left( {42 \times \left[ {\hbox{HO}} \right] - \left[ {\hbox{CO}} \right] - K} \right)^{ + }} = \max \left\{ {42 \times \left[ {\hbox{HO}} \right] - \left[ {\hbox{CO}} \right] - K,0} \right\} \). The underlying spread or refining margin and the option price is expressed in dollars and cents per barrel. Therefore, the (42 gallons = 1 barrel) conversion rate for HO is used as heating oil is priced according to $ per gallon of product. Crack spread options are widely used as a risk management tool by both refiners and marketers of heating oil. Crack spread options allow refiners to hedge their operating margins at a known up-front cost while simultaneously allowing them to participate in any future widening of refining margins. Puts offer refiners an instrument for locking in crude oil costs. In addition, crack spread options allow refiners to generate income by writing options. Calls, on the other hand, offer marketers protection during unstable spread increases. Crack spread options are traded between 9:00 am to 2:30 pm through open outcry trading at NYMEX. That is, oral “bids” (i.e., requests to buy) and “offers” (i.e., desire to sell) are made in the pits as arms-length transactions between buyers and sellers. In trades, the “High” and “Low” refers to the highest bid price paid and the lowest offer accepted for the crack spread option during the day or life of the contract. The settlement prices, on the other hand, are the prices given by the exchange at the end of the open outcry period. In regard to crack spread options, the settlement prices are derived from available market information including, but not limited to, outright trades, bids, or offers during the close, relevant spread trades, bids, or offers during the close, the settlement price of the underlying future, and the relevant relationships based on option pricing theory using option pricing models employed by the exchange (CBOT Handbook, Rule 813). They are the synchronous prices of the derivative and its underlying asset, and are determined by a settlement committee with input from floor traders.Footnote 2 The settlement price, also referred to as the “actual price” (see Laurence and Wang 2009), best reflects the true market valuation at the time of close. Models that have been proposed for pricing spread options have mainly focused on European style-options. Commonly referred to as standard models are the bivariate binomial model (lattice approach) and the approximate analytical formula (Kirk and Aron (1995) or “Kirk” model). These crack spread option models suffer from three major limitations: (1) the assumption of a normal distribution for underlying asset returns; (2) the use of correlation to model dependence, and (3) the non-consideration of the unique features of crude and heating oil prices, such as seasonality and mean reversion. Dependence between prices is critical in many aspects of multi-asset derivative pricing and modeling price behavior in energy markets. Copula models that allow capturing dependencies have become popular in finance over the last decade. Copulas provide a flexible method for capturing the critical features of financial data such as asymmetry, non-linear dependence, and/or heavy tail behavior (Grégoire et al. 2008). Our research is motivated by the need to address these weak assumptions and an attempt to model the unique features of commodity prices that may affect the valuation of crack spread options. The idea behind the concept of a copula is to split a multivariate distribution function into two parts that describe the dependence structure and marginal behavior. More formally, copula models allow the unknown joint distribution of two or more random variables to be modeled as a combination of their marginal distributions and a copula function to capture the dependence, i.e., let X and Y be continuous random variables with distribution functions \( F(x) = \Pr \left( {X \leqslant x} \right) F(x) = \Pr \left( {X \leqslant x} \right) \) and \( G(y) = \Pr \left( {Y \leqslant y} \right) G(y) = \Pr \left( {Y \leqslant y} \right) \), respectively. Following Sklar (1959), there exists a unique function C, such that: \( \Pr \left( {X \leqslant x,Y \leqslant y} \right) = C\left( {F(x),G(y)} \right) \Pr \left( {X \leqslant x,Y \leqslant y} \right) = C\left( {F(x),G(y)} \right) \) where the copula function \( C\left( {u,v} \right) = \Pr \left( {U \leqslant u,V \leqslant v} \right) C\left( {u,v} \right) = \Pr \left( {U \leqslant u,V \leqslant v} \right) \) is the distribution of the pair \( \left( {U,V} \right) = C\left( {F(X),G(Y)} \right) \left( {U,V} \right) = C\left( {F(X),G(Y)} \right) \) whose margins are uniform on [0, 1] and that characterizes the dependence in the pair (X, Y). When the joint distribution of (X, Y) is unknown, it can be modeled by assuming specific parametric forms for F, G, and C (Grégoire et al. 2008). The objective of this article is to examine the choices of copula C appropriate for pricing crack spread options, determine which copula function provides the best model in terms of goodness of fit, and to compare the copula crack spread option models with the standard models for pricing these options. Several articles have proposed copulas to study the relationship between prices for futures on crude oil and natural gas (Grégoire et al. 2008) and the evolution of electricity and natural gas prices for pricing “spark” spread options (Benth and Kettler 2006). We contribute to the literature by using copulas to study the dependence relationship between futures on crude oil and heating oil, and pricing a heating oil–crude oil crack spread option. More specifically, this article develops a copula-based Monte Carlo simulation model to price crack spread options and compare the copula models with settlement prices and the currently used standard models. To the best of our knowledge, there has been as yet no application of copula methodology to pricing of crack spread options. Our approach does not require stringent normality assumptions (any marginal distribution can be used) and would be of interest to both academia and practice. The paper is organized as follows. Section 2 provides the motivation and a literature review. We demonstrate empirical evidence that is at odds with the assumptions of normal distribution of returns and linear dependence. The literature review discusses limitations of correlation as a dependence measure and articles that have examined copula models for pricing energy derivatives. In Section 3 we present a copula-based Monte Carlo simulation approach for pricing a crack spread option. Numerical results are presented in Section 4. In Section 5, we discuss the managerial use of the proposed models. Section 6 concludes with a discussion on model limitations. Appendix A provides a brief survey of the copula families used in this article, goodness of fit tests for selecting a copula and simulation algorithms. Appendix B summarizes the standard models.",5
37,1,Journal of Economics and Finance,05 February 2011,https://link.springer.com/article/10.1007/s12197-011-9172-0,The productivity and performance of Australia’s major banks since deregulation,January 2013,Malcolm Abbott,Su Wu,Wei Chun Wang,Male,,,Mix,,
37,1,Journal of Economics and Finance,17 March 2011,https://link.springer.com/article/10.1007/s12197-011-9173-z,The Fed’s TRAP,January 2013,Alexander Erler,Christian Drescher,Damir Križanac,Male,Male,Male,Male,"During the Great Moderation the US, like most developed economies, has been subject to modest consumer price inflation. While this development conforms with one of the Fed’s goals, as these focus on consumer price inflation, there have also been considerable increases in asset prices until the set in of financial crisis in 2007. From 1985q1 to 2007q1 consumer prices rose quarterly on average 0.76%, while corporate equity and real estate prices increased 2.74 and 1.29%, respectively.Footnote 1 Among the often discussed reasons for the spreads between growth rates of consumer and asset prices are different price elasticities (see Belke et al. 2008) and the “paradox of credibility”Footnote 2 (see Borio et al. 2003). Since asset prices are claims on future goods and services, it should come as no surprise that former Federal Reserve Chairman Alan Greenspan already asked in 1996:
 But where do we draw the line on what prices matter? Certainly prices of goods and services now being produced—our basic measure of inflation—matter. But what about futures prices or more importantly prices of claims on future goods and services, like equities, real estate, or other earning assets?  Economic literature on these questions is still twofold. The traditional view claims that asset prices should only be taken into account to the extent that these influence consumer price inflation (“benign neglect” strategy), whereas the new view claims that asset price booms should be dampened to prevent high-cost busts (“lean against the wind” strategy). Since there is no common sense about the optimal response to asset price booms, we should first take a closer look at the main problems regarding this question.
 Problem of identification: Bernanke and Gertler (1999, 2001) argue that it is not possible to identify asset price booms in real-time with sufficient certainty. Roubini (2006) claims that even if their identification come with some degree of uncertainty, these information should in general not be ignored. Problem of information: Kohn (2008) argues that responses to asset price booms require central banks to have an information advantage against other market participants. Cecchetti (2005) holds against that central banks have different incentives and different measures to act in case of a similar assessment of the underlying asset price. Problem of destabilization: Posen (2006) claims that pricking asset price booms can trigger market panics, whereas Borio (2005) points out that ‘leaning against the wind’ should take place in the early stage of asset price booms to avoid unpredictable market behavior. Problem of focus: Bernanke and Gertler (1999) argue that the mere focus on price stability reduces the likelihood of financial crises. But Borio and Lowe (2002) state that the sole focus of monetary policy on consumer price stability does not necessarily lead to a stable financial system since asset price booms indicate excess liquidity even though other indicators neglect any indications. Problem of transmission: Interest rate increases do not necessarily dampen the formation of asset price booms (see Kohn 2008), but it has to be considered that changes in interest rates have a stronger effect on investors as expected due to the ‘risk-taking’-channel of monetary transmissions, which affects the willingness to take risk (see Gambacorta 2009). Problem of economic costs: While the traditional view points out that pricking an asset price boom causes collateral damage for the real economy and not directly affect asset markets, the new view annotates that a mild collateral damage of an early intervention should be seen as an insurance premium against an even worse damage in the wake of a later asset price bust (see Bordo and Jeanne 2002). The debate whether to respond to asset price booms (or not) is not conclusively discussed in literature yet and is still open to further discourse. To date the Fed officially follows the traditional view not to respond to asset price booms. We examine if US monetary policy at least implicitly responds to asset price booms. Using real-time data and a GMM framework we estimate a Taylor-type rule as shown in Clarida et al. (1998) and Orphanides (2001). In our article we use a modified Taylor rule to investigate the monetary policy of the Fed.Footnote 3 Empirical studies show, that the interest rate setting behavior of the Fed—except from the period prior to financial crisis—can certainly be explained by the Taylor rule (see Taylor 2007; Poole 2007). Despite all this however, the rule is unable to provide accurate and satisfying explanations for this period of controversy. The aim of this article is therefore to search for potential alternatives to adequately interpret the behavior of the Fed. To take account of asset price movements we extend a Taylor-type rule by a dummy variable that captures asset price booms.Footnote 4 This dummy variable refers to real estate prices which take up an important share in households’ asset portfolio. Moreover, real estate prices seem to have a close connection to monetary conditions (see Deutsche Bundesbank 2007, p. 19). By using deflated asset prices we attempt to extract shifts in relative prices with respect to consumer prices.Footnote 5
 The article is organized as follows. Section 2 describes the asset cycle dating procedure that we use to identify phases of asset price booms. The empirical framework which consists of a Taylor-type Rule with Asset Prices (TRAP) is given in Section 3. The results of our estimations are discussed in Section 4. Our main findings are summarized in Section 5.",1
37,1,Journal of Economics and Finance,28 October 2011,https://link.springer.com/article/10.1007/s12197-011-9213-8,A note on the evaluation of long-run investment decisions using the sharpe ratio,January 2013,Ken Johnston,John Hatem,Elton Scott,Male,Male,Male,Male,"The Sharpe ratio measures excess return per unit of risk where risk is measured by the standard deviation of the excess returns (Sharpe 1994). The ratio itself is a simple calculation but as pointed out by Lo (2002) the accuracy of the ratio is dependent on the time series properties of the return series. Mean reversion, serial correlation, and aggregation methodology all have an effect on the calculation. Bao and Ullah (2006) examine the bias that results from autocorrelation in the return series, while Bao (2009) relaxes the normality assumption. The above cited works suggest that the Sharpe ratio should be adjusted for the estimation error in order to make correct inferences about asset return series. To further highlight this point, the work of Best et al. (2007) (hereafter referred to as BHY (2007)) is reexamined. It is shown that absence the presence of serial correlation, other properties of the return series, such as higher order moments, are an additional source of estimation error that must be accounted for before inferences can be made with the Sharpe ratio.",2
37,2,Journal of Economics and Finance,05 April 2011,https://link.springer.com/article/10.1007/s12197-011-9175-x,An examination of ex-ante factors and their influence on equity carve-out long-term performance,April 2013,Thomas Hall Thompson,,,Male,Unknown,Unknown,Male,"Although there have been other long-term carve-out and IPO studies, this is the first study to examine the impact of partial price adjustment and other ex-ante variables on the long run returns of equity carve-outs. Also, we report long-term returns by type of second divestiture event. In addition, we observe, contrary to Vijh (1999), that three year carve-out returns are negative and statistically significant. In addition, we observe that our independent variables explain 14.56% of the multiple regression three-year returns for carve-outs. Moreover, our negative correlation of three-year returns with initial period returns supports the “leaning against the wind” hypothesis of Loughran and Ritter (2002). In addition, our results for the post-bubble period (2001–2006) provide an extension of the changing issuer objective function noted by Loughran and Ritter (2004) for IPOs and Hogan and Olson (2004) for equity carve-outs. We examine combination carve-out market-adjusted returns over a three year period and extend the Klein et al. (1991) carve-out announcement study that observes that carve-outs are temporary corporate revisions. Similar to Vijh (1999) and Maxwell and Rao (2003), we use three-year returns to capture long-term results. As the initial leg of a two-stage process and with approximately 20% of the subsidiary’s value offered, carve-outs such as Lyondell ($1.2 billion in 1988), Allstate ($2.12 billion in 1993), Conoco ($4.4 billion in 1998), and Kraft ($8.68 billion in 2001) were the largest public offerings of their time. The mean IPO offering from 1988 to 2006 is $68.5 million (Ritter, 2007). For our sample, carve-out mean offering sizes are $292.15 million, over four times the mean IPO offering. The large carve-out offering sizes imply low information asymmetry and lower underpricing than for regular IPOs (Vijh 1999; and Carter et al. 1998).",2
37,2,Journal of Economics and Finance,05 April 2011,https://link.springer.com/article/10.1007/s12197-011-9174-y,Economic freedom and the mispricing of single-state municipal bond closed-end funds,April 2013,Samuel Kyle Jones,Michael D. Stroup,,Male,Male,Unknown,Male,"The existence of premiums and discounts on closed-end funds, where the market price of the fund’s stock differs from the value of the fund’s underlying investment portfolio as measured by its net asset value, is a well known issue in the financial literature. Various economic and behavioral explanations have been put forth and tested. One explanation in particular centers on market segmentation. Research has shown that the mispricing of single country closed-end funds can be explained by the degree to which the country where the fund’s underlying assets trade is segmented from the U.S. market where the country fund shares trade. This market segmentation can arise from explicit sources such as investment restrictions and from implicit sources such as informational and liquidity asymmetries. Another type of regionally-specific closed-end fund is the single state municipal bond fund. These funds invest solely in municipal bonds issued by a single state, while the shares of the fund trade nationally. Since market segmentation has been shown to be important in explaining country fund mispricing it is relevant to ask if local market segmentation might offer a possible explanation for the mispricing of these single state municipal bond funds. To measure market segmentation at the state level we use the Fraser Institute’s Economic Freedom of North America Index (EFI), which measures the economic freedom of each state as a function of government size relative to state GDP, state taxation burden, and labor market regulations (Karabegovic and McMahon 2008). The Fraser Institutes notes that states with high levels of economic freedom are “those that tend to have lower taxes, smaller government, and flexible labor markets. These conditions create jobs and opportunities leading to economic growth…and prosperity”.Footnote 1 The literature on economic freedom suggests that states can be considered institutionally efficient given the extent to which they rely on personal choice and free market forces to allocate productive resources. Empirical analysis by Karabegovic and McMahon has shown that higher EFI is positively correlated with economic development and prosperity. As such, EFI might be seen as an implicit indicator of relative market segmentation. Those states that rely more heavily upon market institutions to determine resource allocations tend to convey more accurate, consistent and reliable market information to both consumers and producers, as well as generate a greater number of entrepreneurial opportunities for production and exchange, which allows these states to prosper more than those states that place more of a burden of resource allocation on the shoulders of government. We use the EFI measure as a proxy for state level market segmentation and show it to be significant in explaining the mispricing of municipal closed-end funds. Specifically, a fixed-effects panel regression of 62 municipal bond funds shows fund mispricing diminishes with higher EFI, which we infer to mean less market segmentation between the state where the fund’s underlying assets are valued and the national market where the fund’s shares trade. Further, this relationship is shown to be non-linear, having a diminishing marginal effect as economic freedom increases. Thus, our research adds to the body of evidence that suggests, similar to the case for closed-end country funds, market segmentation is a driver of mispricing in single state municipal bond closed-end funds. However, our paper is unique in that it suggests that institutional factors are the driver of market segmentation and the resulting mispricing of these funds. In the following section we review relevant literature on closed-end fund mispricing and economic freedom. In the third section we describe the data and model. The results of the model are presented in section four and in section five we provide conclusions to our analysis.",2
37,2,Journal of Economics and Finance,12 April 2011,https://link.springer.com/article/10.1007/s12197-011-9177-8,The effects of cash holdings on corporate performance during a credit crunch: evidence from the sub-prime mortgage crisis,April 2013,Frederick Adjei,,,Male,Unknown,Unknown,Male,"The subprime mortgage crisis is an ongoing financial crisis initiated by the bursting of the housing bubble leading to a sharp rise in mortgage delinquencies in the United States. Specifically, the underestimation of risk in the sub-prime mortgage market resulted in significant losses in the housing market. Banks and other financial institutions incurred most of these losses resulting in limited credit availability for non-financial firms. The effect of the crisis was first felt in the banking sector in August 2007 with a spillover to non-financial firms in early 2008. The historic scale of the ongoing crisis stresses the significance of comprehending the effects of shocks to credit availability on the real economy. In this study, we provide evidence on this topic by examining the role of non-financial firms’ pre-crisis financial characteristics in moderating the effect of the crisis on their performance. This study is important because we provide evidence on a non-trivial but largely unexplored issue of the impact of a negative shock to the supply of external finance on non-financial firms’ performance. The hypotheses we postulate are based on established theories on corporate performance with financing frictions. To test these hypotheses, we compare the performance of firms before and after the commencement of the crisis as a function of the firms’ pre-crisis cash position, leverage, corporate governance, and financing constraints, controlling for investment opportunities, firm fixed effects and industry effects. Given that variation in firm financial characteristics could be endogenous to unobserved investment opportunities, inferences from our results could be misleading if this endogeneity is not controlled or mitigated. Hence, to control for endogeneity, we use only firm characteristics measured 1 year before the commencement of the crisis in our regressions. Controlling for the endogeneity described above, we find that corporate performance significantly declines following the onset of the crisis. The performance decline is more pronounced in firms that have low cash reserves or are financially constrained.",5
37,2,Journal of Economics and Finance,12 April 2011,https://link.springer.com/article/10.1007/s12197-011-9179-6,Economic policy and the presidential election cycle in stock returns,April 2013,Ray R. Sturm,,,,Unknown,Unknown,Mix,,
37,2,Journal of Economics and Finance,15 April 2011,https://link.springer.com/article/10.1007/s12197-011-9178-7,Stochastic volatility model under a discrete mixture-of-normal specification,April 2013,Dinghai Xu,John Knight,,Unknown,Male,Unknown,Male,"Beginning with seminal works by Mandelbrot (1963) and Fama (1965), substantial empirical evidence indicates that most time series of returns on financial assets are not normally distributed, but instead exhibit significant leptokurtosis (fat tails relative to the normal distribution) and, in many cases, skewness. More recently, empirical research has indicated that financial time series also display some properties or stylized facts such as time-varying volatility and volatility clustering. These findings have sparked considerable interest in searching for alternative non-normal model specifications to capture these empirical characteristics. A benchmark model was developed by Engle (1982), called the Autoregressive Conditional Heteroscedasticity (ARCH) model. In a standard ARCH model, the conditional variance is a linear function of past squared errors. Bollerslev (1986) proposed a Generalized ARCH (GARCH) specification allowing the conditional variance to be a linear function of both the past squared errors and past conditional variances. Alternatively, Stochastic Volatility (SV) models by Taylor (1986) provide another specification of dynamic properties. In the SV framework, the conditional variances are specified to follow some latent stochastic process themselves. Therefore, two innovations give the time-varying characteristics in the SV specifications, while one error process is specified in the GARCH families. There is a vast existing literature discussing statistical properties of both model specifications, such as Kim et al. (1998), Bai et al. (2003), Carnero et al. (2004), etc. In this paper, we will not attempt to conduct a general survey,Footnote 1 instead, we will provide relevant references relating to our work. The SV model has an intuitive appeal and realistic modeling specification, but empirical applications have been limited due to the intractability of its likelihood function. More specifically, since the volatility is modeled as a latent variable, the objective likelihood function involves a series of integrals with the dimension of the sample size. As is well-known, it is extremely difficult to solve the integral in any analytical form and consequently, see e.g. Broto and Ruiz (2004), alternative estimation methods have been devised and used for estimating the SV models. Unlike the likelihood-based methods, those based on moments are relatively easy to implement because they avoid the high dimensional integration. Taylor (1986) and Melino and Turnbull (1990) proposed using the (Generalized) Method of Moments (GMM), matching a finite number of selected (weighted) theoretical moments with the corresponding sample moments. Andersen and Sorensen (1996) employed a modified weighting matrix to improve the efficiency of the GMM procedure. Duffie and Singleton (1993) introduced Simulated Method of Moments (SMM) by selecting theoretical moments based on a simulated process. Jiang et al. (2005) provided general analytical expressions for both conditional and unconditional theoretical cross moments, which allow us to examine the SV model’s statistical properties more easily. However, these GMM type methods suffer from efficiency and convergence problems, see Broto and Ruiz (2004). A Quasi Maximum Likelihood (QML) approach on a linearized transformed model was suggested by Harvey et al. (1994). Instead of concerning two error product processes in the original SV specification, they transform the model to a linearized state-space structure. They then apply standard numerical optimization techniques to the linearized model, treating the non-normal transformed error as if it were normally distributed. Knight et al. (2002) considered an estimation using the Empirical Characteristic Function (ECF) approach. There are also other alternative estimation procedures, such as Markov Chain Monte Carlo (MCMC) by Jacquier et al. (1994), Simulated Maximum Likelihood (SML) by Danielsson and Richard (1993), Efficient Method of Moments (EMM) by Gallant and Tauchen (1996) etc. A recent survey paper, Broto and Ruiz (2004), collected various estimation methods for the SV models and compared them in several classifications. In this paper, we retain the convenient linearized model originally from Harvey et al. (1994). We extend the SV model under a discrete bivariate mixtures of normal (MN) specification and use a continuous empirical characteristic function (CECF) based method for the estimation. There are several characteristics of our model which we now note. First, retaining the linear transformation reduces some complexities from the original nonlinear product process. As Harvey et al. (1994) and Knight et al. (2002) mentioned, the transformed data loses no information except the sign information on the original series. Second, our proposed specification which models the log-transformed error with MN is more flexible for capturing various tail behavior of error distributions. As is well known, any continuous distribution can be approximated arbitrarily well by an appropriate finite MN. Third, our proposed model easily accommodates the possible correlations between the two random processes. Consequently, the proposed model provides a better description for the empirical data than other competing alternatives. Furthermore, the evolution (dynamics) of the absolute returns can be well captured under our model framework. The paper is organized as follows. Section 2 details the model specification and presents its statistical properties. Section 3 discusses an estimation method based the CECF. Section 4 considers empirical applications of the model and the estimation procedure. We further apply the proposed model in examining the dynamics of the absolute returns and the associated Taylor effect and Machina effect.Footnote 2 Section 5 concludes the paper. All the proofs are collected in the Appendix.",1
37,2,Journal of Economics and Finance,19 April 2011,https://link.springer.com/article/10.1007/s12197-011-9180-0,Expectancy balance model for cash flow,April 2013,Marcos A. S. Melo,Feruccio Bilich,,Male,Unknown,Unknown,Male,"Analysis of previous studies identified 13 cash models that can be classified according to their mathematical structure. The objective of analyzing the models is to identify their ability to represent the aspects of cash management. As models incorporate additional aspects of cash management, they tend to improve their accuracy of obtaining the Total Cost minimization. The models can be grouped according to their mathematical structure:Inventory Theory (IT)
 Baumol (1952) Tobin (1956) Miller and Orr (1966) Constantinides and Richard (1978) Linear Programming (LP)
 Robichek et al. (1965) Eppen and Fama (1968) Stone (1972) Dynamic Programming (DP)
 Girgis (1968) Eppen and Fama (1969) Daellenbach (1971) Hausman and Sanchez-Bell (1975) Milbourne (1983) Vickson (1985) Analysis of additional articles contributed to form the basis of EBM with respect to different approaches of dealing with demand for cash and suggestions of optimization algorithms. The articles are: Miller and Orr (1968), Frost (1970), Daellenbach (1974), Eppen and Fama (1971), and Neave (1970). Models based on Inventory Theory (IT) assume unchanging time. The cost values and rates, as well as the forecasted amount of money demanded are considered constant. The assumptions for the short run cash flow model do not hold for the long run reality (Milbourne et al. 1983). The models equations have to be recalculated for each fluctuation of variable values. Three of the four IT reviewed models consider the flows of cash as discrete values. The application of the model assumes that the money converted to or from other forms of assets along the period has to be taken in predetermined lots. The optimizing model proposed by Miller and Orr (1966) creates bounds where cash balances may fluctuate. If the money held in cash exceeds the upper bound, the manager has to withdraw certain amount of cash to reduce the balance level. If the balance value drops to less than the lower bound, the manager has to provide the amount necessary to bring the balance back to the allowed band. The procedure that creates fluctuation bands is called Simple Policy. The optimal cash balance is the amount that minimizes the combined Holding and Shortage Costs. There is only one exact value of cash balance that indicates the decision to be made to minimize Total Cost. The application of Simple Policy models does not produce the minimum Total Cost on the long run because of losses accumulated due to accuracy failure. Linear Programming (LP) models also use the cash flows as discrete values. The exception is Stone (1972). The problem with using discrete values is the same as described in the Simple Policy. The models that work with lots of cash instead of treating the flows as continuous values, hinder the firm in the achievement of the objective of finding the optimal cash balance. In spite of the accuracy failure, the LP models brought two contributions. The first one is the possibility of the cost variables to be adjusted according to the business activities. The other contribution is the strengthening of the adoption of proportional transfer costs, first used by Tobin (1956), who considered the money transfer cost as a function of the amount transferred. The recursive equations of Dynamic Programming (DP) models were built upon enhancements made in LP. DP models also inherited some deficiencies of LP models besides the mechanisms created to represent cash management, like the adoption of Simple Policy to control cash balance levels (Girgis 1968; Smith 1989; Baccarin 2002). The major contribution of DP models is the flexibility to assimilate the variation of the costs and cash flows in time, as used by Takahashi and Alexander (2002) to represent illiquid alternative funds. Due to the principle of optimality, the DP structured models can reflect the fluctuations of variables without having to reconstruct the equation each time they vary. Another mathematical structure that can be developed in further studies is the real options approach (Cossin and Hricko 2004). The initial enhancement may be considering a set of investments instead of a particular one. The conclusion inferred from analysis of the models is that the authors ought to pursue the representation of aspects of the economy and cash management as defended by Romer (1986). The cash flows have to be considered as continuous and stochastic values and the models variables (costs and cash flows) have to admit fluctuations over time. Cash flow models have to be built with a stochastic-dynamic mathematical structure to heed all requirements to represent real context of cash management, and to pursue the exact value for the decision variable. Models built to explain the demand for money have to incorporate the benefits and costs of renouncing other forms of assets. The objective of EBM is to indicate the optimal cash balance level for each instant in the future such as day periods, in order to reduce the Total Cost of retaining money according to the costs of renouncing other forms of assets. The demand for money is not a function only of its liquidity, interest rate and inflation, but also of what it means for economic agents not to retain other forms of assets. Section 3 presents the Expectancy Balance Model (EBM), developed respecting the characteristics of everyday cash management. Initially, the cash management elements that are represented by the EBM variables are described. Next, these elements are grouped forming the EBM block.",11
37,2,Journal of Economics and Finance,30 April 2011,https://link.springer.com/article/10.1007/s12197-011-9182-y,Daily momentum profits with firm characteristics and investors’ optimism in the Taiwan market,April 2013,Chiao-Yi Chang,,,Unknown,Unknown,Unknown,Unknown,,
37,2,Journal of Economics and Finance,03 May 2011,https://link.springer.com/article/10.1007/s12197-011-9183-x,Cross-listing in the home market after going public in the U.S.,April 2013,Yaseen S. Alhaj-Yaseen,,,Male,Unknown,Unknown,Male,"For over 80 years, firms have been using international listing of their stocks as a successful alternative to raise capital. The earliest cross-listing took place in the U.S., specifically on the NYSE; on December 20, 1928, an iron-ore producer was cross-listed. Raising capital is not the only benefit firms can gain from cross-listing abroad. The literature reports several other benefits: increasing the firm’s ability to improve its affiliation with the host market participants and reducing the cost of capital when exposed to different market risks and become more diversified (Karolyi 1998). These benefits and others have motivated more firms to cross-list their stocks from less to more developed markets or the other way around, though the latter trend is not as significant. This international cross-border listing trend has led some firms to bypass their home market and list their IPOs in foreign stock exchanges to distinguish them from other locally listed stocks and to derive the benefits of being listed in major exchanges abroad. These firms are well-established and large in their home countries, and they maintain these characteristics in the host market. Being listed in major foreign stock exchanges, especially on the U.S., puts these foreign IPOs among the top firms globally since they met all U.S. stock exchanges’ listing requirements, including the high disclosure and regulatory standards (Bruner 2004; Valero 2009). Some of these foreign IPOs, after going public abroad, pursue further cross-listing outside the initial host market to achieve growth internationally.Footnote 1 On the other hand, some firms cross-list their foreign IPOs in their own home market stock exchanges after going public abroad. In this study I analyze a sample of 34 Israeli stocks that went public in the U.S. and then cross-listed in their home market, Tel Aviv Stock Exchange (TASE). Further cross-listing in other markets after going public abroad can be interpreted as a trial to grow and expand the firm’s operations in other markets. However, cross-listing in the home market after going public abroad does not offer any valuable information, since these firms are already established on the home market in which they are cross-listing their stocks. The market segmentation theory suggests that if two markets are integrated then assets with the same risk should receive the same risk premium; however, if these markets are segmented, the same assets will receive different risk premiums in each market (Bekaert 1995a, b). In the context of international cross-listing, due to the existence of investment barriersFootnote 2 between two markets, investors with foreign assets on their portfolios are likely to ask for a positive risk premium, represented in higher expected returns, to compensate for the higher risk that arises from market segmentation. The elimination of any pre-existing investment barriers between the two capital markets is likely to cause the expected returns to decrease and, hence, the equilibrium prices of cross-listed stocks to be different after cross-listing (Errunza and Losq 1985). In general, a large body of literature shows that cross-listing is associated with a positive pre-listing and negative post-listing abnormal returns, which is the expected outcome of cross-listing when two markets are segmented and investment barriers exist between them (Karolyi 2006).Footnote 3
 For the Israeli firms in this study, investment barriers between the U.S. and Israeli markets are expected to be eliminated for these firms when they gone public in the U.S. and have full access to the U.S. capital market. After going public in the U.S., local U.S. traders can trade these Israeli stocks and access their information at no additional cost, without being exposed to the foreign exchange risk since they are traded in U.S. dollars. Therefore, one would expect that the positive risk premium, suggested by the market segmentation theory, would not exist for the Israeli stocks and that there will be no difference in stocks’ equilibrium prices when they cross-list in TASE. My findings in this study suggest that Israeli stock prices react to the event of cross-listing in TASE as if the investment barriers exist between the U.S. and Israeli markets. That is, going public abroad might suggest that the market segmentation between the host and the home market are eliminated. However, not having the security available to trade in the home market implies that home market risk is not completely reflected in the security’s price. Therefore, cross-listing in the home market would attach the home market risk in the security’s price and expose it to the home and host market risks, which will increase market integration between the two markets, as the findings of this study suggest. Further investigation of risk exposure due to cross-listing supports the claims of this study and indicates lower risk exposure after cross-listing due to diversification effect, which explains the pattern of pre- and post-listing abnormal returns. While the majority of studies in international cross-listing focus on the impact of cross-listing on the price of the underlying shares in the home market, this study focuses on changes in the prices of Israeli stocks listed in the U.S. using their U.S.-based returns to study their behavior around the event of cross-listing in the home market (TASE). This research analyzes the valuation effect of cross-listing in TASE on the prices of Israeli stocks traded on U.S. exchanges. In addition, it examines changes in five different measurements of risk associated with cross-listing.",3
37,2,Journal of Economics and Finance,26 May 2011,https://link.springer.com/article/10.1007/s12197-011-9184-9,"Volatility, trade size, and order imbalance in China and Japan exchange traded funds",April 2013,Valeria Martinez,Yiuman Tse,Jullavut Kittiakarasakun,Female,Unknown,Unknown,Female,"Exchange-traded funds (ETFs) are among the most popular and fastest-growing class of financial assets available in today’s markets. ETFs are diversified portfolios of securities that combine positive attributes of closed-end and open-end mutual funds. Similar to closed-end mutual funds, ETFs are traded throughout the day in organized exchanges. But ETFs allow for the creation and redemption of securities like open-end mutual funds.Footnote 1 Moreover, ETFs generally have lower expense fees and are more tax efficient than mutual funds. As of November 2008, there were 843 ETFs and ETNs (exchange traded notes) in the U.S., all of which held approximately $487 billion in assets. Although it seems like a small dollar amount compared to the $10 trillion mutual fund industry, ETF assets are increasing at remarkable speed.Footnote 2
 Managed by Barclays Global Investors, international iShares ETFs include exchange traded funds of assets in many countries. They were created in 1996 under the name of World Equity Benchmark Shares (WEBS). International ETFs have grown in popularity as they allow investors to diversify their portfolios using diversified baskets of international assets that are traded in U.S. markets and denominated in U.S. dollars. Investors who are interested in purchasing stocks of international companies can purchase ETFs without worrying about trading in local currencies through overnight markets and holding undiversified positions in international stocks. Recently, there have been reports of excessive price volatility and value differences between market prices at which ETFs are traded and values of their underlying assets (also known as net asset value, NAV). Lauricella and Gullapalli (2007) report that ETFs do not track the underlying assets they represent as closely as we think. The more volatile the markets are, the less likely the ETF shares closely track their underlying assets. This situation seems to be aggravated by two factors. First, ETFs allow for short selling which can potentially pull their values away from their NAVs. Second, underlying assets of international ETFs are traded in markets that operate in different time zones. In an extreme case, international markets in which ETFs’ underlying assets are traded may be closed when the U.S. markets are open, or vice versa. We examine Japanese and Chinese ETFs because of high trading activity in the two markets. Examples of noticeable price deviation include ETFs of Chinese and Japanese funds. Lauricella and Gullapalli report that on February 27, 2007, the Chinese ETF price dropped 9.9% while the underlying index only fell 2.1% in the home market. In the following day, the index fell another 3.1% in the home market but its ETF price rose 4.3% in the U.S. The objective of our study is to shed light on these facts by taking a closer look at the relationship between return volatility, trade size, and order imbalance for international ETFs. Order imbalance represents the difference between the number of securities bought and sold for a specific financial asset. This variable has proved to be a key predictor of volatility and yet is frequently overlooked in the relation between returns and volatility. Sias (1997), Chan and Fong (2000), Chordia et al. (2002), and Chordia and Subrahmanyam (2004) show that order imbalance can help explain return volatility for individual stock, closed-end funds, and for the market as a whole. However, there has not been any work done on the relation between volatility and order imbalance for international ETFs. What makes the analysis of volatility and order imbalance particularly interesting for International ETFs is the fact that the value of the underlying assets lags their prices in U.S. markets, especially for Asian markets. The reason is that the local markets for underlying assets of these ETFs close a few hours before U.S. markets open. This allows U.S. ETF market prices to incorporate more information beyond the value reflected in the underlying NAVs. In addition, ETFs have a distinct characteristic that allows for the creation and redemption of shares. Small orders tend to be traded in a stock exchange but large orders go to large investors or market makers who are able to create or redeem ETF shares. These large market players attempt to extract profit from the price gap between NAVs and market prices of ETFs. Nonetheless, in fast-moving volatile periods, it will be hard even for these investors to determine what the true price is. This can create large differences between market prices and NAVs as well as increased price volatility. This increased volatility can mislead investors and cause them losses beyond the drops in value of these funds’ underlying assets. This study attempts to use order imbalance to explain price returns and volatility in the Japanese and Chinese ETFs so that investors may use this information to help mitigate their losses. We focus on the Japanese and Chinese ETFs because they are highly liquid international investments traded in dollars in U.S. markets. These ETFs’ characteristics allow us to analyze order imbalance and trade size impact on the volatility of international investments, which to our knowledge has not been done before. The rest of this paper is organized as follows: Section 2 gives a brief overview of research on information transmission, efficiency, and diversification benefits of international investments; Section 3 describes the characteristics of the data used in the current analysis; Section 4 presents the main results and Section 5 concludes.",2
37,2,Journal of Economics and Finance,14 July 2011,https://link.springer.com/article/10.1007/s12197-011-9199-2,Do MNCs spur financial markets in corrupt host countries?,April 2013,Mohsen Bahmani-Oskooee,Shady Kholdy,Ahmad Sohrabian,Male,Male,Male,Male,"Do Multinational Corporations (MNC) stimulate, or do they deter the growth of financial markets in developing countries with corrupt governments? The government control and the slow development of financial markets in the developing countries has long been a major concern among the domestic investors, who need external funds to acquire new management and technical skill, as well as MNCs, which are affected by quick access to foreign exchange and other financial facilities (Chor et al. 2007; Feinberg and Phillips 2004). However, unlike small domestic firms which have no or limited influence over the ruling class, MNCs have strong influence on the economic—political setting of the host country through their interactions with the government agencies and other market actors including the suppliers and shareholders. The existing literature conjectures MNC can sway the growth of local financial markets in two different and very opposite directions. One line of literature pioneered by Rajan and Zingales (2003) argues that MNCs can stimulate the financial markets by exposing the local economy to vigorous foreign competition. This group of economists believes that inflow of foreign goods and capital can force the local elites to adopt more market friendly policies and support the institutions necessary for a more competitive domestic market. There is some historical evidence on concurrence of periods of high international mobility of goods and capital (1900–1930 and 1990–2000) with periods of financial market maximum development. The other line of literature argues MNCs operating in developing countries with more advanced financial markets capture a smaller share of the domestic market and have to export a larger share of their products to a third country (Antràs et al. 2009). To avoid this problem and other difficulties including low economic freedom and day-to-day direct intervention by government, MNCs have to implicate a broad range of political activities in the vein of lobbying influential government representatives, hiring personnel with direct political experience as mangers or consultants and providing financial support or bribery to government agencies. (Baron 2005; Carroll and Buchholtz 2006; van Tulder and van der Zwart 2006). These types of MNC activities can consequently enhance the power of incumbent corrupt elites and may deter the expansion of financial markets. Despite the strong theoretical literature, the empirical evidence on the direct impact of MNCs on domestic financial markets is scarce. The issue addressed in this paper is, how does MNCs contribute to local financial market development in the context of local political corruption? In a search of the literature on the relation between foreign investment, financial development, and economic growth, we came across studies that could be clustered into three groups. First, extensive line of literature (Bencivenga and Smith 1991; King and Levine 1993a, b; Frankel and Romer 1999; Rajan and Zingales 2003; Alfaro et al. 2004; Durnev et al. 2001, 2004; Bekaert et al. 2005; Beck et al. 2006; Ahlin and Pang 2008; Baltagi et al. 2009) examines the links between financial development and growth. A parallel line of literature (Mansfield and Romeo 1980; Romer 1993; Borenztein et al. 1998; Aitken and Harrison 1999; Rappaport 2000; Keller 2002; Carkovic and Levine 2005; Yasar and Morrison Paul 2007) investigates the interaction between foreign direct investment (FDI) and growth. Finally, a third line of literature (Mauro 1995, 1998; La Porta et al. 1998, 1999, 2000; Morck et al. 2000; Morck and Yeung 2004; Rodrik 2004; Morck et al. 2005) concentrates on the effect of political corruption and domestic institutions on growth. In spite of the foregoing broad literature, very few studies have addressed the link between FDI, financial development, and political corruption in an integrated manner. This study, by examining the effect of MNCs on financial development in countries where foreign firms have to interact with corrupt political regimes, is one of the few attempts to investigate the integrated effect of foreign firms and domestic corruption on financial development. Discovering the existence and direction of this interaction is particularly important because it reveals whether liberalizing restrictions on foreign equity participation alone can be used as an effective tool in promoting economic growth despite corruption, or whether liberalization has to be coupled with policies concentrating on controlled level of political corruption. To provide evidence on interaction hypothesis, we apply dynamic generalized method of moment (GMM) panel data technique developed by Arellano and Bond (1991) and Blundell and Bond (1998). To quantify the influence of MNCs on the local economy, we use the FDI data. Using annual data of a panel of 22 developing countries during 1986–2005, we examine the direct affect of FDI and political corruption as well as the impact of their interaction on financial development. The interaction term between FDI and political corruption allows us to examine whether the effect of MNCs depends on the degree of political corruption. The dynamic GMM estimator eliminates any endogeneity that may arise because of correlation between country- specific, time invariant, factors and the right hand side regressors. To prevent the potential endogeneity arising from reverse causality between financial development and the exogenous variables in the model, we use the lagged values of all the exogenous variables. For comparison purpose, we also report the results using the fixed effect method, even though these results are biased of order 1/T in dynamic panels. Our results reveal that FDI has positive and significant effect on financial development. Our results further suggest that FDI has a stronger stimulate effect on financial development in the more corrupt countries. This paper proceeds as follows: in Section 2 we introduce a model that outlines the determinants of financial development. Data and empirical results using panel techniques are presented in Section 3. Finally, a conclusion is provided in Section 4.",
37,3,Journal of Economics and Finance,27 May 2011,https://link.springer.com/article/10.1007/s12197-011-9185-8,FDI activity and worker compensation: evidence from US non-manufacturing industries,July 2013,Özlem Eren,James Peoples,,Female,Male,Unknown,Mix,,
37,3,Journal of Economics and Finance,08 June 2011,https://link.springer.com/article/10.1007/s12197-011-9192-9,Investment banks advising takeover targets,July 2013,Qingzhong Ma,,,Unknown,Unknown,Unknown,Unknown,,
37,3,Journal of Economics and Finance,09 June 2011,https://link.springer.com/article/10.1007/s12197-011-9187-6,Myopia and pensions in general equilibrium,July 2013,Frank N. Caliendo,Emin Gahramanov,,Male,Male,Unknown,Male,"The OASI social security tax rate in the US has doubled in the last half century, and it is five times larger than when the program was created in 1935. Currently, the OASI part of social security is funded by a 10.6% payroll tax (split evenly between employers and employees), making it the largest government program in the world. Social security is traditionally justified among the lay public on the grounds that people are myopic and have trouble saving for retirement on their own. If we take this foundation seriously in modeling the program, has it grown too big? That is, does the degree of myopic behavior that we observe in the US justify the size of the social security program?Footnote 1 Our focus on the US is merely for convenience since pay-as-you-go programs with similar institutional features are common throughout the OECD. To study this question we build a general equilibrium life-cycle model with overlapping generations of individuals who choose when to retire. Our economy is composed of a mixture of life-cycle permanent-income (LCPI) consumers who save optimally and “hand-to-mouth” consumers who don’t save at all. The LCPI consumers solve a two-stage optimal control problem that defines optimal consumption, saving, and retirement age for a given set of factor prices, social security taxes, and social security benefit incentives. The hand-to-mouth consumers just consume their disposable income and never save for the future, but they do choose when to retire. At the macro level, factor prices are determined competitively and the social security administration runs a pay-as-you-go program with a balanced budget. Lifetime utility in a stationary competitive equilibrium is the welfare criterion for both consumers, even though the hand-to-mouth consumers do not maximize utility. This paternalistic approach opens up the possibility that social security could be welfare improving. Quantitatively speaking, our model performs fairly well along some dimensions that are important for studying the question that we have posed. For example, the model endogenously produces a realistic capital-output ratio and a realistic interest rate, realistic retirement choices, a realistic discrete drop in consumption spending at the date of retirement, and a realistic incentive structure that rewards delayed collection of social security benefits. The model also bears a realistic portion of the population who never save for retirement (20% in the US), so it appears to be a reasonable instrument for quantitatively studying myopia as a justification for social security. We compare lifetime utilities in a stationary competitive equilibrium with the current tax rate to lifetime utilities in a different equilibrium with a smaller tax rate. We find that both groups of consumers prefer the equilibrium with the smaller social security tax. Indeed, a small reduction in the tax rate can generate fairly large welfare gains, measured as the percentage increase in consumption in each period that is necessary to achieve the level of lifetime utility in an equilibrium with a smaller tax rate. This “compensating variation” is similar for both groups of consumers and it increases nearly linearly with the size of the tax cut. A reduction in the social security tax rate from 10.6 to 9% leads to compensating variations for both groups of consumers that are above 1%. A reduction in the tax rate that is twice as large leads to compensating variations that are above 2%, etc.Footnote 2 But, we emphasize that these results do not necessarily mean the US program is too big. We remain agnostic about that. Instead, in the context of the model, the results mean the social security program is bigger than can be justified by the level of myopia present in the US, leaving other roles such as redistribution of income as a necessary part of the rationale for social security. To our knowledge, our paper is the first general equilibrium study of the optimal provision of social security when the economy is a mixture of LCPI and hand-to-mouth individuals and when retirement is endogenous. Feldstein (1985), Docquier (2002), Hurst and Willen (2007), and Cremer et al. (2008) study the optimal design of pay-as-you-go social security for a mixed economy in partial equilibrium. Caliendo and Gahramanov (2009) study the optimal design of social security for a mixed economy in general equilibrium, but retirement is exogenous. Therefore, the optimal rate of social security taxation is an open question, for a general equilibrium, mixed economy with endogenous retirement. Our model is a continuous-time, general equilibrium extension of the two-period, partial equilibrium model created by Cremer et al. (2008).Footnote 3 The purpose of their paper is to derive the optimal level of social security taxation and the optimal degree of redistribution (from elderly wealthy to elderly poor) when the economy is composed of both LCPI and hand-to-mouth consumers who vary by labor productivity. They primarily make an analytical contribution, while we try to make a quantitative point with a computable general equilibrium model. We abstract from intra-cohort heterogeneity in labor productivity to focus on the mandatory saving role of social security.Footnote 4 More broadly speaking, our work falls into a large, normative literature that studies the optimal provision of social security under myopia, pioneered by Feldstein (1985) and revived by İmrohoroğlu et al. (2003).Footnote 5
 It is useful to study this question in general equilibrium for a number of reasons. In actuality, social security affects life-cycle consumption and saving decisions and it affects retirement choices. These distortions affect the supply of aggregate capital and labor and hence factor prices. Also, the social security budget comes from the tax base, which is affected by retirement choices and factor prices. A general equilibrium model is able to account for these distortions by assuring that, whatever households choose to do at the micro level in response to social security, at the macro level factor markets must clear and the social security budget must balance. The drawback is that we have fewer analytical results since our analysis is largely computational, but the fundamental micro-level intuition is already well understood from papers like Cremer et al. (2008).Footnote 6
 Our work is also closely related to a large, positive literature that studies the effect of various institutional features of social security programs on labor force participation in old age (recent examples include Diamond and Gruber 1997; Gruber and Wise 1998; Samwick 1998; Börsch-Supan 2000; Coile et al. 2002; Cremer and Pestieau 2003; Gustman and Steinmeier 2005; Coile and Gruber 2007; Cigno 2008; Liebman et al. 2009; Mastrobuoni 2009; Orti 2009 to name just a few). For example, Diamond and Gruber (1997) have done a thorough analysis of the incentives associated with initiation of benefits at different ages by calculating how social security wealth evolves as a function of retirement age. Our contribution is to extend their work by embedding these incentives in a computable general equilibrium model and then to quantify the welfare consequences of reductions in the size of the social security tax rate. Finally, we pay special attention to the issue of whether it is optimal to delay collection of benefits beyond the date of retirement. In the US, collection of social security benefits and retirement are not required to be synchronized. One doesn’t actually need to work longer to qualify for a larger annuity, one can retire early and just postpone collection. Coile et al. (2002) point out that allowing for this institutional feature can prevent the model from generating biased predictions. We attempt to build the type of model that they have put on the research agenda: A full analysis of the problem of delayed claiming would model jointly the retirement and the claiming decision. Such a model is beyond the scope of the current effort. We make progress along this front by augmenting our optimal control problem of endogenous retirement to also allow for delayed claiming. The problem becomes a complicated three-stage optimal control problem with two endogenous switch points. The agent jointly optimizes along the consumption, retirement, and benefit collection dimensions. We show that our welfare results are robust, in general equilibrium, to the inclusion of this institutional feature.",7
37,3,Journal of Economics and Finance,14 June 2011,https://link.springer.com/article/10.1007/s12197-011-9194-7,Carry-trades on the yen and the Swiss franc: are they different?,July 2013,André Varella Mollick,Tibebe Abebe Assefa,,Male,Unknown,Unknown,Male,"Interest rate differentials are perhaps the most frequently used among the class of “fundamentals” used when explaining exchange rates. Following Meese and Rogoff (1988), several works have questioned the ability of interest rate differentials, as well as other fundamentals, to explain or predict exchange rate changes.Footnote 1 Engel and West (2005), for example, have examined a conventional class of asset-price models in which the exchange rate is the expected present discounted value of a linear combination of observable fundamentals and unobservable shocks. They argue in this class of models that as the discount factor (β) approaches unity the model puts relatively more weight on fundamentals far into the future in explaining the asset price. They find, in particular, that β > 0.90 suffices to yield near-zero correlations between exchange rate at t and information at t-1. Engel and West (2005) also argue that much of the short-term fluctuation in exchange rates is driven by changes in expectations about the future. If the models are good approximations and expectations reflect information about future fundamentals, the exchange rate changes will be useful in forecasting these fundamentals. Therefore, in contrast to the causality inherited in monetary models from fundamentals to exchange rates it is possible that exchange rates help forecast the fundamentals. Using quarterly bilateral U.S. dollar (USD) exchange rates against six major currencies from 1974 to 2001, Engel and West (2005) find evidence of causality in the Granger sense, especially for nominal variables. In international economics, the ex-ante uncovered interest parity (UIP) condition that high-paying currencies must be associated with a depreciating currency is commonplace in standard models, such as Dornbusch (1976)’s overshooting model. If not, capital would only flow to the highest-paying currency country. The empirical evidence on UIP is mixed, however. Flood and Rose (2002), for example, provide evidence that crisis battered currencies in the 1990s had UIP most clearly validated. Most of the empirical evidence is grounded on the forward premium anomaly: regressions of changes in exchange rates on the forward discount (forward minus spot rate) usually yield negative coefficients, contrary to UIP predictions. Clarida et al. (2009) argue that these regressions overlook volatility in exchange rates and, when appropriately separated by volatility incidence the coefficient on forward premium changes signs. Frankel and Poonawala (2010) find for a sample of 14 emerging market currencies a smaller bias than for advanced country currencies. See also Bai and Mollick (2010) for breaks in the forward premium anomaly in emerging market currencies. Recent work has also explored profit opportunities associated with this deviation from the UIP condition: Hochradl and Wagner (2010) develop trading strategies aimed at exploiting the forward bias. A particular violation of UIP is the carry-trade activity, in which investors may benefit from shorting low-yield funding currencies and being long in high-yield target currencies. Burnside et al. (2009) focus on the microstructure of FX markets and argue that adverse selection problems between participants can account for the “forward premium puzzle”: the fact that high interest rate currencies tend to appreciate relative to low interest rate currencies. Galati et al. (2007, p. 28) characterize in this way: “A currency carry trade is usually defined as a leveraged cross-currency position designed to take advantage of interest rate differentials and low volatility. The strategy involves borrowing funds at a low interest rate in one currency (the funding currency) and buying a higher-yielding asset in another (the target currency). Ex ante, the strategy is only profitable as long as the gains from interest rate differentials are not expected to be overwhelmed by exchange rate movements in the short to medium term; that is, UIP is not expected to hold.” Carry-trade activities, and the possibility of profiting from them, have been enormously influential in press commentaries and academic works alike. In their overview of the crisis in the foreign exchange (FX) market, Melvin and Taylor (2009, p. 1318) identify two major instances of the recent period: “Carry trades tend to unwind during conditions of market stress and relatively modestly unwinds have been seen historically once or twice a year on average. Prior to 2007, the most recent major carry trade unwind was in October 1998 following a Russian bond default and the collapse of Long-Term Capital Management. The carry trade unwind occurring on August 16, 2007 was as devastating for many currency managers as was the 1998 episode: the 1-day change in the JPY price of the AUD on August 16, 2007 was −7.7%, compare(d) to the average daily change in that exchange rate for 2007 prior to August 15 of only 0.7%.” Melvin and Taylor (2009, p. 1318) date the beginning of the crisis in the FX market as August 2007. Brunnermeier et al. (2009) use eight major developed markets from 1986 to 2006 for nominal exchange rates, 3-month interest rates, futures position data from the Commodity Futures Trading Commission (CFTC) as a proxy for carry trade, and data on FX options to measure the cost of insurance against crash risk. They use a VAR with abnormal return to a carry trade (interest rate differentials minus changes in the exchange rate), interest rate differentials, skewness, and futures. Their VARs confirm the basic facts from univariate regressions, which are consistent with violations of the UIP condition.Footnote 2
 Jordá and Taylor (2009) have recently suggested that the fundamental equilibrium exchange rate (FEER) is crucial since expected returns are lower, all else equal, when the target currency is overvalued. We reexamine this possibility in this paper over a long time span of monthly data from 1986 to 2009. We place a special emphasis on the Japanese Yen as well as on the Swiss franc for at least two reasons. First, anecdotal evidence suggests plenty of evidence that these two currencies have been especially used in carry-trade.Footnote 3 For the Japanese Yen see additional evidence in Gagnon and Chaboud (2007) and Obstfeld (2009). For the Swiss Franc, for the pre-crisis period see the following: “Historically, Switzerland’s currency has been the strong, silent type: the tiny nation’s steady economy and peaceful geopolitical have long lured investors to the Swiss franc in times of global economic turmoil. But lately the Swiss franc has started behaving more like the Japanese yen—a darling of currency speculators who use it to fund risky investments across the globe. The reason: Switzerland’s extremely low interest rates of 2.25%. Currency traders are borrowing massive amounts of Swiss francs, then investing that money in countries like Iceland, where the money earns higher interest rates of 14.25%—a maneuver known as carry trade.” (The Wall Street Journal, March 16, 2007, C3). Second, our calculations below indicate that both the JPY and CHF have a negative mean of own interest rates with respect to the USD over the very long-run. Using monthly data from 1986 to 2009 for 11 major economies, we find that interest rate differentials between nine of these countries and the U.S. are generally positive (sample mean of 0.86%) but become strongly negative for Japan (mean of −2.78%) and for Switzerland (mean of −2.22%). Since much has been said about carry-trade in these two currencies in the recent period, we investigate nominal exchange rate changes using the class of empirical models proposed by Jordá and Taylor (2009)’s as starting point. A common finding for all panels is that about 2% of real exchange rate misalignments are corrected in the following month. While it is true that nominal exchange rates responded to these real exchange rate fluctuations, we found no evidence that carry-trading currencies respond in any way differently from all the other currencies with respect to overvaluation or undervaluation. We also find, however, important differences across samples. For the two carry-trade currencies the results are as follows. First, interest rate differentials have a negative impact on exchange rates: higher paying currencies should appreciate, against the uncovered interest rate parity (UIP) condition. Yet interest rate differentials have no significant impact on exchange rates for the subsamples other than the carry-trade currencies. This result is robust to money supply (M1) differentials serving as instrumental variables to inflation rates. Money supply is used as instrument for inflation as “inflation is always and everywhere a monetary phenomenon.”Footnote 4 Second, inflation rate differentials have no impact on currency changes. Third, dummy variables for periods of market turmoil suggest a particularly strong appreciation of these currencies against the USD. Fourth, these two currencies depreciate slightly when money supply increases when M1 differentials are used instead of inflation rates. This study is a contribution in at least three specific ways: First, the countries of choice in the empirical analysis are different from previously studied, including all major currencies traded in foreign exchange markets. Second, we use money supply as instrumental variable for inflation. Third, we consider a very important sub-sample of countries that behave differently according to interest rate differentials and carry-trade activities. The paper is organized as follows. The data description is presented in Section 2. The methodology follows in Section 3, and then the empirical results appear in Section 4, followed by the concluding remarks.",4
37,3,Journal of Economics and Finance,17 June 2011,https://link.springer.com/article/10.1007/s12197-011-9195-6,"Remittances and economic growth in Africa, Asia, and Latin American-Caribbean countries: a panel unit root and panel cointegration analysis",July 2013,Christian Nsiah,Bichaka Fayissa,,Male,Unknown,Unknown,Male,"In spite of the recent worldwide contraction in private financial flows to developing countries, remittances still continue to be a lifeline for more than 700 million people in developing countries. According to the World Bank estimates, remittances totaled $420 billion in 2009 of which $317 billion went to developing countries, involving some 192 million migrants or 3% of the world population (World Bank, 2010). For many developing countries, remittances represent a major part of international capital flows, surpassing foreign direct investment (FDI), export revenues, and foreign aid (Giuliano and Ruiz-Arranz, 2005). Figures 1, 2, 3, and 4 below depict the average annual growth rate of international financial flows in the form of remittances, overseas development assistance (ODA), and foreign direct investment (FDI) for African, Asian, and Latin American/Caribbean countries as a group and for each of the three regions, respectively for the years between 1985 and 2007. FDI leads the way in all the regions in terms of growth followed by remittances, which have already surpassed official development assistance as a source of foreign financial inflows to the three regions
 Average annual growth rate of foreign inflows from 1985 to 2007 for the whole area Average annual growth rate of foreign inflows from 1985 to 2007 for Africa Average annual growth rate of foreign inflows from 1985 to 2007 for Asia Average annual growth rate of foreign inflows from 1985 to 2007 for Latin America/Caribbean The main objective of this study is to estimate the long-run macroeconomic impact of remittances on the per capita GDP of African, Asian, and Latin American/Caribbean countries while controlling for some key sources of economic growth such as openness of the economy, capital/labor ratio, and economic freedom. As such, the study makes contributions to the existing literature on three fronts. First, the paper utilizes rich panel data covering three regions of the world (Africa, Asia, and Latin America/Caribbean) where the majority of the developing countries reside to investigate the relative impact of remittances on their economic growth as a group and/or individually. Secondly, we use newly developed panel unit-root tests, cointegration tests, and Panel Fully Modified OLS (PFMOLS to establish the long-run relationship between per capita GDP growth and remittances while taking into account some key control variables such as the openness of the economy, capital labor ratio, and the measure of economic freedom. Thirdly, the study provides a unified comparative analysis of the relative impact of remittances and other control variables of the economic growth African, Asian, and Latin/Caribbean countries. The findings suggest that remittances, openness of the economy, and capital labor ratio have positive and significant effect on economic growth for the regions as a group and in each of the regions. The paper is organized as follows. The next section gives a brief review of the literature. Section 3 describes the data and empirical methodology. The empirical results are presented in Section 4. The final section draws conclusions based on the results.",28
37,3,Journal of Economics and Finance,11 June 2011,https://link.springer.com/article/10.1007/s12197-011-9190-y,Exchange rate volatility and demand for money in less developed countries,July 2013,Sahar Bahmani,,,Female,Unknown,Unknown,Female,"The first of many studies that introduced the idea of including the exchange rate as another determinant of the demand for money in the money demand function was by Robert Mundell (1963). Since he made this argument without any empirical proof, many studies followed and empirically justified the link between exchange rate changes and the demand for money. These studies all emphasize that the demand for money depends not only on the interest rate and income, but also on the exchange rate. A study conducted by Arango and Nadiri (1981) explained and empirically proved that when a domestic currency depreciates, foreign currency appreciates and the domestic currency value of foreign assets held by domestic residents rises. This causes the demand for money to increase because of the domestic perception of an increase in wealth. On the other hand, Bahmani-Oskooee and Pourheydarian (1990) argued when domestic currency depreciates, this can most certainly cause market participants to expect further depreciation, causing the public to demand less domestic currency and more foreign currency.Footnote 1 Thus, rather than raising the demand for money, a depreciation could have a negative impact on the demand for domestic currency. The end result depends on the strength of the wealth effect versus the expectation effect.Footnote 2
 Since exchange rate volatility could cause the wealth effect or expectation effect to be uncertain, exchange rate volatility is certainly a variable that could have a direct impact on money demand. Mcgibany and Nourzad (1995) have already recognized the importance of exchange rate volatility in the demand for money specification by estimating a money demand function for the U.S. over the period of 1974–1990. As they argued, exchange rate volatility induces investors to substitute safer assets for riskier currency. They find that exchange rate volatility has a negative impact on M2 money demand in the U.S. The main objective of this paper is to extend the literature related to the effects of exchange rate volatility on the demand for money by considering the experiences of as many less developed countries (LDCs) as data permits. The impact of exchange rate volatility on the money demand in LDCs is as relevant as it is in developed countries. Although the public in LDCs may not have good access to foreign assets, because of the black market for foreign exchange in many of these countries, they do speculate in holding domestic currency versus foreign currency.Footnote 3 The volatility of the exchange rate can induce portfolio adjustment in favor of or against foreign currency depending on expectations, leaving the impact of exchange rate volatility on the demand for domestic currency to be ambiguous. The rest of the paper is organized as follows: Section 2 introduces the model and the methodology. Section 3 presents empirical support showing that indeed, exchange rate volatility has short-run as well as long-run effects on the demand for M2 monetary aggregate in LDCs. Section 3 provides a summary. Lastly, in the Appendix, data sources and the definitions of variables are listed.",6
37,3,Journal of Economics and Finance,26 August 2011,https://link.springer.com/article/10.1007/s12197-011-9205-8,"The influence of ETFs on the price discovery of gold, silver and oil",July 2013,Stoyu I. Ivanov,,,Male,Unknown,Unknown,Male,"In this study we examine the relation among gold, silver and oil exchange traded funds (ETFs) and their respective futures instruments and commodities in the period March 01, 2009–August 31, 2009. Most of the literature in ETFs is based on examining ETFs tracking financial indexes. As of the end of August 2009, commodities ETFs held assets worth $58.176 billion or 8.86% of the assets of all existing ETFs according to the Investment Company Institute.Footnote 1
 The commodities ETFs that we study are the S&P Depository Receipts (SPDR) Gold Trust, with ticker symbol GLD which has approximately $40 billion in assets, the iShares Silver Trust, with ticker symbol SLV, which has approximately $5 billion in assets, and the United States Oil Fund, with ticker symbol USO, which has approximately $3 billion in assets. For comparison the most popular financial ETFs are the SPDR S&P 500 Trust, with ticker symbol SPY, the Diamonds Trust Series 1, with ticker symbol DIA, and the PowerShares Nasdaq 100 Trust, with ticker symbol QQQQ. The assets held by these financial ETFs are approximately $73 billion, $8 billion and $17 billion, respectively. This is the first study, to the best of our knowledge, to examine gold, silver and oil ETFs and their relation to the futures market and underlying commodity prices. We find that the gold, silver and oil ETFs, that we study, closely track the performance of their underlying assets by using tracking error and pricing deviation metrics. We test whether in recent times the existence of ETFs has changed the dominating role of the futures market in price discovery. We find that price discovery for gold and silver predominantly occurs in the ETF market, while the oil market has price discovery occurring predominantly in the futures market. Oil has been in the spotlight for quite some time now and the recent surge in gold prices has been quietly shadowed by a surge in silver prices (Opdyke 2009). Considering the fragile state of the real economy and stock market and the fact that gold has always been a safe place in bad times, the interest in commodities is expected to get even greater based on the expectations of recovery in the economy and the increased demand for commodities for end consumption. And ETFs are going to benefit from this surge and get even more popular because of the easy access to commodities that they bring to smaller individual investors as discussed by O’Connell (2007) for gold ETFs. However, the increased introduction of more commodity ETFs has stirred controversy in the investment community because of perceived shift in the price discovery processes for assets in the different commodity markets. Price discovery is the process of adjustment of asset prices due to the constant flow of fundamental information about the asset. Considering that there are different markets available for trading the same asset, the analysis of price discovery helps identify the market where the fundamental information is incorporated into prices most efficiently. In a recent article Bernstein (2009) suggests that the demand for ETFs causes major changes in the underlying assets and futures markets of commodities. In other words, the investment community is faced with the possible shift in the price discovery dominance structure of markets in that ETFs provide the leadership in the price discovery processes for commodities now whereas futures markets have been providing the leadership in the past (Hasbrouck 2003).Footnote 2
 Hasbrouck (2003) studies price discovery in the U.S. equity index markets. He examines the role that the introduction of ETFs on indexes has played in the price discovery in these markets. Hasbrouck finds that for indexes with E-mini futures contracts the price leadership is provided by the E-mini futures market. However, in the market of financial assets with lack of E-mini instruments having only traditional futures contracts the ETFs dominate the price discovery. The reason for this discrepancy, suggested by Hasbrouck, is due to the smaller denomination of the E-mini contracts, designed to be one fifth of the regular futures contract. The smaller capital requirement for investment in the E-mini makes the instrument more accessible to the investment community in contrast to the larger denomination of the traditional futures contract accessible only to large institutional investors.",44
37,3,Journal of Economics and Finance,27 May 2011,https://link.springer.com/article/10.1007/s12197-011-9186-7,Broadcast meteorology and the supply of weather forecasts: an exploration,July 2013,Daniel Sutter,,,Male,Unknown,Unknown,Male,"Democracy places authority over government decision making in the hands of the public, but except in a small community citizens require information that must be provided by intermediaries, the news media. Governments across the world have often monopolized radio and television, and the U.S. Constitution’s 1st Amendment singles out the press for protection from regulation. Journalists and journalism scholars contend, however, that commercial forces undermine the quality of news coverage. Among their alleged sins, commercial broadcasters excessively cut costs, expect too many stories per reporter, pursue entertainment and soft news at the expense of hard news, provide excessive coverage of scandals, pursue scoops at the risk of error or manipulation, and shy away from reporting news adversely affecting large advertisers. The traditional separation of news and business operations in many newspapers symbolizes the effort to minimize commercial influence, but scholars worry that the wall has been slowly dismantled.Footnote 1
 Economists have discovered news as an industry for study over the past decade, in part due to the number of interesting properties of the market. News is subject to quality assurance problems, as news consumers cannot typically verify the accuracy of reports, or evaluate the selection of stories. Consequently the news market faces potential for a lemons problem and bias. News relies heavily on advertiser support, creating a two-sided market and an additional potential source of bias. The effect of advertising has long been studied (Steiner 1952), and results in several well understood inefficiencies (Spence and Owen 1975; Owen and Wildman 1992). Notably, advertisers’ willingness-to-pay depends on audience size, not the viewers’ willingness to pay for a program. Advertiser support may fail to supply a program valued highly by a small audience, although the inefficiencies of advertising decline as the number of channels increases. News has high first unit costs and a low cost of supplying an extra customer, can generate positive externalities, and the accuracy of stories is difficult for consumers to verify (Hamilton 2004). Television broadcasters undoubtedly supply news, but whether they supply sufficiently high quality coverage is less certain. Economists have also explored the consequences of news coverage. The media have been linked to the occurrence of famine (Besley and Burgess 2001) and play an important role in economic development (Coyne and Leeson 2004; Leeson 2008). Djankov et al. (2003) examined patterns of media ownership around the world and the close link between the nature of government and an independent media. The effects of competition between local and national news products has been examined (George and Waldfogel 2006) as well as how economic forces shape news coverage (Hamilton 2004). Several aspects of news media bias have been examined, including the potential for competition to eliminate bias (Mullainathan and Shleifer 2005), the media’s alleged liberal bias (Groseclose and Milyo 2005; Gentzkow and Shapiro 2010), and the potential for advertisers to influence editorial content (Reuter and Zitezwitz 2006). The lack of an independent, objective measure of the truth of news stories complicates an assessment of how all these forces affect the news market. We cannot know whether chain newspapers or liberal reporters or advertisers shape news if we cannot systematically access the truth. Because of these difficulties, I turn instead to TV weather to provide evidence on the news market. In contrast with the news, objective measures of the incidence and impact of weather are available, providing a benchmark for the comparison of coverage. In addition, the incidence of severe weather varies substantially across the U.S., with cities along the Atlantic and Gulf coasts (plus Hawaii) facing the risk of hurricanes, while Plains states face a high risk of tornadoes. The variation in severe weather allows a test of the supply of weather coverage across markets. The public good character of weather forecasts provides the economic rationale for government meteorological services.Footnote 2 Nonetheless commercial TV in the U. S. supplies a large volume of weather coverage, from the Weather Channel to local stations. Several factors explain commercial broadcasters’ weather coverage. Because their product content changes from day-to-day, cannot be promoted very far in advance, and is driven by external factors, news organizations brand their product around the on-air personalities who deliver the daily changing content (Hamilton 2004). Broadcast news takes advantage of the strong bonds between viewers and television personalities, known as para-social interaction (Horton and Wohl 1956). Crises heighten this connection (Loges 1994), and severe weather can constitute such a local crisis. Weathercasters are very visible members of a news team, and warnings, advice or reassurance during threatening weather can provide an enduring bond and create brand loyalty among viewers. Severe weather is life threatening, and weather coverage is a potential life saver.Footnote 3
 Severe weather coverage constitutes perhaps the most instrumentally valuable information conveyed by local TV. Contrast, for instance, broadcast of a tornado warning with coverage of a city council meeting. As a consequence of rational ignorance, the value of the details of city council decisions is low, and most viewers across a TV market will not live within any one city. By contrast, local television is the primary means by which people receive severe weather warnings (Hammer and Schmidlin 2002; Paul et al. 2003; Drobot 2007; Hayden et al. 2007). Response to natural hazard warnings depends on people recognizing that they face personal risk from the threat and take the warnings seriously (Mileti and Sorensen 1987; Drobot et al. 2007). Sherman-Morris (2005) found that 80% of respondents would take cover if warned to do so by their local weathercaster during severe weather, and thus local weather coverage can potentially do more than simply transmit warning information. Finally, TV weather has policy relevance, as the Federal Communications Commission requires transmission of severe weather warnings by broadcasters. Optimal regulation will depend on whether market incentives result in adequate severe weather coverage. I examine weather coverage by local TV stations. Local television news is commonly regarded as the lowest quality segment of the news market (see for instance Powers 1977).Footnote 4 Because of the “if it bleeds, it leads” news standard, shameless efforts to attract viewers during sweeps months, expectations for local weather coverage may be very modest. On the other hand, stations incur significant costs to cover the weather, including using mobile units with satellite uplink capability, and preempting programming and commercials for wall-to-wall coverage of severe weather. Specifically I focus on two inputs into this coverage: investment by a station in their own Doppler weather radar, and employment of weathercasters holding the American Meteorological Society (AMS) Television Seal of Approval. I test whether the incidence of severe weather—hurricanes, tornadoes, floods, and snow—affects investment in these inputs across U.S. TV markets. Viewer demand for weather information should be greater in markets with more severe weather, and I test whether competition between local stations leads to this pattern of investment. The remainder of this paper is organized as follows. Section 2 discusses the weather input measures and data used in this paper and defines the variables. Section 3 presents the analysis of the determinants of weather coverage and discusses the findings. The local tornado rate is a statistically and economically significant determinant of weather investments. I find weaker evidence that hurricanes increase investment, while floods and snowfall are generally insignificant. Section 4 offers a brief conclusion and directions for future research.",2
37,4,Journal of Economics and Finance,28 June 2011,https://link.springer.com/article/10.1007/s12197-011-9188-5,Simultaneous dependence between firm-level stock returns,October 2013,Kenneth P. Moon,James P. LeSage,,Male,Male,Unknown,Male,"For illustrative purposes and to make our discussion concrete, we use the model from Eq. 1 of Pirinsky and Wang (2006). Least-squares estimates and inferences are frequently based on an average of the coefficients from a set of time-series regressions for each firm in the sample. We let y
1, ..., y

N
 denote a set of T × 1 vectors representing the time-series containing T observations for each firm, i = 1, ..., N. Similarly, we use U
1, ..., U

N
 and Z
1, ..., Z

N
 to represent the local index and industry index explanatory variable vectors used in the basic model from Eq. 2 of Pirinsky and Wang (2006). This allows us to write the sequence of time-series regressions shown in Eq. 1.
 where ι

T
 is a T × 1 vector of ones and α

i
 are associated intercept parameters. The T × 1 vectors ε
1, ... ε

N
 represent disturbances each of which obeys the Gauss–Markov assumptions that the time-series disturbances are zero mean, independent and exhibit constant variance over time. We can also assume for simplicity that the disturbances are independent across firms i = 1, ..., N, which is consistent with the practice of drawing a single overall or pooled inference regarding the relationship between the dependent and independent variables based on an average of the coefficient estimates β

i
, γ

i
, i = 1, ..., N from the separate firm-level time-series regressions. We wish to consider the implication of constructing variables U

i
 (Z

i
) using an average of stock returns from other firms in the sample of i = 1, ..., N firms. We can denote these constructed variables using:
 where the summation over j ∈ R

i
 denotes a group of r

i
 firms in the set R

i
 representing those in the same MSA as firm i. Similarly, k ∈ S

i
 represents the set S

i
 of s

i
 firms in the same industry as firm i. Using this notation, we can re-write the set of equations from Eq. 1 as:
 Of course, the sets R
1, R
2, ..., R

N
 and S
1, S
2, ..., S

N
 consist exclusively of elements taken from y
1, y
2, ..., y

N
, the set of firms in the sample. If we view the data sample on which our inference regarding:
 is drawn, as consisting of: {y

i
, i = 1, ..., N} then the averages of firm-level stock returns included as explanatory variables on the right-hand-side of the time-series regression equations should be viewed as endogenous. This of course leads to an endogeneity problem and associated simultaneity bias. It is important to note that the endogeneity criticism does not apply to the basic CAPM model where individual firm returns are related to a market-wide average return. In that circumstance the regression relationship could be written as: \(R_i = \alpha + \beta \bar R + \varepsilon_i\), where \(\bar R\) denotes the market average return = \(\bar R = \sum_{i=1}^n R_i /n\). For the case where β = 1, this could be rearranged as: \(R_i - \bar R = \alpha + \varepsilon_i\), which is a statement about the distribution taken by returns when they are transformed to deviation from means form. Specifically, the distribution of returns (in deviation from means form) is a scale shifted version of the distribution assumed to govern the vector of disturbances ε. Because there are a large number (n) of returns used to construct the sample average or market return, it is reasonable to view each individual observation R

i
 as contributing insignificantly to formation of the average or market return, eliminating endogeneity concerns. The problem arises when the number of firms r

i
, s

i
 in the sets j ∈ R

i
 and k ∈ S

i
 are small so it is implausible to argue that individual observations y

i
 are not involved in a simultaneity relationship with observations y

j
, j ∈ R

i
. We can rearrange the sample data to take the form where y

t
, X

t
, t = 1, ..., T represent sample data for each regression consisting of a cross-section of n

t
 firms at each time t. The matrix X

t
 represents an n

t
 × 3 matrix of explanatory variables containing the intercept and two variables U

t
, Z

t
. This allows us to focus on the vector constructed using an average of the stock returns for firms located in the same MSA as firm i at time t. This can be written as W

t

y

t
, where W

t
 is an n

t
 × n

t
 matrix that contains non-zero elements in row i,i = 1, ..., n

t
 having values 1/r

i
, where r

i
 is the number of firms located in the same MSA as firm i (at time t). For example, consider the matrix W

t
 for the case of n

t
 = 5 firms at time t located in two MSA’s shown in Eq. 5.
 In Eq. 5, firms 1, 2 and 3 are located in one MSA and firms 4 and 5 are located in the other MSA. The main diagonal of the matrix contains zeros to prevent firm i from being included in the average returns of (other) firms located in the MSA, which is produced by the matrix product W

t

y

t
. This would result in an n

t
 = 5 × 1 vector where the first observation represents the average stock returns from firms 2 and 3 located in the same MSA. Similarly, the third observation in the vector W

t

y

t
 would be the average return of firms 1 and 2, those located in the same MSA as firm 3, and so on. The sample from Pirinsky and Wang (2006) was restricted to a sample of MSA’s containing at least 5 firms, which would require a much larger matrix W

t
. However, their matrix would exhibit the same block-diagonal structure as W

t
, where each block represents an MSA. It should be clear that the same approach could be used to construct a matrix V

t
 with non-zero elements (of equal magnitude) in each row that create an average of stock returns from firms in the same industry as firm i (at time t). The matrix product would be: V

t

y

t
. We can use these expressions to re-state the model of Pirinsky and Wang (2006) for the cross-section of firms at time t as in Eq. 6, with the implied data generating process (DGP) shown in Eq. 7.
 where ρ is a scalar parameter that reflects the influence of average returns from firms located in the same MSA (the local return index) on the return of each firm at time t, and ϕ is a scalar measuring the influence of the industry index. Additional explanatory variables can be included in extended regression variants of this base model as control variables when testing for the significance of the parameters ρ and ϕ. In addition, Eq. 6 represents only a single regression for one time period t, and the ultimate inference is often based on an average of the estimates for ρ and ϕ over all time periods and firms. We will have more to say about both of these issues later. For now, we focus on the properties of the least-squares coefficient estimates ρ and ϕ. From the DGP, it should be clear that there is a Jacobian term involved in the transformation from ε to y. The log-likelihood function for this model takes the form in Eq. 8 (Anselin 1988; Lacombe 2004) where ω is an n

t
 × 1 vector of eigenvalues of the matrix W

t
, and ψ is a similar vector of eigenvalues for the matrix V

t
. If ω and ψ contain only real eigenvalues, a positive definite variance–covariance matrix is ensured by the conditions (Ord 1975):
 
ρ ∈ (min(ω) − 1, max(ω) − 1), 
ϕ ∈ (min(ψ) − 1, max(ψ) − 1), 
ρ + ψ < 1 Since the row-sums of the matrices W

t
, V

t
 equal one, the maximum eigenvalues max (ω) = 1, max (ψ) = 1, leading to an upper bound on ρ and ψ of one (Ord 1975). The term: \(|I_{n_t} - \rho W_t - \phi V_t|\) represents the determinant of the n

t
 × n

t
 matrix. Lee (2004) shows that maximum likelihood estimates are consistent for these models.
 The log-likelihood in Eq. 8 can be concentrated with respect to the parameters α and σ resulting in a bivariate optimization problem over the parameters ρ, ϕ. For details on maximum likelihood estimation of this type of model see Lacombe (2004) or LeSage and Pace (2009). Of course, the motivation for including explanatory variables reflecting an average of returns from firms in the same location or industry is to produce estimates and inferences regarding the magnitude and significance of the parameters such as ρ, reflecting the relation between the local (MSA) index of returns and those of individual firms located in the MSA. One can show that the least-squares estimate \(\hat \rho\) is biased with the term R reflecting the magnitude of bias. Since the plim of the quadratic form in the disturbances in Eq. 10 does not go to zero as n → ∞, least-squares estimates are inconsistent (Moon and LeSage 2008).Footnote 1
 A few points to note. First, since elements of the disturbance vector can be positive or negative, we cannot sign the bias term R. Second, it should be clear that a similar proof of inconsistency could be constructed for the parameter ϕ estimated using ordinary least-squares.",
37,4,Journal of Economics and Finance,21 June 2011,https://link.springer.com/article/10.1007/s12197-011-9193-8,A comparison of corporate versus government bond funds,October 2013,George Comer,Javier Rodriguez,,Male,,Unknown,Mix,,
37,4,Journal of Economics and Finance,22 June 2011,https://link.springer.com/article/10.1007/s12197-011-9191-x,Are commercial bank lending propensities useful in understanding small firm finance?,October 2013,James E. McNulty,Marina Murdock,Nivine Richie,Male,Female,Unknown,Mix,,
37,4,Journal of Economics and Finance,28 June 2011,https://link.springer.com/article/10.1007/s12197-011-9189-4,The effect of banking market structure on the volatility of growth of manufacturing sectors in developing countries,October 2013,Indrit Hoxha,,,Male,Unknown,Unknown,Male,"The effect of financial intermediaries on real output has generated a heated debate in the literature. In the last decade, a consensus has been reached that higher financial intermediation facilitated higher economic growth, where Rajan and Zingales (1998) have put a cornerstone in the literature. To evaluate the effect of financial intermediaries on real output another important issue is to look whether an increase in financing through financial intermediaries has any effect on the volatility of output. Based on this debate, in this paper, I study the effect of banking competition and banking concentration on the volatility of output at industrial level using data from a group of developing countries. I find empirical evidence that banking competition increases the volatility of the growth of manufacturing sectors, while banking concentration reduces their volatility. From the theoretical point of view, the effect of banking development on the volatility of output is ambiguous. Morgan et al. (2004) suggest that improved access to banking finance allows firms to smooth out their idiosyncratic shocks. However, the effect of banking development on volatility of economic growth can be affected by the stage of the development of the country (Aghion et al. 2004), the type of shocks that the economy faces, such as monetary or real shocks (Bacchetta and Caminal 2000), or whether the economy faces credit demand versus credit supply shocks (Morgan et al. 2004). No consensus has been reached on the effect of banking development on volatility of economic growth in the empirical studies, also. Denizer et al. (2002) find that countries with more developed financial sectors experience less fluctuation in the growth of real per capita output, consumption and investment. On the other hand Easterly et al. (2001) show that between financial development (measured as credit to private sector) and aggregate volatility there is a U-shaped relationship. Comin and Philippon (2005) suggest that the effect of financial development on the volatility of growth depends on aggregation level, and show a positive relationship of financial development to firm level volatility. On the other hand, Correa and Suarez (2008) exploiting the staggered timing of state-level bank deregulation show a negative relationship of US banking deregulation and firm-level volatility. While there is a discussion about the effect of financial development on the volatility of economic growth, the main focus of this paper is to look at the effects of banking competition and banking concentration on the sectoral-level volatility. The novelty of this study is that it brings together two strands of the literature. One strand of the literature looks at the effect of financial intermediation on volatility of economic growth and the second one looks at the effect of banking concentration and banking competition on the credit access. There is another debate on the effect of banking concentration and banking competition on the credit access of firms. Theoretically, on one side Pagano (1993) and Guzman (2000) show that the markets where banking sector is concentrated and less competitive, grow less than their best potential, because firms do not get access to credit. On the other side, Petersen and Rajan (1995) suggest that only in concentrated banking systems, banks have an incentive in investing in relationship banking, which leads to more credit access. Marquez (2002) suggests that an increase in banking competition causes banks to screen firms less rigorously, which worsens the pool of the customers and leads to higher interest rates, resulting in a decreased amount of credit made available to manufacturing sectors. Many empirical studies have shown a relationship between banking market structure and the economic growth. Berger et al. (2004a) and Cetorelli and Strahan (2006) show that higher concentration and more restrictions on competition lead to less new firm creation, and less economic growth. Claessens and Laeven (2005) using a cross-section estimation method for bank competition, find that banking competition is important for the growth of industries dependent on external finance. On the other side, Cetorelli and Gambera (2001) in a cross country study find that industries in need of external finance grow faster in countries where there is more bank concentration, while Mitchener and Wheelock (2010) using historical data for US states find the industries have grown faster in states with higher banking concentration. In a panel study for a sample of 36 countries, Hoxha (2011) finds that industries in need of external finance perform better in markets with more concentrated banking sector, and worse in the markets with higher banking competition.Footnote 1
 The identification strategy used in the study is a similar approach to Larrain (2006) and Raddatz (2006), who find that industrial level volatility decreases with more private credit and financial development. However, the main focus of my study is on banking competition and banking concentration, instead of financial development measured as private credit as a share to GDP. In the literature banking concentration has been used as a measure of banking competition; however, I examine them separately, because they measure two different things. Banking concentration measures the share of the market that is controlled by the largest banks, but does not necessarily measure the competitive environment in the banking sector in a country. There are cases where a nearly competitive environment exists in markets with two or three banks, or where a substantially non-competitive environment has been observed in markets where there are thousands of suppliers, such as the credit card market (Shaffer 2004). To investigate the effect of banking competition on the volatility of growth of sectoral level output, I look at the main effect of banking competition and the effect that it has on specific industries. I measure competition with an index estimated as the sum of the elasticities of bank revenue to input prices. I find that banking competition has a positive effect on the volatility of growth of all industries, and in addition to that, it increases the volatility of industries with higher investment opportunities even more. Using a similar approach, I investigate the effect of banking concentration on the volatility of growth of sectoral level output. In order to measure banking concentration, I use the five-bank concentration ratio and Herfindahl–Hirschman index. I find that the more concentrated is the banking sector the lower is the volatility of growth of manufacturing sectors. However, I do not find enough supportive evidence to show that industries with higher investment opportunities, have a significant reduction in volatility of growth. The results of this study show that a higher banking concentration, and/or a lower banking competition would result in lower volatility of growth of industries. These results are in accordance with the previous literature. Many studies (Petersen and Rajan 1995; Cetorelli and Gambera 2001; Zarutskie 2006; Bonaccorsi di Patti and Dell’Ariccia 2004) show that more banking concentration and/or less banking competition leads to more credit access for firms. On the other hand, Larrain (2006) and Raddatz (2006) show that more private credit dampens the volatility of growth. The rest of the paper is organized as follows. A description of data and the estimation of banking competition and banking concentration indices is given in Section 2. In Section 3, I describe the methodology used in the paper. Sections 4–6, explain the empirical results using banking competition, banking concentration and both of them, respectively. Some robustness tests are shown at Section 7 and Section 8 concludes the paper.",3
37,4,Journal of Economics and Finance,28 June 2011,https://link.springer.com/article/10.1007/s12197-011-9196-5,"Dividend growth, stock valuation, and long-run risk",October 2013,Claude Bergeron,,,,Unknown,Unknown,Mix,,
37,4,Journal of Economics and Finance,06 July 2011,https://link.springer.com/article/10.1007/s12197-011-9197-4,Top performing banks: the benefits to investors,October 2013,Greg Filbeck,Dianna Preece,Xin Zhao,Male,Female,,Mix,,
37,4,Journal of Economics and Finance,20 July 2011,https://link.springer.com/article/10.1007/s12197-011-9198-3,The effects of listing changes between NASDAQ market segments,October 2013,Wenbin Tang,Hoang H. Nguyen,Van T. Nguyen,Unknown,,,Mix,,
37,4,Journal of Economics and Finance,30 July 2011,https://link.springer.com/article/10.1007/s12197-011-9201-z,The role of fixed capital depreciations for TFP growth: evidence from firm level panel data estimates,October 2013,Nicholas Apergis,John Sorros,,Male,Male,Unknown,Male,"The role of fixed capital stock depreciations has been established in the literature of investment theory and economic growth by Hulten and Wykoff (1981), Hulten (1990) and Jorgenson (1996). However, their impact on Total Factor Productivity (TFP) has not been well investigated, while the limited empirical studies have been implemented only for the case of the U.S. economy. The role of TFP accounting in growth is extremely important, considering that it determines the trend of knowledge, skills, public services and technology, which are factors crucially associated with the growth pattern of modem economies. According to OECD (1993, 2001), advanced industrial countries impose different criteria for determining fixed capital stock depreciations vis-a-vis the developing countries and imposing a unified depreciation rate is not a valid assumption, since the capital stock is highly affected by differences in the cost of its maintenance as well as by differences in new capital investments across countries (and/or industries). In other words, how fixed capital stock is generated from investment differs to a great extent across countries. Studies in the relevant literature (Easterly and Rebelo 1993; Nehru and Dhareshwar 1993) that impose uniform depreciation rates across countries seem to suffer from wrong growth accounting, which, in turn, generates misleading implications for investment and growth theories (Pritchett 2000). In addition, Howitt (1998), Musso (2004) and Mukoyama (2008) argue that faster depreciation expenses lead to a more rapid replacement of the capital stock, therefore, to more investments in fixed capital, implying higher levels of TFP, given that the new investments are characterized by higher levels of quality embodied in the newer capital goods. In other words, there is a positive relationship between depreciation expenses and TFP. Finally, Bu (2004) investigates the role of capital depreciations for developing countries and finds that in selected developing countries capital depreciations are faster than other OECD economies, leading to a lower contribution of savings, investment and foreign aid for the growth process. In this case, there is a negative association between depreciation expenses and TFP. The goal of this paper is to make use, for the first time, of firm level data on total fixed assets from a group of 25 OECD countries to illuminate any possible impact from fixed capital stock accounting depreciation expenses on TFP. Next, provided that such a statistically significant association is present and given that the employment of firm-level data takes explicitly into consideration the differentiation of the fixed capital depreciations profile, the empirical analysis will attempt to identify the sign between the two variables under investigation. The novelties of this study are: i) a unique feature of this date set that provides firm-specific observations for outputs and inputs, thus, allowing for the construction of a more reliable measure of firms’ TFP productivity, and ii) obtaining depreciation data directly from the balance sheets, which differentiates the accounting measurement of fixed capital depreciations from their economic measurement which imposes a uniform pattern across firms. Firms are given certain incentives, such as tax incentives, to impose fixed capita! depreciations in a shorter time-period than the economic life of fixed capital requires. These non-uniform depreciation rules lead to different rates of fixed capital accumulation and, therefore, to different estimates of TFP. The fact that this study does not impose a uniform partem of depreciations but it considers data on the firm level to reflect the accounting depreciations, also leads to more reliable results, since a uniform constant pattern of depreciations generates invalid estimations of capital growth, and thus, invalid estimations of TFP (Boucekkine et al. 2008). In other words, firm-level estimations of fixed capital depreciations validate our empirical findings, in a sense that the employment of accounting measurement of depreciations increases the firm’s efficiency (Kim and Moore 1988). The remaining paper is organized as follows. Section 2 provides the main literature on TFP accounting and fixed capital stock depreciations, while Section 3 describes the data used to serve the empirical analysis and the methodological approach. Section 4 describes the estimates and discusses the empirical findings, while Section 5 concludes the paper and offers certain policy implications.",3
37,4,Journal of Economics and Finance,14 June 2012,https://link.springer.com/article/10.1007/s12197-012-9238-7,Gender dynamics and smoking prevalence in Japan,October 2013,Rajeev K. Goel,Xingyuan Zhang,,Male,Unknown,Unknown,Male,"Given the heightened concerns about the ill-health effects of smoking both on smokers and non-smokers in recent years, policymakers and researchers have been quite active in examining the potential of various tax and non-tax initiatives in reducing tobacco use (see Lanoie and Leclair (1998), U.S. Department of Health and Human Services (2000)). However, most of the empirical research is devoted to a handful of nations, with the United States garnering a disproportionate share of attention (for literature reviews, see Chaloupka and Warner (2000), Gallet and List (2003), Goel and Nelson (2008) and U.S. Department of Health and Human Services (2000); and Baltagi and Levin (1986), Cebula et al. (2011) and Goel (2008) for specific applications to the United States; and Lance et al. (2004), Lee et al. (2004) and Wan (2006) for other nations). The result is that some broad determinants of smoking behavior such as the role of prices and income are now relatively well understood. However, the effects of some other influences and the smoking behavior of certain population subgroups (e.g., males versus females; elderly versus youth) are not yet fully recognized and the present research aims to contribute in this respect. Policymakers have enacted various policies to control smoking, both at a national level and at the international level. These have included price and non-price initiatives. A key recent development in the cross-national smoking control initiatives was the signing of the World Health Organization’s (WHO) Framework Convention on Tobacco Control (FCTC) by numerous countries in 2004. This multinational agreement aims to have some uniform tobacco control guidelines across nations in order to promote better health worldwide and to plug spillovers or leakages across jurisdictions with differing anti-smoking regulations (see www.who.int/fctc/en/). The FCTC also placed far reaching restrictions on the labeling and marketing of tobacco products among signatory nations. Other examples include guidelines regarding governments’ dealing with the tobacco industry, guidelines on dissemination of anti-tobacco information, etc. (see http://www.who.int/fctc/protocol/guidelines/adopted/article_5_3/en/index.html). There is limited evidence, however, on the success of FCTC in combating smoking, driven partly by its recent nature. This paper adds to the literature by focusing on smoking in Japan, especially with regard to gender smoking differences.Footnote 1 We examine whether smoking rates across different demographic groups show differing responsiveness to economic parameters. Specifically, we investigate the causes behind the changes in smoking prevalence for Japanese adults and how tobacco taxes (prices) affect the behavior of adult smoking across age and gender. Recently, the World Health Organization has also stressed the need for paying special attention towards tobacco use among females.Footnote 2 Are the determinants of smoking in Japan similar across gender and age? While there has been some research on cigarette demand in Japan (see Chaloupka and Laixuthai (1996), Goto et al. (2007), Haden (1990), Igarashi et al. (2008) and Wan (2006), Yorozu and Zhou (2002)), the present research uniquely examines smoking prevalence across Japanese demographic groups over a long period of forty years and studies the effectiveness of FCTC in shaping cigarette use in Japan. Japan was one of the first countries to sign the FCTC agreement, but little is formally known about its effectiveness. The long span of our data also includes the opening of the Japanese cigarette market to imported U.S. cigarettes in the eighties and early nineties (Chaloupka and Laixuthai (1996)). Besides contributing to the literature, the findings should provide useful policy inputs for smoking control in Japan. In 2010, there were about 95 Japanese males for every 100 females (http://data.un.org/CountryProfile.aspx?crName=JAPAN). This relatively larger proportion of females is unlike many other countries, for example India. This further justifies the gender focus of this research. In addition, attention to the elderly in Japan is especially important, given the country’s large and growing portion of this population subgroup, with elderly (aged sixty years and over) forming about a third of the overall population.Footnote 3 Results show some remarkable differences in the forces driving Japanese smoking rates across gender and age.",3
38,1,Journal of Economics and Finance,14 September 2011,https://link.springer.com/article/10.1007/s12197-011-9204-9,Euro conversion and return dynamics of European financial markets: a frequency domain approach,January 2014,Axel Grossmann,Emiliano Giudici,Marc William Simpson,Male,Male,Male,Male,"The international financial literature shows that investors, due to lower correlations between different equity markets, benefit from internationally diversified portfolios (e.g. Grubel 1968; Levy and Sarant 1970; Solnik 1974; Errunza 1983; Elton and Gruber 1995). More recent papers emphasize the time-varying nature of correlation coefficients between countries, and that diversification benefits may have been weakened as international equity markets have become more integrated (e.g. Longin and Solnik 1995; Bekaert and Harvey 1995; Bekaert and Harvey 2000; Goetzmann et al. 2005; Gupta and Donleavy 2009; You and Daigler 2010). Despite this enhanced market integration, studies show that investors can still gain from investing internationally, using a more tailored approach, depending on the investor’s country of origin and target market. (e.g. Li et al. 2003; Chiou 2008; Gupta and Donleavy 2009; Berger et al. 2011). In the same vein, one may expect that the Euro-introduction has enhanced the synchronization of equity markets within the Euro-zone due to more price and information transparency and has, thus, lowered stock market volatility and increased co-movements among Euro-zone equity markets. This may especially be the case for the equity markets of those countries that joined the Euro-zone, since a common currency may have eliminated, or at least mitigated, the systematic risk factor related to exchange rate risk. Subsequently, it may be interesting to study whether the return dynamics of European equity markets have altered with the introduction of the Euro. Studying return characteristics based on volatility and correlation alone, however, does not provide information on the dynamic changes that an event, such as the Euro-introduction, has introduced in the markets. Hence, in this paper, we use a frequency domain approach to study the return dynamics of European equity markets surrounding the introduction of the Euro. The decomposition of returns into periodic components may add more insight on the specific dynamic characteristics that are responsible for changes in volatility and co-movements; hence, it may provide additional implications with respect to information transmission among markets, market interdependence and, most importantly, for diversification. The literature documents an immediate impact of the new currency on Euro-zone interest rates and fixed income markets (Adjaouté and Danthine 2003; Hartmann et al. 2003; Rajan and Zingales 2003), yet, there is less consensus with respect to the Euro’s effect on equity markets. In fact, any impact of the Euro on European equity markets has been questioned, as currencies have traditionally played a minor role with respect to equity returns in the Euro-zone area (e.g. Adjaouté and Danthine 2003). To shed more light on the issue, studies have analyzed the impact of the Euro on equity markets with the following findings: Galati and Tsatsaronis (2003) state that the Euro has facilitated industrial sector investments rather than investment strategies based on individual European countries. Along those lines, Flavin (2004) and Moerman (2008) find that for European countries, industrial diversification benefits outweigh country diversification benefits in the post-Euro period. Others have looked more specifically at enhanced stock market integration, though with inconclusive results. For example, Holmes (2003) finds weaker evidence for stock market integration with the introduction of the Euro; while Askari and Chatterjee (2005); Kim et al. (2005); Leon et al. (2007), and Morelli (2010) report an increase in stock market integration. Bley (2009) finds an increase in European stock market integration in the period following the Euro introduction, but states that markets have started to drift apart. Guidi and Gupta (2010), using a Dynamic Conditional Correlation model, find increased correlations between Germany and several Central Eastern European equity markets that joined the EU. Green and Bai (2008) report significant abnormal returns in most European countries after the introduction of the Euro, but not for non-European countries. Furthermore, Bartram et al. (2007), using a GJR-Garch-MA model, find that the Euro has increased equity market co-movements between larger equity markets within the Euro-zone, as well as between the Euro-zone, the UK and Sweden. Haselmann and Herwartz (2010) find that the Euro-introduction has mitigated home country biases. Finally, Billio and Pelizzon (2003); Morana and Beltratti (2002), and Bartram and Karolyi (2006) study the impact of the Euro on stock market volatility. While the reduction of exchange rate volatility has been acknowledged as the factor driving formation of the European Monetary Union (Fratzscher 2002), the impact of the Euro on the volatility of European equity market returns seems to be less than conclusive. For example, Bartram and Karolyi (2006) show that the Euro-introduction has caused an increase in total stock return volatility, though to a lesser degree, for companies that have more Euro exchange rate exposure. Billio and Pelizzon (2003) use a switching two-regime beta model, and find that Germany and Spain’s stock market volatility has increased after the EMU through the idiosyncratic risk factor in the low volatility regime. On the other hand, they report a decrease in the volatility of the Italian stock market through the idiosyncratic risk factor in the high volatility regime. Further, Morana and Beltratti (2002) study the German, French, Italian, Spanish, British and American equity markets using a GARCH model, and find no impact by the new currency on stock market volatility. However, using a Markov three-regime model, they show that the Euro initially increased volatility among all European stock markets, though it has eventually reduced volatility in Spain and Italy’s stock markets. Additionally, they find that the Euro-introduction had no effect on US stock market volatility. Hence, while the literature seems to concur on a more stabilized Italian stock market, the findings with respect to the Euro’s general impact on the equity return dynamics seem to be ambiguous. This paper differs from previous studies as it employs a frequency-domain approach to analyze the effect of the introduction of the Euro on the volatility of the equity markets of 12 Euro-zone member countries, as well as the UK, the US and Japan; hence, it goes beyond the analysis of second moments. Orlov (2006) shows that a conventional time-domain approach, using the series’ variances, can be misleading, because the time-series approach lumps all variability into a single number (i.e., a series’ variances). He suggests that a frequency domain approach such as spectral analysis should be used when comparing volatilities. Spectral analysis is a decomposition of a series into a weighted average of nonlinear periodic patterns of different frequencies. These components can morph to fit a very large array of nonlinearities of the series, and can provide information that goes beyond what one can detect by analyzing variance or even higher moments of the series. To illustrate the advantage of spectral analysis over traditional methods of comparing changes in volatility, consider the following example. Suppose a series is characterized by white noise and a monthly pattern. Further suppose that the monthly cycle increases, while the white noise component decreases. If those changes occur at the right rate, the variance of the series can be identical despite the considerable change in dynamics. Spectral analysis captures both changes in overall volatility and shifts in patterns. Another advantage of spectral analysis is that the methodology is completely data-dependent, while traditional time series analysis requires the researcher to specify a model a priori, potentially leading to specification errors, especially if the series contains complex non-linearities. Our analysis primarily entails an examination of the spectra of returns of various equity indexes over different periods of time. This allows us to gather insights on what patterns are responsible for changes in volatility rather than simply assessing whether volatility has changed. The spectral analysis lets us disaggregate the return series into its different periodic components and allows us to determine the relative importance of the different components in creating the original time series. The relative weights of the components in a given time series are called a series’ spectrum; the estimate of a spectrum is called a periodogram, which can be plotted in a graph called a spectral graph. Thus, based on plotted spectral graphs, we discern if changes occurred in the return dynamics of several countries’ national stock indexes concomitant with the Euro conversion. This is carried out on two levels. First, we assess if the return dynamics for each individual index is different before and after the Euro conversion. Second, we compare the dynamics across the national stock indexes to determine the similarity of the dynamics before and after the Euro conversion, allowing us to document whether or not the latter period is characterized by greater similarity in return dynamics. Additionally, we analyze the American, Japanese, and British equity markets to establish a benchmark. Investigating whether the dynamics of the UK have altered after the introduction of the Euro may provide important implications with respect to the decision as to whether the UK will join the Euro-zone in the near future. Graphing the spectra obtained from the frequency domain analysis, we find the following results. First, prior to the Euro-introduction the spectra of most Euro-zone equity markets exhibit different dynamics. Yet in the immediate aftermath of the introduction of the Euro, we find a reduced volatility over all frequencies in which no strong cyclical components are present. However, in the long run the largest European equity markets experience an increase in volatility and their dynamics are characterized by two identical patterns. Second, while these patterns are not present in the smaller Euro-zone equity markets, we find that the dynamics of the UK’s returns becomes remarkably similar to that of the larger Euro-zone equity markets in the post-Euro period. Footnote 1 Third, before the introduction of the Euro, the dynamics of the European equity markets shared patterns similar to those found in the US, while after the introduction of the common currency these similarities have decreased. Japan, also included in the study, is found to maintain its own dynamic characteristics. These findings provide important implications for investors seeking international diversification benefits. The remainder of the paper is organized as follows: Section 2 discusses the frequency domain approach employed in this study. Section 3 discusses the utilized data and the summary statistics. Section 4 presents the results. Finally, Section 5 concludes the study.",1
38,1,Journal of Economics and Finance,22 September 2011,https://link.springer.com/article/10.1007/s12197-011-9202-y,The influence of industry concentration on merger motives—empirical evidence from machinery industry mergers,January 2014,Florian Geiger,Dirk Schiereck,,Male,Male,Unknown,Male,"The influence of market structure on corporate performance and merger success has long been emphasized by industrial organization theory. However, little is known about its specific impact on merger motives. Following recent research recommendations, we examine how stock prices react to transaction announcements of merging firms and rival companies in order to determine how industry concentration influences merger decisions and motives. The motive for a firm to engage in a merger usually comes from the market environment. But the decision to merge usually arises from an equilibrium in product markets that potentially reflects strategic interactions among market participants. The structure of product markets can affect such things as the riskiness of a firm’s cash flows and its investment decisions. A firm’s underlying rationale to engage in mergers is influenced correspondingly. We argue, similarly to Hou and Robinson (2006), that industry concentration influences the risk behavior of firms. We are thus further developing the traditional views of Schumpeter (1912) and Bain (1954), who both argued that industry concentration influences innovation dynamics and a firm’s distress risk, and directly impacts the process of creative destruction. We then link that line of reasoning with industrial organization theory, which highlights how endogenous market conduct is influenced by exogenous market factors. We argue that this influence should also be reflected in the strategic merger motives of firms. For example, as innovation dynamics and efficiency pressure increase in competitive industries (Knott and Hart 2003), mergers in this environment should be motivated primarily by efficiency gains from operational, managerial, and financial synergies. In contrast, mergers in concentrated industries should be driven primarily by monopolistic collusion motives, because limiting output, raising product prices, and/or lowering factor prices are more promising efficiency methods in that environment (Chatterjee 1986). The capital markets should recognize this impact of industry concentration by evaluating each type of transaction differently. We explore three lines of reasoning to explain merger motives: 
Efficiency theory, which posits that mergers are motivated by synergies, and that wealth creation depends on the operational and strategic fit of both companies. 
Monopolistic collusion theory, which argues that mergers are executed to improve market positioning and to achieve market power. 
Agency and hubris theories, which assume either agency problems in the form of wealth transfers between acquiring and target shareholders, or the hubris of management through the overestimation of potential synergies and overpayment for the target. We indirectly test for the efficiency, collusion, and hubris hypotheses under two different market structure conditions (concentrated and fragmented) by examining the wealth effects of the target, acquirer, combined entity, and rival firms (Eckbo 1983; Stillmann 1983; Trautwein 1990; Fee and Thomas 2004; Shahrur 2005). We also compare correlations between acquirer excess returns and change in industry concentration (Ghosh 2004). By following this method, we are able to examine how specific industry characteristics influence management merger motives. Our sample is comprised of 330 M&A transactions from the machinery industry from 1997 through 2007. We focus on machinery for three specific reasons. In reviewing empirical research, we believe a cross-industry analysis would fall short of properly capturing specific industry-related developments and trends. In fact, general market trends across industries (e.g., banking, telecommunications, pharmaceuticals) are difficult to compare.Footnote 1 By concentrating on only one industry and assuming a basic homogeneity of prevailing market trends and developments, we can overcome the methodological shortcomings of broader empirical studies. In contrast to other industries, the machinery industry is highly fragmented, which provides sufficiently heterogeneous market structures to allow a thorough examination of how industry concentration influences merger motives. In the standard industrial classification (SIC) system, there are thirty-seven separate subindustries for machinery, of which nine are classified as concentrated (Herfindahl-Hirschman index >0.18).Footnote 2 The U.S. Census Bureau (2002) confirms the market fragmentation, stating that the largest twenty machinery manufacturers in the U.S. control an approximately 27.8% aggregate market share, compared to 56.3% for commercial banking and 78.8% for telecommunications. The machinery industry is a main driver of the economy and one of the most important sectors worldwide, with a global production output of more than U.S. $1,850 billion in 2007 (Bureau of Economic Analysis 2008; xVDMA 2008). Note that we would expect the importance of this industry alone to have catapulted it into the academic spotlight. However, until now, there has been only limited research in this area. Thus, we believe machinery is a particularly suitable industry from which to examine our research question. Using univariate and cross-sectional analysis, we find support for our hypothesis that industry concentration influences merger motives of firms. By interpreting capital market reactions to transaction announcements, we observe significantly different abnormal return reactions for rival firms in fragmented versus concentrated industries. Complementing abnormal return analysis with the correlation approach developed by Ghosh (2004), we reject monopolistic collusion motives for concentrated industries, because the observed negative rival reaction can only be explained by competitive disadvantages that would result from efficiency gains of the merging firms. In fragmented industries, we find evidence of both productive efficiency motives and monopolistic collusions motives. There are no indications of agency or hubris motives, however, because acquirer returns are positive and we observe no wealth transfers between shareholders. The results from our cross-sectional analyses confirm the direct influence of the contingency variable “industry concentration.” Furthermore, our findings indicate that the wealth creation mechanics of the acquirer, target, and rivals also differ for transactions in concentrated versus fragmented industries. Considering the general value-maximizing behavior of management, those differences should lead to different merger motives and strategies. The remainder of this paper is structured as follows. In Section 2, we provide an overview of the current strand of merger motive research, before illuminating the ideas behind industrial organization theory. To summarize those findings, we derive our research hypotheses. In Section 3, we provide descriptive statistics for our sample, identify explanatory factors that can explain the observed wealth effects of the transactions, and discuss our econometric model. In Section 4, we conduct univariate and multivariate analyses to identify the determinants of wealth effects and their impact on merger motives. More specifically, we examine whether excess transaction returns in fragmented or concentrated industries have different cross-sectional determinants of merger success. Section 5 concludes with a discussion of our findings.",10
38,1,Journal of Economics and Finance,21 October 2011,https://link.springer.com/article/10.1007/s12197-011-9209-4,The impact of governance characteristics on the stock price of cross listed companies,January 2014,Inga Chira,,,Female,Unknown,Unknown,Female,"Previous studies investigate the valuation effects of international cross-listings. However, they do not focus on country-specific effects as related to governance characteristics. This study fills the gap in the prior literature by assessing the degree of influence of country-specific governance indicators on the stock price of cross-listed companies. The study also examines if and, to what extent, the impact of country-specific governance characteristics on the abnormal returns of cross-listed foreign firms is influenced by the adoption of the Sarbanes-Oxley Act (SOX). The main motivation behind the current study is the bonding hypothesis introduced by Coffee (Coffee 1999 and Coffee 2002), who argues that foreign companies by entering the U.S. market subject themselves to the stricter regulatory system of the SEC and, as a result, achieve higher governance standards. Bonding is, therefore, most desirable for foreign companies headquartered in countries with weak protection of shareholder rights as well as weak enforcement of market rules and laws. Such a voluntary “migration” from lower market and legal standards to more rigorous ones should generate positive results both for cross-listed firms and their investors, as captured by higher returns (Miller 1999) and a lower cost of capital (Stulz 1999). The effect of cross-listing on the stock price has not been researched from the perspective of country-wide governance characteristics. At this point, it is still not clear if abnormal returns can be predicted for companies that decide to cross-list based on specific country characteristics beyond the origin identified by Miller (1999) (emerging versus developed). I attempt to assess the impact of the governance indicators of the country of origin for foreign companies that have elected to cross-list in the U.S. since 1996. The main purpose of the study is to investigate the existence as well as the extent of the impact of cross-listing on the stock price of foreign firms that choose to cross-list in the U.S. An ancillary motive is to identify those governance characteristics that have the strongest impact on the market response to the announcement of cross-listing by foreign firms. By identifying those characteristics, this study attempts to establish which companies, i.e., from which countries with particular governance characteristics, are likely to benefit the most from cross-listing on the U.S. markets. Further, I make a number of contributions to the cross-listing literature. First, I extend the previous studies by employing a much bigger sample of cross-listed foreign firms which covers the 1996–2009 period. For example, Miller’s study is based on the data up to 1996. Since cross-listings on the U.S. exchanges have been dominated by firms from emerging economies, in particular beginning in 1996, it is of paramount importance to include those firms in the sample of companies that cross-list in the U.S. Second, prior studies on the impact of SOX on cross-listing have only concluded that the adoption of SOX has generally been detrimental to the volume of cross-listing, but they have not assessed how that impact varies across the sample of firms that cross-list before and after SOX. The current study looks at the difference between the abnormal returns derived for cross-listed foreign firms in the pre- and post-SOX period to find out if those returns differ significantly and if that difference (if any) can be explained by the bonding hypothesis. Generally speaking, the overall findings of the study are consistent with those of the prior literature and show that foreign companies tend to elicit a positive market response when cross-listing on the U.S. exchanges. The abnormal returns experienced by foreign cross-listed firms are experienced in the short run and persist in the long run. In addition, the new contribution to the cross-listing literature is the finding that country-specific governance characteristics can account for the abnormal returns around the cross-listing announcement better than firm-specific characteristics. The results support the bonding hypothesis and, thus, demonstrate that there is an inverse relationship between the country-specific governance indicators of the companies that cross-list in the U.S. and the cross-listing returns. Worse country-specific governance determinants for cross-listed firms result in higher positive post-listing abnormal returns for those firms. Finally, country-specific governance characteristics explain significantly more of the variation in the abnormal returns of cross-listed firms than the firm-specific characteristics of those firms. The significance of country-specific governance characteristics holds even when company-specific control variables are included in the analysis. The remainder of the paper is structured in the following way: section 2 reviews the existing literature on the subject, section 3 presents the hypotheses, section 4 describes the data and method, section 5 presents the results, section 6 offers robustness checks, and section 7 concludes.",4
38,1,Journal of Economics and Finance,05 November 2011,https://link.springer.com/article/10.1007/s12197-011-9212-9,Budget deficits and real interest rates: a regime-switching reflection on Ricardian Equivalence,January 2014,Daniel F. S. Choi,Mark J. Holmes,,Male,Male,Unknown,Male,"The potential impact of budget deficits on long-term real interest rates is a key concern for policy-makers. The traditional view emphasizes a positive impact as a direct result of a budget deficit reducing the pool of national savings thereby raising interest rates and crowding out investment by the private sector. In contrast, the Ricardian Equivalence theorem (RET) espoused by Barro (1974) then considered further by Barro (1989), Evans (1988), Gale and Orszag (2004), Elmendorf and Mankiw (1999), Seater (1993) and Kormandi and Protopapadakis (2005) and many others argues that for a given path of government spending, a deficit-financed cut in current taxes leads to higher future taxes that have the same present value as the initial cut. If household demand for goods depends on the expected present value of taxes insofar as this present value is subtracted from the expected present value of income to determine a net wealth position, budget deficits have no impact on interest rates. Moreover, a decrease in government saving (a current budget deficit) leads to an offsetting increase in desired private saving, and therefore no change in desired national saving. This is because infinitely-lived agents are rational to the extent that they will offset higher future taxes required to repay the current deficit by increasing private savings today, canceling out the impact of current budget deficits on current interest rates. Numerous theoretical questions can be leveled at the working assumptions that underpin the RET. These questions are related to whether capital markets are perfect, the foresight and planning horizons of agents,Footnote 1 altruistic intergenerational transfer motives and so on. Given the theoretical debate, the potential relationship between the budget deficit and real interest rate is an empirical issue that has stimulated many studies. In the case of the US, strong evidence of a positive linkage is provided by a range of studies led by the work of Cebula (see, for example, Cebula (2003, 2005)), and Cebula and Pablo (2010). In contrast, Kliesen and Schmid (2004), Findlay (1990) and Evans (1985, 1987a and b) find against such a linkage. Footnote 2 While Hartman (2007) concludes that the linkage is unclear, Allen and Wohar (1996) suggest that it is in fact regime-dependent and applicable only to the post-1981 period which followed key fiscal changes introduced by the Reagan administration. Mandal and Payne (2007) are able to reject the null of non-cointegration between public and private savings once allowance is made for an endogenous break in the cointegrating relationship. In the context of the EU, Cuaresma and Reitschuler (2007) find evidence of a change in the fiscal behavior of individuals during the last decade after the introduction of a fiscal rule, namely the Maastricht criteria. A further nonlinear perspective is also offered by the work of Nickel and Vansteenkiste (2008) who provide evidence that households in very high-debt countries tend to become Ricardian. In this study, we test the validity of the RET for the US. In doing so, our contribution to the debate is on two fronts. First, our testing procedure is based on the application of a Markov regime-switching approach. The above-mentioned studies employ a mixture of methodologies that include non-cointegration testing and error correction analysis, VAR modeling and IV methods. In contrast to this, our approach allows us to explicitly address the types of nonlinearity issues raised by Allen and Wohar (1996) and Nickel and Vansteenkiste (2008). We investigate whether the US economy has experienced sub-periods where the RET has held in terms of being in a specific regime, and whether the probability of switching regimes has been influenced by drivers that include cyclical output, international openness or economic and financial integration, federal debt and degree of debt monetization. The second contribution of our study is in terms of the US dataset we analyze. Our data are annual and cover a study period of over two centuries from 1798 to 2009. This is in contrast to Evans (1985) who analyzes US data back to 1858 and the other US-based studies that are almost entirely concerned with analyzing the post-World War II period. The structure of the paper is as follows. The following section outlines the methodology and describes the dataset. The third section reports and discusses the results. We find evidence that the US economy switches between two regimes where one is supportive of the RET while the other is supportive of the traditional view of a positive relationship between interest rates and deficits. The final section offers some concluding thoughts.",9
38,1,Journal of Economics and Finance,28 October 2011,https://link.springer.com/article/10.1007/s12197-011-9210-y,Interest-rate and calendar-time effects in money market fund and bank deposit cash flows,January 2014,Vladimir Kotomin,Stanley D. Smith,Drew B. Winters,Male,Male,Male,Male,"Money markets are markets for trading liquidity (see Blackwell et al. (2007)) where investors with temporary cash surpluses provide short-term loans to borrowers with temporary cash shortages. The temporary nature of the cash surplus means the investor has an identified cash obligation for the funds and therefore is unwilling to put these funds at risk because the funds are needed. Money market funds (MMFs) and bank deposits are the most liquid investment vehicles with extremely low risk. Blackwell et al. (2007, p. 304) suggest that earnings are not the primary goal of a MMF. Instead, for a MMF, earnings are secondary to the preservation of capital and the ability to provide liquidity when needed. Since earnings are a likely secondary objective, we examine whether money market investors move their funds in and out of these accounts to take advantage of higher interest rates. Currently, there are two separate streams of research on money market investor behavior. One provides evidence of calendar-based effects, while the other provides evidence of preferences for higher interest rates. The calendar-based money market literature reaches back to at least Park and Reinganum (1986), who find that the last T-bill to mature in a month trades at a yield discount to adjacent bills, with the last bill to mature in a calendar year having the largest discount. Ogden (1987) describes the Park and Reinganum calendar-based regularity as a preferred habitat for investors (the preferred habitat hypothesis stems from Modigliani and Sutch (1966)) with calendar-based cash obligations. Griffiths and Winters (1997) further describe the calendar-based regularity in money markets as a preferred habitat for liquidity: Money market investors schedule their investments to mature in time to return their stored liquidity to pay their cash obligations. Griffiths and Winters (1997) find a year-end preferred habitat for liquidity in short-term repos. Griffiths and Winters (2005) find that the year-end preferred habitat for liquidity extends across US money markets, while Kotomin et al. (2008) find it in LIBOR for U.S. Dollar, Euro, Yen, Swiss Franc, and German Mark. Kotomin et al. (2008) also find some evidence of quarter-end preferred habitats in LIBOR. Farinella and Koch (2000) examine MMF cash flow patterns and find calendar-time effects due to liquidity preferences around quarter-ends and tax dates. Bank deposits are a substitute for money market investments. Kotomin and Winters (2006) suggest that bank quarter-end balance sheet changes are related to bank customers’ preferred habitats. This body of literature suggests that calendar-based preferred habitats for liquidity permeate money market instruments and their substitutes. The preferred habitat literature focuses on calendar time effects and shows that money market investors are willing to forego some return to strategically time their cash flows to meet calendar-based cash obligations.Footnote 1 Accordingly, literature on investors seeking higher returns in the money markets is limited. Lyon (1984) reports that some institutional MMF investors are willing to arbitrage away the under- and overvaluation created by the use of a particular valuation technique, amortized cost valuation. Koppenhaver and Sapp (2005) find that some investors move their money between U.S. Treasury money funds and direct investment in Treasury bills in response to changes in fund’s performance and fees. We contribute to this literature by providing an examination of the effect of interest rate changes on MMF and bank deposit flows while controlling for calendar-time effects. Lyon (1984) examines whether institutional MMF investors engage in arbitrage made possible by the use of the amortized cost valuation technique by MMFs. We believe the question deserves to be revisited for several reasons. First, Lyon (1984) used only 2 years of weekly data in the early 1980s, when MMFs were in their infancy. Second, we also study retail MMF and bank deposit balance changes in addition to those of institutional MMFs. Third, MMDAs (money market deposit accounts), which may be viewed as the closest substitute for retail MMFs, were introduced after the Lyon’s study period ended. Accordingly, we study the effect of changes in short-term interest rates on aggregate balances of money market funds (MMFs) and their potential substitutes—different types of bank deposits—while controlling for the money market calendar-time effects identified in prior studies. We find calendar-time effects consistent with investors' preferences for liquidity around year-ends, quarter-ends, and tax payment dates. Consistent with Lyon (1984), we find that institutional MMF investors take advantage of arbitrage opportunities between returns on MMFs and alternative investments. Retail MMF flows are substantially less sensitive to interest rate changes than institutional MMF flows. In addition, we find that time deposits, both large and small, vary directly with short-term interest rates. Footnote 2
 Section 2 describes the data and methods used. Section 3 reports and discusses empirical results. Section 4 concludes the paper.",
38,1,Journal of Economics and Finance,04 September 2011,https://link.springer.com/article/10.1007/s12197-011-9206-7,OPEC and political considerations when deciding on oil extraction,January 2014,Khalid Kisswani,,,Male,Unknown,Unknown,Male,"OPEC represents a considerable political and economic force. Two-thirds of the oil reserves in the world belong to OPEC members; likewise, OPEC members are responsible for half of the world’s oil exports. According to its statutes, OPEC’s main goal is to unify and protect the interests of oil-producing countries. It allows oil-producing countries to guarantee their income by coordinating policies and prices among members. OPEC is the best known cartel in the world which controls the oil market by limiting production, thus creating “shortages” and forcing oil prices up. Oil is the world’s main source of energy, and its price has a substantial impact on all economies. Crude Oil prices ranged between $2.50 and $3.00 from 1948 through the end of the 1960s. By the end of 1973 the price of oil had quadrupled to over $12, when the Arab oil producers imposed the 1973 oil embargo against the U.S. and Western Europe (supported by production cuts from other OPEC members). After that, prices stayed flat, ranging from $12.21 to $13.55 per barrel until 1979, when the Iranian revolution occurred. The combination of the Iranian revolution and the Iraq-Iran War caused crude oil prices to more than double, increasing from $14 in 1978 to $35 per barrel in 1981. These higher prices resulted in increased exploration and production outside OPEC. Between 1980 and 1986, non-OPEC production increased 10 million barrels per day. This caused crude oil prices to drop below $10 per barrel by mid-1986. The price of crude oil spiked in 1990 due to what was known as the first “Gulf War”. However, after the war, crude oil prices entered a period of steady decline until 1994. After that, prices started to recover again until 1997 (price of crude oil reached $25), because the U.S. economy was strong and the Asian Pacific region was booming, but then prices dropped to $16 in 1999. By the end of 2001, oil prices started to steadily increase, reaching $40–50 by the end of 2004. Then crude oil prices surged to a record high above $60 in June 2005, over $77 in July 2006, above $90 in October 2007, and reaching a new record high of $147 in July 2008. However, since then, oil prices have declined. Figure 1 summarizes the time trend in oil prices. Major Events and Real World Oil Prices, 1970–2007 When we look at the trend of oil prices over the past years, one would argue that OPEC is responsible for most of these increases due to their production cuts and market power. Others believe that OPEC is not to be blamed for the price increases, since OPEC’s ability to control the price of oil diminished somewhat after the 1973 oil crisis, due to the subsequent discovery and development of large oil reserves in the Gulf of Mexico and the North Sea, the opening up of Russia, and market modernization. There is a rich literature on the process of oil price determination, but the question of whether OPEC is a cartel or not remains hotly debated. Economists have devoted much attention to this issue, since it reflects different arguments regarding OPEC’s role in determining oil prices. Some economists have assumed OPEC as a monopolist and others use models that regard OPEC as a dominant firm.Footnote 1 In this paper, OPEC is assumed to act as a dominant firm, and non-OPEC countries are regarded as fringe firms. The main concern here is to explore the objective function for OPEC, and how OPEC determines the time path of production. This is done by applying optimal control theory in studying the dynamics of oil prices and therefore the time path of quantities extracted. We develop a model that explores the objective function of OPEC, which includes economic as well as political considerations. We determine the optimal extraction quantities of oil under this model, and compare them to actual quantities to check the validity of the model. Also we provide policy guidance to decision makers around the world. The paper aims to contribute to a further understanding of the real dynamics of OPEC production behavior and its impacts on the world oil market. It contributes to the literature in several ways. From a technical standpoint, one of the important additions is the estimation of the residual demand curve for oil that OPEC faces in the market. Previous economic studies that have applied optimal control theory have not used any specific estimated demand equation, but rather employed general demand equations to show analytical solutions. Non-optimal control models though did estimate different types of demand functions, using lagged prices and/or lagged quantities. However, these exercises in econometrics appear to have little or no foundation in traditional demand theory. It also provides estimates of the oil production cost function for OPEC members, which has not been done previously. The estimation of both the demand function and the cost function for OPEC allows one to apply the mathematical model that is developed here using optimal control theory. Finally, another important addition is the test of political economic considerations. The test looks at the hypothesis that OPEC has another objective, beyond just maximizing profits, in deciding on extracted quantities of oil. This other goal is the desire to achieve political support among citizens of OPEC countries. This support comes from the level of harm or damage to western countries’ economies, by affecting the world oil market through high oil prices. The remainder of the paper is structured as follows. Section 2 reviews the literature. Section 3 describes the theoretical model and assumptions used in estimating the residual demand for oil and the cost function for OPEC. Section 4 describes the theoretical optimal control model and the solution of this model. Finally, Section 5 summarizes the main findings of the research and offers some concluding remarks.",
38,1,Journal of Economics and Finance,28 February 2012,https://link.springer.com/article/10.1007/s12197-012-9228-9,A nonparametric approach to market timing: evidence from Spanish mutual funds,January 2014,José Alvarez,Laura Andreu,José Luis Sarto,Male,Female,Male,Mix,,
38,1,Journal of Economics and Finance,04 December 2011,https://link.springer.com/article/10.1007/s12197-011-9217-4,"Hedge funds, fund attributes and risk adjusted returns",January 2014,Gökçe Soydemir,Jan Smolarski,Sangheon Shin,,Male,Unknown,Mix,,
38,1,Journal of Economics and Finance,02 December 2011,https://link.springer.com/article/10.1007/s12197-011-9214-7,Similarities in fan preferences for minor-league baseball across the American Southeast,January 2014,Tyler Anthony,Tim Kahn,Andrew Weinbach,,Male,Male,Mix,,
38,1,Journal of Economics and Finance,08 September 2011,https://link.springer.com/article/10.1007/s12197-011-9207-6,Empirical analysis of the impact of cigarette excise taxes on cigarette consumption: estimates from recent state-level data,January 2014,Richard J. Cebula,Maggie Foley,Robert Houmes,Male,Female,Male,Mix,,
38,2,Journal of Economics and Finance,05 October 2011,https://link.springer.com/article/10.1007/s12197-011-9203-x,Hedge funds versus private equity funds as shareholder activists in Germany — differences in value creation,April 2014,Mark Mietzner,Denis Schweizer,,Male,Male,Unknown,Male,"It is well known that the separation of ownership and control can lead to agency problems when managers use investor funds to finance investment projects (Coase 1937; Jensen and Meckling 1976). Because managers control the capital, they may not act in the best interest of shareholders if the company does not impose a concentrated ownership structure. Numerous studies have addressed this problem and suggested mechanisms for managing it.Footnote 1 For example, Grossman and Hart (1980) and Shleifer and Vishny (1986) have posited that large shareholders should take on a monitoring role.Footnote 2
 Prior research has analyzed the effectiveness of large shareholders’ monitoring activities, as well as their ability to directly influence corporate policy.Footnote 3 In theory, large investors can be very effective at solving agency problems. However, empirical research has found ambiguous evidence of successful changes in corporate policy. Previous research usually distinguished between active and passive blockholders when considering their effects on corporate policy. However, Cronqvist and Fahlenbrach (2009) argue that activist shareholders can differ greatly from each other. They show that specific groups of active shareholders seem to be more successful at making changes in investment policy, financial policy, and operations. They conclude it is necessary to consider the presence of large and active blockholders, but it may be even more vital to determine who they are. Given blockholders’ distinct ability to change corporate policy, we believe capital markets should react to engagements of financial investors who actively address agency problems in their target companies (Barber 2007). However, determining the impact of specific investors also requires considering the corporate governance system in which the targets are located. For example, firms in continental Europe do not tend to share a pure shareholder value orientation; rather, they are concerned about the interests and requirements of all stakeholders in the firm, including employees, banks, suppliers, and customers. In Germany, this stakeholder orientation is apparent in the co-determination concept and in the composition of the supervisory board. In such an environment, it might be more difficult for active blockholders to be heard and to address the above-mentioned agency problem. Thus, this paper empirically investigates the valuation effects associated with hedge fund and private equity engagements in target firms in Germany, and the subsequent change in ownership structure. To the best of our knowledge, this is the first study that relates market valuation effects to an active investor’s ability to mitigate agency conflicts in a stakeholder-oriented environment. Hence, this paper contributes to the still limited body of literature on the market for corporate control by exploring the impact specific blockholders can have on a corporate governance system and its resistance to change. It is obvious that both hedge funds and private equity funds seek to increase the market value of their pooled capital. Active engagement in publicly traded companies is a relatively recent popular strategy. Both hedge funds and private equity investors may be active shareholders, and their engagement may change a firm’s objectives and value. However, their level of activism can differ substantially, depending on the level of funding and the management reimbursement structure of the funds. For example, hedge funds can face the problem of capital withdrawals by investors if they underperform. Partnoy and Thomas (2007) find that hedge funds tend to trade more frequently than other institutional investors, which might be attributable to their funding structure. On the other hand, private equity funds are less subject to capital withdrawals, and can thus focus on longer investment horizons. With respect to management compensation, it is important to note that hedge funds calculate their performance fees on unrealized capital gains, while private equity fund fees (carried interest) are derived solely from realized capital gains. This can lead to the notion that hedge fund investors have a strong preference for short-term and trading-induced profits, which may translate into an aggressive activist strategy in their portfolio companies. Given their various investment strategies and specific organizational setups, however, we know nothing about how these new institutional investors would perform in a stakeholder-oriented economy. For our study, we construct a unique hand-collected data set of 159 private equity and 67 hedge fund engagements in German exchange-listed companies between 1993 and 2007. We apply a standard event study methodology to analyze whether the engagement of these specific active investors is associated with an increase in shareholder value. We also examine whether increases in stock returns are related to several corporate characteristics and market variables, and whether those effects persist over time. We first find that significant positive abnormal returns of around 4.5% are triggered by an announcement that a hedge fund or a private equity fund has acquired at least 5% of a company’s voting rights. However, our cross-sectional results reveal that only private equity fund managers successfully address agency problems in their target firms. We believe this is due to their longer-term investment perspective and a higher adaptability to the local stakeholder-oriented corporate governance system as compared to hedge fund managers. Second, we find that the long-lasting return drift to hedge fund and private equity target firm shareholders is significantly negative and statistically lower in magnitude for hedge fund targets. Over a 250-day period, the median buy-and-hold abnormal returns (BHARs) are −2.47% for private equity targets and −21.46% for hedge fund targets. We provide two explanations for this empirical finding. First, both active investors operate in a distinct stakeholder environment, under co-determination and with other stakeholder groups represented on the supervisory board. Hedge funds may find it difficult to align their interests with advisory board members. Therefore, we believe that the negative post-announcement stock performance of hedge fund targets may be a result of the capital markets misinterpreting hedge funds’ investment intentions. It seems likely that a negative benchmark-adjusted performance is the result of an (initially) expected but (eventually) not realized reduction in agency costs. Second, we believe there may be general disappointment in the case of rampant speculation about a possible takeover and the related premium that does not ultimately occur (Greenwood and Schor 2009). The rest of the paper proceeds as follows. In section 2, we differentiate among hedge funds and private equity funds with respect to their incentives and capabilities to reduce agency costs. Section 3 describes our data set, while section 4 provides our empirical methodology and research design. Our results are reported in section 5, and section 6 provides our conclusions.",36
38,2,Journal of Economics and Finance,07 October 2011,https://link.springer.com/article/10.1007/s12197-011-9208-5,An empirical analysis of the Carbon Financial Instrument,April 2014,Omid Sabbaghi,Navid Sabbaghi,,,Male,Unknown,Mix,,
38,2,Journal of Economics and Finance,01 December 2011,https://link.springer.com/article/10.1007/s12197-011-9211-x,Investigating the PPP hypothesis using constructed U.S. dollar equilibrium exchange rate misalignments over the post-bretton woods period,April 2014,Axel Grossmann,Marc W. Simpson,Teofilo Ozuna,Male,Male,Male,Male,"The collapse of the Bretton Woods system in 1973, which lead to the floating of foreign exchange rates, has resuscitated, inter alia, the search for the appropriate fundamental model of exchange rate determination. The concept of a fundamental model, of necessity, implies the existence of equilibrium and the idea of equilibrium intrinsically implies the possibility of disequilibrium. Furthermore, the extent to which exchange rates are misaligned, or in disequilibrium, may be of critical importance to both policy makers and business executives. A general definition of exchange rate misalignments is the deviation of the exchange rate from its equilibrium. While a broad consensus exists on how to define exchange rate deviations, there seems to be less agreement on how to construct a proper equilibrium exchange rate (EER). The difficulty in estimating the equilibrium is that the true EER, as such, is never observable. Consequently, the literature generally evaluates the appropriateness of any EER by the reverting behavior of the actual spot exchange rate towards the constructed equilibrium and/or the ability of the EER to predict the future path of the actual spot exchange rate. When constructing EERs many economists rely on relative purchasing power parity (PPP), which generally means the construction of the real exchange rate.Footnote 1
 The literature puts forward different methods to analyze the PPP hypothesis. One strand of studies tests if the real exchange rate is stationary (e.g. Meese and Rogoff 1988; Mark 1990; Xu 2003; and Narayan 2005, 2006). Other methodologies are the Engle and Granger (1987) cointegration test (see for example Karfakis and Moschos 1989; Kim 1990; Patel 1990; Bleaney 1991) or the Johansen (1991) cointegration test (see for example Cheung and Lai 1993; Salehizadeh and Taylor 1999). The empirical evidence provided from those studies suggests that PPP holds in the very long-run, while there is little evidence to support PPP in the short-run, and particularly fails to hold during the current, rather short, period of floating exchange rates (e.g. Breuer 1994; MacDonald 1995; Froot and Rogoff 1995; Rogoff 1996; Lothian and Taylor 1996; Taylor 2002; and Taylor and Taylor 2004). The literature suggests that tests fail to reject the null hypothesis of a unit root or no-cointegration due to the low power of the tests as well as the short-time period after the Bretton Woods system (Frankel 1990; Froot and Rogoff 1995; Lothian and Taylor 1997; Taylor and Taylor 2004). Based on the above findings, researchers have applied panel data in an effort to mitigate the low power of the tests, with mixed results. For example, Frankel and Rose (1996), Jorion and Sweeney (1996), MacDonald (1996), Oh (1996), and Kalyoncu and Kalyoncu (2008) find stronger evidence in favor of PPP. The stronger support, however, seems to vanish if one controls for serial correlation Papell (1997) and Papell and Theodoridis (1998), or contemporaneous correlation O’Connell (1998) in the panel data estimation. Moreover, Taylor and Sarno (1998) and Karlsson and Lothgren (2000) emphasize that studies using heterogeneous panel data containing one stationary series and otherwise nonstationary series are more inclined to find evidence in favor of PPP. Perron (1989) and Perron and Vogelsang (1992) demonstrate the importance of incorporating structural breaks when testing the unit root hypothesis. Yet, studies that account for structural breaks in univariate time series of the real exchange rate are only able to reject to null of a unit root for some currencies (e.g. Serletis and Zimonopoulos 1997; Narayan 2005). Others have combined the two strands of literature presented above by applying panel unit root tests with structural breaks and find strong evidence in favor of PPP (e.g. Papell 2002 and Narayan 2006). More current papers find evidence that the real exchange rate follows a non-linear mean reverting behavior (e.g. Obstfeld and Taylor 1997; Taylor et al. 2001; Sollis et al. 2002; Kilian and Taylor 2003; Sarno et al. 2004; Sollis 2008; Peltonen et al. 2009; McMillan 2009). They reason that the tendency of the real exchange rate to converge to its mean strengthens if the deviations become larger. Thus, the real exchange rate is commonly modeled within two regimes; an inner regime that defines a random walk and an outer regime that defines the mean reverting behavior of the real exchange rate. For example, Taylor et al. (2001) use an exponential smooth transition autoregressive (ESTAR) model and report half-lives of less than 3 years. Peltonen et al. (2009), who use panel data and a nonlinear productivity augmented ESTAR, as well as Sollis (2008), who uses a time-varying ESTAR model with structural breaks, find similar results. The above presented literature demonstrates that, during the rather short post-Bretton Woods period, more complicated models are indeed able to present evidence in favor of PPP. Nevertheless, one may still conclude that current empirical methodologies based on mere economic fundamentals leave much to be desired with respect to short-run evidence of the PPP hypothesis; especially during the relatively short period of floating exchange. Hence, the main aim of this paper is to augment on an alternative representation of the relative PPP-based EER (Hakkio 1992) to investigate the degree to which PPP holds over short and medium terms during the post-Bretton Woods period. Moreover, we investigate the forecast ability of the constructed EERs. The approach taken in this paper differs from previous studies in the following ways. Instead of testing the stationarity of a constructed real exchange rate or performing cointegration tests, which have proven to be of low power when applied to data from the post-Bretton Woods period (e.g. Hakkio and Rush 1991; Froot and Rogoff 1995), a relative PPP-based equilibrium is constructed as suggested by Hakkio (1992). The Hakkio based EERs are extended in the following ways: First, motivated by the discussion in Hinkle and Nsengiyumva (1999) and the findings by Kim (1990) and Xu (2003) that the validity of the PPP hypothesis might depend on the choice of the appropriate price index, we construct EERs using different price indexes. Second, the Balassa-Samuelson effect is accounted for by adjusting the relative PPP-based EER for productivity differentials. Third, relative capital enhanced equilibrium exchange rates (CHEERs) are constructed by adjusting the relative PPP-based EER by international interest differentials. Fourth, in addition to Hakkio (1992), who used the period from 1980 to 1982 as a base period,Footnote 2 EERs are constructed based on a rolling base period to check the results for robustness. Fifth, based on the Plaza Accord agreement in September 1985 we divide the sample in two sub-periods from 1974 to 1985 and 1986 to 2005. We calculate mean lifetimes it takes the nominal exchange rate to converge towards the constructed equilibrium. The mean lifetimes are then converted into half-lives to compare our results with the half-lives commonly reported in the literature.Footnote 3 Further, we test the forecast ability of the constructed EERs. First we calculate, in line with Hakkio (1992), the probabilities that the spot exchange rate will move back towards the constructed EERs in a given interval. Second, we test if constructed relative PPP-based EERs are able to outperform a pure random walk in forecasting spot exchange rates. The importance of utilizing the EERs for exchange rate forecasting is twofold. First, the ability of EERs to forecast future spot exchange rates provides a measure of the robustness of the constructed equilibriums. Second, exchange rate forecasting is of vital interest for any participant in foreign exchange markets. The failure to outperform the random walk in forecasting foreign exchange rates over short-time periods was until recently one of the most puzzling anomalies in international finance (e.g. Mark 1995; Qi and Wu 2003; Xu 2003; Qi and Wu 2003; Cheung et al. 2005). Furthermore, several studies even question the long-run predictability of exchange rates (e.g. Kilian 1999; Berkowitz and Giorgianni 2001; Faust et al. 2003). Kilian and Taylor (2003) reason that the disappointing results with respect to exchange rate forecasting may be due to the non-linear mean reverting behavior of real exchange rates. Thus, they use an ESTAR model to exchange rate forecasting and find predictability at horizons of 2–3 years, but not at shorter horizons. While the literature provides general evidence with respect to an in-sample fit of non-linear models, their forecasting ability of exchange rates seems to be rather limited (e.g. Rapach and Wohar 2006). Sarantis (2006), using a Bayesian vector autoregressive model with time-varying parameters, however, provides a breakthrough in forecasting exchange rates. That is to say, his model is able to outperform the random walk for the mark/US dollar, the yen/US dollar, the pound/US dollar, and the mark/pound exchange rate at a 1-day horizon. Though the forecast ability at a daily basis may be very interesting for traders and speculators, business managers who are responsible to allocate international capital flows and policy makers who have to set appropriate monetary policies still may be interested in forecast horizons of less than 1 year. The following major findings can be reported based on the investigation of the bilateral exchange rates between the US dollar and seven spot exchange rates. The use of different base periods changes the path of the constructed EERs only slightly; while the EERs based on a rolling base period seem to provide the best fitting equilibrium. That means they exhibit the lowest mean and standard deviations of the misalignments over the sample period. Interestingly, the EERs based on the 1980–1982 base period are almost identical to those of the rolling EERs, which may be seen as evidence that exchange rates were in equilibrium during that time. The results with respect to the mean reverting behavior of the spot exchange rates towards their equilibrium, the success rate in predicting future movements of the spot exchange rates, and the ability to forecast future spot exchange rates are very impressive if one relies on simple relative PPP-models. The results improve slightly when one adjusts the EERs for interest rate differentials across countries; while CPI-based EERs extended by productivity differentials (Balassa-Samuelson effect) do not indicate substantial improvements. Additionally, using export price indexes (EPI) as a proxy for traded goods prices, instead of consumer price indexes (CPI), leads to better results with respect to the reverting behavior of the spot exchange rate as well as exchange rate forecasting. These results are further improved if one relies on Xu’s (2003) traded goods price index (TPI), while other proxies for traded goods prices, such as producer price indexes (PPI) or the GDP-deflator, provide insubstantial or no improvement. The results further illustrate that PPP is more likely to hold during the post-Plaza Accord period than during the early years of floating exchange rates. For example, over the entire post-Plaza Accord period average half-lives of less than 1 year are reported for the TPI-based CHEERs. In the post-Plaza Accord period the reported half-lives are two quarters for Switzerland using EPI-based CHEERs and less than three quarters for Japan using TPI-based CHEERs. Finally, our approach provides evidence with respect to the predictability of exchange rates. The success rate of convergence towards the constructed EERs for the post-Plaza Accord period is always higher than 50%, with the exception for Canada. Further, the success rates increase if we consider only large misalignments. Moreover, the constructed TPI-based relative equilibrium exchange rates are able to outperform the pure random walk over short-term horizons of less than 1 year, for some of the spot exchange rates. For example, the TPI-based equilibriums of the Japanese yen and the Dutch guilder are able to outperform the random walk at forecast horizons of less than 6 months at a 5% significance level.",
38,2,Journal of Economics and Finance,26 November 2011,https://link.springer.com/article/10.1007/s12197-011-9215-6,Acquisitions of family owned firms: boon or bust?,April 2014,Kimberly C. Gleason,Anita K. Pennathur,Joan Wiggenhorn,,Female,Female,Mix,,
38,2,Journal of Economics and Finance,03 December 2011,https://link.springer.com/article/10.1007/s12197-011-9216-5,The determinants of business start-ups in tertiary education: evidence for Greece through a panel data approach,April 2014,Nicholas Apergis,Irene Fafaliou,,Male,Female,Unknown,Mix,,
38,2,Journal of Economics and Finance,29 November 2011,https://link.springer.com/article/10.1007/s12197-011-9218-3,"Stock markets, banks and the sources of economic growth in low and high income countries",April 2014,Felix Rioja,Neven Valev,,Male,Male,Unknown,Male,"An extensive literature has established that financial development has a strong positive effect on economic growth.Footnote 1 Financial markets overcome transaction costs and informational asymmetries to reduce liquidity constraints and improve the allocation of capital. The positive effect on economic growth is obtained through both greater physical capital accumulation and greater productivity growth. The literature also shows that stock markets and banks both enhance economic growth (e.g., Beck and Levine 2004). Less is known, however, about the effects of stock markets and banks on the two sources of economic growth – physical capital accumulation and productivity growth. And in particular whether these effects vary between high- and low-income countries. In a widely discussed article in The Economist (July 11, 2009), the Chief Economist of the World Bank, Justin Lin, proposed that sophisticated financial institutions “are not appropriate in low-income markets.” Further, Lin suggests that “small local banks are the best entities for providing financial services” in these countries. In this paper, we attempt to answer the following questions: Do stock markets and banks both enhance productivity growth? Is the effect of stock markets more pronounced? Do stock markets and banks both increase physical capital accumulation? Is the effect of banks more pronounced? Do banks have a stronger effect on the sources of growth in low-income countries? Do stock markets have a stronger effect on the sources of growth in high-income economies? Our motivation for this analysis is twofold. First, as described in Acemoglu et al. (2006) countries grow in different ways. A country that is behind the technological frontier will typically pursue a capital accumulation growth strategy (""investment-based growth""). In contrast, advanced countries have a strong incentive for innovation. Financial markets will fund these innovation activities leading to larger productivity gains (“innovation-based growth”).Footnote 2 It is important then to understand the driving forces for each of the two sources of growth and in particular the role that different components of the financial markets play. Furthermore, Aghion et al. (2005) and Rioja and Valev (2004) have shown that the effects of finance on growth may vary according to the country’s income level. Lin et al. (2009) show theoretically that the structure of the financial system depends on the country’s stage of development. Second, in addition to the effect on economic growth, the literature provides theoretical arguments for the effects of stock markets and banks on the sources of growth, but the empirical literature has analyzed primarily their effect on overall growth. Hence, our objective is to focus on the sources of growth. The theoretical literature proposes that both banks and stock markets are expected to enhance productivity. Allen (1993) and Allen and Gale (1999) argue that stock markets are essential for productivity growth. In Allen and Gale’s (1999) model, individual investors “agree to disagree” on the feasibility of new investment projects. With disaggregated decision making in stock markets, each investor makes a decision whether or not to invest; as a result more innovative projects receive financing. Similarly, Boyd and Smith (1998) show that stock markets become more important when economies approach the technological frontier where innovation is the primary source of growth. According to theory, banks are also important for productivity growth. Bhide (1993) argues that banks raise productivity by monitoring firm managers and improving corporate governance. Similarly, both banks and stock markets provide financing for physical capital accumulation. Levine (1991) shows that liquid stock markets allow investors to convert shares into cash in case they experience a liquidity shock. With reduced liquidity risk, investors are willing to commit funds to capital investments. Furthermore, stock markets allow investors to diversify idiosyncratic productivity risks which also serves to raise investment. Other theoretical work shows that banks are also important for physical capital accumulation. For example, Gerschenkron (1962) argues that banks can exert pressure on firms to service their debts. Therefore they finance capital investments even in weak institutional environments. Stulz (2001) points out that banks can commit funds to capital investments that require financing in successive stages. In summary, the theoretical literature argues that banks and stock markets both enhance productivity growth and physical capital accumulation.Footnote 3 We use data from a large panel of countries to test these hypotheses. Furthermore, we investigate the influence of stock markets and banks on the sources of growth in developed and developing countries. Banks may be especially important in developing countries where stock markets are smaller and less active. As countries develop, their stock markets may start to play a more significant role. Our empirical findings are that:1) banks primarily affect capital growth while stock markets primarily affect productivity; 2) in high income countries, however, there is strong evidence that banks and stock markets have independently affected capital growth, while productivity seems to be driven by the stock market only; and 3) in low income countries, conversely, bank credit is the primary driver of capital accumulation. However, neither stock markets nor banks seem to affect productivity growth. Our paper is not the first one to consider the effects of stock markets and banks on the sources of growth. Levine and Zervos (1998) find that measures of stock market and credit market development both enter significantly in equations explaining capital and productivity growth. We extend their work in three ways. First, increased data availability allows us to expand the number of countries and the length of the time series used by Levine and Zervos (47 countries with data ending in 1993). Both credit markets and stock markets have developed significantly since the mid 1990’s. We use data for 62 countries covering the period of 1980–2009. Second, we confront well-known potential endogenity problems by using GMM dynamic panel techniques to try to establish causality. Levine and Zervos use cross-country OLS regressions which, while suggestive of a positive effect of finance on the sources of growth, fall short of establishing causality.Footnote 4 Third, we investigate the roles of financial markets in developed and developing countries separately. Here, we are motivated by Rioja and Valev (2004) who find that the effects of finance on growth vary with income. However, Rioja and Valev (2004) focus on the effects of private credit and do not study in detail how the effects of stock markets vary according to income levels. The remainder of the paper is organized as follows. Section 2 describes the data and the measures used. Section 3 describes the methodology and Section 4 discusses the results. Section 5 concludes.",46
38,2,Journal of Economics and Finance,15 December 2011,https://link.springer.com/article/10.1007/s12197-011-9220-9,Subprime lending over time: the role of race,April 2014,Marvin M. Smith,Christy Chung Hevener,,Male,Female,Unknown,Mix,,
38,2,Journal of Economics and Finance,05 February 2012,https://link.springer.com/article/10.1007/s12197-011-9224-5,A road to assimilation: immigrants and financial markets,April 2014,Swarnankur Chatterjee,Velma Zahirovic-Herbert,,Unknown,Female,Unknown,Female,"Economic integration of immigrants is an important issue for economists and policy makers of immigration studies. A substantial amount of literature has emerged assessing how the relative human capital (e.g., educational attainment, language ability, and health status) and labor market dynamics (e.g., earnings, occupation, and employment rates) of immigrants vary from the time they first migrate. Although financial market participation represents an important aspect of economic assimilation, little attempt has been made to study how the asset preference and investment allocation of immigrants compares to native-born residents. Few have examined specific components of wealth such as financial equity separately for groups, except in a descriptive manner (Keister 2000). Studying inequality in total wealth ignores any unique processes that generate racial-ethnic differences in specific types of wealth. We add to this literature by conducting a detailed examination of financial asset ownership. In particular, our study examines the differences in financial asset ownership and market participation of immigrants and native-born residents. Moreover, we establish the relationship between immigrant nativity status, their financial market participation and wealth holdings within these financial assets. We argue that since financial market participation requires a certain level of market access and the ability to synthesize available investment information, understanding the differences in investment preferences of immigrants and native-born residents can lead to more effective policies and programs aimed at mitigating the existing gap in asset ownership and wealth accumulation. Our study is based on the analysis of the National Longitudinal surveys, NLSY79, data set. We conduct a two-stage empirical estimation. In the first stage, we examine the decision to participate in financial markets using a probit model. The second stage, then, studies the correlates of amounts allocated to financial assets for immigrants and native-born residents after controlling for numerous socioeconomic, demographic, and human capital related factors. In addition to this, we also control for the nativity of respondents when analyzing the determinants of financial asset allocation among immigrants. There are many reasons to believe that both the financial asset choice and level of financial wealth of immigrants will diverge from those of native-born residents. Many immigrants face earnings profiles that differ from native-born residents in terms of levels and earnings risk. The migration process itself leads immigrants to be a highly selected sample of individuals (Borjas 2002). Similarly, there may be a cultural basis to saving behavior (Medina et al. 1996; Carroll et al. 1994, 1999). For example, Medina et al. (1996) shows that Mexican-Americans exhibit lower willingness to delay spending money to achieve gratification. In addition, limited access to the social welfare system and the prospect of remigration may further alter immigrants’ incentives for risky asset allocation (Amuedo-Dorantes and Pozo 2002; Shamsuddin and De-Voretz 1998; Dustmann 1997; Galor and Stark 1990). In addition, access to and use of financial services can affect a wide range of economic behaviors, including decisions about consumption, saving, home ownership, business formation, investment and retirement. However, little is known about the financial market participation among immigrants to the United States. To what extent do they participate in mainstream financial institutions? Does their general assimilation into American society include assimilation into American financial life as well? For example, a recent study by Rhine and Greene (2006) looks at the ownership of banking accounts among immigrants. The authors argue that, upon arrival to the United States, immigrants are immediately faced with making financial decisions and conducting transactions ranging from cashing payroll checks and paying living expenses to remitting income to family members residing in their home country. Thus they examine immigrant participation in the financial mainstream and show that immigrants as a group are significantly more likely to be unbanked and not have an account at a bank or another financial institution than the U.S. born. The unbanked households are more likely to access check cashing and payday lending services, which fall outside of the mainstream financial institutions. The results from this study provide new perspectives on the financial asset ownership of immigrants. This paper proceeds as follows. Section 2 contains a detailed account of theory, empirical methodology and data, including the descriptive evidence on immigrants’ and native-born residents’ financial wealth composition. Our findings are presented in section 3. Section 4 concludes the paper.",14
38,3,Journal of Economics and Finance,10 January 2012,https://link.springer.com/article/10.1007/s12197-011-9221-8,Hedge fund attributes and volatility around equity offerings,July 2014,Robert M. Hull,Sungkyu Kwak,Rosemary Walker,Male,Unknown,Female,Mix,,
38,3,Journal of Economics and Finance,21 February 2012,https://link.springer.com/article/10.1007/s12197-011-9222-7,"Cross listing, disclosure regimes, and trading volume sensitivity to stock returns",July 2014,Haiyan Zhou,Stephen Owusu-Ansah,,Unknown,Male,Unknown,Male,"In this paper, we investigate whether investors of cross-listed firms use trading volume to revise their perception of firms’ value, and whether cross-listed firms from different disclosure regimes have different trading volumes’ sensitivity to stock returns. Kim and Verrecchia’s (2001) model makes it possible to use trading volume, as a direct and better proxy for firms’ information environment, to test informational effects of international cross listing. We use trading volume sensitivity to returns to measure the economic consequence of foreign firms cross listing in the U.S. Because U.S. equity markets have stringent disclosure requirements relative to other countries, we expect foreign firms that are cross-listed in the U.S. to experience economic benefits of increased levels of required accounting disclosure to the extent that their investors can use the additional information to revise their perception of firm value (Lang et al. 2003). Our expectation is justified by the fact that the increased disclosure associated with international cross listing reduces investors’ uncertainty and diversity of opinions about valuation of cross-listed firms. Further, because disclosure requirements in low-disclosure regimes are relatively lax and less stringent, less-informed investors depend more on trading volume to draw inference from well-informed investors’ private information about firm value of cross-listed firms from low-disclosure regimes than those from high-disclosure regimes. Therefore, we expect cross-listed firms from low-disclosure regimes to have relatively higher trading volume sensitivity to stock returns compared to those from developed economies. Prior studies have documented several positive economic consequences of cross listing in the U.S. For instance, cross listing in the U.S. lowers cost of capital, widens stockholder base, makes stock more liquid, makes controlling stockholders commit to limiting expropriation from other investors, and enhances firms’ visibility and exposure to participants of both capital and consumer markets (see, e.g., Foerster and Karolyi 2000; Errunza and Miller 2000; Baker et al. 2002; Reese and Weisbach 2002; Doidge et al. 2004). These economic benefits are driven by additional disclosure, stricter enforcement regime, expanded legal liability or any combination of these factors. However, as pointed out by Leuz (2003), the extant literature fails to indicate the source(s) of the benefits. Baker et al. (2002), Lang et al. (2003) and Bailey et al. (2006), however, broke ranks with those studies, and examined the source(s) of international cross listing effects.Footnote 1 For instance, Lang et al. (2003) investigate whether cross-listed firms in the U.S. improve their information environment, and ultimately, their valuations. Using both analysts following and accuracy of analysts forecast, as proxies for firms’ information environment, they find that cross listing in the U.S. yields greater analysts coverage and increased forecast accuracy.Footnote 2 They also document that firms that have more analysts’ coverage and increased forecast accuracy have a higher valuation. Leuz (2003), however, shows that the effects of cross listing by Canadian firms on analyst following and forecast accuracy are mixed, suggesting that these variables are not direct and proper proxies for firms’ information environment. Leuz (2003) argues that studies that would use a better proxy to capture firms’ information environment in examining the effects of cross listing would add a value to the literature. This motivates our study. While prior studies (see, e.g., Doidge et al. 2004) investigate the incremental effect of cross listing by including a dummy variable in their stock returns and/or trading volume models, we investigate the reaction of trading volume to changes in stock returns, measured by the slope coefficient on trading volume in a stock return model. We call the reaction of trading volume to changes in stock returns as trading volume sensitivity to stock returns. Kim and Verrecchia’s (2001) model predicts that a firm’s returns depend on the trading volume when it postpones disclosure because less-informed investors use trading volume information to infer about the private information held by well-informed investors on the firm’s value. Thus, they posit that the sensitivity of trading volume in a stock returns model offers a useful tool to measure the economic benefits of shifts in disclosure regimes. Focusing on their model in a general setting of disclosure, we propose that: (1) investors of cross-listed firms use trading volume to revise their perception of firms’ value, and (2) firms that cross list from low-disclosure regimes have higher trading volume sensitivity to returns than those that cross list from developed economies, as those from low-disclosure regimes have relatively lax and less stringent disclosure requirements. Our paper focuses on foreign firms that are cross listed in the U.S. as exchange-listed American Depositary Receipts (ADR), adopted international financial reporting standards ([IFRSs], formerly international accounting standards [IASs]), and filed Form 20-F reconciliation with the U.S. Securities and Exchange Commission (SEC) during the period of 1994–2005. Using these cross-listed (ADR) firms and a matched sample of U.S. firms (as non-ADR firms) based on exchange, industry and firm size, we find that returns and trading volume are positively related for cross-listed firms, and the relationship is statistically significant. We classify a country’s disclosure regime as low or high based on its mean Standard and Poor’s (S&P) transparency and disclosure scores as used in Khanna et al. (2004). Thus, a country whose mean S&P transparency and disclosure score is above the median value is classified as a high-disclosure regime. On the other hand, a country is classified as a low-disclosure regime if its mean S&P transparency and disclosure score is below the median value.Footnote 3 As hypothesized, we find that cross-listed firms from low-disclosure regimes exhibit higher trading volume sensitivity to returns. This study extends prior literature on international cross listing in two ways. First, our study addresses modeling complexities in capturing information asymmetries by explicitly incorporating in such models trading volume information, a proxy that “more directly (or at least reliably)” relates to the theoretical constructs used in developing the hypotheses in prior studies (Leuz 2003). Prior research either ignored the role of trading volume (e.g., Leuz and Verrecchia 2000; Bailey et al. 2006) due to modeling difficulties or used indirect and poor proxies for firms’ information environment (e.g., Lang et al. 2003) in valuing firms that have cross listed. Second, our study empirically applies Kim and Verrecchia’s (2001) model, which uses the first-order Taylor expansion of the log of absolute value of returns on trading volume, to predict firms’ stock returns that depend on trading volume when information asymmetry exists and less-informed investors use trading volume information to infer the private information held by well-informed investors about firms’ value. Thus, our study helps to understand better the relationship among disclosure, returns, and trading volume. We organize the remainder of the paper as follows. We review prior literature and develop our hypotheses in the next section. We respectively introduce the theoretical model, and discuss the research design in sections 3 and 4. We describe our sample and data in section 5. We present our tests results in section 6, and conclude the paper in section 7.",2
38,3,Journal of Economics and Finance,08 January 2012,https://link.springer.com/article/10.1007/s12197-011-9223-6,The day-of-the-week effect revisited: international evidence,July 2014,Mehmet F. Dicle,John D. Levendis,,Male,Male,Unknown,Male,"The existence of consistent patterns in returns is an important marker of inefficiency and of arbitrage opportunities. Thus, calendar anomalies receive considerable attention from scholars as well as from practitioners. Earlier studies provided sufficient evidence of the day-of-the-week (DOW) effect in returns for U.S. equity and non-equity markets as well as for international markets. Recent studies, however, show that the DOW effect has disappeared in some of the developed markets and is disappearing in emerging markets. There are three motivating questions for this study. Are there still any market-level DOW effects? Are there DOW effects at the individual stock level? If so, what explains them?
 The first question is: are there any remaining market-level DOW effects? On the market level, this study provides evidence that the DOW effect exists in 24 of 51 markets for Mondays and in 32 on Fridays.Footnote 1 It should be noted that there is a tendency in the literature to evaluate market wide averages for the DOW effect and/or indices comprised of a portfolio of stocks. Evaluation of individual stocks for a DOW effect is rare, which leads to the second question for this study. 
Are there any DOW effects for individual stocks? At the individual stock level, the DOW effect exists for about 7% of stocks in each market. In fact, the difference between markets with and without DOW effects are quite minimal: (1) markets with a market-level DOW effect have, on average, 8.28% (8.37%) of their stocks with individual-level DOW effect on Mondays (Fridays), (2) markets without market-level DOW effects have, on average, approximately 7.25% (6.74%) of their stocks with individual-level DOW effects on Mondays (Fridays). Thus, it is concluded that the DOW effect has not disappeared completely; instead, it has disappeared to the point that it cannot be detected using market averages. A subsidiary question is: if a market has no DOW effect in market returns, does that mean that a statistically insignificant number of stocks exhibit a DOW effect in those specific markets? It could be argued that since the DOW effect has disappeared and some of the markets have no unexplained DOW effect left in them, there should either be no, or very few, stocks with a DOW effect. This is not the case. As we will show, some of the most important markets such as the NYSE show no DOW effect in market returns and yet approximately 9% of its stocks show a statistically significantFootnote 2 DOW effect in returns on Mondays and Fridays. Stocks with DOW effects tend to be smaller, and are therefore not detectable at the market-wide level. The overall evidence reveals that, on average, 7.73% (8.03%) of all stocks show DOW effect on Mondays (Fridays) at the 0.05 level. Moreover, as will be explained below, this result is not attributable to Type-I error. The third question is: what explains the DOW effect? Is it a true anomaly, or can we find its causes? Hypotheses explaining the DOW effect include (among others) “differences in settlement and trading days”, “the non-trading problem”, “institutional trading”, “spill-over effects” and “inefficient econometric methodology”. There is also evidence to suggest that the DOW effect is in “calendar time” but not in “trading time”. Additionally, persistent anomalies in liquidity may be to blame for the DOW effect. Since equity markets around the world have diverse microstructures and clientele, there may be no one-size-fits-all explanation for the DOW effect. What is important, however, is that the DOW effect in market returns can be explained—as we will show—for almost all equity exchanges using one of the evaluated hypotheses here. Based on data for 37,631 stocks traded in 51 equity markets in 33 countriesFootnote 3 for the period between January 2000–December 2007, this study contributes to the existing literature by providing evidence that the DOW effect in market returns may seem to be disappearing but the DOW effect for a statistically significant number of individual stocks still exists. Furthermore, the hypotheses evaluated here fail to explain the reported DOW effect in individual stocks’ returns. While this study may be about DOW effect—perhaps one of the most reported market anomalies—our results apply to other market-level work. For instance, for studies that evaluate Granger type causality or cointegration between markets, the results could simply be due to a few select individual stocks which would present a completely different inference. The intent of the paper is not to limit the contribution to just DOW effect but to point out the potential differences between market level evaluations and individual stock level analyses. To investigate this, we employ a unique dataset consisting of bid-ask-spreads and returns of over 37,000 stocks. This allows us to evaluate almost every DOW hypothesis for even developing markets. This mass application, and its potential generality, are what make this study unique. In the following section a brief summary of the previous literature is provided about the evidence of the DOW effect and related hypotheses. The third section provides the variable descriptions and the data. Definitions of hypotheses along with the econometric models are provided within the fourth section. Market level and individual stock level models are discussed with empirical results and their implications. An Appendix which includes the tables follows the concluding remarks of the study.",23
38,3,Journal of Economics and Finance,15 February 2012,https://link.springer.com/article/10.1007/s12197-012-9226-y,"Over-investment in corporate R&D, risk, and stock returns",July 2014,Mohsen Saad,Zaher Zantout,,Male,Male,Unknown,Male,"We form a sample of significant increases in corporate-wide R&D expenditures by identifying firms that increased their R&D expenditures ‘significantly’ following a period of ‘no significant change.’ Specifically, we first examine the distribution of the percentage annual increase in R&D expenditures for each firm on COMPUSTAT and across all years. We find that 90% of the observations pertaining to R&D-expenditures-increasing firms have a value that is greater than 4.43%, and the median increase is 21.83%. We also examine the distribution of the percentage annual decrease in R&D expenditures for each firm on COMPUSTAT and across all years. We find that 90% of the observations pertaining to R&D-expenditures-decreasing firms have a value that is more than 2.71%. Therefore, these statistics suggest that a firm that has an increase in R&D expenditures not exceeding 4.43% or a decrease not exceeding 2.71% during a year is a firm with relatively minimal or insignificant change in R&D expenditures. Second, we identify all COMPUSTAT firms that have an insignificant change in R&D expenditures (as just defined) for a period of two consecutive years and then have a significant increase in investment expenditures. By significant increase, we mean an increase that exceeds the median value of 21.83%. We obtain 462 firm-year observations. Third, we exclude from the 462 observations, all those that have an R&D intensity ratio (i.e. R&D expenditures to total sales) of less than 0.19%, being the minimum observation for the top 75% of all the COMPUSTAT annual observations for all firms across all years. Our final sample of firms significantly increasing their R&D expenditures following a two-year period of no significant change in investment activity includes 438 observations over the period from 1962 to 2005 by firms listed on NYSE, AMEX and NASDAQ. Panel A in Table 1 displays the chronological distribution of the sample events. There is no major clustering of announcements in any sub-period of the 1962–2005 sampling period, during which the performance of the economy and the stock market varied substantially. We observe that the number of events per year trends up over the sampling period despite the annual fluctuations, but this latter is a reflection of the underlying population. Panel B in Table 1 examines whether the sample events cluster during certain month(s). We find that 63% of the sample cases cluster in the month of December. Panel C in Table 1 examines the industry representation of the sample. The firms of our sample represent 151 different four-digit Standard Industry Classification (SIC) industries. We follow the classification of Chan et al. (1990) and Zantout (1997) who separate industries into two groups: high- versus low-technology industries. The high-technology industries are: aircraft/defense, electronics, information processing, instruments, pharmaceuticals, semiconductors and telecommunications. We find that 217 (or 49.54%) of the 438 cases pertain to the high-technology industries. Therefore, significant corporate-wide R&D expenditure increases apparently occur with equal frequency in high- and low-technology industries. We check whether these sample firms had a significant change in their capital expenditures activity which may confound our results, and we find that they do not experience any significant change in their capital expenditures intensity, over neither 1 nor 3 years prior to the significant increase in R&D expenditures. We classify in Table 2 the event-firms in our sample into NYSE market capitalization quintiles and NYSE quintiles of R&D-expenditures-to-sales ratio (using data from fiscal year prior to event). These classifications provide good measures of the relative size and R&D-intensity of the sample firms, while controlling for the normal up trend in the market capitalization of firms over time and for the effects of business cycles on security prices and sales. We observe that in terms of size representation we find similarly that about one-third of the sample cases pertain to the smallest-size firms and about one-third of the cases to the largest-size firms. Therefore, the population of technology-acquirers is neither exclusively nor predominantly formed of the largest-firms in our economy, as may be thought. In terms of the R&D investment intensity of the sample firms relative to other publicly-traded firms, we find that most of our firms invest in R&D intensively. About 38% of them belong to the quintile of the most R&D-intensive firms.",5
38,3,Journal of Economics and Finance,28 February 2012,https://link.springer.com/article/10.1007/s12197-012-9227-x,Insider trading around open-market share repurchases,July 2014,Fei Leng,Kevin Min Zhao,,,Male,Unknown,Mix,,
38,3,Journal of Economics and Finance,10 March 2012,https://link.springer.com/article/10.1007/s12197-012-9229-8,Volatility analysis of precious metals returns and oil returns: An ICSS approach,July 2014,Lucía Morales,Bernadette Andreosso-O’Callaghan,,Female,Female,Unknown,Female,"The dual objective of this paper is to examine the effects of oil returns on both precious metals markets and on including the major financial markets (the US-Dow Jones Industrial, Japan-Nikkei 225, and UK-FTSE 100) as proxies to identify the strength of volatility persistence among these different markets.Footnote 1
 Early research in this area (Hamilton 1983; Gilbert and Mork 1984) concluded that increases in oil prices generate important effects on macro variables such as GNP. In particular, Hamilton (1983) found that increases in oil prices are responsible for declines in real GNP. Therefore, if oil prices play an important role in an economy, it is thus reasonable to expect the existence of high correlations between oil prices and stock prices; however, what would the relation of these variables with precious metals be? The analysis of interlinkages between these variables is of high importance for investors; if our analysis does not find conclusive evidence on the existence of relationships between these variables, this will imply that there are possibilities for diversification between these markets. Likewise, Ross (1989) argued that volatility is a measure of information flow; the analysis can be view as an investigation of the extent to which the rate of information flow is correlated across markets. The increasing integration of major financial markets throughout the world has generated interest in studying the transmission of financial markets shocks across markets (Ewing et al. 2002) due to the important implications with regard to contagion effects. Consequently, it is crucial for financial markets’ participants to understand volatility transmission mechanisms across time and sectors in order to facilitate optimal portfolio decisions. Thus, it is natural to be concerned with how information, and thus, volatility, may flow from one market to another. Since index futures and options are an important tool that financial market participants can use in order to hedge against portfolio risk, and since the volatility of the index is a key determinant of futures and options valuation, it is thus important to understand what affects index volatility in those markets (Ewing et al. 2002). A number of techniques have been used to model volatility. The autoregressive conditional heteroskedasticity (ARCH) model developed by Engle (1982), and later generalized by Bollerslev (1986) is the most popular method used for analyzing high-frequency financial time series data. Multivariate generalized autoregressive conditional heteroskedasticity (GARCH) models have been used to estimate the spillover effects and volatility among different markets. It has been concluded that higher levels of conditional volatility in the past are associated with higher conditional volatility in the current period, which is associated with a volatility persistence value that is normally very close to one. In this case, a persistence value close to one is indicative of an integrated GARCH (IGARCH) process. The estimates of volatility persistence for each return series provide information about the extent to which post shocks and volatility matter in the construction of forecast of future conditional variance. The greater the persistence, the more weight should be given to recent observations of volatility, in terms of explaining future behavior of the variance. The volatility of the series will return to its conditional variance faster than would be the case when there is greater persistence. These are very important features to be considered by investors when managing their portfolios. The current situation in equity and energy markets is an area of great interest for researchers, at a moment when oil prices are soaring, and major oil producers are affected by political and economic unrest. This situation has led to investors’ new behavioral trends, given the potential offered by the commodities markets. Therefore, we consider that the analysis of interactions between stock returns, oil returns and precious metals returns is a topic that requires due consideration during this these challenging times. The behavior of oil prices has received special attention, as the oil market has undergone structural transformations that have placed oil prices on a new high path, where new large consumers such as China and India are playing an increasingly important role in the current market. The rise in oil prices and the increase in oil price volatility are explained by a wide list of drivers including strong demand (mainly from outside the OECD area), lack of spare capacity in upstream oil, distributional bottlenecks, OPEC supply response, geopolitical and weather shocks and the increasing role of speculators and traders (Fattouh 2007). The level of volatility in financial markets influences the corporate sector’s investment decisions and the bank’s willingness and ability to extend credit facilities (Panetta et al. 2006). Therefore, it is important to know and understand the impact that changes in volatility levels might have on financial stability. The current economic situation is reminiscent of pre-Bretton Woods times and brings to our minds the following questions: Could it be that the importance of precious metals markets becomes a key variable that could eventually be used by governments to help maintain financial stability? Precious metals are sources of increasing value and are perceived as important and stable stores of value; why not use them to diversify portfolios? Has there been enough research done analyzing the role of precious metals in the economy? What are the connections between these markets and major economic and financial variables such as equity and oil markets? Because risk management practices have improved during the past years, financial institutions are better equipped to mitigate the adverse effects in volatility; financial factors affect volatility investor’s risk tolerance, hedging strategies, and structural changes in financial markets. Financial derivatives, for example, have allowed market participant to price, unbundle and disperse risk throughout the financial system. Then, the analysis of equity markets, precious metals markets and oil prices deserves particular attention, given that these markets could be used for the purpose of diversifying the investors’ risks. The graphical analysis of oil prices (Fig. 1 in the Appendix) from January 1995 to May 2008 shows how crude oil prices are following a clear upward movement; it is possible to identify the same behavior between the Brent and the WTI prices during the time under study. As a result, we decided to use Crude oil Brent prices in our analysis, as this index is considered a leading indicator for Atlantic basin supplies of crude oil in general. In relation to the Dow Jones Industrial, FTSE 100 and Nikkei 225, the graph for the last 13 years (Fig. 2 in the Appendix) allows us to identify quite clearly the higher instability that exists in the main world equity markets. It is possible to appreciate pronounced downturns in the markets during the following periods of time: Asian Crisis (1997–1998); terrorist attacks of September 11, 2001; dot.com bubble or I.T bubble crisis spanning roughly over 1995 to 2001; bursting of the Internet Bubble during 2002; and Chinese Correction during 2007 that was an initial sign of the global stock market plunge of February 27, 2007. The equity indices reflect a clear instability of the markets during this particular time period, with the Nikkei 225 being seemingly the most volatile index. The oil and stock market prices seem to be reacting in a negative manner to these major market crises, but if we pay attention to what is going on in the precious metals markets (Fig. 3 in the Appendix) the patterns exhibited by these series are not in line with what is happening in the oil and equity markets. Precious metal markets seem to reflect an independent behavior; a clear upward trend is visible in the prices of gold and platinum and in a softer manner in that of silver after the Asian Crisis, a situation persisting at the time of writing, and that stirs interesting questions about the real potential of these markets and suggestions with regard to bubble formations. We believe that stock markets are influenced by shocks in oil and precious metals markets, while precious metals markets are more stable and secure assets that will not be much affected by external shocks. Therefore, if we can identify an independent behavior of precious metals with respect to oil and equity markets during times of crises, this would be a highly valuable information for investors that would be able to design their investment strategies by taking into account the use of precious metals in the composition of their portfolios. The remainder of the paper is structured as follow. In section 2, we present the main literature analyzing this issue. Section 3 describes the data and the methodology that is used to detect changes in variance; the ICSS algorithm and the EGARCH and GARCH models are presented and discussed. Section 4 reports and analyse the main results and finally, section 5 concludes and offers some notes for further research.",15
38,3,Journal of Economics and Finance,24 March 2012,https://link.springer.com/article/10.1007/s12197-012-9230-2,On the loss structure of federal reserve forecasts of output growth,July 2014,Hamid Baghestani,,,Male,Unknown,Unknown,Male,"This study is motivated by recent studies examining the loss structure of Federal Reserve forecasts. In particular, Capistrán (2008) shows that inflation forecasts are biased but that this bias is rational due to asymmetric loss. More specifically, for the period since Volcker, the Federal Reserve forecasts of inflation over-predict. However, this bias, as shown by Capistrán (2008), is due to cautiousness (instead of an inefficient use of information). With price stability as a crucial monetary policy goal, it is argued that under-predictions are costlier than over-predictions and thus the bias in inflation forecasts is rational. The study by Capistrán (2008) is among those examining the behavior of Federal Reserve inflation and growth forecasts to better understand the monetary policymaking process. As pointed out by Gavin and Mandal (2001, p. 13), we value forecasts for their accuracy and, in some cases, for what they reveal about the forecasters. This study examines these two issues for the Federal Reserve forecasts of output growth by investigating rationality under flexible loss. In contrast to the inflation forecasts, our findings indicate that the Federal Reserve forecasts of growth are rational (efficient) under symmetric loss for 1984–2006. In addition, these forecasts are equally accurate in predicting both the upward and downward moves and are thus of value when similar cost (loss) is assigned to both incorrect upward and downward predictions. Over-predictions (under-predictions) are costly when the Fed responds by implementing an unnecessary contractionary (expansionary) monetary policy and thus causes lower growth (higher inflation). Accordingly, the existence of symmetric loss suggests that the Fed is equally concerned about both low growth and high inflation. This is indeed consistent with the Fed’s dual mandate of promoting maximum sustainable output coupled with price stability. Furthermore, our forecast evaluation employs comparable private predictions of growth as benchmarks. This enables us to further investigate the asymmetric information hypothesis that the Federal Reserve has useful information about the state of the economy that is not known by the public. Studies by Romer and Romer (2000), Gavin and Mandal (2001), and Sims (2002) convincingly support this hypothesis for the inflation forecasts. However, as noted by Gavin and Mandal (2001, p. 16), the findings are rather weak for the output growth forecasts. Consistent with this observation, we are unable to empirically support the asymmetric information hypothesis for 1984–2006. The format of this study is as follows: Section 2 discusses the data and the methodology and then presents the empirical results. Section 3 concludes this study by discussing the implications of our findings.",2
38,3,Journal of Economics and Finance,22 July 2012,https://link.springer.com/article/10.1007/s12197-012-9241-z,Comparing U.S. regions for selected economic and financial variables,July 2014,Edward Nissan,Shahdad Naghshpour,,Male,Unknown,Unknown,Male,"Armstrong and Taylor (1993) spell out the concern of finding answers regarding questions that contribute to regional differences. Questions include differences in living standards, interregional migrations and levels of unemployment. They point out that regions’ analytical techniques are derived from theories related to the behavior of national economies because regional economies are similar to national economies. Yet, there are differences between nations and regions. Armstrong and Taylor list some significant differences. Regional economies are more open than national economies. Trade between regions is free from barriers to trade, and regions use the same currency. Furthermore, labor and capital are more mobile between regions as compared to nations. Also, barriers such as legal, political and language operate differently between nations than between regions. In particular, Garnick (1984, 1986) pursued the issue of noticeable reversal in population and industrial growth patterns of metropolitan and non-metropolitan areas in the United States during the 1970s and 1980s. Garnick considered the Bureau of Economic Analysis’ classification of regions as urbanized (New England, the Mideast, the Great Lakes and the Far West) and less urbanized (the Plains, Rocky Mountains, Southeast and Southwest). The rural/urban regional distinction as outlined by Garnick could affect monetary and manufacturing policy outcomes. The distinction can also affect the level of employment as consequence of specialization, competition and industry concentration. In general, it is expected that a greater variety of goods and services are produced in cities where retail outlets are found in more abundances as compared to rural areas. Cutler et al. (2007) support these views. They state (page 375) that “Agglomeration economies in urbanized areas suggest inherently stronger interest linkages in the regional economy.” They further cite (Page 576) the work of Rosenthal and Strange (2001) that “input sharing is significant in an agglomerated area which would also tend to promote co-movements between more integrated urbanized sectors.” Casselman (2012) points out to another general break in the US economy by explaining that the economy as viewed from unemployment can stem from weakness in the economy pushing demand down, or from structural change in employment. The former is cyclical resulting in lowering the needs for workers, while in the later, unemployment does not disappear even though the economy improves. Casselman also points out that evidence4 for or against structural shift cannot be proved or disproved until the recent job market improves. However, there is little agreement on the solution in that structural unemployment does not respond to stimulus government spending or reduced interest rates as does cyclical unemployment. Hilsenrath and Peterson (2012) reinforce these ideas by stating (page A4), that “The Fed chairman took on some thorny economic issues in making his case for low rates. Among them is the question of whether the nation’s still high unemployment rate represented a so-called cyclical problem that can be resolved simply by encouraging economic growth with low interest rates, or a fundamental structural problem in the labor market itself and the Fed can’t fix.” The purpose of this paper is to identify similarities and dissimilarities between U.S. urbanized and less urbanized regions as classified by the Bureau of Economic Analysis (BEA), not just for population and industrial growth, but to include some other major economic and financial indicators such as demographic, economic, bank performance and banking structure. In all, the data are for 13 variables at the state level; these will be aggregated at the eight regional levels. It should be noted that the choice of the BEA classification is prompted by Perlman (1982) explanations. Perlman emphasized that the BEA classification is organized according to economic homogeneity and industrial output similarities, as well as physical proximity and historical development. Full discussion of the variables will be presented in the data and method section. The source of data is DePrince et al. (2011), who gathered the data from various sources. This section is followed by a literature review, the data and method, the results and the conclusions.",
38,4,Journal of Economics and Finance,25 April 2012,https://link.springer.com/article/10.1007/s12197-012-9231-1,Investor behavior in the mutual fund industry: evidence from gross flows,October 2014,George D. Cashman,Federico Nardari,Sriram V. Villupuram,Male,Male,Unknown,Male,"Net cash flows into equity mutual funds have increased from $7 billion in 1985, to $124 billion in 1995, and to $136 billion in 2005 (Investment Company Institute (2006)). During that same time period inflows (outflows) rose from $40 ($33) billion to $434 ($309) billion to $1,210 ($1,074) billion. Although extensive research has been conducted on the determinants of net flows, relatively little research has been conducted on the determinants of inflows and outflows. This relative lack of research is startling given that, as one can infer from the figures above, gross flows tend to be several times larger than net flows, and examining gross flows allows for a richer understanding of investor behavior. Johnson (2007) argues that beyond gaining a better understanding of investor behavior, examining gross flows is important because of the potential effect of gross flows on performance. Edelen (1999) shows that inflows and outflows, to the extent that they do not offset one another, cause funds to engage in (liquidity motivated) trading that they otherwise would have avoided. Edelen (1999) and Alexander et al. (2007) show that liquidity motivated trading is costly to long-term fund investors. Thus, both fund managers and investors have an interest in understanding the dynamics that drive inflows and outflows. Investor behavior can also affect advisor incentives with respect to risk. Chevalier and Ellison (1997) note that mutual fund advisor compensation is typically tied to funds under management which implies that investor flows serve as an implicit incentive mechanism. Brown et al. (1996) and Chevalier and Ellison (1997) argue that compensation tied to funds under management and the convex relation between performance and flows combine to create incentives for managers to alter fund risk within a year conditional on performance in the first part of the year. Johnson (2007) contends that if existing investors are less sensitive to poor performance than they are to good performance (i.e., they purchase more shares in response to good performance than they sell in response to poor performance), then the manager’s incentive to shift risk is even greater than previously assumed. In other words, the relative lack of responsiveness of existing investors to poor performance leads to an incentive for fund managers to exploit existing fund investors. Examining gross flows allows us to examine this issue directly. We investigate the dynamics of gross flows, examining the role of flow persistence and fund performance. This allows us to make three contributions to the existing literature. First, we document the importance of persistence in monthly flows.Footnote 1 Second, we document differences in investor behavior across fund types. Third, we provide evidence on the relation between performance and flows at the monthly frequency. We find that mutual fund flows are highly persistent. This is true regardless of whether we are considering inflows, outflows, or net flows. Prior research on gross flows has either not focused on the role of persistence in inflows and outflows (Bergstresser and Poterba (2002), Johnson (2007) and Keswani and Stolin (2008)) or uses a sample construction algorithm that excludes persistence from the sample (Ivkovich and Weisbenner (2006)).Footnote 2 Thus, our results on flow persistence represent new evidence on an important aspect of investor behavior. Namely, the importance of persistence suggests that investors do not completely re-evaluate their mutual fund selection each month. Second, we document economically important differences in investor behavior across fund types. Prior studies on mutual fund flows focus almost exclusively on (large) domestic equity funds. We find that hybrid fund investors are much less sensitive to fund performance than are investors in domestic or international equity funds.Footnote 3 Borrowing Johnson’s (2007) verbiage, this result suggests that equity fund investors are more active monitors of fund performance than hybrid fund investors. We also find differences in flow persistence. Specifically, gross flows are more persistent in international equity funds then they are in domestic equity funds, which are, in turn, more persistent than gross flows in hybrid funds. These results suggest that there are meaningful differences in investors across fund types, which might induce important variation in fund advisor incentives. Third, and perhaps most obviously, we extend prior research by providing evidence on the relation between performance and net flows at the monthly frequency. Large-sample analysis of the performance flow relation at the monthly interval is largely absent. Footnote 4 We find that net flows respond both contemporaneously and with a lag to fund performance—with the lagged effect diminishing over the next twelve months.Footnote 5 Together, these results suggest that at least some investors evaluate and respond to performance over windows shorter than previously identified. This is potentially important to the debate regarding how investors’ reaction to annual performance creates incentives for advisors to manipulate risk within a year. To the extent that investors evaluate and respond to performance over periods shorter than a year, the incentive for the advisor to manipulate risk within the year is diminished. The rest of the paper proceeds as follows. Section 2 describes the sample and provides preliminary analysis. Section 3 presents the empirical results for domestic equity funds. Section 4 extends the analysis to international equity funds and domestic hybrid funds and provides comparisons across fund types. In Section 5, we assess the robustness of our main empirical findings. In Section 6, we discuss the main implications of our results. Section 7 concludes.",28
38,4,Journal of Economics and Finance,17 May 2012,https://link.springer.com/article/10.1007/s12197-012-9233-z,An asset protection scheme for banks exposed to troubled loan portfolios,October 2014,Anders Grosen,Pernille Jessen,Thomas Kokholm,Male,Female,Male,Mix,,
38,4,Journal of Economics and Finance,01 June 2012,https://link.springer.com/article/10.1007/s12197-012-9237-8,Monetary policy implications of housing shift-contagion across regional markets,October 2014,MeiChi Huang,,,,Unknown,Unknown,Mix,,
38,4,Journal of Economics and Finance,07 June 2012,https://link.springer.com/article/10.1007/s12197-012-9235-x,The impact of private benefits on institutional ownership change: evidence from markets with different sentiments,October 2014,Yong Wang,,,,Unknown,Unknown,Mix,,
38,4,Journal of Economics and Finance,09 September 2012,https://link.springer.com/article/10.1007/s12197-012-9242-y,Causality-in-variance and causality-in-mean between the Greek sovereign bond yields and Southern European banking sector equity returns,October 2014,Go Tamakoshi,Shigeyuki Hamori,,Male,Male,Unknown,Male,"Since the onset of the recent Greek sovereign debt crisis, bank managers and monetary authorities not only in Greece but also in neighboring countries such as Portugal, Italy, and Spain have become more cautious about the relationships between their bank stock returns and changes in Greek government bond yields. Their imminent concerns seem to stem from the fact that these neighboring countries hold considerable amounts of Greek sovereign bonds. In fact, according to the results of the stress test conducted by the European Banking Authority in July 2011, the exposure of banks in Greece, Portugal, Italy, and Spain to Greek sovereign debt amounts to 54.4, 1.4, 1.4, and 0.4 billion euro, respectively. This degree of exposure implies that even a moderate level of haircuts on these debts might cause significant losses in the banking sectors of those countries. This conclusion suggests that it is worthwhile investigating the potential causality between Greek government bond yields and the bank stock returns of these four Mediterranean countries. Previous studies have shown that the causal linkage between bank stock returns and bond yields can display different directions and signs. Present value models imply that stock prices fall when long-term interest rates increase. Nonetheless, as Shiller and Beltratti (1992) contend, a positive relationship between stock prices and long-term interest rates can also exist when changes in interest rates carry information about the outlook for future dividends. Moreover, we can also consider the opposite causality, namely from stock returns to long-term interest rates. As Alaganar and Bhar (2003) argue, because stock markets have a forward-looking nature, current stock prices, especially those of the banking sectors whose profit levels can be closely related to interest rates, may reflect expectations about future interest rates. Another stream of research has empirically analyzed the relationships between bank stock returns and interest rates. Earlier studies that typically employed a two-index model (i.e., interest rates and market factors) under the assumption of constant variance yielded mixed results in terms of the causality between them. Further, several studies have also contended that interest rates do not significantly affect the stock returns of financial institutions (e.g., Lloyd and Shick 1977; Chance and Lane 1980). By contrast, Flannery and James (1984) and Bae (1990) provide evidence of the negative impact of interest rates on returns on banking sector stocks. As Bae (1990) points out, a varied construction of the interest rate series may be one reason for such mixed results. Moreover, recent studies of the relationship between interest rates and bank stock returns have assumed time-varying conditional variance. Based on this assumption, they have adopted different classes of the autoregressive conditional heteroskedastic (ARCH) models developed by Engle (1982) or the generalized autoregressive conditional heteroskedastic (GARCH) models proposed by Bollerslev (1986). Song (1994) was among the first to use an ARCH-type model in order to demonstrate that the time-varying risk measures of interest rates are incorporated in the pricing of U.S. banking sector stocks, while Elyasiani and Mansur (1998) employed a GARCH-in-mean (GARCH-M) model to identify the negative effects of long-term interest rates and their volatilities on both the means and the variances of U.S. bank stock returns. Tai (2000), using three different approaches including a multivariate GARCH-M model, confirms the significant impacts of interest rates, the world market, and exchange rate risks on U.S. bank stock returns. Using a multivariate GARCH model, Elyasiani and Mansur (2004) also find evidence of the significant influence of short-term and long-term interest rates and their volatilities on U.S. bank stock returns. Verma and Jackson (2008) extend that study by employing a multivariate exponential generalized autoregressive conditional heteroskedasticity (EGARCH) model in order to demonstrate the asymmetric influence of positive and negative interest rate changes on U.S. bank stock returns, indicating that bank equity returns are more sensitive to negative than positive changes in interest rates. Another approach in the literature employs cross-correlation function (CCF) methodologies primarily to investigate short-term dynamics in the relationship between interest rates and bank stock returns. Alaganar and Bhar (2003) find support for a two-way information flow between the interest rates and financial sector returns of G7 countries by using the causality-in-mean and causality-in-variance tests suggested by Cheung and Ng (1996). One of the key advantages of this approach is that it can detect not only the direction of causality but also the leads and lags structure of causality at the variance as well as at the mean levels. It is important to analyze causality-in-variance because volatility contains useful data on information flows, as Ross (1989) points out. In addition, Engle et al. (1990) attribute the volatility movement of asset price changes to the time necessary for investors to process new information. The present paper uses daily data from January 2007 to June 2011 in order to examine the causality-in-variance and causality-in-mean between long-term government bond yields in Greece and the banking sector stock returns of four Southern European countries, namely Greece, Portugal, Italy, and Spain. We thus extend the existing literature on the relationship between interest rates and bank stock returns in the following two directions. First, ours is one of the few studies that assess the two-way cross-border spillover of information flows between bond yields and bank stock returns. Most previous studies of this topic have investigated only within-country transmission effects. Indeed, we are among the first to study how the recent Greek sovereign debt crisis might affect the relationship between Greek long-term government bonds and the banking sector stocks in neighboring countries. Second, we use the CCF approach recently developed by Hong (2001). This methodology improves on Cheung and Ng’s (1996) model, which is constrained by weighting each lag uniformly, making no distinction between recent and distant cross-correlations. The results of our study are relevant not only for policymakers who intend to monitor and prevent cross-country spillover effects between sovereign bond yields and bank stock returns, but also for the bank managers and investors who manage the portfolios of banking sector stocks in the affected countries. The remainder of the paper is organized as follows. The next section presents the empirical framework used in this study, followed by an explanation of our dataset in Section 3. Section 4 reports our findings from the causality tests, while Section 5 concludes.",12
38,4,Journal of Economics and Finance,09 October 2012,https://link.springer.com/article/10.1007/s12197-012-9243-x,The long-term performance following dividend initiations and resumptions revisited,October 2014,Sheng-Syan Chen,Robin K. Chou,Yun-Chi Lee,Unknown,,Unknown,Mix,,
38,4,Journal of Economics and Finance,20 December 2012,https://link.springer.com/article/10.1007/s12197-012-9248-5,Herding in the strategic allocations of Spanish pension plan managers,October 2014,Laura Andreu,Cristina Ortiz,José Luis Sarto,Female,Female,Male,Mix,,
38,4,Journal of Economics and Finance,20 December 2012,https://link.springer.com/article/10.1007/s12197-012-9246-7,Technological change and the U.S. real interest rate,October 2014,Constantine Alexandrakis,,,,Unknown,Unknown,Mix,,
38,4,Journal of Economics and Finance,08 March 2013,https://link.springer.com/article/10.1007/s12197-013-9253-3,"Greek sovereign bond index, volatility, and structural breaks",October 2014,Go Tamakoshi,Shigeyuki Hamori,,Male,Male,Unknown,Male,"The sudden large shocks to the Greek sovereign bond markets during the recent European debt crisis stunned the market participants. This is primarily because there is convention of treating government bonds as risk-free based on a sovereign entity’s ability to either raise taxes, cut spending, print money or some combination of the three. Until recently, government debt markets in developed nations had been regarded as proxy for the unobservable risk-free rate (e.g., Blanco et al. 2005; Delis and Mylonidis 2011). Since the introduction of the Euro and until the onset of the recent financial crises, many investors believed Euro-zone government bonds behave as if they were free of country-specific risks (e.g., Oliveira et al. 2012). The present article is motivated by the implications of excess volatility in sovereign bond markets on the decision making of both investors and policymakers. First, extreme volatility may disrupt the investors’ abilities to forecast and manage the interest risks of assets held in their portfolio. This is a significant issue especially for those traders constructing the portfolio, whose value could directly be affected by interest rate variability. Second, a striking increase in volatility disrupts the smooth functioning of the financial system, because financial institutions holding substantial amounts of sovereign bonds may suffer from potential impairment losses. Indeed, this was observed for major European banks during the debt crisis period. A sudden change in bond yield volatility may motivate policy makers to consider the introduction of new regulatory schemes. Given such importance of excess volatility, we explore empirical models of time-varying conditional volatility for the 10-year Greek government returns within a regime switching framework. Many studies have modeled volatility by controlling for structural changes in the case of stock returns (e.g., Cunado et al. 2004; Malik et al. 2005; Hammoudeh and Li 2008; Wang and Moore 2009), exchange rate returns (e.g., Malik 2003; Rapach and Strauss 2008), and real Gross Domestic Product (GDP) growth (e.g., Cecchetti et al. 2006; Fang and Miller 2008, 2009). However, few studies have focused on investigating the sudden volatility changes in sovereign bond markets. A rare example is Covarrubias et al. (2006), who modeled the volatility of the 10-year US Treasury note and detected its structural changes using the iterated cumulative sums of squares algorithm. They found that the identified regime shifts were associated with main economic events, and that volatility persistence measure in a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) modelFootnote 1 largely decreased when considering such regime shifts. Moreover, their empirical results indicated that a volatility model based on GARCH, controlling for structural breaks, was superior when performing out-of-sample forecasts. To our knowledge, this study is the first to examine structural changes in the volatility of sovereign bond index returns in the context of the European debt crisis. We focus on the 10-year sovereign bond yields of Greece, the origin of the debt crisis, covering the sample period April 1999 to March 2012. We use the multiple structural change test by Bai and Perron (1998, 2003) to detect the break dates in volatility and incorporate them into an Exponential GARCH (EGARCH) model.Footnote 2 One of the major advantages of this methodology is that it allows for estimating multiple structural changes endogenously. It also enables us to generalize specifications, for example, to select whether to allow for heterogeneity and autocorrelation in the residuals. Our results show that for the Greek government bond returns, the structural break date in the variance and mean is April 2010. Incorporating structural break dummy variables into our EGARCH-based model results in superior estimation results, and the positively significant coefficient of the dummy on structural break in variance suggests that the regime shift was triggered by the European sovereign debt crisis. Moreover, we find that a measure of volatility persistence decreases substantially when incorporating the structural change into our model, which is basically consistent with the finding of Covarrubias et al. (2006).Footnote 3
 The rest of the article is structured as follows. Section 2 provides a brief explanation of our dataset. Section 3 describes our empirical findings. The last section concludes the article.",14
38,4,Journal of Economics and Finance,21 May 2013,https://link.springer.com/article/10.1007/s12197-013-9260-4,The impact of the disposition effect on asset prices: insight from the NBA,October 2014,Richard Borghesi,,,Male,Unknown,Unknown,Male,"Researchers frequently turn to sports betting markets to test theories of investor behavior.Footnote 1 The reason is that sports betting and equities markets have important similarities that make comparisons and inferences meaningful and useful. One significant advantage of examining sports betting markets is that the joint hypothesis problem is mitigated because, unlike in traditional equities markets, the true value of the security (bet) is clearly revealed when the underlying contest is over. The Tradesports market has been the focus of several studies that address price momentum and the information processing power of investors during periods of low and high information flow. For instance, Borghesi (2007) finds that NFL contracts are most overpriced after information shocks occur, especially when that news is negative. This market also seems to be ideal for testing the disposition effect (Shefrin and Statman 1985), a phenomenon frequently attributed to the S-shaped value function of Kahneman and Tversky’s (1979) prospect theory, and which proposes that investors are reluctant to realize a loss by selling an asset that has declined in value since purchase. Conversely, the theory also proposes that investors tend to sell assets that have increased in value in order to experience the utility of realizing that gain. The result is that assets which have previously lost (gained) value tend to perform worse (better) than expected after the investor unwinds her position. The disposition effect has been affirmed in a variety of theoretical, experimental, and empirical research (Weber and Camerer 1998; Barberis and Xiong 2009; Henderson 2012) and has been used to explain anomalous equities returns (Grinblatt and Han 2005; Frazzini 2006). Further, Ben-David and Hirshleifer (2011) contend that there is a significant time element to the disposition effect, with short holding periods subject to a stronger effect, while Kaustia (2010) proposes that prospect theory is unlikely to explain the disposition effect. However, any conclusions made from studies that rely on observing asset prices relative to modeled values (e.g., Jin and Scherbina 2011; Goetzmann and Massa 2008) may be driven by a lack of precision in calculating true asset value (the joint hypothesis problem). Further, studies that examine equities markets may be affected by traders’ erroneous belief in mean reversion of stock returns (Odean 1998). In betting markets, the joint hypothesis problem is mitigated because contract values (win/lose outcomes) can be clearly observed. So, by examining Tradesports data we can provide a clean dynamic test of the disposition effect and determine whether it systematically causes significant deviations between true asset values and prices. In this study we utilize the Tradesports market for NBA totals contracts as a laboratory to test for the presence of the disposition effect, which causes traders to hold losing positions to avoid realizing losses yet prematurely unwind winning positions to realize gains. We find that contract returns are consistent with the presence of a strong disposition effect. That is, contracts which have fallen off pace to exceed the stated total become overpriced and experience negative future returns while contracts on games in which teams are on pace to exceed the stated total become underpriced and experience significantly positive future returns. We also find evidence that investors become more emotionally attached to their buy/sell decision over time, which further supports the theory that the disposition effect causes significant deviations between prices and values.",2
38,4,Journal of Economics and Finance,08 March 2014,https://link.springer.com/article/10.1007/s12197-014-9280-8,An exploratory empirical inquiry into the impact of federal budget deficits on the ex post real interest rate yield on ten year Treasury notes over the last half century,October 2014,Richard J. Cebula,Fiorentina Angjellari-Dajci,Maggie Foley,Male,Female,Female,Mix,,
39,1,Journal of Economics and Finance,12 May 2012,https://link.springer.com/article/10.1007/s12197-012-9234-y,Can we consistently forecast a firm’s earnings? Using combination forecast methods to predict the EPS of Dow firms,January 2015,Naresh Bansal,Jack Strauss,Alireza Nasseh,,Male,Unknown,Mix,,
39,1,Journal of Economics and Finance,26 May 2012,https://link.springer.com/article/10.1007/s12197-012-9236-9,An empirical analysis of the impact of large changes in institutional ownership on CEO compensation risk,January 2015,Yixi Ning,Xiankui Hu,Xavier Garza-Gomez,Unknown,Unknown,Male,Male,"The debate on CEO overpay has been an ongoing issue since the start of the financial crisis in 2008. Proposed legislation by the U.S. Department of the Treasury would give “the SEC authority to require companies to give shareholders a non-binding vote on executive compensation packages.” (Statement by Treasure Secretary Tim Geithner on Compensation on June 10, 2009). On July 31, 2009, The House of Representatives passed the “Corporate and Financial Institution Compensation Fairness Act of 2009” (H.R. 3269), which included a section that allowed for a “say on pay” for all public institutions in the United States. Additionally, it had a provision for a shareholder vote on golden parachutes. The similar Shareholder Bill of Rights has also been introduced in the Senate. The Wall Street Reform and Consumer Protection Act signed by President Obama on July 27, 2010, strengthens shareholder rights by legally providing shareholders with a say on pay with the right to a non-binding vote on executive compensation. So an interesting research issue is to examine how shareholders, especially institutional investors, influence corporate executive compensation practices in firms they invest in. With an aggregate ownership of over 50 % of the U.S. equity market, institutional investors have emerged as an important external control mechanism in mitigating agency problems as documented in prior studies (e.g. Shleifer and Vishny 1986; Gillan and Starks 2003; Hartzell and Starks 2003). Large institutional shareholders can influence management through various means such as their voting power, election of board members, and shareholder activism. For example, the California Public Employees’ Retirement System (CalPERS) voted against 43 % of executive stock plans from 1999 to 2000, and TIAA-CREF has been listing and filing resolutions in many firms in which executive compensation has been abused (Khan et al. 2005). An extreme example of large institutional investors is state-controlled ownership. Several studies (i.e. Cragg and Dyck 2000; Firth et al. 2007) find evidence that CEOs in state-controlled companies earn significantly lower compensation than their peers in publicly-traded companies. While numerous studies in the literature have documented how institutional holdings affect executive total pay and pay-for-performance sensitivities (Hartzell and Starks 2003; Almazan et al. 2005), very few studies have explored their effect on the risk level of CEO compensation (from now on referred as CEO compensation risk) which is defined as the percentage of equity-based (incentive) compensation relative to total CEO pay (Toyne et al. 2000). A rarely-explored area is how the dispersions of institutional ownership influence the risk of chief executive pay. We use two forms of institutional ownership dispersion, namely, concentration and aggregation, to analyze the impact of institutional holdings on CEO compensation risk. In this study, we focus on the impact of large changes in institutional holdings on CEO compensation risk because such substantial changes are more likely to result from institutions’ strategic considerations. With a large equity stake in a company, institutional investors possess strong incentives to directly monitor and thus improve the firm’s corporate governance system. However, for general institutional investors with relatively small-size holdings, direct monitoring is too costly. Therefore, they may choose to voice their satisfaction or dissatisfaction with the corporate governance practices through buying or selling their firm shares (Parrino et al. 2003). We conduct cross-sectional and endogenous examinations of a panel of publicly traded firms with large changes in institutional ownership during the period between 1992 and 2003 while controlling for various firm-level and CEO characteristics. We find that the CEO compensation risk was much higher in the firms with a higher level of aggregate institutional ownership excluding top five holdings, a larger market value of equity, and more investment opportunities. On one hand, boards of directors in firms with high level of institutional holdings might be pressed to improve CEO pay structure characterized by a higher CEO compensation risk. On the other hand, the boards, on behalf of shareholders, might also be willing to design a better CEO pay arrangement to attract institutional buying and holdings of their company’s shares. In contrast to the effect of aggregate institutional ownership on executive pay, we find that the concentrated institutional ownership, measured by top five to total institutional holdings and institutional Herfindahl index, had an inverse impact on CEO compensation risk. The presence of large outside shareholders is likely to result in closer monitoring and thus reduce top managers’ influence on their pay (Shleifer and Vishny 1986). When top institutions pressure boards of directors to lower CEO total compensation (Hartzell and Starks 2003; Almazan et al. 2005) under the concern of public outrage (Bebchuk and Fried 2003), they are more likely to help reduce the non salary compensation (Cyert et al. 2002) and stock option granted to top executives (Benz et al. 2001), but not the cash-based compensation. These attempts might lead to a lower level of CEO compensation risk, which is defined as the equity-based compensation over total CEO pay. This means that top institutional holdings are unable to boost the risk level of CEO pay when they intend to control the total amount of CEO pay packages. We need to note that the compensation risk variable in this study is the proportion of total equity-based compensation (stock option grants and restricted stocks) over total CEO pay, which is not the pay-for-performance sensitivity variables widely used in previous literature (e.g. Hartzell and Starks 2003) We organize the remainder of the paper as follows. Section 2 reviews previous literature and develops hypotheses. Section 3 describes data selection, methodologies, variable definitions, and descriptive statistics. Section 4 presents the empirical analysis and Section 5 has some robustness tests. We conclude the paper in Section 6.",3
39,1,Journal of Economics and Finance,06 June 2012,https://link.springer.com/article/10.1007/s12197-012-9232-0,Property-type diversification and REIT performance: an analysis of operating performance and abnormal returns,January 2015,Randy I. Anderson,Justin D. Benefield,Matthew E. Hurst,,Male,Male,Mix,,
39,1,Journal of Economics and Finance,24 June 2012,https://link.springer.com/article/10.1007/s12197-012-9239-6,The effects of bank capital constraints on post-acquisition performance,January 2015,Chris Brune,Kevin Lee,Scott Miller,,Male,Male,Mix,,
39,1,Journal of Economics and Finance,02 November 2012,https://link.springer.com/article/10.1007/s12197-012-9245-8,Mean and volatility transmission for commodity futures,January 2015,Terrance Grieb,,,Male,Unknown,Unknown,Male,"An important issue in the finance literature concerns the transmission of volatility across national boundaries. King and Wadhwani (1990), using contagion theory, argue that in a less than fully revealing equilibrium, investors and security traders respond directly to publicly available information and also observe price changes in other markets as a proxy for information that is not available in their local markets. Hence, information shocks in one market may have an impact on the pricing and volatility of other markets. In the context of international investing, this has become a concern for investors and policy-makers. The work done in this area also has potential applications for the study of price discovery and risk-pricing interaction between asset classes. Most of the studies in this branch of the literature employ a version of the generalized autoregressive heteroskedasticity (GARCH) procedure in order to examine the extent to which the first and second moments of the returns distributions of one market are influenced by information shocks in other markets. Koutmos and Booth (1995) and Kanas (1998) use bivariate Exponential GARCH (EGARCH) models to observe volatility spillovers between developed countries. These studies document larger conditional volatility in down-markets than in up-markets, consistent with the leverage effect (Black 1976). In addition, both studies find significantly higher degrees of volatility transmission among all markets after the October 1987 stock market crash. Similar transmissions for markets in developed countries are documented by Theodossiou and Lee (1993); Theodossiou et al. (1997); Susmel and Engle (1994), and for emerging markets by Liu and Pan (1997); So et al. (1997), and Booth et al. (1997). Hamao et al. (1990) use a two-step GARCH-M procedure which allows them to simultaneously observe transmission effects for a large number of countries. They test for price and volatility spillovers pre- and post-October 1987 and find limited volatility spillovers pre-crash, with strong price and volatility spillovers after the crash. While this branch of the literature deals mostly with international investments, this paper suggests that an interesting application of King and Wadhwani’s contagion theory, and of the mean-return and volatility transmission methodology, can be applied to commodity markets within the United States. In particular, commodity markets are quickly becoming a more standard asset class within domestic U.S. markets, so price discovery behavior is becoming a topic of interest. For example, Driesprong et al. (2008) use a standard OLS regression model to document a relationship between oil futures and world stock market returns. Lien and Yang (2008) demonstrate that the minimum variance hedge ratio for a given commodity is affected differently by a positive basis (i.e., backwardation) than a negative basis (i.e., contango). Tse (1998) applies an EGARCH model to Euroyen and Eurodollar futures and finds no evidence of volatility spillovers, although the results are consistent with covered interest arbitrage. Chng (2009) uses a vector autoregression to document information transfer across auto industry related commodities (i.e., rubber, palladium and gasoline). However, a broader examination of information transfer effects across commodities is not currently represented in the literature. Our study seeks to fill this gap. In this study, we employ the two-stage GARCH-M procedure proposed by Hamao et al. (1990) to study spillover effects between futures contracts for nine commodity markets: corn, rough rice, soybeans, wheat, feeder cattle, lean hogs, live cattle, crude oil, and natural gas. We employ a two-stage technique because a multivariate GARCH-M estimation with nine assets requires a large number of coefficients to be estimated, and convergence is very difficult to achieve. Using the two-step procedure, we also investigate the influence of external markets on commodity returns. In particular, we examine transmissions from Eurodollar, S&P500, and currency futures contracts to each of the commodities. Eurodollar returns provide a proxy for the cost of carry relative to a futures contract, while the S&P500 provides information from the broader markets as well as from a competing asset class. The relative strength of the dollar has a direct effect on both demand and supply for commodities produced in the U.S. The format of this paper is as follows. Section 2 “Data and methodology” describes the data and the methodology. Section 3 “Results” presents the empirical results and the conclusions are summarized in Section 4 “Conclusion”.",
39,1,Journal of Economics and Finance,23 February 2013,https://link.springer.com/article/10.1007/s12197-013-9250-6,The effects of stock splits on stock liquidity,January 2015,Gow-Cheng Huang,Kartono Liano,Ming-Shiun Pan,Unknown,Unknown,Unknown,Unknown,,
39,1,Journal of Economics and Finance,01 March 2013,https://link.springer.com/article/10.1007/s12197-013-9251-5,A time series test to identify housing bubbles,January 2015,Diego Escobari,Damian S. Damianov,Andres Bello,Male,Male,Male,Male,"The recent housing boom and bust in the United States was marked by large differences in the run-up and the subsequent decline of housing prices both across metro areas and across market segments in the same area. One common phenomenon observed in many metro areas is that the low-tier homes realized the largest price gains during the boom and saw the sharpest declines during the bust of the market. There is now a consensus in the rapidly growing empirical literature on the housing boom and bust that subprime lending and low interest rates were major contributing factors to the bubble (see e.g., Mayer 2011 for a recent survey). These factors, however, have a differential effect on the price tiers. Landvoigt et al. (2011) present a theoretical model in which movers of different age, income and wealth, demand houses that differ in quality. These three dimensions of mover characteristics and the quality of houses are then mapped into an equilibrium distribution of house prices. Applying micro data on buyer characteristics and house prices from San Diego to this model, Landvoigt et al. conclude that “cheap credit for poor agents was most important in generating higher capital gains at the low end of the market.” The links between lending behavior by banks, default rates, and real estate prices have been studied theoretically in a recent paper by Hott (2011). This model explicitly accounts for the relationship between lending practices by banks and housing prices and explains why housing prices exhibit cyclical movements. It is important to note that asymmetries in the movements of different housing tiers appear when credit is extended to a varying degree in the different segments of the housing market. It is also plausible to assume that these asymmetries continue to persist, or are even exacerbated under alternative (behavioral) assumptions regarding the lending behavior of banks, i.e. mood-swings, momentum forecasts, and disaster myopia (see Hott 2011, pp. 35–42). In this paper, we present a new empirical test for the existence of housing bubbles which exploits the specific feature that low tier homes appreciate more during the boom and fall faster during the bust of the market. We use time series data of the S&P home price tiers to identify the exact dates at which housing market bubbles emerge and burst. Our methodology does not require information on market fundamentals. Instead, it analyzes the differences in the rate of change of the tiered price indices to identify breaks, which correspond to the origin and the burst of the bubbles. We implement our empirical test on the metropolitan areas covered by the Case-Shiller tiered price indices.Footnote 1 The procedure allows the data to endogenously dictate the breaks, which mark the beginning and the end of the housing market bubbles. The results show that from the metropolitan areas considered in the analysis, all bubbles started between June 1997 (with Seattle, WA) and May 2001 (with Washington DC). Moreover, all bursts occurred between June 2006 (in San Diego, CA and Tampa, FL) and July 2008 (in Portland, OR). The bubble that lasted the longest was the one in Seattle, between June 1997 and July, 2007. It was in the San Diego metropolitan area where the high-tier prices went up the most, increasing by 134.5 % (between January, 1999 and June, 2006). In other cities the increase in high-tiered prices was less severe. For example, in Minneapolis the increase was 48.4 % (between November, 2000 and September, 2007), while in Portland it was 56.3 % (between June, 1997 and November, 2008). Within the same city, the appreciation in the low-tier prices was greater. In San Diego the increase was 236.4 %, while in Minneapolis and Portland the increase was 81.4 % and 115.5 %, respectively. The extant literature on market bubbles has taken two distinct approaches to identify bubbles. The main approach views bubbles as a rapid and unsustainable growth in asset prices that cannot be explained by “fundamental” factors. In his summary article on the symposium on bubbles, in the Journal of Economic Perspectives, Stiglitz wrote that “[I]f the reason that the price is high today is only because investors believe that the selling price will be high tomorrow—when “fundamental” factors do not seem to justify such a price—then a bubble exists” (Stiglitz 1990, p. 13). Using this definition, a number of empirical tests have been developed to exploit the link between asset prices and various fundamental values. West (1987) proposes an empirical test for the existence of a bubble using the constant expected return model. His approach relies on comparing two sets of parameters. One set of estimates is obtained by a projection of stock prices based on past dividends, and the other is obtained by a set of equations describing the discount rate and the dividend process. This and other tests to identify bubbles are reviewed in Flood and Hodrick (1990). Meese and Wallace (1994) examine whether the real expected return on home ownership is close to the real homeowner cost of capital by studying the relationship between price, rent, and the cost of capital. Abraham and Hendershott (1993, 1996) study the relationship between housing prices and construction cost, real income growth and interest rate. They find that these factors explain half of the historical variation in house price appreciation. The bubble, then, manifests itself in the “sustained serially correlated deviations,” yet, it remains unclear whether these deviations are due to a “bubble” or to a misspecification of the econometric model. Himmelberg et al. (2005) compare the level of housing prices to local rents and incomes for a period of 25 years. They explain that changes in the price-to-rent and price-to-income ratios might suggest the existence of bubbles even when the houses are reasonably priced because they fail to account, for example, for differences in risk, property taxes and maintenance expenses, and anticipated capital gains from owning a home. Glaeser et al. (2008) present a theoretical model of housing bubbles which predicts that areas with more elastic supply will have fewer bubbles with shorter duration and smaller price run-ups. Their data indicate that the price increases in the 1980s were almost exclusively experienced in areas with inelastic supply. The existence of speculative bubbles of real estate assets has been recently examined for real estate investment trusts (REITs) markets. Waters and Payne (2007) develop econometric approaches for detecting the presence of (positive and negative) periodically collapsing bubbles (see Evans 1991) for four classes of REITs: equity, mortgage, hybrid, and all REITs. They find evidence of negative periodically collapsing bubbles in which prices fall significantly below the asset’s fundamental value. The alternative approach to the analysis of bubbles, promoted by Case and Shiller, views housing bubbles as a result of unrealistic expectations of future prices sustained by speculative feedback and social contagion. In addition to the analysis of “fundamentals” —including personal per capita income, population, and employment— for the time period 1985–2003, Case and Shiller (2003) present the results of a survey of people who bought houses in 2002. This survey asked respondents a set of questions about their expectations of future prices and whether they feared that houses will become unaffordable in the future. The article reports that the term “housing bubble” had essentially no popularity prior to 2002 while the term “housing boom” had been in much more frequent use since the 1980s. An extensive overview of these approaches to understanding housing bubbles, and housing dynamics in general, is presented in Mayer (2011). Recent tests for speculative bubbles in regional US housing markets typically consider deviations from market fundamentals. Goodman and Thibodeau (2008) explore to what extent house appreciation rates over the time period 2000–2005 can be attributed to economic fundamentals, and what portion can be attributed to speculation. According to these authors much of the appreciation is due to inelastic supply, and speculative motives are present in less than half of the cities they examine. Mikhed and Zemčík (2009) present a panel test to detect bubbles using price-rent ratios for the period 1975–2006. The bubble indicator they construct detects bubbles around the decade turn in the late 1980s and the early 1990s as well as around the end of the 1990s. Peláez (2012) argues that the housing bubble in the late 1990s and the early 2000s could have been predicted when considering the unprecedented growth rate of the house price to per capita income ratio. The main innovation in this paper lies in identifying bubbles without observing fundamentals and without the reliance on surveys or on measurements of sentiment. This approach can be implemented in housing markets due to the availability of the tiered price indices. Defining the relevant periods in which bubbles grow and collapse opens new venues for future research on the impact of fundamentals on housing price movements both in and outside of the bubble periods. There is a rapidly growing strand in the recent literature on housing price dynamics which tries to identify the effects of various fundamental values on prices. Using simulation of the US housing market, Khandani et al. (2009) find that the declining interest rates and the growth of the refinancing business contributed significantly to the recent housing boom and the massive defaults during the bust. Favilukis et al. (2010) argues that much of the housing price appreciation can be explained by relaxation of credit constraints and Mayer and Sinai (2009) show that markets with the highest subprime lending experienced the greatest growth in price-to-rent ratios. In contrast, Glaeser et al. (2010) present evidence supporting the view that easy credit, in the form of low real interest rates and permissive mortgage approval standards is not a strong contributor to the rising house prices. Our approach permits these relationships to be revisited in the context of the relevant time period in each metropolitan area because we do not use fundamental factors to determine the bubble periods. The organization of the paper is as follows. In Section 2 we describe the data. The empirical model, the estimation methods, and the identification strategy are outlined in Section 3. Section 4 presents the estimation results and Section 5 concludes.",9
39,1,Journal of Economics and Finance,15 March 2013,https://link.springer.com/article/10.1007/s12197-013-9255-1,Health care and the cross-section of US stock returns,January 2015,Brian C. Payne,John M. Geppert,,Male,Male,Unknown,Male,"Modern asset pricing theory seeks to find underlying economic, or risk, factors that account for the temporal and cross-sectional variation in asset returns. The literature has progressed from the aggregate market return as the only relevant factor (Sharpe 1964, and Lintner 1965, CAPM), to multifactor models based on the Ross (1976) arbitrage pricing theory (APT) or consumption (C-CAPM and ICAPM). Considering health care costs as one such risk factor, we find medical inflation contributes to the cross-section of stock returns even in the presence of previously considered factors. The literature relating stock returns to aggregate inflation dates as early as Bodie (1976), who finds an anomalous negative relation between stock returns and inflation.Footnote 1 Fama (1981) and Geske and Roll (1983) argue that the observed relation between inflation and stock returns is spurious. In contrast, Modigliani et al. (1979) offer a real causal link between inflation and asset returns based on investor mispricing. Incorporating US health care costs (i.e., medical inflation) into a factor pricing model is materially different than previous research that has used aggregate inflation. Medical inflation acts as a priced factor because of its unique impact on individual consumption and firm cash flows. In the context of modern asset pricing theory, investors attempt to smooth their consumption. Assets with returns that covary positively with consumption are risky, while those that covary negatively with consumption act as hedges. Within the context of the ICAPM, investors demand assets with returns that covary positively with medical expenditures, and these assets will command a lower expected return based on their hedging function. Traditionally, consumption is modeled at the aggregate level; however, medical care consumption warrants special treatment from other items in the consumption basket because of its potential catastrophic impact on individuals and its high demand inelasticity. Rather than use changes in medical consumption expenditures directly, we proxy medical care consumption with the medical care component of the consumer price index (CPI). Using the price index data in place of medical expenditures has the advantage that medical inflation data are available monthly, which corresponds to the frequency of our return data. Comparing the OECD figures for annual per person health care expenditures to annual medical inflation figures supports this proxy decision. Between 1985 and 2008, per person health care expenditures in the US increased approximately 6 % annually; annual medical inflation increased 5 % annually over the same period. Thus the consumed quantity of medical expenditures changes at approximately the same rates as medical inflation, and indeed the two annual series are highly correlated with a Pearson value exceeding 0.80. As for the justification to separate medical inflation from aggregate inflation, we note medical inflation is distinct from aggregate inflation and its other components in its degree, its statistical properties, its ability to be hedged, and its impact on consumers and firms. The extreme level of the medical component of the CPI is illustrated in Fig. 1. Although we recognize levels are not of ultimate interest, this figure demonstrates the marked divergence of medical goods from others in the consumption basket during the 25-year study period. Consumer Price Index (CPI) levels. This Figure shows the monthly index levels for the aggregate CPI (CPIAUCSL) and its major components: Medical CPI (CPIMEDSL), Housing CPI (CPIHOSSL), Food CPI (CPIUFDSL), and Transportation CPI (CPITRNSL) from January 1967 to August 2009. Data comes from the St. Louis Federal Reserve Economic Database (FRED) The medical component of the Consumer Price Index (CPI) breaks from the other components of the CPI in approximately 1985. The CPI components in Fig. 1 currently represent almost 80 % of the aggregate CPI, with the Medical component representing 6.39 %, Housing 43.42 %, Food 15.76 %, and Transportation 15.31 %.Footnote 2
 Medical inflation also differs from other items in the CPI basket in terms of individuals’ and firms’ ability to manage the associated costs. Certainly the use of medical insurance allows consumers to partially “smooth” their consumption of medical expenses over time. However, the empirical evidence on personal bankruptcies suggests that even with insurance medical expenses are unique and cannot be fully pooled. For example, Himmelstein et al. (2009) find 62 % of personal bankruptcies filed in 2007 were linked to medical expenses even though nearly 80 % of those filing for bankruptcy had health insurance. This rate of medical-cost-induced personal bankruptcies has increased by almost 50 % since 2001. The statistical properties and elasticities of medical expenses distinguish it from other components of the CPI basket. The five components of Housing, Food, Energy, Education and Transportation account for over 85 % of the CPI. Unlike Food and Energy, which have price movements that contain a large transitory component, shocks to medical inflation are largely permanent.Footnote 3 While shocks to Transportation can have a significant impact on consumers, there are a variety of methods for hedging these shocks—for example, public transportation, carpooling, or choosing less discretionary travel.Footnote 4 Another often discussed consumer good whose inflation outpaces aggregate inflation is Education. However, one can manage education expenses over time by saving in advance or through borrowing.Footnote 5 Housing inflation has an ambiguous effect on consumers due to the asset characteristics associated with housing and one’s ability to borrow against housing appreciation. However, even accounting for the recent wide swing in housing prices, the average annual real change in housing prices between 1948 and 2008 is just over one-half percent; the 95th and 5th percentiles are 11 and −10 %, respectively. Thus, even though Housing appreciation is skewed, it is much less so than medical expenditures. For instance, in 2004 the average annual medical expenditures for individuals ages 19 to 64 were approximately $4,500. A single medical event such as a coronary bypass surgery with 10 days of hospital stay can easily cost $50,000, or over 10 times the annual average. To be clear, we are not arguing that the idiosyncratic risk to an individual consumer of bearing high medical costs is a priced factor in the market. Rather, the consumer or investor will pay a premium for assets that provide a hedge against medical shocks (i.e., assets that covary positively with medical inflation). The expected return on these assets will reflect this covariance with medical inflation distinct from traditional factors such as the market, SMB, or HML. We confirm this explanation in subsequent sections. The above discussion illustrates how medical inflation differs from other components of inflation from the standpoint of the individual consumer. Medical inflation is also a unique expense for firms relative to other input costs. Depending on the industry, firms can manage many of the input costs by altering suppliers, changing product mixes, or using formal commodity derivative markets. Managing labor costs—the avenue through which medical inflation bears upon the firm—is more constrained. Firms can alter the capital/labor production mix to some degree, use part-time workers who are not compensated with health benefits, pass the cost onto customers, or relocate internationally. The firm’s remaining medical expenses must be borne by the firm in the form of lower cash flows.Footnote 6
 Medical expenses (i.e., inflation) are therefore different in magnitude and kind from other consumer and business expenses. The ability of investors to diversify away these expenses—or equivalently, whether the markets differentially price assets that hedge medical inflation—poses an empirical question that the remainder of this paper addresses using a standard factor pricing methodology.",2
39,1,Journal of Economics and Finance,16 July 2013,https://link.springer.com/article/10.1007/s12197-013-9264-0,The effect of the U.S. federal income tax appraisal requirement on noncash charitable contributions for individuals,January 2015,James S. Serocki,Kevin J. Murphy,,Male,Male,Unknown,Male,"The charitable contribution deduction under the U.S. federal income tax system is allowed under Internal Revenue Code (IRC) Section 170. Generally, the charitable contribution deduction is currently available to individual taxpayers who itemize their deductions, with numerous limitations on the charitable contribution deduction (e.g. percentage limitations based on types of property, adjusted gross income). There have been many changes over the years to the charitable contribution deduction since its enactment in 1917. Prior to 1985, there was a general disclosure requirement with an attachment to the individual tax return for claiming noncash contributions, including description of the property and estimated value. After 1984, there were a couple of important law and regulatory changes for certain noncash contributions. Buchheit et al. (2005) document the impact of one such law change on noncash contributions. Specifically, they address the effect of elimination of the substantiation requirement for noncash gifts of $500 or less that was implemented via Treasury Decision 8002 (TD 8002). They hypothesize and provide empirical evidence that the elimination of the substantiation requirement for small noncash charitable contributions induced a surge of claims under the $500 limit. According to a Treasury report “there are still very real problems with noncash charitable contributions,” which U.S. Senator Charles Grassley, the Senate Finance Committee’s ranking Republican, asserts “contribute significantly” to the nation’s “tax gap”—the difference between what the IRS collects each year and what it thinks it should be collecting (Herman 2007). The “tax gap” was also discussed by Fischer et al. (1992) in the context of taxpayer motivations due to detection probability. A bipartisan federal tax reform panel recommended in part that taxpayers sell their noncash property (without recognizing gain) and contribute the proceeds as a way to curb valuation abuses (Report 2005). Kovach (2006) notes that this sale and proceed contribution anti-abuse provision was implemented under the 2004 Act with respect to vehicles (e.g. cars) but not to most noncash property contributions. Another study (Ackerman and Auten 2006) found evidence that a proposal to extend the charitable deduction to non-itemizers in conjunction with a floor (e.g., one percent of adjusted gross income) would lead to increased charitable contributions. Some more recent proposals include reducing the tax benefit of the charitable contribution deduction for higher income taxpayers (Orszag 2009). A change in the law occurring in 1984 not previously studied is that the substantiation burden for taxpayers deducting noncash gifts totaling more than $5,000 increased significantly. Specifically, under the mandate of the 1984 Tax Reform Act P.L. 98–369, effective for charitable contributions after 1984, a regulatory provision modifying Reg. Section 1.170A-13(c) requires a “qualified appraisal” for noncash contributions greater than $5,000 (RIA 2007). Final Regulation 1.170A-13(c), effective after 1984, was established implementing section 155 of the Tax Reform Act of 1984, under TD 8199 (TD 8003 implemented the temporary regulations). In Reg. Sec. 1.170A-13, a qualified appraisal must be done contemporaneously to the donation by a qualified appraiser according to rigorous requirements, with a required “appraisal summary” (with IRS Form 8283) to be attached to the donor’s tax return.Footnote 1 Regulation 1.170A-13 was finalized pursuant to TD 8002, published effective as of January 1, 1983. In 2004, the American Jobs Creation Act P.L. 108–357 was enacted, basically codifying under the IRC, the rules under Reg. Section 1.170A-13(c) as IRC 170(f)(11) which remains substantially the same currently. This most recent change requires a qualified appraisal generally for noncash property contributions over $5,000 after 1984 (RIA 2007).Footnote 2 In 2013, the IRS has agreed to revise Form 8283 and certain audit procedures, in response to a 2012 Treasury Inspector General for Tax Administration (TIGTA) report (KPMG 2013). The TIGTA report found about 60 % of the taxpayers claiming more than $5,000 noncash contributions on their 2010 tax returns did not comply with the noncash charitable contribution reporting requirements (TIGTA 2013). This paper focuses on post 1984 tax year developments that affect noncash contributions of more than $5,000. Specifically, we examine taxpayer behavior response to TD 8199, a tax law change that increased taxpayer documentation requirements for noncash contributions over $5,000.",
39,1,Journal of Economics and Finance,23 October 2013,https://link.springer.com/article/10.1007/s12197-013-9274-y,"CEO age, education, and introduction of hedging in the oil and gas industry",January 2015,Zahid Iqbal,,,Male,Unknown,Unknown,Male,"Prior studies show that oil and gas firms are subject to price risk [e.g., Al-Mudhaf and Goodwin (1993), Jin and Jorion (2006), Scholtens and Wang (2008), Mohanty and Nandha (2011), Gogineni (2010), and Bredin and Elder (2011)]. To reduce price risk, these firms engage in hedging activities [e.g., Haushalter (2000) and Jin and Jorion (2006)]. In this study, we examine the extent to which CEO age and education explain the introduction of hedging in the oil and gas industry. There is evidence that certain managerial characteristics impact corporate decision making [e.g., Cronqvist et al. (2012), Malmendier et al. (2011), Malmendier and Nagel (2011), and Graham et al. (2010)]. Among them, CEO age and education are the two important attributes. There are opposing arguments on their impact, however [e.g., Holmstrom (1999), Zwiebel (1995), Hambrick and Mason (1984), and Prendergast and Stole (1996) on CEO age; and Gottesman and Morey (2010) and Bhagat et al. (2011) on CEO education]. Our study differs from prior studies in several respects. First, we examine the age of a CEO at the time hedging is introduced in the firm. Prior study by Serfling (2012), for example, examined CEO age in relation to stock return volatility, but not at the time hedging is first introduced. It is quite possible that the CEO initiated hedging practice when he was young. Our study examines CEO age at the time the hedging decision is first made. Second, prior studies [e.g., Haushalter (2000) and Jin and Jorion (2006)] focus on the conventional motives of hedging in the oil and gas firms, but not on the CEO characteristics. It is important to know if age and educational background of the CEOs explain hedging decision for these firms that are subject to significant price risk. The board of directors may consider these attributes to hire a CEO for the purpose of initiating hedging. Third, we focus on the use of financial derivatives which is directly related to managing price risk. Prior empirical studies examine other measures such as R&D intensity and level of operating leverage [e.g., Serfling (2012)]. Fourth, we examine the relationship between specialized degree of the CEO and hedging. Specifically, in the context of oil price risk, we examine the influence of petroleum degree on the use of hedging in the oil and gas business. Finally, our study compares CEO age and education of hedge firms against a sample of nonhedge firms that never used derivative instruments during an extended period of time, from 1994 to 2010. In contrast, prior studies use data for a limited number of years. For example, Haushalter (2000) used data for 3 years—1992, 1992, and 1994, to examine hedging. It is quite possible that hedging activities of a firm may not be captured in such a short time period. Our findings indicate that the age of a hedge CEO is significantly less than that of a nonhedge CEO in the year hedging is initiated. The average age of the CEO who initiated the use of financial derivatives is 51.77 years and that of the CEO who did not hedge is 55.87 years. We also find evidence that a higher percentage of the hedge CEOs received degrees in petroleum engineering or related areas. Also, a smaller percentage of these CEOs received their college degree in business when compared to the nonhedge CEOs. The results of our logistic regressions indicate that CEO age explains the use of derivatives in the oil and gas industry. In addition to the main findings, our data show that a higher number of oil and gas firms introduced hedging during periods of declining and volatile oil prices. More than two-third of our sample hedge firms introduced hedging during the 2000–2008 period. Also, a higher percentage of oil and gas producers hedge while a higher percentage of oil and gas drilling and field exploration services firms did not hedge. The rest of the paper is organized as follows. The next section provides a review of literature, followed by descriptions of the data and sample and the empirical results. Concluding remarks are given in the last section.",4
39,1,Journal of Economics and Finance,03 January 2014,https://link.springer.com/article/10.1007/s12197-013-9278-7,On the demand for smoking quitlines,January 2015,Rajeev K. Goel,,,Male,Unknown,Unknown,Male,"Given the habit forming nature of smoking, smokers looking to give up smoking face difficulties in doing so. Thus, governments often try to facilitate quitting by various means, including providing information and subsidizing treatments. In this context, mainly due to their ability to aid smokers quit smoking across large geographic areas, smoking quitlines have proven to be effective tools in the fight for smoking cessation.Footnote 1 As a result, public quitline services are now available in all 50 U.S. states (see www.naquitline.org for details). However, formal research on quitlines, especially on economic aspects, has been lacking and the present research attempts to make a contribution by estimating the demand for quitlines. Besides purely academic interest, policymakers facing tough resource allocation decisions would also be interested in ascertaining factors driving the demand for quitlines. This is even more pertinent as quitlines face competition from new technologies with the spread of the internet. How do internet resources affect the demand for quitlines? The broader literature has in recent years considered the importance of studying the factors driving the propensity to quit smoking (Goel (2007), Hammar and Carlsson (2005), Hsieh (1998) and Laxminarayan and Deolalikar (2004)). However, there is relatively less attention to the study of quitlines, especially their economic aspects (see Anderson and Zhu (2007) and Cummins et al. (2007)). With the growing popularity and effectiveness of quitlines in helping smokers quit smoking (see Anderson and Zhu (2007), American Lung Association (2010)), it is useful that a formal study of the factors driving the demand for quitlines be conducted. Recognizing the increasing role of the internet in information dissemination, we focus on its effects. However, given its multifaceted nature, the role of the internet is not easily captured. On the one hand, there are numerous general internet-based cessation resources by various sources (health practitioners, government organizations, NGOs, general public etc.); while on the other hand, state-specific quitline internet resources by health authorities in individual U.S. states exist. These include: (i) information about the quitline in a state; (ii) information about tobacco cessation; (iii) self-directed web-based intervention; (iv) automated e-mail messages; (v) chat rooms; and (vi) interactive counseling and/or e-mail messaging with cessation counselor (www.naquitline.org). However, there is considerable variation in the prevalence of these web-based resources across individual states. In 2010, only two states, New York and Wyoming, had all six of these internet resources available, while 17 states (Alabama, Arizona, Delaware, Idaho, Kentucky, Massachusetts Minnesota, Montana, Nevada, New Mexico, North Dakota, Pennsylvania, Rhode Island, South Dakota, Utah, Vermont and Washington) had none of the six available. To get a handle on the role of the internet, we employ two measures of internet resources for smokers looking to quit smoking. These indicators incorporate a general and a specific measure. A general measure, Internet, captures internet hits (number of webpages following an internet search) about smoking cessation resources employing a widely used internet search engine, and a specific measure, NoWeb, identifies the 17 states without a specific internet program about quitlines in any of the six listed categories. These controls are employed along with other factors to study the determinants of quitline demand. Which factors significantly affect the demand for quitlines? In a nutshell, the results find non-internet influences on quitline demand to be relatively more powerful. The formal setup follows.",
39,2,Journal of Economics and Finance,28 June 2012,https://link.springer.com/article/10.1007/s12197-012-9240-0,Do hubris and the information environment explain the effect of acquirers’ size on their gains from acquisitions?,April 2015,Ivo Ph. Jansen,Lee W. Sanning,Nathan V. Stuart,Male,,Male,Mix,,
39,2,Journal of Economics and Finance,24 October 2012,https://link.springer.com/article/10.1007/s12197-012-9244-9,A Kalman filter control technique in mean-variance portfolio management,April 2015,James DiLellio,,,Male,Unknown,Unknown,Male,"Mean-variance (MV) portfolio optimization remains the most popular optimization technique practiced by quantitative fund managers. (Fabozzi et al. 2007) The solution to this optimization problem continues to provide institutions and investment professionals with recommended portfolio weights that lie along the efficient frontier of risky assets. Unfortunately, the out-of-sample behavior of mean, variance and covariance has long been known to change in unexpected ways, quickly leading to a portfolio that is sub-optimal. (Dickinson 1974; Frankfurter et al. 1971) Ultimately, this creates a dilemma for practicing investment professionals who wish to take advantage of mean-variance optimization, but recognize that rebalancing is another important aspect to managing portfolio risk. (O’brien 2006; Boscaljon et al. 2009) Many approaches have been considered to address single period out-of-sample performance issues. For example, Michalowski and Ogryczak (2001) measured downside deviations differently, thereby improving modeling of potential losses. Britten-Jones (1999) developed a procedure for testing the weights developed in mean-variance efficient portfolios. Board and Sutcliffe (1994) tested the Bayes-Stein method and other variations of mean-variance for portfolio selection. And, work by Disatnik and Benninga (2007) applied shrinkage methods to develop an improved estimate of the covariance matrix. For a comprehensive review of MV models in portfolio analysis, and their analytical treatment, readers are referred to Steinbach (2001). Readers are also suggested to review the latest in robust optimization, such as in Kawas and Thiele (2011). We revisited this fundamental problem of rebalancing an optimal portfolio by developing a recursive algorithm to continually incorporate dynamic market information. To this end, we develop an approach using a Kalman filter (KF), which has a long history in engineering to develop time series state estimates based on combining transition models with measurement models, as originally developed by Kalman (1960). The successful algorithm recently celebrated its 50 year anniversary (Mills et al. 2011), and consists of a transition equation used to update state estimates using a predictive model, and a measurement equation that updates the state with current observations. Both models are assumed to operate in the presence of disturbances, which are ideally calibrated based on a set of parameters related to the expected errors in the transition and measurement models. When the disturbances are known to be generated by a zero-mean Gaussian process, the KF blends the process and measurement model results in an optimal way while also providing a statistical estimate of the error in the time series states, as described in Grewal and Andrews (2008). Readers interested in other textbooks that apply Kalman filtering to financial applications are suggested to see Harvey (1989) and Wells (1996), where Kalman filtering method is used to support maximum likelihood estimation over a historical set of observable data. Within our application of the KF, we will generally follow the notation in Grewal and Andrews (2008). The transition model is developed based on a three factor model similar to the Fama-French model, which is known to explain the majority of the variation of a asset’s return based on market capitalization, book to market ratio, and the market index (Fama and French 1993; Davis et al. 2000). We define our measurement model based on a mean-variance optimization, under both the constrained and unconstrained cases. Our approach suggests that this work can be broadly classified as multiple criteria decision analysis, with an emphasis in developing appropriate portfolio weights, similar to Costa and Soares (2004). Also, by including the aspects of a Kalman filter that includes measurement model and parameter uncertainty, this work has some similarities to Garlappi et al. (2007). But, we also consider constrained mean-variance portfolio’s, so go beyond the classic unconstrained analytic solution for the optimal portfolio analyzed by Garlappi et al. (2007). For these reasons, and as far as this paper has uncovered, this work represents the first application of a Kalman filter to portfolio construction and management.",3
39,2,Journal of Economics and Finance,04 January 2013,https://link.springer.com/article/10.1007/s12197-012-9247-6,Two-step acquisitions and liquidity spread,April 2015,Mufaddal Baxamusa,Dobrina Georgieva,,Unknown,Female,Unknown,Female,"Harford (2005) suggests that macro-level liquidity is perhaps one of the most important determinants of mergers. The reason is that liquidity reduces financial constraints which in turn reduce transaction costs. An implication of this argument is that during periods of high liquidity there is more competition for the target as many more firms are able to raise financing for potential acquisitions. A consequence of it should be that the acquirer will choose the mechanism for acquiring the target that will lead to a profitable acquisition despite potential competition for the target. Tender-mergersFootnote 1 have characteristics such as shorter time to complete the acquisition and cash payment. These characteristics can potentially discourage rival bids. Thus, tender-mergers may increase with liquidity. Our research is different from Harford’s as Harford (2005) investigates merger waves while this research investigates the choice between the merger methods. In the first part of our analysis we empirically investigate if macro-level liquidity affects the choice between the acquisition methods. To test our hypotheses, we employ a technique developed by Bai and Perron (1998) to identify structural breaks. This technique is necessary as the number and location of structural breaks is unknown. Between 1980 and 2007, we find three structural breaks and these are – December 1989, July 1994 and November 2000. We follow the literature (Hansen 2001 and Enders and Sandler 2005) and use these structural breaks to test the relevance of the hypothesized cause. Strikingly, these structural breaks coincide well with liquidity shocks suggesting that the choice between the acquisition mechanisms is driven to some extent by liquidity. Neoclassical theory suggests that mergers can also occur as a response to other shocks, such as regulations, governance innovations, business cycle, etc. Thus, we investigate several factors such as anti-takeover laws, poison pills, recessions, and stock market gyrations. We find that in comparison to these competing causes, changes in liquidity has the strongest impact on the acquisition method. Mergers depend on negotiations and therefore are more likely to be affected by targets’ threat to find alternate bidders. Hence, during periods of higher liquidity the fraction of synergy obtained by the acquirer in a merger should be lower. We test this hypothesis by using a matched sample to study the share of the synergy obtained by the acquirers. Following Lang et al. (1991) and Wang and Xie (2009), we measure synergy as the combined dollar abnormal returns of the acquirer and the target around the acquisition announcement day. We find that the share of the synergy to the acquirer in tender-mergers (henceforth, all comparisons refer to those between targets of tender-mergers and the publicly traded merger targets) is lower when there is a decrease in tender-mergers. Additionally, liquidity increases the fraction of synergy obtained in a tender-merger. The implication here is that liquidity plays a crucial role in the choice of acquisition method. Our study has important implications. First, it shows a link between liquidity and the acquisition method chosen by the acquirer. Second, it documents a new finding which is the fall and rise in tender-mergers. In 1985 the number of tender-mergers was approximately 55 % of all acquisitions of public targets. Twenty years later this ratio is less than 7 %. Third, this study uses the Bai and Perron procedure to identify structural breaks and, to the best of our knowledge, this research is the first application of this procedure in corporate finance. The rest of the paper proceeds as follows. Section 2 provides an overview of the existing literature and develops the hypotheses. Section 3 describes the data and the methodology used. Section 4 discusses the empirical results. We conclude in Section 5.",1
39,2,Journal of Economics and Finance,16 January 2013,https://link.springer.com/article/10.1007/s12197-012-9249-4,"Informed trading, institutional trading, and spread",April 2015,Malay K. Dey,B. Radhakrishna,,Male,Unknown,Unknown,Male,"McInish and Wood (1992) classify the significant cross sectional determinants of bid ask spread in the equity market into activity, risk, information, and competition in the dealers market. Empirical research using spread decomposition techniques pioneered by Glosten and Harris (1988) and Stoll (1989) and enabled by intraday order and quotes data finds order processing cost to be the dominant component of bid ask spread followed by the adverse selection cost component, a measure of information risk premium included in the spread.Footnote 1 Bollen et al. (2004) skillfully connect the two strands of literature, determinants and components of spread. More recently, the finance literature has focused on how ownership structure, a presumed conduit for private information affects bid ask spread and its adverse selection component. Empirical findings indicate that insider, block, and institutional holding considered proxies of private information have mixed effects on adverse selection costs and spreads.Footnote 2 Further, based on the intuition that trades reveal information, (Easley et al. 1996, 1998) compute probability of informed trading (PIN), a measure of information content in trades from excess buy or sell orders and find it to be a significant determinant of spread after controlling for trading volume and financial analyst coverage as denoting order processing cost and information respectively. Chung and Li (2003) and Lei and Wu (2005) confirm PIN as a significant determinant of adverse selection cost and spread. In this paper, using TORQ data from the NYSE, a unique transaction database that identifies institutional trades, we extend and refine Easley et al. (1998) to empirically investigate how institutional trading may affect spreads and further the components of spreads after controlling for among others PIN and volume as a proxies for multiple motives including liquidity and information. This investigation is important because institutional trading in equity in the US has grown significantly during the last two decades; yet, its impact on aggregate market indicators is uncertain due to the lack of clarity as to whether and to what extent institutional trades are motivated by information and/or liquidity needs. Bozcuk and Lasfer (2005) and Kaniel et al. (2008) contend information motive for institutions, while Seppi (1990) and Dey and Kazemi (2008) argue that institutional trades reflect information and liquidity motives. Empirically, Sias et al. (2006) find that institutional trades reflect information, price pressure, and/or positive feedback effects in the market; Griffin et al. (2003) confirm positive feedback trading by institutions; and Dey and Radhakrishna (2007) document multiple trading motives for institutions. Therefore, institutional trades may increase the risk of information trading but may also provide liquidity, with the two counteracting effects having an uncertain impact on spreads. The introduction of PIN as a proxy for information content in trades further complicates the discovery of information content in institutional trading. Easley et al. (1998) find PIN denoting the information content in trades subsumes the information content in analyst coverage, while Dey et al. (2011) find institutional trading is a determinant of PIN only for large but not small trades. Hence, besides inconclusive/lack of evidence on the information content in institutional trades, it is also unclear how institutional trading may affect spread and its adverse selection component after controlling for PIN denoting information content in trades. We empirically investigate these issues as below. First, we compute PIN from the parameters of a maximum likelihood model of securities trading with competitive market makers as in Easley et al. (1996). Second, we estimate the parameters of a linear model for spread in which the independent variables are institutional trading, PIN, trading volume, and buy sell ratio (net order flow), which are accepted proxies for order processing cost, liquidity, information, and momentum. Hence we not only determine the information content in institutional trades, if any, but also disentangle the information contents in institutional trading and PIN. Finally, we decompose spread into adverse selection and order processing components and test a linear relation between these spread components and changes in trading volume, net (buy over sell) order flow, PIN, and institutional trading. We find that after controlling for PIN and volume among others, institutional trading explains a significant portion of the variation in effective spreads across stocks. The significantly positive coefficients for both PIN and institutional trading confirm information contents in both PIN and institutional trading. However simple correlation test finds zero (not significant) correlation between institutional trading and PIN, which raises question as to the nature of information that PIN and institutional trading reveal. We find spread increases as institutional trading increases across both high and low levels of institutional trading and that adverse selection costs increase while order processing costs decrease, albeit not statistically significant with increased institutional trading. Taken together, these two findings indicate that the net effect of institutional trading as a cross sectional determinant on spread is positive but may net to zero where the positive and negative effects of adverse selection and order processing costs respectively cancel each other. Based on the findings in Dey and Radhakrishna (2007), we conjecture that the increase in spread is due to the increase in volatility associated with increased institutional trading. The rest of this paper is organized as follows. Section II provides a brief and precise literature review followed by two testable hypotheses. Section III introduces the data and further details the computations of bid ask spread, institutional trades, and PIN. It also contains sample statistics and a brief discussion of those statistics. In section IV, we present a least squares regressions model for quoted spreads and discuss the results relating institutional trading and PIN to spreads. In section V, we describe spread decompositions and report results from our analysis of the PIN and institutional trading as determinants of adverse selection and order processing cost components of spread. Section VI concludes the paper.",8
39,2,Journal of Economics and Finance,16 March 2013,https://link.springer.com/article/10.1007/s12197-013-9254-2,Do shareholder rights influence managerial propensity to engage in earnings management?,April 2015,Kenneth Small,Seung Woog Kwag,Joanne Li,Male,,Female,Mix,,
39,2,Journal of Economics and Finance,02 May 2013,https://link.springer.com/article/10.1007/s12197-013-9258-y,"The differing efficiency experiences of banks leading up to the global financial crisis: A comparative empirical analysis from Australia, Canada and the UK",April 2015,Dong Xiang,Abul Shamsuddin,Andrew C. Worthington,,Unknown,Male,Mix,,
39,2,Journal of Economics and Finance,23 May 2013,https://link.springer.com/article/10.1007/s12197-013-9261-3,"Teachers’ salaries and human capital, and their effects on academic performance: an institution-level analysis of Los Angeles County high schools",April 2015,Richard J. Cebula,Franklin G. Mixon Jr.,Mark A. Montez,Male,Male,Male,Male,"Since the No Child Left Behind Act (hereafter NCLBA) became effective in 2002, schools, students, and parents have become almost hyper-vigilant about standardized tests scores, preparation for those standardized tests, and school performance ratings based on those scores. Concern about educational quality is understandable. As Ferreyra and Liang (2012) explain, information asymmetries are prevalent in education provision. A school’s efforts to educate children are often unobservable to parents and oversight officials, potentially giving schools or teachers an opportunity, if not an incentive, to provide less-than-desired efforts in educating students. Although this problem can plague both public and private schools, public schools often face the added problems of limited competition and diminishing taxpayer support. Monitoring of schools by those with vested interests, such the parents of school-aged children, can help to address the issue. However, monitoring efforts are subject to shirking, as parents attempt to free-ride on the expected monitoring behavior of other parents or parties (Ferreyra and Liang 2012). Despite these adverse economic incentives, examination in Dee and Jacob (2012) of panel data (state-level) on National Assessment of Educational Progress (NAEP) test scores shows that the NCLBA has led to significant improvement in eighth-grade math, especially among “traditionally low-achieving groups” and at the lower percentiles. Dee and Jacob (2012) also find an improvement in fourth-grade math, in terms of both average scores and higher scores at the bottom and top percentiles. Research on the correlates of education quality and outcomes such as that described above has touched on many areas, most of which are reflected in recent literature, which includes studies examining the link between education quality and spending on education (Holmlund et al. 2010), class and cohort size (Babcock et al. 2012), school district size (Phuong 2012), teacher pay (Leigh 2012; Muralidharan and Venkatesh 2011; Dolton and Marcenaro-Gutierrez 2011; Lin 2010; Lavy 2009), teacher aptitude or quality (Leigh 2012; Gilpin 2012; Hanushek 2011; Larsen 2010), and administrator pay (Lavy 2008). Of particular interest, largely because of the extensive attention devoted to them in the recent literature, are two of these relationships, namely, that between teacher pay and academic performance, and also that between teacher quality and academic performance. The current study explores these two relationships further using a large institution-level data set of academic performance scores from the Los Angeles County (CA) high schools (hereafter LACHS). These scores reflect schools’ performances on statewide testing of multiple content areas. Although prior studies have produced mixed results, our analysis indicates that education outcomes across LACHS are positively related to both teacher pay and teacher quality.",5
39,2,Journal of Economics and Finance,23 May 2013,https://link.springer.com/article/10.1007/s12197-013-9259-x,Multiple reverse stock splits (investors beware!),April 2015,Claire E. Crutchley,Steven Swidler,,Female,Male,Unknown,Mix,,
39,2,Journal of Economics and Finance,25 June 2013,https://link.springer.com/article/10.1007/s12197-013-9263-1,On the accuracy of private forecasts of inflation and growth in Brazil,April 2015,Hamid Baghestani,Cassia Marchon,,Male,Female,Unknown,Mix,,
39,2,Journal of Economics and Finance,10 August 2013,https://link.springer.com/article/10.1007/s12197-013-9268-9,Macroeconomic sources of foreign exchange risk premium: evidence from South Africa,April 2015,Bernard Jagre Walley,,,Male,Unknown,Unknown,Male,"As an emerging market economy, South Africa has one of the largest and technologically advanced economies in Africa. The country is also relatively stable and therefore an attractive investment destination for investors seeking to take advantage of opportunities in Africa. However, like many emerging market economies, South Africa’s interest rate spread relative to some of the major investment destinations such as the U.S., Japan, Australia and Europe has been consistently positive and time varying. If the uncovered interest parity condition holds, then, the interest rate spread may suggest that foreign investors anticipate a depreciation of the South African currency and therefore require additional compensation in order to hold assets denominated in South African currency. However, the general consensus among economists is that the uncovered interest parity condition has not been an empirical success. One explanation for this failure is that investors care about risk and there is a time-varying risk premium for any given exchange rate. In the context of emerging market economies, there are two types of risk premium: Currency risk premium which reflects both the size of the expected depreciation of the currency and the degree of volatility of the currency and default or country risk premium which reflects country specific risk factors that may affect the probability of default. A positive risk premium increases the interest rate paid on bonds denominated in that currency. Unfortunately, little is known about how or why the risk premium changes over time. The primary objective of this study is therefore to examine the macroeconomic sources of risk premium in South Africa using the stochastic discount factor approach. The stochastic discount factor model is a framework for pricing assets. The goal is to assess which specific macroeconomic factors drive the risk premium in South Africa and thus help provide information to investors seeking opportunities in South AfricaFootnote 1 The rest of the paper is organized as follows: Sections 2 review the relevant literature and the contribution of this study to the literature. Section 3 discusses the stochastic discount factor model. Section 4 examines the econometric model and data sources. Section 5 discusses the empirical results. Finally, the conclusion and policy recommendations are discussed in section 6.",
39,2,Journal of Economics and Finance,10 September 2013,https://link.springer.com/article/10.1007/s12197-013-9271-1,"Economic, welfare, demographic, and gender inequalities among selected Arab countries",April 2015,Edward Nissan,Farhang Niroomand,,Male,Unknown,Unknown,Male,"Progress for countries is most often measured by market indicators such as gross domestic product (GDP) and stock market indexes, creating the impression that if the GDP or GDP per capita and the stock market indexes are doing well, so are the countries. Policy makers and the popular media share, to a great extent, this view. A major deficiency of market indicators, according to Miringoff (2001), is that they are based on narrow views of the quality of life and, therefore, ought to be challenged. The use of quality of life indicators and indexes – such as unemployment rates, education levels, health, and the like – are alternative ways to gauge the progress of nations. Since 1990, the United Nations Development Program (UNDP) has provided a yearly publication, “Human Development Report,” to assess quality of life across countries. The World Bank, through its yearly publication “Work Development Report,” also provides socio-economic indicators for a host of countries. The rationale of these reports is that per capita income is not satisfactory to measure the factors connoted by the term “quality of life.” Mbaku (1997) made the point that many scholars criticize the emphasis of economic development at the country level measured by per capita income as inadequate to measure social progress. To this end, Gundlach (2001) attempts to assess the links between education and development. Yang (2000) ties female labor supply to fertility and economic growth. Graff (1999) tests several hypotheses on the significance of education as a determinant of economic development. Barlow (1998) describes the effect of population growth on the growth of output. Sinha (1998) studies the relationship between gross domestic product in Malaysia and government expenditures in their various forms. Hirschman (1989) contends that when the focus is on quality of life, the mind turns to ingredients of human satisfaction other than consumer goods and services. Frank (1989) illustrates how the narrow focus on consumption misses important facets of human well-being, even though consumption levels are obviously important, and Stark (1989) relates effects of altruism on the quality of life as yet another example of the scope of research dealing with issues of satisfaction other than income A question relating to oil-exporting countries of the Arab countries as part of OPEC is addressed by Korhonen and Juurikkala (2009). Their intention was to address the determinants of real exchange equilibrium of these countries. The data were taken between 1975 and 2005. Their findings indicate that there is a statistically significant effect. When oil prices rise, real exchange rates appreciate while GDP does not have a clear effect. Behbudi et al. (2010) assess human capital and economic growth of petroleum exporting countries. They indicate that human capital, education, and technology are important factors inducing economic growth. Yet, their findings point to negative effects between abundant natural resources and human capital growth as well as economic growth. In other words, countries that are rich in minerals and oil do not devote enough attention and expenditure to education, with the result of having slower economic growth than other countries with less abundant resources. Mohammadi and Jahan-Parvar (2012) tackle again the question of exchange rates in oil-exporting countries. Their aim is to examine the possibility of Dutch disease in such countries. Only three of thirteen countries (Bolivia, Mexico, Norway) displayed the Dutch disease. Their overall conclusion is that in the majority of cases there were no strong links between oil prices and real exchange rates. The implication here is that the evidence in support of the Dutch disease in oil-exporting countries is of a limited effect. While the previous referenced articles were concerned mostly with the effects on exchange rates in oil-producing countries, other researchers devoted their interests to other areas. El-Wassal (2012) investigated the relationship between Foreign Direct Investment (FDI) and economic growth among 16 Arab countries for the time period between 1970 and 2008. His findings point to a limited, or even negligible, FDI impact. Furthermore, growth benefits from FDI for such factors as financial development, trade openness, human capital, and infrastructure are not improving significantly. What is needed are efforts to improve or reform institutional quality. Also needed are reforms in macroeconomic policies as well as domestic financial markets. A similar theme is devoted by Pelaez (2009), who studied economic freedom among 103 countries grouped into five categories (free, mostly free, Islamic, Latin American, a subset of EU countries). Statistical significance for testing differences between group means for some 10 measures of economic freedom showed that Islamic countries have les economic freedom compared to the other four groups during the 13-year period of the study. Contessi et al. (2013) contend that Middle Eastern and North African (MENA) countries when compared to other countries stand out as hampering female employment and entrepreneurship. There is a low rate of female labor participation, entrepreneurship, and ownership. There is also a strong link between a specialization in a country and measures of female labor participation. Finally, Jean Louis et al. (2012) studied the feasibility of a monetary union among the Gulf Cooperation Council (GCC) countries and whether common economic shocks are prevalent with the United States or France, Germany, and Italy to determine the currency employed (dollar, Euro). Their findings concur with the suggestion that the shocks are symmetrical with those of the United States and, therefore, the US dollar would be the appropriate currency. US monetary policy makes it possible to make demand shocks in GCC countries operate more smoothly than a monetary policy based on the Euro currency.",3
39,2,Journal of Economics and Finance,26 October 2013,https://link.springer.com/article/10.1007/s12197-013-9275-x,The effects of agglomeration on interregional hospital patient flow,April 2015,Johnathan G. Munn,Caroliniana S. Padgett,,Male,Unknown,Unknown,Male,"Many communities have come to realize the importance of industrial targeting as an economic development strategy. However, most have been slow to recognize the importance of the healthcare sector to local economies. According to the Centers for Medicare & Medicaid Services, healthcare spending in the U.S. reached $2.7 trillion in 2011, representing nearly 18 percent of the nation’s GDP. They project that this spending will rise to $4.7 trillion by the year 2021 (CMS 2011). This growing segment of the economy could represent fertile ground in the economic development community. In an effort to increase economic growth, many communities have set up committees and task-forces in an effort to better market their area to export-oriented industries. If, however, economic development is defined simply as the expansion of the economic base through the efficient allocation and use of available resources, the concept of economic development is opened up to industries that are not traditionally viewed as export oriented. While manufacturing and tourist-oriented sectors have long been associated with this attribute (Keil and Mack 1986), the local hospital sector has often been overlooked as an exporter. Although many studies (American Hospital Association 2009; Doeksen and Biard-Holmes 1998; Erickson et al. 1986) have examined the healthcare industry or hospital sector as a generator of business activity, they have not examined its potential to be an exporter of services. This study extends and contributes to the current literature by viewing hospitals as engines of economic growth due to their potential as an exporting industry, an avenue of research not heavily investigated. The implications of this potential could be significant as spending on hospital care is projected to total $1.5 trillion by 2021 (CMS 2011). It is a long understood tenet of economics that economic development is dependent on incremental money flowing in from outside the region (Woodward 2002). From this perspective, some regions are better positioned to take advantage of their hospital sector than others. We empirically examine the relationship between agglomeration and an area’s ability to export hospital services to patients from outside of the region to illustrate the economic importance of hospitals as exporters. To examine the hospital industry as an exporting sector, a novel panel data set is developed to track patient-hospital interaction. A county-pair fixed effect spatial interaction model is developed to test the hypothesis that external economies of scale are important to an area’s ability to export hospital services. The use of patient flow data has the advantage of separating demand and supply effects because each effect has a distinct geographical reference (Oliveira 2004). Therefore, spatial interaction models like the one explored in this study can be exploited to determine if exports are a function of local supply factors or external demand factors. This will help policymakers make more informed decisions when determining how to properly encourage investment in local healthcare services. The study is organized as follows. Section 2 discusses the growing importance of hospitals to local economies. The data and model specification are explained in Section 3. Section 4 contains results. Section 5 concludes.",2
39,3,Journal of Economics and Finance,22 March 2013,https://link.springer.com/article/10.1007/s12197-013-9256-0,Reducing agency conflicts with target debt ratios,July 2015,Unyong Pyo,Yong Jae Shin,Howard E. Thompson,Unknown,,Male,Mix,,
39,3,Journal of Economics and Finance,24 March 2013,https://link.springer.com/article/10.1007/s12197-013-9257-z,Mutual fund fees and performance: new insights,July 2015,Sharon Garyn-Tal,,,Female,Unknown,Unknown,Female,"A massive literature has been written about predicting funds performance. Expense ratio seems to take a substantial part in explaining performance, although findings are contradictory. Several academic studies have documented a negative relation between a fund’s operating expense ratio and performance. Gruber (1996) shows that high fees are associated with inferior rather than superior management and concludes that the investor is better off buying funds with low expense ratios. Carhart (1997) finds that higher-fee funds do not perform as well as lower-fee funds. Sirri and Tufano (1998) find that lower fee funds and funds that reduce their fees grow faster, since mutual fund consumers are fee-sensitive. On the other hand, Grinblatt and Titman (1994) find no such correlation at all. This is consistent with Berk (2005) that claims investors cannot expect to receive excess returns (after fees) on a risk-adjusted basis, since a manager captures all the economic rent she generates using her skills. Moreover, Zheng (1999) finds that high performance appears to be most salient for funds that exert higher marketing effort, as measured by higher fees; funds that receive more money subsequently perform significantly better that those that lose money. This effect is short-lived and is largely but not completely explained by a strategy of betting on winners. The literature also connects loads and performance. Barber et al. (2005) document a negative relation between fund flows and total fund expenses, and this relation is driven by a significant negative relation between front end loads and flows. They argue that the purchase decisions of mutual fund investors are influenced by salient, attention grabbing information, like front end loads, than operating expenses. Chordia (1996) focuses on costs associated with liquidity-motivated trading and develops a theoretical study of load fees. Chordia argues that a load fee can induce a separating equilibrium in which flow causing investors cluster at no load funds, and long term investors willingly invest in a load fund, to avoid the costs that flow imposes. Chordia’s results suggest that redemption fees can be more successful than front-end load fees at curbing redemptions. Consistent with that, Dellva and Olson (1998) find that funds with redemption fees, on average, earn higher risk adjusted returns, and funds with front-end load charges earn lower risk adjusted returns. International evidence suggests that there is no consistent statistically significant relation between expenses mutual funds charge and the performance they achieve. Ferreira et al. (2012) examine open–end actively managed equity funds in 27 countries over 1997–2007 and find a negative relation between the expense ratio and net-of-fees performance. This relation, however, is statistically insignificant for US funds and only significant in some specifications for non-US funds. Thus, they conclude that there is no consistent evidence of a statistically significant negative relation between fees and performance. They also conclude that higher priced management, as measured by the expense ratio, can generate higher gross returns, but returns are not high enough to cover the fees. In addition, they do not find a statistically significant relation between performance and loads in their sample of US and non-US funds. Afza and Rauf (2009) focus on Pakistani mutual funds. They run regressions over the sample of all the open-ended mutual funds listed on Mutual Fund Association of Pakistan (MUFAP) for the years 1999–2006, and find that Pakistani Mutual fund risk-adjusted return is uncorrelated with expenses. Korkeamaki and Smythe (2004) examine the cross-sectional return characteristics of Finnish mutual funds. Following U.S. studies that show expenses differ among fund types (e.g. Malhotra and McLeod 1997), Korkeamaki and Smythe use indicator variables for fund objectives to allow for differences in returns across fund types. The objectives included are short-term bond funds, intermediate-term bond funds, long-term bond funds, balanced funds, domestic equity funds, derivative funds, international bond funds, international balanced funds, and international equity funds. They find no significant relation between Finnish mutual funds performance and their expense ratios in their samples. However, they identify bank-managed funds and older funds that charge higher expenses but do not offset them by superior returns. Other studies of funds characteristics and returns also take into consideration the funds’ investment objectives. Prather et al. (2004) examine US Mutual funds in 1996 to 2000. Their sample of funds spans seven different investment objectives, including aggressive growth, growth, growth and income, equity and income, small company, foreign, and global. After taking into consideration general market conditions and fund investment objective, they find that expense ratios and loads are negatively related to fund performance. This paper analyzes mutual funds performance from a different perspective and it is distinguished from other papers since funds classifications are taken into consideration: the relations between the expense ratios and loads the funds charge and the alphas they earn are examined within each of the classification group. As previous research suggests, my analysis confirms that there are statistically significant relations between funds expense ratio and alpha and between rear loads and alpha for over 1,395 U.S non-specialized mutual funds in 2001–2010. But, when taking into consideration funds (Lipper) classification, I find that within each of the 12 classification group there are no such consistent relations between the expenses and loads the funds charge and the alphas they earn. Moreover, I mostly find no relations at all within each of the Lipper classifications between the expenses and loads and the alphas. Thus, choosing a fund among the universe of open-end non-specialized mutual funds, an investor should take into consideration the funds expenses and loads. However, the 2001–2010 sample of open-end non specialized equity funds implies that once an investor settles on her preferred or favored investment style (as determined in this paper by Lipper classification), the investor should re-consider the expenses criteria by re-examining the relations between expense ratios and loads charges, and the performance achieved by the fund managers in that specific style group.",2
39,3,Journal of Economics and Finance,24 March 2013,https://link.springer.com/article/10.1007/s12197-013-9252-4,The response of state employment to oil price volatility,July 2015,Wei Kang,David Penn,Joachim Zietz,,Male,Male,Mix,,
39,3,Journal of Economics and Finance,14 June 2013,https://link.springer.com/article/10.1007/s12197-013-9262-2,Lumpy investments and capital adjustment patterns in the food manufacturing industry,July 2015,Pinar Celikkol Geylani,,,Unknown,Unknown,Unknown,Unknown,,
39,3,Journal of Economics and Finance,16 July 2013,https://link.springer.com/article/10.1007/s12197-013-9266-y,Are private banks the better banks? An insight into the principal–agent structure and risk-taking behavior of German banks,July 2015,Frank Schmielewski,Thomas Wein,,Male,Male,Unknown,Male,"Our paper examines whether multistage principal–agent relationships can explain the risk-taking behavior of German banks in the course of the financial crisis from 2007 to 2008. Within this context, it seems to be reasonable to distinguish between banks with dispersed shareholders and banks with a high ownership concentration.Footnote 1 The classical principal–agent theory assumes that managers pursue different objectives and show different risk-taking attitudes than firms’ owners. Amihud and Levy (1981) and Hirshleifer and Thakor (1992), for example, argue that managers generally avoid risk-taking due to career concerns. Accordingly, managers show more averse risk-taking behavior than firms’ owners, because they are not able to diversify their unemployment risk. By contrast, dispersed shareholders have more incentive to behave in a risk-neutral manner (Jensen and Meckling 1976; Demsetz and Lehn 1985; Esty 1998), because they are capable of diversifying their risk by engaging in a large number of projects. Furthermore, dispersed shareholders have less incentive to control managers because they have to share the benefits of controlling activities with other shareholders irrespective of their capability to control. Thus, large shareholders are able to overcome this incentive problem, and have a higher chance of preventing low risk-taking by managers (Morck et al. 2005; Stultz 2005). Our paper contributes to this discussion by modeling the theoretical background of principal–agent problems in the banking industry, taking into account the probability of problematic occurrence depending on the type of bank. Moreover, our study offers empirical evidence of the distinguishable risk-taking behavior of German banks that relates to the ownership structures and monitoring capabilities of different banking sectors (Appendix). By taking into account the ambiguous effects of shareholder structure on the risk-taking behavior of banks, as considered by Stultz (2005), Beck et al. (2009), and Barry et al. (2011), a number of recent studies have clearly distinguished between shareholder concentration and shareholder rights to explain the influences of shareholder structure on the risk-taking attitudes of bank managers. Gropp and Köhler (2010) report that shareholders prefer more risk compared with managers regardless of whether using shareholder rights or ownership concentration as a measurement of owner control. Moreover, they argue that bank managers generally prefer less exposure to risk compared with owners, whether dispersed or not. Thus, their hypothesis is contradictory to policy reports that assume that the extremely generous performance-based compensation plans obtained by poorly controlled bank managers are leading to extreme risk-taking by bank managers (Kirkpatrick 2008). Laeven and Levine (2009) also show the relationship between banks’ risks and capital regulation depends critically on the ownership structure of a bank and therefore has important policy implications.Footnote 2 In general, these authors offer empirical evidence that risk-taking by banks positively correlates with the comparative power of shareholders. This hypothesis coincides with the results provided by Saunders et al. (1990) that suggest that owner-controlled banks enter into higher risks than banks controlled by managers with small shareholdings. Furthermore, Beck et al. (2009) provides evidence that larger privately held banks move closer to insolvency than their smaller peers, but face lower distress probability. Moreover, they show that within the German banking industry, privately owned banks are less stable than savings banks or cooperatives, by showing savings banks report greater distance-to-default (DD) than cooperatives. This view is consistent with that of Fonteyne (2007), who highlights that cooperative banks in Europe are engaging in less risky activities than commercial banks (see also Cihák and Hesse 2007). Barry et al. (2011) found contradictory results by comparing five categories of shareholders, namely: managers/directors, institutional investors, non-financial companies, individuals/families, and banks. The study purports ownership structure clearly explains differences in risk exposures of privately owned banks. Accordingly, high equity stakes held by individuals/families or banking institutions correlated with a decrease in asset risk and default risk. These findings confirm the results published by Iannotta et al. (2007) who showed higher ownership concentration is associated with better loan quality, lower asset risk, and lower insolvency risk.Footnote 3
 De Nicolò and Loukoianova (2007) indicate that owing to country- and firm-specific characteristics, the risk exposure of foreign banks seems to be significantly higher than that reported for private domestic banks. Nevertheless, they were not able to maintain their hypothesis in the case of state-owned banks and private domestic banks. In addition, the authors verify that private domestic banks enter into more risk than state-owned or foreign banks due to the larger market share of the latter. By contrast, Barry et al. (2011) found no significant relationship between ownership structure and risk-taking attitudes in the category of public banks. Moreover, they did not report significant difference between publicly held and privately owned banks. At the same time, Berger et al. (2005) demonstrated state-owned banks report poorer loan quality and higher default risk than privately owned banks. Another stream of research differentiates between risk categories by showing that mutual banks enter into lower asset risk and lower default risk than government-owned banks (Fraser and Zardkoohi 1996; Hansmann 1996; Esty 1997; Iannotta et al. 2007). Finally, Kwan (2004) illustrated the exposure to risk of publicly held and privately owned banks are statistically indistinguishable when considering US bank holding companies. This is consistent with the findings of Altunbas et al. (2001) who show low significance in the German banking system that privately owned banks operate their businesses more efficiently than mutual and publicly held banks. Furthermore, Beltratti and Stultz (2009) studied the influence of bank-level governance, country-level governance, country-level regulation, and banks’ balance sheets and profitability characteristics on bank performance in the course of the financial crisis from 2007 to 2008. In accordance with the ownership structure of banks, they found there is no consistent evidence that better governance leads to better performance during crisis; however, they did find strong evidence that banks with more shareholder-friendly boards performed worse. A considerable amount of research indicates a significant relationship between ownership concentration and risk-taking. Nevertheless, there is apparently no consensus whether this relationship is positive or negative (Iannotta et al. 2007; Barry et al. 2011). These ambiguous results in recent literature may be due to the fact that besides ownership concentration, a number of further conditions also determine the risk-taking attitudes of bank managers and bank owners. These are, for instance: the role of banking regulation (Macey and O’Hara 2003; Levine 2004; Laeven and Levine 2009), deposit insurance (Prowse 1997; Beck and Laeven 2008), and the globalization of the banking industry (Pathan 2009). Furthermore, the concentration of the banking market (Boyd and De Nicolò 2005; De Nicolò and Loukoianova 2007), stock ownership programs, annual compensation schemes for bank managers (Erkens et al. 2009; Bebchuk and Spamann 2010), and the strength of bank boards (Sullivan and Spong 2007) seem to relate to banks’ risk-taking attitudes. Moreover, general macroeconomic circumstances seem to influence the risk-taking behavior of banks.Footnote 4
Footnote 5 Existing theoretical and empirical literature does not answer the questions of which bank sector favors risk-taking behavior, which roles are bank managers playing in this field, or what influence do bank owners have. Modeling and testing different ownership structures, monitoring capabilities, and capital market pressure should help answer these open questions. In this study, we hypothesize that whether a bank is willing to enter into substantial risks, or not, depends on the relevance of the principal–agent problems. In particular, banking sectors seem to be vulnerable to risks that do not allow bank owners to monitor bank managers. Furthermore, the risk attitudes of banks depend on the capability of bank owners and bank managers to distribute risks across several firms. Finally, we argue that bank owners’ incentives for gaining high returns strongly influence banks’ risk-taking behavior. Bank managers that strongly depend on high profit-motivated principals (capital market-oriented or profit-seeking public owners) to a large degree tend to operate their businesses by adopting strategies that emphasize above average returns without looking at risks. Therefore, we use several aspects of the principal–agent model to predict bank managers’ risk-taking behavior. Following our basic empirical hypothesis about risk-taking bank behavior, it is necessary to go in detail about principal–agent structures. Most western countries have different sectors in the banking industry. In the US there are commercial banks, investment banks, credit unions, savings and loan associations, and mutual banks. In Germany, we can divide banks into savings banks, cooperatives, stock companies, and Federal state authorities. German bank sectors can be classified by the relative importance of their principal–agent problems. We expect that such a classification can be carried out for other countries. Moreover, we apply an empirical study to the DD of German banking sectors to shed light on banks’ risk-taking attitudes prior to and during the financial crisis of 2008.Footnote 6
 In Section 2, we outline the theoretical background to explain whether bank managers can deviate from the expected return-to-risk relation that bank owners prefer. Section 3 summarizes the data with a focus on explaining the different ownership structures of German banks. Section 4 compiles the results and assesses them within the scope of our proposed hypothesis. The paper closes in section 5 with a summary and a conclusion of our findings.",3
39,3,Journal of Economics and Finance,17 July 2013,https://link.springer.com/article/10.1007/s12197-013-9265-z,The linkage between the U.S. “fear index” and ADR premiums under non-frictionless stock markets,July 2015,Omar A. Esqueda,Yongli Luo,Dave O. Jackson,Male,Unknown,Male,Male,"American depository receipts (ADRs) have become one of the most important financial instruments for companies from emerging economies to overcome capital barriers by accessing U.S. stock markets. Additionally, ADRs have been commonly used as convenient instruments for international diversification as they are available in the U.S. and denominated in U.S. dollars. Consequently, the breadth and availability of ADRs has been constantly increasing as more firms continue cross-listing their shares in the U.S.Footnote 1
 According to the law of one price (LOP), the ADR price must equal the price of the underlying security after adjusting for the ADR ratio and exchange rate. However, there seems to be a systematic mispricing, or ADR premium, in the ADR market. We attempt to identify the sources of the ADR mispricing. One of the most important limitations to calculate the ADR premium is the lagged effects. For instance, there is a significant timing difference between Asian and North American stock exchanges. However, Kim et al. (2000) find that the effect of stock markets and exchange rate fluctuations on ADR prices is mostly completed during the same calendar day. Unlike their Asian and European counterparts, Latin American countries share a high and homogeneous linkage to the U.S. financial markets. In addition, simultaneous trading hours also have the beneficial effect of reducing the likelihood of lagged effects. Our findings on ADRs from Latin American countries can be extended to other emerging market ADRs. The outstanding growth of emerging economies has been reflected in the high number of ADR programs initiated in recent years. Figure 1 depicts the number of exchange-traded ADR programs by region and indicates that Latin American countries comprise a substantial portion of the ADR market. Number of cross-listed firms by region. ADR levels 2 and 3 as of December 2010. Source: Citigroup depositary receipt services Using 69 ADR programs from Argentina, Brazil, Chile, and Mexico over the period from January 1995 to May 2009, we test whether investor sentiment has a significant impact on ADR premiums. We use the Volatility Index (VIX) as a proxy for investor sentiment. High levels in the VIX indicate that investors are likely to have pessimistic expectations about the U.S. stock market.Footnote 2 We define the ADR premium as the disparity between the ADR price and the price of the underlying share, after adjusting for the corresponding exchange rate and ADR ratio.Footnote 3 In addition to the aforementioned factors, we control for the domestic and the U.S. stock market indices which are widely documented as relevant determinants of ADR prices in the literature (Kim et al. 2000; Fang and Loo 2002; Bin et al. 2004; Bae et al. 2008; Alhaj-Yaseen 2011; Esqueda and Jackson 2012). We find a negative and significant relationship between investor sentiment and ADR premiums. The deviations from the LOP in ADRs can be partially explained by investors’ fears about the future of the stock market, as evidenced by changes in the volatility index. Additionally, current U.S. stock market behavior is also a relevant pricing factor. We conclude that investors can improve their hedging strategy by incorporating the lagged values of the volatility index in determining ADR prices. This paper contributes to the existent literature in the following ways. First, we relate ADR premiums to the volatility index, which measures investors’ “fear” about future stock market movements. To test the statistical inference between ADR premiums and investor sentiment, we use a generalized autoregressive conditional heteroskedasticity in mean model (GARCH-M) and acknowledge the potential conditional variance of the ADR premium in our sample period due to country events such as currency depreciations. Second, we control for the relevant factors in ADR pricing by comparing the response of ADR premiums to changes in the volatility index. The results have important implications for constructing country portfolios by observing investors’ preferences.Footnote 4
 In the next section, we review the existing literature. Section 3 describes the data and sampling procedure. In section 4, we explain the econometric techniques employed and address our research questions. Section 5 presents a detailed description of the empirical results and Section 6 concludes.",13
39,3,Journal of Economics and Finance,18 August 2013,https://link.springer.com/article/10.1007/s12197-013-9267-x,Demand for broad money in Singapore: does wealth matter?,July 2015,Ahmad Zubaidi Baharumshah,Siew-Voon Soon,,Male,Unknown,Unknown,Male,"Singapore, a small and open economy, currently ranks number four, after Hong Kong, as a global financial center. Although the economy recovered from the 1985 recession, fear of renewed inflation has prompted the Monetary Authority of Singapore (MAS) to allow the Singapore dollar to appreciate and to fluctuate within a zero-appreciation exchange rate band. The exchange rate–centered monetary policy adopted by the MAS since the mid-1980s is based on the assumption that money adjusts passively to real economic activities in an open economy. Accordingly, the money supply has little, if any, independent influence on domestic economic activity. Apart from the importance of the exchange rate in an economy that is highly dependent on international trade, the adoption of exchange rate targeting by the MAS also reflects concern that the demand for money was becoming more unstable as deregulation in the financial markets accelerated in the 1980s (Dekle and Pradhan 1997). Many scholars hold that the benefits of exchange rate targeting are strongest in countries that lack credibility of monetary policy and have a history of high inflation.Footnote 1 However, this is not the case for Singapore, which over the past several decades has successfully maintained macroeconomic stability and recorded high growth rates. However, Dekle and Pradhan (1997) argued that the benefits of the monetary framework imposed in Singapore must be weighed against other considerations, including the greater difficulties of managing capital inflows and other real shocks.Footnote 2 It has often been claimed by scholars that because of the open nature of the economy and the adoption of exchange rate targeting, the stock of money will adapt to changes in demand. Choy (2011), however, demonstrated that monetary aggregates are leading rather than lagging in the business cycles, implying that the reverse causation from output to money could explain what the Singapore economy has experienced in recent years. The notion that rising asset prices have boosted consumer spending has received much support in industrialized countries. Focusing on Singapore, the housing price index increased significantly from 1978 to 1996 before falling at the end of 1998. Thus, given the sharp movements in the equity and housing markets in the past few decades, it is conceivable that movements in these asset prices could affect the demand for money. To the best of our knowledge, the impact of asset price inflation on the money demand function (MDF) in Singapore has not been systematically examined empirically, which has motivated us to conduct this study. The main purpose of this study is to determine the extent to which changes in the asset market have affected money demand behavior. We also examine how the recent bursting of the new-economy bubble affected monetary aggregates as individuals shift their portfolio from risky to less risky (safer) assets. Specifically, this study is designed to answer two questions: (1) Does wealth matter in the demand for money?; and (2) Have asset price bubbles and the recent global financial crisis affected the stability of Singapore’s MDF? We attempt to show whether a stable long-run relationship exists between monetary aggregates and related variables including income, interest rate differential, inflation and asset prices. In the empirical analysis, special emphasis is placed on addressing the issue of stability and wealth effects. The stability on the MDF is an important issue, in particular for drawing a conclusion that monetary policy has a predictable influence on the real economy (Poole 1970). If the demand for money turns out to be unpredictable, then the transmission mechanism of monetary policy becomes more complicated and the central bank will face difficulty in controlling the money supply and, hence, the level of inflation (Bahmani-Oskooee and Rehman 2005 and several of the articles cited therein). In this research, we extend our sampling period to 2012 in order to investigate the impact of the global financial crisis on the underlying relationship between the demand for money and its core determinants. We depart from existing literature by considering a broader set of wealth indicators (residential property prices and share prices) to critically analyze the relationship between money and its determinants. Which indicators should be included in the equation is a subject of ongoing debate in the recent literature and should be subject to empirical analysis. If the wealth variable turns out to be important in the equation, its exclusion may lead to parameter instability, and policy developed from the model may undermine the effectiveness of monetary policy. For this purpose, we perform a stability analysis on the short- and long-run relationships between money and its major determinants. The remainder of the article is organized as follows: Section 2 is a literature review. Section 3 reviews the specification of the long-run MDF, with emphasis on the wealth variables; Section 4 briefly describes the data and presents our empirical results; and Section 5 concludes the paper.",3
39,3,Journal of Economics and Finance,08 April 2014,https://link.springer.com/article/10.1007/s12197-014-9282-6,Bankruptcy predictions for U.S. air carrier operations: a study of financial data,July 2015,Chiuling Lu,Ann Shawing Yang,Jui-Feng Huang,Unknown,Female,Unknown,Female,"Recent liberalization of the airline industry caused not only decreasing profits but also unstable operations leading to bankruptcy (Button 2003; Evans and Koch 2007). In the case of the airline industry, over investment intended to accommodate deregulation through low-cost capital has been observed as one of the major causes of operational failure leading to unsatisfactory profit levels and signaling likely bankruptcy (Wojahn 2012). Airlines with high levels of loan financing or debt issuing for aircrafts may also exhibit high operational gearing. Consequently, airline operations are easily affected by natural disasters, terrorist attacks, health scares, and government regulations, causing high operational expenses and declining competitiveness and performance (Morrell 2007; Guzhva 2008). Whereas firm failure is more likely to occur for large firms with internal or external funding, firm profit is critical in preventing firm failure (Assadian and Ford 1997). An early warning model for bankruptcy, therefore, is crucial for operational purposes. Thus, airline performances differ between full-service network carriers operating in hub-and-spoke systemFootnote 1 for international flights and low-cost carriers operating in route-for-route systemFootnote 2 for domestic flights (Windle and Dresner 1999a, b; Hofer et al. 2008). Airline deregulation allowing new carrier entries since 1978 and reoccurring international catastrophes including September 11 terrorist attack and sub-prime mortgage crisis caused tremendous cost pressure to full-service network carriers in flight crew remunerations, catering services, aircraft rental and depreciation, and increasing fuel costs (Goetz and Vowles 2009; Nolan et al. 2004; Tretheway 2004). Consequently, full-service network carriersFootnote 3 (FSNC) filed for bankruptcy protection via Chapter 11 and low-cost carriersFootnote 4 (LCC) entered the airline market with a new business model (Morrell 2005; Dennis 2007; Cheng et al. 2009). Contrary to round-trip booking for business or economic classes with several flight crews and catering services from FSNC, LCC offer only one-way trip booking via one single flight crew (Morrell 2005; Dennis 2007). Such operational differences lead to more competitive turnaround time for aircrafts from 1 h for FSNC as compared with 25 min for LCC (Hazledine 2011; Barrett 2004). Under great elasticity of demand for air travel, LCC are capable of selling more lower-priced tickets to generate more cash to pay-off suppliers’ debt than their counterparts in FSNC with higher-priced tickets and less volume causing inability to support the “empty core” activitiesFootnote 5 (Button 2003; Goetz and Vowles 2009; Nolan et al. 2004). In response, FSNC formed alliances or established LCC subsidiaries to maintain competitiveness in airline market (Hazledine 2011; Tiernan et al. 2008). However, air carriers continue to overinvest in aircrafts to provide more travel capacity and overlook the potential liquidation risk from asset mismanagement consisting in aircraft maintenance and depreciation, air terminal utilization, flight crew productivity, and service values (Daft and Albers 2013; Wojahn 2012). Therefore, the financial analysis of air carrier performance is vital to competitive success and critical to bankruptcy prevention. We propose a prediction model for the U.S. aviation industry for both healthy and bankrupt airlines by utilizing a binary quantile regression, a Bayesian quantile regression, and logit models to identify optimal bankruptcy prediction accuracy. Although the binary quantile regression and the logit regression are capable of identifying bankruptcy probabilities, we found the Bayesian binary quantile regression to exhibit optimal bankruptcy prediction accuracy. Total assets significantly and positively influence bankruptcy probability for air carriers. Operational variables, such as quick asset to expenditure for operation, increase in sales, and working capital to assets, however, significantly and negatively influence air carriers’ bankruptcy probability. Cumulative accuracy profile (CAP) ratio and the Brier score for the Bayesian binary quantile regression also provided better prediction among all three regression methods.",6
39,3,Journal of Economics and Finance,11 May 2014,https://link.springer.com/article/10.1007/s12197-014-9284-4,On the spillovers between patents and innovation in Japan,July 2015,Jin Guo,,,Female,Unknown,Unknown,Female,"The interactive relationship between patents and innovation is one of the most important areas of study in economics. Innovation, or technical progress, is recognized as an essential element of economic growth. It is well known that in the neoclassical growth theory (Solow 1956; Swan 1956), the rate of technological progress is independent of economic system. However, in the new endogenous growth theory (Lucas 1988; Rebelo 1991; Romer 1986, 1990; Barro and Sala-i-Martin 2003), technological progress is determined as an endogenous variable since it takes place through innovations, in the form of new products, processes and markets many of which are the result of economic activities. The endogenous growth theory also argues that investment in human capital, innovation, and knowledge are significant contributors to economic growth. Romer’s model (Romer 1990) showed that the market power generated by intellectual property protection mechanisms, such as patents, make a substantial contribution to economic growth. In recent years, innovation has played an increasingly significant role in Japanese economic growth. In 2013, Japan’s labor force participation rate for ages 15–64 dropped to 62.47 % (78,957,764), a more than 0.85 % decline in less than a year and the greatest single-year fall in the history. The shrinking labor force will push down the potential economic growth rate of Japan. Since quantitative economic expansion is currently unlikely in view of Japan’s rapidly ageing population and declining birth rate, qualitative improvement through innovation has been recognized as a strategy for stimulating the Japanese economy. Since the collapse of the bubble in 1990, Japan’s economy has been mired in two decades of uninterrupted stagnation. Against this background, In June 2003, the Japanese government designed the Strategic Framework for Intellectual Property to strengthen Japan’s intellectual property system and enhance Japan’s industrial competitiveness. The main point of the Strategic Framework is introduced pro-patent policies, which include improving the efficiency of patent examination procedures and protecting Intellectual Property in biotechnology and information technology. Given the significance of innovation and patenting in Japan, this study engages in an empirical analysis to examine the relationship between the two in the Japanese context. Patent application numbers are considered an index for mapping the approximate scale of knowledge production. Japanese patent law is based on the first-to-file principle, which necessitates patent filing soon after knowledge production, for ensuring the right to the grant of a patent for a given invention. Given the requirements of this system, the number of patent applications can be considered a suitable index of knowledge production in the case of Japan. Total factor productivity (TFP), a productivity index expressing the relationship between inputs and output of all production factors, as opposed to productivity of individual production factors, such as labor productivity or capital productivity. Thus, an increase in TFP is generally associated with productivity growth, implying that TFP as a variable can be understood as an index for measuring innovation. Moreover, TFP is also a variable that accounts for elements involved in economic growth, which cannot be determined by measureable production factors such as capital and labor. The connection between TFP and economic growth can be expressed by the dynamic Cobb–Douglas production function as following:Footnote 1
 where t represents time, Y

t
 represents production output (GDP), K

t
 represents capital stock, L

t
 represents labor input, and A

t
 represents technological progress. \( {\dot{Y}}_t \), \( {\dot{A}}_t \), \( {\dot{K}}_t \), and \( {\dot{L}}_t \) represent the respective values differentiated with respect to time t. This equation indicates TFP growth, which entails growth in technology, constitutes that part of economic growth rate (increase in GDP). Under the Japanese Patent Act, inventors are granted exclusive monopoly rights in exchange for public disclosure of their invention. The patent application disclosure system put in place in Japan since 1971 requires in principle the disclosure of invention details within 18 months of the application date. This is believed to prevent duplication of technological developments, thus increasing the efficiency of research and development (R&D) activities and promoting R&D collaboration between companies. Once an invention is disclosed through a patent application, positive external effects can be expected, subsequently increasing productivity growth as measured by TFP.",3
39,3,Journal of Economics and Finance,27 May 2014,https://link.springer.com/article/10.1007/s12197-014-9287-1,The determinants of effective corporate lobbying,July 2015,Richard Borghesi,Kiyoung Chang,,Male,Unknown,Unknown,Male,"Recently financial researchers have intensively begun to evaluate why firms lobby and to attempt to quantify the effect of lobbying on firm performance. Lobbying is the expenditure of funds by a firm to attempt to influence legislation to the manager’s, firm’s, and/or industry’s advantage. Lobbying expenses are generally used to pay the salary of those who have or can obtain access to politicians who have regulatory influence over an industry or business practice. As such, lobbyists are frequently former politicians or political insiders who are familiar with legislative procedures (e.g., former congressmen) or have pre-existing relationships with politicians that currently hold office. Lobbying is an incredibly important part of the means by which corporations influence policy. There is no limit on the amount of money that a particular firm can spend on lobbying, whereas donations made directly to politicians are limited to $5,000 per candidate per election cycle. The result is that total lobbying expenditures are nine times greater than campaign contributions (Kerr et al. 2011). In addition, as pointed out by Ansolabehere et al. (2003), most political contributions ‘from corporations’ are actually made by individuals employed by those corporations. Therefore, these donations may represent personal consumption (in that they advance personal political ideology) rather than a form of corporate political involvement. Apparently, firms have recognized the importance of lobbying, as the proportion of firms that lobby doubled from 1999 to 2006 as did lobbying expenditures (Vidal Jordi et al. 2011). An important determinant of whether a firm chooses to lobby is the cost associated with entry into lobbying (Bombardini 2008; Kerr et al. 2011). In the latter study, only about 10 % of firms lobby, but lobbying is highly persistent as 92 % of firms who lobbied in the previous year chose to lobby in the next. The up-front costs include learning the laws regarding lobbing and educating newly hired lobbyists about the firm’s interests. One hypothesis is that once these costs are incurred, it often makes sense for the firm to continue lobbying. Alternatively, it is possible that not all firms need to lobby, but those that found it necessary to lobby in the past need to continue their lobbying efforts. The critical issue that remains unresolved and that we address below is whether lobbying actually benefits shareholders, or instead manifests managerial agency problems. Hersch et al. (2008) fail to find a significant relationship between lobbying and Tobin’s Q and claim that lobbying does not create a long-term intangible asset for the associated firms. The majority of extant studies find that lobbying increases firm value and creates excess returns (e.g., Hill et al. 2013; Mathur et al. 2012), however methods employed in these studies may be problematic (we address concerns in detail below). Interestingly, the former finds that there are decreasing returns to increased lobbying intensity while the latter finds additional value is created with more intense lobbying. This proposed positive relationship between political activity and firm value does not hold exclusively for lobbying efforts. For example, Faccio (2006) finds that executives or large shareholders joining politics can produce the same value increases as do lobbying efforts, and a disruption in political connectedness can have the opposite effect (Faccio and Parsley 2009). Andres (1985) and Masters and Keim (1985) examine the relationship between PAC donations and the industries in which donating firms operate and find that industry regulatory factors are potentially important determinants of PAC contributions. The bigger picture seems to be that political connectedness increases firm value. There are indeed some theoretically legitimate value-increasing regulatory issues that firms can influence via lobbying. For example, a firm might lobby to influence tax policy (Richter et al. 2009), or determine visa policy (Kerr et al. 2011). If the firm’s efforts prove to be successful, these actions might increase firm value over the long-term. In the same vein, firms operating in heavily regulated industries (e.g., airline carriers [Banker et al. (1997)], tobacco, health care, alcohol, and pharmaceutical firms [Bowman, 2000], etc.) may have more to gain from intense lobbying. In addition, large firms may benefit more from lobbying (Agrawal and Knoeber, 2001) both because governments are more likely to be customers of large corporations and because of the impact of regulations on large businesses. Importantly, Mathur et al. (2012) find that firms with higher R&D intensity lobby more to gain information on policy initiatives in R&D intensive industries. The hypothesis is that this helps firms plan for the future more wisely, and is likely to lead to higher shareholder value. Oddly, despite the body of evidence that lobbying increases firm value, only between 10 and 20 % of publicly traded firms lobby. This implies that there are certain conditions under which firms should not lobby, or else that lobbying does not maximize shareholder wealth. For example, there are large collective action problems associated with lobbying, and one reason a firm may choose not to lobby is a potential free rider problem. If more than one firm in an industry would benefit from passage of a revised tax code, for instance, then only those firms that lobby bear the cost burden, but benefits will be shared by all. Under this scenario, non-lobbying firms extract rents from lobbyers. To improve a firm’s competitive position, one strategy for incumbent firms is to invest either in research and development (R&D) activities or other non-market strategies (e.g., lobbying) to prevent market entries of potential rivals (Grossmann and Steger 2008). If political capital exists, the byproduct of lobbying expenditures can be thought of as something very similar to the by byproduct of R&D spending. Alternatively, firms may benefit from long-term investing in both R&D and lobbying, as R&D projects can be protected from competition through lobbying activities. Therefore, whether lobbying and R&D are complements or substitutes is an empirical issue, and in this paper we find that they function as economic complements. Specifically, long-term excess stock returns show that lobbying contributes to shareholder value when companies invest more heavily in R&D. However, for non-R&D firms, we observe that lobbying destroys long-term shareholder value. There may be agency problems that cause managers to lobby for reasons that are counter to the best interests of shareholders. Under agency theory, managers make the decision to lobby out of self-benefit. Such benefits may arise in a number of ways. For example, managers may lobby to manipulate stock price in the short-term for personal gain. Yu and Yu (2011) find that lobbying efforts can delay or prevent fraud detection. This delay may provide managers with extra time to sell their shares before the fraud is discovered. Managers may also use firm resources for lobbying activities that promote their own personal political ideologies (Mathur et al. 2012 rather than for corporate gains. There are peripherally-related non-political examples of managers using firm resources for personal gain. For example, also under the agency rubric, Brown, Helland and Smith (2006) find that corporate spending on philanthropic pursuits destroys firm value, and thus represents an agency problem facing shareholders. A less extreme but similarly selfish motivation is for managers to engage in lobbying is to seek to stabilize their firm’s or industry’s economic climate to ensure his/her own survival within the industry, and/or secure their own employment at the firm. That is, managers may have a natural tendency to preserve their jobs because so much of their personal wealth is tied up in salary and stock options, and even though lobbying may destroy a small amount of firm value, in the end the managers are better off for having reduced their personal exposure to unemployment. This risk reduction hypothesis is supported by Grinstein and Hribar (2004). In this spirit, our goal is threefold. First, we want to understand whether lobbying makes sense from a shareholder perspective. That is, does lobbying truly maximize firm value? Second, if political capital exists, under what conditions can lobbying contribute to shareholder wealth maximization? Third, if lobbying does not add to firm value for the majority of firms, then we would like to explain why firms continue to lobby. If managers lobby even though firm value is not as a result maximized, then this evidence would support the presence of an agency problem. In contrast to the majority of extant literature examining the relationship between lobbying and firm performance, we find that once we account for firm size and endogeneity issues, there is on average a negative relationship between lobbying and long-term excess returns. Further, we provide evidence that lobbying increases share price volatility. In general, our findings strongly support the agency view that lobbying comes at the expense of shareholder wealth maximization on average. But at the same time, consistent with previous studies, we find positive long-term excess returns for lobbying firms having high R&D intensity, especially when free cash flows (a proxy for agency problems) are low. The paper proceeds as follows. In Section II we describe our data, in Section III we present results, and we conclude in section IV.",11
39,3,Journal of Economics and Finance,31 May 2014,https://link.springer.com/article/10.1007/s12197-014-9286-2,Asymmetry and revenue in second-price auctions: a majorization approach,July 2015,Jihui Chen,Maochao Xu,,Unknown,Unknown,Unknown,Unknown,,
39,4,Journal of Economics and Finance,24 August 2013,https://link.springer.com/article/10.1007/s12197-013-9269-8,When does CalPERS’ activism add value?,October 2015,Thomas I. Smythe,Chris R. McNeil,Philip C. English II,Male,,Male,Mix,,
39,4,Journal of Economics and Finance,28 August 2013,https://link.springer.com/article/10.1007/s12197-013-9270-2,Liquidity and price discovery in Latin America: evidence from American depositary receipts,October 2015,Alma D. Hales,,,Female,Unknown,Unknown,Female,"Holdings of foreign equities by U.S. investors have increased dramatically over the last two decades—from $567 billion in 1994 to $4,647 billion in 2012. A convenient way for U.S. investors to acquire foreign stocks is through the purchase of American Depositary Receipts (ADRs)—negotiable instruments issued in the U.S., in dollars, but that represent ownership of foreign equities. The nature of ADRs, namely that they represent claim to the same set of cash flows as their underlying securities trading in their respective home markets raises the question: which market matters most—home or host? Existing literature has examined this question from various perspectives: (a) the sensitivity of ADR returns to home and U.S. market returns (Aquino and Poshakwale (2006); Choi and Kim (2000); Fang and Loo (2002); Jiang (1998)); (b) the distribution of trading volume between the home and host markets (Baruch et al. (2007); Halling et al. (2008)); and most important to this study (c) which of the two markets drives price discovery (Chen et al. (2002); Chen et al. (2010); Eun and Sabherwal (2003); Frijns et al. (2010); Korczak and Phylaktis (2010); Pascual et al. (2006); Su and Chong (2007)). This study examines price discovery for a set of ADRs that originate from Latin America, specifically from Argentina, Brazil, Chile and Mexico. Existing evidence for stocks from developed markets cross-listed in the U.S. overwhelmingly shows that the source of price discovery is the stock’s home market (Eun and Sabherwal (2003); Grammig et al. (2005); Korczak and Phylaktis (2010); Pascual et al. (2006)). Furthermore, there is evidence in emerging markets, particularly those in Asia, that also indicates that the ADR’s home market generates most pricing information (Chen et al. (2002); Chen et al. (2010); Su and Chong (2007)). One notable exception is India where the domestic and foreign markets contribute equally to price discovery (Kadapakkam et al. (2003)). However, markets have two important functions—liquidity and price discovery (O’Hara (2003)). Not only are emerging markets typically characterized by low liquidity (Lesmond (2005); Bekaert et al. (2007)) but those in Latin America are underdeveloped (in both size and liquidity) even relative to other emerging market counterparts (De la Torre and Schmukler (2007)). Furthermore, Silva and Chavez (2008) document that trading costs are high in Latin America even for large firms with cross-listed stocks. Given these markets’ limited ability for providing liquidity, they are also likely to be limited in their price discovery function. Because the price discovery function may shift to the U.S. market, price discovery in these Latin American markets warrants further study. This study asks the following questions for ADR issues from Latin America: (1) where does price discovery occur? and (2) is lower liquidity in the home market associated with lower home market contributions to price discovery? The study contributes to the existing literature in several, distinct ways. First, the present analysis examines price discovery allowing for an endogenous role of exchange rate fluctuations. Using data from 3 German stocks, Grammig et al. (2005) suggest that studies of price discovery should explicitly account for exchange rate fluctuations; when prices are converted to a common currency, the converted price reflects any contributions corresponding to currency changes. This is a particularly important consideration for this Latin American ADR sample, because as shown by Esqueda and Jackson (2012), currency depreciations in Latin America during the period 1994–2009 are associated with statistically and economically significant losses to ADR investors. Second, this study significantly extends the sample of Latin American firms studied—87 exchange-listed ADR programs are included—a significant increase for price discovery studies in Latin America.Footnote 1 The increased sample size allows for an examination of differences in price discovery across countries and across firms within countries in Latin America. Third, this study extends the understanding of price discovery in emerging markets by exploring the relationship between liquidity and price discovery. Specifically, the study employs a cross-sectional approach to investigate whether illiquidity at home is associated with the U.S. market’s contribution to price discovery. The analysis, based on Johansen’s (1991) tests, yields evidence of cointegration between exchange rates, home share prices and ADR prices across most Latin American ADR issues during 2003–2010 period. Furthermore, the results from Vector Error Correction Models (VECM) corroborate the importance of accounting for currency fluctuations. While fluctuations in the exchange rate are not primarily driven by stock market innovations, the exchange rate is not exogenous to the stock market. There is significant information flow from the stock market to the currency market particularly for Brazil and Mexico. Moreover, there is a mixture of price discovery locations across Latin American countries. The evidence supports that price discovery occurs primarily in the home market only for Chile. For Brazil, contributions to price discovery are made by both the home and host markets. Finally, in Argentina and Mexico, price discovery occurs primarily in the U.S. market. Therefore, the results here indicate that the U.S. market matters most for price discovery of cross-listed stocks from Argentina and Mexico. Finally, the results from a cross-sectional analysis that explores the differences in U.S. contributions to price discovery across firms indicate that higher illiquidity in the domestic market is associated with higher U.S. contributions to price discovery even after controlling for factors such as U.S. share of trading and firm size. The remainder of the paper is organized as follows. Section 2 reviews the relevant literature. Section 3 includes the sample, data sources and descriptive statistics. Section 4 details the methodology while Section 5 presents the empirical results. Section 6 concludes.",1
39,4,Journal of Economics and Finance,11 September 2013,https://link.springer.com/article/10.1007/s12197-013-9272-0,Exchange rate movements and policy coordination in Latin America,October 2015,Hem C. Basnet,Subhash C. Sharma,,Unknown,Male,Unknown,Male,"For decades the exchange rate dynamics have been at the core of the monetary policy discussion, particularly in emerging markets. In an open economy, a central bank often tends to respond to exchange rate movements as it works as a shock absorber. A variety of economic shocks such as investors’ expectation, changes in commodity prices, foreign business cycles etc. influence exchange rate movements, which, in turn, will have a destabilizing effect on an economy. After the collapse of the Bretton Woods system, policy discussions have put a greater emphasis on exchange rate stability and correct exchange rate alignment to improve economic performance in less developed countries.Footnote 1 One of the reasons for establishing the European Monetary Union (EMU) was to promote exchange rate stability among member countries and to encourage trade inside the European Union (EU) (Dell’Ariccia 1999). The exchange rate movements even play a decisive role in making decisions for a monetary union or Optimum Currency Area (OCA) (see e.g. Wilson and Choy 2007). This study explores common movements among the exchange rates of Latin American countries both in the long and short term. More importantly, we analyze the common features (common trend and common cycles) of the exchange rates through monetary policy coordination perspectives. We consider sample of seven countries i.e. Argentina, Brazil, Chile, Colombia, Mexico, Peru, and Venezuela. The countries chosen are the leading economies in the region in terms of size, population and influence. Together, these seven countries account for more than 90 % of the continent’s Gross Domestic Product (GDP) and about 93 % of its population in 2010. Moreover, their role in trade, commerce and policies is undeniably significant and growing in the region. Studying exchange rate dynamics in a Latin American context is important since exchange rates have been used extensively as an economic policy tool by many Latin American countries (Rodriguez and Romero 2007), and the exchange rate policy has played a vital role in shaping macroeconomic outcomes in Latin America (Frenkel and Rapetti 2010). During the last decades Latin American countries were more open and financially more integrated (Ruiz 2009). More importantly Latin America is striving for a greater economic and monetary integration in the region (Allegret and Sand-Zantman 2009). In fact, the decade of the 1990s was characterized by a series of regional trade agreements in Latin America. More than 14 agreements—free trade areas and customs union—since 1990 have been made. The formation of the common market of the south or MERCOSUR also took place during that period, suggesting a momentum gain towards greater integration. It is believed that increased trade and financial integration calls for policy coordination among authorities to deal with external shocks. Policy coordination in this study is limited to coordinating national monetary policy in a way that benefits member countries with regard to economic stability. Some studies have investigated the exchange rate series to assess the feasibility of a monetary union (Cortinhas 2009). We, however, feel that an issue of that important (i.e. monetary union) should be evaluated thoroughly utilizing other macroeconomic fundamentals including political agreements. To that extent, this study does not provide an exhaustive evaluation for that possibility. Our main proposition is that if the exchange rates of these countries exhibit comovements both in the long and short run then the central banks may have limited scope for an independent monetary policy as a response to common external shocks. In this case, policy coordination might be good ideas to minimize risks caused by exchange rate uncertainty. Equivalently, if the group of these countries demonstrates asynchronous movement in their exchange rate behavior then relinquishing exchange rate independence in the name of coordination would be costly. This study aims to open up a discussion as to where the Latin American countries stand with respect to the currency movement. The Latin American countries have gone through several episodes of hyperinflation and a series of currency and financial crises time and again. More interestingly, the crisis of one country affects the whole region in a similar fashion. For instance, the profound crisis of the 1980s originated in Mexico and spread all over the Latin American region. Similarly, the 1990s economic crisis first emerged in Brazil then spilled over quickly to the region. In light of greater economic and monetary integration in Latin America it is important to understand the trend and cyclical behavior and intra-regional relationship of the currencies of leading economies in the region. However, movements in the exchange rate are not well investigated in a Latin American context. Prior research has primarily focused on whether exchange rates in Latin America are cointegrated in the long run. For example, in a recent article Giannellis and Koukouritakis (2013) investigate the long run relationship between the nominal exchange rates of selected Latin American countries (Brazil, Mexico, Uruguay and Venezuela) and document a meaningful long run relationship for each of the four exchange rates. Likewise, Ruiz (2009) examines whether exchange rates in the Latin American region are cointegrated in the long run and the bivariate cointegration test reports that most Latin American currencies are cointegrated with the Brazilian real. Rodriguez and Romero (2007) examined the permanent and transitory fluctuations in four Latin American real exchange rates (Argentine, Brazil, Chile and Mexico) and found that both components are equally important in explaining the dynamics of the real exchange rate. Some other studies have been devoted to the examination of purchasing power parity (PPP). For instance, Yang-Cheng et al. (2011) test the validity of long run PPP on a sample of 15 Latin American countries and find only six of the 15 countries hold the PPP true. The study by Chang et al. (2010) includes 10 Latin American countries and finds evidence supporting long run PPP for only five of the 10 countries. Su et al. (2011) have the similar findings with respect to the PPP in Latin American context. In contrary, the study by Divino et al. (2009) supports the PPP for all Latin American countries. The idea of testing the PPP is to determine whether currencies between two countries return to a constant equilibrium value in the long run. Another strand of literature analyzes the impact of exchange rate diversity or pass through to assess the feasibility of monetary union in Latin America. Allegret and Sand-Zantman (2009) assess the coordination of economic policies between the Mercosur countries to infer the possibility of monetary union. Their finding indicates that the Mercosur countries are not ready to form a monetary union as the diversity of exchange rate regimes inside the area appeared to be an obstacle to macroeconomic policy coordination. Ghosh (2013) studies exchange rate pass through in Latin America over the last four decades and finds that the region is equally susceptible to changes in their nominal exchange rate vis–à–vis the US dollar. This result suggests that the Latin American countries may have several commonalities in absorbing and responding to external shocks. All of these studies, however, left an important aspect of exchange rate movement (i.e. shared trends and shared cycles and its economic implication) out of the investigation. Kenen (1969) proposed that the diversification of macroeconomic indicators should be examined to assess the feasibility of policy coordination between nations. Frankel and Rose (1998) note that a group of countries should have, among others, the similarity of the shocks and cycles in their economies to initiate policy coordination. To the best of our knowledge no study has been done in an attempt to assess the shared trends and shared cycles in a Latin American context, and especially from policy coordination point of view. The present study intends to contribute to the literature by examining the common feature analysis in the exchange rate of these seven countries. The common feature analysis has been extensively used in literature especially to assess the feasibility of higher level of policy coordination such as economic or monetary union. In a recent study Basnet and Sharma (2013) study the feasibility of economic integration in Latin America by examining the long term trend and short term cycles among key macro-variables such as real GDP, Private investment and intra-regional trade. Their trend-cycle decomposition results indicate that the two important preconditions—long and short run synchronous fluctuations in economic indicators—for economic integration are met by seven major Latin American countries i.e. Argentina, Brazil, Chile, Colombia, Mexico, Peru and Venezuela. Likewise, Adom et al. (2010) investigate the short and long term relationship among key macro-variables in eight largest African countries. They found that macro variables in those countries have common trend in the long term and do share common cycles in the short term, and conclude that those countries may accelerate their efforts toward greater economic integration. Another study by Vatsa and Sharma (2010) analyze exchange rate movements in the ASEAN-5 countries in an effort to evaluate the possibility of monetary union among the ASEAN-5 countries. Their test results indicate that the possibility of monetary union among the ASEAN-5 is not quite plausible as not all five countries are uniformly ready for it. A number of other researchers (e.g. Sharma and Wongbangpo 2002; Sato and Zhang 2006; Abu-Qarn and Abu-Bader 2008) have used the common trend and common cycle approach to assess the feasibility of economic or monetary union. The main premise of these studies is that if macroeconomic variables of a group of countries display comovement both in the long and short run then it would be less costly for them to formulate economic or monetary union. The effect of exchange rates on macroeconomic stability is linked to interest rates, and other important macroeconomic variables such as output and inflation (Taylor 1993, 2001). A good understanding of the nature of comovement of exchange rate is paramount as it is an important transmission channel of monetary policy. An examination of the long and short term dynamic of exchange rates may provide some grounds for policy coordination in the region and shed some light on the extent of autonomy of monetary policies. Kang et al. (2002) support the idea of monetary policy cooperation between Japan and South Korea as they find the presence of co-movement between the Yen and the Won. Additionally, our study may shed light on the presence or absence of opportunities for investors to benefit from the diversification of their foreign investment portfolios. For instance, the existence of common trends between two or more exchange rates indicates that the long-run benefits arising from diversifying a foreign investment portfolio among the corresponding currencies will be vanished. Similarly, investors seeking short-term investment gains can benefit from monitoring short-term responses of exchange rates to the markets. For instance, ceteris paribus, the depreciation (appreciation) of a currency causes the value of the assets denominated in that currency to decline (rise) as well. Therefore, co-movement among various currency exchange rates may cause co-movement among the values of the corresponding assets. Transmission of permanent shocks experienced by one or more exchange rates to the other exchange rates is more likely among exchange rates sharing common trends. In the past, most of the Latin American countries suffered sudden stop and speculative attacks on their currencies from international investors mainly because of, among others, inconsistencies in macroeconomic policies and the adopted exchange rate regime (Caporale et al. 2011). Thus, this study may help the Latin American government i) to make a common defensive mechanism to prevent speculative movement in the exchange rates in future, ii) to enhance further dialogue in the region for a common exchange rate system, and iii) to bring stability in the currency market and enhance credibility of policy in the region. The rest of the paper is organized as follows: the next section explains data and econometric methodology used. In Section 3, results are discussed and finally, a conclusion is drawn.",4
39,4,Journal of Economics and Finance,26 October 2013,https://link.springer.com/article/10.1007/s12197-013-9273-z,Marketable and non-hedgeable risk in a duopoly framework with hedging,October 2015,Matthias Pelster,,,Male,Unknown,Unknown,Male,"Every business venture contains economic risk and the management of economic risk is more important than ever to ensure the further economic development and the well being of the firm. Therefore, the theoretical analysis of economic risk is an important part of research. In this paper, economic risk describes the risk that future cash flows of an investment are not sufficient to cover the initial payment(s). Economic risk can be divided into two main categories: marketable risk and non-hedgeable risk.Footnote 1 Marketable risk can be traded with other market participants on liquid markets and includes price risk, exchange rate risk and many more, whereas non-hedgeable risk is not easily transferable and includes background risk such as uncertainty about future income, uncertainty about the future value of fixed assets or uncertainty about future tax liabilities and technological risk that arises from uncertainty about production.Footnote 2
 Recent studies show that more than 91 % of international firms use derivatives to manage their risk exposure. The studies show that firms can be characterized to be risk-averse and their primary risk management goals are to reduce profit and cash-flow volatility and ensure an even cash-flow.Footnote 3 These studies show the importance of financial risk management in practice. Giventhe importance of the topic, a great body of literature deals with hedging risk. Froot et al. (1993) show that hedging adds value to corporations as it ensures that sufficient internal funds are available to take advantage of attractive investment opportunities. Bolton et al. (2011) show that hedging with derivatives is a complementary risk management tool to liquidity management. Campello et al. (2011) show that corporations that use hedging have to pay lower interest spreads and at the same time have less capital expenditure restrictions in their loan agreements. Mackay and Moeller (2007) show for a sample of 34 oil refiners, that between 2 % and 3 % of firm value stems from hedging concave revenues. Buraschi et al. (2010) deal with multivariate intertemporal portfolio choice. They show that optimal portfolios include distinct hedging components to deal with stochastic volatility and correlation risk. Additionally, numerous studies have examined the competitive firm under uncertainty, under marketable and non-hedgeable risk, with or without hedging opportunities, with direct or indirect hedging opportunities, etc., ever since Sandmo (1971).Footnote 4
 However, a large part of today’s economy consists of markets with imperfect competition. The market structure of oligopoly or duopoly is more important and more common than generally recognized, see for example Horstmann and Markusen (1992). Well known examples of duopoly are for example the airline industry. While Airbus and Boing basically split the market for jumbo jets into two pieces, there are as well several flight routes that only two airlines are allowed to serve (see Brander and Zhang (1990)). Other examples include the phone market or the credit card market. Additionally the electricity market or the gas market are dominated by a few firms. Recent studies such as Broll et al. (2011) or Pelster (2013) show that results that are valid for the competitive firm do not necessarily apply to an oligopoly. In contrast, due to the interplay of an oligopoly some unexpected results emerge. Previous studies on the firm in imperfect markets with hedging opportunities restrict their analysis to marketable risk without any existing non-hedgeable risk. Thus, there is a lack of treatment of imperfect markets such as oligopolies. We contribute to fill this void as we consider a Cournot duopoly under demand uncertainty in combination with background risk respectively technological uncertainty. First, we consider the firm under demand uncertainty and additional additive background risk. Furthermore, we investigate the firm under multiplicative technological risk in combination with demand uncertainty. Thus, this paper is the first to consider the duopoly case with non-hedgeable risk. A familiar model of an oligopoly is the one of Cournot (1838). Fishelson (1989) analyzes how the players react to uncertainty in the oligopoly setup. The author shows that uncertainty leads to a less competitive market but that a single firm may benefit from uncertainty. Thus, in an oligopoly a firm may benefit from uncertainty while a competitive firm always suffers. A more general model of duopoly with several types of risk is introduced by Asplund (2002). Studies that are concerned with the oligopoly case under uncertainty and introduce hedging opportunities can be distinguished in two categories, depending on the order of the hedging and the production decision. Within the simultaneous setting the firm decides on the production and the hedging quantity at the same time, while in the sequential setting the players first make the hedging decision and subsequently decide on the production with the previous hedging decisions of all firms being common knowledge. Thus, firms are able to commit to their position on the forward market before interacting on the output market. Eldor and Zilcha (1990) are the first to consider a hedging opportunity in a duopoly framework. They analyze the simultaneous setting under price risk. The authors cannot find any strategic effect of the hedging opportunity. However, they exemplify that both firms of the duopoly may suffer from the hedging opportunity. Broll et al. (2011) consider the simultaneous setup within a duopoly under exchange rate risk and confirm the results from Eldor and Zilcha. Pelster (2013) is the first to study the repeated interaction in a Cournot duopoly under uncertain demand with a simultaneous hedging opportunity. The author introduces storage and considers the case with iterated competition on the output market. Pelster shows that in this setup the players regard the influence of their decisions on future interactions but use the hedging device only for risk management purposes and not as a strategic device. A different but also simultaneous approach is taken by Liu and Parlour (2009). They consider the influence of a hedging opportunity within an auction and show that the market will be more competitive with hedging and firms may bid above the value of the project. Von Ungern-Sternberg and von Weizsäcker (1990) are the first to consider the sequential setting. However, the authors do not consider a connection between the hedging and the production decision. The sequential setting with a connection between these decisions is examined by Allaz (1992) and Allaz and Vila (1993). They consider the duopoly case and show the strategic impact of a hedging opportunity under certainty. The players reach a strategic dilemma and the equilibrium converges to the competitive outcome. Hughes and Kao (1997) further examine the model proposed by Allaz (1992). Ulukut (2008) considers the sequential setup under price risk, while Broll et al. (2009, 2011) analyze the sequential hedging case under exchange rate risk. The authors show that hedging is used as a strategic device and not only for risk management purposes. Adam et al. (2007) consider a different sequential model. In their setup firms hedge uncertain input costs within a Nash equilibrium before they compete in a Cournot setting. Adam et al. show that firms may have an incentive not to hedge and thus asymmetric equilibriums may emerge. All of these studies treat the oligopoly case under marketable risk and ignore the existence of non-hedgeable risk. In reality however, firms do not only have to deal with marketable risk but are facing non-hedgeable risk at the same time. Thus firms have to be aware of the external effects non-hedgeable risk have on their decisions. This paper makes a significant impact to this area as we are the first to consider the case of non-hedgeable risk in the duopoly case. We aim to answer the following questions: 1. How does the existence of additive background risk affect the production and hedging decision? 2. How does the existence of multiplicative technological risk influence the firm’s decisions? 3. Do any strategic effects evolve as the additional risk is introduced? To answer these questions this paper extends the existing literature in many ways. We examine the optimal strategies in a Cournot duopoly under price risk with additional additive background risk. We also study the effects of additional multiplicative technological risk on the optimal strategies. Kreps and Scheinkman (1983) show that in an oligopoly the unique equilibrium outcome is the Cournot outcome as mild assumptions about demand are met. We restrict ourselves to a duopoly case to simplify the math. However, this does not limit the main results of the study. We show that the additional additive background risk does not have any influence on the output decision within the simultaneous setup. The separation theorem remains valid and only the forward position is adjusted. Thus, the additive background risk does not effect the market equilibrium. However, within the sequential setting the background risk does influence the production as well as the hedging decision. The strategic dilemma can either be intensified or weakened depending on the stochastic dependence of the price risk and the background risk. As far as the multiplicative technological risk is concerned the production decision is even affected as a simultaneous forward market exists. The separation theorem is not valid and to reach a hedging decision the firm has to consider the complete risk situation and cannot focus on the marketable risk. The full hedge theorem is not valid in any scenario but only in some special cases. The paper is organized as follows. Section 2 introduces the model with demand uncertainty and additional background risk. The simultaneous as well as the sequential setting are considered. Section 3 studies the model with demand uncertainty and technological risk. The last section concludes.",
39,4,Journal of Economics and Finance,30 October 2013,https://link.springer.com/article/10.1007/s12197-013-9276-9,On inter-arrival times of bond market extreme events. An application to seven European markets,October 2015,Vasileios G. Siakoulis,Ioannis A. Venetis,,Male,Male,Unknown,Male,"The ongoing European sovereign debt crisis has renewed the discussion over the role of contagion and linkages among financial markets. During the course of the crisis a number of depressing events have emerged such as pressures in stock and currency markets, widening of sovereign debt and credit default swap (CDS) spreads along with a series of sovereign credit rating downgrades. These events have occurred under a financial integrated environment that was the result of a series of policy measures following the 1957 treaty of Rome. This fact differentiates the European sovereign debt crisis from past financial crises such as the Latin America crisis (1994) and the Asian crisis (1997–1998) which were erupted in an environment of different (at least) exchange rate regimes and financial infrastructures. We adopt the frequently used definition of contagion in the literature as a significant temporary shift in cross-market linkages after a major distress event (see Longstaff 2010). Metiu (2011) found that large-scale fluctuations in sovereign bond yields signal a structural shift in cross-market linkages with respect to tranquil periods. In a survival analysis framework this would be expressed through a clustering behavior of inter-arrival times between consecutive extreme fluctuations where small and large inter-arrival times would appear in distinctive clusters. Focardi and Fabozzi (2005) expressed contagious defaults behaving in this way as point processes whose intensity is conditional upon previous intervals between defaults through a self-exciting ACD point process. In this framework, high frequency of extreme events in a number of markets generates financial turbulence and abnormal market linkages which shortly after lead to additional extreme events, whereas prolonged periods without abnormal movements signify the absence of contagious dependence which increases the relative periods between extreme events appearance. As such, we construct a duration series that measures the inter-arrival times of distress events in seven European sovereign bond markets. We build upon the methodology developed by Bae et al. (2003) by identifying a “crisis” occurrence or distress event as the day where at least one of seven developed European countries, namely Austria, Belgium, Holland, Switzerland, Germany, United Kingdom, France, experiences a positive Bond yield percentage change that exceeds the 95 % percentile of its distribution in the sample period. The generated distress events are irregularly spaced in time and the inter-arrival times of distress events exhibits clustering behavior and autoregressive dynamics similar to that of GARCH models (Engle 1982; Bollerslev 1986). Since we are dealing with a duration series, we employ the Autoregressive Conditional Duration (ACD) model of Engle and Russell (1998) to model the inter-arrival times of Bond market distress events. To the best of our knowledge, such an approach has not been attempted in the past in the domain offinancial contagion studies while it is one of few applications of autoregressive conditional duration models outside the high frequency transactions data literature (e.g. Christoffersen and Pelletier 2004; Fischer and Zurlinden 2004; Focardi and Fabozzi 2005; Hamidieh et al. 2013). An extended review of conditional duration models can be found in Hautsch (2012) and Pacurar (2008). Our contribution lies in modeling the correlation structure of inter-arrival times of distress events. Within self-contained autoregressive models, we find significant evidence of clustering behavior and duration persistence. The duration process exhibits self-exciting behavior, i.e., demonstrates a high degree of persistence, even when conditioned on variables related to various risk factors. The degree of persistence diminishes substantially only when low frequency variation in the conditional duration mean is encompassed in the form of mean level shifts. This finding confirms the existence of long-run shifts in the contagion mechanism in sovereign bond markets which generates both prolonged periods of tranquility and turbulent periods where distress events appear consecutively in short period intervals. An asset class effect of substitution from equities to safer markets and a credit risk related flight-to-quality effect are present affecting significantly the probability of abnormal movement occurrences. The rest of the paper is structured as follows. In Section 2, we provide an overview of the alternative ACD specifications used in the paper. Section 3 contains a summary of the methodologies employed for evaluating density forecasts. Section 4 contains the data description and the empirical application. In Section 5, we introduce additional explicative variables in the conditional mean duration specification. Section 6 tests for the presence of unconditional duration mean level shifts and investigates their potential effect on estimated persistence. Finally, Section 7 offers some concluding remarks.",
39,4,Journal of Economics and Finance,19 November 2013,https://link.springer.com/article/10.1007/s12197-013-9277-8,"Credit, venture capital and regional economic growth",October 2015,Barbara Pistoresi,Valeria Venturelli,,Female,Female,Unknown,Female,"During the last few years the general conclusion from the empirical literature on economic growth is that countries with “better” financial systems enjoy more rapid economic growth, but they have not provided unequivocal answers to what “better” is, that is which financial system characteristics, for example size, efficiency, competitive context, regulation, role of non-banking intermediaries, are most significant to foster economic growth. With respect to this literature, that we partially review in the next section, the present paper is one of the first attempts to investigate the nexus between finance and regional economic growth by means of an international comparison, making joint use of banking system structural indicators (i.e. the predominance of mutual bank circuits or large commercial banks) and risk capital values obtained in a local context. The distinction between commercial and mutual banks was intended to categorise mutual banks as those intermediaries which also use soft information and implement relationship lending policies, important for the provision of finance to support local economies. To assess the nexus between banking system structure, risk capital operators and economic performance, information about the structure of the regional financial system must be added to the information about local economic development. In our study, regional size is taken as approximate to NUTS level 1 and 2 depending on the countryFootnote 1, while the banking system characteristic of significance here is approximated by the amount of credit supplied and the share of loans granted by mutual and commercial banks. The role of risk capital operators is proxied by the value of investments for the initial stages of development (seed and start-up), measured at the local level. The evidence of a statistical relationship between financial variables and growth does not allow unequivocal identification of the direction of the relationship and a problem of simultaneous bias exists. OLS estimates are inconsistent and different estimators are necessary. Hence, following for example Levine et al. (2000) and Beck et al. (2000a), we use the dynamic panel generalised method of moments (GMM) to address the potential endogeneity of the data. In particular, we analyse a panel of 53 regions belonging to three countries, Germany, Italy and Spain, during the period 1995–2008. As usual in the empirical literature on finance and growth (see, for example, King and Levine 1993; Rioja and Valev 2003; Beck and Levine 2004)Footnote 2, we use data averaged over 5-year intervals to focus on longer-run relationships. For robustness, we also use 3-year intervals and a one-year overlapping horizon as in Bekaert et al. (2001, 2005) in order to maximize the time-series content of our dataFootnote 3. The number of countries investigated is limited by data availability. The choice of Italy, Germany and Spain stems from the fact that these countries differ with regard to the characteristics of their financial systems and the degree of regional disparity in terms of local economic development. For this reason the financial system characteristic of significance here is disaggregated depending on whether the region can be classified as an EDA (Economically deprived area) or not (other regions)Footnote 4, in order to ascertain whether the behaviour of the financial variables differs in relation to the different regions’ level of economic development. The results underline the important role played by different types of financial intermediaries in regional economic growth: both mutual and commercial banks have a positive effect on regional economic growth, but the role of mutual banks is greater in economically deprived areas [EDAs]. Finally, the positive effect of venture capital investments is also more effective in EDAs regions. Similar results are also obtained by a panel quantile regression useful to study possible different impacts of the financial variables to different maturity of the regional economies. These results are also robust to the inclusion of regional and national control variables and to change in the data frequency. The paper is structured as follows. Section 2 reviews the theoretical and empirical literature on the nexus between finance and economic development. Section 3 briefly presents the econometric methodology and the data used. Section 4 describes the results and discusses the robustness. Finally, Section 5 concludes.",4
39,4,Journal of Economics and Finance,22 March 2014,https://link.springer.com/article/10.1007/s12197-014-9281-7,Should the Fed take extra action for the recent housing bubble? Evidence from asymmetric transitory shocks,October 2015,MeiChi Huang,LinYing Yeh,,,,Unknown,Mix,,
39,4,Journal of Economics and Finance,06 May 2014,https://link.springer.com/article/10.1007/s12197-014-9279-1,Hedonic valuation of land protection methods: implications for cluster development,October 2015,Robert W. Kling,T. Scott Findley,David M. Theobald,Male,Unknown,Male,Male,"In response to both urban and rural sprawl concerns, hundreds of local ballot initiatives have been approved across the United States, which legally support, finance, and/or mandate open space conservation (Kline 2006). While tax-financed public acquisition of open space is one strategy, such efforts often face political and economic constraints on the acceptable tax burden. An alternative strategy operates through the local development planning process, called “cluster development”. Cluster development is characterized as having an approved number of residential housing units that are grouped together on land parcels or lots that are smaller in size and in closer proximity to each other than what residential zoning laws usually require. The remaining undeveloped land within the development is typically designated as open space for ecological, recreational, aesthetic, or agricultural purposes. An important unanswered question is how residential real-estate values are affected by inclusion within cluster housing developments that are designed to provide open space that might be valuable to both the development residents and to the broader community. Several studies report that proximity to protected open space generates higher property values to homeowners (e.g., Kitchen and Hendon 1967; Weicher and Zerbst 1973; Correll et al. 1978; Nelson 1988; Lee and Linneman 1998; Bolitzer and Netusil 2000; Lutzenhiser and Netusil 2001; Acharya and Bennett 2001; Smith et al. 2002). Other studies have compared the impacts on property values resulting from open space that is simply undeveloped and open space that is protected from development for the long run (Geoghegan 2002; Irwin and Bockstael 2001; Irwin 2002; Geoghegan et al. 2003). It is conceivable that a premium for protected open space is intrinsic to the values of properties within cluster developments. If they do provide sufficient internalized benefits to developers and eventual owners in the form of aesthetic and recreational opportunities in the protected space, then the market values of these developments would provide private motivation for developers to undertake cluster development. Yet in many areas, the cluster development approach to land management must be legally mandated and/or incentivized by policymakers, suggesting that private motives are small or non-existent. In such cases, mandated cluster development might impose significant private costs without commensurate internalized benefits, as is often the case with imposed regulations (Cheshire and Sheppard 1989). Indeed, land management authorities frequently make appeals to perceived external benefits that will accrue to present and future citizens at-large, as rationale for such land-development policies. The present study examines whether open space conserved in cluster developments generates value for proximate homeowners. We consider the case of cluster development policy in Larimer County, Colorado where rural sprawl has received high attention by local land management authorities. Larimer County is a high-growth, high-amenity region of north-central Colorado, roughly 50 to 100 miles north of Denver. The county is bordered on the north by Wyoming and on the west by the Continental Divide. The county includes urban and rural areas, mountains and plains, and it is home to approximately 300,000 residents. The Land Use Code of Larimer County was modified in the late 1990’s to require that all new residential housing subdivisions in non-urban areas be designed as cluster developments. Two types of cluster developments have been designated in the Code. One is known as a Rural Conservation Development (RCD), while the other is known as a Rural Land Use Plan/Process (RLUP). While both require that open space be preserved by private protective covenants or conservation easements, the two alternatives really differ only in what type of non-urban land is considered for subdivision and cluster development (Ernst and Wallace 2008; Wallace et al. 2008). We employ a generalized spatial hedonic pricing model to estimate and examine the impact of cluster development practices in non-urban areas. More specifically, we measure the effect of proximate protected lands of various types, in addition to and separate from the basic clustering effect that has resulted from the creation of county-approved cluster developments. We use a sample of 4,008 non-urban single-family housing sales transactions from 2001 to 2004, and we employ a variety of model specifications. Our spatial hedonic model is “generalized” from the perspective that we are able to simultaneously control for the spatial dependence of observations and for spatial autocorrelation. We find that proximity to national or state park land and to city or county open space has a positive impact on property values, while proximity to national forest or to privately conserved land exhibits no significant effects. Although private conservation efforts do not have any measurable impact on property values in general, we do find that county-mandated cluster development lowers the value of an included property by 17 to 26 %, depending on the model specification. These findings suggest important considerations for developers, subsequent homeowners, and for policymakers who oversee policies that affect land development and a property tax base.Footnote 1 Yet, we acknowledge an important caveat with respect to our empirical findings and statistical inference: our estimated sample is comprised entirely of non-urban observations. As such, the possibility exists that cluster development might increase the values of included properties in urban areas where population densities are notably higher. Footnote 2
",1
39,4,Journal of Economics and Finance,09 May 2014,https://link.springer.com/article/10.1007/s12197-014-9285-3,"Capital structure choice, information asymmetry, and debt capacity: evidence from India",October 2015,Surenderrao Komera,Jijo Lukose P.J.,,Unknown,Unknown,Unknown,Unknown,,
39,4,Journal of Economics and Finance,27 May 2014,https://link.springer.com/article/10.1007/s12197-014-9288-0,Industry cluster and performance sensitivity,October 2015,Michaël Dewally,Yingying Shao,,Male,Unknown,Unknown,Male,"The economics literature is rich in studies of industrial aggregation. Historically, these studies evolve from the early 20th century urban economics studies that identified that scale economies would benefit both cities and firms located in these cities (Quigley (1998)). The theoretical and empirical evidence builds over the ensuing century to recognize that beyond economies of scales, shared inputs of production, reduction in transaction cost and finally knowledge spillover all underpin the location decision of firms.Footnote 1 Whereas these studies extensively identify the drivers of firm’s choice of location, few studies in the finance literature address the corresponding impact to firms upon their decision. Recently, Almazan et al. (2010) find that firms that decided to co-locate with competitors adjust their financial strategy so as to benefit the most from their location. In particular, the study finds that firms located in an industry-cluster carry slack cash on their balance sheet and engage in more merger activity. The availability of more information about competitors and familiarity with their prospects lead firms to maintain the slack resources in order to take opportunistic advantage of the market for corporate control. This study investigates the effect of co-location has on a firm’s performance in conjunction with its strategic decision of production sensitivity. As suggested by previous studies that co-location in clusters provide positive economic externalities from the aggregation of firms, co-located firms can alter more easily their production decision knowing that co-location facilitates their overcoming economic and production hurdles. For example, a firm located in a cluster will have easier access to highly specialized skilled human capital and can rely on a more informed banking infrastructure. In other words, such a firm finds a dedicated established network that will cater to its needs in times of expansion. In contrast, a firm located away from the cluster will not benefit from such permeability of the constraint and will find it harder to capitalize on its growth prospects. These differences should be reflected in different sensitivities of firm performance across the two groups. In a first step, we investigate the difference in performance persistence in response to changes in firm’s profitability, namely return on assets (ROA) and its two components, profit margin (PM) and assets turnover (ATO), across the cluster locations. Then we investigate if these differences are related to the firm’s strategy. We describe a firm’s strategy with the marginal rate of substitution (MRS) between PM and ATO. In other words, our objectives are two-fold: (1) documenting that performance persistence depends on companie’s strategic decision, and (2) investigating if cluster location has an impact on the persistence. We use a panel data of U.S. firms in large industries where an industry is defined as large if its membership exceeds 20 firms at the 3-digit SIC level, and we separate firms as being within or outside an industry cluster. We define a firm to be within a cluster if 10 firms in the industry are co-located within the same Metropolitan Statistical Area (MSA). Overall, our results show that innovations in PM and ATO have a negative impact on future profitability. However, this adverse effect is curbed by a firm’s strategic management decision, depending on the area of specialty. Firms who specialize in operating efficiency benefit more from innovation in PM but, on the other hand, firms specializing in asset utilization are not able to sustain improvement in ATO. Finally we observe that firms that locate within an industry cluster are able to redress this deficiency and are able to retain the benefit of innovation in ATO compared to firms located outside the industry cluster. By documenting the links between performance forecasting and strategic management positioning and co-location decision, this study contributes to the literature in several ways. First, it systematically investigates the impact of strategic decision on firm performance by providing empirical evidence on the effect of changes in the components of ROA on future firm performance. Secondly, this paper complements the growing literature on the impact of firm location on corporate decision-making and how it impacts firm performance. In the next section, we review the literatures on profitability forecasting, strategic management and firm location. Section 3 provides an overview of the methodology and data. Section 4 presents the empirical results and Section 5 concludes.",2
40,1,Journal of Economics and Finance,05 June 2014,https://link.springer.com/article/10.1007/s12197-014-9283-5,The location of initial public offering headquarters: An empirical examination,January 2016,Michael Cichello,Douglas J. Lamdin,,Male,Male,Unknown,Male,"The growth of new firms and industries has been a focus of economic analysis for years. Some believe that the determination of new firm location is somewhat by chance. For instance, Baumol et al. (2007, pp. 109–110) state, “Locations hosting vibrant activity and innovation develop largely by serendipity, but once one or two firms in a location in a particular industry (or industries) become successful, they attract labor and entrepreneurs, and other services and suppliers, who build thicker and thicker networks, which in turn help spawn other new firms.” Under Armour, the athletic wear company, had its IPO in 2005, and is headquartered in Baltimore, MD. It was founded by Kevin Plank, a graduate of the University of Maryland. Chipotle Mexican Grill, which had its IPO in 2006, is headquartered in Denver, CO. Its founder, Steve Ells, graduated from the University of Colorado. Facebook, the social network firm, had its IPO in 2012. It is headquartered in Menlo Park, CA, not where it began in Massachusetts where Mark Zuckerberg started Facebook while a Harvard student. Are these corporate headquarters location decisions mainly serendipitous, with little explanation to be found, or can we explain why some states attract more IPO headquarters than others? Our goal is to shed some light on this issue. Despite an appearance of serendipity, policymakers at the federal, state and local level have made efforts to encourage entrepreneurs to form and grow their businesses. For example, federal legislation in the form of the Jumpstart Our Business Startups Act (JOBS Act) was signed into law in April 2012 with the intention to increase small business funding. However, unless there is a good understanding of the factors that matter for business growth, policies designed to enhance it are not likely to succeed. We attempt to increase this understanding. From a geographic perspective, differences exist in both institutions and population characteristics at the state level. Do these differences matter for the spatial distribution of IPOs across states? More specifically, we examine whether education, economic climate, urbanization, and the location of a financial center in a state influences state-level IPO intensity. Our results show that a higher level of education attainment corresponds to higher IPO intensity. We also find a positive relationship between IPO intensity and the economic climate measure, degree of urbanization, and having a financial center. There is a large IPO-related research literature. There is another large literature that focuses on new business formation (but not IPOs in particular). Our focus appears to be unique viewed from the perspective of either of these literatures. We are unaware of studies that have studied geographic differences in IPO headquarters location. Wojcik (2009, p. 193) comments: “Although Initial Public Offerings (IPOs) belong to the most publicised events on financial markets and one of the most researched areas in finance and economics, little research has been conducted within these disciplines on the geography of IPOs within countries, as if it is assumed that geography of regions and urban centres did not matter for public equity markets.” He continues (p. 194) “there is no systematic literature on why propensity to go public could depend on corporate location within a country.” Although not a large focus of interest in their work Kenney et al. (2012)) comment that their data show “there are marked differences between states in terms of their ability to create new firms that grow sufficiently to be eligible for an IPO.” It is these “marked differences” that we try to explain, and the gap in the existing literature we seek to begin to fill. As mentioned earlier, policymakers have made efforts to encourage and enhance new business formation and growth. Without a research base that informs policy development, these policies are less likely to achieve their goals. Our results may shed further light on what factors appear to be conducive to a particular form of business growth, the IPO. These results can be useful to economic development policy. It is not surprising that state and local governments seek to retain existing corporate headquarters, and be the location for new ones. The contribution to the local economy of, for example, the Home Depot headquarters (IPO in 1981) to the Atlanta, GA area, or the Microsoft headquarters (IPO in 1986) to the Redmond, WA area, or the Apple headquarters (IPO in 1980) to the Cupertino, CA area is undeniable. Of course, not all of Home Depot’s 331,000 full-time employees, or Microsoft’s 94,000 full-time employees, or Apple’s 63,300 full-time employees reside in the immediate area of the headquarters.Footnote 1 The corporate headquarters do, however, tend to have highly-paid employees. Headquarters activities also tend to require highly-paid providers of business services (Katz 2002). Card et al. (2010) also document the financial importance of corporate headquarters to local charities. One aspect of the Facebook IPO that generated much attention was the expected large income tax revenue windfall that California would receive when Facebook shareholders residing there realized large capital gains after the IPO. We see at least two reasons for the state-level focus. One is that at a national level, variation in IPOs over time within a nation is unlikely to uncover differences in factors that influence IPO activity because factors measured at this level change slowly over time. The time-series variation in IPO activity at the national level is strongly influenced by the state of the stock market. IPOs activity rises and falls with bull and bear markets. This makes detecting of other factors difficult to identify. States, however, have the potential to exhibit the type of cross-sectional differences in characteristics that can help explain why IPO intensity differs. A second reason for a state-level focus is that public policies designed to influence new business location are often a state-level policy matter.Footnote 2 States differences in policies, as well as institutions and demographic characteristics can be large. Thus, these differences can be used to potentially identify factors that influence IPO intensity. The paper is organized as follows. In Section II, we review the literature. Section III provides a description of the data and empirical model. Section IV provides results of our analysis. Finally, Section V concludes the paper.",1
40,1,Journal of Economics and Finance,08 June 2014,https://link.springer.com/article/10.1007/s12197-014-9289-z,The role of the seller’s stock performance in the market reaction to divestiture announcements,January 2016,Pascal Nguyen,,,Male,Unknown,Unknown,Male,"Existing studies provide consistent evidence that divestitures create shareholder value.Footnote 1 A number of arguments can be called upon to explain the positive market reaction to divestiture announcements. First, the divested assets can be reallocated to more valuable uses outside the firm and the seller is usually able to capture a significant portion of the gains (Alexander et al. 1984; Jain 1985; Hite et al. 1987). Second, divestitures allow firms to eliminate negative synergies (John and Ofek 1995; Daley et al. 1997; Berger and Ofek 1999; Hanson and Song 2003). Internal resources, and especially management effort, can hence be redeployed to higher-value activities within the firm. Third, divestitures contribute to mitigate financial constraints and decrease the firm’s cost of capital (Shleifer and Vishny 1992; Lang et al. 1995). As a result, the firm may be able to undertake additional value-enhancing investments to the benefit of its shareholders. However, the scope for value creation is likely to depend on the firm’s performance. Well-performing firms are expected to have few opportunities for further improvement. Their assets are already invested in high-value uses. They are also unlikely to conceal negative synergies or suffer from financial constraints. Accordingly, it should be difficult for them be extract additional value (externally or internally) from their existing assets. In contrast, poorly-performing firms are likely to hold assets that do not cover their opportunity costs, interfere with the firm’s other operations, or over-consume internal resources such as management attention. For these firms, divesting assets is expected to generate greater value. The idea is similar to the one that returning capital to shareholders through higher dividends or share repurchases is more likely to create value for firms with poor investment opportunities as opposed to firms with good investment opportunities (Lang and Litzenberger 1989). The objective of this paper is to evaluate the hypothesis that divestiture announcements by poorly-performing firms are associated with higher abnormal returns. To measure firm performance, we use the firm’s stock return net of the market return. Low or negative excess returns in the 1-year or 2-year periods leading up to the divestiture announcement are likely to reveal that the firm’s use of its assets has become less effective or that the firm is no longer extracting an appropriate return from its assets (for example, because of a higher cost of capital). This market-based indicator may thus be more relevant than the widely-used accounting-based indicators (e.g., industry-adjusted ROA or industry-adjusted leverage) to evaluate the benefit from undertaking a divestiture. To perform our tests, we use a sample of 738 divestitures announced by French firms listed on the Paris Stock Exchange over the period 1990 to 2010. Some deals are very large and valued in billions of euro. Many though are relatively small, representing less than 2 % of the firm’s market value of equity. The results show that shareholders of the divesting firm receive on average a significantly positive abnormal return of 1.12 % in the 11-day window surrounding the announcement that is consistent with the market reaction to the announcement of divestitures by US firms (Alexander et al. 1984; Jain 1985; Hite et al. 1987; John and Ofek 1995; Lang et al. 1995). However, the abnormal return to well-performing firms is insignificant while the abnormal return to poorly-performing firms can be up to 4 % higher. Interestingly, the accounting-based indicators traditionally associated with the extent of the market reaction tend to be insignificant. For instance, poor return on assets relative to industry peers prior to the divestiture does not lead to higher abnormal returns contrary to what has been reported for US firms (e.g., Lang et al. 1995). In some specifications, divestitures by poor performers are even associated with lower abnormal returns. Similarly, divestitures by firms characterized by a lack of industry focus as indicated by their low sales-based Herfindahl indices attract a positive reaction from investors, but only over the shorter event window and not over the two longer event windows. Overall, our results suggest that the firm’s stock performance prior to the divestiture announcement is a more useful indicator of the potential value associated with the divestiture. In comparison, none of the accounting-based variables highlighted in literature is able to reliably explain the extent of the market reaction. Their effectiveness in previous studies may have come from larger deviations from efficiency that these variables were able to pick up (e.g., large inefficiencies due to over-diversification or excessive leverage). As firms have moved toward reducing inefficiencies, accounting-based variables prove less accurate and hence less useful, in explaining the difference in announcement returns. In contrast, our market-based indicator is better able to capture all possible deviations from efficiency as well as market expectations of what the firm’s efficiency should be. It follows that the market reaction to the divestiture announcement is more closely tied to that indicator. The rest of this paper is structured as follows. Section 2 describes the sources of value involved in divestitures and the determinants of abnormal returns. Section 3 presents the methodology. Section 4 describes the characteristics of the sample. Section 5 contains the empirical results. We review our findings and conclude in Section 6.",3
40,1,Journal of Economics and Finance,20 June 2014,https://link.springer.com/article/10.1007/s12197-014-9291-5,Effects of time horizon and asset condition on the profitability of technical trading rules,January 2016,Roy L. Hayes,Jingwei Wu,William T. Scherer,Male,Unknown,Male,Male,"Technical trading strategies rely on observation of past prices and trade volume to predict future returns. Previous research on technical trading strategies has focused on large indexes, such as the Dow Jones Index. In these studies, traders time horizons are often considered to be years. Furthermore, the trader is assumed to use the same technical trading strategy independent of the state of the asset or the broader markets. In the numerous academic studies that bear on the topic, no clear consensus can be found on the profitability of technical trading strategies (Isakov and Hollistein 1999; Bollinger 2002; 1992; Kaufman 1995; Ellis and Parbery 2005; Cootner 1962; Van Horne and Parker 1967; James 1968). Although, there is no consensus on technical trading among academics, there is evidence to suggest that practitioners implement and rely on technical indicators. Cheung and Wong (2000) assert that as many as 40 % of foreign currency exchange traders use technical trading strategies, a figure corroborated by several other papers (Carter and Van Aukenm 1990; Menkhoff 1997; Taylor and Allen 1992; Oberlechner 2001). Additionally, a recent study by De Groot et al. (2012) showed that short term trading can be profitable. This paper builds on previous research and studies that compare the performance of technical trading strategies to that of a long-term investment strategy often termed buy and hold. The goal is to determine, whether, for short time horizons (1 month), technical trading strategies can outperform the buy and hold strategy. This study uses 296 stocks from 1997-2012. This is the same collection of stocks used by Feng et al. (2012). The 296 stocks are the stocks that were continuously listed in the S&P 500, between 1997-2006. These stocks were selected because they are likely to be traded by investors. The following technical trading strategies are studied: 
 Moving Average, Adaptive Moving Average, Bollinger Bands, KDJ, Commodity Channel Index. These strategies were chosen because they are among the most popular methods in technical trading. This paper also examines technical trading strategies in a new manner, where stocks are segmented into high- and low- volume and volatility groups. Additionally, months are segmented into high- and low- volume and volatility. The goal is to determine if one strategy outperforms another under different stock conditions. The mean monthly return across all stocks for each strategy was compared against that of the buy and hold using Tukey’s test (see, e.g., Ramachandran 1956), it was found that the Bollinger Bands strategy was able to perform as well or better than the buy and hold strategy, under all asset conditions. By contrast, none of the other strategies were able to consistently match or exceed the performance of the buy and hold strategy, for all asset conditions. However, the Commodity Channel Index was able to consistently exceed the buy and hold strategy for highly volatile assets, during low volume trading months. This paper shifts the three-decade-long debate over technical trading’s profitability and asks the question: are certain stock conditions conducive to technical trading? The paper is organized as follows: Section 2 examines previous literature on technical trading rules. Section 3 presents the data used in this study and explains in greater detail how stocks are classified into volume and volatility states. Section 4 lays out the experimental methodology. Section 5 presents the results. Lastly, Section 6 concludes with final thoughts and plans for future work.",
40,1,Journal of Economics and Finance,11 July 2014,https://link.springer.com/article/10.1007/s12197-014-9292-4,"International strengthen of intellectual property rights, and financial markets development",January 2016,Giuseppe Di Vita,,,Male,Unknown,Unknown,Male,"Recent researches have demonstrated that financial markets development and technological progress are positively correlated (Ang 2011; Garcia and Liu 1999). In particular, stock market capitalization is considered the main indicator of capability of financial market to address risk capital to the most efficiency firms, demanding external founds, to support R&D activity (Bena and Ondko 2012). In a world where the stock markets are strongly internationally connected (Lothian 2002), a reinforcing of international uniform legal protection of Intellectual Property Rights (IPR) is expected to have a positive impact on financial markets, in consideration of simplest commercialization of new products performed by innovative technologies. The legal strengthen of IPR protection may promote the international standardization of laws to protect innovation. This way each country has no room to introduce policies on IPR protection level. Otherwise, they could increase the protection of innovations within the borders of each nation, thus promoting an increase in GDP. While the first kind of policy promote international movement of capital to finance R&D, the second could constitutes an obstacle for firms in rising external founds in stock market, as a negative consequence of unlike regime of innovations. Thus a possible trade-off could emerge between internationally standardized versus within border country strengthening of IPR legal protection. On 15 April 1994 was signed in Marrakesh (Morocco) the Uruguay Round Agreement establishing the World Trade Organization (WTO) among 123 countries. The aim of agreement is to extend the GATT trade rules to increasingly important new areas previously not included like, among others, intellectual property (Kanwar 2012). In particular, the WTO Agreement on Trade-Related Aspects of Intellectual Property Rights (TRIPs) constitutes Annex 1 C of the Uruguay Round Agreement, to strengthen the Intellectual Property Rights (IPR) and raise the common minimum level of protection of innovation among countries members (Sell 2010). The TRIPS Agreement may be considered a step forward compared to previous agreements on IPR,Footnote 1 because its ratification was a mandatory condition to become a member of the WTO. Each nation attempting to obtain access to international markets opened by the WTO must embody the strict intellectual property laws established by the TRIPS Agreement. This is the reason why the TRIPS Agreement is the most important multilateral instrument for the globalization of IPR laws. Several developing countries were forced to implement the requirements of the TRIPS in their legal systems, to become members of the WTO. The TRIPs agreement come in force in 1 January 1995. Developing countries have 5 years to implement the prescription of agreement in their legal systems. The extremely poor economies have an extra term of 6 years to embody this international agreement in their legal systems. In economic literature there are many excellent papers studying the relationship between the stock market capitalization and the technological progress (Ang 2011), but there are no research regarding TRIPs effects on financial market. To account for the spill-over of TRIPs convention to explain the stock markets development of the financial markets due to the technological improvement related to the increasing protection of IPR, a dummy variable (tripsdummy) is considered. While, the differences in IPR legal protection, among countries, are undertaken considering the index elaborated by Ginarte and Park (1997) and recently updated by Park (2008) (G&Pindex).",1
40,1,Journal of Economics and Finance,24 July 2014,https://link.springer.com/article/10.1007/s12197-014-9297-z,Specification errors of asset-pricing models for a market characterized by few large capitalization firms,January 2016,Nader Shahzad Virk,Hilal Anwar Butt,,Male,Female,Unknown,Mix,,
40,1,Journal of Economics and Finance,27 July 2014,https://link.springer.com/article/10.1007/s12197-014-9295-1,Powerful CEOs and employee relations: evidence from corporate social responsibility indicators,January 2016,Joan Wiggenhorn,Seema Pissaris,Kimberly C. Gleason,Female,Female,,Mix,,
40,1,Journal of Economics and Finance,31 July 2014,https://link.springer.com/article/10.1007/s12197-014-9293-3,The debt-equity financing decisions of U.S. startup firms,January 2016,Susan Coleman,Carmen Cotei,Joseph Farhat,Female,Female,Male,Mix,,
40,1,Journal of Economics and Finance,31 July 2014,https://link.springer.com/article/10.1007/s12197-014-9294-2,The great recession and changes in faculty expected retirement age,January 2016,Kathleen G. Arano,Carl D. Parker,,Female,Male,Unknown,Mix,,
40,1,Journal of Economics and Finance,05 August 2014,https://link.springer.com/article/10.1007/s12197-014-9300-8,Federal Home Loan Bank advances and bank risk,January 2016,Travis Davidson,W. Gary Simpson,,Male,Unknown,Unknown,Male,"The Federal Home Loan Bank system (FHLB) has evolved into a powerful force in the financial markets and U. S. banking system since its creation in 1932 and re-direction following the savings and loan crisis of 1989 (Ashcraft, Bech, and Frame 2010). At the end of 2008, the FHLB had $1.258 trillion in debt obligations outstanding and had extended $929 billion in advances (loans) to member financial institutions (Office of Finance of the Federal Home Loan Banks 2008). The ability of this government sponsored enterprise (GSE) to obtain over a trillion dollars in funds from the financial markets and channel almost a trillion dollars of these funds into member banks, thrifts, credit unions, and insurance companies illustrates the potential influence of this “large, complex, and understudied” GSE (Ashcraft, Bech, and Frame 2010). For comparison, the FHLB had total assets of $1.349 trillion in December 2008 compared to the Federal Reserve System with $2.258 trillion in total assets (Office of Finance of the Federal Home Loan Banks 2008 and Board of Governors of the Federal Reserve System 2008). Ashcraft, Bech, and Frame (2010) refer to the FHLB as the “lender of next-to-last resort” because of the amount of liquidity provided to financial institutions by the FHLB during the 2007 financial crisis. The FHLB has reduced the size of the system balance sheet significantly since the financial crisis to $766 billion at year end 2011, but the expansion of FHLB borrowing and lending observed during the financial crisis serves to emphasize the magnitude and importance of FHLB financing (Office of Finance of the Federal Home Loan Banks 2011). The FHLB issues debt obligations with numerous complex structures and an implied U. S. Treasury guarantee to obtain large amounts of funding from both the U. S. domestic and international financial markets at rates near the U. S. Treasury rate (Office of Finance of the Federal Home Loan Banks 2011). The funds obtained by the FHLB are not only loaned to small community banks and credit unions for housing finance and community lending, but the FHLB extends advances to some of the largest financial institutions in the world, including Bank of America Corporation, JP Morgan Chase & Co., MetLife, Inc, Citigroup Inc., and Banco Santander, S. A. (Office of Finance of the Federal Home Loan Banks 2011). FHLB advances are collateralized by the borrowing bank and the FHLB has a super-lien over general creditors of the bank, but the interest rates charged member financial institutions on the advances are not adjusted for the risk of a specific borrower. The observation that the FHLB is a major source of inexpensive funds without risk-adjusted interest rates for banks motivates our investigation of the consequences of FHLB lending to commercial banks. In addition, the fact that the FHLB was an important lender to several troubled financial institutions, like IndyMac, Washington Mutual Bank, and Countrywide Federal Savings Bank, during the recent financial crisis supports the relevance of an analysis of the relation of FHLB advances and bank risk taking (Scott and Hein 2011). The objective of this investigation is to empirically explore the relation between FHLB advances to commercial banks and the risk of those banks. The hypothesis of increased risk taking due to moral hazard appears to be conceptually straightforward. However, FHLB advances combined with other FHLB products and services may potentially increase some types of bank risk but decrease other types of bank risk (Stojanovic, Vaughan, and Yeager 2008). We consider the relation between FHLB advances and the following dimensions of risk for our sample of banks: financial leverage, credit risk, liquidity, and interest rate risk. The empirical analysis of the basic hypothesis is complicated because the structure of the economic event under investigation is subject to unobserved heterogeneity, simultaneity, and endogeneity. We address the aforementioned econometric challenges with system dynamic panel generalized method of moments (GMM) estimation as presented by Wintoki, Linck, and Netter (2012). Pre-tests and post-tests provide solid evidence that system dynamic panel GMM estimation is appropriate for this investigation. The panel of data has the advantage of covering almost all of the member banks of the FHLB for a ten year period from 2001 to 2010 including the time period of the 2007 financial crisis. The data panel contains 77,023 firm year observations for a set of banks that includes 8,413 individual banks in 2001 and decreases to 6,821 banks in 2010 as a result of consolidation in the banking industry. The contributions of this investigation are two-fold. First, we provide new evidence derived from a recent panel of data on the implications of financing arrangements provided by a large but understudied GSE that has evolved into a major source of liquidity for the banking system. To our knowledge, the only published investigation of the direct relation between FHLB advances and individual bank risk is Stojanovic, Vaughan, and Yeager (2008). Second, we apply system dynamic panel GMM estimation to control some of the inherent econometric problems associated with tests of bank performance. Our evidence indicates a nuanced association between the use of FHLB advances and bank risk. If the economic environment is normal and banks have relatively low probabilities of default, we find no evidence that advances are associated with higher credit risk, interest rate risk, leverage, and liquidity risk. To the contrary, we find that advances are associated with lower interest rate risk when banks have normal default probabilities. However, if banks have high default probabilities, we find that advances are associated with higher levels of risk. These results suggest that as banks operate closer to the default boundary, they seek to shift risk to the deposit insurance fund. The remainder of the paper is organized as follows. Section 2 presents the hypotheses that are tested and Section 3 provides a review of the previous evidence on these hypotheses. Section 4 describes the data, variables, and methodology and Section 5 provides the results of the hypotheses tests. The final section is the conclusion.",5
40,1,Journal of Economics and Finance,14 August 2014,https://link.springer.com/article/10.1007/s12197-014-9296-0,Persistence and cyclical dependence in the monthly euribor rate,January 2016,Guglielmo Maria Caporale,Luis A. Gil-Alana,,Male,Male,Unknown,Male,"The recent global crisis has generated a renewed interest in understanding how financial markets work and risk is measured. The standard theoretical paradigm for asset prices and interest rates has been called into question, a lot of criticism being raised against the usual no-arbitrage, single-curve approach; models including market segmentation, counterparty risk and interest rate dynamics are being developed by academics and practitioners, especially for the Euribor rate, since the effects of the credit and liquidity crisis have been very pronounced in the euro zone in particular. In addition to these theoretical developments, fresh efforts have been made to specify empirical models that can track well the behaviour of interest rates. The choice of such models is hotly debated, since it is unclear whether I(0) or I(1) processes are more appropriate. A well-known stylised fact is their high persistence. This could be approximated by an AR(1) (I(0)) process with a root close to 1 or by unit root (I(1)) processes. Earlier studies typically focused on whether interest rates can be characterised as an I(0) or I(1) series. For instance, Cox, Ingersoll and Ross (1985) concluded that the short-term nominal interest rate is a stationary and mean-reverting I(0) process, whereas Campbell and Shiller (1987) assumed a unit root. The drawback of the I(0) models is that they imply long-term rates which are not volatile enough (Shiller 1979) whereas the problem with the I(1) models is that they imply that the term premium necessarily increases with bond maturities (Campbell, Law, and MacKinlay 1997).Footnote 1
 More general I(d)-type specifications provide additional flexibility to model persistent behaviour. In the last two decades some studies have taken this approach. For instance, Shea (1991) investigated the consequences of long memory in interest rates for tests of the expectations hypothesis of the term structure. He found that allowing for the possibility of long memory significantly improves the performance of the model, even though the expectations hypothesis cannot be fully resurrected. In a related paper, Backus and Zin (1993) reported that the volatility of bond yields does not decline exponentially when the maturity of the bond increases; in fact, the decline is hyperbolic, consistently with a fractionally integrated specification. Lai (1997) and Phillips (1998) provided evidence based on semiparametric methods that ex-ante and ex-post US real interest rates are fractionally integrated. Tsay (2000) employed a fractionally ARIMA (ARFIMA) model and concluded that the US real interest rate can be described as an I(d) process, with d very different from 0 or 1. Further evidence can be found in Barkoulas and Baum (1997), Meade and Maier (2003) and Gil-Alana (2004a,b). Couchman, Gounder and Su (2006) estimated ARFIMA models for ex-post and ex-ante interest rates in sixteen countries. Their results suggest that for the majority of these countries the fractional differencing parameter lies between 0 and 1 and is considerably smaller for the ex-post real rates compared with the ex-ante ones.Footnote 2
 Concerning the euro area, Busch and Nautz (2009) estimate a long memory model and find that the persistence of policy spreads has significantly decreased since the changes to the operational framework of the ECB introduced in 2004 (when the maturity of the main refinancing operations (MRO) was reduced from 2 weeks to one, and the MRO were aligned with the reserve maintenance period and interest rate decisions). In other words, the persistence of deviations of longer-term money market rates from the central bank’s policy rate has decreased, which implies that monetary policy has become more effective in controlling interest rates. Hassler and Nautz (2008) and Cassola (2008) find that the fractional integration parameter for the spread between the overnight rate and the ECB’s policy rate is around 0.25, which indicates a lower degree of controllability of interest rates. Similar evidence is reported by Cassola and Morana (2008) for interest rates of one-week maturity. As for the US, Balduzzi et al. (1998) suggest that the persistence of policy spreads increases along the yield curve. However, their analysis assumes that they are stationary I(0) variables. Caporale and Gil-Alana (2009) focus on the Federal Funds effective rate and report that the estimate of the fractional integration parameter is sensitive to the choice of specification for the I(0) error term. Another well-known feature of interest rates is their cyclical structure (see, e.g., Kessel 1965) that is not well captured by I(0), I(1) or I(d) models, the last two of which are characterised by a spectral density function which is unbounded at the origin (i.e. the zero frequency). Typically, interest rates exhibit a peak in the periodogram at non-zero (as opposed to zero) frequencies, which indicates a certain degree of cyclical behaviour. This cyclical structure can be captured in an I(0) environment by a simple AR(2) process with complex roots; however, such a process is characterised by a very rapid decay in the autocorrelations, which is not consistent with the high level of persistence observed in interest rates. A long memory cyclical I(d) model can instead overcome this limitation, and the aim of the present study is to propose a model which takes into account these two important features (cyclicality and time dependence) in the case of the Euribor rate. The aim of this paper is to provide alternative specifications for the Euribor rate that take into account these two features of the data. These are based on long memory I(d) processes, which are more general than the standard methods based on the dicothomy between I(0) and I(1) processes. The paper is organised as follows. Section 2 outlines the concepts of long-range dependence and fractional integration. Section 3 discusses the main features of the data. Section 4 presents the estimation and testing results. Section 5 summarises the main findings and suggests some extensions.",4
40,1,Journal of Economics and Finance,21 August 2014,https://link.springer.com/article/10.1007/s12197-014-9298-y,Corruption perceptions versus corruption incidence: Competition for rents across Russian regions,January 2016,Veronika Belousova,Rajeev K. Goel,Iikka Korhonen,Female,Male,Male,Mix,,
40,1,Journal of Economics and Finance,23 September 2014,https://link.springer.com/article/10.1007/s12197-014-9301-7,"Comparing Africa, Asia and Latin America/Caribbean countries using per capita GDP, remittances, openness, capital/labor ratios and freedom",January 2016,Sondra Collins,Edward Nissan,,Female,Male,Unknown,Mix,,
40,1,Journal of Economics and Finance,06 December 2014,https://link.springer.com/article/10.1007/s12197-014-9310-6,Determinants of public spending efficiency in education and health: evidence from selected CEMAC countries,January 2016,Dobdinga C. Fonchamnyo,Molem C. Sama,,Unknown,Unknown,Unknown,Unknown,,
40,2,Journal of Economics and Finance,02 October 2014,https://link.springer.com/article/10.1007/s12197-014-9302-6,Can structural change explain the Meese-Rogoff puzzle?,April 2016,Nicholas Mangee,,,Male,Unknown,Unknown,Male,"Whether asset price fluctuations are connected to movements in market fundamentals remains a central question facing macro and financial economists.Footnote 1 Beginning in the 1980’s, researchers found that deviations from hypothesized equilibrium asset price-relations are too persistent to be reconciled within the traditional REH-based framework predicated largely upon time invariant, determinate modeling structures.Footnote 2
 The inability of conventional approaches to reconcile the long swings in security prices in terms of market fundamentals has led economists to pursue alternative avenues of research. One avenue rests on the “bubble” view which posits long swings in security prices as the result of crowd psychology and technical momentum-related considerations absent from trends in fundamental factors, such as earnings and interest rates (Barberis and Thaler 2003). Another strand of literature, gaining popularity among researchers in recent years, hinges on the view that market participants understanding of the future undergoes changes in ways that would be difficult to adequately capture in advance with a determinate overarching specification. Because individuals must cope with fallibility and ever imperfect knowledge of market relationships when formulating forecasting strategies, and how evolution of this process may occur over time, how change underpinning asset price-relations occurs cannot be foreseen. Therefore, change in the structure of an asset pricing model cannot be fully specified in advance.Footnote 3
 Included in this latter view are the recent development and advancements of both learning-based approaches (Pastor and Veronesi 2009; Adam and Marcet 2011) and models based on Imperfect Knowledge Economics (IKE) (Frydman and Goldberg 2007; 2011; 2013a,c). Both classes of models emphasize the instability of fundamentals-based asset price relations and the contingent change underpinning individuals’ understanding of this interdependence and, thus, the weights they attach to informational factors when thinking about the future. When investigating whether fundamentals matter for, say stock and currency behavior, researchers have found the robustness of their results to be highly sensitive to the empirical structure of the analysis such as model specification, choice of predictors, sample period, asset type, country, and forecast horizon (Campbell and Thompson 2008; Sarno and Valente 2009; Rossi 2013). Such contingent econometric findings are to be expected within a world of fallibility in which unforeseeable, or non-routine, structural change in the process underpinning market dynamics is an innate and important feature of capitalistic economies. Whether asset price fluctuations are connected to fundamentals once structural change is recognized is the main question addressed in this paper. It has become increasingly apparent to financial economists that temporal instability is a salient feature of the data generating process. A growing body of empirical evidence suggests that allowing for non-routine change in asset price processes can assist in understanding observed regularities in return behavior while simultaneously shedding light on plausible explanations for the lack of evidence supporting a fundamentals-based account.Footnote 4
 Structural change may arise from events including, but not limited to, changes in regulatory policy and institutional structure, the prevalence of economic and financial shocks, or from changes in the social context, all of which imply contingent asset price-relations over time. Sarno and Valente (2009), for instance, find that the difficulty of fundamentals-based accounts to predict returns is a consequence of poor model selection criteria and that the best predictive model of exchange rate fluctuations is one in which the set of fundamentals is subject to change - a finding that the authors attribute to “swings” in market participants’ expectations over time. For equity markets, Campbell and Thompson (2008), in response to Goyal and Welch (2008)’s finding that nothing can beat the historical mean in forecasting the equity premium out-of-sample, find that a time-varying information set of fundamentals, generated from imposing multiple sign restrictions on coefficients and equity premia, improves significantly the predictive ability of macroeconomic and financial regressors. Parameter instability, has also been offered as a prime candidate for explaining the famous Meese-Rogoff (MR) disconnect puzzle.Footnote 5 The seminal study of Meese and Rogoff (1983) dealt a devastating blow to the field of international finance by showing that structural exchange rate determination models could not out-perform the random walk model’s naive prediction of “no change” at various forecasting horizons. Their results maintained even when actual information on future fundamentals was provided. The MR results sparked a voluminous literature on the out-of-sample properties of exchange rate determination models. Alas, researchers have failed to convincingly overturn the original results (Chinn and Meese 1995; Cheung et al. 2008; Rogoff and Stavrakeva 2008). However, as Menzie Chinn - a leading international macro and financial economist - has recently stated, “...the book is still open for parameter variation to be an explanation for the Meese-Rogoff results” (Chinn 2010). It was already acknowledged in Meese and Rogoff (1983) that structural change may explain the poor out-of-sample performance of structural exchange rate determination models. Indeed, numerous studies have re-investigated the MR analysis with a focus on parameter nonconstancy. Some researchers have found that the allowance for structural change and regime shifts in the data generating process can improve out-of-sample predictability in asset markets (Massimo and Timmermann 2009; Henkel et al. 2010). Footnote 6 Frydman and Goldberg (2007, Chapter 15), for example, found that explicitly accounting for structural change in a composite monetary model of currency returns yields predictive ability superior to that of the pure random walk, even at short horizons, generating coefficients consistent with theoretically hypothesized signs and magnitude. This paper reexamines the Meese-Rogoff analysis in the context of the stock market with two main departures from extant studies. First, our econometric analysis explicitly allows for structural change in present value stock price-relations to unfold ex ante in open ways: recursive techniques are employed which do not prespecify the timing and magnitude of parameter instability. Indeed, events like firm management shake-ups and mergers and acquisitions are clear candidates for catalysts altering the process driving stock price behavior. However, such events do not occur all the time. Thus, the IKE-based theoretical model brought to the data implies that change in the process underpinning asset-price fluctuations will tend to be moderate for protracted intervals of time. As such, the Meese-Rogoff methodology is conducted in a piece-wise linear fashion within regimes of “statistical parameter constancy”. Second, the temporally unstable fundamentals-based IKE model of stock prices is specified with an order-one moving average residual process based on serial autocorrelation typically present when conducting econometric analysis involving nonstationary regressors.Footnote 7 This formulation allows for a better description of the data generating process. Subsequent econometric tests are employed to deal specifically with this issue. To our best knowledge, Flood and Rose (2010) - hereafter FR - is the only study explicitly extending the MR methodology to the stock market. Comparing Gordon Growth model variants, VAR modeling structures, and a battery of univariate models against the pure random walk benchmark, the authors are unable to overturn the seminal findings of MR leading them to conclude that, “International finance seems to be no worse at modeling important asset prices than domestic finance...” (pp. 130-31). The authors, however, make no explicit allowance for structural change in their analysis. Once this modification is allowed for, the IKE-based present value model is able to outperform the random walk model based on conventional measures of forecasting error and direction of change statistics, overturning the recent results of FR. Additionally, single multivariate-equation parsimonious tests for cointegration reveal that a longer-run relationship is operating between stock market prices and fundamentals, but only when structural change is allowed for. The paper is organized as follows. Section 2 provides a theoretical motivation for the subsequent empirical analysis. Section 3 presents the structural change test methodology and results. Sections 4 and 5 present the out-of-sample fit and cointegration results, respectively, while Section 6 provides concluding remarks.",6
40,2,Journal of Economics and Finance,23 October 2014,https://link.springer.com/article/10.1007/s12197-014-9299-x,Long memory in the Ukrainian stock market and financial crises,April 2016,Guglielmo Maria Caporale,Luis Gil-Alana,Inna Makarenko,Male,Male,Female,Mix,,
40,2,Journal of Economics and Finance,07 November 2014,https://link.springer.com/article/10.1007/s12197-014-9303-5,Insider behavior and R&D changes around seasoned equity offerings,April 2016,Robert M. Hull,Sungkyu Kwak,Rosemary L. Walker,Male,Unknown,Female,Mix,,
40,2,Journal of Economics and Finance,09 November 2014,https://link.springer.com/article/10.1007/s12197-014-9304-4,Macroeconomic factors and firm’s cross-border merger and acquisitions,April 2016,A. N. Bany-Ariffin,Mohamad Hisham,Carl B. McGowan Jr.,Unknown,Male,Male,Male,"There have been significant numbers of studies on the effect of local merger and acquisition activity on a firm’s shareholders wealth. However, the results have been inconclusive. For instance, the meta-analysis conducted by Homberg et al. (2009) and Cartwright and Schoenberg (2006), shows that the wealth effect resulting from within-border mergers and acquisitions range from value-additive to value neutral. Such results are in accordance with the financial theory that suggests that diversification via merger and acquisition at the corporate level is redundant. This is because in a capital market that is relatively efficient, investors can directly derive the gains of such diversification by holding a diversified portfolio. On the other hand, cross-border diversification via mergers and acquisitions, M&A, could yield benefits to firms that might not be fully realized by shareholders through cross-corporate, domestic, portfolio diversification. Therefore, cross-border diversification may produce greater wealth effects. These cross-border diversification benefits arise from the firm’s greater ability to exploit the strategic advantage of countries with differences in variables such as natural resources, tax structure, government regulations, technology, trade agreement, correlations between countries’ economics cycles and even monopolistic market power. Empirical evidence indicates that a firms’ ability to exploit these differences in macroeconomics factors can be value enhancing. Manzon et al. (1994) show that in the 1990’s, US multinationals derived incremental benefits from foreign acquisitions with the introduction of the new Internal Revenue Code. US multinationals were able to do so by utilizing their international financial network to selectively repatriate dividends or foreign income in a manner that brings greatest tax benefits. Evenett (2003) studies cross-border M&A and evaluates the effect of US acquisitions in 13 OECD countries in banking. Empirical results show that profitable acquisitions made by US multinationals depended on the characteristics of the target country, such as gross domestic product (GDP), distance from the U.S., corporate tax rate, average tariff rate, and the type of legal system. Rossi and Volpin (2004) conduct an econometric study on cross-country determinants of international and domestic M&A. They report that firms in countries with weaker investor protection are more likely to be acquired than those in countries with stronger investor protection, whereas buyers are more likely to be from countries with relatively strong investor protection. In this study, we argue that the benefits of country diversification via M&A are closely related to the degree of difference in the macroeconomic factors of the countries in the study. The greater the divergence, the better the opportunities for firms to realize the benefits of country diversification. Therefore, we hypothesize that abnormal returns are significantly related to the macroeconomics factors of the host countries. The primary objective of this study is to investigate the wealth effect of cross-border M&A on Malaysian multinationals firms. This study further explores the differences in wealth effects of firms with respect to the macroeconomics factors in the countries in which the foreign target companies domicile. Malaysia, among the developing economies in Asian region, has long been identified as the target of FDI flows from developed nations. However, since the mid-1970s, Malaysian companies have started outward foreign direct investment (OFDI). From 1993 onwards, Malaysian multinationals participated increasingly in investment activities abroad. According to UNCTAD (2006), for the period of 1990–2005, Malaysian outward FDI grew from USD 129 million to USD 2,971 million of FDI outflows. Malaysia is now recognized as a significant contributor of outward FDI from Asian developing economies. The increase in OFDI from Malaysia demonstrates the growing confidence of Malaysian firms to venture overseas. The OFDI shows the ability and willingness of Malaysian companies to invest abroad. A few Malaysian multinational corporations (MNCs) that have actively participated in OFDI by investing abroad have been included in the Top 100 non-Financial Transnational Corporations. Based on world ranking in terms of foreign assets, PETRONAS is ranked 84th and is Malaysia’s only company in the world’s top 100 non-financial TNCs in 2007. Malaysia also has five other companies in the top 100 non-financial TNCs from developing countries, ranked by foreign assets in the year 2007. These companies include Petronas (Petroliam National Berhad) (5th), YTL Corporation Berhad (31st), Genting Berhad (38th), Sime Darby (46th), and Telekom Malaysia Berhad (57th) (UNCTAD 2008). In conjunction with the trend of OFDI from Malaysia is the issue of international mergers and acquisitions (M&A). Mergers and acquisitions have become the key corporate strategy MNCs use to expand, diversify, or consolidate their businesses in international markets. However, research on Malaysian M&A is scarce and only a few recent studies on acquisitions have been found. Lee and Mansor (2008) examine the long-term share performance of acquiring firms listed on the Malaysian stock exchange during the period of 2000 to 2004. Studies by Fauzias (1992, 1993) assessed the efficiency of the Malaysian stock market’s reaction to acquisition announcements and examined the effect of acquisition announcements on the price behavior of Malaysian bidders and target firms by using three alternative models (one factor market model, Capital Asset Pricing Model, regression estimation model). In addition, there is only one recent study by a Malaysian researcher which examined the effect macroeconomic variables on overall outward FDI from Malaysia, but this study does not specifically look at cross-border acquisitions (Kueh et al. 2009).Footnote 1
 Thus, with the dearth of research on Malaysian cross-border acquisitions from the macroeconomic perspective, the current study contributes to the existing literature on firms’ gains from internationalization, especially on the issue of cross-border acquisitions. Specifically, this study will provide some insights and implications regarding the effect of macroeconomic factors on M&A activity of MNCs from a developing nation.Footnote 2
 The empirical results of this study show that on average Malaysian MNC experience statistically significant wealth gains during their cross-border acquisitions announcement. This study examines three factors with regards to the proposed macroeconomic variables that may have an influence on the wealth effect. They are foreign economic conditions, GNP correlation between countries, and the level of economic development of the target’s country. Of the three variables, the findings indicate only foreign economic conditions and the level of economic development of the target’s country significantly affect the wealth effect of cross-border acquisitions of MNCs. Specifically, foreign economic conditions affect the wealth effect negatively, while the level of economic development of the target’s country affects the wealth effect positively. This paper is organized into four additional sections. Section 1 outlines the evolution of Malaysian MNCs followed by the previous empirical studies. Section 3 describes the sample selection and methodology. Section 4 presents and analyzes the wealth effects of the foreign M&A and analyzes the determinants of the wealth gains. Section 5 presents the conclusions of the paper derived from the empirical results. Malaysia used to be known as a destination for FDI but is now spearheading the region’s outward foreign direct investment (OFDI) with Singapore (Pananond 2009). The OFDI is directed mainly to neighboring countries in the region. As supported by BNM (2009), there has been a sharp increase in OFDI, averaging 3.5 % of GDP per annum between 2000 and 2009, and peaking at 6.8 % of GDP in 2008. Malaysia ranks fifth among the top 10 FDI sources from South, East and South-East Asia for 2008–2009, leading other ASEAN neighboring countries, UNCTAD (2010). As referred to by Ariff and Lopez (2008) to the BNM report, during 1999–2005, investments abroad were dominated by Malaysian government linked companies (GLCs) and RCCFootnote 3s with 61 % of OFDI. These investments were primarily in the services sector at 48 % followed by mining (48 %), agriculture and manufacturing (8 %) each. GLCs dominated the oil and gas, and the telecommunications industries, while RCCs dominated manufacturing, utilities, distributive trade, leisure, plantations, construction, and banking and finance industries. The investment abroad by both GLCs & RCCs were undertaken mainly through joint ventures with foreign partners abroad and acquisition of equity stakes. According to Tham (2006), the growth of OFDI was attributed to the high labor cost in Malaysia, weakening of export competitiveness, and increased liberalization of investment regulations in the region. Increased OFDI was as a result of structural changes in the Malaysian economy as Malaysia was experiencing impressive economic growth (Ariff and Lopez 2008). Malaysian firms relocated production activities overseas in response to competitive pressure from globalization as well as increased trade openness in Malaysia. With a small domestic market and constraints from domestic factors of production, MNCs can use investment activities abroad to expand their market base abroad and benefit from the increasingly globalized economy. Malaysian GLCs and private companies were encouraged by the government to venture abroad to foster the creation of successful MNCs in the longer term so that they can become part of the global production network (Soo and Koi 2010). Hiratsuka (2006) suggested that the push factors for investment abroad by Malaysian MNCs include the rising wealth of individuals and corporations, high domestic savings, rising labor costs vis-à-vis Malaysia’s regional neighbors, limits of domestic markets, domestic deregulation in strategic sectors such as telecommunications and utilities, the promotion of OFDI by the government, and trade liberalization in countries especially in the ASEAN region. The rising accumulation of wealth among domestic companies as a result of liberal foreign exchange administrative rules, high domestic savings rates, and strong economic growth rates have accorded companies in Malaysia the flexibility to invest abroad (BNM 2010). In the study by Syed-Zamberi and Kitchen (2008), firms would increase their investment abroad once ownership advantages have been accumulated by the Malaysian company. Besides brand and technology, ownership advantages include non-business factors such as government network and support by the Malaysian government. The government facilitated MNCs’ initiatives to venture abroad and provided special incentives such as all income remitted by Malaysian companies investing overseas are fully exempted from income tax since 1995 and an additional incentive was introduced in 2003 for acquiring foreign-owned companies abroad for high-technology production within the country (Soo and Koi 2010; Tham 2006). With the encouragement by Malaysian government to invest abroad and become MNCs by identifying and building upon niche products and services for specific markets, Malaysian MNCs have now invested in Thailand, Sudan, Indonesia, Mauritius, Vietnam, South Africa, Philippines and China (Kamaluddin 2008). The pull factors for Malaysian firms to invest abroad on the other hand include cheap labor, abundance of raw materials, large and growing domestic markets, geographic proximity, special tax and other incentives, and the development of export markets through preferential treatment (Hiratsuka 2006). These pull factors can be divided into efficiency seeking OFDI, resource seeking OFDI, and market seeking OFDI. In addition, Malaysian MNCs consider brands and technology to be strategic assets and decentralization of operations to be useful to diversify risks and improve return when selecting host locations (Ariff and Lopez 2008). Generally, the factors driving Malaysian companies to invest abroad are similar to other global companies and conform to the theories of developing a rationale for the evaluation of international business such as comparative advantage, imperfect markets, and product cycle theory. In addition, other factors considered in the OFDI decision are expanding markets for MNCs’ products, gaining access to more competitive supplies of land and labor, expanding into rapidly growing industries, acquiring brands and technology, and taking advantage of the comparative advantage of host countries (BNM 2010).",2
40,2,Journal of Economics and Finance,19 November 2014,https://link.springer.com/article/10.1007/s12197-014-9305-3,Developments in non-expected utility theories: an empirical study of risk aversion,April 2016,Dorsaf Ben Aissia,,,Unknown,Unknown,Unknown,Unknown,,
40,2,Journal of Economics and Finance,11 December 2014,https://link.springer.com/article/10.1007/s12197-014-9309-z,A private universal voucher program’s effects on traditional public schools,April 2016,Nathan L. Gray,John D. Merrifield,Kerry A. Adzima,Male,Male,,Mix,,
40,2,Journal of Economics and Finance,17 December 2014,https://link.springer.com/article/10.1007/s12197-014-9311-5,"Underpricing, partial price adjustments, and equity carve-outs",April 2016,Thomas H. Thompson,,,Male,Unknown,Unknown,Male,"An equity carve-out is a partial IPO of a subsidiary firm. The parent must retain at least a one-third interest in the subsidiary.Footnote 1 Schipper and Smith (1986) note that subsidiaries benefit from carve- outs in many ways. First, the subsidiary can obtain separate financing for its own growth opportunities. Next, the newly traded company reduces uncertainty by disclosure of information. Also, subsidiary managers can provide increased shareholder wealth and their own as well, if awarded shares through stock ownership plans. Equity carve-outs can provide funds that can be used to retire debt, to increase cash for investment, or to cash-out insiders. Also, Schipper and Smith (1986) observe that the carve-out phase of the two- stage combination creates a market for the new subsidiary’s stock prior to a spin-off or other second divestiture and provides more corporate information from additional analyst coverage. In addition, Klein, Rosenfeld, and Beranek (1991) observe that carve-outs allow parents to showcase their subsidiaries to prospective buyers. Carve-outs reflect insiders’ preferences to hold the shares to be divested later and the potential market price increases (Leland and Pyle 1977; and Loughran and Ritter 2002). The low levels of secondary offerings and reported insider selling result in an overhang of future share sales. Underwriters discount IPO prices to ensure that all shares are sold in the offering. This discount is called underpricing. Loughran and Ritter (2004) report that underpricing leads to money left on the table. Also, they note that overhang, the ratio of the subsidiaries’ shares retained by parents to the shares in the offering, influences the amount of money left on the table. With higher percentages of subsidiary ownership retained and the resulting increased levels of overhang, equity carve-outs can result in more underpricing and money left on the table than pure IPOs. Carve-out insiders allow this increased underpricing since only the shares offered are underpriced. The retained shares gain in value. We evaluate four hypotheses related to equity carve-out underpricing: partial price adjustment, prospect theory, managerial discretion, and leaning against the wind. Also, we investigate the extent to which carve-out (partial IPO) underpricing can be predicted based on public information available before the announcement and offer dates, for a sample of 260 carve­ outs and their parents during the period 1990–2012. In addition, we examine impact of our variables on the three-year returns for equity carve-outs and their parents. Although Thompson (2010, 2013) and Ghosh et al. (2012) study carve-outs or their parents, the specific contributions of this study are that we examine results from different perspectives in our hypothesis tests. Unlike the referenced and other carve-out studies, we contrast initial period and long-term returns for carve-outs and their parents.Footnote 2 Also, we examine the change in wealth over the short and long terms. In addition, we examine results during low and high market volatility periods and for increased, decreased, and unchanged initial filing adjustments. Moreover, we show that 29–98 % of the variation in market-adjusted equity carve-out returns can be predicted using public information known prior to the offer date. Furthermore, we show that due to prospect theory, the increase in share values retained by carve-out parents offsets the impact of underpricing. Consistent with Bradley and Jordan (2002) and Thompson (2010) we conclude that many factors available prior to an equity carve-out can predict underpricing. The implications are that models with public information reduce the need for information asymmetry models. We examine several variables to test our hypotheses. Ritter (199 1) introduced the study of filing range adjustments for IPOs. Bradley and Jordan (2002) extended Ritter (1991) by examining changes in filing ranges from the initial filing to the final filing to the offering price. Thompson (2010) applied two factors, filing range adjustments and option market volatility to equity carve- outs. Consistent with Thompson (2010) we observe filing range adjustments are highly significant for carve-out initial returns and that our volatility measure, the S&P 500 option index (VIX), is a highly significant predictor for parent and subsidiary market-adjusted carve-out initial returns. We add a third factor for parents, the ratio of offering size to parent market capitalization, that is significant during the carve-out announcement phase. A fourth factor, the portion of offering proceeds used to retire debt is highly significant for parents during ex-date returns. Figure 1 compares partial price adjustments and percentage underpricing. Generally, the percentage of underpricing varies with the level of partial price adjustment. This figure shows the relationship between partial price adjustment and % underpricing",1
40,2,Journal of Economics and Finance,30 December 2014,https://link.springer.com/article/10.1007/s12197-014-9313-3,An uncertain suggestion for gold-pricing models: the effect of economic policy uncertainty on gold prices,April 2016,Adam T. Jones,William H. Sackley,,Male,Male,Unknown,Male,"The money-like properties of gold have long been recognized – it is a store of value that is typically immune to physical deterioration – and it has been used periodically throughout history as a medium of exchange. Studies have indicated that gold can be an effective, long-term hedge against inflation and most investors consider it to be a safe-haven during times of financial crisis. Its correlation has been measured against numerous variables and indices and considering the Buffett quote above it makes sense that economic policy uncertainty should be one of these comparisons. Perhaps akin to economic uncertainty – whether by perception or reality – a measure of political uncertainty has found its way into gold-pricing models. However, the political uncertainty was a multi-national (ex-US) aggregate focused on top oil-producing countries. With gold being priced in terms of US dollars, intuition suggests that an index of economic rather than political uncertainty, and US rather multi-national (ex-US), might have stronger price-forecasting ability. The purpose of this research is to econometrically test an ex-post, short-term pricing model for gold, with one of the critical refinements being the inclusion of an index measuring economic uncertainty in the United States. After some brief background and a review of the relevant literature in the next section, we present a model, data, and estimation results followed by some concluding remarks.",64
40,2,Journal of Economics and Finance,08 January 2015,https://link.springer.com/article/10.1007/s12197-014-9314-2,An empirical analysis of the Wall Street Journal’s SmartMoney fund screen,April 2016,George Comer,Javier Rodriguez,,Male,,Unknown,Mix,,
40,2,Journal of Economics and Finance,10 January 2015,https://link.springer.com/article/10.1007/s12197-014-9315-1,Freedom and gross in-migration: an empirical study of the post-great recession experience,April 2016,Richard J. Cebula,Maggie Foley,Joshua C. Hall,Male,Female,Male,Mix,,
40,3,Journal of Economics and Finance,22 June 2014,https://link.springer.com/article/10.1007/s12197-014-9290-6,Does investor sentiment impact the returns and volatility of Islamic equities?,July 2016,Daniel Perez-Liston,Daniel Huerta,Sanzid Haq,Male,Male,Unknown,Male,"At the heart of modern financial theory lies the notion that markets are efficient and that security prices act as precise signals for investors, so that they may allocate their resources efficiently (Fama, 1970). Furthermore, the theory operates under the assumption that investors’ demand for financial instruments is rational and driven only by fundamentals. This means that, on average, security prices do not consistently exhibit significant pricing errors. However, this view of finance is now under considerable scrutiny (Leroy and Porter, 1981; Shiller, 1981; De Bondt and Thaler, 1985; Lee et al., 1991; Lee et al., 2002). In general, the aforementioned studies suggest that market irregularities, like the closed-end fund puzzle and the underreaction and overreaction of stock prices, inter alia, provide evidence that the theory of efficient markets might be incomplete. These results also suggest that a significant number of investors seem to misperceive the true distribution of expected returns and as a result their trading leads to observed security prices that are inconsistent with the idea of market efficiency. The impact of investor sentiment on the returns of equities has been empirically tested. Many studies suggest that sentiment does influence asset prices (Lee et al., 1991; Lee et al., 2002; Brown and Cliff, 2005; Baker and Wurgler, 2007; Ho and Hung, 2009; Baker et al., 2012).Footnote 1 These studies find a positive contemporaneous relationship between investor sentiment and stock market returns. Furthermore, the literature also examines how investor sentiment influences stock market volatility (Brown, 1999; Lee et al., 2002). The results of these studies show that investor sentiment and stock market volatility are correlated. The above-mentioned literature shows that investor sentiment plays an important role in the data generating process for equity returns. In spite of these findings, the literature on Shari’ah-compliant securities has yet to examine if investor sentiment significantly influences the returns and volatility of Islamic equities (Hussein and Omran, 2005; Yusof and Majid, 2007; Girard and Hassan, 2008; Kok et al., 2009; Perez-Liston and Soydemir, 2010; Alam and Rajjaque, 2010; Hayat and Kraeussl, 2011; Ferruz et al., 2012). To the best of our knowledge this is the first article to examine the link between investor sentiment and Shari’ah-compliant equities. This study contributes to the literature in the following distinct ways. First, we examine the impact of investor sentiment on Islamic equity excess returns. Second, we study how investor sentiment might influence the conditional volatility of these securities. Specifically, we examine whether sentiment has an asymmetric impact on the formation of conditional volatility. Third, in order to assess whether the impact of investor sentiment varies across different size portfolios, we study these relations for three portfolios formed on market capitalization (i.e., large-, medium-, and small-cap). Fourth, because investor sentiment and Shari’ah-compliant equities might behave as a system we examine the dynamic link between these variables in a vector autoregressive setting. Lastly, more awareness is drawn to one of the fastest growing religious investment strategies. The results from GARCH estimations indicate that a positive change in investor sentiment has a positive impact on the returns of Shari’ah-compliant equities. Similar results hold for three firm-size portfolios (i.e., large-, medium-, and small-cap). In particular, the results show that harder to arbitrage Shari’ah-compliant equities are more susceptible to waves of investor sentiment; that is, investor sentiment has a larger impact on small-cap stocks. Results also suggest that bullish shifts in sentiment are accompanied by lower conditional volatility in the ensuing period. Furthermore, we find that this effect is larger for small-cap stocks. On the other hand, bearish shifts in sentiment do not have a significant impact on the conditional volatility. Finally, VAR model estimations suggest that investor sentiment does influence the return generating process of Shari’ah-compliant stocks. The remainder of this paper is organized as follows: Section 2 describes the literature; Section 3 the data employed in the estimations; Section 4 details the econometric methodology; Section 5 discusses the results; and Section 6 concludes and provides practical implications of our findings.",17
40,3,Journal of Economics and Finance,13 January 2015,https://link.springer.com/article/10.1007/s12197-014-9312-4,The relationship between board characteristics and performance of bank holding companies: before and during the financial crisis,July 2016,Jennifer O’Sullivan,Abdullah Mamun,M. Kabir Hassan,Female,Male,Unknown,Mix,,
40,3,Journal of Economics and Finance,21 January 2015,https://link.springer.com/article/10.1007/s12197-014-9308-0,The regime-switching risk premium in the gold futures market,July 2016,Seth J. Kopchak,,,Male,Unknown,Unknown,Male,"Physical gold pays no cash dividends, incurs costs for security and transport, and its role as a store of value, while rooted in the cultural history of humanity, is arbitrary. Gold has applications in the factor markets, significant demand for the physical commodity exists through jewelry and other retail uses, and gold production makes an important contribution to many national economies. While institutional and other large investors may earn a convenience yield through gold lease arrangements, retail investors holding physical gold for consumption face significant transactions costs transforming ownership of physical gold held, for example, as jewelry or coins, into purchasing power. In contrast, the market liquidity and volume of trading in the gold futures markets is substantial. The Chicago Mercantile Exchange, CME (2013), reports daily transactions with notional value of 20 million oz. of gold, supporting the commonly held understanding that the purely financial role of gold for speculation, hedging, or portfolio diversification is significant. The contributions of Keynes (1930) and Cootner (1960) are synthesized to what (Deaves and Krinsky 1995) define as the risk premium theory; futures prices are less than expected realized spot prices when hedgers are net short and the converse holds when hedgers are net long. Fama and French (1987) decompose the basis, or difference between the futures contract and spot price, to test the ability of the basis to forecast changes in spot prices and risk premiums. Over a sample spanning 1975 to 1984, Fama and French find that gold futures prices do not forecast changes in spot prices or the risk premium. More recently, Brooks et al. (2013) revisit the (Fama and French 1987) study and find insignificant results for gold in both price forecast and risk premium regressions. Expanding the sample, Brooks et al. find significant results only for the risk premium at the 10-month horizon. This article takes an alternate approach to test the risk-premium theory. A bivariate specification of the unbiased expectations hypothesis in a regime-switching framework identifies a time-varying risk premium in the gold futures market. The identified underlying states are persistent, have distinct implications for risk premium and spot price forecasts, and help explain the empirical failure of canonical tests of the risk premium theory in the gold futures market. The main result, the modeling and estimation of a time-varying risk premium, is also examined in the context of work relating gold’s role as a safe haven or hedge. During the “low” state, a large basis, or high futures relative to spot prices, forecasts (large) positive changes in spot prices. Inflation rates during the high price appreciation regime are higher, with statistical and economic significance. During the “high” state, lower spot price appreciation and higher futures market risk premia correspond to periods of high stock market returns. However, the risk-free rate (measured by three-month Treasury bill returns) is also higher during the low price appreciation state and excess returns on the market portfolio are not significantly different between the high and low states. This study focuses on gold futures contracts for several reasons. First, while gold production and demand may be influenced by factors informing the business cycle, annual or seasonal demand and supply fluctuations that influence goods primarily sold for consumption such as coffee, wheat, or orange juice are avoided. Also, the market is large and liquid and both futures and spot prices are available with a uniform and long time series. Finally, a time-varying manifestation of the risk premium in the gold futures market provides additional insight on the traditionally understood role of gold as a financial safe haven and store of value in times of crisis and means for protection against the effects of inflation. Since spot-price forecasts and risk premia in this market are less closely tied to factor demand than in purely consumable commodity markets, changes in price and risk premia regimes provide an avenue to test the relationship of expected price appreciation and inflation.",
40,3,Journal of Economics and Finance,30 January 2015,https://link.springer.com/article/10.1007/s12197-015-9318-6,New insights into mutual fund brokerage commissions,July 2016,Monika K. Rabarison,,,Female,Unknown,Unknown,Female,"Over the years, the commissions paid by mutual fund managers when they buy or sell securities have attracted the media attention and are sometimes described as hidden costs (e.g., Prior 2010; Gao and Livingston 2013). Mutual funds are not required to account for brokerage commissions in the fund prospectus or in the annual report to shareholders. While the U.S. Securities and Exchange Commission (SEC) mandates funds to disclose the commissions in a supplement to the prospectus called the Statement of Additional Information (SAI) and in the N-SAR semi-annual and annual filings from registered investment companies, many investors are not familiar with these documents.Footnote 1
 Brokerage commissions are not factored in the reported expense ratios. However, these commissions are the second largest mutual fund cost as a percentage of total net assets (TNA) after management fees. I find that, over the period 1996 to 2009, the average management fees paid by U.S. equity mutual funds are 0.73 % of TNA, while the average commissions are 0.33 % of TNA, followed by average marketing and distribution fees of 0.26 % of TNA. The Investment Company Institute (2010) reports that the U.S. mutual funds had $11.1 trillion of assets under management at year-end 2009, with 33 % held in U.S. domestic equity funds. I find that the average equity fund paid 0.16 % of TNA in commissions in 2009. Thus, U.S. domestic equity funds paid a total of approximately $5.9 billion in brokerage commissions for fiscal year 2009. Commissions frequently include payments, termed “soft dollars”, for products and services other than execution of transactions. On the one hand, these soft dollar payments may raise agency conflicts between fund managers and shareholders because such costs could have been covered by the ongoing costs already deducted from shareholders’ assets and reported under expense ratios. On the other hand, the use of soft dollars may help in providing better research products and services, thus better returns. Two natural questions arise. First, do high expense ratio funds pay less brokerage commissions? Second, while it is certainly the case that commissions are a cost for a fund, if funds know ex ante that these trading costs hurt performance, why do some funds continue to pay higher brokerage commissions? Most previous studies find that brokerage commissions hurt fund performance and that higher expense ratio funds also pay higher brokerage commissions (e.g., Gao and Livingston 2013). Soft dollar arrangements may be used to pay for overhead or research support that allows managers to reduce expenses charged to their investors. Therefore, a negative relation between expenses and commissions suggests that expenses and commissions may serve as substitutes for one another. On the other hand, a positive relation implies that managers of such funds are either investing in securities that are both difficult to research and difficult to trade or they are simply both charging high fund fees and potentially subsidizing payments for research or services other than portfolio transactions with soft dollars. Thus, such positive relation between expenses and commissions is a puzzle. This study contributes to the existing literature in the following ways by using actual (not estimated) brokerage commissions paid by individual funds. First, I study the determinants of brokerage commissions and analyze the puzzling positive relation between expenses and commissions. Second, I investigate whether and how brokerage commissions improve or impair fund performance by taking into account the levels of investors’ inflows and outflows that may induce fund managers to engage in suboptimal trading. My findings in cross-sectional analyses concur with results from prior studies (e.g., Livingston and O’Neal 1996; Edelen et al. 2007). First, I find that higher portfolio turnover funds are associated with higher commissions, and that larger funds incur lower commissions, as well as the puzzling positive relation between expense ratios and commission ratios (i.e. commissions as percentage of TNA). The latter suggests that commissions and expenses seem to be complements rather than substitutes. By analyzing the issue in a panel regression context, I find that not to be always the case once unobservable fund heterogeneity (i.e. differences due to fund characteristics that are unobservable and cannot be measured such as managers’ skills) is considered. Second, I show that the relation between commissions and performance is more nuanced, as a fund may incur commissions from trades meant to reflect private information (informed trades) or simply incur commissions as the results of flow shocks to the fund (liquidity trades). After controlling for fund inflow (outflow) levels, I find that low inflow (outflow) funds that pay higher commissions outperform high inflow (outflow) funds by 89.7 bps (55.1 bps) per year. In sum, as an alternative explanation to the negative impact of commissions on performance reported by studies that attribute this inverse relation to the use of soft dollars or to poor governance (e.g., Edelen et al. 2012; Gao and Livingston 2013), my results suggest that the underperformance of funds that pay higher brokerage commissions are due (at least in part) to involuntary suboptimal trading induced by investors’ inflows and outflows. The remainder of this study is structured as follows. The next section reviews related studies. Section 3 presents the sample and descriptive statistics. In Section 4, I investigate the relation between expenses and commissions, and explore whether paying high commissions improve or impair performance. Section 5 concludes.",5
40,3,Journal of Economics and Finance,11 March 2015,https://link.springer.com/article/10.1007/s12197-015-9316-8,The analysis of corporate governance policy and corporate financial performance,July 2016,C. Joe Ueng,,,Unknown,Unknown,Unknown,Unknown,,
40,3,Journal of Economics and Finance,24 March 2015,https://link.springer.com/article/10.1007/s12197-015-9319-5,The Buffett critique: volatility and long-dated options,July 2016,Neeraj J. Gupta,Mark Kurt,Reilly White,,Male,,Mix,,
40,3,Journal of Economics and Finance,15 April 2015,https://link.springer.com/article/10.1007/s12197-015-9322-x,Additional evidence of heuristic-based inefficiency in season wins total betting markets: Major League Baseball,July 2016,Bill M. Woodland,Linda M. Woodland,,Male,Female,Unknown,Mix,,
40,3,Journal of Economics and Finance,26 May 2015,https://link.springer.com/article/10.1007/s12197-015-9325-7,The day-of-the-week effect is weak: Evidence from the European real estate sector,July 2016,Georgios Bampinas,Stilianos Fountas,Theodore Panagiotidis,Male,Male,Male,Male,"The existence of seasonality in financial asset returns would defy market efficiency if investors could exploit them by consistently implementing profitable trading strategies. Since the seminal works of Fields (1931) and Osborne (1962) many studies have documented the day-of-the-week effect (or the weekend effect) in financial assets (Cross1973; French1980; Keim and Stambaugh1983; Jaffe and Westerfield1985 among others). The day-of-the-week effect refers to the finding whereby the mean rates of returns are significantly higher on some days of the week.Footnote 1 Indeed, only partial justification of these intraweek patterns has been presented so far. Potential explanations include settlement procedures and measurement errors (Gibbons and Hess 1981), systematic movements between the bid-ask spread (Keim 1989), the distinction between trading and non-trading periods (Fortune 1991; Penman 1987), differences in trading behavior of individual and institutional investors (Lakonishok and Maberly 1990; Sias and Starks 1995) and investors’ speculative short sales (Chen and Singal 2003). The European market for publicly traded real estate companies has come into prominence over the course of the last two decades. According to the European Public Real Estate Association (EPRA), the market capitalization of European real estate companies has increased from around €6.6bn in 1990 to €321bn in the middle of 2011.Footnote 2 Investigating the day-of-the-week effect in the European real estate market is interesting for more than one reasons. First, investing in real estate has become an attractive strategy in Europe, especially after the influx of REITs in beginning of the last decade which resulted in a marked expansion in the listed real estate sector.Footnote 3 This phenomenon is reflected in the increased growth of European securitized real estate market which in November 2013 totaled an aggregate market capitalization of €325bn, accounting for 25 % of the global listed property market. The outperformance of listed real estate in both their REITs and corporate forms, led to increasing investor’s awareness for this segment of the market.Footnote 4 With this increased attention, a further insight into the market anomalies of securitized real estate returns is of crucial importance. Second, whilst a large array of literature examines the calendar effects for stock market indices, there has been less interest on more disaggregated segments of the market. Considering that real estate securities are traded on major stock exchanges it would be insightful to examine whether the anomalous patterns observed in stock markets are also presented in real estate market.Footnote 5 Likewise, recent evidence provided by Kaplanski and Levy (2012) in favour of seasonality in real estate prices raises the question on whether the seasonality that is present in prices is also reflected on real estate indices. Third, despite the large growth of the European real estate companies, most studies thus far have concentrated on the US real estate market (Redman et al.1997; Friday and Higgins2000; Hardin et al.2005 among others). This study builds upon previous research on European real estate including Lenkkeri et al. (2006) which is closer to our approach.Footnote 6
 The first goal of this paper is to examine the existence of the day-of-the-week effect at both aggregate (global and European level) and country specific level, by using an extended dataset. The existence of calendar anomalies implies that investors could develop trading strategies based on seasonal patterns in order to gain abnormal returns.Footnote 7 Some recent studies assert that the day-of-the-week effect for stock returns has disappeared in some countries since early 1990s due to improvements in market efficiency (Kohers et al. 2004; Steeley 2001). As Lakonishok and Smidt (1988) pointed out in their 90-year period study, one must be very skeptical of what is considered an anomaly. To confirm an anomaly, supporting evidence is required in various data sets over different periods of time. The second goal of this paper is to establish the robustness of the day-of-the-week effect using the returns of the European securitized real estate indices, as this anomaly could be the result of data snooping or data mining bias. Sullivan et al. (2001) argue that the practice of using the same data set to formulate and test hypotheses introduces data-mining biases that, if not accounted for, invalidate the assumptions of underlying classical statistical inference. They used 100 years of daily data and a bootstrap procedure to show that although nominal p-values for individual calendar rules are extremely significant, once evaluated in the context of the full universe from which rules were drawn, calendar effects significance faded. Additional critical evidence is provided by Hansen et al. (2005) who claim that this phenomenon has diminished in the late 1980s (with the exception of small-cap stock indices). They inspect the time path of p-values that account for data-mining biases and find significant calendar effects only in specific sub-samples of DJIA returns and standardized returns during the 20th century. Following Hansen et al. (2005), the robustness of our findings is also assessed via sub-sample analysis based on rolling regressions. Recently, Zhang and Jacobsen (2013) provide evidence that monthly seasonality for the UK stock prices strongly depends on the sample period considered. They show that many calendar months significantly alter their performance relative to the market, but few have done it persistently over their 300 years sample period. In that respect, the day-of-the-week effect could also be an apparent but not real phenomenon.Footnote 8
 Our contribution is twofold: (i) we employ a dataset that has not been widely used in testing for calendar effects, given the recent evidence of Kaplanski and Levy (2012) in favour of seasonality in real estate prices and (ii) we attribute the apparent evidence for the day-of-the-week effect to sample dependence by employing rolling regression techniques. We find that six out of twelve European real estate indexes exhibit positive Friday returns. Particularly, the Friday anomaly is present in Finland, France, Netherlands, Spain, Sweden, Switzerland and in the European and global indices. We also find a significant positive Monday effect for Denmark, Finland and Sweden and a Wednesday effect for Sweden and Switzerland. When a rolling regression approach is adopted, very weak evidence (if any) in favour of the day-of-the-week effect is found. The rolling p-values analysis suggests that significant daily seasonality is not an economically important phenomenon in European securitized real estate returns. This result is valid for both the individual European markets and the European and global indexes. It is also robust to alternative distributions of the error term. The weak day-of-the-week effect evidence abides the claim that data mining, noise and selection bias could drive this market anomaly. The rest of the paper is organized as follows: Section 2 summarizes the relevant literature, Section 3 presents data, Section 4 describes the econometric methodology and Section 5 discusses the results. Finally, Section 6 concludes.",10
40,3,Journal of Economics and Finance,16 June 2015,https://link.springer.com/article/10.1007/s12197-015-9326-6,Spillover effects on the sectoral returns for australian and New Zealand equity markets,July 2016,Faruk Balli,Hatice O. Balli,Ronglan Hong,Male,Female,Unknown,Mix,,
40,3,Journal of Economics and Finance,16 June 2015,https://link.springer.com/article/10.1007/s12197-015-9328-4,"Financial competence, overconfidence, and trusting investments: Results from an experiment",July 2016,Bryan C. McCannon,Colleen Tokar Asaad,Mark Wilson,Male,Female,Male,Mix,,
40,3,Journal of Economics and Finance,14 September 2015,https://link.springer.com/article/10.1007/s12197-015-9339-1,Structural break tests and the Greek sovereign debt crisis: revisited,July 2016,Bruce Q. Budd,,,Male,Unknown,Unknown,Male,"The purpose of this research is to ascertain the starting date of the Sovereign Debt Crisis for the Greek economy. The contribution of this paper purports that the contentious issue of the start of this Greek debt crisis was taking place much earlier than reported by previous research. Empirical results from this paper suggest that earlier studies may have underestimated the degree of persistence in the variance of returns, i.e. the property of momentum in the conditional variance, whereby past volatility explains current volatility. Historical data of Government Sovereign 10-year Bond yields provide one of many important signals of assessing and forecasting the financial climate of a country. Government bonds offer an invaluable and essential source of income for public expenditure as well as an instrument for monetary control. They are, by reputation, considered risk free or rather, default free, not inflation free, and are traditionally used as a benchmark for all other subordinate debt securities. It is assumed that each country’s debt market performance corresponds to their country’s risks. Country risk refers to the risk of lending or investing in that country arising from possible changes that may affect operating costs or returns on assets. In recent years many individual capital markets have witnessed spectacular financial volatility, creating systemic risks which in some cases, has led to a breakdown in the financial system. Greece’s near exit from the euro in 2011, and again in 2015 are such examples. These particular Greek experiences were marked by domestic economic disarray requiring stringent austerity measures as stock and bond prices plunged. Research implications of such excess volatilities within sovereign bond markets have crucial implications for regulators, investors and portfolio risk managers alike. Using monthly 10-year Greek government bond returns within a regime switching framework different empirical models of time-varying conditional volatility are implemented. The volatility of sovereign bond index returns in the context of this crisis is researched with particular focus on the changing of parameters at dates within the sample time period using different models. Many studies have modeled volatility by controlling for structural changes for exchange rates (Malik 2003, Rapach and Strauss 2008), real Gross Domestic Product growth (Cecchetti et al. 2006, Fang and Miller 2008, 2009) and stock returns (Cunado et al. 2004, Malik et al. 2005, Hammoudeh and Li 2008, and Wang and Moore 2009). A structural break essentially corresponds to a shock with a lasting effect on the series (Perron and Vogelsang 1992). This paper is the first to examine structural changes in the volatility of sovereign bond index returns in the context of the Greek debt crisis using Markov regime switching model (Hamilton 1989), Dickey Fuller Break test (Lee and Strazicich 2003) and Multiple break structural test (Bai and Perron 2003). All models provide empirically plausible frameworks for examining the volatility of bond returns in an evolving time series behaviour. This research is based on the assumption that bond returns may move across different volatility regimes. Failure to identify presence of structural breaks can lead to sizeable upward biases in the degree of volatility persistence (Hammoudeh and Yuan 2008; Hillebrand 2005; Mikosh and Starica 2004; and Salisu and Fasanya 2013). Using an autoregressive EGARCH model (Glosten et al. 1993; Nelson 1990), and a conventional GARCH model (Bollerslev 1986), dummy variables are incorporated within the coefficients of the mean and variance equations to trace the structural breaks in each equation to capture and validate any potential statistical impact derived from each break test result. The following section provides a description of the data used. Section three explains the model specifications. Section four outlines the structural break tests and their outcomes. Results of the final estimations follow in section five. The final section provides discussion and concluding observations.",1
40,3,Journal of Economics and Finance,10 February 2016,https://link.springer.com/article/10.1007/s12197-016-9354-x,Interest rate movements and US consumers’ inflation forecast errors: is there a link?,July 2016,Hamid Baghestani,,,Male,Unknown,Unknown,Male,"According to the Fisher effect, movements in short-term interest rates largely reflect changes in expected inflation. This implies that such movements contain useful predictive information for future inflation. However, expected inflation is subject to error. As such, we ask whether interest rates move in response to observed over- and under-predictions of inflation. There are many studies in the literature investigating the validity of the Fisher effect with mixed results (Mishkin 1992; Koustas and Serletis 1999; Kaliva 2008). But few have examined questions similar to the one posed above. For instance, De Bondt and Bange (1992) show that past inflation forecast errors have useful predictive information for premia on US government bonds. Research often utilizes survey data to measure expected inflation. One source of such data is the Michigan Surveys of Consumers (MSC). This survey probes consumer sentiment on personal finances, buying attitude, business conditions, and expectations. We focus on the latter category and utilize consumers’ beliefs about future inflation as the measure of expected inflation. Several studies, including Ang et al. (2007), have shown that this measure outperforms univariate time-series and macroeconomic models in out-of-sample forecasting. In addition to the MSC inflation forecasts, we utilize monthly data on the CPI, federal funds, 3-month Treasury, 1-year Treasury, 5-year Treasury, and 10-year Treasury rate.Footnote 1 Our analysis produces a number of important results. First, the MSC inflation forecasts for 1978–2013 were directionally accurate, unbiased, and superior to comparable forecasts derived from an integrated moving-average (MA) model. Second, in-sample regression estimates for 1978–2007 reveal that the interest rates moved downward (upward) in response to MSC over-predictions (under-predictions) of inflation. Third, out-of-sample evidence for 1978–2007 indicates that MSC inflation forecast errors had directional predictability for the interest rates. Fourth, no link between interest rate movements and MSC inflation forecast errors is detected for 2008–2013 when monetary policy kept the short-term interest rates unusually low. Section 2 describes the data along with the forecasts. Section 3 presents the results. Section 4 concludes.",3
40,4,Journal of Economics and Finance,06 February 2015,https://link.springer.com/article/10.1007/s12197-015-9317-7,Ex-dividend day abnormal returns for special dividends,October 2016,Jaideep Chowdhury,Gokhan Sonaer,,Unknown,Unknown,Unknown,Unknown,,
40,4,Journal of Economics and Finance,20 February 2015,https://link.springer.com/article/10.1007/s12197-014-9306-2,Modeling share returns - an empirical study on the Variance Gamma model,October 2016,Andreas W. Rathgeber,Johannes Stadler,Stefan Stöckl,Male,Male,Male,Male,"Distribution assumptions and jump discontinuities play an important role in modeling share returns. Jumps can generate fat tails, and in doing so they can influence the excess skewness and kurtosis. Fama (1965) studies the distribution of daily returns on the Dow Jones Industrial Average Index (DOW JONES) over the period from 1957 to 1962. In his analysis, he includes the stocks of the thirty companies listed on the DOW JONES, rejects the normality assumption and suggests that non-normal distribution assumptions would probably fit more accurately. However, he is not able to find a Paretian stable distribution (at least one finite mean and one finite variance according to Mandelbrot 1963) for his non-normal distribution assumptions. Based on these results Officer (1972) or Hsu et al. (1974) - amongst others - demand better modeling approaches for non-stationary share returns as their attempts to estimate Paretian stable distributions failed. Praetz (1972) was the first trying to improve share return modeling by means of an inverse gamma distribution which implies a rescaled Student t-distribution for returns. His results are superior to Fama’s (1965) and are regarded as the basis for all future stochastic variance models. Since Madan and Seneta (1987) took the results of Praetz (1972) and published the first symmetric version of the Variance Gamma (VG) process with mean zero, there has been some progress in developing the VG process, a Lévy process (other known Lévy processes are for example the hyperbolic or normal inverse gaussian process by Barndorff-Nielsen (1977), (1995) or Eberlein and Keller (1995)), as alternatives to the common Brownian Motion model for stock market returns. There exist two major parts in literature about the VG process. First, the univariate case, in which Madan and Seneta (1990) extend the Black-Scholes model by applying the VG process within the pricing framework to the Variance Gamma option pricing model. Madan et al. (1998) conclude that Variance Gamma option pricing reduces the pricing bias - in contrast to the Black-Scholes model - as the VG process covers the excess kurtosis, which is a result of jumps. Daal and Madan (2005) use this new idea for an empirical examination of the Variance Gamma option pricing model, the traditional Black-Scholes model and Merton’s (1976) jump diffusion model for foreign currency options. They reaffirm Madan and Seneta’s (1990) findings that the Variance Gamma option pricing model performs better than the others do. For further applications of the VG process see for example Leicht and Rathgeber (2014). Furthermore, there exist several variations of the VG process, like the CMGY process by Carr et al. (2002). Second, the multivariate case, which cares about the integration of correlations and therefore of the dependence between the Lévy processes. For an overview see for example Luciano and Schoutens (2006), Luciano and Sameraro (2008), Semeraro (2008) or Luciano et al. (2014). Summed up, the VG process, like the other Lévy processes, offers lots of possibilities in asset pricing, risk modeling by reducing pricing errors or model miscalibrations. They help to incorporate jumps, map a more realistic market behaviour in comparison to traditional models and therefore are important instruments in the field of financial economics. In contrast to the two-parametric Brownian Motion the VG process mentioned above is a four-parametric stochastic process. Therefore, these two methods used for stock market modeling differ widely as the VG process captures the skewness and kurtosis in addition to the mean and standard deviation. The VG process considers both, the symmetric increase in the left and right tail probabilities of the return distribution (kurtosis) and the asymmetry of the left and right tails of the return density (skewness). These properties allow for a more accurate representation of stock returns. The VG process parameters (for the univariate as well as most parts for the multivariate case) can be obtained by the application of several methods, such as the simplified method of moments, the method of moments, the maximum likelihood estimation, the empirical characteristic function, the Bayesian inference and Markov chain Monte Carlo method, or the minimum χ
2 method. For an overview see Madan and Seneta (1987), Madan and Seneta (1990), Seneta (2004) and particularly Finlay and Seneta (2008). However, there has been only little research on some essential issues of the VG process, so far. We have recognized a gap in literature as to the performance of the various estimation methods for modeling empirical share returns. While some papers present only few estimated parameters for a very small, selected empirical database, Finlay and Seneta (2008) compare most of the possible estimation methods using simulated data. In contrast to Finlay and Seneta (2008), we utilize a broad, daily, and empirical data set consisting of the stocks of each of the companies listed on the DOW JONES over the period from 1991 to 2011. Additionally, the calibration quality as well as the parameters’ range of the VG process are dependent on time. This means that parameters vary in different market phases. As market participants are exposed to varying situations, the selection of the correctly fitted model is an essential element of their work. This leads us to apply a regime switching model in order to identify normal and turbulent regimes within our data set and to fit the VG process to the data in the respective period. This approach has two major advantages. First, it results in a more accurate parameter estimation, which avoids over- or underestimation of the real VG process. Second, the fitting rate - the fact that the returns follow a VG process - increases significantly. Thus, the use of the regime switching model adds new knowledge to the framework of the VG process. The remainder of this paper is structured as follows. The next section provides an overview of the theoretical background of the VG process itself, the several estimation methods for the VG process parameters, and our hypotheses. Section 3 introduces the research design including the data set and the methodology of differentiating between normal and turbulent times in financial markets. Section 4 presents the results of the VG process parameter estimation. Subsequent to the discussion of the results in Section 5, Section 6 concludes the paper.",13
40,4,Journal of Economics and Finance,24 February 2015,https://link.springer.com/article/10.1007/s12197-015-9320-z,The impact of government intervention on the stabilization of domestic financial markets and on U.S. banks’ asset composition,October 2016,Peter V. Egly,Diego Escobari,David W. Johnk,Male,Male,Male,Male,"The 2007–2009 financial crisis that spawned from various factors such as the housing boom, aggressive mortgage lending activity, financial innovation through the creation of new funding products, and an increased access to money and capital markets culminated with unprecedented U.S. government intervention in the financial sector. Cecchetti (2009) and others claimed that by the summer of 2007 it was clear banks and other financial institutions stood to lose billions of dollars from their exposure to subprime mortgage loans. In the advent of the housing boom, Watson (2008) mentions that riskier loans to less creditworthy borrowers became common, and thus the market of potential borrowers had expanded beyond traditional bounds using a variety of non-traditional mortgage contracts. The aforementioned author claims the development that had a strong impact on the credit problems during the financial crisis was the funding of uninsured mortgage credit to borrowers whose credit history prevented them from obtaining conventional loans. Many researchers argue the housing finance model was predicated on rising real estate prices. After several years of double-digit increases, fueled in large part by real estate investor purchases rather than homeowner purchases, the market appreciably softened. Factors that contributed to the weakening of the housing market included rising interest rates between 2003 and 2006 and a reduced pool of qualified homeowners. The ensuing bust in the housing market impacted the financial markets since falling house prices contributed to rising mortgage loan delinquencies and an increase in home foreclosures. The growing uncertainty with respect to the value of banks’ balance sheets was at the core of the financial crisis and was captured by sharp increases in money market rates. The rising uncertainty about the value of banks’ balance sheets prompted banks to hoard cash as they became concerned about their continued ability to tap into the capital markets to cover funding requirements. Rising liquidity constraints that initially arose in the interbank markets and perceived increases in counterparty risk eventually led to an overall disruption in the capital markets. With the financial system on the verge of collapse as a result of the spillover effects from the housing bust, and given the threat of a sharp contraction in credit and bank lending, it was clear government intervention would soon emerge. Due to the weak response to the Fed’s monetary policy actions that involved reductions in target and primary lending rates, the Fed experimented with innovative short-term bank lending programs designed to inject liquidity into the financial system. Other important government-led efforts included debt and deposit guarantees, large scale asset purchases, and direct assistance through the U.S. Treasury’s Troubled Asset Relief (TARP) and Capital Purchase (CPP) programs. The purpose of these programs was to stabilize the ailing financial system and to restore investor confidence. An interesting area of research that has received some attention is the U.S. government’s intervention in response to the 2007–2009 financial crisis. Some researchers focus on the effectiveness (or lack thereof) of the Fed’s short term lending programs (e.g., Taylor and Williams (2009) and Cecchetti (2009)), while others examine the impact of the government bailout programs on bank lending (e.g., Lei (2013), Ivashina and Scharfstein (2010) and Egly and Mollick (2013)). Other researchers explore the impact of the government intervention on the stock market and overall investor confidence (e.g., Subrahmanyam et al. (2011) and Huerta et al. (2011)). To gain an overall economic perspective of the government intervention, Veronesi and Zingales (2010) calculate the costs (i.e., cost to tax-payers) and benefits (i.e., increased value of banks’ financial claims) of the government bailout and determine it was an overall success. These authors contend that from an economic viewpoint the government intervention created value by preventing a run on banks and by providing capital that reduced banks’ inefficiencies related to excessive leverage, which in turn impacted the banks’ ability to exploit future investment opportunities. With the exception of the work of Ivashina and Scharfstein (2010), none of the referenced papers focus on the impact of the government intervention on the balance sheet composition of federally insured commercial banks. Moreover, Ivashina and Scharfstein (2010) examine bank lending during the crisis with a focus on loan level data on syndicated transactions. These authors conclude the stress placed on liquidity from drawdowns under existing credit commitments and the run by short-term bank creditors led to an overall reduction in bank lending. This paper expands the literature by investigating whether changes in commercial banks’ balance sheet composition are associated with the government intervention whose objective was to stabilize the financial system. This study investigates the following research questions: What was the impact of the government intervention on bank liquidity? Has the proportion of residential real estate loans with respect to the total bank loan portfolios reverted to pre-crisis levels? Have new differences emerged in balance sheet composition between large banks and small banks as a result of the government intervention? Examining the liquidity behavior of banks is interesting given the recent environment of expansionary U.S. monetary policy. The Fed’s unprecedented expansion of its balance sheet as a result of multiple rounds of quantitative easing (QE) beginning in late 2008 has led to massive increases in liquidity in the financial system that to a large extent is held by banks. The U.S. Government intervention offers a unique opportunity to explore changes in bank behavior that are manifested through their balance sheets. Researchers including Cornett et al. (2011) suggest that in times of crisis, banks tend to build up liquidity reserves as part of an overall strategy to manage liquidity risk. Liquidity risk in turn stems from multiple channels including exposure to unfunded loan commitments, withdrawal of wholesale deposits, or the loss of other sources of short term financing (i.e., debt rollover risk). This research is of interest to economic policy makers, regulators, bank managers, and the investor public. As noted by He et al. (2010), there has been a massive restructuring of the financial sector balance sheets since late 2007. These authors claim between 2007Q4 and 2009Q1 commercial bank holdings of securitized assets increased by approximately U.S. $550 billion while holdings of these asset types by hedge funds and broker/dealers decreased by roughly U.S. $800 billion. On the liability side of the balance sheet, the repurchase agreement (REPO) finance market declines by roughly U.S. $1.5 trillion whereas government backed debt issued by the banking sector, including FDIC insured deposits and FDIC guaranteed bonds, increases by approximately U.S. $1.3 trillion. These authors claim the balance sheet restructuring is driven by weakening financing conditions in debt and equity markets during the crisis as well as the loss of liquidity in the secondary markets for many asset classes. This paper contributes to the literature in several distinct ways. First, we examine bank behavior capturing periods of economic expansion, financial crisis, and post-crisis. The work by He et al. (2010) that uses quarterly data spanning from 2007Q4 to 2009Q1 serves to motivate our research. Yet our paper differs from their work in several ways. Our paper focuses on the behavior of commercial banks while their work covers a broader base of the financial sector that includes hedge funds and broker/dealers, insurance companies, commercial banks, and the government. A deeper understanding of commercial bank behavior is driven by their role in stimulating business and economic activity. He et al. (2010) investigate balance sheet adjustments with respect to asset and mortgage-backed securities attempting to track how these assets have shifted across the institutions. Our paper also examines changes in balance sheet composition of commercial banks from a wider view examining various asset categories that exhibit varying degrees of liquidity. Second, the liquidity models presented in our paper that control for the Fed’s expansion of its balance sheet through quantitative easing incorporate a macro perspective view on bank liquidity building. Notably QE had a large impact which began in late 2008 and subsequently led to massive increases in liquidity in the financial system that ended up on bank balance sheets. Finally, this paper explores whether banks have stabilized to pre-crisis levels and if there has been any permanent changes in banks’ asset composition in the post-crisis period resulting from the impact of government intervention. Dynamic panel data methods are applied to examine commercial bank liquidity behavior over a sample period 2005Q1 to 2010Q4. Our estimation approach allows agents to behave dynamically and form expectations, it controls for unobserved bank-specific characteristics, and controls for potentially endogenous regressors. We sample banks following a four-size classification scheme based on asset size that yields 4982 small banks, 475 medium banks, 34 large banks, and 14 money center banks. Findings show the initial round of quantitative easing positively impacts bank liquidity across all bank samples and a positive impact of the repurchase agreement market rates on bank liquidity for small and medium banks. Our descriptive analysis shows banks have become more liquid in the post-crisis period, especially larger banks (large and money center banks). We also show real estate loan portfolio exposures have reverted to pre-crisis levels for money center banks and remained flat for all other bank samples. The balance of the paper is organized as follows: Section 2 describes the data and sources and presents descriptive statistics. Section 3 introduces the methodology and empirical model, while Section 4 puts forth our hypotheses. Section 5 discusses the results, and in Section 6 we present our conclusion.",5
40,4,Journal of Economics and Finance,04 March 2015,https://link.springer.com/article/10.1007/s12197-015-9321-y,Wavelet decomposition of heterogeneous investment horizon,October 2016,W. D. Chen,H. C. Li,,Unknown,Unknown,Unknown,Unknown,,
40,4,Journal of Economics and Finance,21 May 2015,https://link.springer.com/article/10.1007/s12197-015-9323-9,NCAA basketball: when does recruiting talent translate into wins for power conferences?,October 2016,Julianne Treme,Robert T. Burrus,,Female,Male,Unknown,Mix,,
40,4,Journal of Economics and Finance,28 June 2015,https://link.springer.com/article/10.1007/s12197-015-9327-5,Interest rate dynamics and volatility transmission in the European short term interest rate market,October 2016,Frances Shaw,Finbarr Murphy,Fergal O’Brien,Female,Male,Male,Mix,,
40,4,Journal of Economics and Finance,30 June 2015,https://link.springer.com/article/10.1007/s12197-015-9331-9,Financial development and economic growth: an empirical evidence from the GCC countries using static and dynamic panel data,October 2016,Naeem Muhammad,Abu Reza Mohammad Islam,Hazem A. Marashdeh,Male,Male,Male,Male,"The fundamental relationship between financial sector and economic growth runs as follows. Financial sector mobilizes & pools savings, and eventually allocates them to various capital needs. It makes necessary information available prior to prospective investments. It monitors investments and exerts corporate governance. It facilitates trade & commerce, diversification and management risk, and finally, it does ease the exchange of goods and services through various financial instruments. All these measures aim to promote efficient capital allocation for productivity improvement (Levine 1997). Financial sector is a vehicle to diversify and share risks to induce capital allocation towards high risky projects with high expected returns that results in productivity improvement and economic growth (Greenwood and Jovanovic 1990). The impact of the above functions on economic growth depends on the level, composition and efficiency of the financial sector. Given the above impetus of the relationship between FSD and economic growth, this paper addresses this relationship for the GCC countries. Since the groundbreaking work of King and Levine (1993a) on finance-growth nexus, a large number of studies examined this relationship applying an array of econometric techniques. Except few studies of several variants on the Middle East and North African (MENA) countries, hardly any systematic studies on the finance-growth nexus of the Gulf cooperation Council (GCC) countries is available in the literature using the most appropriate estimation techniques that address the endogeneity issue; some studies are sporadic and country specific (available only for Saudi Arabia and UAE). Since all these studies are limited in terms of data coverage and econometric techniques used, the findings are inconsistent either with positive results or negative results or no results at all. For instance, Marashdeh and Al-Malkawi (2014) using ARDL approach for Saudi Arabia found positive impact of FSD on growth, and similarly with Ibrahim (2013) using a fully modified ordinary least squares (FMOLS) method for Saudi Arabia. The studies that found no influence of FSD on economic growth is Grassa and Gazdar (2014) and Omran and Bolbol (2003). The former used OLS, panel data and GLS techniques for GCC countries (except Oman) and the latter used OLS for Arab countries including the GCC countries to get their results. Malkawi et al. (2012) using an ARDL approach found a negative effect of the financial sector on the economic growth of UAE. The GCC countries are oil dependent. They commonly share high dependency on hydrocarbons (as share of oil and gas revenues to total fiscal and export revenues) and its share to GDP. Also they share common structural policy changes with reduced dependency on hydrocarbon sector to diversify their economy ensuring private non-oil sector development, and all other services sectors to create employment opportunities for the GCC nationals. They enjoy a favorable platform with necessary financial means to implement necessary economic reform and structural changes to assist the private sector development and economic diversification. Its financial sector is largely bank based, and instruments are mostly short term maturities. Banks in the GCC countries are well capitalized and profitable. Financial markets are underdeveloped except few significant developments of stock markets, but not in all the GCC countries. Companies in the GCC countries rely on bank financing irrespective of it being either bilateral or syndicated. The financial sector, especially the GCC credit growth increased manifold; personal loans for consumption increased dynamically; retail banking developed throughout the GCC region due to favorable demographics, making revenue diversification of the financial institutions in the GCC region (Sturn et al. 2008). Also there is a loan expansion throughout the Gulf region along with better and adequate liquidity in GCC banking measured by M2. High energy prices and increased hydrocarbons production are feeding through the non-oil sector with higher liquidity. Thus, higher growth in GCC money supply enables the private sector to expand economic activities (Stubing 2014). Bank lending was driven by infrastructure development & manufacturing. Given the above financial developments, it is legitimate to investigate their impact on economic growth in the GCC region. Other motivations relate to the inconsistent and inconclusive results shown above on the finance-growth nexus of the GCC countries. Our first objective is to examine whether or not FSD of the GCC countries contributes to their economic growth. The second objective is to explore the role of foreign direct investment (FDI) and the interaction between FSD and FDI while examining the finance-growth nexus of the GCC countries. The second objective relates to an ambiguous link found in the literature between FDI and economic growth across countries. One group of studies focused on the exogenous effect of FDI on economic growth without considering the importance of FSD, while other group stressed on the importance of FSD on economic growth without taking account of FDI, and yet another group examined the role of FSD and FDI simultaneously considering FSD instrumental to assist FDI to impact positively on economic growth. Further, with few exceptions, majority of the studies did not consider the interactions between FDI and financial development while examining the finance-growth nexus (Lee and Chang 2009). To achieve the second objective, we treat the FDI and the interaction variables as control variables to provide evidence if any, whether or not FDI and the interaction term of it with FSD contributes to the economic growth of the GCC countries. Our study focuses on a longer data period: 1975 to 2012 stressing the point that most financial sector development took place in the last 15 to 20 years. Unlike any previous studies in the GCC countries, four estimation techniques (Pooled OLS, fixed effect estimation, random effect estimation, and the system GMM estimation approaches) are used to estimate the empirical model in order to see whether our results on finance-growth nexus are robust. The study results indicate that FSD contributed positively and significantly to the economic growth of the GCC region. This signifies that FSD reforms and supervision in the GCC countries during the study period was a strong catalyst to promoting their economic growth. FDI also contributed to the economic growth of the GCC countries. Fixed capital formation and oil production were important contributing factors to the economic growth of this region. This paper proceeds as follows. In section 2 we present a brief overview of the relevant literature on the finance-growth relationship in general, historically and the GCC countries in particular. Then the paper discusses the empirical model in section 3 inclusive of the FSD indicators and controlled variables employed in the study followed by the methodology of this study. The nature of data and their sources, and results of the econometric works are explained in section 4. Finally, the conclusions of the study are presented in section 5.",27
40,4,Journal of Economics and Finance,28 July 2015,https://link.springer.com/article/10.1007/s12197-015-9333-7,"Give me sanctuary! The impact of personal freedom afforded by sanctuary cities on the 2010 undocumented immigrant settlement pattern within the U.S., 2SLS estimates",October 2016,Richard J. Cebula,,,Male,Unknown,Unknown,Male,"The issue of undocumented immigrants in the U.S. has received increasing attention in the literature in recent years. The range of issues covered in the more recent literature include public versus private school enrollment, tuition concessions at public colleges and universities, the increased inflow of undocumented immigrants to the U.S., identity theft, and settlement patterns within the U.S. (Gradstein and Justman 2000, 2002; Betts and Fairlie 2003; Hanson 2006; Unemori 2007; Koch 2008; Kaushal 2008; Mavisakalyan 2011; Cebula et al. 2013; Gerdes 2013; Cebula et al. 2014; Nair-Reichert and Cebula 2015; Nair-Reichert 2015; Cebula et al. 2015, Chapter 11). The present study seeks to add to this literature by identifying key factors that have influenced the settlement pattern of undocumented immigrants in the U.S. The primary focus of this study is on “Sanctuary cities” and the personal freedom they provide for undocumented immigrants and the resulting influence that those Sanctuary cities exercise on the settlement pattern of undocumented immigrants across the 50 states; this is a topic that has been surprisingly overlooked in the scholarly literature. This study seeks to extend a small number of earlier studies dealing principally with 2005 settlement patterns of this cohort (Cebula et al. 2013, 2014; Nair-Reichert and Cebula 2015; Nair-Reichert 2015). In this study, we use state-level data (Cebula et al. 2013, 2014; Nair-Reichert and Cebula 2015; Nair-Reichert 2015), but with the focus on settlement patterns of undocumented immigrants in 2010, a year that has been largely been overlooked in the scholarly research literature. “Research on undocumented immigration in the U.S.” section of the study provides a brief literature review. Following that, “The framework and data” section develops a basic model of well-being maximization and introduces the variables in the analysis. “2SLS findings: the linear model” and “2SLS findings for the semi-log specification” sections provide the empirical results, which are obtained from 2SLS estimations intended to reflect the finding of the Pew Hispanic Institute (2013, p. 2) that the mobility of the undocumented immigrant population in the U.S. is quite sensitive to changing conditions regarding earnings. “Conclusion” section provides concluding observations.",2
40,4,Journal of Economics and Finance,22 September 2015,https://link.springer.com/article/10.1007/s12197-015-9340-8,Revisiting the growth-governance relationship in developing Asian and Oceanic economies,October 2016,Bichaka Fayissa,Fahad Gill,,Unknown,Male,Unknown,Male,"Over the last three decades, many Asian countries, particularly East and Southeast Asian countries have experienced significant rate of economic growth in the order of more than 7 % per year. Many researchers have attributed to such fast paced economic progress to the relative stability of their fiscal policy (well managed deficit), the openness of their economies, and the active, but in-excessive role of government in the economic activities of the Asian corridor. With respect to the role of the institutional quality, or “good governance” as a catalyst of the rapid rate of growth experienced in Asia, the findings of recent studies are mixed at best.Footnote 1 In particular, for developing Asian economies, there appears to be a consensus among researchers on the existence of a negative relationship between growth and governance (Quibria 2014). We revisit the role of governance on economic growth in developing Asian and Oceanic economies.Footnote 2 Our analysis challenges the existence of “growth-governance paradox”,Footnote 3 by using more comprehensive data and econometric analysis. An understanding of the relationship between good-governance and economic growth is vital given that many governments, international financial institutions, and aid agencies have focused on good-governance as a key aid allocation criterion (Radelet 2002; Hoebink 2006). The International Monetary Fund and the World Bank now set conditions, which often include governance improvement and economic liberalization, prior to committing to any financial assistance (Kapur 2001). This exclusive focus on governance may, however, be misguided if the data show a negative relationship between growth and governance, at least for developing Asian countries. In line with the majority of governance-growth literature, we use six governance indicators formulated by Kaufmann et al. (2013) who use a number of sources including surveys by private and government agencies to construct them. These indicators, referred to as World Governance Indicators (WGIs), are voice and accountability, government effectiveness, political stability and absence of violence, regulatory quality, rule of law, and control of corruption. We notice that previous studies have not controlled for unobservable characteristics of developing countries of Asia and Oceania that are likely to be correlated with governance and growth. These characteristics may include informal systems that may circumvent inefficient institutions to get business done. North (1990) highlights the importance of the function of informal institutions on the growth performance of some countries. Knowles (2005) discusses the link between social capital and economic performance of a country while noting that informal institution and social capital are similar concepts. Zhuang et al. (2010) argue that these concepts can explain the cross country growth performance of different countries. The unobservable nature of these concepts, however, does not allow researchers to include them in their empirical analyses. At the same time, an empirical model that does not control for the unobservable systems will result in biased estimates of the coefficients of governance because of the correlation of these unobservable systems with growth and governance. Using panel data that spans over 18 years and includes 37 countries, we employ econometric techniques that address the endogeneity bias resulting from omitted variables and reverse causality. Our results show that governance exhibits a positive relationship with growth once different sources of endogeneity are addressed. Hence, this paper extends the literature by using richer data and more sophisticated empirical methodologies to find results that are different from those of previous studies, but are consistent with the conventional intuition on the relationship between growth and governance. The paper is structured as follows. Section 2 presents the literature review and also highlights the contributions of this article to the literature. Section 3 presents the data and discusses the empirical strategy. Section 4 presents the empirical results. Section 5 draws conclusions based on the empirical results.",2
40,4,Journal of Economics and Finance,02 October 2015,https://link.springer.com/article/10.1007/s12197-015-9343-5,Do travel visa requirements impede tourist travel?,October 2016,Robert A. Lawson,Saurav Roychoudhury,,Male,Unknown,Unknown,Male,"Robert Molesworth’s remarks about 17th Century travel restrictions in Denmark, and the government’s motivations for them, highlight that travel rules have been with us for centuries. Notwithstanding that brief period before 1914 when a person could travel “without passport or other formality” (Keynes 1920: 11), governments have long sought and continue to seek to regulate the mobility of people. In current times, take the hypothetical situation of an upper-middle class family considering traveling to the United States to visit Walt Disney World in Florida and maybe catch a show in New York City. Upon reading the U.S. Embassy’s instructions (see below) for obtaining travel visas for the family, they decide to go to DisneyLand Paris and see the Louvre instead. After all, France doesn’t require travel visas for Brazilian visitors. This paper examines whether modern tourist travelers are in fact deterred by travel visa requirements and if so by how much. Using a travel visa data set developed by Lawson and Lemke (2012) and travel flow data from the World Bank and the UN’s World Tourism Organization (UNWTO), we investigate the deterrent effect of travel visa requirements on travel flows. 
United States Diplomatic Mission to Brazil
 
Temporary (Nonimmigrant) Visas
 
How to Apply
 
All visa applicants should schedule two appointments—one at the Applicant Service Center (ASC) and another at the Consulate or Embassy—following the steps below.
 
Note that for security reasons applicants are not allowed to enter the Applicant Service Centers (ASC) or embassy/consulates with bags (other than one small purse) or electronic devices. Applicants may enter the ASC with their cell phones as long as they are turned off.
 
Step 1: Complete DS-160 online application form.
 
Step 2: Go to the appointment website:
http://brazil.usvisa-info.com
or call the Call Center at one of the local numbers. Register online using your DS-160 confirmation code. Read the interview waiver and renewal pages to determine if a visa interview is required in your case. Pay the visa application fee (MRV fee). This can be done by credit card through our appointment website or by telephone. Applicants may also pay through a bank boleto at any bank in the boleto network. Schedule your appointment at the ASC and your visa interview at the Consulate or Embassy. Appointments can be made online, through the Call Center or through Skype.
 
Step 3: Attend your CASV appointment to have your photo and fingerprints collected. Applicants will need to present their current passport, old passports with previous visas and DS-160 confirmation page. Applicants age 66 and over and minors 15 years old and younger do not require fingerprint collection at the CASV so they, or a representative, should deliver a 5x7in photograph, less than 6 months old and with a white background. Click here for more information on State Department Photo Standards.
 
Step 4: Attend your interview appointment, if required, at our Embassy or Consulates. You will need to bring the following: Current passport. Any old passports with previous visas. DS-160 bar code confirmation page. Any additional documents the applicant feels are necessary for the visa interview
 
                            Step 5: Most applicants will receive their visas within 10 businesses days. However some applications may require additional processing time. We recommend that you do not finalize travel plans until you have received your visa.
                            Footnote 1
 In recent years, travel visas and customs procedures have garnered increasing popular and political attention (CNN 2013; The Economist 2011; UNWTO 2013). Neumayer (2006) and Luedtke et al. (2010) created and analyzed travel visa datasets similar to Lawson and Lemke’s (2012). However, despite the ubiquity of travel visas in our world, almost all of the extant work on this topic has focused on temporary worker, immigrant, or student visas. Tourist-class visas have gotten little scholarly attention. Nieman and Swagel (2009) argued that post-9/11 visa policy changes were not responsible for declines in travel to the U.S. Hu (2013) found significant benefits associated with the U.S. Visa Waiver Program. Neumayer (2011) found substantial negative effects of travel visas on trade and foreign direct investment volume between countries. On the topic at hand, Neumayer (2010) using a different travel visa dataset, alternate specifications, and different time period, estimated the impact of travel visas on travel flows and found that “visa restrictions reduce such travel by on average between 52 and 63 %”. Belenkiy (2014) finds large increases in travel flows to the United States among those countries recently added to the Visa Waiver Program. There is a large literature on the determinants of tourism demand (Song and Li 2008; Witt and Witt 1995; and Crouch 1995). At the theoretical level this literature identifies several traditional economic factors that should influence tourism demand: income, own price, destination costs, and other qualitative effects. Obviously, the income of the traveler is an important factor. At the country level, we should expect more tourist travelers from higher income nations. The price of the travel, measured in dollars or time is another commonly cited influence: the higher the price of travel, the fewer travelers. While a few studies use explicit airline ticket prices, most use distance or a related variable to proxy the price of the travel. The cost of the destination is often covered using exchange rate, price level, or income levels of the countries to which the tourist is traveling. Finally, idiosyncratic or qualitative controls for demand shocks (wars, natural disasters) or political influences may be included. This study’s emphasis on the impact of travel visa requirements falls into this latter category. At the end of the day, empirical studies of tourism demand must make use of various proxies that hopefully capture the desired theoretical concepts.",16
40,4,Journal of Economics and Finance,27 October 2015,https://link.springer.com/article/10.1007/s12197-015-9345-3,Institutional convergence: exit or voice?,October 2016,Joshua C. Hall,,,Male,Unknown,Unknown,Male,"While the study of the institutions of economic freedom has a long history in economics going back at least to Adam Smith, the creation of the Economic Freedom of the World (EFW) index by Gwartney et al. (1996) has led a large number of economists to study the effect of economic freedom on social, political, and economic outcomes.Footnote 1 Hall and Lawson (2014) provide an accounting of a subset of the literature using the EFW index. Focusing only on journals listed in the Social Science Citation Index, they find 402 articles citing the EFW index since 1996. Of those 402 articles, 198 are empirical papers using the EFW index as an explanatory variable. After reading and categorizing all the articles, Hall and Lawson (2014) find that fewer than 4 % of the articles surveyed found economic freedom to be associated with a normatively “bad” outcome such as income inequality or obesity.Footnote 2
 A large number of papers in the literature using the EFW index focus on the relationship between economic freedom and growth. This literature almost uniformly shows that an institutional environment more consistent with economic freedom is conducive to long-term growth. An early paper by Gwartney et al. (1999) finds that a one unit change in the EFW index during the 1975–1985 period was associated with a 0.8 percentage point increase in a country’s long term growth rate. Looking at the 1990s and controlling for the effect that economic freedom has on the productivity of investment, Gwartney et al. (2006) find that a one unit change in the EFW index is associated with a 1.5%age point increase in growth. A critical survey of the literature by De Haan et al. (2006) finds strong evidence that increases in the EFW index stimulates growth. A 2011 meta-regression analysis by Efendic et al. (2011) finds similar results, albeit with some suggestions for improvement within for scholars working in the area. Other important recent works include Williamson and Mathers (2011); Rode and Coll (2012), and Young and Sheehan (2014). If the institutions of economic freedom are associated with economic growth, then the next logical question is what causes countries to turn towards or away from market oriented institutions? What exactly are the determinants of economic freedom? While many have been discussed in the literature, a prominent determinant in the literature has been political freedom, or democratic institutions. An early paper in this literature is Dawson (1998), who finds that the level of economic freedom in 1990 is related to political freedom in 1975. Wu and Davis (1999), however, finds no effect of political freedom on economic freedom. More recent work by De Haan and Sturm (2003); Pitlik and Wirth (2003); Lundström (2005), and Rode and Coll (2012) find that political freedom or democratic transitions lead to economic freedom. This finding is supported by papers by Dawson (2003); Vega-Gordillo and Alvarez-Arce (2003), and Aixalá and Fabro (2009) who present evidence that political freedom Granger-causes economic freedom. Sobel and Coyne (2011) find that political and economic institutions are cointegrated and therefore move together through time within a country.Footnote 3 The case for political freedom has also been backed by within-country case studies, such as the work of Beaulier and Subrick (2006) on Botswana. Other long-run determinants of the institutions of economic freedom include the historical origins of a country’s laws (La Porta et al. 1999, 2008). Legal origin theory is based on the idea that the British common law created more effective constraints on the power of the executive than did French civil law or Scandinavian or Socialist legal origins (Du 2010).Footnote 4 Similarly, countries that are fractionalized in terms of ethnicity, religion, or language might have lower levels of economic freedom as it is harder for diverse individuals to agree on publicly-provided goods (Alesina et al. 2003). In a paper looking at the long-run determinants of growth, Easterly et al. (2006) find that ethnolinguistic fractionalization affects institutional quality, which in turn influences growth. An important addition to this literature is the work of Brown (2014). Building off of the work of Diamond (1997) who briefly notes that the shape of Europe versus China could have influenced their institutional development. Europe, unlike China, is geographically indented and has more peninsulas and islands, which created natural barriers to population centralization and control. Brown (2014) links this literature to institutional competition, which requires the ability of citizens to “vote with their feet” (Tiebout 1956). The closer an individual is in a country to a border, the closer they are to an alternative institutional environment. Brown (2014) creates a variable called ‘exitability” to capture this concept.Footnote 5 Defined as the sum of land borders and coastline divided by total geographic area, his measure of the ‘exitability’ of a country is a long-run determinant of economic freedom in his empirical work.Footnote 6
 This paper adds to the literature on the determinants of economic freedom by seeing how specific determinants contributed to institutional convergence from 1980 to 2010. This is an important question for four reasons. First, the question of income convergence across countries has interested economists since the seminal papers by Barro and Sala-i-Martin (1992) and Sala-i-Martin (1996). Second, Knack and Keefer (1995); Knack (1996), and Keefer and Knack (1997) show that convergence depends on the quality of institutions. By better understanding the process of convergence in economic freedom across countries, scholars can better understand the causes of continued income differences across countries. Third, while many studies have documented how various determinants such as political freedom have influenced economic freedom, no studies have quantified how these determinants contribute to the speed at which countries with poor institutional quality catch up to those with good institutional quality. Finally, this paper contributes to the literature on exit versus voice in determining the quality of institutions (Hirschman 1978).Footnote 7
 The paper proceeds as follows. In Section 2, the data and empirical approach is described. Section 3 presents the empirical results and Section 4 concludes.",30
41,1,Journal of Economics and Finance,03 July 2015,https://link.springer.com/article/10.1007/s12197-015-9330-x,Exploring stock recommenders’ behavior and recommendation receivers’ sophistication,January 2017,Chih-Hsiang Chang,,,Unknown,Unknown,Unknown,Unknown,,
41,1,Journal of Economics and Finance,26 July 2015,https://link.springer.com/article/10.1007/s12197-015-9329-3,Capital investment and non-constant marginal cost of capital,January 2017,Robert Stretcher,Mary Funck,Steve Johnson,Male,,Male,Mix,,
41,1,Journal of Economics and Finance,11 August 2015,https://link.springer.com/article/10.1007/s12197-015-9332-8,Institutional investors’ activism and credit ratings,January 2017,Javeria Farooqi,Surendranath Jory,Thanh Ngo,Unknown,Unknown,,Mix,,
41,1,Journal of Economics and Finance,20 August 2015,https://link.springer.com/article/10.1007/s12197-015-9334-6,Index trading and portfolio risk,January 2017,Joakim Kvamvold,Snorre Lindset,,Male,Male,Unknown,Male,"In this paper, we study trading in index-linked assets and the variance of portfolio returns. Based on data from the Norwegian stock market, we find that return variances are correlated with trading volume in exchange traded funds (ETFs). We do not find that flows to index-linked mutual funds are correlated with return variances. The Norwegian stock market is a small, yet mature market. The market has many of the same characteristics as larger and more important stock markets when it comes to return distributions and risk premiums (see e.g., Che et al. (2009) for a comparison of the Norwegian stock market and the US stock market). There are two main stock indices in Norway; the OBX index and the OSEBX index (both indices are described in detail in Appendix A). The OSEBX is a broader index than the OBX index. The market and its indices can be illustrated with the three sets in Fig. 1. The OBX index contains the stocks in the set A. The OSEBX index contains the stocks in the set D=A∪B. If E is the set of all stocks listed on the Oslo Stock Exchange, the set C=E∖D is the set of all stocks that are excluded from the indices OBX and OSEBX. For our analysis, it will be important to isolate the returns on the stocks in the three sets A, B, and C.
 Illustration of Oslo Stock Exchange. The stock exchange consists of all stocks in sets A, B, and C. The stocks in set A are the constituents of the OBX index. \(A \cup B\) is the set containing the constituents of the broader index OSEBX and C is the set of stocks excluded from both indices The main advantage of analyzing a small stock market is illustrated in Fig. 1. The figure shows how uncluttered the Norwegian stock market is. Considering the US stock market, there is a wide range of different indices that are tracked by index funds and ETFs. Many of the indices overlap, and funds tracking one index will also partially track other indices that have many of the same constituents. Thus, for a large market with many indices and many index funds, it is far more difficult to isolate the effects of index trading than for a small and uncluttered market like the one we analyze in this paper. Although ETFs written on sectors do exist on the Oslo Stock Exchange, trading volumes are zero or close to zero for all trading days in our sample. There is extensive literature documenting that as stocks are included in an index, they receive an index price premium. It has also been documented that inclusion also affects return comovement with returns on other constituents of that index. These effects are present for both the S&P 500 index (Barberis et al. 2005; Wurgler 2011; Goetzmann and Massa 2003; Morck and Yang 2001) and the Nikkei 225 index (Greenwood and Sosner 2007). Barberis et al. (2005) attribute the return-comovement effect to the fact that stocks that are included in the index enter a new “habitat” used by many investors as a benchmark. Morck and Yang (2001) argue further that this effect grows with the growth of indexing (more indices covering the same stocks). Barberis and Shleifer (2003) argue that many investors allocate funds to categories such as growth stocks and investment grade bonds, not to individual securities. They show that such “style investing” can lead to increased price comovement between the assets within a category. ETFs and index funds work in the same way as categories. Bai et al. (2012) and Trainor (2010) analyze the effect from ETFs’ rebalancing trades on returns and volatility. While Bai et al. (2012) find that these trades move prices and increase volatility, Trainor (2010) finds similar effects to be spurious. Sullivan and Xiong (2012) find it likely that the increases in pairwise return correlations for index constituents are related to the increased popularity of index-linked mutual funds and ETFs. They conclude that this increase will increase systematic risk on an investor’s portfolio; hence, reducing diversification possibilities. Da and Shive (2013) find evidence that ETF activity affects return comovement. The effect is stronger for small and illiquid stocks. The effect is also stronger during periods of market turbulence. Ben-David et al. (2014) analyze the effect from ETF ownership on the volatility of individual stocks. They document that stocks owned by ETFs exhibit significantly higher intraday and daily volatility. One problem in the empirical part of this field of research is that it is difficult to identify flows into the markets made by index-linked portfolios. Here is another advantage of analyzing a small stock market: We have exclusive data on all domestic mutual funds with Norway as the primary investment region and can identify those who are linked to indices. We also have data on all trades in ETFs that are linked to the OBX index. This information enables us to separate the effects from trades in index-linked mutual funds and from trades in ETFs. Our research question is related to the above literature. We study the correlation between index-linked trading (i.e., ETF trading and fund flows) and return variances. We also look at the relationship between index-trading and squared returns. Finally, we include some tests to identify any causality between trading and variance.",1
41,1,Journal of Economics and Finance,22 September 2015,https://link.springer.com/article/10.1007/s12197-015-9338-2,Mortgage loan securitization and personal consumption smoothening,January 2017,Jenny Gu,Rodrigo J. Hernandez,Yingying Shao,Female,Male,Unknown,Mix,,
41,1,Journal of Economics and Finance,28 September 2015,https://link.springer.com/article/10.1007/s12197-015-9336-4,Short selling restrictions in 2005–2009 in Indian market and underpricing of initial public offerings,January 2017,Tanya Gulati,S. K. Bose,Supriyo Roy,Female,Unknown,Unknown,Female,"Research on IPOs provide ample empirical evidence that the closing price of an IPO on the first trading day, on average, is more than the price at which this is offered to the investors (see, e.g., McDonald and Fisher 1972; Reilly and Hatfield 1969; Ritter 1984). This anomaly is termed as underpricing. Previous research explains the underpricing anomaly as a rational response of investors to the valuation uncertainty and the existence of informational friction in IPO markets (see, e.g., Baron 1982; Benveniste and Spindt 1989; Rock 1986). Models on underpricing are mostly built on the assumption that the investors have homogeneous expectations (Sharpe1964) and the realized returns are good surrogate for the expected returns (Muth 1961). This paper focuses on an alternative theory of underpricing proposed first by Miller (1977) that questions the homothetic expectations and investors’ rationality. Miller argues that, when short sellers are constrained from incorporating their negative opinion, the realized returns measure the ex ante expectations of the most optimistic investors. Underpricing arises because the investment banker sets the offer price based on the aggregate expectations of all potential investors, which is lower than the valuation imparted by the optimistic investors. Miller hypothesizes that, when short selling is constrained, the greater is the divergence of opinion among the investors, the higher is the underpricing. This study tests the relationship between the underpricing of IPOs and the divergence of opinion among the investors in Indian stock market from 2005 to 2009 when short selling was almost prohibited and provides the evidence consistent with Miller’s hypothesis. Miller (1977) hypothesis rests upon the premise that the stocks are subjected to short sale constraints as well as investors disagree on their present value. Empirical research to test Miller hypothesis, therefore, generally proceeds in two directions (Boehme et al. 2006). One direction of research investigates the impact of variation in short selling constraints on the stock prices assuming the existence of divergence of opinion among the investors by using measurable proxies for constraints. However, research is not unanimous on the proxies for short selling constraints and therefore provides conflicting evidence and interpretations (Chen et al. 2002; Edwards and Hanley 2010; Figlewski 1981). For example, Figlewski (1981) and Edwards and Hanley (2010) provide conflicting explanation to whether the increase in short selling would be construed as a proxy for tightening or loosening of constraints. The other direction of research by Diether et al. (2002) and Houge et al. (2001) assumes the existence of short selling constraints in general and uses different proxies for the divergence of opinion to test Miller’s hypothesis. But these studies fail to take into consideration that in a given market, stocks can be subjected to various degrees of short selling constraints. For example, borrowing costs may not be uniform across different stocks. Cross sectional variations of impact of short selling constraints, if not considered, are likely to reduce the power of statistical tests. We overcome the difficulty of locating a suitable proxy for short selling constraints by investigating Indian stock market for a period when it faced prohibitive restrictions on short selling. This also controls for the cross sectional variations in the severity of the constraints affecting individual stocks as prohibition affects all stocks to the same degree. Our findings, therefore, provide more robust evidence consistent with Miller’s hypothesis. Literature on IPOs documents that selling methods and regulatory features of a market influence underpricing (Spatt and Srivastava 1991). To control for this, IPOs sold by book building method only have been examined. We test the relation between underpricing and divergence of opinion, by using the first day flipping ratio (Houge et al. 2001; Krigman et al. 1999) and the first day trading ratio (Chemmanur and Krishnan 2011; Harris and Raviv 1993; Hong and Stein 2007; Loughran and Westberg 2005) as proxies for divergence of opinion among the investors. Flipping ratio is defined as total number of IPO shares sold on settlement basis on the first trading day as the percentage of issue size and trading ratio is defined as the total number of IPO shares traded on the first trading day scaled by issue size. The previous studies also show that underpricing of IPOs is also related to ex ante risk, returns on composite market index prior to IPO date, the partial incorporation of information in book building process, and the shares retained by original owners as a percentage of issue size. This study uses these variables as control variables along with the variables of divergence of opinion in multivariate regressions to prevent bias in the findings. Short selling restrictions were marginally relaxed in 2008–09. This study finds that underpricing of this period is significantly lower than that of the preceding years. To control for the influence of the relaxation in short selling constraints, the study uses 2008–09 dummy variable in the multivariate regression. This study finds statistically significant contribution of variables for divergence of opinion on underpricing and the results are robust after controlling for variables that are predicted to influence underpricing. During the period of study, 24 IPOs were introduced into futures and options (F&O) markets on the listing day. Previous research hypothesizes that trading into F&O markets amount to loosening of short selling constraints (Diamond and Verrecchia 1987). We test the robustness of our results by excluding 24 IPOs introduced into F&O markets in a separate multivariate regression. Both proxies of divergence of opinion show positive and statistically significant relation with underpricing for rest of the sample. The remainder of this paper is organized as follows. Section 2 presents review of related literature, section 3 explains regulatory restrictions in Indian stock market, section 4 describes data and presents summary statistics, section 5 provides empirical analysis, and section 6 summarizes the results and presents the conclusions.",1
41,1,Journal of Economics and Finance,28 September 2015,https://link.springer.com/article/10.1007/s12197-015-9342-6,Foreign exchange rate moments and FDI in Ghana,January 2017,Lord Mensah,Godfred Alufar Bokpin,Eric Dei Fosu-Hene,Unknown,Unknown,Male,Male,"Goldberg in her report to the Princeton Encyclopaedia of the world Economy documented the effect of exchange moments on FDI flows. Looking at exchange rate levels, she stated that currency depreciation in a host country has two possible effects on FDI. Firstly, the currency depreciation will reduce the cost of production and wages in the host country relative to the source country. All else equal, a country undergoing real currency devaluation positions itself to attract production capacity investments. Further, a devaluation of the host-country currency raises the relative wealth of source country agents and can raise multinational acquisitions of certain host market assets. To the extent that source country agents hold more of their wealth in their own currency-denominated form. In effect, depreciation of the host currency increases the relative wealth position of source country investors, lowering their relative cost of capital. This allows the investors to bid more aggressively for assets abroad. The impact of volatility in exchange rate movements on FDI has been well documented from the perspective of risk aversion. For instance, Cushman (1985, 1988) documented risk aversion in foreign investors and that exchange rate volatility is likely to reduce FDI flows. This can be associated to investor’s requirements for compensation for extra risks that an exchange rate movement introduces into the returns on investment. Higher exchange-rate variability reduces the safe bet equivalent expected exchange-rate level. For firms that make investment decisions today, profit realization in the future periods require the certainty equivalent levels as part of their expected profit functions. If exchange rates are highly volatile, the expected value of investment projects dwindles thereby reducing FDI. The existing literature on the relationship between FDI and exchange rate movement focuses on the first two moments. Bloningen (1997) provides an excellent review of this literature. Narrowing the scope to recent papers on specific countries, Kyereboah-Coleman and Agyire-Tetteh (2008), Osinubi (2009) and Sharifa-Renani and Mirfatah (2012) documented the impact of foreign exchange volatility on foreign direct investment in Ghana, Nigeria and Iran respectively. The above literature has ignored the effect of large and small shocks of exchange rate on FDI flows. However, it has been noted in other branches of economics that a large shock in exchange rate movement can differ significantly from sequence of small shocks with the same cumulative effect. Extremity bias in impression formation is a well-known idea in psychology (see Skowronski and Carlston (1989) for a survey) which refers to the extra attention and importance that investors give to jumps. The higher the skewness, the greater the exchange rate ‘jumps’ (unusual large movements in exchange rates). It is important to examine the idea that investors may react differently to large exchange rate movements as opposed to series of small movements in the same direction. Therefore, the study of the effect of exchange rate movements on FDI should not concentrate only on the total devaluation over a period of time but also the relative sizes of the shocks with the period. It is imperative to note that the presence of large shocks in the data series can also be captured by the fourth moment, Kurtosis. However, Kurtosis captures shocks in both directions while skewness reveals the presence of a few relatively large shocks in one direction. In a situation where the large shocks have a disproportionately larger impact on expectation formation, a skewed distribution of devaluations will lead to expectations of mean reversion, subsequently affecting FDI flows. On the other hand, in a leptokurtic distribution, the presence of large shocks in opposite direction cancels each other with no impact on future exchange rate expectations and therefore, no effect on FDI flows (Chakrabarti and Scholnick (2002)). To empirically examine the hypothesis that large exchange rate shocks generate mean-reverting expectations, one has to investigate if there is a relationship between exchange rate changes and FDI. If it is established that there is a relationship between the skewness of exchange rate movement and FDI flows, then, this would confirm the hypothesis. Further, the United Nations 2013 world report indicates that global FDI fell by 18% in 2012, and that recovery will take longer than expected because of global economic fragility and policy uncertainty. Regardless to the fall in global FDI, developing countries for the first ever, absorbed more FDI than developed countries (52% of FDI flows). Specifically, African economies witnessed 5% increase in FDI inflows. The concentration of the flows was in the extractive industries. However, the investment in the consumer-oriented manufacturing and services industry is also expanding. The question that comes into the researchers' mind is “what is the cause of the increase in FDI flows to Africa whiles the global situation dwindles."" The bucked of the FDI flows to developing economies have been linked to investors’ expectation regarding exchange rate. As far as the flows of investment around the world are concerned, investors will definitely worry over foreign exchange expectations. Exchange rates can affect both the quantum of foreign direct investment that takes place and the allocation of this investment spending across countries. The literature on international finance presumes exchange rates levels, and moments are one of the most important factors in a firm’s FDI decision. This paper estimates the impact of exchange rate movements on the volume of FDI flows to Ghana by using robust regression and bootstrapping technique. In Figure 1, it can be seen that during the period of our study, the Ghanaian currency devaluation and FDI inflows show high volatility before the year 2002. The volatility seems to reduce after this period and the devaluation and FDI inflows moves in the same direction. Looking at the dynamics of the FDI inflows and the volatility of the Cedi devaluation, one would like to investigate whether their relationship. Further, over the past two decades, the government of Ghana has been making the effort to create a friendly investment environment through institutional and legal frameworks and to the extent of promotional campaigns, just to encourage FDI inflows. The economic payoff for offering special incentives to attract FDI is the common believe that foreign investment generates positive externalities in the form of technology transfer and spillovers on growth. The question is “what is the investors’ long term expectation about the Ghanaian currency."" In this paper, we make inference about the investor's longer-term exchange rate expectation on the Ghanaian currency from studying FDI. The paper makes a contribution to three distinct strands of the literature. It contributes to the exchange rate expectations literature by examining expectations using data representing investor behavior towards developing countries (i.e. Ghana). Secondly, it provides the explanation for the link between FDI and exchange rate changes based on mean reverting expectation. Lastly, it contributes to the literature by extending the traditional methodologies using the bootstrapping technique. The extensive empirical literature on the impact of exchange rate moments on FDI focuses on the VAR and panel data approach and mostly on annual data. However, the limited sample size of annual data calls for another technique, which will use the actual data to generate several scenarios (bootstrapping) and estimate the parameters of interest. The advantage of the bootstrapping technique is that it allows the investigator to make inferences without any strong distributional assumptions, since the analysis assumes the distribution of the actual parameters. Dynamic description of the Cedi devaluation to the Dollar and FDI As indicated by Anderson (2008), the OLS is the spine of statistical methods in quantitative social science; however, influential observations might cause havoc to its estimates. Therefore, our paper adopts the OLS method and affirms the results by using the robust regression method and the bootstrap method. We find that the current mean level of depreciation or devaluation of the host country currency attracts FDI flows to Ghana. The impact of depreciation volatility on the FDI is positive. Finally, skewness of the depreciations does not show any relationship with FDI. Subsequent sections of the paper are organized as follows. Section II presents an overview of related papers on the impact of exchange rate moments on FDI. Section III provides theoretical justification of the relationship between the exchange rate moment and FDI. Section IV outlines the data, and the empirical methodology used in the research. We present the empirical results in Section V. Finally, the Section VI concludes. In the past decades, extensive work has been done in the area of exchange rate movement and FDI flows. The traditional view is that, exchange rate movements should not affect FDI flows because if an asset in a host country is seen as a claim to profit in that country’s currency, and if the profit is converted to the source countries' currency at the same exchange rate, the level of exchange rate does not affect the present discounted value of the investment (McCulloch (1989), reiterated by Chakrabarti and Scholnick (2002)). This view is based on the assumption that exchange rate movement is a random walk process (specifically, mean reverting); meaning, expected future exchange levels should be the same as the current rate. It also means a perfect elasticity of exchange rate expectations to current exchange rate. However, modern literature on the effect of exchange rate changes on FDI is based on assumption that the capital market is imperfect and that debtors face a premium for external borrowing. From this assumption, it has been shown that devaluation of host country currency would increase FDI inflows because the comparative wealth of foreign investor would rise, and the cost of inputs falls in terms of source country currency, giving them the opportunity to finance more of the investment internally. For instance, Froot and Stein (1991) documented that in competing for acquisitions in the host country; foreign investors are more likely to outbid host country competitors in times of host currency devaluation for US inward FDI during the 1974–1987. Blonigen (1997) looked at the relationship between the exchange rate changes and FDI flows between the USA and Japan from the perspective of capital market imperfection. He adopted a model that includes the assumption that goods market is imperfect. Therefore, investor don’t have equal access to all markets in order for exchange rate changes to affect the comparative returns available to the source and host country from owning the particular assets. Specifically, he finds a strong negative relationship between the value of the US dollar and the volume of Japanese FDI for industries that involves firm specific assets. On the German market, Buch and Kleinert (2008) established the validity of the goods market friction hypothesis by finding that an appreciation of the source country currency promotes the acquisition of firm specific assets abroad. Furthermore, Takagi and Shi (2011) studied the Japanese FDI flows to nine dynamic Asian economies during the years 1987–2008 and found that FDI decline with the depreciation of the yen against other host country currencies. On the contrary, there are few papers that nailed the capital market assumption in the coffin. In effect, they find a flat relationship between FDI inflows and the foreign exchange devaluation. For example, Healy and Palepu (1993), Stevens (1998) and Chakrabarti and Scholnick (2002) find no evidence of foreign exchange rate devaluation encouraging FDI inflows. There exist some degree of uncertainty in the literature; this led to the survey by Pain and Van Welsum (2003) which conclude that the impact of exchange rate changes on the FDI flows may differ across countries and types of investment. The literature on foreign exchange rate volatility and FDI flows is equally enormous. There are competitive views on how foreign exchange rate volatility relates to FDI. One aspect has argued and suggested risk aversion in foreign investors, that past volatility in the exchange rate is likely to reduce FDI inflows (see Kohlhagen(1977), Itagaki(1981), Cushman(1985)). In addition, it has been shown that exchange rate volatility can postpone risk neutral investor's entry decision. For instance, Dixit (1989a, 1989b) used option theoretic argument to establish that exchange rate volatility would make risk-neutral investors postpone their entry decision. Specifically, Campa (1993) also showed that risk neutral investors desire to postpone investment as a result of exchange rate volatility caused a reduction in FDI inflows to US in the 1980s. The investor behaviour is well spelt in industries with high sunk costs in physical and intangible assets. Dunning (1993) and Bloningen (1997) argues that even though the delaying investment removes any expected profit stream from that investment, the ability to make profitable choices in the future is reserved. That is why the likelihood of delay in investment when faced with uncertainty is greater for industries which the product life cycle is long, or for which the expected lifespan of firm-specific assets is long. Aizenman and Marion (2004) looked at the FDI relationship with exchange and emphasize the difference between vertical and horizontal FDI. They argue that vertical FDI which consists of the division of the production process across different countries may be discouraged by exchange rate volatility because of the need to engage in the intra firm, whereas horizontal FDI, in which similar activities are undertaken in different places, might respond positively. On the other hand, Crowley and Lee (2003) suggest that exchange rate volatility could even still be a deterrent to horizontal FDI, if the investment involves some form joint venture, reversibility will depend on whether any implicit knowledge has been transferred. Such knowledge takes place in the vast majority of foreign investments. Using data disaggregated by industry, Kiyota and Urata (2004), found that exchange rate uncertainty has discouraged Japanese FDI flows to a large number of countries during 1990–2000, for all types of industries. On the contrary, recent paper by Takagi and Shi (2011) found exchange rate volatility increases the Japanese FDI flows to nine dynamic Asian economies during the period between 1987 and 2008. The above research on the foreign exchange volatility and FDI flows concentrate on the developed markets. Limited research has been done on developing markets. The few that exist finds a negative relationship between FDI and foreign exchange rate volatility. For example, Sharifa-Renani and Mirfatah (2012) documented a negative relationship between foreign exchange uncertainty and the FDI in Iran between 1980 and 2006. In addition, Osinubi and Amaghionyeodiwe (2009) suggest a foreign investors need not to worry about exchange rate volatility, and that structural adjustment programme introduced in Nigeria in 1986 had a negative impact on real inward FDI, which could be due to the deregulation that was accompanied by exchange rate volatility. On the Ghanaian economy, Kyereboah-Coleman and Agyire-Tetteh (2008) showed that the volatility of the real exchange rate has a negative influence on FDI inflow, and that the liberalization process has not led to greater inflow of FDI to Ghana. They also reveal that while both the stock of FDI and political factors are likely to attract FDI, most foreign investors do not consider the size of the market in making decision to invest or otherwise in Ghana. Our study differs from this paper in terms of the span of the data and the methodology. With regards to volatility, there is no less mix up. Strands of the literature suggest the exchange rate volatility promote FDI flows. For example, Itagaki (1981) and Cushman (1985) propose conditions in which uncertainty could theoretically increase FDI if it is used as a substitute for exports. Goldberg and Kolstad (1995) propose two assumptions needed to show that greater short-run real exchange volatility will raise the share of foreign production in total production. This is negative utility associated with the variability of profits and factors of production cannot adjust immediately after an exchange rate shock, which cause firms to diversify their production across countries. In relation to exchange rate changes and FDI, the effect of exchange volatility on FDI is hazier, both theoretically and empirically. The hypothesis that large exchange rate shocks generate mean-reverting expectations is empirically examined by establishing the relationship between skewness of exchange rate changes and FDI flows. If the data shows that FDI flows are significantly affected by skewness of exchange rate's movements, then this would be consistent with the mean reverting hypothesis. Further, if skewness proxies for expected mean reverting changes, this reinforces the idea that source country investors’ care about the future stream of revenues and returns dominated their own currency. Very few researchers explore this channel of influence in the literature. Chakrabarti and Scholnick (2002) used panel data techniques on exchange rate movements and FDI flows from the US to 20 OECD countries and find that skewness of devaluation has a robust positive significant relationship with FDI flows, while average devaluation and its volatility do not. They conjecture their results confirm the hypothesis that relatively large exchange rate movements produce mean-reverting long run expectations. By using panel data on Japanese FDI flows to nine dynamic Asian economies between 1987 and 2008, Takagi and Shi (2011) documented a negative reaction of FDI to the third moment of monthly exchange rate changes. This implies smaller volume of FDI when the distribution of exchange rate changes is positively skewed, thus, when the yen was biased towards relatively large depreciation shocks. Lack of the studies on the impact of the skewness on FDI is unfortunate in view of the fact that, in making a decision to invest in Ghana; foreign investors will ultimately be interested in the future stream of returns and revenues denominated in their own currency, which must depend on the mean reversion hypothesis of the exchange rate expectations. This is a gap in the literature that this paper seeks to fill.",3
41,1,Journal of Economics and Finance,14 October 2015,https://link.springer.com/article/10.1007/s12197-015-9337-3,Nominal GDP targeting under learning,January 2017,George Waters,,,Male,Unknown,Unknown,Male,,2
41,1,Journal of Economics and Finance,26 October 2015,https://link.springer.com/article/10.1007/s12197-015-9344-4,Asymmetric reaction is rational behavior,January 2017,Philip A. Horvath,Amit K. Sinha,,Male,Male,Unknown,Male,"By calibrating utility functions by assigning probabilities to uncertain outcomes (Savage 1954) individuals are expected to make economically rational decisions by applying the five axioms of cardinal utility.Footnote 1 Accordingly, the outcome with the highest utility would be the most preferred outcome, and ought to be selected (Samuelson and Zeckhauser 1988). Applying this rationale, von Neumann and Morgenstern (1947), Tobin (1958), Friedman and Savage (1948), Pratt (1964)-Arrow (1971), and Markowitz (1959) developed various versions of Theory of Choice. Specifically, Markowitz (1959) developed the mean-variance approach which required symmetry of valence in within moment preferences. Primarily, symmetry of valence requires individuals to be risk averse to downside risk, as much as they would be willing to accept upside risk of equal magnitude. Samuelson and Zeckhauser (1988) also point out that fundamental rational decision requires an individual decision to not be influenced by the order in which the alternative outcomes are presented. Advances in behavioral finance,Footnote 2 however, provides several well established artifacts which are frequently cited as evidences of investors being neither totally rational decision makers nor being symmetric in valence of their within moment preferences. De Bondt, Werner, Muradoglu, Shefrin and Staikouras (2008) summarize several establish facets of behavioral finance that have been argued to be evidences not consistent with expected utility theory based on rational decisions. These are (a) anchoringFootnote 3 (De Bondt 1993; Muradoglu and Onkal 1994), (b) representativenessFootnote 4 (De Bondt et al. 2008), (c) availabilityFootnote 5 (Barber and Odean 2008), (d) overconfidenceFootnote 6 (Daniel et al. 1998; 2001), (e) loss aversionFootnote 7 (Tversky and Kahneman 1992), (f) mental accountingFootnote 8 (Henderson and Peterson 1992; Shefrin and Thaler 1988), (g) myopic loss aversionFootnote 9 (Haigh and List 2005; Benartzi and Thaler 1995), (h) self-controlFootnote 10 (Thaler and Shefrin 1981; Shefrin and Statman 1985), and (i) regret aversionFootnote 11 (Shefrin and Statman 1985). A set of anomalies that observed in discounted utility functions are summarized in discussed in Frederick et al. (2002). These are sign effectFootnote 12 (Thaler 1981), magnitude effectFootnote 13 (Ainslie and Haendel 1983; Benzion et al. 1989), “Delay-Speedup” asymmetryFootnote 14 (Loewenstein 1988), preferences for improving sequencesFootnote 15 (Frederick et al. 2002; Loewenstein and Prelec 1993), violation of independence and preference for spreadFootnote 16 (Loewenstein and Prelec 1993), and existence of hyperbolic discountingFootnote 17 (Horvath and Sinha 2013; Keller and Strazzera 2002; Liabson 1997; Read 2001; Rubinstein 2003; among others). While acknowledging that it is beyond the scope of one paper to explain every behavioral phenomenon as rational decision, in this paper we explain that weighing losses (loss aversion and myopic loss aversion) more than gains of similar magnitude to be a consequence of rational decision. Similarly, the observed phenomenon of holding onto loser stocks, while booking profits early is, perhaps, also the outcome of rational decision, especially if one allows for individual utility preferences for positive and negative partial moments to be different. Preferences for partial moments could also explain Thaler’s (1981) observation regarding sign effect. To explain these specific behavioral observations as outcomes of rational decisions, we turn to the literature that explores the choice implications of the second and higher order moments in the distribution of returns for investors. Several research manuscripts (Arditi 1967; Jean 1971; Arditi and Levy 1972; Levy and Sarnat 1972; Kraus and Litzenberger 1976; and Simkowitz and Beedles 1978; Christie-David and Chaudhry 2001; Chiao et al 2003; Ali 2011) provided intuitive and empirical support for higher moment in distributions of returns and asset pricing models to accommodate third and fourth moments. Scott and Horvath (1980) showed that investors alternate between preferences for odd, and aversion even moments. That is, investors prefer returns and skewness but would rather avoid variance and kurtosis. The results of both streams of research contributed immensely to our understanding of the role and existence of moments in theories of choice. For instance, while Levy and Markowitz (1979) defended the mean-variance analysis as an approximately correct criterion, their reliance upon variance as a measure of risk suggests that individuals are as averse to upside outcomes as much as downside outcomes. Markowitz (1959) presented semi-variance as an alternative measure to the mean-variance approach. The semi-variance model may be said to be concerned only with adverse, downside, deviations. The research on semi-variance (c.f., Quirk and Sasposnik (1962), Mao (1970), Hogan and Warren (1972), Klemkosky (1973) and others) and Fishburn’s (1977) more general lower partial moment approach (see Nawrocki (1999) for a concise history) suggests that it is a better measure of risk than variance. Since Estrada (2002) recent literature has also witnessed a proliferation of manuscripts incorporating semi-variance in asset pricing models. These studies are Estrada (2004, 2007), Patari (2008), Post and van Vliet (2006), CheremushkinFootnote 18 (2009), Antle (2010), Beach (2011), Nawrocki (2014), and Ayub et al. (2015) to mention a few. Invariably, these studies are empirical in nature and consider the partial upper and lower second moments only. This manuscript differs from these as we establish, theoretically, a direction of preference for partial moments for the second, third and fourth moments. Accordingly we make an additional contribution to the literature as we provide a theoretical foundation for the empirical work that may consider partial moments approach to asset-pricing models beyond those of the second moment. While some asset-pricing models (Fama and French 1992, 1993, 1996, 2014; and Carhart 1997, to mention a few) ignore moments as price influencing variables, utility based models when considering higher moments (Fang and Lai 1997; Jondeau and Rockinger 2006; Boyer et al. 2010) have largely assumed symmetry in preferences for partial moments. The underlying implication of symmetry of strength, valence, of preference within these moments may lead to imprecise and misleading results. Thus, the purpose of this paper is to use a Bawa (1975) and Fishburn (1977) partial moments approach with Scott and Horvath’s (1980) alternating directions of preference to demonstrate that utility-based theory of choice requires that investors consider upside and downside outcomes and further that valence of within-moment preference is asymmetric within each higher moment. This paper also indicates that valence is sensitive to the size of the symmetrically distributed gamble relative to wealth.",7
41,1,Journal of Economics and Finance,14 November 2015,https://link.springer.com/article/10.1007/s12197-015-9348-0,Litigation risk and financial leverage,January 2017,James Malm,Marcin Krolikowski,,Male,Male,Unknown,Male,"This paper examines the link between litigation risk and financial leverage. An extensive literature exists on the relationship between litigation risk and stock market reaction. For instance, Romano (1991); Bizjak and Coles (1995); Bhagat et al. (1998); Bhattacharya et al. (2007); Gande and Lewis (2009) all find that the filing of lawsuits against corporations often lead to significantly negative market reactions. Other studies show that litigation risk is related to managerial turnover (Niehaus and Roth (1999)), to firm disclosure behavior (Skinner (1994)), to IPO underpricing (Lowry and Shu (2002)), to auditor resignation (Shu (2000)), and to firm reputation (Karpoff and Lott (1999), Karpoff et al. (2008), Karpoff (2011)). Surprisingly, despite the growing body of research, very little work has been done to explore the relationship between litigation risk and financial leverage. One reason for the dearth of research in this area is the non-existence of a reliable proxy to measure litigation risk. In recent times, a number of papers have attempted to establish a link between litigation risk and external financing. A recent paper by Autore et al. (2014) finds that financial misconduct adversely affects subsequent external financing. The authors use debt and equity issues to proxy for external financing. In another contemporary study, Yuan and Zhang (2015) study how the pricing of debt changes after firms are sued by shareholders. Using security class action lawsuit information, they find that banks charge higher interest spreads on loans made to borrowers with higher litigation risk. The authors also document that banks closely monitor firms in a high litigation environment by requiring more collateral, putting in place covenants and providing short term maturity debt. The closest studies to this paper are Crane (2011) and Autore et al. (2014). Crane (2011) uses the number of product liability litigation events within a firm’s industry to examine whether management strategically uses financial policy when facing litigation risk and finds that greater litigation exposure leads firms to choose higher leverage. The author documents that the increase in leverage is as a result of an active decision to repurchase shares. Using securities litigation as a proxy for litigation risk, Autore et al. (2014) examine the effect of financial fraud litigation on external financing and find that firms with recent history of securities litigation are less likely to seek external debt and equity financing. With a few exceptions, prior literature on litigation risk focuses primarily on securities class action lawsuits. In contrast to Crane (2011) and Autore et al. (2014), we analyze the relationship between litigation risk and financial leverage using a broad sample of corporate lawsuits beyond product liability and securities litigation. We expand this line of research to include several other lawsuit types including lawsuits related to the violation of securities, product liability, intellectual property, environmental, medical liability, labor, contract, and antitrust law. In addition we use lawsuits at the firm rather than industry level to proxy for litigation risk. These additional facets are the distinctive features of this paper. We examine whether litigation risk is related to different leverage measures including total leverage, long-term-term debt leverage and short-term debt leverage. We construct a comprehensive database on corporate lawsuits from 1996 to 2011 focusing on firms belonging to the S&P 1500. We use six different measures to proxy for litigation risk in the main empirical analyses. We test the hypothesis that there is a negative relationship between litigation risk and the level of financial leverage. After controlling for other determinants of financial leverage, we find that high litigation risk firms have lower levels of total debt leverage. Splitting total debt into long-term and short-term debt, we find that higher litigation risk firms have significantly lower levels of long-term debt leverage. We do not find a statistically significant relationship between litigation risk and short-term debt leverage. The results hold when we eliminate the financial crisis period from the sample. The results are also robust to using alternative regression methodology, and accounting for the effect of off-balance sheet debt. This evidence is consistent with high litigation risk firms’ using lower levels of leverage as a result of higher interest spreads on credit facilities. The finding is also consistent with creditors monitoring higher litigation risk firms. Our paper contributes to the litigation and corporate finance literature. We add to an emerging body of research in this area by shedding light on the costs associated with corporate litigation. We identify another determinant of financial leverage and document another avenue whereby legal institutions affect corporate financial policy. The link between litigation risk and financial leverage is of interest to the business community, financial economists, management, and the investing public. Our study provides a critical source of information for practitioners on how the legal environment affects corporate behavior. We also provide basic information on the corporate lawsuits of S&P 1500 companies. To the best of our knowledge, this is one of the first studies to use a comprehensive dataset on corporate litigation to empirically investigate the relationship between litigation risk and corporate financial policy. The remainder of this paper proceeds as follows: In Section 2, we review the relevant literature and develop our testable hypothesis. Section 3 provides a description of the sample, data, and methodology employed in the analysis. Section 4 sets forth and discusses the empirical results on the link between litigation risk and the levels of financial leverage. We also conduct a series of robustness checks in Section 4, while the conclusions are detailed in Section 5.",15
41,1,Journal of Economics and Finance,29 January 2016,https://link.springer.com/article/10.1007/s12197-016-9353-y,Do credit associations compete with each other in Japanese regional lending markets?,January 2017,Kazumine Kondo,,,Unknown,Unknown,Unknown,Unknown,,
41,2,Journal of Economics and Finance,29 September 2015,https://link.springer.com/article/10.1007/s12197-015-9341-7,Factors driving systemic risk of banks in Latin America,April 2017,Jacob Kleinow,Andreas Horsch,Mario Garcia-Molina,Male,Male,Male,Male,"Which factors determine the systemic importance of banks in Latin America? In this paper, we investigate the drivers of systemic riskFootnote 1 in the Latin American financial sector as well as contagion among banksFootnote 2. Furthermore, we propose a novel measure of systemic risk – the Systemic Risk Index (SRI) – to capture the impact a single financial institution has on the financial sector and vice versa. The topic of our paper is of considerable interest to regulators and economists as well: Our results offer new insights on the drivers of financial instability and provide implications for the prudential regulation of banks. Financial systems as a whole tend toward instability. This is due to the fragile nature of the players, i.e. financial institutions, especially banks, and their business models. The instabilities of players in the financial sector do not usually remain isolated events but are of a contagious nature, thus tending to spread through the financial system and to cause severe negative macroeconomic shocks (Allen and Gale, 2000; Giesecke and Kim, 2011). Because of their role as a financial intermediary (or delegated monitor), their opaqueness, their interconnectedness, and the typical characteristics of their lenders, banks are particularly prone to infecting other banks with financial distress – or to being infected by them. This in particular holds for those banks that almost certainly and rather quickly would destabilise the system as a whole, so called systemically important financial institutions (SIFIs). Consequently, the identification of drivers of systemic risk of financial intermediaries is of vital importance. Recent papers on systemic risk of financial institutions produced substantial findings. Existing literature in this field, however, is comparably young and leaves questions unanswered: We contribute to the closing of the research gaps by using innovative key indicators for systemic risk, by focusing on a region less researched to date and by distinguishing between crisis and non-crisis periods. This is carried out as follows: second section offers a review of related literature as our background and starting point. The subsequent third section explains our research design. In the fourth section, we derive key determinants of systemic importance of Latin American Banks, while fifth section concludes our findings.",13
41,2,Journal of Economics and Finance,30 October 2015,https://link.springer.com/article/10.1007/s12197-015-9346-2,Output subsidies in mixed oligopoly with research spillovers,April 2017,Shoji Haruna,Rajeev K. Goel,,Male,Male,Unknown,Male,"Schumpeter (2004) asserted in his famous book, “Theorie der Wirtschaftlichen Entwicklung,” that innovation takes an important role in economic development. Since this seminal work, the importance of research and development (R&D) has been widely recognized. In present times, when globalization has intensified competition among firms, managers realize that they cannot survive without performing R&D. Policy makers have also recognized the importance of research and have enacted various policies to directly and indirectly facilitate it. In the context of the literature on technological change, the issue of research spillovers and their consequent impacts on firm behavior is well researched (see d’Aspremont and Jacquemin (1988); Goel and Haruna (2007); Kamien et al. (1992); Kawasaki et al. (2014), and Reinganum (1984) for early theoretical models; and Bernstein and Yan (1997), Jaffe (1986), and Maurseth and Verspagen (2002) for related empirical analyses). Research analyzing R&D behavior in traditional pure oligopolies abounds. In contrast, there is a smaller literature analyzing R&D behavior in mixed oligopolies. However, mixed oligopolies are present in many nations and industries. In mixed oligopolies, firms are located in both the private and public sectors, and thus have different objectives while competing in the same marketplace. Mixed markets might emerge when government-controlled industries are partially privatized (Choi (2012); Dadpay and Heywood (2006); Haruna and Goel (2015); Heywood and Ye (2009); Kato (2006); Poyago-Theotoky (2001); Tomaru and Saito (2010); Wang and Wong (2009)).Footnote 1 Examples of mixed oligopolies include transportation (public and private sector airlines), postal and shipping services, mining, manufacturing, etc. The broadcasting industry is also one mixed oligopoly industry in which government-controlled and commercial broadcasting companies coexist, e.g., in India and Japan. Yet, implications of competition in such markets are not fully understood, thereby inhibiting optimal resource allocation by the government (especially since the government has direct involvement via the public firm).Footnote 2 The present research will shed some formal light on this context. This paper adds to the literature on mixed oligopolies by examining the behavior of firms engaged in research with imperfect appropriation of rewards. Another contribution of the paper is to incorporate output subsidies. Output subsidies are akin to a lowering of production costs of subsidized firms and enhancing social welfare through resource redistribution. What complicates decision-making for firms and for policy makers is when firms are situated in both the public and private sectors, engaging in (imperfectly appropriable) research, and the government chooses to subsidize output. On the other hand, the subject of research subsidies has also received attention (see, e.g., Gil-Molto et al. (2011)), although there is some debate about their effectiveness (see Kauko (1996) and Rebolledo and Sandonis (2012)). Governments might choose to subsidize output rather than R&D in instances where output enhancement is politically more popular and more straightforward, when public firms lack the capability of doing research, or when the design and administration of research subsidies is relatively more complicated than that of output subsidies. Our paper considers output (not R&D) subsidies in mixed oligopolies with R&D, its related spillovers, and their effects.Footnote 3 This aspect has not been previously considered in the literature. Such examples of output subsidies include agriculture subsidies (e.g., European agriculture subsidies), fuel production subsidies (e.g., ethanol production subsidies in the United States), fertilizer subsidies, electric car production subsidies, etc. (see Anderson et al. (1997)).Footnote 4 We compare firm behavior and government policy in two alternate models with R&D, and to the baseline model without R&D. A key finding shows that output subsidies cannot attain optimal resource allocation in a mixed duopoly with R&D, and, furthermore, R&D and its spillovers lead to a corner solution: specifically, when both public and private firms invest in R&D and spillover rates are medium to large (or zero), the optimal policy of the government is a laissez-faire policy. Interestingly, in the absence of spillovers, the public firm monopolizes the market. We also find that the rankings of output subsidies and the output of the private firm are significantly affected by R&D spillover rates, but the ranking of welfare is not affected. Furthermore, by comparing our findings with earlier findings (e.g., Gil-Molto et al. (2011); Kesavayuth and Zikos (2013)), the roles of R&D spillovers and the effectiveness of output and R&D subsidies will be better understood. The rest of this paper is organized as follows. Section 2 presents a basic two-stage mixed duopoly game without R&D. The duopoly is comprised of a profit-maximizing (private) firm and a welfare-maximizing (public) firm. In Section 3 a three-stage game model of the duopoly is introduced. Only the private firm undertakes R&D to reduce production costs in the second stage of the model. We examine the effects of R&D on the behavior of the duopoly. In Section 4 both firms non-cooperatively conduct R&D in a three-stage game model. We consider the effects of R&D competition on the behavior of the duopoly. Section 5 compares results obtained in these models and Section 6 concludes.",10
41,2,Journal of Economics and Finance,14 November 2015,https://link.springer.com/article/10.1007/s12197-015-9347-1,Schwab’s equity ratings: value added or old news?,April 2017,Ray R. Sturm,,,,Unknown,Unknown,Mix,,
41,2,Journal of Economics and Finance,01 December 2015,https://link.springer.com/article/10.1007/s12197-015-9350-6,Monetary targeting in Sri Lanka: how much control does the central bank have over the money supply?,April 2017,Wasanthi Thenuwara,Bryan Morgan,,Unknown,Male,Unknown,Male,"In countries where authorities use a monetary targeting approach to conduct monetary policy, the issues of the stability of the money multiplier and whether money is endogenous or exogenous are critical to successful policy implementation. The Central Bank of Sri Lanka (CBSL) has a long history of using a monetary targeting framework and currently seeks to achieve its final target of price stability by controlling the quantity of broad money (M2) in the economy. The rationale of monetary targeting is that excess money growth will ultimately lead to inflation. Consequently, effective monetary targeting hinges on several prerequisites: the target monetary aggregate must be well controlled by the central bank, there must be a strong and reliable relationship between the goal variable (inflation) and the target aggregate (money supply), there must be a stable money demand function and the targets for the intermediate aggregate must be announced to shape public expectations for inflation. Such a monetary strategy thus relies on the understanding from orthodox monetary economics that the money supply is exogenous. The orthodox (monetarist) approach emphasizes that the monetary base (reserve money or high powered money) is a constraint on the money supply. The conventional money multiplier models hence argue that monetary base is exogenous since it is the monetary liability of the central bank and broad money multiplier is stable and predictable. If this is true, then monetary authority could control money supply as long as monetary base is kept at the desired level in line with the targeted broad money expansion and hence the money supply is exogenous. The monetary targeting framework then relies on the assumption that there is a stable relationship between monetary base and money supply. The money multiplier links these two measures of money and it measures the changes in money supply to a given change in the monetary base. Consequently, provided that the monetary base is under the control of the monetary authority and the money multiplier is stable, it follows that the money supply is exogenous. However, there is growing body of research, mainly from the Post Keynesian School, which asserts that money supply is endogenously determined. Seminal work was provided by Kaldor (1980, 1982) and developed by researchers such as Moore (1988a); Palley (1987–88, 1994) and Wray (1995).Footnote 1 Post Keynesian theories of endogenous money emphasize that, apart from the central bank’s role in changing the monetary base, the financial system as a whole contributes to the creation of money in its response to the demand for credit by the public. Further, Cottrell (1994) argues that in a modern ‘credit-money-economy’ a central bank is bound to accommodate the private sector credit demand not as a matter of ‘political choice’ but as a matter of ‘structural necessity’. Very little research has been carried out into whether monetary targeting is an appropriate policy approach for Sri Lanka. While money demand studies exist for Sri Lanka, for example Weliwita and Ekanayake (1998); and Dharmaratne (2004), the impact of financial innovation on money multipliers and monetary aggregates has largely been ignored. There are no recent published studies that test how much control the CBSL has over the money supply. This paper fills these gaps and investigates the endogeneity/exogeneity of money supply in Sri Lanka. This research further provides an insight into the viability of the monetary targeting framework used in Sri Lanka. To do this, we employ two different approaches. The first is to follow the orthodox monetarist’s approach and test the stability of the broad money multiplier. The second draws on Post Keynesian money supply theories and tests whether broad money in Sri Lanka is endogenous. The results do not support the monetarist (orthodox) view. Both the monetary base and broad money are non-stationary. The two variables are integrated of order one but they are not cointegrated. Also, the stationarity tests for broad money multiplier confirm that broad money multiplier is not stable. However, the findings support the Post Keynesian contention that money supply is endogenous. The tests results show bidirectional causality between bank credit and the broad money supply and between bank credit and the broad money multiplier and significant long-run and short-run relationships between variables. The remainder of the paper is structured as follows. Section 2 reviews the endogenous money theory and recent empirical studies that test it. Section 3 provides an overview of monetary policy and the behaviour of monetary aggregates in Sri Lanka. Sections 4 and 5 describe the data and empirical methodology respectively. The empirical results will be discussed in Section 6. The final section reports conclusions and policy implications.",4
41,2,Journal of Economics and Finance,11 December 2015,https://link.springer.com/article/10.1007/s12197-015-9351-5,Okun’s law: evidence of 13 selected developed countries,April 2017,Matiur Rahman,Muhammad Mustafa,,Unknown,Male,Unknown,Male,"Okun’s Law is the negative statistical relationship between real GDP growth and unemployment rate that is subject to changes in a dynamic economy. The usefulness and the validity of this relationship continually inspire empirical re-investigations. Using US quarterly data from 1947:Q2 to 1964:Q4, Okun (1962) found an inverse statistical relationship between unemployment rate and real GDP growth. According to the finding of this seminal paper, 1 % rise in unemployment rate is associated with approximately 3 % decline in real GDP growth. Okun assumed that unemployment rate can serve as a useful summary of the amount of labor being used in the economy. In reality, Okun’s Law is a statistical relationship rather than a structural feature of the economy (mathematical derivation of Okun’s Law in growth rate form is shown in the Appendix). Like any statistical relationship, Okun’s Law is subject to revisions in a transforming economy. As a rule of thumb, this Law proves to be useful to policymakers and economists with some modifications. Okun’s statistical relationship varies over time and over various phases of the business cycle. The alternative versions of Okun’s Law include the difference version, the gap version, the dynamic version and the production function version. Each of the above versions is presented briefly in simple words as follows: 
The difference version: Okun’s Law first captured the relationship on how changes in the unemployment rate from one quarter to the next quarter moved with changes in real output growth. 
The gap version: While Okun’s Law first relied on readily accessible macroeconomic statistics, the second version connected the level of unemployment to the gap between potential output and actual output. Okun sought to identify how much the economy would produce “under conditions of full employment”. In full employment, Okun considered what he believed to be an unemployment level low enough to produce as much as possible without generating too much inflationary or defaltionary pressure. 
The dynamic version: One of Okun’s observations suggested that both past and current output can impact the current level of unemployment. In the difference version of Okun’s Law, this implies that some relevant variables have been omitted from the right hand side of the equation. Partly based on this suggestion, many economists now use a dynamic version of Okun’s Law. A common form for the dynamic version of Okun’s Law would have changes in current and past real output growths and past changes in the unemployment rates as variables on the right hand side of the equation. These variables would then explain the current change in the unemployment rate on the left hand side. This dynamic version of Okun’s Law has some similarities to the original difference version of Okun’s Law. However, it is fundamentally distinct since it no longer only captures the contemporaneous correlation between changes in the unemployment rate and real output growth. The dynamic relationship is not restrictive in terms of the timing of the connection between output growth and unemployment rate. 
Production-function version: Okun also noted another shortcoming in his proposed relationship. The unemployment rate is at best “a proxy variable for all the ways in which output is affected by idle resources”. Idle resources can come from a number of sources. Economic theory suggests that a country’s production of goods and services requires a combination of labor, capital, and technology. The unemployment rate is but one factor in determining the total amount of labor used as an input. Other factors include the population, the fraction of the population that is in the labor force, and the number of hours for which the employed workers are used. By accounting for all of these components along with the components of capital and technology, economists have a more complete picture of what affects output. This approach has led to Production-function version of Okun’s Law, which typically combines a theoretical production function or a particular way in which labor, capital, and technology combine together to produce output with the gap-based version of Okun’s Law. This allows economists to assess all the idle resources in the economy. Production-function version of Okun’s Law has the benefit of an underlying theoretical structure. Different versions of Okun’s Law are likely to offer different results. Labor markets are structurally asymmetric due to different labor laws across countries. Even labor force is defined differently in different countries. So, drawing of any general conclusion is cumbersome. The principal objective of this paper is to re-explore the validity of the dynamic version of Okun’s Law for 13 selected advanced economies (Australia, Canada, Finland, France, Germany, Italy, Japan, South Korea, The Netherlands, New Zealand, Sweden, UK and USA) by using annual data from1970 through 2013. These countries are selected because of their historical global importance, high per capita real income, and technological progress. Among them, 11 countries belong to the OECD except South Korea and New Zealand. These two countries are included in the mix for the same reasons, as above. The balance of the paper proceeds as follows: Section II deals with brief review of the related literature. Section III outlines the empirical methodology. Section IV reports results. Finally, Section V offers conclusions and some policy implications.",6
41,2,Journal of Economics and Finance,23 December 2015,https://link.springer.com/article/10.1007/s12197-015-9349-z,Admitting mistakes pays: the long term impact of goodwill impairment write-offs on stock prices,April 2017,Yingmei Cheng,David Peterson,Karen Sherrill,Unknown,Male,Female,Mix,,
41,2,Journal of Economics and Finance,29 January 2016,https://link.springer.com/article/10.1007/s12197-016-9352-z,Determinants of export diversification in Sub-Sahara African region: a fractionalized logit estimation model,April 2017,Dobdinga Cletus Fonchamnyo,Afuge Ramsy Akame,,Unknown,Unknown,Unknown,Unknown,,
41,2,Journal of Economics and Finance,10 February 2016,https://link.springer.com/article/10.1007/s12197-016-9355-9,The fisher relationship in Nigeria,April 2017,Borja Balparda,Guglielmo Maria Caporale,Luis Alberiko Gil-Alana,Male,Male,Male,Male,"Obtaining empirical evidence on the Fisher relationship is particularly important for countries where there is political instability and a consequent lack of clear directions from the fiscal policy side. In this case the only policy available tools to address policy challenges and pursue objectives such as price and exchange rate stability are of a monetary nature, more specifically nominal interest rates. Nigeria is one such country, with the Central Bank of Nigeria (CBN) being essentially the only functioning policy-making body. Understanding whether interest rate changes can be an effective policy tool or instead can only be used to predict inflation by testing the Fisher Hypothesis is therefore crucial in such a case. This motivates the analysis in the present paper, that examines the Fisher relationship in Nigeria by carrying out standard unit root tests and also applying fractional integration techniques to 1-month, 3- month, 6-month and 12-month deposit rates and inflation. The paper is structured as follows. Section 2 reviews the relevant literature. Section 3 describes the data and the empirical analysis, and Section 4 concludes.",3
41,2,Journal of Economics and Finance,12 February 2016,https://link.springer.com/article/10.1007/s12197-016-9357-7,A simple empirical investigation into the optimal size of the NGDP Target and Level targeting,April 2017,Ryan H. Murphy,Jiawen Chen,,,Unknown,Unknown,Mix,,
41,2,Journal of Economics and Finance,12 February 2016,https://link.springer.com/article/10.1007/s12197-016-9356-8,"Financial development, role of government, and bank profitability: evidence from the 2008 financial crisis",April 2017,Hsiu-I Ting,,,Unknown,Unknown,Unknown,Unknown,,
41,2,Journal of Economics and Finance,29 February 2016,https://link.springer.com/article/10.1007/s12197-016-9359-5,"Bank capital, loan activity, and monetary policy: evidence from the FDIC’s Historical Statistics on Banking",April 2017,Paul E. Orzechowski,,,Male,Unknown,Unknown,Male,"The motivation behind this study is to examine the long-run relationship behind the bank’s capital ratio (i.e. bank capital to bank total assets), lending growth, and monetary policy. This study analyzes two different groups of capitalized banks (high and low capital banks) to see if their lending patterns differ when subjected to the same monetary policy. Do banks with different capital structures have different lending patterns given the same monetary policy? This paper hopes to contribute to the overall ‘capital credit crunch’ debates and the ‘bank capital channel’ (BKC) literature. The analysis also looks at the bank’s loan portfolio mix regarding the ratio of real estate loans to commercial loans. This may help extend some of the related literature that question if banks adjust their loan portfolio in reaction to monetary policy. Is there a long-term significant relationship between the federal funds rate (FFR) and the ratio of real estate to commercial loans held at banks? The organization of this paper contains a background and literature review in Section 2. Section 3 outlines the predictions of the study. Section 4 describes the data and methodology used in the analysis. Section 5 outlines the model and variables estimated. Section 6 describes the empirical results. The last section summarizes relevant conclusions.",1
41,2,Journal of Economics and Finance,14 March 2016,https://link.springer.com/article/10.1007/s12197-016-9358-6,The demand for money in Angola,April 2017,C. P. Barros,João Ricardo Faria,Luis A. Gil-Alana,Unknown,,Male,Mix,,
41,3,Journal of Economics and Finance,18 April 2016,https://link.springer.com/article/10.1007/s12197-016-9360-z,The many conditions under which monopolistic advertising can differ from the social optimum,July 2017,Richard E. Just,Rulon D. Pope,,Male,Unknown,Unknown,Male,"Many classic studies considered the question of whether advertising by a monopoly is socially excessive (e.g., Kaldor 1950). Early formal models following Dixit and Norman (1978), who assume that advertising alters underlying preferences (the persuasive advertising paradigm), generally conclude that it is excessive. Without stable preferences, however, conventional concepts of economic welfare analysis that define the social optimum do not apply. This problem was solved by the complementarity paradigm of advertising inspired by Stigler and Becker (1977) and refined by Nichols (1985).Footnote 1 With this approach, more recent studies such as Becker and Murphy (1993) and Bagwell (2007) refute the conclusion that advertising is necessarily excessive. For example, one of Bagwell’s (2007, p. 1760) main results, illustrated in the complementary paradigm, demonstrates a plausible case where monopoly advertising is inadequate. The purpose of this paper is to compare static monopolistic advertising to the social optimum under a much broader set of conditions. We assume that advertising is a complement to consumption in observable consumer preferences as in the complementary paradigm of advertising characterized by Bagwell (2007) where advertising is a direct argument in consumer utility. The complementary paradigm is best understood in the household production framework following Stigler and Becker (1977) where advertising is combined with the purchase of a physical market good to produce non-market items of underlying value to the consumer, such as satisfaction, health, or prestige. Bockstael and McConnell (1983) have demonstrated the validity of using traditional economic welfare concepts based only on observable market behavior when household production is unobservable. Further, Kőszegi and Rabin (2008); Bernheim and Rangel (2009), and Just (2011) show that such economic welfare concepts have validity even when demands are based on mistakes and misjudgments such as might be due to persuasive advertising. While these concepts have not been previously integrated into the advertising literature, they can rationalize the Dixit-Norman framework where market demands are altered by advertising while admitting a stable set of underlying preferences by which economic welfare concepts and the social optimum are well-defined. We assume that advertising is rationed (provided in seller-determined amounts) rather than sold at a price (in consumer-determined quantities) as differentiated by Becker and Murphy. Although clear cases of consumer-chosen advertising exist, they typically involve bundling, such as in newspapers or attendance at sporting events, where making independent choices may not be possible for consumers, and consumer prices of advertising are either unavailable or extremely difficult to untangle from prices of other bundled goods. By assuming seller-determined advertising, our approach enhances clarity and specificity of preference properties and their social welfare implications, which are considered only in summary differential form by Becker and Murphy, e.g., in their Eqs. (5)-(14). By basing our model only on advertising expenditures, our framework lends itself to simple estimation with common data availability (market prices, quantities, and advertising expenditure data). While feasibility of the variety of conditions delineated in our results may not be readily apparent, we propose a relatively simple utility specification that obeys standard properties while representing and differentiating all of the unusual cases. Section 2 develops benchmark cases. Section 3 presents the major result and then various special-case results that build on earlier results in Becker and Murphy (1993) and Bagwell (2007). While the monotone comparative static approach yields results more elegantly and general when it is applicable (Milgrom and Shannon 1994), we examine comparisons that do not lend themselves to standard comparative static analysis. Our strategy in such cases is to consider an arbitrary path that connects equilibrium solutions of interest which mimics the empirical approaches of Bresnahan (1989) and Just and Chern (1980). Section 4 proposes a preference specification that is plausible for empirical possibilities, but general enough to admit all of the special cases of Section 3.",
41,3,Journal of Economics and Finance,25 May 2016,https://link.springer.com/article/10.1007/s12197-016-9362-x,Effects of Bush Tax Cut and Obama Tax Increase on corporate payout policy and stock returns,July 2017,Andre C Vianna,,,Male,Unknown,Unknown,Male,"Firms can set different strategies for their payout policy, using dividend payments and stock repurchases as ways of sharing profits to their shareholders. Stocks repurchases are usually an opportunity for firms to have some control of the return of their shares, since buying them back can affect the market expectations about future stock price levels. Dittmar and Field (2015) show that, after controlling for risk factors, repurchasing firms earn positive returns. Dividend payments, on the other hand, are the instruments most commonly used by companies who wish to signal to the investor that they can make enough profit to distribute profits and, at the same time, have reached a safe and mature level of growth. The choice of the best corporate policy depends on the firm’s strategy. Fama and French (2001) show that dividend payment decision is related to the characteristics of the firm. Besides that, even after controlling for those firm characteristics, they show the existence of a long-term declining trend in dividend-paying firms. This paper analyzes two important U.S. tax reforms’ implementation effects on the dividend payment and the stock repurchase policies. Those tax reforms are the Jobs and Growth Tax Relief Reconciliation Act of 2003 (JGTRRA 2003, also known as Bush Tax Cut) and the American Taxpayer Relief Act of 2012 (ATRA 2012, also known as Obama Tax Increase). On May 28, 2003, President George W. Bush signed the JGTRRA 2003, which was later extended by the Tax Increase Prevention and Reconciliation Act of 2005 and by the Tax Relief, Unemployment Insurance Reauthorization and Job Creation Act of 2010. The main modification made by this tax law is the reduction of the maximum statutory personal tax rate on dividends from 38.1% to 15%. On January 2, 2013 President Barack Obama signed the Act of 2012 after a long battle with the Congress. The most important adjustment by this tax reform was the increase in the dividend tax rate for the top income tax payers – single filers who exceed the threshold of $400,000, heads of households above $425,000, joint filers beyond $450,000 and estates and trusts with more than $11,950 – from 15 to 20%. The U.S. government faced different challenges when President George W. Bush cut dividends taxes in 2003 and when President Barack Obama signed the dividend tax increase in January 2013. Dubay (2013) states that, in 2003, lower interest rates were not generating economic benefits fast enough, and that was the reason for the Bush Tax Cut. In concerns to the Obama Tax Increase, the U.S. government faced several fiscal challenges in 2012, as well as income inequality concerns. Hungerford (2013) shows a 15% increase in income inequality among U.S. tax filers between 1991 and 2006: the largest contributors to that increase are the changes in income from capital gains and dividends. Huang and Marr (2012) affirm that the top 1% of taxpayers would receive 71% of all capital gains in that year, while the bottom 80% would receive 6% of all capital gains. Thus, one can argue that both tax reforms have advantages and disadvantages in terms of income inequality versus tax benefits: the Bush Tax Cut boosted dividend distribution through a tax relief that makes dividend payments more attractive and helps to improve economic growth, even though income inequality rose in the following years, while Obama Tax Increase focuses on reducing this income inequality from the previous years by raising dividend tax rate only for the top-income tax payers – who receive most of the capital gains income – as a tool to help redistributing economic welfare among taxpayers. The main questions in this paper are: a) how do dividend tax rate change effects on corporate payout policy compare between the Obama Tax Increase and the Bush Tax Cut? b) In the announcement windows in which those laws are passed, how quickly do those effects dissipate? c) How do the dividend tax rate changes, controlling for dividend payments and stock repurchases, affect stock returns? The goal of this paper is to perform a study around announcement periods in order to quantitatively compare the effects of Bush Tax Cut and Obama Tax Increase on dividend payments, stocks repurchases and stock returns. This study analyses 18,229 firms listed on the NYSE, Amex and Nasdaq from January 1990 to June 2014. Financial firms and utilities are excluded, as done by similar studies, to avoid noise in the regressions from those firms’ regulatory issues. Panel-data logit regressions measure the effect of the independent variables – especially the dividend tax rate – on dividend payment and stock repurchase decisions. Panel-data fixed-effect regressions quantify the relationships between the studied variables and the dollar amounts paid in dividends and spent in stock repurchases. The hypotheses of this paper are: 1) both tax reforms’ implementations had short-lived effects on corporate payout policy and stock returns, since markets should rapidly adjust; 2) Controlling for dividend payments and stock repurchases, stock returns are more affected by the Bush Tax Cut than the Obama Tax Increase, since the latter was not as large as expected by the market. works as a null hypothesis that will be rejected if there is any evidence that a tax law passage has persistent effects on dividend payments, stock repurchases and stock returns. Hypothesis 2 states that stock returns receive a larger impact from the Bush Tax Cut than from the Obama Tax Increase. Although a similar comparison on these two tax law reforms has not been done yet in the literature to knowledge, the second hypothesis is based upon Yousuf (2013) who reports that fund strategists expected a higher tax hike and are relieved because dividend stocks will still be attractive compared to other alternative investments. Moreover, both hypotheses are based upon Jagannathan et al.’s (2000) evidence that dividend payments and stock repurchases are used at different times from one another and that repurchases are very pro-cyclical. The results show that these dividend tax law passages are more persistently correlated with dividend payments than with stock repurchases and also confirm the hypothesis that, in absolute terms, stock returns are more affected by the Bush Tax Cut (+1.3% in the shorter event window) than the Obama Tax Increase (−0.75% in the shorter event window). Moreover, larger firms are more likely to pay dividends than to repurchase stocks in the quarters immediately after the dividend tax reform implementation. The evidence suggests that less solvent firms persistently spend larger dollar amounts in stock repurchases, especially in the Bush Tax Cut (+1.11 % per solvency ratio percentage). Finally, the results show that the previous findings are robust for different firm sizes (small, mid, and large cap). The rest of the paper proceeds as follows. Section 2 outlines the previous literature. Section 3 describes the data and the methodology. Section 4 reports the regressions results, while section 5 provides robustness checks for different firm sizes. Finally, section 6 provides the conclusion of the paper.",
41,3,Journal of Economics and Finance,26 May 2016,https://link.springer.com/article/10.1007/s12197-016-9363-9,The real miss-specification in the forward rate premium puzzle,July 2017,Amit K. Sinha,Philip A. Horvath,Robert C. Scott,Male,Male,Male,Male,"A considerable amount of time and energy has been devoted to the study the forward-spot premium relationship (Fama 1984; Fama and Bliss 1987; Baillie and Bollerslev 1994 and 2000; Maynard and Phillips 2001; Baillie and Kilic 2006; Sarno et al. 2006; Sakoulis et al. 2010; Hall et al. 2013; Ho and Mo 2015; among others). Prior to Fama (1984), the prevalent view as summarized in Shiller et al. (1983) was, “The simple theory that the slope of the term structure can be used to forecast the direction of future changes in the interest rate seems worthless.” Since Fama’s (1984) observation of predictive power in the one-month forward-spot regression relationship, the literature has reported contradictory findings. While some have found support (Fama and Bliss 1987; Mishkin 1988; Gerlach 1995; Davis 2000; Fama 2006; Brooks et al. 2012, among others) others have not (Hodrick and Srivastava 1986; Froot and Frankel 1989; Frankel and Poonawala 2010; Hall et al. 2013; Ho and Mo 2015; to name a fewFootnote 1). Consequently, a significant number of manuscripts attempt to explain this conundrum. The schools of thought most commonly applied to explain the forward-spot anomaly are, (a) use of multiple structural breaks, (b) non-linearity, (c) fractional integrated process, and (d) time-varying coefficients. While Sakoulis et al. (2010) explain forward premium puzzle by allowing for multiple structural breaks, Ho and Mo (2015) argue against such models’ ability to explain the forward premium puzzle. Although, Baillie and Kilic (2006), and Sarno et al. (2006) may not fully resolve the forward-spot premium puzzle, they argue that consideration of non-linearity in the forward spot relationship may provide some understanding and insight. Baillie and Bollerslev (1994 and 2000), and Maynard and Phillips (2001) argue that a fractionally integrated forward discounting process and stationary exchange rate returns may explain the forward premia puzzle. Hall et al. (2013) suggest a time-varying-coefficient approach to estimate bias-free coefficients, Frankel and Poonawala (2010), do not feel time-varying exchange risk premium to be an explanation for the bias. Some manuscripts, for example Ho and Mo (2015) and Hall et al. (2013), have also suggested econometric techniques to correct for misspecification. Clearly, there is no consensus about the causes and remedy for the existence of the forward-spot premium puzzle. This paper adds to the literature as it takes another look at the forward-spot modeling relationship. Using simulated data, we find surprising resilience in the forward-spot relationship for the short-maturity government bond rates. Given the simulated data and with its inherent lack of any economic relationship, by construction and design of the study, the similarity of the value of the coefficients and their statistical significance to Fama’s (1984) results, either provides unquestionable support to the modeling process, or perhaps identifies the modeling process itself as the sources of the forward-spot premium puzzle. In section 2 we provide the theoretical basis for our manuscript. Section 3 has our illustrative example and discussion about our bootstrapped results. We continue our model discussion and provide the implications for the long-maturity forward-spot relationship in section 4. Section 5 concludes.",
41,3,Journal of Economics and Finance,07 June 2016,https://link.springer.com/article/10.1007/s12197-016-9364-8,"Animal spirits, beauty contests and expected returns",July 2017,Jukka Ilomäki,,,Male,Unknown,Unknown,Male,"Keynes (1936) introduced the beauty contest phenomenon into the financial markets. He notes that a rational investor is interested in other investors’ beliefs about the returns of the asset, other investors’ beliefs about these beliefs, and so on. Keynes argues that the beauty contest is the combination of market psychology and rational higher-order beliefs. We connect both of them in the rational choice model. We have short-lived risk averse (CARA) investors with asymmetric information in the overlapping generations (OLG) framework. Short-lived investors can be motivated by performance based arbitrage, where the capability of investing is measured by short period intervals (Shleifer and Vishny 1997). Our model captures the idea of Shiller (2014) that uninformed investors trade on animal spirits component in their pricing and informed investors cannot observe this directly. Thus, the Keynesian market psychology stands for animal spirits in this study. Correlated animal spirits can be motivated so that the uninformed investors can herd with common heuristic but that heuristic dependence can be fragile (Hirshleifer and Teoh 2003). We find that the animal spirits component reduces expected returns for investors even though the uninformed ones coordinate with their animal spirits component. In our analysis, stock markets follow P

t
 = V

t
 + C

t
, where P

t
 is the equilibrium price, V

t
 is the fundamental value and where C

t
 is an additional component describing all errors when P

t
 ≠ V

t
 with P

t
 ~ I(1) (i.e. non-stationary), V

t
 ~ I(1) and C

t
 ≠ 0. Section 2 presents the benchmark equilibrium prices. Section 3 presents the model and Section 4 the theoretical analysis of the behaviour of the model. Section 5 concludes.",1
41,3,Journal of Economics and Finance,01 July 2016,https://link.springer.com/article/10.1007/s12197-016-9366-6,Effects of derivatives usage and financial statement items on capital market risk measures of Bank stocks: evidence from India,July 2017,Gaurango Banerjee,Abhiman Das,Shekar Shetty,Unknown,Unknown,Unknown,Unknown,,
41,3,Journal of Economics and Finance,18 August 2016,https://link.springer.com/article/10.1007/s12197-016-9367-5,Bank risk in a decade of low interest rates,July 2017,Yen-Ling Chang,Daniel A. Talley,,Unknown,Male,Unknown,Male,"During a lengthy period (i.e. 2008–2014) of low interest rates, the primary venue for banks to generate revenues, the traditional loan business becomes less profitable. Therefore, banks as any other corporation have to find alternatives to maintain acceptable level of returns in order to satisfy their clients and stakeholders. Since a low interest rate regime remains in place in the U.S. after the Financial Crisis of 2008 and banks nevertheless need to find ways to boost the economic value of the shareholders, would it be possible for banks to stay the same course and pursue risky assets or investments regardless of the fact that regulators imposed restrictions on banks asset portfolio formation during and after the Financial Crisis? The data in Fig. 1 suggest this possibility. Therefore, we hypothesize that banks still engage in both highly risky yet profitable investments and services even after the Financial Crisis of 2008 while the interest rate remains low. We suspect that banks still have strong incentives to undertake profitable yet riskier activities and investments in order to improve their performance (Fahlenbrach et al. 2012
). Duchin and Sosyura (2014) analyze TARP banks and non-TARP banks and show that TARP banks shifted credit originations to higher risk yet profitable loans after they received financial aid from the government. This research examines publicly available bank data to see if banks seek higher yield by providing fee-based services such as loan commitments, credit substitutes, and other services such as trusts. Pattern of macroeconomic and bank level variables from 200q1 through 2014q3. A. Pattern of GDP growth from 2003q1 through 2014q3 B. Pattern of 10-year Treasury bond interest rate from 2003q1 through 2014q3 C. Pattern of on-balance-sheet activities for different sizes from 2003q1 through 2014q3 D. Pattern of off-balance-sheet activities for different sizes of banks from 2003q1 through 2014q3 E. Pattern of return on assets for different sizes of banks from 2003q1 through 2014q3 Stiroh (2006a, b) use publicly traded U.S. bank-holding company data to examine the determinants of risk and analyze how they have evolved. Using the standard deviation of equity return as the proxy for risk, the results show that balance sheet activities, such as commercial and industrial loans and consumer lending, as well as income statement items, such as other noninterest income, drive the cross-sectional differences in bank holding company risks. Sources of risk have changed since banks shifted their business model from deposit-driven activities to fee-based activities after 2001. The results also indicate that income statement activities (items) become more important than balance sheet items as determinants of risk. In particular, the non-interest income and expense items generated by fee-based activities, i.e. off-balance-sheet activities, have drawn more attention by the banking community in the last decade. Several papers have discussed the impact of off-balance-sheet activities on bank performance and different kinds of risks, including systemic risk, total risk, and market risk. An early study by Allen and Jagtiani (2000) finds that the benefit of diversification by expanding bank business into nontraditional business might not be large enough to offset the increase in systemic risk and market risk. Stiroh and Rumble (2006) use U.S. financial holding companies to examine whether engaging in more off-balance-sheet activities helps bank diversify risk. Their results indicate that the benefit of diversification has been diminished and offset by more risk exposure to unstable noninterest earnings generated by those fee-based activities. Calmés and Théoret (2010) use an ARCH-M model to examine Canadian bank data and confirm that noninterest income generated by off-balance-sheet activities no longer impacts bank returns positively. It turns out the surge in off-balance-sheet activities eventually increases bank risk. DeYoung and Torna (2013) analyze the relationship between bank failure and noninterest income activities. They further decompose noninterest income businesses into nontraditional stakeholder activities, fee for services activities, and traditional banking activities such as depositor services and fiduciary services. Their analysis suggests the likelihood of bank failure is much higher for already financially distressed banks engage in stakeholder activities such as underwriting, investment banking, venture capital and other investments that may require banks to hold risky assets. On the other hand, Ziadeh-Mikati (2012) uses U.S. commercial bank data to examine the impact of off-balance-sheet activities on bank failure and risk exposure. Their study specifically separates off-balance-sheet activities into three different categories including credit substitutes, credit derivatives and derivative contracts. The results show that banks engaging in more credit substitutes have a lower degree of volatility on return on assets (ROA), higher quality of assets, and lower credit risks.Footnote 1 A recent study by Papanikolaou and Wolff (2014) documents the impact of off-balance-sheet activities as part of the risk-weighted capital ratio on overall banking risk. According to their finding, off-balance-sheet leverage is negatively linked to the soundness of the banking system as a whole before and after the Financial Crisis of 2008. Although early literature documents inconclusive results on whether off-balance sheet activities (fee-based activities) increase or reduce bank risk, recent studies show a positive relationship between off-balance-sheet activities and bank risk. Furthermore, most studies conduct a unidirectional test, that is, they assume off-balance sheet activity is an exogenous factor to be included in the regression model. This treatment of off-balance sheet activity ignores the fact that sometimes banks use off-balance sheet activities (or services-based activities) to enhance or improve bank profitability when focusing on loan originations or other on-balance-sheet activities that do not adequately increase bank profit. That is, the motivation to increase off-balance-sheet activities could be partly due to falling revenues on interest-bearing assets over a period during which interest rates first fall and later are stable but at a very low level. We take a different approach to allow a contemporaneous relationship between off-balance sheet activities, which are the primary sources of non-interest income, and interest rates while accounting for other relevant bank variables. We argue that the primary consideration for a bank to engage in more off-balance-sheet activities depends on economic conditions that affect the ability of a bank to generate revenues. During an economic boom, a bank is more than willing to make loans to individuals and businesses in anticipation of low default risks and higher payoffs. However, during a slow economy, firms reduce investment at the same time that banks require stricter credit standards (Ivashina and Scharfstein 2010
). Banks also find themselves scaling down the amount of new loans. Furthermore, it might be more costly for banks to make loans, for example if the cost of taking deposits is not recovered when interest rates are being held low by the monetary authority. The most closely related prior study is Delis and Kouretas (2011). They conclude that lower interest rates are associated with increasing risk-taking behavior using European bank data. Their paper also suggests that the impact of lower interest rate on holdings of riskier assets was diminished for banks with higher equity capital and amplified for banks with higher off-balance-sheet items. One possible reason is that banks have to develop business other than traditional banking activities in order to generate extra revenues to satisfy their shareholders. Under this scenario, banks naturally engage in more services-based activities such as providing standby credits, letters of credit, loan commitments and financial derivative contracts that are not for hedging purposes. Our main research interest centers on the investigation of the relationship between off-balance sheet activities and interest rates and the impulse response relationship among all dependent variables. Our paper differs in two important ways from Delis and Kouretas (2011). First of all, we use risk-weighted off-balance-sheet items and risk-weighted on-balance-sheet items as proxies for bank risk-taking behavior.Footnote 2 Secondly, our model addresses endogeneity by using Panel VAR in addition to the Dynamic GMM model for panel data used in their paper. Delis and Kouretas (2011) address the issue of the dynamic nature of bank risk-taking and how the potential endogeneity of some of the control variables may affect the results by using a dynamic GMM model for panel data (Arellano and Bond (1991); Arellano and Bover (1995); Blundell and Bond (1998)). Following a similar methodology in the first part of the paper, the results of our model are remarkably similar results to theirs. Specifically, we find that both bank size and 10-year bond rates have a significant and negative relationship with bank risk behavior while economic growth frequently shows a positive relationship with bank risk behavior. However, a dynamic GMM model for panel data does not fully consider endogeneity of some of the control variables such as off-balance-sheet items, on-balance-sheet items, interest rates, and economic growth. Our paper addresses this issue by employing a Panel Vector Autoregression (Panel VAR) model. This model allows us to take advantage of the rich information embedded in the cross-sectional and time-series data while examining the interaction among the included endogenous variables. Ivashina and Scharfstein (2010) investigate bank lending behavior and suggests that a possible reason that the rise in commercial and industrial loans during the Financial Crisis of 2008 (documented by Chari et al. 2008) is partly due to an increase in the drawdown of revolving credit facilities. Therefore, an increase in on-balance-sheet items in this period may be caused by a decrease in off-balance-sheet items in the previous quarter. That is, risk has transferred from off-balance-sheet activities to on-balance-sheet activities and accordingly affects bank profitability. Hence, we find it proper to separate bank risk-taking behavior into on-balance-sheet risk taking and off-balance-sheet risk taking and examine how they simultaneously interact with lower interest rates during different economic conditions. Using a dynamic model for panel data, our empirical results strongly suggest that banks, on average, engage in more off-balance-sheet activities during a period of falling interest rates when controlling for economic conditions and other bank level variables. The results obtained by the Panel VAR model are similar in that off-balance-sheet activities negatively respond to a change in lagged 10-year T-bond rate between 2008q2 and 2010q4, suggesting banks increase their off-balance-sheet activities in response to falling interest rates. The negative impact is persistent for more than 12 quarters. We further break down all banks into four different groups by total assets and examine how each group responses to falling interest rates. On one hand, larger banks from Groups 2, 3 and 4 respond negatively to falling interest rates subsequently and remain in negative territory for more than 12 quarters. On the other hand, the smallest banks that form Group 1 have almost no response to falling interest rates contemporaneously and subsequently. The different response suggests that larger banks are able to act more quickly in response to a less hospitable business environment than smaller banks. Yet larger banks also have higher risks since 2008 when businesses have faced considerable economic uncertainty. The remainder of the paper is organized as follows. Section 2 introduces the econometric model. We discuss the rationale behind the model specification and possible limitations of the model associated with the nature of the banking data. Section 3 presents empirical results and provides a discussion of insights from the results. Section 4 discusses some relevant policy implications and concludes the paper.",2
41,3,Journal of Economics and Finance,25 August 2016,https://link.springer.com/article/10.1007/s12197-016-9369-3,Searching for rational bubble footprints in the Singaporean and Indonesian stock markets,July 2017,Gilbert V. Nartea,Muhammad A. Cheema,Kenneth R. Szulczyk,Male,Male,Male,Male,"Since the 1970s, both the Singaporean and Indonesian stock markets witnessed several periods of price inflation, followed by sharp drops. Many consider these cycles of boom and busts as evidence of asset bubbles. Bubbles are characterized by an extended inflation in stock prices or returns that attain a peak, and then suddenly crash. The popular press usually attributes bubbles to irrational investors’ behaviour as herding, fashions, or fads drive their expectations and their irrational optimism (Schiller 2000). However, some bubbles could develop entirely from rational behaviour of investors. Rational speculative bubbles could continue growing although investors know a bubble had formed as long as the probability of a continued gain exceeds the probability of a loss.Footnote 1
 McQueen and Thorley (1994) showed that rational bubbles possess negative duration dependence. This means that the conditional probability of a positive run of returns ending, falls with the length of the run. As a rational bubble inflates, unforeseen price changes originate from unanticipated changes in the fundamental value of the stock and unexpected changes in the bubble. As a bubble continues growing, it begins dominating the fundamental component. Hence, a negative shock to the fundamental component would have little impact on total returns. Thus, the bubble continues growing despite these shocks, and the larger the bubble inflates, the less effect a negative shock would have on total returns, hence yielding the negative duration dependence. The bubble continues growing until a significant negative innovation bursts the bubble. We search for rational bubbles in Indonesia and Singapore because Indonesia is the largest economy in the ten-nation ASEAN block, while Singapore is ASEAN’s financial centre.Footnote 2 Singapore has traditionally enjoyed the biggest share of foreign direct investments (FDIs) in the ASEAN, followed by Indonesia. The presence of bubbles in these economies has important implications on market efficiency, allocation of resources, and asset pricing. A collapsing bubble inflicts damage on an economy as it can trigger capital flight, a depreciating currency, and rising unemployment. Thus, bursting bubbles could lead to instability in the economy and spread to other countries via contagion. Prior research has tested for the presence of bubbles in Singapore and Indonesia but the results are mixed and contradictory. Sarno and Taylor (1999) use cointegration tests and report the presence of stock market bubbles in both Singapore and Indonesia over the period from 1988 to 1997. In a more recent study, Ahmed et al. (2010) employ regime-switching tests and also report the presence of non-linear speculative bubbles in both Singapore and Indonesia over the period from 1990 to 2006. In contrast, using duration dependence tests, Rangel and Pillay (2007) report the absence of rational bubbles in Singapore over the period from 1975 to 2007, while Yu and Hassan (2009) also using duration dependence tests report the absence of bubbles in Indonesia over the period from 1992 to 2003.Footnote 3 Though the sample periods for the duration dependence tests of Rangel and Pillay (2007) and Yu and Hassan (2009) encompass the 1997 Asian Financial Crisis (AFC), they both miss the Global Financial Crisis (GFC) of 2007–2008. This is significant since if their sample period contained only one bubble episode this could severely affect the power of their duration dependence tests (Chan et al. 1998). In this study, we employ a mix of descriptive statistics, explosiveness tests and duration dependence tests to search for evidence of rational speculative bubbles in the Singaporean and Indonesian stock markets between 1970 and 2013. Hence our data set includes at least two alleged bubble episodes – the AFC and the GFC, unlike in Rangel and Pillay (2007) and Yu and Hassan (2009) whose sample period only encompass the AFC. Consequently, our duration dependence tests should have more power than theirs. In addition we use an extended sample period that is 120 (520) months (weeks) longer than Yu and Hassan’s (2009) and 75 (331) months (weeks) longer than Rangel and Pillay’s (2007) which also adds to the power of our tests. The two alleged bubbles and longer sample period would strengthen the confidence of our findings and help resolve the conflicting results of previous studies. The duration dependence test for rational bubbles is superior to the other tests since negative duration dependence is unique to rational bubbles (Mcqueen and Thorley 1994). However Harman and Zuehlke (2004) and Lehkonen (2010) have recently questioned the efficacy of the duration dependence test by suggesting that the test results are sensitive to data frequency. We also test for the practical efficacy of the duration dependence test in our study by using both weekly and monthly returns in our tests.Footnote 4 Both Rangel and Pillay (2007) and Yu and Hassan (2009) conducted their duration dependence tests only on monthly returns. There is currently no cogent theory of how bubbles develop and then burst (Chan et al. 1998). Innocuous events or a major incident such as a terrorist attack could trigger steep declines in equity markets (Ramiah and Graham 2013). Furthermore, institutional investors could spark a selloff by systematically using signals from insider trading as they decipher poor earnings reports or problems before the public finds out (Wang 2011). Finally, managers with their earnings tied to stock prices may inflate their companies’ earnings that boost stock prices higher while they sell and profit from greater insider trading (Huddart and Louis 2007). Once the correction starts, the bubble burst and stock prices begin plummeting.Footnote 5
 Researchers developed four categories of bubble tests in the literature. For the first category, researchers examine the descriptive statistics of return distributions to determine the presence of a bubble. As a bubble develops, the return distributions show autocorrelation, skewness and kurtosis. The long run up in prices characteristic of bubbles means returns are positively autocorrelated. The unusually large negative returns that occurs when the bubble bursts also means that the return distribution is negatively skewed and leptokurtic (i.e., have fat tails). McQueen and Thorley (1994) also indicate that as the bubble grows, the mixing of low variance distributions associated with small bubbles with high variance distributions as the bubble grows results in fat tails in return distributions. Nevertheless, these tests are limited because factors other than bubbles could induce autocorrelation, kurtosis, and skewness. Time-varying risk premiums (Fama and French 1988) and fads (Poterba and Summers 1988) could also cause autocorrelation. Finally, asymmetric fundamental news could induce skewness while the flow of information into the market can create leptokurtosis (Tauchen and Pitts 1983). For the second category, researchers use the explosiveness test to detect a bubble’s presence (Chan et al. 1998). This test examines a particular bubble footprint – explosiveness of returns that cause local price peaks of suspected bubble episodes. This analysis becomes anecdotal because this test only uses portions of the data set. Consequently, researchers should use this test in conjunction with other tests. For the third category, researchers have applied cointegration analysis and regime-switching tests to test for price deviation from fundamentals. In theory, the fundamental variable such as stock prices, dividends, and earnings would be an I(1) process and thus be cointegrated. Consequently, stock prices would grow as a multiple of dividends or earnings but a growing bubble would obscure the long-term relationships between stock prices, dividends, and earnings. Cointegration analysis could however, have trouble detecting bubbles because it is effectively a joint test of correct model specification and the existence of a bubble (Brooks and Katsaris 2003). Moreover, if a bubble periodically collapses, then the cointegration tests could fail to detect it (Evans 1991). Finally, the lack of cointegration is not a sufficient condition for proving the existence of bubbles because other factors could weaken the cointegration between stock prices and fundamental variables such as large and persistent shocks (Johansen 1991) and changes in economic regimes (Chow 1998). Finally, the cointegration test possesses little power if researchers use short time spans (Brooks and Katsaris 2003, Pierse and Snell 1995). Another test of price deviation from fundamentals involves the use of the regime-switching test of Hamilton (1989) as applied in Engel and Hamilton (1990) and Van Norden and Schaller (1993) to examine trends in time series and switches in trends. Ahmed et al. (2006) and Ahmed et al. (2010) use this methodology to test for bubbles by first estimating a vector autoregressive model (VAR) as a measure of the presumptive fundamental. The residuals from the resulting VARs are then used for the bubble test, i.e., test for the absence of movements away from fundamentals. This approach however suffers from the same problem as cointegration tests, that of the misspecified fundamental. For the last category, McQueen and Thorley (1994) developed the duration dependence test that overcomes the many restrictions of traditional bubble tests. First, the test analyses a unique characteristic of rational bubbles – negative duration dependence, which means the conditional probability of a positive run of returns ending continually falls given the run has made it to the current period. Second, unlike cointegration tests, researchers do not need to specify the relationships in the model, and this model overcomes the problem of non-linearity by allowing the parameters to vary. Finally, the time series being investigated does not need to be normally distributed for the duration dependence test.",5
41,3,Journal of Economics and Finance,25 August 2016,https://link.springer.com/article/10.1007/s12197-016-9368-4,An economic analysis of the trial penalty: a comparative analysis of three alternative trial settings,July 2017,Christopher J. Boudreaux,,,Male,Unknown,Unknown,Male,"Many scholars believe that there is a trial penalty for those who decide to invoke their sixth amendment right to a trial (Rubinstein and White 1978; Brereton and Casper 1982; Holmes et al. 1992; Dixon 1995; Johnson 2003; Ulmer and Bradley 2006). Jury sentencing supports the existence of a trial penalty if the sentence length is more severe than comparable cases under guilty pleas and bench trials (King and Noble 2004). Some studies find the existence of a trial penalty (Ulmer et al. 2010; Ulmer and Bradley 2006; King and Noble 2005; Johnson 2003). On the other hand, Breen (2011) finds the exact opposite; judges impose tougher sentences on defendants than jury trials in military courts. Using data on murder cases in 33 large urban counties, this article finds evidence of a substantial trial penalty. The article uses Tobit regressions with a random effects design to control for idiosyncratic errors (i.e., unobserved differences) by county and state as well as the controlling for the lower bound of zero sentence lengths. The findings in this article show that some defendants might face differential treatment under alternative trial settings. For example, defendants convicted by juries face an 11-year trial penalty on their sentences over plea bargains. Further, we find that judges might treat defendants with prior convictions and gang affiliations more fairly than juries, since juries are more likely to be swayed emotionally in court. These findings could be particularly useful to defense attorneys who could use them to make better choices between plea bargains and jury and bench trials. Plea bargaining, the practice of pleading guilty to a less severe charge, is a very popular outcome in the criminal justice system. The research on plea bargaining commonly estimates that 90 % of all convictions in the criminal courts are the result of guilty pleas (Alschuler 1981). A study in 1962 on 132 state county courts showed that 70 % of cases were decided by plea bargaining. The same study also found 73 % of defendants in US district courts in 1967 chose guilty pleas (Landes 1971). For the data analyzed in this paper, the percentage of convictions that result from plea bargains is 54 %. One major concern with plea bargaining is that innocent defendants might plead guilty. This fear often sparks a heated debate (Alschuler 1981). The issue is further exacerbated if one considers risk averse agents. These agents are more likely to accept plea bargains because the risk of being convicted and facing a larger sentence is not worth the gamble, even if the probability of conviction is small. Zeisel (1980) supports this incentive scheme by showing that the sentences of New York City defendants convicted at trial were 136 % more severe than the plea bargains the prosecutors had proposed to the same defendants. Defendants have incentives to plead guilty, even when innocent, under certain circumstances because a prosecutor might offer a deal that reduces the potential sentence of the trial to such a degree that it is almost too good to pass up (Bar-Gill and Ayal 2004). Proponents of plea bargaining cite the need for this system as a way to successfully navigate through the slew of trials the court face. The guilty-plea system has grown as a product of circumstances, not by choice. Today there is an administrative crisis in criminal courts largely due to the increasing volume of crime in recent decades, the regulation of human activity that were formerly beyond the scope of criminal law, and the substantially increased length of the average felony trial (Alschuler 1976). This process unambiguously lightens the workload of judges, prosecutors, and defense lawyers but does not necessarily mean the is acceptable. After all, the United States Constitution provides the right to a trial by jury, and some believe that a plea bargain is a mechanism that eliminates this right because it increases the costs of the trial (Lynch 2003). The other widely cited issue is the preservation of resources such as time, money, and even the effort by those involved in the judicial process. However, while saving resources is a top priority for the courts, this justification of plea bargaining raises difficult constitutional problems (Grossman and Katz 1983).Footnote 1
 The remainder of the article is organized as follows: Section 2 presents the case of Bordenkircher v. Hayes. Section 3 provides the motivation behind the importance of analyzing plea bargaining. Section 4 describes the data. Section 5 addresses the question of whether there is a trial penalty. Section 6 explores the conviction outcomes of defendants under the three alternative trial settings, and Section 7 provides concluding remarks and future possible research extensions.",
41,3,Journal of Economics and Finance,23 September 2016,https://link.springer.com/article/10.1007/s12197-016-9371-9,"Market share, firm innovation, and idiosyncratic volatility",July 2017,Frederick Adjei,Mavis Adjei,,Male,Female,Unknown,Mix,,
41,3,Journal of Economics and Finance,28 November 2016,https://link.springer.com/article/10.1007/s12197-016-9378-2,An examination of the REIT return–implied volatility relation: a frequency domain approach,July 2017,Emmanuel Anoruo,Vasudeva N. R. Murthy,,Male,Unknown,Unknown,Male,"This paper seeks to explore the dynamics between implied volatility (the VIX) and REIT returns using the frequency domain approach. The development of the behavioral finance field has given rise to the notion that investors are not perfectly rational in making investment decisions. For instance, Kaniel et al. (2008) argue that the irrational behavior of investors coupled with their sentiment-based investment decisions contribute to the volatility in the stock markets. Similarly, De Long et al. (1990) have shown that investor sentiment plays significant role in determining stock price volatility. The VIX as a measure of implied market volatility was introduced in 1993 by the Chicago Board of Options Exchange (CBOE). It is often referred to as investors’ fear barometer or sentiment. The conventional wisdom holds that an increase in implied volatility is associated with fear in the financial markets. On the other hand, a decrease in implied volatility indicates complacency. Simply put, low readings of the VIX index suggest that investors are optimistic and complacent indicating that their perception of risk in the financial markets is minimal. On the other hand, high readings of the VIX index suggest that investors perceive significant risk and expect the market to move sharply in either positive or negative direction. A clear understanding of the return-volatility relationship is important because it has many policy implications for investments, security valuation and portfolio risk management. It is therefore, not surprising that a number of studies have been undertaken to examine the relationship between asset returns and implied volatilities in the literature. Most of the studies in the extant literature are predicated on the assumption that the relationship between asset returns and implied volatilities is asymmetric. For instance, Badshah (2013), Dennis et al. (2006), Fleming et al. (1995), Giot (2005), Hibbert et al. (2008), Low (2004), Whaley (2000) and Wu (2001) have examined the relationship between implied volatilities and asset returns in the context of asymmetry. The presence of asymmetry is generally interpreted as an evidence of inverse relationship between asset returns and implied volatilities. This statement suggests that negative changes in asset returns will have greater impact on implied volatilities than positive changes or vice versa. The leverage and the volatility feedback hypotheses are frequently used in the literature to explain the asymmetric relationship between asset returns and implied volatilities. The leverage hypothesis proposed by Black (1976) and Christie (1982) stipulates that negative returns cause firms with high debt-to-equity leverage to become riskier. This will in turn make the stock prices of these firms to be more volatile. On the other hand, the volatility feedback hypothesis, also known as the time-varying risk premium, posits that changes in conditional volatilities tend to have opposite effects on changes in stock prices (Campbell and Hentchel 1992, and French et al. 1987). Bekaert and Wu (2000) developed a framework that combined the leverage and feedback hypotheses. Wu (2001) using the Bekaert and Wu (2000) framework finds that both the leverage and volatility feedback effects do account for the existence of asymmetric volatility in the return-volatility relationship. However, in recent years, a number of studies have suggested that the two conventional hypotheses alone cannot be used to fully explain the existence of asymmetry in the return-volatility relationship. For instance, Avramov et al. (2006) maintain that trades by uninformed individual investors are partially responsible for the existence of asymmetry in the risk-return relationship. Similarly, Hibbert et al. (2008), and Han et al. (2012) have attributed the observed asymmetric volatility in the daily or higher frequency levels to the psychological bias of market participants. Hibbert et al. (2008) have examined the short-term dynamic relationship between the S&P 500 index return and changes in implied volatility at both the daily and intraday levels. They find that neither the leverage hypothesis nor the volatility feedback hypothesis alone was able to adequately explain the relationship between index returns and changes in implied volatility. They therefore, proposed a framework that takes into account the behavior of traders. Using their framework, Hibbert et al. (2008) find that implied volatility has a significant effect on both daily and intraday returns. It is important to point out that most of the earlier studies on the return-volatility relationship were undertaken in the context of equity markets. Only a handful of studies ― including Daigler et al. (2014), Cairns et al. (2007), Padungsaksawasdi and Daigler (2014) and Hibbert et al. (2008) investigated the return-volatility relationships for non-equity instruments such as currencies (i.e. the euro) and commodities (i.e. gold and oil). The present study therefore contributes to the literature by extending the debate on return-volatility relationship to the real estate market. In particular, the study explores the return-volatility relationship for REITs using the relatively new technique called, frequency dependent regression method proposed by Ashley and Verbrugge (2009) and the frequency domain causality tests developed by Breitung and Candelon (2006). The standard time domain techniques such as the Ordinary Least Squares (OLS) and the vector autoregressive models (VAR) ignore the possibility that the effects of the independent variables on the dependent variable may vary at different frequencies within the study period. However, the frequency domain methods enable one to assess the impact of independent variables on the dependent variable at different frequencies across the spectra. Specifically, the frequency dependent regression method allows the return-volatility relationship to vary across frequency bands. Similarly, the frequency domain causality test enables one to decompose the full causal relationships between the variables in the model into different frequencies. Hosoya (1991) and Granger and Lin (1995) have shown that the degree and direction of causality may differ across frequency bands. The present study finds that the implied volatility and REIT returns negatively influence each other at the low-, medium- and long-term frequencies. The investigation undertaken in this study also finds that causality runs from the implied volatility to all and equity REIT returns in the short- and medium-term frequencies. Furthermore, it finds that causality runs from mortgage REIT returns to the implied volatility in the medium term. Overall, the empirical evidence furnished in this study supports that the implied volatility has asymmetric effect on REIT returns. To the best knowledge of the authors, the relationship between implied volatilities and REIT returns has not been examined in the literature, using the method of frequency domain. The remainder of the paper is organized as follows. Following the present introduction, section 2 presents the literature review. Section 3 discusses the methodologies of the study. Section 4 presents the data and descriptive statistics. Section 5 discusses empirical results and Section 6 offers the conclusions of the study.",7
41,3,Journal of Economics and Finance,10 March 2017,https://link.springer.com/article/10.1007/s12197-017-9389-7,Preaching and politics: disentangling religiosity and political choice,July 2017,Brian L. Goff,Michelle W. Trawick,,Male,Female,Unknown,Mix,,
41,3,Journal of Economics and Finance,04 April 2016,https://link.springer.com/article/10.1007/s12197-016-9361-y,Foreign direct investment in the Dominican Republic: consequences and recommendations for sustainable growth,July 2017,Harri Ramcharran,,,Male,Unknown,Unknown,Male,"Several studies have examined the impact of international trade and foreign direct investment (FDI) liberalization policies implemented during the 1980s and 1990s on the economy of many developing countries; these include Santos-Paulino and Thirlwall (2004), Dollar and Kraay (2004) and Ocampo and Vos (2008). Other important studies, (United Nations 2005; “Equity and Development” World Development Report 2005), have reported extensive income disparity within some countries. Ocampo and Vos (2008) identify several factors that contribute to the internal disparity, they include: (a) slow structural change (shifting out of agriculture into manufacturing and services) that retards economic growth and labour productivity, (b) inadequate capital accumulation and allocation to new sectors much needed for economic transformation, (c) the concentration of FDI in “enclave” sectors, and (d) inadequate access to export market. These findings have important economic ramifications for countries aiming to achieve sustainable growth and poverty eradication. A significant portion of the FDI flows in the 1980s and 1990s was destined to developing countries that have established Export Processing Zones (EPZ) as a development strategy aimed at export-oriented growth, employment creation, and the generation of foreign exchange reserves.Footnote 1 The increasing scale of the operations of these entities is documented by Grunwald and Flamm (1985), and Romer (1993).Footnote 2 The Dominican Republic has developed an extensive system of industrial EPZ over the last 30 years. In response to the economic crises in the 1980s, the government implemented a series of economic stabilization reforms during the1990s; these are discussed in another section of the paper. The main objective of these policies is export-oriented growth based on FDI inflows to expand operations in the Free Trade Zone (FTZ). Table 3 shows the rapid increase in the number of firms, value added, capital accumulation, level of employment, foreign exchange generated and exports value. The main purpose of this paper is to empirically estimate the impact of the international trade and FDI liberalization policies on the economy of the Dominican Republic and discuss the ramifications of the results within the frame work of strategies to achieve sustainable economic growth. We use a production function methodology that enables us to analyze the production efficiency of the FTZ sector and the rest of the economy (ROE). The investigation procedure is four fold: (a) using a non-homogenous production function (NHPF) specification to estimate the production efficiency, measured by the output elasticity of capital (EK) and of labour (EL) and returns to scale (RTS), in the FTZ sector and of the ROE, (b) comparing the productivity parameters in other to identify the sources of efficiency/ inefficiency between the two sectors, i.e. analyzing the degree of technological dualism between the sectors, (c) identifying the presence/absence of economic linkages between these sectors, and (d) analyzing the degree of regional dualism and suggesting recommendations/proposals for improving the spatial interaction between these sectors to achieve sustainable growth. The model specification and estimation technique are discussed in another section of the paper. Additionally, we document other data/statistical analysis to substantiate our results. The results have important ramifications for current policies aimed at promoting sustainable growth. Serval reports have documented the conditions in the Dominican economy in the post-liberalization period and emphasized the need for economic transformation. The World Bank (1995) and the IMF (2001) report significant economic growth in the Dominican Republic during the 1990s, with the FTZ sector outperforming the rest of the economy (see Table 4). The World Bank (1995) also contends that although the FTZ sector has cushioned the economy temporarily from external shocks, it cannot propel the economy towards sustained economic growth, thus more drastic structural reforms will be necessary to increase productivity in other sectors of the economy. Strategies based on the World Bank’s recommendation require, inter alia, an in-depth knowledge of the underlying production characteristics (factor productivity and returns to scale) of the main sectors of the Dominican economy. The island now ranks fourth (behind USA, China, and Mexico) among the world largest EPZ economy. This study adds significantly to the current literature of the economy of the island. Current studies of the FTZ of the Dominican Republic are very descriptive, for example, Kaplinsky (1993) analyzing the process of transformation of commodity production to manufacturing, and Raynolds (1998) investigating the gender composition of the labor force. Our research interest is stimulated by some special studies of the impact of international trade and FDI on growth and development in other regions, for example, Krugman and Elizondo (1996), and Hanson (1996, 1998) on the maquiladora sector of Mexico and Fan and Scott (2003) on industrial agglomeration and development in East Asia and regions of China. Our methodology adds significantly to the empirically literature on this topic. The rest of paper is structured as follows: Section 2: Relevant literature, Section 3: Sectoral performance of the economy, Section 4: Methodology and Data, Section 5: Results Section 6: Implications of Results, Section 7: Challenges and Proposals; and Section: Summary and Conclusion",
41,3,Journal of Economics and Finance,21 June 2016,https://link.springer.com/article/10.1007/s12197-016-9365-7,Erratum to: Foreign direct investment in the Dominican Republic: consequences and recommendations for sustainable growth,July 2017,Harri Ramcharran,,,Male,Unknown,Unknown,Male,,
41,4,Journal of Economics and Finance,25 January 2017,https://link.springer.com/article/10.1007/s12197-017-9386-x,Improving (E)GARCH forecasts with robust realized range measures: Evidence from international markets,October 2017,Beatriz Vaz de Melo Mendes,Victor Bello Accioly,,Female,Male,Unknown,Mix,,
41,4,Journal of Economics and Finance,12 September 2016,https://link.springer.com/article/10.1007/s12197-016-9370-x,"Security issuance decisions, idiosyncratic risk, and macroeconomic dynamics",October 2017,Abdul Rashid,,,Male,Unknown,Unknown,Male,"Empirical evidence on the influence of risks on corporate security issuance decisions is rather limited. Most of the previous empirical studies have mainly focused on how risk associated with firms’ earnings affects firms’ leverage decisions (e.g., Lemmon et al. (2008), Baum et al. (2009), and Caglayan and Rashid (2014)).Footnote 1 In contrast, studies, such as Korajczyk and Levy (2003), Covas and Den Haan (2007), and Korteweg (2010), have largely examined security issuance decisions over the business cycle, rather than the effects of risk associated with the overall state of the economy. Therefore, we do not know how idiosyncratic and macroeconomic risk affects the change in firms’ leverage or their ability to use internally generated funds (i.e., retained earnings), debt borrowing, and equity financing. This paper investigates two questions related to the impact of risks on the capital structure decisions of UK manufacturing firms. In contrast to most of the prior studies, the paper examines how idiosyncratic and macroeconomic risk affects the change in leverage rather than the target leverage. Second, the paper examines the change in leverage with respect to changes in debt financing, equity financing, and retained earnings as it analyzes how each component responds to risk. By doing so, the paper not only shows the role of risk in external financing decisions of firms (debt borrowing and equity financing) but also provides evidence on how risk affects firms’ use of internally generated funds (the newly retained earnings). Hence, we can determine whether risk affects leverage through a reduction in debt financing, or through changes in equity financing, or the use of internal funds. The paper applies the two-step system GMM dynamic panel estimator on an annual unbalanced panel data covering the period 1981-2009 for publicly listed UK manufacturing firms. The results indicate that changes in firms’ leverage are negatively related to both idiosyncratic and macroeconomic risks. This finding implies that firms are likely to reduce their leverage when they operate in periods of heightened risk. In particular. we find that idiosyncratic risk affects the incremental changes in debt negatively, while it affects both the issuance of equity and the use of retained earnings positively. This implies that the negative sensitivity of a change in leverage to idiosyncratic risk arises not only because of a reduction in debt but also due to a rise in net equity financing and newly retained earnings. However, the results suggest that macroeconomic risk affects negatively all three sources of financing; during times of heightened macroeconomic risk not only external debt and equity issues fall but also the use of internally generated funds declines significantly. This finding suggests that firms time their equity and debt issues as they are less likely to issue securities when macroeconomic prospects are not favorable. It should be noted that the negative effects of macroeconomic risk on changes in leverage are purely the result of a reduction in debt financing. These findings suggest that in periods of macroeconomic uncertainty, firms are likely to design pro-passive investment and external financing strategies. That is, during uncertain states of the economy, even cash-rich firms may prefer to hold cash rather than investing it to guard against financial distress. Last but not the least, our observations regarding firm-specific factors are generally similar to those reported in earlier research (e.g., Baker and Wurgler 2002; Alti 2006; Chen and Zhao 2007; Lemmon et al. 2008; Chang and Dasgupta 2009). The remainder of the paper is organized as follows. Section 2 presents a detailed discussion about how risks concerning firms’ own business activities as well as macroeconomic conditions influence firms’ financing decisions. Section 3 presents the data, variable construction, and the methods we use for generating risk proxies. Section 4 describes the empirical model. Section 5 presents and discusses the empirical findings. Section 6 concludes the paper.",
41,4,Journal of Economics and Finance,15 September 2016,https://link.springer.com/article/10.1007/s12197-016-9373-7,Litigation risk and cash holdings,October 2017,James Malm,Srinidhi Kanuri,,Male,Unknown,Unknown,Male,"Lawsuits can have significant impact on shareholders and corporations (see for example, Romano 1991; Bhagat et al. 1994; Bizjak and Coles 1995; Bhagat et al. 1998, Bhattacharya et al. 2006; Gande and Lewis 2009). For the defendant firms, lawsuits can result in significant monetary and reputational impact, tarnishing the firms’ relationship with customers, investors, and other stakeholders. Moreover, some large lawsuits may linger in the courts for many years and may ultimately lead firms to bankruptcy. There is empirical evidence on the ramifications of lawsuits (see for example, Niehaus and Roth 1999; Skinner 1994; Shu 2000; Lowry and Shu 2002; Karpoff and Lott 1999; Karpoff et al. 2008; Karpoff 2010, 2012). Thus, while there is some empirical evidence on the effects of lawsuits, our knowledge of the relationship between litigation risk and cash holdings is quite sparse. A reason for the dearth of work in this area is the nonexistence of a reliable proxy for litigation risk. More recently, a number of papers have attempted to establish the link between industry securities litigation and cash holdings. Arena and Julio (2015) find a positive relationship between industry securities class action lawsuits and cash holdings. Although prior research provide evidence regarding the association between industry securities litigation risk and cash holding, the literature has not addressed the relationship between a broad sample of corporate lawsuits and cash holdings. With a few exceptions, previous literature on litigation risk focuses primarily on securities class action lawsuits. In this paper, we study the relationship between litigation risk and cash holdings using a broad sample of lawsuits beyond securities litigation. Given that firms are exposed to other types of legal risks, we expand this line of research to include labor, intellectual property, contracts, securities, environmental, product liability, medical liability, and other litigation. In addition we use lawsuits at the firm rather than industry level to proxy for litigation risk. These additional facets are the distinct features of this paper. We test the hypothesis that there is a relationship between litigation risk and the level of cash holdings. We implement several proxies for litigation risk in the main empirical analyses. We provide evidence to show that firms in a high litigation environment hold significantly more cash. Our analyses further reveals a positive relationship between the level of cash holdings and lawsuits related to the violations of labor, intellectual property, contracts, securities, environmental, product liability, medical liability, and other laws. This paper contributes in several ways to the litigation and corporate finance literature. This study complements the recent work by Arena and Julio (2015), who study the relationship between industry securities litigation risk and cash holdings. We identify another determinant of cash holdings and document another avenue whereby legal institutions affect corporate liquidity. The link between litigation risk and cash holdings is of interest to the business community, financial economists, management, and the investing public. Our study provides a critical source of information for practitioners on how the legal environment affects corporate behavior. We also provide basic information on the corporate lawsuits of S&P 1500 companies. The rest of the paper proceeds as follows: Section 2 reviews the relevant literature and develops the main testable hypotheses. In Section 3, we describe the sample, data, and methodology employed in the analysis. Section 4 reports and discusses the results of the empirical tests on the relationship between litigation risk and cash holdings. Section 5 concludes the paper.",12
41,4,Journal of Economics and Finance,05 October 2016,https://link.springer.com/article/10.1007/s12197-016-9375-5,"Liquidity, overpricing, and the tactics of informed traders",October 2017,Richard Borghesi,,,Male,Unknown,Unknown,Male,"To quantify the price efficiency of financial markets one must first be able to precisely measure asset value. This presents a fundamental problem in stock market research because the true values of stocks are not revealed. The best one can do is propose a model of market efficiency to approximate value. One solution to this problem is to extrapolate from a market in which security values can be clearly observed, and Tradesports provides such a setting. On this online exchange participants bought and sold binary options contracts via a continuous double auction. The experimental value of this market is that when a contract expires its true value is unambiguously revealed. As in traditional equities markets, on Tradesports there exist informational asymmetries across participants. We would like to better understand the mechanics and effectiveness of the trading tactics used by informed stock traders to capitalize at the expense of naïve traders. For example, prior research suggests that informed traders may have preferences regarding lot size rounding, price rounding, and transaction size (e.g., Alexander and Peterson 2007; Hodrick and Moutlon 2009; Barclay and Warner 1993). Timing may also be important, as research suggests that informed traders attempt to disguise their activities by acting during periods of heavy trading (Admati and Pfleiderer 1988). When exploitable trading opportunities exist, a liquid market would presumably provide a more conducive venue for informed trader activity because of reduced transactions costs (lower bid-ask spreads) and increased depth. The increased activities of these traders may serve to better align prices and values. For example, Sadka and Scherbina (2007) find that more liquid stocks are on average less severely mispriced than are less liquid stocks. However, there may be circumstances under which informed traders cannot not correct significant deviations between prices and values. Subrahmanyam (1991) suggests that increased liquidity leads to reduced price efficiency, and Bloomfield et al. (2009) find that the most liquid markets exhibit the largest price anomalies. If highly traded markets are dominated by noise traders, it may be that the most liquid markets exhibit the largest price anomalies. In traditional equities markets, short sellers have an informational advantage and are more skilled information processors than are buyers (Engelberg et al. 2012). Such an advantage is evident in the Tradesports market as well. Data suggest that average returns to buyers are significantly negative (Borghesi 2007), indicating that short sellers on average earn excess profits. We propose that confusion over the over the mechanism of short selling may cause unsophisticated participants to stick to buying bets, even at unfavorable prices. In this paper we explore the tactics of informed traders and find that they prefer to sell round lots and attempt to disguise their activities by making relatively small trades. We also find a statistically and economically significant relationship between asset liquidity and price efficiency, with the most highly liquid assets being the most overpriced. Results suggest that cognitive limitations deter many traders from short selling thereby systematically inflating asset prices, even in the absence of liquidity constraints, risk of recalls or regulatory action, and the upward movement in stock prices over time. The remainder of the paper is organized as follows. In section two we describe the data and methodology employed. In section three we present our analysis and results, and in section four we offer conclusions.",
41,4,Journal of Economics and Finance,06 October 2016,https://link.springer.com/article/10.1007/s12197-016-9376-4,Uninsured deposits and excess share insurance at US credit unions: the impact on risk and returns to members,October 2017,Shane A. Van Dalsem,,,Male,Unknown,Unknown,Male,"Credit unions are a significant and growing source of retail banking services in the United States. As of June 2014, approximately 100 million Americans were members of a credit union, a 25 % increase since 2000 (Marte 2014). In March of 2015, deposits in US credit unions passed the $1 trillion threshold (Rick 2015). While generally considered less risky than commercial banks, credit unions did not emerge from the 2008 financial crisis unscathed. The rate of credit union failures almost doubled between 2008 and 2010 (Cropp 2011). There were 186 credit union failures or National Credit Union Association (NCUA) assisted mergers between 2008 and the end of 2012 (Gold 2013). The increased risk did not go unnoticed by customers, and the dollar limit on deposit insurance was increased from $100,000 to $250,000 per depositor in October 2008 (Sahadi 2008). Despite the increased failure rate, the proportion of credit unions that have deposits that exceed the $250,000 limit per customer has increased over the past several years. At the end of 2008, 42.22 % of US credit unions had uninsured deposits.Footnote 1 Due largely to industry consolidation and inflation, at the end of 2014, 3855 (60.21 %) of 6402 credit unions in the United States have deposits that were not insured by the National Credit Union Share Insurance Fund (NCUSIF). For those that had uninsured deposits, the average (median) percentage of uninsured deposits was 3.23 % (2.23 %) of total deposits. The likelihood of having uninsured deposits increases with credit union size. In 2014, 0.94 % of credit unions in the smallest size decile had uninsured deposits while 97.81 % in the largest size decile did. Over the same period, the proportion of credit unions that had uninsured deposits but had supplemental deposit insurance to cover those deposits had fallen from 12.28 % to 7.68 %.Footnote 2
 In this paper, I develop and test two competing hypotheses on the relation between uninsured deposits and measures of risk and return for credit union members. Additionally, I condition the relations on whether the credit union uses supplemental third-party deposit insurance (hereafter excess share insurance) to insure the otherwise uninsured shares. Both hypotheses assume that uninsured depositors exercise some control over credit unions. Uninsured depositors have the option of dividing their deposits so that they maintain less than $250,000 at any credit union. However, maintaining multiple relationships with different institutions increases the cost and time needed by the depositors to manage their money. Uninsured depositors can use their power to reduce the risk associated with holding uninsured deposits at a credit union. If this risk reduction strategy is the result of uninsured deposits, I expect that capital and liquidity ratios will increase with the percentage of deposits that are uninsured. I refer to this as the risk reduction hypothesis. Unlike the situation for commercial banks, for which higher rates paid on deposits act as a form of discipline against excessive risk-taking by lowering returns for equity holders, the uninsured depositors at credit unions are the residual claimants and benefit from higher rates paid to attract deposits. As the residual claimants, uninsured depositors should provide increased monitoring of the credit union. Improved monitoring should decrease the required level of capital and liquidity through higher asset quality and result in higher returns to all credit union members, because the returns for uninsured and insured depositors are not easily divisible. If uninsured depositors take the role of equityholders and improve monitoring as a result of their role, I expect that the capital ratio, liquidity ratio, delinquent loans to total loans and leases, and the interest rate spread will decrease with the percentage of uninsured deposits. I refer to this as the equity substitution hypothesis. Assuming that uninsured depositors do exercise control over the firm, the impact of excess deposit insurance should enhance the predicted relationships because excess deposit insurers can increase premiums or deny coverage to credit unions that are deemed too risky. Additionally, excess deposit insurers have the ability to improve monitoring because they are not as restricted by privacy laws as depositors are. The evidence is consistent with the equity substitution hypothesis. For every 1 % increase in uninsured deposits to total deposits: the capital ratio decreases by 0.16 %, liquid-to-total assets decreases by 0.17 %, delinquencies to loans and leases decreases by 0.39 %, and the interest rate spread decreases by 7.74 basis points. With the exception of the relation between uninsured deposits and the capital ratio, the relationships are more economically significant for firms that use excess deposit insurance to insure part or all of their otherwise uninsured deposits. In robustness tests, I find that the results are generally more significant for smaller credit unions than larger credit unions, and that the results are inconsistent with the results prior to 2008 but remained consistent between 2009 and 2014. The existing literature on uninsured deposits focuses on commercial banks and other stock corporations, so increased returns to depositors are viewed as a cost of risk used to discipline depository institutions against taking excessive risk. The results of this paper demonstrate that the increased returns to credit union members come primarily from decreased rates on loans, which account for 7.33 of the 7.74 basis point decrease referenced above. The results also demonstrate that this increased return may be the result of improved monitoring rather than just a reward for risk. Additionally, as the first paper to examine the impact of uninsured deposits of credit unions on risk and return measures, this paper finds that the reputation of credit unions as a safer alternative to commercial banks may have resulted in uninsured depositors not monitoring asset quality prior to the financial crisis but doing so after. The remainder of the paper is organized as follows. Section 2 provides a literature review on the control of credit unions, uninsured depositors, and excess share insurance. Section 3 develops the hypotheses. Section 4 provides the data and methodology used. Section 5 provides the results of the study. Section 6 provides robustness tests. Section 7 provides a conclusion.",4
41,4,Journal of Economics and Finance,12 October 2016,https://link.springer.com/article/10.1007/s12197-016-9374-6,"The role of HFTs in order flow toxicity and stock price variance, and predicting changes in HFTs’ liquidity provisions",October 2017,Bonnie F. Van Ness,Robert A. Van Ness,Serhat Yildiz,,Male,Male,Mix,,
41,4,Journal of Economics and Finance,17 October 2016,https://link.springer.com/article/10.1007/s12197-016-9377-3,Investing strategies as continuous rising (falling) share prices released,October 2017,Manhwa Wu,Paoyu Huang,Yensen Ni,Unknown,Unknown,Unknown,Unknown,,
41,4,Journal of Economics and Finance,08 November 2016,https://link.springer.com/article/10.1007/s12197-016-9381-7,The effect of gold market speculation on REIT returns in South Africa: a behavioral perspective,October 2017,Omokolade Akinsomi,Mehmet Balcilar,Rangan Gupta,Unknown,Male,Unknown,Male,"Recent market crises and volatility spillovers across global financial markets have triggered renewed interest in investor behavior, particularly during periods of market stress, and how it relates to stock market movements. Consequently, a number of papers have recently been published focusing on herd behavior among investors, with a wide range of applications in different contexts. While most of these studies document evidence of investor herding particularly in emerging stock markets and more prevalently during periods of market stress (e.g. Balcilar et al. 2013; Babalos et al. 2013; Yao et al. 2014; Balcilar and Demirer 2015), a natural research question is whether certain global proxies of market stress can explain the evolution of herding or anti-herding in stock markets. If one can identify such global proxies of market stress that significantly influence investor behavior, particularly in emerging markets, then regulators in those countries can focus on those stress proxies in order to develop safety nets and circuit breakers that may help to prevent the destabilizing effects of investor herding as herd behavior might contribute to market volatility and pricing inefficiencies (e.g. Bikhchandani and Sharma 2001; Blasco et al. 2012). This study provides novel insight to the evolution of herd behavior during crisis periods by relating the time-variation in investor herding to speculation in the gold market, an asset traditionally considered a safe haven during periods of market crisis (e.g. Baur and Lucey 2010). Using firm-level data from South African Real Estate Investment Trusts (REITs), a major emerging market in BRICS, we examine how speculative activities in the gold market relate to the time-variation in investors’ herding or anti-herding patterns. The decision to look at herding behavior in South African REITs is motivated by certain statistical facts: First, according to SA REIT Association (2015a), South Africa’s listed properties asset class has outperformed cash, equities and bonds for more than 10 years. Second, SA REITs have also outperformed developed economies’ REITs over the last 10 years as the listed property sector in South Africa has grown quite significantly and continues to grow (SA REIT Association 2015a).Footnote 1 Therefore, one can argue that the fast growth and the attractive yields offered in this market segment have triggered interest among investors domestically and otherwise, potentially contributing to the presence of herding through correlated trades. Recognizing that this question can be answered empirically at best, the REITs market in this country provides an interesting avenue to examine herd behavior. Relating investor behavior in South Africa to speculation in the gold market is primarily motivated by the role of gold as a traditional safe haven that investors flock to during periods of market crisis. It can be argued that gold market dynamics, particularly during market crisis periods, are closely related to global expectations of economic fundamentals. To that end, the speculative ratio, a measure of speculative activity recently suggested by Chan et al. (2015), provides an interesting opening in that it allows us to examine whether the time variation in the level of speculation in this safe haven asset relates to investor herding in a major emerging stock market that has experienced some of the worst side effects of turbulence in global financial markets. A second motivation to relate gold speculation to herding in the South African REITs is due to the importance of the gold mining industry in this country that has been the driving force behind its economic growth and participation in the global economy (Mining Production and Sale Report, Statistics South Africa 2015). It can be argued that, speculative activities in the gold market as a proxy of future expected prices, drives currency and stock market dynamics in South Africa. In addition to its contribution to the herding literature, this study also contributes to the strand of the literature that deals with the relationship between commodities and stock markets as a number of papers have documented that commodities like gold and oil can help forecast real exchange rates in major commodity exporters (e.g., Apergis 2014). To that end, relating speculation in the market for gold to investor behavior in this major emerging stock market can enlarge our understanding of how commodity market dynamics relate to stock market dynamics in major exporting nations. Finally, a third contribution of this study is from an econometric perspective in that we propose a Markov switching time-varying parameter (MS-TVP) herding model that not only takes into account different market states during which herding or anti-herding may be present, but also directly relates the speculative ratio in the gold market to time-variation in the estimated herding coefficients. By doing so, this study contributes both to the herding literature and to the literature on commodity-stock market nexus. We find that the benchmark herding models that are based on a static as well as a two-regime Markov switching (MS) specification fail to detect herding in this stock market. On the other hand, the MS-TVP specification which tracks the time-variation in the herding coefficient clearly identifies periods of herding and anti-herding where herding is largely concentrated on the period that corresponds to the duration and the aftermath of the global financial crisis. Interestingly, the two-regime MS model that tests for the presence of herding during the low and high volatility market states fails to detect herding even during the high volatility state, underscoring the significance of econometric specifications that directly track the time-variation in herding. Examining the time-variation in speculative activities in the gold market, we observe that higher level of speculation in gold significantly contributes to herding in this emerging market. The significance of gold speculation is robust to alternative model specifications and suggests that speculation in this safe haven potentially contains significant information that relates to investor behavior in stock markets. Our analysis of dynamic correlations between the level of gold speculation and herding further supports the estimation results from the MS-TVP model, indicated by a negative correlation between the estimated herding coefficients and gold speculation, particularly during the mid-2008 to 2011 period, exactly matching the duration and aftermath of the global financial crisis. Overall, our findings suggest that speculative activities in the gold market contain valuable information regarding market fundamentals that drive investor behavior in emerging markets. A natural policy implication, therefore, is to monitor indicators of hedging and speculative activities in the gold market in order to implement circuit breakers that may help prevent market crashes in emerging markets that are prone to global shocks. The rest of this paper is organized as follows. Section 2 briefly summarizes the literature on herding tests in stock markets and the gold- stock market relationship in the South African context. Section 3 presents the data and methodology. Section 4 provides the empirical findings and Section 5 concludes the paper.",15
41,4,Journal of Economics and Finance,15 November 2016,https://link.springer.com/article/10.1007/s12197-016-9379-1,Deposit dollarization in emerging markets: modelling the hysteresis effect,October 2017,Anna Krupkina,Alexey Ponomarenko,,Female,Male,Unknown,Mix,,
41,4,Journal of Economics and Finance,26 November 2016,https://link.springer.com/article/10.1007/s12197-016-9380-8,Open market operations and associated movements of the federal funds rate during the week prior to target changes,October 2017,Yasuo Nishiyama,,,Male,Unknown,Unknown,Male,"The Federal Open Market Committee (FOMC) meets usually eight times a year and decides the nation’s monetary policy. Before the 2007–09 financial crisis, the FOMC’s main task was primarily to decide on a level of the federal funds target rate (the target). (Because this paper’s sample period is 1994–2005, all of this paper’s discussions and analyses refer to this sample period. The choice of the sample period is explained below.) After the target is changed, the Open Market Desk at the Federal Reserve Bank of New York executes the FOMC’s decision by conducting open market operations. While textbooks (for example, Hubbard 2002; Mishkin 2010) explain how the Federal Reserve (the Fed) carries out open market operations to hit a new target after a change in the target was decided, this paper is concerned with how the Fed conducts open market operations prior to a change in the target when market participants, or banks, anticipate the forthcoming change. Market expectations of a forthcoming change in the target are clearly evidenced in the Treasury securities market (Roley and Sellon 1995; Gürkaynak et al. 2002; Lange et al. 2003) and in the federal funds futures market (Owens and Webb 2001; Poole et al. 2002; Carlson et al. 2006; Swanson 2006; Hamilton 2006). Carpenter and Demiralp (2006) investigate these market expectations, and the associated behavior of banks that economize on reserve requirements based on their expectations, in the federal funds market. This paper elaborates further on the associated behavior of the Fed, thereby extending Carpenter and Demiralp (2006). Specifically, the objective of this paper is to develop a model of open market operations that is consistent with the day-to-day procedure of open market operations. The novelty is to discriminate between various open market operations that are conducted in response to the banks’ demand for reserves during the week prior to a target change: complete, partial, no, or anti-accommodation. Target changes are differentiated by size (25 basis points, or 50 basis points or larger) and by direction (an increase or a decrease), and the model is estimated for two subsample periods. The estimation results show that the Fed conducts open market operations very differently between small target changes (plus/minus 25 basis points, in which case partial, complete or no accommodation is carried out) and larger target changes (plus/minus 50 basis points or larger, in which case anti-accommodation is carried out). Observed, and well-documented, movements of the federal funds rate (Carpenter and Demiralp 2006)—away from the existing target level and toward an expected new target level during the week prior to a target change—are consistent with this paper’s estimation results.",2
41,4,Journal of Economics and Finance,28 December 2016,https://link.springer.com/article/10.1007/s12197-016-9382-6,Litigation risk and investment policy,October 2017,James Malm,Hari P. Adhikari,Nilesh Sah,Male,Male,Unknown,Male,"Theory suggests that agency conflicts resulting from exposure to risk can have an impact on a firm’s investment policy. Corporations face different kinds of risks, some of which can result in significant losses. One type of risk businesses face is litigation risk. Lawsuits can have adverse financial and reputational impact on corporations and other stakeholders. Some large lawsuits may persist in the courts for several years and ultimately lead firms to bankruptcy. Given that litigation risk increases firm risk; we might expect it to influence investment policy choices as well. In this paper, we empirically test whether litigation risk is associated with investment policy. Various arguments have been made regarding the relationship between litigation risk and investment in tangible and intangible assets. One view is that there is a positive relationship between litigation risk and investment policy (see, for example Gormley and Matsa (2011)). In this case, firms in a risky environment can choose to adopt more aggressive investment policy by investing more in tangible and intangible assets. Grossman and Hart (1983) and Gilson (1989) note that the likelihood of bankruptcy and the threat of job loss can encourage management to exert more effort. This is because not only will these events tarnish the reputation of managers, they can also lead to losses in benefits and human capital. A second view is that there is a negative relationship between litigation risk and investment policy. In this case, firms can choose to adopt more conservative investment policies. Here rather than increasing investments in capital expenditure or research and development, high risk firms possibly may choose to decrease investments. For example, Arena and Julio (2015) report that high litigation risk firms cut back on capital expenditure. Given the mixed results from previous studies, we attempt to investigate the relationship between litigation risk and total investments. We also partition total investments into its two components of capital expenditure and research and development expense and examine their link with litigation risk. We implement several proxies for litigation risk in our main empirical analyses. We provide evidence to show that firms in a high litigation environment have significantly higher levels of total investments. Our analyses further show that high litigation risk firms have higher capital expenditure and higher and research and development expense. We contribute to the litigation and corporate finance literature in several ways. We add to an emerging body of research in this area by shedding light on the consequences associated with corporate litigation. We identify another determinant of investment policy and document another avenue whereby legal institutions affect corporate policy choices. The link between litigation risk and investments is of interest to the business community, financial economists, management, and the investing public. Our study provides a critical source of information for practitioners on how the legal environment affects corporate behavior. We also provide basic information on the corporate lawsuits of S&P 1500 companies. The rest of the paper is organized as follows: In the next section, we review the relevant literature and develop the testable hypotheses. Section 3 provides a description of the sample, data, and methodology employed in the analysis. Section 4 reports and discusses the empirical tests on the link between litigation risk and investment policy. Finally, we conclude the paper in Section 5.",7
42,1,Journal of Economics and Finance,30 December 2016,https://link.springer.com/article/10.1007/s12197-016-9383-5,Does geographical location matter for managerial compensation design?,January 2018,Jing Zhang,Jieun Chung,,,Unknown,Unknown,Mix,,
42,1,Journal of Economics and Finance,10 October 2016,https://link.springer.com/article/10.1007/s12197-016-9372-8,New insights about the relationship between corporate cash holdings and interest rates,January 2018,Anna-Leigh Stone,Benton E. Gup,Junsoo Lee,Unknown,Male,Unknown,Male,"Baumol (1952) and Tobin (1956) claimed that as interest rates fall corporations will hold more cash due to the lower opportunity cost. With interest rates at 40 year lows, it leaves one to wonder if this negative relationship has held over time. The primary contribution of this paper is that it is the first extensive empirical study of the relationship between interest rates and cash holdings in the United States. While theoretical relationships were proposed in the 1950’s, they were not examined empirically. Rather than simply including a measure of interest as a control variable, it is the primary focus of this paper. In addition to empirically testing for the relationship between cash holdings and interest rates, the methodology and variables capturing cash holdings are distinguished from recent papers on corporate cash holdings. The relationship between cash holdings and interest rates is tested using a random effects threshold model, which distinguishes the relationship over different ranges of interest rates. To emphasize why it is necessary to control for varying interest rates, Fig. 1 plots the federal funds rate from January 1970 until December 2014. During the 1970’s, rates varied from 4.43 to 11.19 %. This is the only time during the sample where we see a uniform increase in rates. Rates reached a peak at 16.38 % in 1981 but subsequently declined to levels seen during the 1970’s. In the 1990’s, rates continued to decline, but at a slower pace than was experienced during the 1980’s. Finally, during the 2000’s and the most recent decade, rates have continued to decline even further to 0.09 % leaving interest rates at the lowest levels experienced over the last 40 years. Because of the variation in rates, the data are divided into interest rate ranges that are found by searching for thresholds in the interest rate series. This allows us to see if the negative relationship holds in the face of the dramatic fluctuation that interest rates experienced during these times. One-year treasury constant maturity rate. Figure 1 plots the federal funds rate from 1970 to 2014. The data was obtained from the Federal Reserve Economic Data While most previous papers have used cash and marketable securities divided by total assets as the main cash holdings variable, we use cash holdings excluding marketable securities divided by total assets (i.e. cash/total assets). The difference between these two variables lies with the marketable securities, which can lead to conflicting relationships between cash holdings and interest rates. Figure 2 plots the mean and median of cash holdings with and without marketable securities and the federal funds rate from January 1970 to December 2014. Panel A of Fig. 2 shows that using two different measures of the mean of cash holdings is vital to the study of the relationship between corporate cash holdings and interest rates. The mean of cash holdings is steadily increasing over time. Cash holdings without marketable securities shows a rather stable mean during the 1970’s and early 1980’s but has experienced various rates of increase since the late 1980’s. In Panel B, the median measure shows that both cash holdings including and excluding marketable securities are relatively stable until the 1990’s when each starts to increase. These dramatic differences over time highlight the need to empirically examine whether the negative relationship holds over time. Graph of cash holdings and the one-year treasury constant maturity rate. Figure 2 plots cash holdings, cash holdings excluding marketable securities, and the federal funds rate from January 1970 until December 2014. Both measures of cash holdings were created by taking the mean and median of Compustat data for all firms used in the paper. The federal funds rate obtained from the Federal Reserve Economic Data. All numbers are expressed as decimals We document a non-negative relationship covering several ranges of interest rates, particularly rates since the 1990’s. Thus, alternative explanations for the non-theorized relationship are tested. The additional tests are based on recent cash holdings papers and they include the tax-based explanation, pension fund contributions, zero-leverage firms, financially constrained and unconstrained firms, firm governance, and high-tech firms. None of these relationships, independently or together, fully explain the positive relationship. Finally, in an effort to find which firms are driving the relationship, quantile regressions are used, which allows for regressions based on different quantiles of the dependent variable and not the mean as is used in other models. In doing so, it is found that the positive relationship is driven not by a particular group of firms, but spans all groups. Thus, we conclude that the prior theorized relationship between cash holdings and interest rates may need to be adjusted to allow for the relationship that is documented. The remainder of the paper is organized as follows: Section 2 discusses the relevant literature surrounding cash holdings and explores the hypothesis. Section 3 discuss the data and Section 4 presents the model. Section 5 presents the empirical results. Section 6 examines alternative explanations for the positive relationship discovered between interest rates and cash holdings. Section 7 presents results based on quantile regressions, and Section 8 summarizes our findings.",14
42,1,Journal of Economics and Finance,01 February 2017,https://link.springer.com/article/10.1007/s12197-017-9385-y,Are standard asset pricing factors long-range dependent?,January 2018,Benjamin Rainer Auer,,,Male,Unknown,Unknown,Male,"In recent decades, the identification and deeper analysis of exploitable cross-sectional stock market effects has received considerable attention in both practice and academia (see Chordia et al. 2014). Among the vast number of revealed phenomena, the size, book-to-market, momentum and beta effects can be considered the most important because related arbitrage portfolios have become important components of modern investment products and/or benchmark variables in standard asset pricing models. According to Fama and French (1992), the size (book-to-market) effect implies that returns are negatively (positively) related to firm size (the book-to-market ratio). Jegadeesh and Titman (1993) show that stocks exhibit momentum behaviour such that buying past winners and selling past losers can lead to substantially high returns. Finally, Frazzini and Pedersen (2014) document that high-beta stocks earn significantly lower returns than low-beta stocks. The properties of the arbitrage portfolio returns exploiting these effects (hereforth, factor returns) have been the subject of numerous empirical studies.Footnote 1 For example, they have been shown to be predictors of economic growth (see Liew and Vassalou 2000), to proxy for variables that describe investment opportunities (see Petkova 2006) and to have strong co-movement across asset classes (see Asness et al. 2013b). However, one important question has not yet been answered: Do factor returns show signs of long-range dependence (LRD)? This issue is especially salient because the presence of LRD in factor returns would have significant impact on many applications in modern financial economics. First, as size, book-to-market, momentum and beta portfolios are typical components of financial products (see, for example, AQR Capital Management, www.aqr.com), optimal consumption/savings and portfolio decisions involving these products would become extremely sensitive to the investment horizon if the factor returns were LRD (see Lo 1991). Second, problems would arise in the pricing of derivative securities (where the arbitrage portfolios are the underlyings) with martingale methods since the class of continuous time stochastic processes most commonly employed is inconsistent with long-term memory (see Maheswaran and Sims 1993; Ohanissian et al. 2004). Finally, traditional tests of capital asset pricing models and the arbitrage pricing theory, in which factor return factors have become standard explanatory variables, are no longer valid since the usual forms of statistical inference do not apply to time-series exhibiting such persistence (see Lo 1991).Footnote 2
 In economics and finance, LRD has a long history (see Mandelbrot 1997). It is a specific departure from random walk behaviour because LRD time-series exhibit an unusually high degree of persistence so that observations in the remote past are nontrivially correlated with observations in the distant future, even as the time span between the two observations increases. Thus, the defining characteristic of LRD has been taken by many to be a slow (hyperbolic) decay of the autocorrelation function (see Grau-Carles 2000).Footnote 3 To detect LRD, various estimators have been proposed in the literature (see Baillie 1996; Kantelhardt 2009; Fernandez 2011). In this article, we focus on the ‘fractal class’ of estimators. Specifically, we use rescaled range analysis (RRA; also often called Hurst R/S analysis) and the detrended fluctuation analysis (DFA) to gain insights into the dynamics of factor returns. These two methods (and their modifications) are the most popular ones in the field and have been extensively applied in recent studies of the return properties of equities (see Cajueiro and Tabak 2004a, b, 2005a, b; Kristoufek and Vosvrda 2013; Hull and McGroarty 2014; Sensoy and Tabak 2015), exchange rates (see Ausloos 2000; Ivanova and Ausloos 2002; Norouzzadeh and Rahmani 2006), commodities (see Tabak and Cajueiro 2007; Alvarez-Ramirez et al. 2008; Wang and Liu 2010; Batten et al. 2013) investment funds (see Crato and Ray 2003; Souza et al. 2004; Wang et al. 2005) and futures (see Crato and Ray 2000; Souza et al. 2008; Wang et al. 2011).Footnote 4 They both provide estimates of the Hurst exponent, a simple metric to judge the degree of LRD. The contributions of our study can be summarised as follows. First, to analyse LRD in factor returns, we construct arbitrage portfolios that seek to exploit the size, book-to-market, momentum and beta effects in the US stock market (1931 - 2014) and the stock markets of 20 other developed countries (1990 - 2014).Footnote 5 This allows an illustrative look at whether there are differences in the magnitude of the factor returns across countries. Second, in addition to producing this by-product, we focus on the research question of whether we can find evidence of LRD within these returns. In the US, our rich sample allows investigation of the dynamics of LRD by using a local Hurst exponent approach for RRA and DFA considering the time-dependence of the Hurst exponent which has been identified in several recent studies (see Carbone et al. 2004; Batten et al. 2008). Third, we apply a more general filter technique for the RRA than used in the previous literature. It is well-known that short-range dependence which is substantial in stock returns (see Lo and MacKinlay 1988) and may be associated with lingering liquidity effects in financial markets can distort the RRA (see Lo 1991). Thus, recent studies apply ARMA filters to eliminate short-range dependences from the data (see Szilagyi and Batten 2007; Batten and Hamada 2009; Batten et al. 2013). However, as the RRA is also sensitive to heteroscedasticity (see Lo 1991) which is another typical feature of stock returns (see Schwert and Seguin 1990), we extend this approach to an ARMA-GARCH framework. Finally, we support our results by a variety of robustness checks. Specifically, we examine their sensitivity with respect to alternative parameterisations and specifications of our methodology, additional approaches to estimate the Hurst exponent and the use of factor returns collected from publicly available databases of well-known researchers. The remainder of the article is organised as follows. Section 2 provides a brief description of our dataset, the portfolio construction methods and our way of estimating the Hurst exponent based on the ARMA-GARCH-adjusted RRA and the DFA. Section 3 presents a compact descriptive analysis of size, book-to-market, momentum and beta portfolio returns. Section 4 contains our results on LRD in factor returns and the outcomes of several robustness checks. Section 5 summarises and concludes.",3
42,1,Journal of Economics and Finance,06 February 2017,https://link.springer.com/article/10.1007/s12197-017-9387-9,Oil and equity: too deep into each other,January 2018,Natalya (Natasha) Delcoure,Harmeet Singh,,Female,Unknown,Unknown,Female,"Relationship between oil prices and equities continue to receive considerable attention from both academics and the popular press. Theoretically, the value of a firm is the present value of expected future cash flows. Rising oil prices affect the future cash flows of a firm, either negatively or positively depending on whether the firm is producing or consuming oil. In addition, higher oil prices lead to higher production costs which in return affect inflation, consumer confidence, interest rates, and ultimately economic growth. Contractionary monetary policy is used to fight inflation. This further puts upward pressure on interest rates. As a result, firm’s cost of capital goes up. Increase in a cost of capital leads to lower stock price, other things equal. From the consumption perspective spending on oil related products (e.g., gasoline for routine transportation) is a substantial portion of consumer disposable income. When oil prices fall, consumers spend fewer dollars on these purchases. This reduction in spending leads to savings for consumers. The way these savings are used depends on the reasons why prices have fallen. If it is due to economic contraction, then consumers are generally conservative due to reduced economic activity. These excess savings are usually not spent by consumers thus offering little positive impact on equities markets. On the other hand, if prices fall due to positive supply shock then the substantial increase in consumer disposable income caused by savings from reduced oil prices may lead to increased economic activity and thus a better prospect for equity market. Dallas Federal Reserve Bank estimates that a 50% reduction in oil prices leads to 0.3% to 1% increase in US real Gross Domestic Product (GDP). This amounts to approximately $50 – $160 billion of extra income in 2015 as oil prices fell by more than 50% from the start to the end of 2014 and further plunged by about 30% in 2015. If very little amount of increased savings are spent back in the economy then this may not add to economic activity. US government estimated that less than 50% of extra savings are being spent by consumers (e.g., Furman 2015). At the same time, J.P. Morgan Chase analysis of 25 million users of its credit and debit cards shows that 80% from the gas price related savings are being spent by consumers on discretionary purchases (Farrel and Greig 2015). An extensive body of financial economic literature suggests a link between oil prices and equities; however, the empirical evidence regarding the sign of this relation and its strength is mixed. Kling (1985) determines that stock market and oil prices move in opposite directions. Chen et al. (1986), on the other hand, find no relations between oil prices and equities. Jones and Kaul (1996) report a stable inverse relation between oil prices and stock market return. Huang et al. (1996) challenge their conclusion. They find no negative relationship between stock returns and changes in price of oil futures. Lee and Ni (2002) investigate the effects of oil price shocks on demand and supply in various industries using VAR models. They find that for industries that have a large cost share of oil (e.g., refinery, industrial chemicals) oil price shocks mainly reduce the supply of oil-intensive industries while they mostly reduce the demand of many other industries, especially the automotive industry. Contrary to Lee and Ni (2002), Kilian and Park (2009) show that the reaction of the US real stock return to an oil price shock differs depending on whether the change in the price of oil is driven by the oil market demand vs. supply shock. They find that between 1975 and 2006, on average, 22% of the variation in aggregate stock returns can be attributed to the shocks in the oil market. Similar to previous findings (e.g., Hamilton 1988; Dhawan and Jeske 2008; Edelsten and Kilian 2007, 2009), they document considerably stronger sensitivity to demand and supply shocks on industry level; however, the degree of sensitivity varies by industry. The impact of oil price changes on stock market returns worldwide is further examined by Driesprong et al. (2008). They determine that oil prices predict stock market returns in emerging and developed countries between October 1973 and April 2003. Authors find that during the sample period a rise in oil prices drastically lowers future stock returns. They determine that investors underestimate the direct economic effect of oil price changes on the economy in oil-related industries. Malik and Ewing (2009) given the popularity of sector index investing study the relations between oil prices and five major equity indices (e.g., financial sector, technology sector, consumer services sector, health care sector, and industrial sector). They uncover that “news” in one sector may impact other markets and provide evidence of significant volatility transmission between oil and equity markets. Vo (2011) study examines the volatility of stock and oil futures markets. His finds that equity and oil futures prices are inter-related. Their correlation follows a time-varying dynamic process and tends to increase when the markets’ volatility grow. Conditioned on the past information, the volatility in each market is persistent and volatility in each market is inter-dependent. Degiannakis et al. (2013) investigates the effects of oil price shocks on stock market volatility in Europe. They conclude that supply-side and oil specific demand shocks do not affect volatility, whereas, aggregate demand shocks influence volatility (aggregate stock market and industrial sector indices) at a significant level. Sector sensitivity to oil prices volatility is asymmetric (Park and Ratti 2008; El Hedi Arouri et al. 2012) depending on degree of competition, concentration in the industry, the capacity of the industry to transfer oil price shocks to its consumers, on direct and indirect effect of oil prices on the industry, and whether oil and oil-related products are an input or an output for the industry. Our paper adds to the previous empirical work on sector index investing and diversification. Our analysis is motivated to extend existing knowledge on transmission channels between oil prices and stock markets for building accurate asset pricing models given the popularity of sector index investing and generating accurate forecast of on the nature and number of factor affecting equity and oil markets. In addition, we estimate the conditional time-varying correlation between oil prices, stock market returns, and economics conditions in the US between 2005 and June 2016 following Engle’s (2002) Dynamic Conditional Correlation (DCC)-GARCH model, where conditional variances are estimated using Glosten et al. (1993) GJR model. Furthermore, we split the entire study period in two sub-periods: January 2005 through December 2010 and January 2011 through June 2016. In the first period, the oil prices rose sharply and in the second period the oil prices fell. In terms of the equity markets, the first period coincided with the global recession causing equity prices to fall flat. During the second period, there was no clear global recession; however the economies in European Union were on the verge of a recession and the growth rates of two of the fastest growing economies of India and China were slowing down causing reasonable stress to the global equity markets. At the same time, the US fundamentals at the beginning of 2016 remained healthy. The most important indicator of economic health is unemployment. The US created 151,000 jobs in January 2016, unemployment rate dropped to 4.9% which is Federal Reserve considers full employment, and average hourly earnings rose 2.5% from the year before. Nevertheless, on February 10, 2016 S&P 500 was down 9% for the year already – disconnect between economics fundamentals and market values. We find evidence of time-varying correlation of oil and stock prices. Based on our results, we conclude that in overall during the study periods, all selected variables transmit more volatility in magnitude (amount of coefficient whether positive or negative) to the US equities than to the US economy. The empirical insights from such study are equally important for accurate asset pricing, hedging strategies, portfolio and derivatives management. The remainder of the paper is organized as follows. Section 2 discusses the findings of selected previous work on the links between oil price and stock markets. Empirical model and data sample are presented in Section 3. Section 4 discusses the obtained results. Section 5 concludes the paper.",4
42,1,Journal of Economics and Finance,13 March 2017,https://link.springer.com/article/10.1007/s12197-017-9388-8,On the relation between exchange rates and stock prices: a non-linear ARDL approach and asymmetry analysis,January 2018,Mohsen Bahmani-Oskooee,Sujata Saha,,Male,Female,Unknown,Mix,,
42,1,Journal of Economics and Finance,10 April 2017,https://link.springer.com/article/10.1007/s12197-017-9390-1,CEO ability and firm performance: Stock market and job market reactions,January 2018,Tarun Mukherjee,Huong Nguyen,,Male,,Unknown,Mix,,
42,1,Journal of Economics and Finance,03 May 2017,https://link.springer.com/article/10.1007/s12197-017-9391-0,The transmission of international stock market volatilities,January 2018,Bruce Q. Budd,,,Male,Unknown,Unknown,Male,"Empirical observations of linked patterns of stock price activity between national equity exchanges are becoming more apparent as global trade and markets move closer together. Manifestly these observations of financial disturbances were particularly witnessed during the aftermath of the 2007 U.S. financial crisis when the subsequent impact on western stock markets displayed distinct linked clusters of global volatilities. This paper examines the spillover effects of cluster volatility between the equity markets of the U.S. and four Asia-Pacific countries (Australia, China, Japan, South Korea) in the context of two significant U.S. market crises: the financial crisis of 2007 and the threat of the U.S. government refusing to raise its “debt ceiling” and subsequently defaulting on its debt in 2011). The former crisis originating from the sprawling U.S. structural domestic financial banking collapse and the latter having developed from the release of U.S. debt ceiling announcements relating to a potential threat of a US debt default for the first time in the history of the economy. Both events triggered periods of high anxiety across U.S. financial markets and the broader U.S. economy. This paper seeks to determine whether market crises in the US and the increased volatility that they generate in the U.S. equity market, lead to similar increases in equity market volatilities in Asia-Pacific countries. The motivation of this paper relates to the identification and implications of excess volatilities affecting cross-country spillovers on global markets generated by exogenous shocks from the richest economy in the world, namely the United States. The events of a financial crisis provide a unique and practical opportunity to measure the changes and characteristics in stock returns. The investigation of any relationship between stock returns and volatility is of vital relevance to modern portfolio theory. Patterns of co-movement between these exchanges as well as potential evidence towards global capital market integration are investigated. It is against this background that this study seeks to answer the following research questions and provide further insight into the internal links between the financial return and the past errors and the relations between the five sample stock exchanges: Are asset returns of each exchange linked? Does the volatility of one market lead the volatility of other markets in the Asia-Pacific region? Does a shock on a market increase the volatility on another market? Are market movements converging and becoming more integrated? Does the US S&P 500 show evidence of financial transmission signals to Asia-Pacific equity markets in a clear leader-follower relationship, or are other equity markets in the Asia-Pacific markets in the region more influential? The index price of each stock market provides a proxy for business confidence and future expectations of economic activity of that country. It is this signal which is used to gauge each country’s market exposure. Countries are becoming more sensitive to the global impact of systemic economies such as China and their financial spillovers. While macroeconomic policies can ease the global exposure of financial shocks, inevitably some impacts are felt. Boorman et al. (2010) researched the slowdown on emerging and developing markets due to the global financial crisis. Initially, though these countries were affected by the crisis, the implementation of swift policy reforms showed evidence of prompt recovery. Raghaven, (2008), researched the impact of the Asian financial crisis on Malaysia, Singapore and Hong Kong. She concludes the world factors to be more prominent during the pre-crisis era while regional factors seem to have gained more impetus in the post crisis era. The economic prominences of Asia-Pacific financial markets have emerged considerably, particularly since the aftermath of the Asian Financial crisis 1997 to 1998 and subsequent exponential growth of global trade. This increase in global expansion has been stimulated by the improved cooperation of heightened cross-border capital flows, transparency of transactions via information technology, the ease of capital mobility regulations, as well as added competitive market pressure on corporations to seek capital outside their home country. This paper compares cluster volatility of asset returns transmission of four Asia-Pacific equity markets and the S&P500 Index using VECH-Multivariate Generalized Conditional Heteroskedasticity (MGARCH) methodology. It further tests for the transmission of returns and volatilities across all the stock markets incorporating asymmetric responses. In addition, a simultaneous estimation, using the Dynamic Conditional Correlations of the four Asia-Pacific countries with that of the US S&P500 is followed to detect any evidence of correlation (contagion) changes during the period 2000 to 2014. Weekly data from the US Standard and Poors 500 and the top four major stock exchanges: Australia, China, Japan and South Korea are collected, collated, analysed and estimated for the period January 5th 2000 to September 8th 2014, (n = 765 observations). Caution is taken of the different methodologies of each country’s index calculation. For example, the S&P500 and Nikkei are different in nature; the former is a weighted average of stocks with each weight being proportional to its market value while the latter is a simple average of stocks. The four selected markets are highly integrated and prominent in the Asia-Pacific region in terms of capitalization of stock markets. As illustrated in Table 1, collectively these top four countries represent, according to the Dow-Jones Asia/Pacific total stock market Indices, more than 73% of market capitalization by value of the 18 countries represented in the region, (Financial, 2014). These samples should therefore provide plausible benchmarks of each countries’ global equity market sensitivity. As a barometer of economic climate, any innovation or shock, economic or otherwise, impacting on these economies will be translated in the movements of their respective stock markets. The Asia-Pacific region is inherently oriented on export trade and therefore possibly more sensitive to any global turbulence than other regions. All financial crises impact stock values to a greater or lesser degree. Shocks impact business expectations, consumer confidence, price earnings and ultimately stock prices. Within a background of widening international trade and the ensuing integration of financial activities, these countries have been selected as a representative sample of a strategically important region in a global perspective of international finance. This empirical analysis investigates any insight into the nature of interaction and linkage between these equity markets. The time series data are tested for any structural breaks. Parameter stability and multiple structural break tests developed by Bai and Perron, (2003) are used separately to identify such endogeneous breaks which, if not identified, could generate spurious results. The subsequent identified structural break dates are then incorporated into the equation as dummy variables to observe volatility integration and contagion. The inclusion of these dummy variables measure any statistical impact identified by the 2008 Financial Crisis or/and the 2011 US Debt ceiling debacle on the sample economies. The volatility persistence within each countries’ stock exchange returns is also measured using these coefficients. The impact of the fiscal ceiling debacle and subsequent global distress of debt default may have had volatility implications for those particular indebted markets. There are no a prior expectations. The following section provides a literature review. Section three describes model specification used in this study. Section four and five, respectively report the data analysis and results. The final section provides discussion and concluding observations.",4
42,1,Journal of Economics and Finance,04 May 2017,https://link.springer.com/article/10.1007/s12197-017-9392-z,The effects of income and population demographics on single-county bank performance,January 2018,Ken B. Cyree,Brandon C. L. Morris,,Male,Male,Unknown,Male,"Bank performance is widely studied, but the results are often inconclusive depending on the focus of the study. For example, the performance of banks is often related to merger activity as in Akhavien, Berger and Humphrey (1997) or ownership effects as in Bonin et al. (2005). DeLong and DeYoung (2007) study the effects of multiple bank acquisitions on changes in return on assets (ROA), changes in return on equity (ROE), and changes in other variables such as interest margin or core deposits. Demographic effects such as county-level income and population levels could be important determinants of bank performance, yet is typically missing in studies of bank performance. Therefore, in this study, we investigate whether county-level demographics of population and income are related to bank performance. Berger and Mester (1997) list profit efficiency correlates that other studies have used including the type of regulator, market structure, geographic diversification, corporate control, concentration, asset size, holding company status, capital levels, and other exogenous variables. Berger and DeYoung (2001) focus on the distance of a banking operation from the home office as an explanation for efficiency differences in banking operations. Berger and Humphrey (1997) provide a survey of the usefulness of efficiency studies and conclude they largely inform government policy, address research issues and help to evaluate managerial performance. One theme discussed by Berger and Humphrey (1997) is the market-power versus efficient-structure debate about the determinants of profitability. In general, market power does seem to affect the prices of some types of local deposits and loans, but has little apparent effect on profit efficiency. The common thread in most studies is to benchmark performance and then study the effects of a firm event, such as consolidation, or different business models on performance. Rarely is the focus of a banking performance study on the market in which they operate. Thus, in this study we explore whether and to what extent bank performance is affected by the demographics of a market. In particular, we ask if per-capita wealth and population size are correlated with accounting performance measures for single-county banks. We use single-county banks since it avoids issues with estimating the appropriate market demographic measures for banks that operate in many different counties. We investigate the effects of including these variables in traditional bank performance models to see if customer demographics in the market are related to performance, and whether the inclusion of these variables change other performance dynamics. To a certain extent, we are testing bank-specific choices made by bank managers versus the good fortune of being in the right market. We recognize that bank managers choose their market, but our focus is on the effects of the wealth and population in those markets on performance, if any. Our results show that population and income characteristics are an important determinant of bank profitability. The initial univariate results show significant performance gains for firms competing in low-population counties as compared to firms competing in high-population counties. In our multivariate tests, we find that both low income and low population levels improve the performance of single-county banks. We also show that low-population mitigates some of the effects of the 2008 financial crisis, both at the onset of the crisis and in the years following. These findings are consistent with the notion that the market a bank chooses to operate in can lead to significant advantages. We conjecture the type of competition in the market is important in low-population areas that have fewer local banking options. In other words, the quality of competition is important if banks choose not to compete on pricing or fees, even though market concentration measures such as Herfindahl indices or the share of deposits by multi-county banks do not show abnormal concentration as is the case in our findings. Our results are consistent with Pilloff (1999) and Cyree and Paul Spurlin (2012) who find that rural areas have less competitive effects as banks do not appear to compete on loan or deposit pricing, even if the market has a reasonable number of competitors and therefore have lower measures of concentration. Therefore, one implication of our findings is that traditional measures of concentration do not necessarily measure the quality of the competition, and our results are consistent with low population areas have the lowest quality of competition.",2
42,1,Journal of Economics and Finance,13 May 2017,https://link.springer.com/article/10.1007/s12197-017-9393-y,A synthesized model of short selling constraints and their impact on stock returns,January 2018,Jose Gutierrez,Steve Johnson,Robert Stretcher,Male,Male,Male,Male,"The finance literature concerning the degree to which short sellers are constrained in their activities designates several key proxies for these constraints. These proxies include institutional ownership, relative short interest, the presence of exchange-traded options, and whether or not the stock pays a dividend. Generally speaking, bullish investors will rationally take long positions, while bearish investors would like to short the stock. A short-sale constraint, however, would keep bearish investors out of the market, leaving the price of the stock to disproportionally represent mostly the opinions of the most optimistic investors. This translates into artificially inflated prices, and thus lower subsequent returns Miller (1977). Beyond this, Lecce et al. (2012) establish that the more pronounced the short-sale constraint, the greater price and return bias can be. However, the question of which constraints are most significant when measured simultaneously, whether these simultaneous constraints are sensitive to firm size, and whether overall market direction impacts these simultaneously-measured constraints remains unaddressed. These questions provide the motivation for this study, based on the notion that a short-sale constraint potentially generates a deviation from fundamental value. Positive pricing (negative return) bias is expected to be related to the presence of short-selling constraints, measured by four traditional constraint proxy variables. We argue that results from prior studies, which address these constraint proxy variables individually, do not capture interrelationships with the other constraint variables. It is quite possible for a stock to be short-sale constrained based on each of the four individual proxies. However, studies which model them individually are unable to differentiate between stocks that have low levels of combined short-sale constraints from those that have high levels of combined constraints. It is our contention that observing the behavior of these constraint variables in isolation or in pairs is not particularly convincing, given the expected interaction with other constraint variables. While more complex, we expect more valid conclusions to be drawn from our full specification, which also accounts for firm size and overall market direction. In the literature, asymmetric effects of individual short-sale constraints in falling versus rising markets have been addressed, as well as whether these individual measures exhibit different behavior for different size firms. Consistent with the approach of Gutierrez et al. (2014), we also test for differences in behavior in falling versus rising markets, as short-sale constraints may be more prevalent in a falling market, but not as relevant as in a rising market. Additionally, different behavior may be forthcoming for different size firms. Smaller firms are generally less liquid, may not have established option structures, and may not capture the attention of institutional investors. A comparison of constraint effects between larger and small firms is also incorporated into this study.",1
42,2,Journal of Economics and Finance,08 February 2017,https://link.springer.com/article/10.1007/s12197-017-9384-z,Family ties and informed trading: evidence from Capitol Hill,April 2018,Serkan Karadas,,,Male,Unknown,Unknown,Male,"Say I find out some information, I tell my wife and she goes and trades on it, what’s the difference? Massachusetts Senator Scott BrownFootnote 1
 Members of Congress (politicians hereafter) have been under public pressure due to the allegations that they use insider information in their stock trades. This pressure played a pivotal role in the bipartisan efforts to pass the Stop Trading on Congressional Knowledge (STOCK) Act, which was signed by President Obama into law on April 4, 2012.Footnote 2 The spouses of politicians also received media attention for their alleged use of nonpublic (i.e., private) information for personal gains.Footnote 3 In addition, CNN uncovered a loophole in the implementation of the STOCK Act regarding politicians’ family members.Footnote 4 The House Ethics committee excluded the family members of U.S. Representatives from complying with the STOCK Act. On the other hand, the Senate Ethics committee required U.S. Senators and their family members both comply with the STOCK Act. The House immediately closed the loophole after the CNN report, but this temporary exclusion of family members from complying with the STOCK Act raised further concerns on whether family members of politicians also benefit from congressional knowledge. Members of Congress are required to periodically report the financial transactions in the accounts that belong to their spouses and their dependent children. Our research question is whether there is evidence of informed trading in the common stock transactions from these accounts. We separately examine the performance of the portfolios owned by politicians’ spouses and their dependent children, since we do not know ex-ante which accounts politicians are more likely to use to take advantage of their private information. The incidence of informed trading in children’s portfolios would suggest that politicians trade through the accounts of their dependent children to exploit their private information. On the other hand, the presence of informed trading in spouses’ portfolios would imply that politicians share value-relevant private information with their spouses. Politicians may extend the benefits accruing from congressional knowledge to their family members for a variety of reasons. First, politicians may lack the expertise in trading stocks and have their family members trade on their information instead. Second, they may want to enhance their family members’ well-being by providing them with value-relevant information. Finally, they may think that there is less publicity associated with their family members’ trades than their own trades. A few academic studies have examined the information content of congressional trading. Using data up to 2001, Ziobrowski et al. (2004, 2011) find that common stock investments of members of Congress outperform the market. Karadas (2015) documents short-term abnormal returns on congressional stock trades over the 2004-2010 period, and these returns are largely determined by power, party membership, and trading experience of politicians. Eggers and Hainmueller (2014) find that politicians’ investments in companies from their districts or states generate 3 % abnormal returns on an annual basis over the 2004-2008 period. These earlier studies on congressional trading lump the transactions by politicians and their family members together, and they are silent on whether the use of congressional knowledge extends to the family members of politicians. Our study is an attempt to fill this gap in the literature. To evaluate the performance of transactions in the accounts that belong to politicians’ family members, similar to Ziobrowski et al. (2004), we construct transactions-based calendar-time portfolios and estimate the abnormal returns on the portfolios that long the buy transactions and short the sell transactions. The calendar-time portfolio approach is commonly applied in measuring the trading skills of retail traders (e.g., Odean 1999; Seasholes and Zhu 2010), and it allows us to measure the information content of the stock transactions in a dynamic fashion under different holding periods. The time frame of abnormal returns provides important insight on the type of information used in congressional stock trades. Short-term abnormal returns would imply the use of time-sensitive private information (Berkman et al. 2014), while long-term returns would suggest the use of information derived from a great degree of familiarity with the firms being invested in (Eggers and Hainmueller, 2014). We obtain the congressional stock transactions from the Center for Responsive Politics (CRP), which makes these transactions data available in a downloadable format starting in 2004. To answer our research question, we focus on the transactions that politicians reported on behalf of their spouses and their dependent children over the 2004-2010 period. This time period presents us with the opportunity to detect the presence of informed trading by politicians’ family members. This is because Congress passed the STOCK Act in 2012 with the committee work starting in 2011. Furthermore, in 2011, a highly influential episode of 60 Minutes and a book by Peter Schweizer (Schweizer 2011) presented anecdotal evidence on how members of Congress and their family members traded on nonpublic information, which helped increase the public pressure on members of Congress.Footnote 5 Therefore, the 2004-2010 period presents a time window before politicians’ incentives to trade on private information and to share private information with their family members may have changed. To measure the effect of the heightened public pressure and recent legislative changes on the trading behavior of politicians and their spouses, we also examine the information content of the congressional stock transactions over the 2011-2014 period. We identify 22,159 common stock transactions that were made from the accounts of politicians’ family members over the 2004-2010 period. We document that the transactions by politicians’ spouses contain value-relevant information that generates abnormal returns exceeding 12 % at a 1-week holding period and 6 % at a 3-month holding period on an annual basis. However, the abnormal returns do not extend beyond three months. These results imply that politicians shared time-sensitive private information with their spouses over the 2004-2010 period. We also do not find any positive abnormal returns for the transactions in the accounts of dependent children. A closer inspection of spouses’ transactions reveals that their portfolio performance improves once we exclude the transactions in their retirement and trust accounts from our analysis. Furthermore, we show that the portfolios of powerful politicians’ spouses outperform the market, while the portfolios of non-powerful politicians’ spouses have an average performance. We also find that when politicians’ spouses trade stocks in industries that fall under committee jurisdictions of politicians whom they are married to, their portfolios earn higher abnormal returns. We conclude our empirical analysis showing that portfolios of politicians and their spouses both underperform the market during the 2011-2014 period, which suggests that public pressure and the STOCK Act altered the trading behavior of members of Congress and their spouses. To the best of our knowledge, this paper is the first academic study that primarily focuses on the performance of common stock transactions in the accounts of politicians’ family members, and how this performance has been affected by public pressure and legislative changes. Thus, we make a modest contribution to the literature on congressional trading. Our study also relates to the work of Berkman et al. (2014), which shows that informed guardians exploit their private information by trading on behalf of babies and young children. We also add to the literature on political connections and political intelligence. For example, Christensen et al. (2017) show that brokerage firms acquire private information for their analysts via their political connections. Gao and Huang (2016) present evidence that political connections enabled hedge funds to exceed the performance of passive portfolios up to 93 basis points a month. However, the performance of these hedge funds significantly deteriorated after the STOCK Act was passed. Our study also contributes to the literature on the effect of federal regulation on insider trading (e.g., Seyhun 1992; Garfinkel 1997). For example, Garfinkel (1997) finds that stricter insider trading restrictions reduce the extent of informed trading by corporate insiders. The rest of the paper proceeds as follows. Section 2 reviews the related literature. Section 3 develops the hypotheses. Section 4 describes the data, and Section 5 outlines the empirical methodology. Section 6 presents the results. Section 7 provides an analysis of politicians’ portfolio performance, and Section 8 examines the role of committee jurisdiction. Section 9 extends the study to the 2011-2014 period, and Section 10 concludes the paper.",5
42,2,Journal of Economics and Finance,08 May 2017,https://link.springer.com/article/10.1007/s12197-017-9395-9,Dividend cuts and predictability,April 2018,Ruey-Shii Chen,Tai-Wei Zhang,,Unknown,Unknown,Unknown,Unknown,,
42,2,Journal of Economics and Finance,13 May 2017,https://link.springer.com/article/10.1007/s12197-017-9396-8,"Hedge fund attributes, insider behavior, and IPO volatility",April 2018,Robert M. Hull,Sungkyu Kwak,Rosemary Walker,Male,Unknown,Female,Mix,,
42,2,Journal of Economics and Finance,16 May 2017,https://link.springer.com/article/10.1007/s12197-017-9394-x,Does wall street affect main street? examining potential spillovers from investor stock market sentiment to personal consumption expenditures,April 2018,KhasadYahu ZarBabal,Jocelyn Evans,,Unknown,Female,Unknown,Female,"The Dow keeps hitting all-time highs and home prices are rising. But many Americans do not feel any richer. That could be bad news for the economy. “Consumers may be skeptical about their wealth,” said Mark Zandi, chief economist at Moody’s Analytics, in a research note. In the past, consumers have spent more when stocks and the value of their homes were climbing...Economists call this the “wealth effect,” but there are concerns that the impact on consumer spending isn’t as pronounced as it used to be. (http://money.cnn.com/2013/03/13/news/economy/stocks-housing-wealth-effect/) It is a long standing empirical result that little to no portfolio wealth effect related to aggregate stock market portfolio returns exists empirically in the U.S. (Case et al. (2013)), or in developing economies such as China (Xuefeng and Xu (2003)). The apparent absence of a traditional stock market wealth effect on spending is puzzling. Well known and respected theories from both Finance and Economics say that stocks should have a consumer wealth effect. In Finance the canonical mechanism is described in consumption-based models of aggregate asset prices in which a representative agent allocates resources between consumption and investment so that expected utility is maximized (Beeler and Campbell (2012) and Dumas et al. (2009)). Similarly, in Economics the permanent income hypothesis implies that permanent changes in future equity wealth and income should have an effect on consumption as people seek to smooth consumption over their lifecycle (Bagchi (2011). As an extension of the literature this study investigates the potential for investor stock sentiment, rather than existing aggregate stock wealth, to affect consumer spending. The contribution is the documentation of a previously undiscovered “sentiment-perceived wealth” effect between aggregate investor sentiment and aggregate consumption of nondurables. The results are consistent with the intuition of an initial occurrence of overspending when investor sentiment is high, and a necessary cut back in subsequent consumption when risk-adjusted stock returns end up lower than previously expected. Individuals’ adjustment of consumption in response to sentiment-based expectations of stock returns also coincides with additional consumption volatility and higher covariance between consumption and the stock market. Additional results confirm that consumption’s realized covariance with the stock market is not related to prior sentiment in a way that reflects rational risk pricing because average realized stock returns are low following periods of high sentiment and vice-versa. Consistent with the existing literature, an aggregate current stock market portfolio wealth effect does not exist in our tests and other factors such as disposable income, housing wealth, savings, and macroeconomic indicators have explanatory power on consumption. Investor sentiment has a strong effect on consumption incremental to all these controls. The results are consistent with individuals adjusting consumption in response to changes in perceived future stock wealth, instead of modifying the amount of their actual current equity investment. To our knowledge, this is the first research paper to find that errors in equity asset pricing affect non-durable spending by consumers. Although links between psychological biases and consumption have been made in prior research (e.g., Bagchi (2011)), previously it was unknown if investor sentiment-based erroneous beliefs about future returns in the equity capital markets play a role in consumers’ fluctuations for non-durable spending choices (Huang et al. (2014)). This finding has important policy implications for regulators that monitor the correlation between the stock market, i.e. Wall Street, on the product market and real economy, i.e. Main Street. Understanding how individuals arrive at their purchasing decisions due to cognitive biases based upon inaccurate stock return judgments is an area of cognitive psychology that has received little attention in the economics and finance literature. Previously, existing papers focus on consumer sentiment as a proxy for the public and investors’ confidence in the overall economy based upon survey assessment of present economic conditions, job availability, the advisability of big ticket purchases, expected changes in business conditions, and respondents income over the next six months. The intuition is that a true economic recovery cannot occur with higher stock prices alone, and that increased consumer spending is sensitive to the perception of lasting economic improvement. The Shiller (2013) discussion about how the financial crisis (starting in 2007) led many to question if biases within the capital market create negative externalities for the society at large. This discussion draws attention to the purported spillover between stock market investor biases and the real economy, but this relationship has little empirical documentation within the academic literature. To date, the study of investor sentiment has a deep literature, but has largely occurred separately from research related to consumption (e.g. Choi (2006), Lansing (2006), and Hirshleifer et al. (2015)).",1
42,2,Journal of Economics and Finance,28 May 2017,https://link.springer.com/article/10.1007/s12197-017-9397-7,Female representation in the boardroom and firm debt: empirical evidence from Italy,April 2018,Fabrizio Rossi,Richard J. Cebula,James R. Barth,Male,Male,Male,Male,"Corporate governance is the system of principles, rules, and regulations covering a firm’s management, practices, and ownership structure. A major determinant of a company’s financial growth and investor confidence levels (Organisation for Economic Co-Operation and Development 2004), corporate governance plays a role as well in the overall development of financial markets and the broader economy (La Porta et al. 2002). A firm’s board of directors plays a central role in its corporate governance mechanisms, including those designed to monitor the opportunistic behavior of managers (Jensen and Meckling, 1976; Fama and Jensen, 1983). In recent decades, studies have examined the relationships between a firm’s performance and specific board characteristics, such as size and composition (e.g., Yermack 1996; Coles et al. 2008; Rossi and Cebula 2015). Furthermore, more recently, particular attention has been paid to the relationship between female board representation and firm performance (e.g., Adams and Ferreira, 2009; Campbell and Minguez-Vera, 2008). Risk aversion has emerged as a factor that may affect firm performance in boards with greater female representation. Over the years, studies looking at confidence in matters of general personal finance and investing have found that women tend to be more risk-averse than their male counterparts (e.g., Prince, 1993; Lundeberg et al. 1994; Barber and Odean, 2001; Charness and Gneezy, 2012; Faccio et al. 2016). The question of whether gender diversity in a boardroom reflects a similar tendency toward risk-aversion and affects the appetite for corporate equity risk motivates a very recent study by Sila et al. (2016). The authors find a tendency among boards with a greater presence of female board members to have lower risk, though there may be firm-specific factors in play. On the other hand, Adams and Funk (2012) conclude that women are not necessarily risk-averse at all—that their attitudes toward risk depend on the environments in which they operate. In Sweden, the authors note, women actually tend to embrace risk more so than their male counterparts. Yet few studies have expressly investigated the relationship between female representation and the corporate debt level (Huang and Kisgen 2013; Alves et al. 2015). Because debt can be seen from two different perspectives – either as a monitoring role with respect to agency costs (Jensen, 1986) or as a proxy for risk taking (Faccio et al. 2016) – an analysis of the relationship between female representation and debt may yield some interesting insights. On the one hand, debt may result in more effective management because it can constrain managers from generating discretionary cash flows (Jensen, 1986). On the other, it may actually incentivize financial risk taking, even resulting in a firm’s default on its debt. But if debt, like a board of directors, represents a control mechanism or if it can be used as such in conjunction with a board, it could arguably limit the opportunistic managerial behavior, reduce monitoring costs, and promote the firm’s growth. And would the greater presence of women in a boardroom have a measurable effect on debt? The prevailing finding in most of the related literature seems to point in that direction—that the presence of women on corporate boards affects financial decisions and can reduce agency costs (e.g., Francoeur et al. 2008; Jurkus et al. 2011). This study uses a sample of 369 firm-year observations during the period 2005–2013 in an effort to verify empirically whether female boardroom representation affects corporate debt level and agency costs. It extends the knowledge on this topic in several ways. First, the numerous corporate scandals that have taken place worldwide, from Enron, WorldCom, and Lehman Brothers in the US to Parmalat in Italy, Volkswagen in Germany, and Toshiba in Japan, have shone a spotlight on effectiveness of the corporate boards in management control and monitoring. To the extent that this study can help determine whether the presence of women on corporate boards promotes greater effectiveness in monitoring agency costs and risk-taking behavior, it adds to the literature on the role of women in the corporate environment. Second, this study looks at Italy, a country that has seen a rapid and remarkable increase in female boardroom representation just since 2011. Historically, women had little presence on corporate boards in Italy, though their share has grown somewhat over the past ten years. But since Parliament passed legislation in 2011 requiring that women constitute one-third of Italian corporate boards by 2015, their relative share has grown significantly. At the end of 2008, the first year for which data are available from the Commissione Nazionale per le Società e la Borsa (CONSOB), Italy’s stock market regulator, there were 170 women (5.9%) on the boards of listed Italian companies. In 2011 the number rose to 193, for a relative share of 7.4%, with 51.7% of companies having at least one woman board member (CONSOB 2014). The percentage then rose from 7.4% in 2011 to 17.8% in 2013, more than doubling in just two years, and 83.5% of listed companies had at least one woman board member, again according to CONSOB (2014). The fact that this study includes the year 2011 and two post-2011 years should be of interest. Third, Italy has a high concentration of listed family-owned companies operating as pyramid-style command structures and employing widespread use of corporate leverage; this increases the private benefits of control and fuels the conflict between majority and minority shareholders. Firms in Italy also rely mainly on bank loans to finance their investments. Most family wealth is invested in the listed family businesses, a circumstance that may limit the possibility of diversifying corporate equity portfolios. Additionally, in the concentrated-ownership environment, corporate debt could be used to mitigate agency costs or expropriate the minority shareholders—or to do both (Faccio et al. 2010). Fourth, Italy is governed by civil law, which generally provides lower levels of shareholder protection than does common law (La Porta et al. 1999; World Bank Group, 2015). This study thus provides an empirical analysis of an environment different from that of previous studies and may yield unique insights into the issues under consideration. Indeed, Italy also ranks rather poorly in terms of protecting creditor rights (La Porta et al. 1998) with a score of just 2 on a scale of 0–12 (World Bank Group, 2015). This analysis deals with a genuinely atypical shareholder protection circumstance. Fifth, this study adopts both the percentage of women in the boardroom and the “critical mass” indicator, introduced by Rosabeth Moss Kanter (1977). The critical mass indicator is a specific level below which female representation is tokenism and likely to be ineffectual. Moreover, this research also examines the relationship between female boardroom representation and debt within a specific time horizon, 2005–2008, which embraces both the pre-financial crisis period and the start of the crisis, as well as the period during the crisis and the recession phase (2009–2013). During the crisis, a severe credit crunch embraced Italy, and this consideration could potentially reveal some interesting results regarding the relationship between women and debt relative to other less problematic periods. It should be observed that this study also investigates the possible difference between the two periods (2005–2008 and 2009–2013) through the use of a dummy variable. Finally, in this study, three different estimation methods are used. In particular, the sample is first econometrically investigated by estimating a random effects model. Next, the estimation is undertaken using a fixed effects model. Thirdly, the estimation is also undertaken using a probit model.",12
42,2,Journal of Economics and Finance,03 July 2017,https://link.springer.com/article/10.1007/s12197-017-9404-z,Differences of opinion and stock market volatility: evidence from a nonparametric causality-in-quantiles approach,April 2018,Mehmet Balcilar,Riza Demirer,Mark E. Wohar,Male,Male,Male,Male,"Numerous studies in the literature have examined the role played by active fund managers in determining stock prices (e.g. Gompers and Metrick 2001; Chen et al. 2002; Kacperczyk et al. 2012; Kaniel and Kondor 2013; and Vayanos and Woolley 2013). One strand of this literature has specifically focused on the effect of divergent beliefs across active fund managers on stock market returns. In an early study, focusing on the dispersion of EPS forecasts across analysts, Ajinkya and Gift (1985) establish a link between divergent beliefs and return volatility, showing that the option implied volatility estimates reflect an incremental component of dispersion in EPS forecasts beyond that can be explained by historical volatility values. Anderson et al. (2005) further show that disagreement among analysts about expected earnings is in fact a priced risk factor in asset pricing models, while it can also predict return volatility out-of-sample. Others including Diether et al. (2002) and Berkman et al. (2009), however, suggest a negative relationship between the level of dispersion in fund managers’ beliefs and subsequent stock returns. Focusing on the channels within which divergent beliefs affect return dynamics, Banerjee (2011) explores two alternative perspectives: rational expectations and differences in opinion. His analysis yields horizon specific results such that a positive relationship between dispersion in beliefs and return dynamics is present at longer horizons (consistent with rational expectations), while the relationship reverses at shorter horizons (in line with the difference is opinion model). More recently, Jiang and Sun (2014) show that both the level and the change in dispersion in fund managers’ beliefs positively predict subsequent stock returns, even after adjusting for risk. Clearly, this issue is not only important regarding the informational efficiency of the stock market, but also allows one to make inferences regarding the value of active management and the information content reflected by active managers’ trades. A number of alternative proxies have been proposed in the literature to measure the divergent beliefs of market participants. These alternative proxies include the dispersion in analyst earnings forecasts (Diether et al. 2002), the breadth of mutual fund ownership (Chen et al. 2002), the dispersion in retail investor trading (Goetzmann and Massa 2005), historical income volatility or stock return volatility (Berkman et al. 2009), and more recently, mutual funds’ active holdings, i.e. deviations from benchmarks (Jiang and Sun 2014). In this study, we utilize data from the weekly surveys conducted by the National Association of Active Investment Managers (NAAIM) and use the dispersion in the equity market exposure of active managers as a proxy of divergent beliefs regarding the stock market. The proxy for divergent beliefs used in this study is similar in spirit to the proxies employed by Chen et al. (2002) and Jiang and Sun (2014) in that it provides a measure of the cross-sectional deviation in active managers’ equity market positions so that the greater the level of heterogeneity in managers’ beliefs, the more dispersed the equity market holdings would be across managers. However, an advantage of the proxy employed in this study is the availability of the equity market exposure data at high frequency, in this case weekly. Equity market exposure data is obtained from the weekly surveys conducted by NAAIM in which active money managers are asked to provide a number that represents their overall equity positions, ranging between 0% (all cash/hedged/market neutral) and 200% (leveraged position). Hence, the first objective of this paper is to capture the dynamic relationship between differences in opinion and realized volatility of returns using the above mentioned proxy. The second contribution of this study is that we employ the nonparametric causality-in-quantiles test of Jeong et al. (2012) to examine the predictability of weekly realized volatility of the S&P500 returns emanating from divergent beliefs of active managers. The causality-in-quantile approach has the following novelties: First, it is robust to misspecification errors as it detects the underlying dependence structure between the examined time series, which could prove to be particularly important as it is well known that equity market returns display nonlinear dynamics (see Bekiros et al. Forthcoming, for a detailed discussion in this regard). Second, via this methodology, we are able to test for causality over the entire conditional distribution of the dependent variable (realized volatility in our case), which is particularly important if the dependent variable has fat-tails – as is the case for financial return series. Consequently, the ability of our method to accommodate for nonlinearity and go beyond the causality at the conditional-mean, as is done in standard linear Granger causality tests and GARCH based models, makes the quantile based approach a more general one, and hence, enhances the possibility of detecting predictability by controlling for misspecification. To the best of our knowledge, this is the first paper that relates the divergent beliefs of active managers to the predictability of realized volatility of the S&P500 using a causality-in-quantile approach. Our tests suggest that dispersion in active managers’ risk exposures to the stock market can predict volatility over the range of quantiles that correspond to moderately high levels of market volatility. This is in fact in line with the previous literature that relates divergent beliefs across investors to subsequent stock returns. However, unlike the previous studies, our tests imply that the effect on subsequent returns is likely to be transmitted via the volatility channel which is consistent with the suggestion by Johnson (2004) that the effect of divergent beliefs on returns can be explained away by financial leverage. Furthermore, given the short-sale constraints that prevent active managers from taking full advantage of negative information, we argue that the effect of divergent beliefs on realized volatility is primarily driven by the large, positive bets placed by informed traders which drives the dispersion in risk exposures across active managers. Overall, our results enhance our understanding of the channels through which divergent beliefs among active managers relate to financial returns and also highlight the importance of detecting and modeling nonlinearity when analyzing the information content of divergent beliefs across market participants. From an investment perspective, the findings underscore the asymmetry in the forecasting power of the dispersion in beliefs with respect to good and bad news and suggest that differentially informed investors can indeed influence return dynamics in financial markets. An important forecasting implication of the findings is the implementation of quantile-based forecasting models for stock market volatility in which divergent belief proxies are used as a predictor only at certain quantiles, while other predictors of market volatility are used at low quantiles. Such a quantile-based forecasting model can then be employed in option pricing models and other valuation exercises. To that end, the quantile-based findings presented in this study have the potential to be used in further related analysis and provide valuable insight to how nonlinearity in market volatility dynamics can be addressed. The rest of the paper is organized as follows: Section 2 presents the methodology, while Section 3 discusses the data and the results. Finally, Section 4 concludes.",4
42,2,Journal of Economics and Finance,13 December 2017,https://link.springer.com/article/10.1007/s12197-017-9422-x,The impact of interest rate volatility on financial market inclusion: evidence from emerging markets,April 2018,Massomeh Hajilee,Farhang Niroomand,,Unknown,Unknown,Unknown,Unknown,,
42,2,Journal of Economics and Finance,15 September 2017,https://link.springer.com/article/10.1007/s12197-017-9410-1,Forecasting inflation in post-oil boom years: A case for regime switches?,April 2018,Vugar Ahmadov,Salman Huseynov,Vugar Rahimov,Male,Male,Male,Male,"Forecasting is central to any decision making process, especially when it deals with far in the future and when its consequences cannot be undone later. Central banks are one of the places where forecasting is undertaken as a routine task and exercised on a daily basis. The reason for this is obvious – any effects of central bank decisions on the economy can only be observed with longer time lags. However, forecasting is not an easy exercise – it requires a considerable amount of time and resources. Failure to correctly forecast economic developments also imposes reputation costs on central banks and wreaks havoc on their credibility. But success at forecasting also depends on the intrinsic predictability of future economic developments. That is, the question is whether future economic developments are predictable given the information set available at present? In a very interesting study, Hendry and Mizon (2014) discuss unpredictability in economic modeling and forecasting. They describe three important sources of unpredictability in economic analysis: intrinsic unpredictability, instance unpredictability and extrinsic unpredictability. They show that our workhorse forecast models (for instance, DSGE models) become unreliable when they are most needed. Edge and Gürkaynak (2010) demonstrate that DSGE models do not fare well in forecasting inflation during the Great Moderation years and that no model is good at forecasting inflation - in fact, inflation become unforecastable during that period. This finding is in line with the results of Stock and Watson (2007). They also show that a despite significant decline in volatility during the Great Moderation years, forecasting inflation has not become easier - its persistence has declined over time. Similarly, Atkeson and Ohanian (2001)﻿ and Groen and Mumtaz (2008)﻿ find that since 1984, Phillips curve forecasts have lost their superiority against naïve inflation forecasting models. According to D’Agostino et al. (2006), the ability of relatively sophisticated models in predicting inflation and real activity in the US has deteriorated during the Great Moderation period. In an inflation forecasting exercise Huseynov et al. (2014) show that almost similar results hold for an oil exporting country, namely Azerbaijan. They demonstrate that despite the considerable decline in economic volatility during the post-oil boom years, naïve models have gained a significant advantage over more complex ones. In fact, according to them, it is impossible to beat a random walk forecast for inflation regardless of which sophisticated models are employed. However, in their forecasting study, they mostly utilize linear models and assume no changes in economic dynamics during that period. In contrary to their study, we investigate inflation forecasting in the post-oil boom era using non-linear methods. We assess the relative performance of non-linear models and examine whether they promote better forecasts of inflation during the same period. For the forecasting exercise we estimate various regime change models encompassing threshold models, smooth transition models, Markov switching models and time varying parameter models. We utilize both univariate and multivariate versions of aforementioned methods and compare their performance with our baseline autoregressive model. We use quarterly data and estimate our models for the period Q1 2003- Q4 2014. We find that for the post-oil boom period of Q1 2012-Q4 2014, non-linear models barely beat the autoregressive model forecasts when they are judged by central forecast accuracy. The quality of these forecasts is even inferior to mean forecasts generated by VAR models. Most importantly, we also show that non-linear models cannot outperform naïve models, such as random walk - a finding which is in line with Huseynov et al. (2014). That is, our results coupled with the evidence from the aforementioned study demonstrate that explaining inflation dynamics with different models at hand is very hard and in fact, inflation is unforecastable for this time period. However, we also find evidence that though non-linear models do not fare well at producing central forecast tendency, they can be confidently used to give probability distributions to future outcomes of inflation. The structure of the paper is designed as follows: the Section 2 discusses data and forecasting methods employed in this study, Section 3 presents the results from forecast comparison experiment, Section 4 discusses implications, causes of the change in inflation process and conclusions.",1
42,2,Journal of Economics and Finance,19 September 2017,https://link.springer.com/article/10.1007/s12197-017-9413-y,Utilitarian preference for redistribution: a concern with max-min,April 2018,George  A. Waters,,,Male,Unknown,Unknown,Male,"Though the thought of sacrificing the well-being of the least fortunate in a group is repugnant to most, the response to The Lottery (Jackson 1948) being a prominent example, developing a rigorous method for determining a desirable degree of redistribution is not trivial. The introduction of heterogeneity into macroeconomic models raises important questions about the proper welfare criterion and the definition of efficiency. The goal of the present paper is to develop a simple model of redistributive taxation and examine the associated political economy issues, particularly Rawls’ conceptions of the Original Position and max-min welfare criterion. The present work examines an environment where wages are heterogeneous due to differences in worker productivity, and the government collects income taxes via a flat marginal rate and redistributes lump sum transfers to all agents, i.e. a negative income tax or basic income plan. There is an extensive margin in labor supply so the income tax may drive some low productivity workers out of the labor force. The primary contribution of the paper is the interpretation of a formal comparison of optimal redistribution regime under utilitarian (Benthamite) welfare criteria and the Rawlsian max-min criterion with minimal assumptions about household preferences. Rawls (1971) advocates for the max-min criterion, where the outcome of the least fortunate member of society is maximized. He argues that the max-min criterion is the rational response of a person behind the “veil of ignorance” a key aspect of the Original Position,Footnote 1 meaning she does not know the realization of personal circumstances when deciding fair and just social relations. In the present environment, the question becomes one of the optimal tax regime when households do not know their level of productivity. While most of the preliminary results have counterparts in the existing literature, the most important result is Proposition 6, which shows that the degree of redistribution satisfying the max-min criterion is necessarily greater than the one that maximizes aggregate utility in an environment with minimal assumptions about preferences. The model formalizes the objection of some philosophers, such as Nagel (1973) and Scanlon (1973), to the max-min criterion that rational agents behind the veil might be willing to risk a bad outcome. Rawls’ Original Position is a valuable concept, but his argument about the rationality of the max-min welfare criterion is suspect. It is common to define welfare using a concave transformation across utilities, where the concavity represents a societal preference for equality above that given by the utilitarian criterion, for example Stiglitz (1987). With such a transformation, the max-min criterion coincides with welfare maximization in a limiting case with respect to the concavity of the transformation. We eschew such transformations representing a desire for equality beyond what is implied by diminishing marginal utility. Whether society-wide agreement about a preference for equality is possible or meaningful is questionable, and the associated transformations are necessarily arbitrary. Though some undoubtedly have such a preference, subjective elements of a model limit it’s relevance. In the present environment without such a transformation, where welfare is simply the sum or expected value of the utilities across the population, the max-min criterion coincides with the tax rate at the peak of the Laffer Curve. The basic income regime is not subject to some common criticisms of redistribution. Under the linear (flat rate) tax, workers do not have incentive to take lower wage jobs to avoid taxes. Also, the lump sum transfer is given to all so there is no distortion of labor supply decisions. For decreasing marginal utility, redistribution is desirable since the gain to low wage workers outweighs the loss for high wage workers. The presence of an extensive margin in labor supply breaks the equivalence between welfare maximization and the usual allocational efficiency condition. In the present model, the usual allocational efficiency condition is obtained when the tax rate is zero and there is no redistribution as long as household choose a positive labor supply at any wage, a questionable assumption. Individually, the elements of the present framework are not novel, but the combination and the focus on the interpretation of the welfare criteria is. These results agree with those found in the standard optimal linear tax model, summarized in Piketty and Saez (2013), but the model here relies only on standard household utility preferences, not on diminishing marginal welfare across utilities on the part of the social planner.Footnote 2 Optimal flat rate or linear tax regimes are studied in Stiglitz (1985) and Dixit and Sandmo (1977). The extensive labor supply margin is included in the models in Mirlees (1971) and Diamond (1980). Atkinson (1973) analyzes optimal taxation under the max-min criterion. Manski (2014) also considers simple tax regimes in a model with government production that affects consumption and productivity. He discusses informally some of the issues around efficiency in the present work. The paper is organized as follows. Section 2 describes the elements of the model and some of the desirable properties of the basic income regime. Section 3 discusses the formalization of the welfare criteria. Section 4 reports the primary formal results of the paper related to optimal tax rate. Introduce a particular calibration of the model to show the quantitative relevance of the results and their relation to the literature on income distribution. Section 5 concludes.",
42,2,Journal of Economics and Finance,24 November 2017,https://link.springer.com/article/10.1007/s12197-017-9420-z,The political economy of local fracking bans,April 2018,Joshua C. Hall,Christopher Shultz,E. Frank Stephenson,Male,Male,Unknown,Male,"In recent years, there has been rapid growth in the use of hydraulic fracturing (“fracking”), a process in which water and chemicals are forced into shale formations lying deep underground in order to release natural gas trapped in the shale formations. However, using fracking to extract previously unavailable underground fuel resources has been controversial. Proponents tout local economic benefits such as jobs created by drillers, wider economic benefits arising from cheaper natural gas, and reduced carbon emissions relative to other fuels such as coal (Joskow 2013; Hefner 2014). Opponents, on the other hand, argue that fracking leads to underground tremors, heavy truck traffic near drilling sites, and environmental harms such as contaminated water (Vengosh et al. 2014). The water pollution claims are depicted in the 2010 movie Gasland, which shows flammable faucet water allegedly contaminated by fracking.Footnote 1
 The perception that fracking is environmentally harmful has led to efforts to ban the practice (Brady and Crannell 2012). Although fracking bans have been proposed in various parts of the country (Rahm 2011; Brady and Crannell 2012), the state of New York has been a hotbed of regulatory activity at both the state and local level (Wegener 2013). Local governments in New York have been particularly active in enacting fracking bans or moratoria (Podolny 2013). The primary reason that the anti-fracking movement has been so active in New York is that the western part of the state lies above the Marcellus shale, a formation that has proven rich for fracking in neighboring Pennsylvania (Jacquet 2012). Fearing that their state might see the high level of fracking activity that has occurred in neighboring Pennsylvania, many New York localities used local zoning laws to pass bans or moratoria (Podolny 2013; Wegener 2013). In December of 2014, New York Governor Andrew Cuomo announced that the state would become the first state to prohibit high-volume hydraulic fracturing (Kaplan 2014). In this paper, we analyze the pattern of fracking bans imposed in New York townships through a political economy lens. While there is a small literature on the political economy of environmental policies related to energy (Joskow and Schmalensee 1998; Bernauer and Koubi 2009; Wiener and Koontz 2012), the political economy of fracking bans has received little attention. Papers such as Davis (2014) and Ritchie (2014) discuss the legal issues surrounding local regulation of fracking while Davis (2012) shows how pro-fracking interests, seeking a regulatory environment most favorable to their position, have sought to have fracking regulated at the state level while anti-fracking advocates have sought to use federal regulation to impede fracking. Warner and Shapiro (2013) discuss local government challenges to fracking and state regulatory responses. Nolon and Polidoro (2012) also discuss the tensions between state and local authorities with respect to fracking. Simonelli (2014) uses interviews and oral histories to discuss the local tensions surrounding fracking bans in New York. One exception to the lack of empirical research on local fracking bans is Walsh et al. (2015). Also using a spatial econometric approach, they find that location in the Utica shale, proportion of registered Democrats, and education levels of citizens increase the likelihood of a locality passing a local fracking ban.Footnote 2
 In determining the social, political, and economic factors driving municipalities in New York State to pass a ban or moratorium on fracking, we make two important contributions. The first contribution is to public policy debate surrounding fracking bans. Our positive empirical approach, while taking no normative stance, clearly informs ongoing public policy battles regarding fracking in states such as Colorado, Ohio, and West Virginia (Goho 2012; Minor 2013). For example, our results show that the percentage of college educated citizens is positively associated with local bans or moratoria. Our second contribution is that we show that spatial dependence exists in the adoption of laws related to energy policy. Not accounting for spatial dependence can cause non-spatial papers to provide incorrect estimates of the marginal effects of explanatory variables (LeSage and Pace 2009; Hall and Ross 2010; Holloway et al. 2014). In addition, there can be sign reversal (LeSage and Dominguez 2012). This finding is especially important for scholarship building off the regional policy diffusion model (Wiener and Koontz 2012), but is also important for any paper using a law or policy as an explanatory variable. Our paper proceeds as follows. Section 2 briefly surveys the literature on the political economy of public policy adoption, including the theoretical basis for the regional diffusion of such policies. The next section describes the data and the Bayesian Spatial Durbin approach employed, while Section 4 presents the empirical results. We conclude with some thoughts on our results and the implications of our paper for the overall literature on policy adoption literature as well as ongoing fracking debates.",2
42,2,Journal of Economics and Finance,27 July 2017,https://link.springer.com/article/10.1007/s12197-017-9407-9,Revisiting the standard lease valuation model: new results,April 2018,James A. Miles,John R. Ezzell,Premal P. Vora,Male,Male,Unknown,Male,"More than forty years have passed since what is now considered to be the textbook or the standard model of valuing lease contracts was published by Myers et al. (1976). Although a number of innovations in valuing lease contracts have subsequently appeared in the literature (see, for example, Franks and Hodges (1978) and Lewis and Schallheim (1992)), the popularity and resilience of this model is testimony to its intuitive structure, flexibility, and ease of use.Footnote 1 Despite this popularity, little has been written about the necessary or sufficient conditions that are implied by the model for leasing to have a net tax advantage. We explore this question and arrive at some novel and interesting—even surprising—conclusions. The standard model implies that lease contracts between lessees and lessors having the same tax rate cannot be explained in terms of a tax savings motivation.Footnote 2 When the lessee and lessor are taxed at different rates, this model is commonly understood to mean that some assets are leased because of the tax savings on the depreciation write-offs that lease contracts transfer from low tax rate lessees to high tax rate lessors. We demonstrate in this paper, however, that the transferred depreciation write-offs are never sufficient for lease contracts to have a net tax advantage. Further, we demonstrate that the standard leasing model actually implies it is the interest tax savings on the debt capacity that a lease contract transfers from the low tax rate lessee to the high tax rate lessor that are necessary for leasing to have tax advantages, i.e., unless the lessor levers the lease it is impossible for the contract to have a net tax advantage. The necessity of the interest tax savings in explaining why some assets are leased instead of purchased has been obscured by the emphasis the literature commonly places on the depreciation tax savings benefit (Berk and DeMarzo (2014, p. 877) and Brealey and Myers (2017, p. 665)). Further, while the value of the depreciation tax savings depends mainly on the tax code, the value of interest tax savings depends critically on such economic asset characteristics as asset life, cash flow timing and risk, the magnitude and risk of salvage value, and on such lease contract provisions as lease payment schedules, penalty clauses, asset use restrictions, tie-in sales, metering and maintenance clauses. However, asset characteristics and contract provisions also have implications for the agency effects of leasing. Thus, our analysis demonstrates that an interaction exists between the tax and agency effects of leasing. Prior literature on leasing has failed to recognize this interaction as a possible source of gains from leasing.Footnote 3
 Our exploration and analysis of the model’s implications has relevance to the equipment leasing industry and to academia. For the former, it is relevant because it addresses prior misconceptions about the source of the tax advantage of leasing and it provides insights into structuring a lease in a way that maximizes its tax advantage. For the latter it has relevance—in addition to both of the reasons cited for the industry—because our analysis of the standard leasing model finally gives an intuitive explanation of why the net tax advantage to leasing is negative for a number of textbook problems. We also discuss what parameter changes in these problems are likely to produce a non-negative net advantage. The paper proceeds as follows: In Section 2 we show that the lessor’s unlevered net advantage to leasing (NAL) is necessarily negative. In Section 3 we show that interest tax savings on debt supported by the after tax lease payments is necessary for the lessor’s NAL to be non-negative and that the magnitude of the interest tax savings depends on the time pattern of the lease payments. In Section 4 we consider the effects of various asset characteristics and lease contract provisions on the magnitude of interest tax savings on lessor debt. We conclude in Section 5.",2
42,3,Journal of Economics and Finance,30 June 2017,https://link.springer.com/article/10.1007/s12197-017-9402-1,Another look at the determinants of the financial condition of state pension plans,July 2018,James R. Barth,Sunghoon Joo,Kang Bok Lee,Male,Unknown,,Mix,,
42,3,Journal of Economics and Finance,12 June 2017,https://link.springer.com/article/10.1007/s12197-017-9400-3,Reassessing the effects of bilateral tax treaties on US FDI activity,July 2018,Abdullah Kumas,Daniel L. Millimet,,Male,Male,Unknown,Male,"The impact of bilateral tax treaties on foreign direct investment (FDI) is surprisingly unclear. This ambiguity exists amid fairly pervasive empirical evidence that cross-country variation in taxation does influence the distribution of FDI activity (e.g., Gresik 2001; Gordon and Hines 2002; De Mooij and Ederveen 2003; Blonigen 2005: Davies et al. 2009: Blonigen et al. 2014), as well as the fact that tax treaties are costly to negotiate and implement, yet nonetheless cover much of today’s bilateral FDI activity. Specifically, the number of tax treaties in force has increased from 100 in the 1960s to over 2500 more recently (Egger et al. 2006). The US presently belongs to roughly 60 such treaties, covering approximately 78% of total US outbound FDI and 96% of total US inbound FDI, with over one-third being implemented since 1990 (Blonigen and Davies 2004). Furthermore, more than 40% of all US imports in 2009 were between related entities (The US Census Bureau). In light of these statistics, and the well known facts regarding the growing importance of FDI, understanding the effects, if any, of bilateral tax treaties on FDI activity is paramount.Footnote 1
 While the theoretical literature on bilateral tax treaties is more developed, empirical studies are relatively sparse. Blonigen and Davies (2004, 2005) find strong positive effects of ‘old’ tax treaties on FDI, but negative effects of ‘new’ tax treaties, using 1980–1999 US and 1983–1992 OECD data, respectively, particularly when modeling FDI in levels (as opposed to logs).Footnote 2 Egger et al. (2006) also obtain a significant negative effect of ‘new’ tax treaties on OECD outbound FDI from 1985 to 2000 using a difference-in-difference propensity score matching estimator. On the other hand, di Giovanni (2005) analyzes cross-border capital flows for mergers and acquisitions from 1990 to 1999 and finds positive effects of tax treaties. Similarly, Stein and Daude (2007) obtain a positive and statistically significant effect using data on OECD outbound FDI stocks from 1997 to 1999. Millimet and Kumas (2009) obtain some positive effects when allowing for both anticipatory and lagged effects of treaties, but primarily for US inbound FDI only. Finally, Davies (2003a) finds no effect of revisions of existing bilateral tax treaties on FDI, and Hartman (1985) and Sinn (1993) find that the expansion of activities of multinational enterprises (MNEs) is essentially independent of withholding taxes. Davies (2004) provides an excellent review.Footnote 3
 The mixed, and perhaps counter-intuitive, empirical results - as well as the salient role played by the decision to model FDI in levels or logs - could be an artifact of the empirical approach that overshadows nearly all of these previous studies: the focus on the mean impact of bilateral tax treaties. As is clear from the literature, there are three likely effects of tax treaties. First, tax treaties may remove barriers to FDI through reductions in withholding taxes and double taxation, as well as by clarifying tax definitions and providing methods for dispute resolution. For example, the introduction to the OECD’s model tax treaty states that primary goal of such treaties is ""removing the obstacles that double taxation presents"" to decrease its ""harmful effects on the exchange of goods and services and movement of capital, technology, and persons"" (OECD 1997, p. I-1). Similarly, the United Nation’s draft manual for negotiating bilateral tax treaties states that the aim of such treaties is to “encourage economic growth by mitigating international double taxation and other barriers to cross-border trade and investment” (p. 2).Footnote 4 Moreover, existing tax treaties always reduce withholding tax rates (Blonigen and Davies 2004). However, the second likely effect of tax treaties is the reduction of transfer pricing and other means of tax avoidance. In fact, the UN’s draft manual continues by stating that a second objective is ""to improve tax administration in the two Contracting States by reducing opportunities for international tax evasion"" (p. 2). Radaelli (1997) and Gravelle (1988) assert that reducing tax evasion is the primary goal of US tax treaties. Finally, tax treaties may promote FDI by reducing uncertainty over the tax environment abroad. See Davies (2004) and Blonigen and Davies (2005) for further discussion.Footnote 5
 While these effects may push FDI in opposite directions, this does not imply that estimating the mean impact of tax treaties provides much useful insight if the magnitudes of the two competing effects are heterogeneous across countries. In terminology from the program evaluation literature, knowledge of the average treatment effect (ATE) or average treatment effect on the treated (ATT), while interesting, may not provide much guidance to policymakers of the likely effects of specific bilateral tax treaties; focusing on the ATE or ATT from signing a bilateral tax treaty conceals the underlying heterogeneity of these effects. The importance of this heterogeneity is brought to light in Blonigen and Davies (2004) who estimate treaty-specific effects for several bilateral tax treaties with the US, finding some positive, some negative, and some statistically insignificant effects. In this paper, we provide a more formal assessment of the potential heterogeneous effects of bilateral tax treaties. To begin, we motivate our move beyond the focus on mean impacts through a simple theoretical model illustrating why the effects of a bilateral tax treaty on FDI activity may vary across country-pairs. In particular, under certain scenarios, the impact of a tax treaty on FDI activity is shown to be positive (negative) if FDI activity at the time of the treaty is low (high). This occurs because the net change in the marginal effective tax rate is positively correlated with FDI activity at the time the treaty becomes effective. For example, if a country currently faces a low withholding tax rate and/or has opportunities to engage in tax avoidance, then FDI activity prior to the treaty is likely to be high. However, a tax treaty in such a situation is likely raise the marginal effective tax rate. Similarly, if a country currently faces a high withholding tax rate and/or has few opportunities to engage in tax avoidance, then FDI activity prior to the treaty is likely to be low, but a tax treaty in such a situation is likely lower the marginal effective tax rate. To assess this empirically, we utilize data on US inbound and outbound FDI stocks, flows, and FAS over the period 1980–1999. The data are from Blonigen and Davies (2004), and thus enable us to compare our findings to the existing literature. Our empirical strategy entails estimating the distributions of FDI while adjusting for covariates using semi-nonparametric methods, and then examining the quantile treatment effects (QTEs). Our results are striking, indicating important effects of tax treaties on the distribution of FDI that are consistent with our theoretical framework. Specifically, the impact of tax treaties is not homogeneous across quantiles, and even though the individual QTEs are imprecisely measured we tend to find non-negative effects of tax treaties at lower quantiles and negative effects at upper quantiles. Moreover, also consonant with our simple theoretical set-up, the distributional results, particularly when FDI is measured in levels, are roughly symmetric across US inbound and outbound FDI. Finally, the distributional results provide some important insight into why estimates of average effects differ depending on the use of a level or log specification. The remainder of the paper is organized as follows. Section 2 provides a theoretical framework. Section 3 describes the empirical methodology and data. Section 4 presents the results, while Section 5 concludes.",5
42,3,Journal of Economics and Finance,17 June 2017,https://link.springer.com/article/10.1007/s12197-017-9399-5,Hedging and hedging effectiveness under required disclosures: a study of the impact of derivatives use on capital investment,July 2018,Hong V. Nguyen,,,,Unknown,Unknown,Mix,,
42,3,Journal of Economics and Finance,20 June 2017,https://link.springer.com/article/10.1007/s12197-017-9398-6,Product innovation in China’s food processing industries,July 2018,Sizhong Sun,Sajid Anwar,,Unknown,Male,Unknown,Male,"Food and shelter are regarded and the basic human needs. Irrespective of the level of economic development, few countries around the globe are prepared to completely rely on food imports. Developing countries with large populations are particularly concerned about food security. While some developing countries, such as China and India, are the largest producers of agricultural products, due to quality concerns, developed counties are not keen to import food products from developing countries. In recent years, serious concerns have been raised regarding the quality of some Chinese made processed food items. While a number of studies have considered the issue of food security, this paper focuses on the innovation activities of firms in China’s food processing industry. This issue is important because through innovation activities product quality can be improved. A number of studies have explored various aspect of the Chinese food industry. For example Zhou (2010), Liao (2010), and Zhu (2011) considered the issue of food security, whereas Sun et al. (2014), Wei et al. (2012), and Scott et al. (2014) focused on food safety issue. This paper focuses on innovation activities of firms in China’s food processing industries, an issue that has not received much attention in the existing literature. Innovation, which plays an important role in a nation’s economic development, is also an important determinant of competitiveness. At the microeconomic level, firms that innovate are successful in producing higher quality products. Innovation can also help to lower the cost of production, which allows a firm to gain competitive advantages over its competitors. At the macroeconomic level, within the context of food processing industries, firm that innovate are also likely to pay more attention to food safety issues (e.g., through producing higher quality products). Firms that innovate can also reduce the cost of production thereby increasing their production capacity, which can also address the issue of food security. Despite the importance of innovation in food processing industries, surprisingly few studies have focused on the behaviour of Chinese firms. By focusing on innovation activities of firms in China’s cereals milling, feed processing, and meat products and by-products processing industries, this paper aims to fill this gap in the existing literature. Innovation has both input and output dimensions. Investment in research and development (R&D) can be viewed as an input into the innovation process. As far as the output dimension of innovation is concerned, Oslo Manual (2005) categorize innovation, by its output, into four types: (i) product innovation, (ii) process innovation, (iii) marketing innovation and (iv) organisational innovation. In this paper, owing to the availability of a comprehensive firm level dataset, we focus on both the R&D investment (input) and product innovation (output) undertaken by the firms in China’s three food processing industries. A good understanding of the innovation activities undertaken by the firms can also help the policymakers to develop more effective strategies to deal with food security and safety issues. The rest of the paper is organised as follows. Section 2 contains a brief review of the related literature. Using both the input (i.e., R&D) and output (i.e., product innovation) perspectives, an analysis of the firm behavior is China’s three food processing industries is presented in Section 3. Section 4 contains an econometric analysis of the factors that affect product innovation in China’s food processing industries. Conclusion and policy implications are presented in Section 5.",
42,3,Journal of Economics and Finance,04 July 2017,https://link.springer.com/article/10.1007/s12197-017-9401-2,Some determinants of life expectancy in the United States: results from cointegration tests under structural breaks,July 2018,Natalya Ketenci,Vasudeva N. R. Murthy,,Female,Unknown,Unknown,Female,"Health economists and health policy makers in developing and developed countries are interested in learning better ways to improve the health status of their citizens. Life Expectancy (LIFE) at birth is considered an important measure of health status. In order to increase the longevity of the people of a country, health policy makers need to know at least some of the major determinants of life expectancy in that country. This observation is very relevant to the US, where health care issues are of paramount importance and are the subject of constant discussion. One important observation that is frequently noted in the literature is that although the US spends a high percentage of its gross domestic product (GDP) on health care compared to other developed countries, especially the Organization for Economic Co-operation and Development Countries (OECD), the gain in life expectancy between 1960 and 2012 was modest. For instance, the LIFE at birth in the US, which stood at 1960 at 69.9 years, increased to 78.7 years in 2012, a difference of 8.8. In comparison, in Japan during the same period it increased from 67.8 to 82.7 years, a difference of 14.2. OECD countries that witnessed rapid gains in LIFE during the same period were Germany and Luxemburg. It is important to note that while in the US health care expenditure as a percentage of GDP increased from 5.1% in 1960 to 17.7% in 2011, in Japan it increased from 3.0% to 9.6%, respectively. It is equally noteworthy that in Germany the corresponding figures were 6.0% in 1970 and 7% in 2011. The OECD publication,Footnote 1 Health at A Glance 2013 (OECD 2013b), states that “In the United States, the life expectancy for both women and men is now slightly shorter than the OECD average, and the gap with the leading countries has been widening” (p.26). The question, then, is if the percentage of GDP spent on health care expenditure is not really the major factor in improving health status, what other factors might explain the gain in LIFE? A brief literature survey shows that several studies investigate the main determinants of life expectancy in the US and OECD countries, among them, Cutler and Lleras-Muney (2007, 2012); Ricci and Zachariadis (2008); Cutler et al. (2008); Kiuila and Mieszkowski (2007); Deaton et al. (2006); Phelan and Link (2003); Shaw et al. (2005); Nixon and Ulmann (2006); Kitagawa and Hauser (1973); and Grossman (1972). None of these studies applies the unit root testing and cointegration analysis to study the determinants of life expectancy in the US. Therefore, this paper, employing the recent methods of unit root testing and cointegration analysis, in addition to structural breaks consideration, attempts to identify empirically some major determinants of life expectancy in the US during the period 1960–2012.",14
42,3,Journal of Economics and Finance,11 July 2017,https://link.springer.com/article/10.1007/s12197-017-9403-0,Overnight versus day returns in gold and gold related assets,July 2018,Laurence E. Blose,Vijay Gondhalekar,Alan Kort,Female,,Male,Mix,,
42,3,Journal of Economics and Finance,25 July 2017,https://link.springer.com/article/10.1007/s12197-017-9405-y,Mutual fund herding and reputational concerns,July 2018,Marius Popescu,Zhaojin Xu,,Male,Unknown,Unknown,Male,"Recent empirical work has documented strong evidence that institutional investors, as a group, herd at individual securities level, as well as at the industry level, and that this behavior is pervasive across markets around the world. The literature has provided two sets of motivations for herding behavior: information-based explanations (investigative herding and informational cascades) and non-information-based explanations (reputational/career concerns, underlying investor flows, and style trading strategies). In this paper, we examine whether mutual fund managers’ career concerns contribute to mutual fund herding behavior. Scharfstein and Stein (1990) theorize that mutual fund managers may herd because they face a reputational cost from acting differently from the herd. Additionally, Popescu and Xu (2014) argue that the reputational cost is higher in down markets, and, therefore, professional money managers will have a stronger incentive to herd in down markets relative to up markets. Specifically, during down markets, deviating from the herd may lead to job loss in most cases (Chevalier and Ellison (1999), but it could also lead to the complete destruction of the manager’s reputation, and even to the possibility of never finding another job in the industry (Kempf, Ruenzi and Thiele (2009)). Consistent with this argument, we find that mutual funds herd, on average, 71% more in down markets than in up markets. We further investigate the reputational herding hypothesis by examining whether herding propensity varies across funds that have different career concerns. Brown, Harlow and Starks (1996) argue that the mutual fund industry can be viewed as a tournament, and that poorly-performing fund managers have incentives to alter the composition of their portfolios when competing with their peers for fund flows. Consistent with this view, Khorana (1996) finds that there is a high probability that managers of poorly-performing funds end up losing their jobs. We argue that losing funds will attempt to implement similar strategies and/or to replicate winning funds’ strategies, as they try to catch up with their successful peers. Consequently, we expect losing funds to exhibit a stronger propensity to herd than winning funds. Consistent with our expectations, we find that poorly performing funds herd, on average, 17% more than well performing funds. Finally, we examine whether winning and losing funds exhibit different herding propensities across the two market states. Kempf, Ruenzi and Thiele (2009) argue that a poorly performing manager’ career concerns will be stronger in down markets, as the threat of the fund closing down and the manger losing his job is more severe. Consequently, we expect losing funds to herd more in down markets than in up markets, and also their herding propensity in down markets to exceed that of winning funds. Consistent with this argument, we find that losing funds herd, on average, 110% more in down markets relative to up markets. Furthermore, we find that the average losing fund herds 61% more than the average winning fund in down markets. We also perform robustness tests to control for other potential explanations for the difference in herding behavior across market states. Specifically, we control for the impact of “style” investing strategies, such as mutual funds’ preference for stocks with high past returns (Nofsinger and Sias (1999), Wermers (1999, 2000)), or for stocks of certain size or book/market ratio (Falkenstein (1996), Barberis and Shleifer (2003), Choi and Sias (2009)). Our results are robust to these alternative explanations, and therefore, suggest that fund managers’ career concerns contribute to mutual fund herding behavior. The remainder of the article is organized as follows. Section 2 reviews related literature and our paper’s contribution. Section 3 describes the data and provides summary statistics. In sections 4 and 5, we empirically examine the reputational herding hypothesis. Section 6 provides the robustness tests to support our main findings, and section 7 concludes.",5
42,3,Journal of Economics and Finance,16 August 2017,https://link.springer.com/article/10.1007/s12197-017-9408-8,Same-sex marriage legalization and wedding tourism: evidence from Charleston and Savannah,July 2018,Michael Earhart,E. Frank Stephenson,,Male,Unknown,Unknown,Male,"In recent years, the potential or actual legalization of same-sex marriage (SSM) in the United States spurred research on its anticipated or actual effects. For example, Alm et al. (2000) predict the income tax consequences of legalizing SSM, finding that federal income tax revenue would increase by up to $1.3 billion following SSM legalization. Lewis (2005) considers the effect that SSM legalization referenda on the ballot in thirteen states had on the 2004 presidential election. Langbein and Yost (2009) and Allen and Price (2015) examine the effects of SSM legalization on marriage, divorce, abortion, out-of-wedlock birth, and female parent householder rates. Similarly, Francis et al. (2012) analyze the relationship between SSM laws and sexually transmitted disease infections, and Hatzenbuehler et al. (2012) study the effect of SSM on health care use and expenditures of homosexual men. SSM legalization has also led to claims or predictions of an increase in the number of weddings and a commensurate increase in economic activity. For example, Mallory (2015) claims that 96,000 SSMs occurred in the four months following the U.S. Supreme Court’s 2015 Obergefell ruling. O’Neill et al. (2014a) predict that in the first three years of SSM legalization, Georgia would see 10,659 additional marriages generating $78.8 million in additional spending within the state on wedding arrangements and tourism by wedding guests. Likewise, O’Neill et al. (2014b) predict neighboring South Carolina would see 3607 marriages generating $24.8 million in total spending within the state. Indeed, Ware (2015) quotes a Charleston, SC event planner saying that she has “seen a direct economic impact to our small business” as a result of SSM legalization in South Carolina. This paper tests for economic impacts associated with SSM legalization by examining daily hotel occupancy data from Charleston, SC and Savannah, GA, two popular wedding destinations. These two coastal cities offer an interesting “natural experiment” for studying the effects of SSM because they are both wedding destinations but have different SSM legalization dates. As a result of a ruling from the Fourth Circuit Court, SSM was legal in Charleston effective November 20, 2014. Savannah, by contrast, did not have legal SSM until the Obergefell ruling on June 26, 2015. If claims that SSM legalization and the accompanying increase in weddings lead to an inflow of visitors and a boost in economic activity are correct, then there should be an increase in hotel occupancy. In addition to analyzing the wedding tourism effects of SSM legalization, this paper makes a methodological contribution by using daily hotel room rental data to look for increased visitor inflows to Charleston and Savannah following SSM legalization. Monthly hotel occupancy data were used by Lavoie and Rodríguez (2005) to examine the economic impact of professional basketball and hockey teams on Canadian cities. Likewise, Toma et al. (2009) use hotel tax receipts to study Savannah tourism following the publication of Midnight in the Garden of Good and Evil. Similarly, Baumann et al. (2009) use daily airline passenger arrivals in their study of Hawaiian sports tourism. To date, however, there are no published papers using daily hotel occupancy data to examine economic impacts of policy changes such as SSM legalization. Since most weddings occur on weekends and the number of weekends in a month may vary, the use of daily data is an improvement over the use of monthly (or less frequent) data. A cautionary note is in order before turning to the analysis. Like the previous research on SSM legalization cited above, this paper examines the economic consequences of SSM legalization. Whether SSM should be legal or not is an ethical and legal question that lies beyond the scope of this paper and nothing in this paper should be construed as taking a position on whether SSM should or should not be legal.",2
42,3,Journal of Economics and Finance,16 September 2017,https://link.springer.com/article/10.1007/s12197-017-9412-z,Asymmetric mean reversion and volatility in African real exchange rates,July 2018,Saint Kuttu,,,Unknown,Unknown,Unknown,Unknown,,
42,3,Journal of Economics and Finance,20 September 2017,https://link.springer.com/article/10.1007/s12197-017-9411-0,Analysis of public investments and economic growth in Cameroon,July 2018,Augustin Ntembe,Aloysius Ajab Amin,Regina Tawah,Male,Male,Female,Mix,,
42,3,Journal of Economics and Finance,31 October 2017,https://link.springer.com/article/10.1007/s12197-017-9415-9,Does a CEO’s hedging ability affect the firm’s capital structure?,July 2018,Lee M. Dunham,,,,Unknown,Unknown,Mix,,
42,4,Journal of Economics and Finance,30 November 2017,https://link.springer.com/article/10.1007/s12197-017-9418-6,"Electrification, the Smoot-Hawley tariff bill and the stock market boom and crash of 1929: evidence from longitudinal data",October 2018,Bernard C. Beaudreau,,,Male,Unknown,Unknown,Male,"The stock market boom and crash of 1929 has garnered much attention over the last eight decades (DeLong and Shleifer 1991; Rappoport and White 1993), with a resurgence of interest in 2004 when Ellen McGrattan and Edward Prescott presented evidence which supported Irving Fisher’s view that the surge in stock prices could be justified by improving fundamentals. Despite being unable to identify a root cause (e.g. patents, organizational capital, technological change), their analysis generated estimates of new, intangible capital in the order of 23% of GDP. This paper draws on earlier work (Beaudreau 2014) to propose a variant and indeed a refinement of the Fisher-McGrattan-Prescott fundamentals view, namely that the stock market boom and crash of 1929 (and 1928) tracked the successes and failures of U.S. tariff reform in 1928 and 1929 (e.g. the proposed Smoot-Hawley Tariff Bill-SHTB) against a background of vastly improved electrification-based fundamentals. Specifically, by tracking the good and bad tariff-related news, we are able to rationalize both the increase as well as the decrease in stock prices. In previous work, Beaudreau (2014) set out to discriminate between two tariff-based the- ories of the stock market crash (October 23 and October 28) using longitudinal event study data.Footnote 1 According to Wanniski (1976), the stock market crash came on the heels of the defeat of the Thomas Recommittal Plan on October 21, 1929, which would result in higher tariffs on manufactures, an ensuing breakdown of world trade and ultimately, a recession. The defeat of the Thomas Recommittal Plan and the first stock market crash were contemporaneous, or so they appeared. Beaudreau (1996, 2005), however, offered an alternative account, one which com- bined tariff policy with macroeconomic fundamentals. In short, he argued that the stock market crashed on October 23 as the result of two, related events, namely the defeat of the Thomas Recommittal Plan and a vote in Senate to reduce the tariff on medicinal tannic acid. The Thomas Recommittal Plan called for a fundamental change in the scope of the Smoot-Hawley Tariff Bill, specifically, of recommitting the bill to raising rates on agricultural goods only. The Insurgent Republican-Democrat coalition that controlled the Senate from July 23, 1929 pro- posed reducing rates on manufactures. The following day, it reduced the tariff on medicinal tannic acid, prompting the first stock market crash (October 23-24). The second crash (October 28), he argued, came on the heels of the Reed Declaration, a speech by Pennsylvania Senator David E. Reed in Philadelphia in which he predicted that the SHTB would die in the current session. In other words, the anticipated increase in sales, profits and dividends promised by the Republicans under the banner of “Prosperity for All” would not materialize. The key element in his argument was the role of fundamentals in the drafting of the SHTB. Drawing from various sources, he argued that the SHTB was in large measure a response on the part of the Republican Party to electrification-based excess capacity. By further restricting access to the U.S. market, domestic manufacturing firms stood to benefit from higher factory utilization rates, sales, profits and dividends. Longitudinal stock price data were used to test this theory. Specifically, using the U.S. Congressional Record, he coded tariff-related legislative news as either good or bad, with the former referring to news to the effect that the Bill would be passed or its scope expanded, and the latter referring to the reverse. As his data set was limited to legislative SHTB news, his sample period consisted of January 1, 1929 to October 30, 1929. His results showed that stock prices were highly positively correlated with tariff news, finding a correlation coefficient of 0.52. More importantly, he showed that the two stock market crashes could be attributed to bad tariff news—and hence, to a fundamental shift in investor expectations. Implicit in his account of the events of October 21–29, 1929 (the two stock market crashes) is a theory of the stock market boom and crash of 1928–1929, one based on (i) the presence of excess capacity in U.S. manufacturing, the result of electrification and the introduction of high throughput continuous-flow production techniques (Fisher 1930) and (ii) the Republican Party’s proposed tariff bill promising “prosperity for all.” Firms finding themselves with excess capacity would stand to gain the most from the proposed greater market share resulting from higher external tariffs. In this paper, we test this hypothesis using individual stock price data. Specifically, we test the hypothesis that stock prices of firms in industries that electrified the most—and hence, would have experienced the greatest increase in their rated capacity—would have responded more to SHTB electoral news (prior to 1929), and legislative news in 1929. Implicit in this view is the idea that variations in stock market prices in these industries during the period June 1928–November 1929 would have been tracking/responding to tariff bill based electoral and legislative news. Using individual Dow Jones Industrial Average stock prices, we were able to replicate Beaudreau (2014)‘s findings for individual shares, and more importantly, were able to show the presence of a statistically-significant relationship between the strength of individual firms share-price response to tariff news (as measured by the estimated coefficient) and a measure of industry electrification, confirming Irving Fisher’s view of the stock market boom was based on fundamentals.Footnote 2
 The paper is organized as follows. To begin with, we examine the role of electric power in U.S. manufacturing in the 1910s and 1920s. We then turn and examine the Republican Party’s response to weakening product and labor markets in late 1927/early 1928. In short, the Party under the guidance of ranking Republicans Reed Smoot, Joseph Grundy and John Fisher proposed another round of tariff hikes aimed at securing a larger share of the U.S. market for U.S. firms. Having won the 1928 presidential election on a platform of tariff reform and “Prosperity for All,” the Party went ahead with tariff reform, introducing a new tariff bill in the House on January 7, 1929. By July 23, the Party had splintered over the proposed SHTB, with thirteen Senators (Insurgents) crossing the floor to vote with Democrats. By October, they had resolved that not only would tariffs not rise, they would fall. This tariff bill “about-face” constitutes the basis for the proposed theory of the stock market boom and crash of 1928–1929 as stock prices would have tracked tariff-based developments. This is then tested by applying a methodology similar to Beaudreau (2014) to a three-factor Fama-French model of stock prices—specifically, of identifying SHTB related good and bad news, and then attempting to measure its impact on nineteen individual DJIA stock prices. We conclude by examining the implications of our findings for the debate over the origins of stock market booms and crashes.",1
42,4,Journal of Economics and Finance,18 November 2017,https://link.springer.com/article/10.1007/s12197-017-9417-7,"Financial turmoil, external finance and UK exports",October 2018,Muhammad Akram,Abdul Rashid,,Male,Male,Unknown,Male,"Availability and easier access to the funds from external resources play an important role in growth and development of a country. The growing literature in trade and finance establishes the fact that access to finance is a key ingredient in firms’ growth. According to the Business Environment and Enterprize Survey (BEEPS 2010), about 66% of the respondent firms in 29 countries of Europe and Central Asia (ECA) and 63% of EU-10 countries respondent firms report that access to external finance is one of the major obstacles in doing business. Several empirical studies have examined the relationship between trade and finance from different dimensions. For example, Beck (2003) finds that countries with better developed financial system have higher export shares and trade balances in the industries that use more external finance. Similarly, Hur et al. (2006), using cross-country industry level data, report that financial development increases export shares and trade balances. On the other hand, some studies have examined the links between access to external finance and cross border economic activities. For instance, Berman and Héricourt (2010) using a large cross-country firm-level dataset of nine developing and emerging economies show that financial constraints create a disconnection between a firm’s productivity and its export status. Manova et al. (2011) using Chinese firms’ data show that credit constraints restrict international trade flows. Specifically, their findings indicate that limited credit availability hinders trade flows of firms. Another dimension extensively explored in the trade and finance literature is how credit constraints affect firms’ decisions to enter into international markets. Minetti and Zhu (2011) find that the probability of exporting in Italian credit-rationed firms is about 39% lower and that credit rationing reduces foreign sales by more than 38%. Similarly, using UK firm-level data, Guariglia and Mateut (2010) study the linkages between firms’ global engagement status and their financial health and show that the global engagement of firms improves their financial health, suggesting that firms’ involvement in cross-border activity shields them from financial constraints. In general, researchers have established the fact that financial constraints reduce international trade (Manova et al. 2015). However, empirical evidence on how firms’ exports respond to financial constraints during the financial crisis is limited. Chor and Manova (2012) explore how international imports to the USA reacted during the financial crisis. They show that countries with higher interbank rates export less to the USA. They also show that this negative relationship has been further intensified during the recent financial crisis when the interbank rate shot up. Furthermore, Coulibaly et al. (2013) state that the sales of export-oriented firms experienced a relatively more severe deterioration than the sales of domestic-oriented firms. Moreover, they showed that exports as a fraction of total sales before the financial crisis fell by 1.4 percentage points more during the crisis. According to WTO (2010), the volume of the global trade contracted by about 12.2% during the 2007–2009 global economic crisis. It was the largest decline in world trade since World War 2. Mora and Powers (2009) consider the decline in trade financing as a major contributor toward the decline in the world trade during the second half of 2008 and early 2009. Summarizing the findings of six surveys of international banks, suppliers, and government agencies, Mora and Powers (2009) point out that trade financing is the second major cause of the decline in global trade. Similarly, Chauffour and Malouche (2011) show that about 40 percent of trade finance was bank intermediated and its premium increased considerably during the 2007–2009 financial crisis. Mora and Powers (2009) document that the price of letter of credit increased by 70 basis points and the price of export credit insurance increased by 100 basis points during the crisis. The trade cost on average increased by about 11% between the second quarter of 2008 to the first quarter of 2009 (Jacks et al. 2009). Manova (2013) proposing a heterogeneous-firm model and using aggregate data for a large sample of countries find that credit constraints have significant impacts on both entry into the foreign market and the level of exporting. However, they show that financial developed countries are more likely to export in the sectors that suffer more from financial constraints. Several recent studies have also documented that credit constraints have a significant impact on exporting. For instance, Manova et al. (2015) show that firms those have foreign affiliations and are joint ventures are likely to export more as compared to private domestic firms in China, particularly in sectors that are more exposed to credit constraints. Paravisini et al. (2015) suggest that credit shortages reduce exports through raising the variable cost of production, rather than the cost of financing sunk entry investments. This rise in the cost of the trade in general and the increase in the cost of trade finances in particular played their role in the collapse of global trade. Similarly, Fan et al. (2015) find that firms facing tighter credit constraints are likely to produce lower-quality products and reduce their optimal prices. Hasan and Sheldon (2016) show that firms those face credit constraints may not be able to enhance the productivity to the level which is required to enter and compete in the international market. They also find that financial constraints do not only have a significant and negative impact on firms’ export activities but also on the investment decisions of firms. Manova and Yu (2016) find that financially constrained firms may not be able to produce higher value-added exports and thus, domestic financial market imperfections negatively influence production process across firms and countries. Altomonte et al. (2016) find that easy access to credit increases the productivity and profitability of exporting firms. Kiendrebeogo and Minea (2016) find that financial constraints have a significant and negative effect on entry into the international market as well as on the level of exports of Egyptian manufacturing firms . Thus, it appears that a reduction in the availability of the trade finance, and the rise in the cost of trade finance resulted in the fall of global trade. Therefore, it would be worthwhile to analyze whether trade finance was indeed a major factor driving the fall in international trade during the recent financial crisis. Keeping in view the gap in the existing literature, in this study, we analyze how UK exports responded to external financing during the financial crisis of 2007-2009. Specifically, we contribute to the existing literature in three aspects. First, the literature estimates the impact of the current global financial crisis on US exports, whereas we focus on UK exports. Second, in the previous literature researchers have analyzed the impact of the financial crisis on exports from the supply side only. Rather, we estimate the impact of the financial crisis on exports considering both the demand and supply sides. Finally, following Chor and Manova (2012), we use monthly data whereas most of the previous studies have used annual data. There are several reasons why we consider the UK is a good case for providing the empirical evidence on the role of external finance in exporting during the financial crisis. The UK is one of the economies that was severely affected by the financial crisis 2007–2009. During the crisis, the UK’s growth rate fell to \(-\) 0.4 percent (Das 2010), unemployment rate increased to 7.8 percent and the lending interest rates shot up to 6.5 percent per annum (Brancaccio and Fontana 2010). The UK’s credit market was dried up and credit provided to the private sector decreased to $3.353 billion. Moreover, during the crisis period 25 percent of UK manufacturers reported lack of credit as a major hurdle in fulfilling their export orders and 15 percent of manufacturers reported lack of credit as a factor likely to constraint their investment in next twelve months (BOE 2011). Further, there is also empirical evidence that firms, particularly small entrepreneurs, face difficulty in occurring external finance during the 2007-09 financial crisis (Cowling et al. 2012). It is also evident that in the run-up of the 2007-09 financial crisis, the economy of the UK was relatively more dependent on credit-fueled consumer spending and on financial services. Finally, there was a significant and rapid growth in the UK financial services sector during the period 2006–2009. By the end of 2009, the contribution of this sector to UK GDP was about 10 percent, the highest as compared to all other G7 economies during the same period. The higher dependence of the UK economy on credit and the dominant role of the financial sector were made the UK economy more exposed to the 2007-09 financial crisis. Indeed, the British economy was hit herder by the financial crisis as compared to all other G7 economies. Due to the financial crash, the share of the financial sector in the UK GDP fell by about 3 percent, although it generally remained unchanged in world’s other major economies The UK economy was suffered from three types of recession. The first one can be termed as a balance sheet recession. Since consumers, banks, and business firms were highly indebted, they cut down their spending and decided to pay off debt. As a result, the saving rate rose from zero percent to 7 percent in a very short period of time. The second major problem which the UK economy faced is the shortage of credit. Although the central bank of England was lowered the base interest rates, banks have been failed to increase lending and investment. This can be referred as an example of a liquidity trap. Third and most important crisis is the productivity crisis. During the credit crunch and in the period leading to the crunch, the UK economy had experienced the sluggish growth of productivity. Less spending on technical innovation and investment, flexible structure of lobar markets, and low growth in wages are the major factors behind stagnant productivity levels. Due to these crises, the adoption of quantitative easing and the base interest rate cuts were less effective in bringing the UK economy out of the crisis and even the UK economy was stopped making progress in 2011 and experienced a double dip recession. Fiscal austerity, housing markets vulnerable, declines in household spending, higher bound yields, and large current account deficits were also considered significant factors in triggering this double dip recession. Given this context, the UK seems a highly relevant and interesting case for studying the influence of external financing on exporting activities during the 2007-09 financial crisis. The results of the study would not only help enhance our knowledge about how credit constraints play a role in exporting but also help in understanding the role of the financial crisis on establishing the effect of credit constraints on exports. The rest of the study is organized as follows. Section 2 reviews the studies that have focused on the effects of financial dependence on exports. Section 3 describes the data we use in our empirical analysis and discusses the construction of the variables. Section 3.2 presents the empirical model which we estimate in order to examine the impact of financial crisis on UK sectoral exports. Section 4 presents and discusses the empirical findings. Finally, Section 5 presents conclusions.",1
42,4,Journal of Economics and Finance,07 November 2017,https://link.springer.com/article/10.1007/s12197-017-9416-8,Do women lag behind men? A matched-sample analysis of the dynamics of gender gaps,October 2018,Joseph Farhat,Naranchimeg Mijid,,Male,Unknown,Unknown,Male,"The latest data from the U.S. Census indicates that female-owned firms, especially those operated by females of color, increased dramatically between 2007 and 2012. However, women are still underrepresented among the U.S. population of small business owners. Furthermore, female business owners tend to run non-employer firms, and their sales/revenues are 20–50 times smaller than the employer firms owned by men. Over the past few decades, the number of studies that have explored female entrepreneurs and gender gaps in business performance have increased rapidly and will continue to grow as the role of women in the economy continues to evolve. Studies have consistently found that female-owned firms are typically smaller than male-owned firms, and are more likely to be organized as proprietorships or partnerships (Coleman and Robb 2012). In addition, women tend to choose highly competitive services and retail industries (Loscocco and Robinson 1991) and are more risk averse than men (Kepler and Shane 2007). These differences stem from the underlying gendered social construct in which women and men exhibit different motivations, preferences, and expectations when running their businesses. However, the question remains as to whether females lag behind men in terms of business performance. Researchers have mixed answers to this question. According to social feminist theories, men and women are expected to act and behave differently. They take different roles within the household, workplace, society, and economy. They are also subject to different perceptions. Through a gendered lens, Loscocco and Bird’s (2012) study investigated the direct and indirect relationship between the gender of a business owner and the success of the business. They argued that females typically start a home-based business as a means of balancing their work-family obligations because businesses of this nature allow them to have full control over the amount of effort and the hours they put into their businesses. Furthermore, home-based businesses allow women to stay closer to home. As a result, this gendered path could provide one explanation for why female-owned firms underperform in comparison to those of males in terms of sales. However, Robb and Watson’s (2012) study found that females do not lag behind men in terms of business performance even after controlling for risk-related factors, key demographic differences, and the firm size. The success of a business largely depends on both human capital and financial capital (Cooper et al. 1994). This is especially the case with family businesses (Fairlie and Robb 2007). Within the retail and services sector, where female-owned firms are heavily concentrated, human capital plays a more significant role in the success of female-owned firms whereas financial capital is more important for male-owned firms (Coleman 2007). Studies have also found that female entrepreneurs not only start their business with smaller start-up capital, but that they also raise lower amounts of financial capital in subsequent years (Coleman and Robb 2009). The purpose of this study is to investigate the relationship between human and financial capital and gender gaps in business outcomes and financing sources using matched sample estimates drawn from the Kauffman Firm Survey (KFS). The matched sample method allowed us to compare the business performance of two identical firms (owned by a woman vs. a man) that had the same human capital and the same preferences (measured by the number of hours worked) within the same industry and that were of a similar set-up (home-based or non-home-based). The data from the KFS also allowed us to explore the impact financial capital had on the business performance of two firms that were identical with the exception of the fact that one was owned by a man and one was owned by a woman. The main research question that underpinned this study was as follows: Are male-owned businesses more successful than female-owned businesses? The analysis assessed and compared male- and female-owned enterprises across three measurements: survival, business performance, and financing. This study makes a contribution to existing research in this domain by building upon previous studies (Robb and Watson 2012; Coleman et al. 2013) that have investigated the survival rate and business performance of male- and female-owned firms. However, this study was the first of its kind to use the matched sample method with panel data to examine the dynamics of female-owned firms using the KFS full sample. Thus, in addition to the methodological contributions the paper makes to the existing body of literature on female entrepreneurs and the success of female-owned firms, it also presents a novel approach to conducting research in this area. Our results revealed that a) female-owned firms have the same survival rate as male-owned firms across the industry (with the exception of the retail sector); b) the consistent gender gap in assets, sales, profits, and employment that can be observed between male- and female-owned firms can be attributed to the fact that female-owned businesses start smaller and stay smaller but grow at the same rate as male-owned firms; c) only half of the asset gaps could be explained by differences in industry and the remaining half were unexplained; and d) although female-owned firms start with a smaller capital and make lower capital injections in subsequent years, there are no significant differences in debt and equity capital injections in relative terms (percentage of total financing).",10
42,4,Journal of Economics and Finance,30 October 2017,https://link.springer.com/article/10.1007/s12197-017-9414-x,Deposit-lending synergies and bank profitability,October 2018,Bruno R. Arthur,Monika K. Rabarison,,Male,Female,Unknown,Mix,,
42,4,Journal of Economics and Finance,22 September 2017,https://link.springer.com/article/10.1007/s12197-017-9409-7,Day of the week effect and stock market volatility in Ghana and Nairobi stock exchanges,October 2018,James Mark Gbeda,James Atta Peprah,,Male,Male,Unknown,Male,"Predictability of stock returns and efficiency of stock markets is a major concern to investment analysts. Efficient Market Hypothesis (EMH) by Fama (1970) assumes that current stock prices reflect all available information and therefore rules out the possibility of investors making abnormal returns by taking advantage of any mispricing of asset. However, in the real world, it is rare to find an efficient market with freely and readily available information, homogenous investor expectations and absence of transaction costs. In both developed and developing financial markets, there exists the tendency for stocks to generate abnormal returns depending on certain day, month and time periods (referred to as calendar anomalies). These anomalies are empirical results that are inconsistent with the EMH. They indicate either market inefficiency or inadequacies in the underlying asset-pricing model. Day-of-the-week effect, which is the tendency of stock returns to display systematic patterns on certain days of the week, is one of the most common anomalies observed in developed and emerging stock markets. There have been many country specific studies on day-of-the week effect in developed markets (Anwar and Mulyadi 2012; Gregoriou et al. 2004; Chen et al. 2001). The justification for studying these two markets (Ghana and Kenya) lies in the following underlying factors and stock market characteristics: Efficiency of the two Markets, Type (Category) of Markets and Trends in the two markets. Thus, both markets have the following characteristics in common. First is the efficiency of the two markets. A number of Studies in the past have established that the Ghana Stock Exchange is weak form inefficient (e.g. Oluoch 2002; Frimpong and Oteng-Abayie 2007; Appiah-Kusi and Menya 2003; Alagidede and Panagiotidis 2006; Ayentimi et al. 2013). Similarly, Kamau (2003), Oluoch (2002), Jeboisho (2014), Kuria and Riro (2013), and Maronga et al. (2015) confirmed that Nairobi Stock exchange is inefficient in the weak form. Given that the two markets are inefficient in the weak form, the study seeks to test for day of the week effect anomaly in these markets. The second justification is based on the Classification of these Markets. According to Financial Times Stock Exchange (FTSE) country classification (2012, 2014, 2013 and 2015) both GSE and NSE are Frontier Markets. Both countries are classified as frontier markets because they have (1) formal stock market regulatory authorities (2) no significant restrictions on repatriation of capital (3) a rare occurrence of failed trades (4) T + 5 or better (clearing and settlement) (5) a timely trade reporting process which are requirement of FTSE’s Quality Markets Matrix (QMM). Although other West African countries (i.e. Cote d’Ivoire and Nigeria) were also classified as frontier markets, the study considered a regional balance (between East and West Africa), hence the choice of Ghana, which represents West Africa and Kenya which also represents East Africa. Furthermore, Ghana and Kenya are Lower middle income country per the 2011 World Bank GNI per capita rating (World Bank 2011). Again, with the same level per capita GNI, is there evidence of day of the week anomalies in their stock markets? In addition to the above, the two markets co-trend. A background check in the two markets (Fig. 1) shows that the GSE and NSE market capitalization as percentage of GDP move in the same direction for the sample period (2005 to 2014). This co-trending nature suggests that the two market are influenced by similar characteristics, such as the efficiency level, types of market regulations and similar economic factors. Trends of market capitalization for NSE and GSE (2005–2014) The above factors and market characteristics (inefficiency of the two markets, both being frontier, lower middle countries and co-trending market capitalisations) may not guarantee the existence or otherwise of day of the week effect anomalies in the markets simultaneously. The study therefore aimed at testing for day of the week effect and stock return volatility in the markets. Similar studies have investigated stock market anomalies and efficiency in Kenya and Ghana separately. For in instance in Kenya, Kamau (2003) and Oluoch (2002) examined calendar effect in Nairobi Stock Exchange (NSE) while Alagidede and Panagiotidis (2006), Asamoah (2010) and Frimpong (2008) focused on Ghana Stock Exchange (GSE). However, no study has been undertaken to compare Day-of-the-week effect and stock returns volatility in these two key capital markets. A further justification for studying these two markets are: (1) Similarity in variance of the two markets. A Goldfeld-Quandt Test proposed by Goldfeld and Quandt (1965) suggests that the variance in GSE and NSE are similar (i.e. constant) hence a bases to compare the two markets. (2) They have the same number of trading days (i.e. 5 days). (3) Both markets started Automated Trading System (ATS) in 2009. (4) Furthermore, each of the stock markets represents a major geographical location, NSE being one of the largest in East Africa likewise GSE in West Africa. The study would bridge the literature gap and provide a basis for further comparative studies across the African stock markets. Finally, previous literature tends to focus on anomaly with respect to various markets. Empirical examination of volatility in stock returns in addition to the test for day of the week effect anomaly in these stock markets is a contribution to the literature. The existence of day-of-the-week effect anomaly makes stock markets inefficient. However, the argument is that although Day-of-the-week effect anomaly was documented in several foreign and African markets in the past, there is no guarantee that they would continue to exist in the future. Day-of-the-week effect and other forms of market anomalies often seem to disappear, reverse, or attenuate after they are published in finance literature. Furthermore, Schwert (2003) argued that data-snooping phenomenon affects the findings of anomaly studies. Thus, the concern is that the process of re-examining data and models affects the likelihood of finding anomalies. To the extent that subsequent researchers reiterate or refine the surprising results by examining the same or at least positively correlated data, there is really no additional evidence in favour of the anomaly hence the inferences drawn from such exercises could be misleading. According to Schwert (2003), one obvious solution to this problem is to test the anomaly on an independent sample. Researchers could use data from other countries or data from prior time periods. After the discovery of an anomaly, if time elapses long enough, the analysis of subsequent data also provides a test of the anomaly. This study aims to investigate existence of day-of-the-week effect anomaly in the Ghana Stock Exchange (GSE) and Nairobi Stock Exchange (NSE) for a more recent period relative to the related past studies. The study looks at an expansive range of data than previous studies. By using an independent and more recent sample, the study makes a comprehensive comparative analysis of the day-of-the-week effects and volatility of stock returns in GSE and NSE. The quality and quantity of data has been improved through the creation of computer database in the two markets. The rest of the paper is structured as follows: the next section reviews related studies; section three discusses the methodology; section four presents the results and discussion and section five concludes.",7
42,4,Journal of Economics and Finance,04 December 2017,https://link.springer.com/article/10.1007/s12197-017-9419-5,"Leasing, legal environments, and growth: evidence from 76 countries",October 2018,Na Zhang,,,,Unknown,Unknown,Mix,,
42,4,Journal of Economics and Finance,27 January 2018,https://link.springer.com/article/10.1007/s12197-018-9426-1,The asymmetric relation between earnings management behaviors: evidence from executive compensation incentives,October 2018,Pei-I Chou,Chia-Hao Lee,,Unknown,Unknown,Unknown,Unknown,,
42,4,Journal of Economics and Finance,17 December 2017,https://link.springer.com/article/10.1007/s12197-017-9421-y,Inflation rate of 14–16% is fair for the sub-Saharan African dollarization,October 2018,Ibrahim D. Raheem,,,Male,Unknown,Unknown,Male,"In International macroeconomics, issues bordering on dollarization have been extensively researched. Essentially, the economic, political and the welfare impact of dollarization have been enormously studied. In the economic perspective, studies have majorly beam searchlight on the determinants of dollarization. To this end, a significant number of theoretical and empirical studies have helped shed light to better understanding issues relating to dollarization.Footnote 1 Towing this background, it has been affirmed that price movements and its effect on the store of value are principal cause of dollarization. Specifically, factors under this category include inflation and exchange rate dynamics (in most cases, successive depreciation and volatility). In the seminal paper by Mundell (1963), demand for money (irrespective of its composition i.e. either domestic or foreign currency) is a function of inflation, exchange rate, interest rate and income. An economy that is characterized by macroeconomic instability such as persistence of inflation and volatile exchange rate would lead to reduction in the value of the wealth holders. In order to forestall this risk, the economic agents would explore the possibility of converting their portfolio into foreign currency denomination. This is due to the implicit assumption that the foreign currency is relatively more stable as compared to the domestic currency. On the empirical front, Arango and Nadiri (1981) were the first to capture the role of foreign market interventions in the model of demand for money. Using a simplified model with underlining data from Canada, United Kingdom, and Germany, they show that inflation and exchange rate expectations are very critical to deciding whether the portfolio of economic agents should be denominated in either local or foreign currency. Mizen and Pentecost (1996) orate that the eventual consequence of high and volatile inflation is the increasing rate of dollarization. Ize and Levy-Yeyati (2003) concluded that the ratio of foreign currency deposit in total monetary base is positively related to inflation volatility and negatively related to exchange rate volatility. Bahmani-Oskooee and Domaç (2003) based on Turkish data found that increasing level of inflation serves as a conduit through which amplifying effect of dollarization is achieved. De Nicoló et al. (2005) using dataset for 100 countries found that high inflation is associated with more dollarization. Luca and Petrova (2008) confirmed that there is a positive relationship between loans domiciled in foreign currency and inflation. Using both aggregate credit and deposit dollarization for 24 transition countries, Basso et al. (2011) confirmed the positive association between the two variants of dollarization (deposit and loan) and inflation. Also, Özbilgin (2012) using DSGE model for Turkey, confirmed the results of earlier studies. Brown et al. (2013) conducted a microanalysis involving 71 regions in Russia. Their results show that high inflation leads to: (i) increase in the dollarization of household deposits; and (ii) decrease in dollarization of household credit. It can be noticed that there is a clear distinct pattern in the approach in which studies cited above have analyzed the linkage between inflation and dollarization. These studies have focused on a unilateral transmission mechanism. They have shown that high inflation would lead to high dollarization. A branch of the literature has refuted this claim and thus goes further to argue that there could be a feedback mechanism in the linkage between inflation and dollarization. In clear terms, it has been suggested that the initial increase in the degree of dollarization, as a consequence of high inflation, will lead to decline in the monetary base. However, though inflation tax, the monetary base would increase later on. Monetary authorities might want to monetize her budget deficit (i.e. compensate for this decline in inflation tax) through increasing administered prices, i.e. higher inflation (Chang 1994; Bahmani-Oskooee and Domaç 2003; Levy-Yeyati 2006). The studies cited above have assumed inflation, at all levels, is bad. This contradicts the logic of elementary economics. Moderate level of inflation is essential for the smooth-running of the economy. This is to say that not all levels of inflationary pressure are bad. All we know, based on the arguments of both theoretical and empirical studies, is that “high” or “excessive” inflation is detrimental to the economy, in general, and dollarization episodes, in particular. Hence, series of questions thus emerge. “What is the optimal level of inflation that is good for the economy and by extension dollarization?” “How high is “high”?” “What is the definition of high or excessive?” These are questions begging for answers! The importance of the threshold effect has been adequately justified in the literature. An overview of the literature suggests two possible strands. In the first strand are set of studies that have argued that economic growth is conditional on not exceeding the threshold level of inflation (Sarel 1996; Bruno and Easterly 1998; Ghosh and Steven 1998; Khan and Senhadji 2001; Phiri 2012, 2013 and Raheem and Oyinlola 2015). These studies further confirmed that low level of inflation has (almost) no effect on economic growth. The inflation threshold argument has also been extended to the financial sector, resulting in similar conclusion to the inflation-growth nexus argument (see Boyd et al. 2001; Naceur and Ghazouani 2007; Rousseau and Yilmazkuday 2005; Lee and Wong 2005 and Bose 2002). Antinolfi et al. (2007) linked the two above set of studies together by examining the transmission mechanism (using dollarization as a conduit) from inflation to financial development and economic activities. Using overlapping generation model, it was established that when the threshold level of inflation is exceeded, economic agents would substitute their dollar portfolio for deposits issued by the domestic financial markets. Thus leading to reduction in the scale of financial intermediation and capital investment. As such, there would be a negative relationship in the trilogy among inflation, financial development, and output growth at higher level of inflation. The objective of this study is to examine if there is a nonlinear relationship between inflation and dollarization. In essence, this study seeks to determine the level of inflation that is conducive for dollarization episodes in the selected sub-Saharan African (SSA) countries for the period 2001 to 2012. Furtherance to the above, the study shall seek to inquire the consequence(s) of exceeding as well as falling short of the obtained threshold level(s). As far as the study is aware, this is the first of its kind in the literature. Thus, this exercise is considered to be novel and pioneer contribution to the existing wide dollarization-based studies. At this junction, it is important to state that inflation is not the only major determinant of dollarization. Studies have confirmed that the determinant of dollarization, inter alia, include the role of governance, monetary authority’s policies/credibility of central banks, exchange rate dynamics and its volatility, inflation volatility, and globalization. We opine that the nonlinear linkage between dollarization and these other determinants are either not feasible or there is no theoretical argument to support the claim. For instance, threshold level of inflation volatility and institution/governance is not feasible because of their persistence over time. The persistence of inflation in Africa has been confirmed by Phiri (2012 and 2016), Balcilar et al. (2015) and Rangasamy (2009). Proxies for institutional quality have shown to be dysfunctional and relatively stable over time, with little or no variation (Andrés et al. 2014; Ajide and Raheem, 2016a, b). Hence, nullify the possibility of having a threshold variable for institutions. As a prelude of our result, it was estimated that institution is a weak determinant of dollarization in SSA. Measuring credibility/ performance of the monetary authorities and or central bank is quite difficult or practically impossible. While acknowledging that maintaining the level of exchange rate is quite important, it is imperative to admit that such motive is beyond the control of the monetary authorities. This is because among the options available to defend the currency is the use of international reserve and there is a limit to which this can be done. On the exhaustion of the reserves, the defence of the peg would have to be abandoned and thus might fuel currency crisis.Footnote 2
 The scope of this study is limited to 25 SSA countries for the period 2001–2012.Footnote 3 Using threshold regression, which has been modified to fit the framework of Tobit regression and system GMM, the following results were found: (i) the optimal level of inflation that is consistent to the degree of dollarization is 14%; (ii) prior to reaching this level, inflation is has a dampening effect on dollarization, however, further increase in inflation beyond this threshold level, would magnify the dollarization level; (iii) other factors that have been found to have strong explanatory power of dollarization are volatility of exchange rate, financial development and trade openness. The rest of this paper is structured as follows: Section two highlights the methodology and data used. The empirical results are presented in the third section. In section four, conclusion, policy implication and suggestions for future research are offered.",1
42,4,Journal of Economics and Finance,06 February 2018,https://link.springer.com/article/10.1007/s12197-018-9428-z,Time-varying correlations between trade balance and stock prices in the United States over the period 1792 to 2013,October 2018,Nikolaos Antonakakis,Rangan Gupta,Aviral K. Tiwari,Male,Unknown,Unknown,Male,"Economic theory dictates that a trade deficit is not necessarily a bad situation because it signals a growing economy, and often corrects itself over time. However, in the long run, a persistently growing trade deficit may lead to fewer jobs being created, which in turn, could lead to even fewer goods being produced in the economy, thus to more imports, resulting in a wider deficit. So it is the persistence in trade deficit over time which is a concern to policy makers. In this regard, the US trade deficit in 2013 stood at 2.2% of GDP ($478 billion), which however, was lower when compared to the post World War II record high of 6.2% of GDP ($762 billion) in 2006. But when we look at the history of trade deficit of the US over 1792 to 2013, highest numbers of 7.3% of GDP were recorded in 1808 and 1816 (see Fig. 1). On average, as indicated in Table 1, the trade deficit has been 0.5% of GDP over the period of 1792 to 2013 – our period of analysis,Footnote 1 with the deficit having a persistent growing trend since 1980, which has economists worried and search for factors that determine the trade balance. In this regard, besides standard trade related factors,Footnote 2 and macroeconomic shocks (productivity, monetary and fiscal shocks), the role of real stock prices on the U.S. trade balance,Footnote 3 as well as internationally, have been analysed recently by a number of studies. Not only empirical approaches, based on constant-parameter Vector Autoregressive (VAR) and Vector Error-Correction (VEC) models (see for example, Fratzscher and Straub 2009, 2010; Kitamura, 2009; Fratzscher et al. 2010; Holinski and Vermeulen 2012; Ncube and Ndou 2013) have been utilized; theoretical general equilibrium models (see for example, Mercereau 2003a, b; Kitamura, 2009; Fratzscher and Straub 2010) have also been developed. Trade balance and real stock market returns. Note: Shaded grey areas denote US recessions as defined by the National Bureau of Economic Research (NBER) and shaded black areas denote world wars There exist primarily two channels through which real stock prices can influence the trade balance, namely the wealth effect channel and to some extent, through the exchange rate channel. Under the wealth effect channel, the general underlying logic is that a rise in stock prices, especially if it is considered to be permanent, increases expected income of households and hence consumption, while also making it easier for firms to finance investment opportunities, thus inducing a decline in a specific country’s trade balance (Fratzscher and Straub 2010). But as pointed out by Simo-Kengne et al. (2015), it is also possible that stakeholders draw on their wealth during stock market booms to increase their financial investment and, hence, reduce their consumption. In other words, the wealth effect can either increase or decrease consumption, thus deteriorating or improving the trade balance. As far as the exchange rate channel is concerned, an increase in real stock prices tends to have a positive impact on short-term interest rates and inflation, and leads to an appreciation of the real effective exchange rate and a sizeable increase in consumption, and thus deterioration of the trade balance (Fratzscher and Straub 2009). However, the trade-balance can also have an impact on stock prices, with an increase in trade balance being inflationary and leading to a response from the monetary authority through higher interest rate, which, in turn is likely to negatively affect stock prices (Hogan et al. 1991; Aggarwal and Schirm 1992, 1998; Mercereau 2003a, b). In sum, both stock prices and trade-balance can affect each other, with the correlation between the variables being either negative or positive depending on which of the effects, discussed above, dominates. Against this backdrop, the objective of our study is to analyse the evolution of the correlation between real stock price and trade-balance for the US economy using Engle (2002) dynamic conditional correlation (DCC)-GARCH model on annual data over the period of 1792–2013.Footnote 4 Besides accounting for time-varying volatility behaviour of the data, a major advantage of the DCC-GARCH approach is its ability to detect changes in the conditional correlation over time. Moreover, it is able to distinguish negative correlations due to episodes in single years, synchronous behavior during stable years and asynchronous behavior in turbulent years. Unlike rolling windows, an alternative way to capture time variability, the proposed measure does not suffer from the so-called “ghost features”, as the effects of a shock are not reflected in n consecutive periods, with n being the window span. In addition, under the proposed measure there is neither a need to set a window span, nor loss of observations, nor subsample estimation required. Note that, an ideal extension of literature would have been to use a sign-restricted time-varying VAR model, which would have allowed us to use time-varying impulse response functions to study the impact of shocks to stock prices and the trade balance. However, without data on consumption, which we do not have for this long-sample, separation of a stock price shock from other macroeconomic shocks would be impossible (Fratzscher and Straub 2009, 2010; Fratzscher et al. 2010). In addition, it is also difficult to provide an interpretation to a stock price shock, with the literature being divided between a “news” shock or rational bubbles (Fratzscher and Straub 2009, 2010; Fratzscher et al. 2010). As discussed above, contingent on the strength of the channels at work, the relationship between real stock price and the trade balance could be either negative or positive. Hence, it is important to pursue a time-varying approach for analyzing the comovement between these variables to check the evolution of this relationship. The DCC-GARCH approach allows us to check if, in fact the relationship is indeed time-varying (state-contingent) or not, besides the nature of the relationship itself. A constant parameter approach, as has been applied so far in the literature, based on an average value of the correlation estimate is likely to be misleading in terms of policy, as it will not allow the policy maker to deduce the importance of the various effects that drive this relationship at specific points in time. So, besides the long sample period which allows us to track the history of U.S. trade balance in relationship to stock prices, our paper is the first attempt to provide a time-varying relationship between the two variables of interest. Our empirical results reveal that correlations between the trade balance and stock market returns are indeed evolving heterogeneously overtime. In particular, the correlations are, in general, significantly positive between 1800 and 1870, and significantly negative thereafter, indicating the time-varying role of the various channels (discussed above) relating the stock market with the trade balance in the U.S. The remainder of the paper is organized as follows: Section 2 describes the empirical methodology, while Section 3 the data used. Section 4 presents the empirical findings. Finally, Section 5 summarises the results, discusses their policy implications and offers some concluding remarks.",6
42,4,Journal of Economics and Finance,08 March 2018,https://link.springer.com/article/10.1007/s12197-018-9433-2,Parameter interchangeability under recursive utility with housing,October 2018,Asiye Aydilek,Harun Aydilek,,Female,Male,Unknown,Mix,,
42,4,Journal of Economics and Finance,27 February 2018,https://link.springer.com/article/10.1007/s12197-018-9431-4,Is the NFL betting market still inefficient?,October 2018,Corey A. Shank,,,,Unknown,Unknown,Mix,,
42,4,Journal of Economics and Finance,16 December 2017,https://link.springer.com/article/10.1007/s12197-017-9424-8,Impact of U.S. federal government budget deficits on the ex ante real interest rate yield on ten-year treasury notes during the post-Bretton Woods (1972–2016) era,October 2018,Richard J. Cebula,,,Male,Unknown,Unknown,Male,"One very visible contemporary public policy issue/concern in the U.S. is the size of federal government budget deficits in recent years and the concomitant growth in the national debt. Part of this concern is based on the potential impact of these budget deficits in elevating interest rates in the U.S. and thereby reducing investment in new plant and equipment on the one hand and in turn reducing economic growth on the other hand. In point of fact, the impact of deficits on interest rates has been investigated in a number of studies especially since the early 1980s (Al-Saji 1993; Barth et al. 1984, 1985, 1986; Cebula 2013, 2014; Cebula et al. 2014; Choi and Holmes 2014; Ewing and Yanochik 1999; Findlay 1990; Gale and Orszag 2003; Gissey 1999; Hoelscher 1986; Johnson 1992; Ostrosky 1990; Saltz 1998; Swamy et al. 1990; Zahid 1988). Many of these studies have indeed found that budget deficits raise longer-term interest rates, such as those on U.S. Treasury bonds or Moody’s Aaa-rated or Baa-rated corporate bonds; furthermore, in a modest number of studies, budget deficits have also been found to raise short-term rates such as Treasury bills. Regardless, since private-sector capital formation is presumably more strongly affected by longer-term than by short-term rates, it has been argued that budget deficits may lead to “crowding out” by elevating longer-term interest rates (Carlson and Spencer 1975; Cebula 1997; Ewing and Yanochik 1999). Interestingly, however, the primary focus of most of these previous studies has been on nominal private sector or nominal federal interest rate yields (cf. Al-Saji 1993; Cebula 2014). Moreover, effectively no emphasis in the literature has been placed on contemporary determinants of the ex ante real interest rate yield on longer-term debt issues. This void in the recent literature is potentially problematic because it is the ex ante real longer-term interest rate yield which arguably acts more directly and profoundly to influence investment in new plant and equipment than do nominal interest rates, be they long-term or short-term. Accordingly, the objective of this exploratory study is to provide insights into the determinants of the ex ante real longer-term interest rate yield, namely, that on ten-year U.S. Treasury notes, and in so doing to seek evidence as to whether or not higher federal budget deficits elevate this interest rate yield. In pursuit of this objective, this study uses annual data for the post-Bretton Woods period from 1972 through 2016 in order to provide contemporary insights into this issue. Section 2 of this study provides the framework/model adopted, whereas Section 3 defines and describes the variables in the empirical model (as well as the full model structure) and describes the data as well. Section 4 provides the empirical results of the autoregressive two-stage least squares estimation of the model. Conclusions are found in Section 5.",3
42,4,Journal of Economics and Finance,17 March 2018,https://link.springer.com/article/10.1007/s12197-018-9437-y,A comment on Paul and Weinbach’s (2005) “Bettor preferences and efficient markets in totals markets”,October 2018,James Francisco,Evan Moore,,Male,Male,Unknown,Male,"Various sports wagering markets have served as fertile ground for testing the Efficient Market Hypothesis (EMH). Paul and Weinbach (2005) did just that, testing whether the totals markets in football wagering yielded any profitable strategies. “Totals” wagering in football refers to bets placed on the combined score of both teams. According to SportsInsights.com, the total indicates “…the total amounts of points that will be scored in the game.” Bettors can wager on the over (under), which is that the teams’ combined score will be greater than (less than) the given total. A winning wager is referred to as a “cover.” Most American bets are offered at odds of −110, which means that the bettor must risk $110 in order to win $100. Because of this, in order to break even, a bettor must win approximately 52.4% of his or bets in order to break even. Paul and Weinbach (2005), using data from 1999 to 2003 and 2000–2004, found that a naïve strategy of betting “under” was profitable for certain totals (i.e. that this strategy would yield a winning percentage significantly greater than 52.4%) in college and arena football. By establishing that such a strategy was profitable, the authors concluded that the totals markets in these sports were not, in fact, efficient. However, they also suggested the possibility that, “a simple explanation of the rejection of efficient markets in totals markets in college football and arena football is that these markets are relatively new and, given time, the inefficiencies will disappear.” (Paul and Weinbach 2005, at 413). In this paper, we find that this statement was prescient, at least with respect to the college football totals market.",3
43,1,Journal of Economics and Finance,29 December 2017,https://link.springer.com/article/10.1007/s12197-017-9423-9,Relation between Credit Default Swap Spreads and Stock Prices: A Non-linear Perspective,January 2019,Miroslav Mateev,Elena Marinova,,Male,Female,Unknown,Mix,,
43,1,Journal of Economics and Finance,09 January 2018,https://link.springer.com/article/10.1007/s12197-017-9425-7,Directors’ liability insurance and investment-cash flow sensitivity,January 2019,Chia-Chung Chan,Yung-Ho Chang,Yuwei Wang,Unknown,Unknown,Unknown,Unknown,,
43,1,Journal of Economics and Finance,06 February 2018,https://link.springer.com/article/10.1007/s12197-018-9427-0,Oil speculation and herding behavior in emerging stock markets,January 2019,Esin Cakan,Rıza Demirer,Hardik A. Marfatia,Female,Male,Unknown,Mix,,
43,1,Journal of Economics and Finance,09 February 2018,https://link.springer.com/article/10.1007/s12197-018-9429-y,Gold and oil prices: stable or unstable long-run relationship,January 2019,Charbel Bassil,Hassan Hamadi,Patrick Mardini,Male,Male,Male,Male,"Oil and gold are two strategic commodities that have recently received much attention partly because they are among the most popular economic indicators and their prices have lately exhibited a high volatility and may affect the prices in the commodity markets. Crude oil is the world’s most commonly traded commodity and perhaps the most important commodity that affects every economy. Its production and consumption are used as indicators of the world economy’s performance. Changes in crude oil prices may cause changes in many macro-economic indicators (inflation, Gross Domestic Product (GDP), unemployment, etc) leading to shifts in monetary policy that have a deep impact on the economy. On the other hand, gold, as a leader in the precious metals, has been used as money through history. While currently it is used for industrial purposes (mainly dentistry and electronics), its demand mostly originates from investment and jewelery. With jewelery demand stable and industrial demand declining, demand has increased because of retail investment especially after 2006 (O’Connor et al. 2015). In fact gold is a basic part of most of the countries’ international reserves portfolio and part of most investors’ portfolio around the world. In addition, investors buy gold to hedge the increasing risk in financial markets. Gold is one of the risk management tools used in hedging and diversifying commodity portfolios. Greenspan (1994) cited gold as a “store of value measure which has shown a fairly consistent lead on inflation expectations and has been over the years a reasonably good indicator”. Moreover, investors in both advanced and emerging markets often switch between oil and gold or combine them to diversify their portfolios (Soytas et al. 2009). The above mentioned characteristics of crude oil and gold show their economic importance not only in portfolio management but also in financial markets and the world economy. The central role played by gold and oil justifies the economic importance of investigating the relationship between them. According to Beahm (2008) the price relationship between oil and gold is one of the five fundamentals that drive the prices of precious metals. Further, special features make the prices of gold and oil not only influenced by the ordinary forces of supply and demand, but also by some other forces. Therefore it is important to know the relationship between crude oil and gold prices and test whether a long-run relationship between them exists. If it does, is it then a stable or an unstable relationship. In the presence of a long-run relationship, it will imply that there is a joint market inefficiency; and that the price of oil can be used to predict gold prices or vice versa. Therefore, the purpose of this paper is to examine the possible presence of a long-run relationship between oil and gold daily prices, and to investigate whether this relation is stable or subject to structural changes. To achieve the goal of this paper, we first conduct standard unit root tests in order to determine individually the order of integration of oil and gold daily prices. Second, we make use of the new structural break tests (Kejriwal and Perron 2010) and cointegration tests (Arai and Kurozumi 2007; Kejriwal 2008) recently developed in time series econometrics. In order to replicate some of the results in the literature we also use Gregory-Hansen (Gregory et al. 1996) cointegration test. However, Arai-Kurozumi test performs better than Gregory-Hansen test when the data generating process does not contain a cointegration vector (Ludwig 2013). Unlike Zhang and Wei (2010) and contrary to Narayan et al. (2010) we allow for more than one break in the cointegration relationship between oil and gold prices. As pointed by Kejriwal and Perron (2010), “most tests may exhibit nonmonotonic power functions if the number of breaks present is greater than the number explicitly accounted for in the construction of the tests”. Hence, the power of one-break tests in finite samples can be poor. On the other hand, testing the presence of multiple structural changes in the cointegrating vector is important for decision makers since it may suggest that the magnitude and the sign of the relationship between oil and gold prices may be different across different sub-periods. Narayan et al. (2010) find that, using Gregory-Hansen test, oil and gold prices are cointegrated but subject to one structural break. However, if we are concerned about cointegration with structural breaks, it seems more appropriate to consider cointegration with breaks in the form of the null hypothesis as proposed by Arai and Kurozumi (2007) rather than in the alternative as proposed by Gregory et al. (1996). This paper is organized as follows. In Section 2 we review the literature. In Section 3 we describe the used methodology. In Section 4 we discuss the results of our estimations. Section 5 stands for the conclusion and policy implications.",5
43,1,Journal of Economics and Finance,26 February 2018,https://link.springer.com/article/10.1007/s12197-018-9430-5,The impact of inflation rate on stock market returns: evidence from Kenya,January 2019,Donald A. Otieno,Rose W. Ngugi,Peter W. Muriu,Male,Female,Male,Mix,,
43,1,Journal of Economics and Finance,01 March 2018,https://link.springer.com/article/10.1007/s12197-018-9432-3,Does the strength of capital market anomalies exhibit seasonal patterns?,January 2019,Benjamin R. Auer,,,Male,Unknown,Unknown,Male,"In recent decades, a wide variety of phenomena in the cross-section of stock returns has challenged the validity of classic asset pricing models and traditional ideas of market efficiency while generating new ideas for active investment strategies. Among these so-called capital market anomalies, size and value effects have the longest research history (see van Dijk 2011; Fama and French 2012), whereas momentum and beta have proven to be the most persistent (see Fama and French 2008; Frazzini and Pedersen 2014). Furthermore, more than 100 additional variables have been identified as effective predictors of future stock returns (see Subrahmanyam 2010; Goyal 2012; Harvey et al. 2016). Motivated by the discovery of such effects, significant research effort has been directed towards analysing the dynamics of the corresponding arbitrage portfolios. So far, several important findings have been made. For example, their returns have been shown to be predictors of economic growth (see Liew and Vassalou 2000), to proxy for variables that describe investment opportunities (see Petkova 2006) and to have strong co-movement across asset classes (see Asness and Frazzini 2013). Furthermore, the anomalous returns appear to be more strongly affected by investor sentiment than by classic measures of limits to arbitrage such as liquidity, transaction costs, volatility or interest rate spreads (see Jacobs 2015). Finally, several studies show that the exploitable returns of capital market anomalies have attenuated in recent years (see Chordia et al. 2014) and exhibit a post-publication phenomenon according to which they are significantly lower after they have become part of the public information set (see McLean and Pontiff 2016). In a recent contribution, Fiore and Saha (2015) analyse another potential property of capital market anomalies. For the US market, they show that the beta anomaly, according to which low-beta stocks tend to have higher returns than their high-beta counterparts (see Baker et al. 2011; Frazzini and Pedersen 2014),Footnote 1 is driven by a kind of summer-winter seasonality.Footnote 2 They highlight that the classic positive risk-return trade-off does hold in the winter months and that switching to a portfolio of low-risk stocks in summer outperforms buy-and-hold strategies and the classic Bouman and Jacobsen (2002) summer-winter strategy of switching to Treasury bills in summer.Footnote 3 In other words, in summer, low-beta stocks outperform high-beta stocks, whereas, in winter, high-beta stocks yield higher returns than low-beta stocks. Because of the additional finding that the winter returns are dominated by the summer returns, this result explains the overall low-risk anomaly documented in previous studiesFootnote 4 and provides an important link between anomalous returns and monthly seasonality. In this note, we extend the study of Fiore and Saha (2015) in several important ways. First, because Jensen (1967) argues that results concerning return predictability are more likely to have general validity when observed in more than one specific market, we use a novel dataset of long-short arbitrage portfolio returns exploiting the beta anomaly in 21 developed markets and test whether the anomaly exists in summer and/or in winter months and whether it is stronger in one of these seasons. Second, some studies of the US market have documented that other anomalies also show signs of monthly seasonality. For example, the size effect appears to be stronger in January than in the rest of the year (see Fama 1991; Larsen 1992; Jacobsen et al. 2005; Lucey and Zhao 2008) and similar observations can be made for value portfolios (see Athanassakos 2010; Das and Rao 2012) and momentum and reversal strategies (see Johnston and Cox 1996; Ji and Giannikos 2010; Yao 2012). Therefore, we additionally test an international set of arbitrage portfolios capturing size, value and momentum effects for summer-winter (and January) seasonality. Finally, we check the robustness of our results against several methodological issues because recent studies have shown that heteroskedasticity, outliers and seasonal definitions can have a crucial influence on the traditional dummy regressions we use to test for seasonality (see Galai et al. 2008; Haggard and Witte 2010; Lean 2011).Footnote 5 Besides revisiting and extending previous results, our study takes an important practical perspective. By using arbitrage portfolios, which have become the basis for many financial products (for example, the investment funds and managed portfolios of www.aqr.com and www.dimensional.com), we can analyse seasonality in returns with direct relevance for investors.Footnote 6 Evidence of seasonality would provide important market timing information and could allow investors to disinvest in phases critical for their overall wealth (see Lee and Rahman 1990; Matallín-Sáez 2006). For example, the results of Fiore and Saha (2015) suggest avoiding US beta arbitrage portfolios in winter because they would realise negative returns in the presence of a positive risk-return relation in winter. Our study allows judging whether such investment advice is benefitial and whether it can also be obtained for other markets and alternative arbitrage portfolios.Footnote 7 The remainder of our note is organised as follows. Section 2 describes our return dataset and our regression models. Section 3 presents the main results and the outcomes of several robustness checks. Section 4 concludes and outlines directions for future research.",2
43,1,Journal of Economics and Finance,02 March 2018,https://link.springer.com/article/10.1007/s12197-018-9435-0,"Structural factors, global shocks and sovereign debt credit ratings",January 2019,Carlos Uribe-Teran,Santiago Mosquera,,Male,Male,Unknown,Male,"Do sovereigns’ solvency-related fundamentals explain sovereign credit ratings’ stickiness within issuing units, across time? We argue that rating criteria among Credit Rating Agencies (CRAs) strongly rely on the evolution of fundamentals that reflect the sovereign’s solvency (structural factors), more than on variables limited to assess the sovereign’s liquidity constraints. Focusing on the long-term foreign currency issuer default rating defined by Fitch Ratings, we exploit a large data set that contains information from 86 countries for the 1995–2011 period, which results in an unbalanced panel with 953 observations. We find that structural features have short- and long-run effects over the observed ratings that are robust to different specifications. Structural factors increase the correctly predicted current rating to 64.4% (from 56.4% obtained in a reduced version of the model), increase the number of correctly predicted downgrades from 40.7% to 52.5%, and provide an anchor to control the wrongly predicted upgrades to investment grade status from a 2.1% probability of a wrong upgrade, to 1.3% when structural factors and global shocks are considered. Our empirical strategy relies on a random effects estimator combined with a parametric approximation for the fixed effects. In order to reduce the bias of our estimations, we introduce global shocks and structural features guided by CRAs’ methodologies. Moreover, we increase precision by allowing the possibility of serial correlation in the error term with clustered standard errors at the country level. In this regard, previous literature avoid global shocks due to the loss of degrees of freedom, generating omitted variable bias, and compute standard errors without allowing for the possibility of serial correlation, a limitation when working with a panel of countries. We also provide a novel strategy to assess the goodness-of-fit of the non-linear model. We exploit the model’s non-linearity by looking at how many notches are correctly predicted within the k-th most likely outcomes. The typical practice in the literature is to count the correct predictions within n notches. The advantage of our approach is that we can pin down the probability of correct predictions within the k-th most likely outcomes generated by the model, allowing us to provide a detailed taxonomy of the model’s goodness-of-fit. We then use the most likely outcome to build a predicted rating scale and obtain a systematic description of the prediction errors, considering not only the direction of the variation but also the number of notches involved in the change. Sovereign ratings are extremely influential in financial markets. Issuing units moving into the investment grade status enjoy yield compression and the sudden enlargement of its investors base. On the other hand, a downgrade into junk status over a short period of time can trigger capital outflows and currency attacks. Because of this, it becomes relevant to understand how different fundamentals affect the ratings of sovereign issuance. In response to this, the existing literature on the determinants of sovereign credit ratings has grown significantly since the seminal study by Cantor and Packer (1996). In this paper the author argues that a small sample of six variables including income per capita, GDP growth, inflation, external debt, history of default, and the level of economic development, were able to predict ratings in a group of 49 countries. Studies that followed included other variables like unemployment rate or investment ratios (Bissoondoyal-Bheenick 2005), or key external variables like foreign reserves, current account balance and terms of trade (Mulder and Monfort 2000). Qualitative variables were also introduced and found to be significant, including indicators of corruption (Mellios and Paget-Blanc 2006) and government effectiveness (Afonso et al. 2011). A compact summary of the literature on the subject can be found in Cantor (2004). But sovereign ratings not only affect the sovereign, but also exert pressure on local markets. The reason for this is that its very rare for rating agencies to grant ratings to corporations that are above the ratings given to the sovereign (the sovereign ceiling effect). In this regard, Borensztein et al. (2013) find evidence that rating agencies’ ceiling policies have a significant effect on the rating of private firms, specially in economies with restricted capital flows and political instability. Moreover, Almeida et al. (2017) show that the effect of sovereign rating downgrades is much higher over firms that have a rating equal or above the rating of the sovereign. This asymmetry affects both investment and debt issuance in bounded firms. Footnote 1 This paper is organised as follows. Section 2 provides some background about sovereign ratings and CRAs’ methodologies. In Section 3 we present the empirical strategy while Section 4 describes the data. Section 5 discusses the results of our study about the determinants of credit ratings. In Section 6 we analyse how structural factors and global shocks affect the model’s predictive power of the evolution of the credit rating. In Section 7 we study how these elements affect the upgrade to the investment grade space. Section 8 concludes.",
43,1,Journal of Economics and Finance,05 March 2018,https://link.springer.com/article/10.1007/s12197-018-9434-1,The flyover effect on IPO returns,January 2019,Nancy Mohan,,,Female,Unknown,Unknown,Female,"Would an investor consider the corporate address when evaluating a purchase of IPO shares? Or, alternatively, does the corporate address affect the demand for shares during the period surrounding the offering date? The possible effect of corporate address on IPO underpricing is the focus of this study. Politically, a voter’s address was critical during 2016. The election results suggest that flyover statesFootnote 1 represent a powerful political opportunity caused by the collective belief that this area has been misunderstood and neglected by “coastal elites”.Footnote 2 This flyover area’s grievances, though, may extend beyond the political. There is also press coverage which indicates that the region is neglected in the financial markets too—or that the corporate address matters..Footnote 3 If the address question is important at the time of the IPO, then it could impact the valuation of a start-up or whether to invest at all. And, a start-up’s corporate address appears to be a factor considered by venture capitalists (VC). A recent article appearing in Wired Magazine suggests that “at least two-thirds of venture capital funding goes to companies in California, New York, and Massachusetts.”Footnote 4 That percentage may underestimate the coastal VC community’s myopic view of funding-worthy companies. Steve Case, the founder of AOL, states that three-quarters of venture capital money flows to Silicon Valley, New York City, and Boston.Footnote 5 His venture capital fund promotes and funds start-up opportunities in the flyover area and has invested more than $850 million. But is funding startups sufficient to clear the financial investment bias which favors coastal firms? VCs require large wins in order to be successful. Thus, early start-ups need sufficient add-on capital to grow large enough for the VC to exit their investment through either a sale of the company or an IPO. Politicians have seized on the funding problem. “If you’re in Silicon Valley, you have the best access to capital in human history,” says McHenry, the North Carolina Republican congressman. “And if you’re born on the wrong side of the tracks, you don’t know deep pockets, and you’re at a severe disadvantage.”Footnote 6 However, there are some exceptions to this overall view. Consider the investment of Tech Coast Angels (TCA) in the Oklahoma firm WeGoLook. The fund invested because WeGoLook was an attractive, undervalued company. And, because the buy-in price was low, a quick exit was possible. The fund’s $200,000 investment provided a 7.8X return when the company was sold for over $42.5 million in 2017—just two years after the TCA’s investment. The managing partner of this fund provides the interesting insight that higher valuations for west coast start-ups require a longer holding period to achieve an acceptable return on exit.Footnote 7 Seven to eight years is the average pre-exit investment period.Footnote 8 “Entrepreneurs and private investors in flyover states don’t have the need—or the huge pressure—to have to hit a home run every time; they are satisfied with a base hit. And that means quicker exits for angel investors”.Footnote 9 The implication is that the current low valuations for flyover startups represent opportunities for venture capital funds. To summarize, there appears to be a venture capital concentration on both coasts and the competition for investing in coastal start-ups may result in undervalued opportunities in the flyover areas. Returning to the opening question, because the company address affects the amount of venture capital flowing to flyover state startups—resulting in lower valuations—does this imply that IPO returns vary by corporate address? The purpose of this research is to consider that question: Is there a flyover effect on IPO underpricing. Specifically this research considers the IPO markets from 2008 through 2016 for potential statistical evidence that flyover IPOs—companies with corporate addresses originating in the midwest, south, and mountain states—experience significantly different first-day returns, after correcting for size and underwriter effects. The size and sign differences for flyover company underpricing is difficult to predict. If flyover IPOs are undervalued (as suggested by the WeGoLook investor) then underpricing could be higher. There could be a bias in that both east coast and west coast underwriters prefer to work with coastal-based firms and undervalue flyover firm initial public offerings. Conversely, flyover IPOs may represent less risk to the underwriters if the flyover firms have a longer operating history. Therefore underpricing may be less. Finally, there could also be an IPO investor taste for coastal firms such that the demand for shares is affected by the corporate address during the first trading day. The net effect of investment banking and investor bias towards corporate address is difficult to predict. The purpose of this research is to document that the corporate address affects IPO underpricing. The period under study dates from the great recession of 2008, and also represents the beginning of the Obama administration and ends with the unexpected 2016 presidential election. It is worthwhile noting that Ritter and Welch (2002) suggest that investigating “non-rational and agency conflict explanations” represents a continued source of IPO research. This topic certainly fits within that subject matter. A second part of this research concerns the long-term performance comparison of these IPO companies one year and three years after going public. Prior research suggests that the long-term performance of IPOs, excluding the first-day return, underperforms the market (Ritter, 1991). However if the issuing firms are matched to non-issuing firms by size and market-to-book ratios then the underperformance is mitigated (Ritter and Welch, 2002). This study adds to the long-term IPO performance literature. More detail on prior IPO research is provided in the literature review, which is the next section. A description of the data collection, variables and hypothesis follows, in section 3. Section 4 presents the regression model while section 5 discusses the long-term return differences between flyover and coastal IPOs. The conclusion provides a summary and suggestions for additional research.",1
43,1,Journal of Economics and Finance,14 March 2018,https://link.springer.com/article/10.1007/s12197-018-9436-z,The impact of the new real estate sector on REITs: an event study,January 2019,Phillip Fuller,Ehab Yamani,Geungu Yu,Male,Unknown,Unknown,Male,"The importance of REITs in the financial markets has been growing over the last ten years as evidenced by the market value of the REITs increasing by approximately 150%. REITs are desirable investments because of their high yields relative to historically low bond yields. By law, REITs must pay shareholders 90% of their taxable income. Consequently, REITs tend to have high dividend yields. Ten years ago, REITs paid $13 billion annually in dividends. Currently, REITs pay $40 billion annually in dividends (Topuz and Isik 2009; Borchersen-Keto 2016). Equity REITs were an important contributor to growth and profitability of the Financials sector. Equity REITs are companies that own residential and commercial real estate and/or other income producing properties, such as health-care and digital storage facilities or timberland. Over the last 25 years, the value of equity REITs increased from $9 billion to more than $1 trillion (NAREIT 2016). Due to the growing importance of REITs, the U.S. Office of Management and Budget reclassified equity REITs in 2007 and transferred them from the Finance and Insurance sector and placed them in the Real Estate and Rental and Leasing sector. Morningstar reclassified equity REITs in 2010 and transferred them from the Financial Service sector to a new Real Estate sector. To reflect the growth in size and importance of equity REITs in the financial markets, the S&P Dow Jones Indices (Dow Jones) and the MSCI on August 31st, 2016Footnote 1 transferred equity REITs from the Financials sector and placed them in a new Global Industry Classification Standard (GICS) sector named Real Estate. The GICS was created in 1999 and had only ten sectors. Now, eleven sectors exist. Accounting for nearly 4% of the market capitalization of the S&P 500, the new Real Estate sector ranks eighth based on market capitalization. The Real Estate sector is larger than Utilities, Materials and Telecommunication Services sectors.Footnote 2 The Financials sector after the change will include: firms involved with banking, mortgage finance, consumer finance, corporate lending, investment banking, brokerage services, assets management, and mortgage REITs.Footnote 3 Dow Jones and MSCI made the change to ensure that the GICS structure continues to represent the global equity market, and to enable investors to make consistent global comparisons by industry (S&P Dow Jones Indices and MSCI (2015). ETFs and mutual funds that have portfolios based on replicating the performance of the Financials sector as defined by the GICS may divest their equity REITs holdings. The divestiture of the equity REITs could cause the yield of the Financials sector, and related investments, to drop since equity REITs tend to have high returns. Selling of the REITs could increase taxes for some shareholders who own Financials sector ETFs and mutual funds in taxable accounts. Saunders (2016) noted that funds that invest in financial service industries but do not track the Financials sector may not be affected by the change. Market professionals have contemplated the impact of the new reclassification on both the Financials and Real Estate sectors. Some market professionals believe that highlighting equity REITs by giving them their own sector will encourage institutional and individual investors to increase their real estate exposure. For example, Katzeff (2016) reports that the creation of the new Real Estate sector will cause the price of REITs to increase. Furthermore, Bohjalian (2016) believes that the new sector will attract new investors and increase the liquidity, thus making REITs more desirable.Footnote 4 Bohjalian also points out that REITs may simultaneously increase the returns on portfolios and reduce their volatility since REITs tend to have a low correlation, 0.55, with the S&P 500. Other market professionals, however, think that the creation of the new sector will not have a significant impact on the real estate industry. For example, Watts (2016) reports that Laszlo Birinyi, a money manager, believes that the new Real Estate sector would needlessly confuse individual investors and that it is designed to benefit vendors but not customers. In addition, Lydon (2016) reports that some active managers will adjust their real estate positions prior to the transition to the Real Estate sector. Some people believe that the impact of the new sector will be minimal since investors can already purchase shares of numerous mutual funds and ETFs that specialize in investing in REITs. Today, approximately 200 mutual funds and 20 ETFs specialize in REITs. Motivated by these diverse opinions among market professionals and the financial press, this paper empirically examines in an event-study context the impact of the creation of the new Real Estate sector on the REITs included in the S&P 500 index. More specifically, the returns and trading volume impact of equity REITs which were transferred from the Financials sector to the Real Estate sector are examined. The event-study methodology is the most commonly used method to study the impact of events (see Brown and Warner 1980; Binder 1998; MacKinlay 1997; Bhattacharya et al. 2000; Savickas 2003; Miyajima and Yahef 2007; Chesney et al. 2011; Charles and Petrasek 2014). The impact of the new Real Estate sector on REITs that were transitioned out of the Financials sector is unknown. The purpose of this study is to shed light on the impact of this transition from the Financials sector to the Real Estate sector on the REITs’, included in the S&P 500, prices and trading volume. This study is one of the first studies to examine the impact of the creation of a new industrial sector on its underlying stocks. The organization of the paper is as follows. Section 2 presents a summary of the previous studies. Section 3 discusses the data sources and sample description. Section 4 presents the empirical methodology. Sections 5 presents the results. Section 6 concludes this study.",
43,1,Journal of Economics and Finance,20 April 2018,https://link.springer.com/article/10.1007/s12197-018-9438-x,Validating empirically identified risk factors,January 2019,Glenn Pettengill,George Chang,,Male,Male,Unknown,Male,"The identification and measurement of non-diversifiable risk factors is a key goal of the finance discipline. Given the importance of this goal, it is surprising that clear guidelines as to what constitutes a systematic or common risk factor are absent. The first identification of a systematic risk factor was associated with a theoretical model, the Sharp-Lintner-Black Capital Asset Pricing Model (CAPM), which was dominant from the early 1970s through the early 1990s. A number of empirical studies found evidence that other variables such as the security’s book-to-market (BtM) ratio and market value of outstanding equity (size) also impacted security returns.Footnote 1 The Fama-French three-factor model (1993) quickly became the dominant method to risk-adjust returns in academic studies and in some instances has replaced the CAPM in application by practitioners. Many researchers have identified new factors based on empirical time-series relationship between factors and portfolio returns. Perhaps, most notably Carhart (1997) found that momentum to be an additonal factor. Other researchers found additional factors and advocated for a five-factor model, including, for example, a variable for liquidity. Indeed, Cochrane (2011) refers to a “zoo” of new factors. Harvey et al. (2016) identify 316 factors that have been tested as possible systematic risk factors and argue that the discipline needs to identify the few relevant factors. But which ones should be selected from the zoo? This dilemma has been placed in bold relief by two recent papers proposing modified factor models. Hou et al. (2015) propose a new multifactor model influenced by the q-theory of investment. Their four factor model includes the market factor, the size proxy factor from the Fama-French three factor model and two new factors representing portfolio returns. Independently, Fama and French (2015), motivated in part by intuition from the dividend discount model propose a new five-factor model. This model includes all three factors from the original three-factor model and two factors similar to those in the Hou, Xue and Zhang model, measuring profitability and investment activity. We argue that this quest for the correct set of factors is quixotic. The sheer number of “validated” factors suggests that the bar for adding factors to the theoretical supported market factor has been too low. In response to this concern, in Section 2 we suggest guidelines for identifying common risk factors. We then apply these guidelines to the factors found in the Fama-French three-factor model. We further argue that the large number of factors that have been advocated suggest that the validation from time-series relationships between suggested factors and portfolio returns must provide an inadequate test. So, in Section 3 we suggest what the time series relationship should be between true factors and portfolio returns. We apply these criteria by carefully examining the relationship between the Fama-French factors and the time series returns of size-value portfolios. Section 4 provides a conclusion in which we argue for a radical departure from current practices in risk-adjustment procedures.",
43,1,Journal of Economics and Finance,22 June 2018,https://link.springer.com/article/10.1007/s12197-018-9442-1,An examination of return and volatility spillovers between mature equity markets,January 2019,Payal Jain,Sanjay Sehgal,,Unknown,Male,Unknown,Male,"Several financial crises in the last two decades, such as the 1994 economic crisis in Mexico, the 1997 Asian Financial Crisis, the 1998 crisis in Russia, and the Argentine economic crisis between 1999 and 2002, originated from emerging market economies (EMEs) and have been extensively documented in literature, see e.g. Yang et al. (2003), Yang et al. (2006), Awokuse et al. (2009), and Kenourgios and Padhi (2012). However, the global financial crisis (GFC) between 2007 and 2009 was a by-product of the faulty financial practices and inefficient regulatory supervision of several large and complex financial institutions of the advanced economies. The International Monetary Fund (IMF) defines an advanced economy as a developed country with a high level of gross domestic product (GDP) per capita and a significant degree of industrialization. Advanced economies, or mature market economies (MMEs) rely on deep institutional underpinnings, that are lacking in developing countries. There was a worldwide slowdown in the aftermath of the GFC, with per capita growth in MMEs declining by nearly 4%, and EMEs slowing to less than 1% (see Derviş (2012)). Weak demand out of Europe, a region accounting for about 40% of the demand for MME exports, further restrained growth in MMEs (see Morel (2015)). Presently, after a prolonged recession, global activity has picked up, due, in part, to strengthening business investment and domestic as well as external demand in most of the advanced economies.Footnote 1 There is a large body of research on MMEs and their stock price movements and spillovers. However, this issue gained more attention from researchers in the wake of the GFC. This paper contributes to existing literature in that it examines the G-7 economies plus Australia, for the time period from 2003 to 2014 – focusing on the linkages prior to, during, and after GFC. The different methodologies employed to examine information transmissions are another contribution of this study. We use cointegration analyses (Johansen cointegration and dynamic cointegration) and GARCH models (ADCC and BEKK) to measure the price discovery and volatility spillovers respectively, across the entire sample period. In addition, the analysis is segmented into pre-crisis, crisis, and post-crisis periods for more insight into the impact of the crisis on information transmission among the sample markets. The objectives of the study are: To analyze the price discovery process among the eight MME stock markets, for identifying dominant markets in the global space; To examine the volatility spillovers among the sample markets for effective risk management and asset allocation. The study finds that, in price discovery process, while equity markets of Italy and United Kingdom lead other MMEs in pre-crisis and crisis period, respectively, there is no clear dominance of any market in post-crisis period. The EU markets in the sample, as well as Australia and Japan are highly correlated, suggesting an important role of geographical proximity on the interdependence among financial markets. The Japanese market is linked via volatility with all sample markets across sub-periods. Spillover effects among all markets are significant during crisis period, confirming contagionFootnote 2 effect on stock market volatility. The study is organized as follows. Section 2 gives a brief review of the literature on information linkages. Details about data are provided in Section 3. Sections 4 and 5 discuss the methodologies employed and empirical results, respectively. Section 6 concludes with the summary and policy implications.",10
43,2,Journal of Economics and Finance,30 April 2018,https://link.springer.com/article/10.1007/s12197-018-9440-3,Cross-market information spillover and the performance of technical trading in the foreign exchange market,April 2019,Yung-Ho Chang,,,Unknown,Unknown,Unknown,Unknown,,
43,2,Journal of Economics and Finance,04 May 2018,https://link.springer.com/article/10.1007/s12197-018-9439-9,People have the power: post IPO effects of intellectual capital disclosure,April 2019,Cristiana Cardi,Camilla Mazzoli,Sabrina Severini,Female,Female,Female,Female,"The role that non-financial information plays on the performance of listing companies has been interesting practitioners and academics for years. In the last couple decades it has been largely hypothesized that, despite accounting information is still the most important source of knowledge of a company, it is insufficient for investors and analysts, especially when they are seeking to value new firms (Lev and Zambon 2003; Mavrinac and Siesfeld 1998; Nielsen et al. 2015). So far, companies are increasingly understanding the importance of disclosing corporate information in relation to strategy, value creation and intellectual capital (IC). This is important because they are competing for attention in a global information environment, addressing a multitude of different stakeholders that potentially will take interest in the message being conveyed. The effects of voluntary disclosure in Initial Public Offering (IPO) prospectuses have been examined noticeably in recent years but studies on the topic mainly concentrated on the short-term effects that such a disclosure plays on the behavior of investors (Cardi and Mazzoli 2015; Cardi et al. 2016; Dimovski and Brooks 2004; Hanley and Hoberg 2008, 2010; Singh and Van der Zahn 2007). Despite some heterogeneity in the results, the most recent empirical evidences seem to steadily suggest a positive effect that the disclosure of non-financial information plays on the IPO short-term results (Cardi and Mazzoli 2015; Cardi et al. 2016; Hanley and Hoberg 2008; Singh and Van der Zahn 2007). On the contrary, notwithstanding the general theme regarding the long run underperformance of IPOs is well-known in the international literature (Allen et al. 1999; Alvarez Otero and Gonzales 2001; Giudici and Paleari 2001; Kim et al. 1995; Leleux 1993; Loughran and Ritter 2000; Loughran et al. 1994; Stehle et al. 2000), the long-term effects of the disclosure of non-financial information during the IPO has received less attention and the few empirical results are often inconsistent. Some authors maintain a negative effect that the disclosure of non-financial information in the IPO prospectus exercise on the long run performance of stocks. In particular, (Heniro et al. 2007) suggest that when more information about IC resources is disclosed, investors’ expectations increase thus driving the price up at the time of listing (‘fads’ and ‘over-optimism’). However, as time goes by, investors disadvantage these firms by discounting their long-run prices, if the performance diverges from the pre-issue expectation. Other authors find opposite results, thus supporting the idea that a larger disclosure of non-financial information is linked to a good long run performance of stocks (Bessler and Bittelmeyer 2008; Guo et al. 2004; Nielsen et al. 2015). Companies who disclose substantial amounts of information on intellectual capital will be associated with better transparency and therefore also better long-term performance. For example, (Guo et al. 2004) find that Research and Development (R&D)-intensity is positively related to both long-term performance and underpricing (UP). As suggested by (Bessler and Bittelmeyer 2008) innovation, patents, and intellectual capital are important factors that have a positive impact on the valuation and on the long-run financial performance of especially young technology firms. The results of (Nielsen et al. 2015) demonstrate that intellectual capital information, as revealed by a 78 variables disclosure index, leads to significantly better long-term performance against a reference portfolio. A feasible explanation for such inconsistencies can be found in the different non-financial variables and IC classifications that are employed across different studies; on the one hand, some authors make use of single non-financial variables such as patents (Bessler and Bittelmeyer 2008; Guo et al. 2004) or the number of products under development together with the development stages of each product and information on alliance agreements (Guo et al. 2004). On the other hand, other authors (Heniro et al. 2007; Nielsen et al. 2015) make use of more comprehensive IC disclosure indexes calculated on a wide number of IC variables, as previously done in many recent disclosure studies of IPO prospectuses.Footnote 1 In particular, (Nielsen et al. 2015) make use of an IC disclosure index consisting of 78 items divided into 6 different categories (employee, customer, information technology, processes, research and development, strategic statements) while (Heniro et al. 2007) build their IC disclosure index on 81 variables grouped into similar IC categories. In this paper, we try to deepen the knowledge regarding the effects of IC disclosure on the IPO long run performance by providing new empirical evidences that overcome the limits deriving from the arbitrary selection or classification of the IC information. In particular, we start from a broad set of 87 IC variables that are provided in the listing prospectuses of a sample of Italian listing firms and we apply a principal component analysis (PCA) to such non-financial variables; this way, we introduce for the first time an unbiased categorizations of IC variables which relieves us from the issue concerning the best classification to be employed or the best variables to be considered. The IC components revealed by the PCA are then put in a series of regressions to study their effects on the behavior of secondary market investors in the 12 and 36 months following the IPO. Moreover, we here include a wide set of variables to control for the short-term performance, the quality of the firm and other concurrent factors which could befog the effect of IC disclosure on the long run performance. Our results show that the non-financial information regarding the human resources and the information technology plays a positive influence on the behavior of secondary market investors in the 12 months that follow the IPO and such results are confirmed on the 36 months with reference to the human resources component. The remainder of this paper is as follows: in section 2, we review the literature on the relationship between IC disclosure and IPO long-term results; in section 3 we describe the research design in terms of data and methodology and we illustrate our main hypotheses, while a discussion of the key findings is presented in section 4. Section 5 concludes.",2
43,2,Journal of Economics and Finance,19 May 2018,https://link.springer.com/article/10.1007/s12197-018-9441-2,Bank lending – what has changed post crisis?,April 2019,Jang Ping Thia,,,,Unknown,Unknown,Mix,,
43,2,Journal of Economics and Finance,04 July 2018,https://link.springer.com/article/10.1007/s12197-018-9443-0,Which immigrant and minority homeownership rates are gaining ground in the US?,April 2019,Durba Chakrabarty,Michael J. Osei,Danyang Zhao,Unknown,Male,Unknown,Male,"Homeownership is often viewed as a major component of the American Dream (Gabriel and Painter 2008; McConnell and Marcelli 2007; Shlay 2006). Roughly two-thirds of all households in the United States own their home, but there is substantial variation in homeownership rates across race, ethnicity, and foreign-born status (Constant et al. 2009; Coulson 1999; Deng et al. 2003; Gabriel and Rosenthal 2005; Hilber and Liu 2008; Painter et al. 2001). Furthermore, immigrant and minority homeownership rate gaps relative to white non-Hispanic natives have varied over time (Borjas 2002; Bostic and Surette 2001; Coulson and Dalton 2010; Haan 2007; Turner and Smith 2009). In particular, Borjas (2002) documents that during the 1980–2000 period, immigrant households had lower homeownership rates than natives and the gap widened significantly during the period because natives homeownership rates increased while immigrant homeownership rates decreased. The US housing boom during the early and mid-2000s increased homeownership rates overall, but the housing bust caused homeownership rates to fall, at least initially. However, the effects of the boom and bust likely varied across groups (Faber and Ellen 2016; Mundra and Oyelere 2017; Painter and Yu 2014). Furthermore, many observers expect homeownership rates to recover at some point, but it is unclear how much recovery has occurred thus far. The current study examines changes in homeownership rates between years 2000 and 2015, focusing on differences by immigrant status, race, and ethnicity. We seek to better understand how homeownership rates changed over time for these groups and where they stand as of 2015. We provide valuable insights relative to previous literature by using newer data and considering a wider set of groups in a single study. We are ultimately interested in which immigrant and minority groups experienced rising relative homeownership rates and which experienced declining homeownership rates. We first document raw trends. Homeownership rates for most groups examined rose during the housing boom of the early and mid-2000s but fell during and after the housing bust. We also use multivariate regression to estimate conditional changes in group-specific relative homeownership rates that control for a number of individual characteristics and locational fixed effects. We find substantial heterogeneity in homeownership outcomes with several immigrant and minority groups gaining ground in homeownership rates relative to white non-Hispanic natives. Some other groups lost ground. While rising relative homeownership rates indicate increased economic achievement for some groups, falling homeownership rates suggest reduced economic well-being for other groups. The choice of whether to own one’s home involves both a consumption decision and an investment decision (Brueckner 1997; Henderson and Ioannides 1983). The consumption motive is important when rental markets are imperfect or thin so that some specific housing options matching household preferences are not available for rent. The investment motive results because homeownership yields housing benefits in both the present and the future. Housing prices also vary over time, often in unpredictable ways, making homeownership a risky investment (Bayer et al. 2016; Gabriel and Rosenthal 2015). Existing homeowners gain wealth when prices rise, but they lose wealth when prices fall. The large financial cost of housing means that many households need mortgage financing to buy a home, which can make them especially vulnerable to price drops that leave them with negative equity and unable to sell their home if needed. Home loans are typically based on income and credit history, and some immigrant and minority groups may face greater challenges securing financing (Courchane et al. 2015; Gabriel and Rosenthal 1991; Wheeler and Olson 2015). The housing boom of the early and mid-2000s was characterized by relaxed home financing requirements including the proliferation of sub-prime loans and increased optimism about expected appreciation (Bayer et al. 2016; Gabriel and Rosenthal 2015). This contributed to increased homeownership rates for most demographic groups. However, widespread defaults on sub-prime loans and corresponding devaluation of mortgage-backed securities triggered the global financial crisis and Great Recession of 2007–2009 (Mishkin 2011). The struggling financial industry tightened credit requirements and households became more averse to homeownership risk during the aftermath, resulting in lower homeownership rates (Acolin et al. 2016; Gabriel and Rosenthal 2015). Figure 1 illustrates the post-2000 trends in raw homeownership rates for native and immigrant groups separately by race/ethnicity computed by the authors using microdata from the 2000 decennial census and the American Community Survey (ACS) for 2001–2015; more details are provided later in the Data section. Homeownership rates in Fig. 1 for whites and most minority groups rose after 2000 and peaked during 2006 or 2007 before falling during and after the housing bust. By 2015 homeownership rates for six of the nine groups in Fig. 1 had fallen below their year 2000 levels. Interestingly, the only group in Fig. 1 with a statistically significantly higher homeownership rate in 2015 than in 2000 is Asian immigrants. Asian natives and Hispanic immigrants exhibit small changes that are not statistically significant at the 10 % level. Thus, homeownership trends after 2000 exhibited considerable differences across immigrant status, race, and ethnicity. Trends in raw homeownership rates for native and immigrant groups The current study investigates these differences in homeownership rates and changes since 2000. We provide a closer look at differences within Asian and Hispanic groups and uncover substantial heterogeneity there as well. We examine the extent to which gaps depend on individual characteristics and location. These factors do matter, but large differences remain. We also consider heterogeneous homeownership gaps across locations, focusing on differences between traditional immigrant gateways, emerging gateways and non-gateways. Gateway status does matter for relative homeownership rates for several groups. The heterogeneity in homeownership rates across groups that we investigate has important implications for researchers and policy makers. In particular, Asian and Hispanic groups are among the fastest growing segments of the US population and their housing outcomes have increasingly important effects on housing market conditions for the US as a whole and for gateway metropolitan areas in particular. There is considerable diversity among both native and immigrant Asian and Hispanic groups and diversity in their homeownership rates and trends both unconditionally and conditional on numerous regression controls. While previous literature has explored homeownership rates for some immigrant and minority groups using earlier data, we contribute to the literature by using more recent data and presenting a more complete picture of the heterogeneity across and within Asian and Hispanic immigrant and native groups. The remainder of the paper is organized as follows. The next section presents a conceptual background and briefly discusses related literature on homeownership differences by immigrant status, race, and ethnicity. The third section discusses the data and empirical approach we use. The fourth section presents our main empirical results, and the final section concludes.",8
43,2,Journal of Economics and Finance,06 July 2018,https://link.springer.com/article/10.1007/s12197-018-9444-z,Price jumps in developed stock markets: the role of monetary policy committee meetings,April 2019,Rangan Gupta,Chi Keng Marco Lau,Hardik A. Marfatia,Unknown,,Unknown,Mix,,
43,2,Journal of Economics and Finance,18 July 2018,https://link.springer.com/article/10.1007/s12197-018-9445-y,Return and liquidity response to fraud and sec investigations,April 2019,Brandon C. L. Morris,Jared F. Egginton,Kathleen P. Fuller,Male,Male,Female,Mix,,
43,2,Journal of Economics and Finance,20 July 2018,https://link.springer.com/article/10.1007/s12197-018-9449-7,Further evidence on the validity of purchasing power parity in selected African countries,April 2019,E. N. Gyamfi,E. F. Appiah,,Unknown,Unknown,Unknown,Unknown,,
43,2,Journal of Economics and Finance,15 December 2018,https://link.springer.com/article/10.1007/s12197-018-9465-7,Loan relation with foreign banks and information asymmetry: evidence from earnings management by local firms in Korea,April 2019,Sangwook Lee,Sang Hoo Bae,Inshik Seol,Unknown,,Unknown,Mix,,
43,2,Journal of Economics and Finance,24 July 2018,https://link.springer.com/article/10.1007/s12197-018-9448-8,American art as an investment: new evidence from an alternative approach,April 2019,Sarah J. Skinner,John D. Jackson,,Female,Male,Unknown,Mix,,
43,2,Journal of Economics and Finance,24 November 2018,https://link.springer.com/article/10.1007/s12197-018-9463-9,Do open-market stock repurchases convey firm-specific or industry-wide information? Evidence from REITs,April 2019,Gow-Cheng Huang,Kartono Liano,Ming-Shiun Pan,Unknown,Unknown,Unknown,Unknown,,
43,2,Journal of Economics and Finance,09 August 2018,https://link.springer.com/article/10.1007/s12197-018-9446-x,Iranian inflation: peristence and structural breaks,April 2019,Luis A. Gil-Alana,Yadollah Dadgar,Rouhollah Nazari,Male,Male,Unknown,Male,"Inflation is a very important determinant of economic growth, and it is one of the most investigated macroeconomic variables from both theoretical and empirical perspectives. A clear understanding of the mechanism that explain the behavior of inflation is thus crucial for the economy, since many theories rely their legitimacy in the statistical properties of inflation. Nevertheless, there is no consensus, about if this variable should be stationary or not, or if this property varies according to the country under study. The degree of inflation persistence is a relevant parameter of interest for different reasons. First, it plays a key role in the design of monetary policy, since it will determine the degree to which monetary policy authorities can maintain a stable level of output and inflation simultaneously, and thus, the performance of monetary policy (Rudebusch 2002; Levin and Williams 2003; Amano 2007). That is, measuring the degree of persistence of inflation is a key issue because of its implications for the empirical validity of macro theories such as the long-run neutrality of money. Persistence of inflation can worsen the efforts of monetary officials for declining inflation rate itself. Moreover, in case of persistence of inflation the reaction of economy to different economic policies takes place with delay. Iranian economy has experienced high inflation rates during the past 50 years. This episode can potentially lead to inflationary expectation, structural inflation and persistence of inflation altogether. In this article we focus on the statistical properties of the inflation rate in Iran, investigating, for instance, if this variable is mean reverting or not, i.e., if the effect of the shocks is transitory or permanent. Additionally, we examine the hypothesis of linearity versus non-linearity in its behavior along with other statistical issues such as the possibility of long memory behavior and the potential presence of structural breaks. For this purpose, we will employ techniques which are based on the concept of fractional integration; using this approach, we allow for a higher degree of flexibility in the dynamic specification of the series than in the standard methods that are exclusively based on integer degrees of differentiation. In other words, allowing the degree of differentiation to be real, we permit fractional values and thus the standard cases of stationarity I(0) and nonstationarity I(1) appear as particular cases of interest when the differencing parameter is 0 or 1 respectively. However, we can consider additional cases, including stationary long memory models (0 < d < 0.5); nonstationary though mean reverting (0.5 ≤ d < 1), and nonstationary explosive behavior (d > 1). The paper is structured as follows: Section 2 briefly describes the evolution of the inflation rate in Iran during the last twenty-five years. Section 3 reviews some previous works on this variable using fractional integration. Section 4 presents the data and the main empirical results, while Section 5 concludes the paper.",1
43,2,Journal of Economics and Finance,20 November 2018,https://link.springer.com/article/10.1007/s12197-018-9462-x,The impact of international bribery on U.S. household stock investments,April 2019,Emre Kuvvet,,,Male,Unknown,Unknown,Male,"The Foreign Corrupt Practices Act (FCPA) of 1977 forbids U.S. firms from paying bribes to foreign public servants in order to attain business or affect regulations or taxes in their favor. However, the FCPA has not deterred many U.S. companies of paying bribes to foreign government officials (Weismann et al. 2004) since those companies can receive significant financial benefits through bribery.Footnote 1 For instance, Cheung et al. (2012) find that “firm market value increases by 11 dollars, on average, for each dollar of bribe they pay (p. 5)”. The motivation of this paper is to examine whether international bribery violations could be one of the causes of limited household stock market participation. Previous studies suggest that household nonparticipation might be attributable to shareholders’ concerns about the expropriation of wealth as the result of fraudulent activities and, thus, subsequent loss of trust in the financial markets (Giannetti and Wang 2016). In this paper, we examine whether households may simply refuse to invest in companies that engage in illegal activities, even when those illegal actions such as bribery violations can increase their wealth significantly. We show that households seem to divest their holdings in the stock market if they perceive the market to be “morally corrupt,” a finding that is consistent with Kim et al.’s (2015) “conscious local investor view.” These authors find that local institutional investors play an important role in mitigating environmental violations. Although environmental violations do not reduce the shareholders’ wealth, Kim et al. (2015) show that local institutional investors help reduce firms’ toxic release. Our findings are also consistent with “the delegated philanthropy view in which some stakeholders (investors, customers, and employees) are often willing to sacrifice money (yield, purchasing power, and wage, respectively) so as to further social goals (Benabou and Tirole 2010, pg. 10).” Our results, which show that shareholders avoid investing in companies that engage in socially irresponsible behavior are supported as well by the findings of Hong and Kacperczyk (2009). They find that “sin” stocks (alcohol, casinos, tobacco) have higher returns, as socially norm-constrained investors abstain from investing in those companies. In this study, we show that households punish the stock market by “walking with their feet” if local firms commit bribery violations. After controlling for state and household factors, we find that there is a negative effect of bribery violations in the state on the stock investments of local households: bribery violations in the state reduce households’ participation in the stock market, diminish the proportion of total household stock holdings within the household’s total wealth, decrease the probability that households will enter the stock market, and increase the probability that they will exit the market. We find that morality could be a potential mechanism by which bribery violations affect household stock investments. We focus our attention on the effect on household stock investments of bribery misconduct on the part of local firms. This is because households are more likely to be informed about misconduct perpetuated by companies headquartered in the same state as theirs. The evidence from the literature suggests that investors are more informed about the activities of local companies and they also tend to invest most heavily in those firms (Grinblatt and Keloharju 2001; Ivkovic and Weisbenner 2005; Hong et al. 2008; Massa and Simonov 2006; Pirinsky and Wang 2006; Brown et al. 2008; Feng and Seasholes 2004; Coval and Moskowitz 1999; Osili and Paulson 2008; Coval and Moskowitz 2001; Huberman 2001; Loughran and Schultz 2005). Local news also tends to focus on local firms,Footnote 2 and households tend to interact socially with the employees of local companies (Gurun and Butler 2012; Engelberg and Parsons 2011; Davis and Henderson 2008; Feng and Seasholes 2004; Loughran and Schultz 2005; Coval and Moskowitz 2001). Our paper makes an important contribution to the bribery literature. Many opponents of FCPA argue that anti-bribery laws should be repealed or relaxed because the cost of implementing those laws is significant.Footnote 3 However, this study shows a negative externality effect of bribery violations: reduced household stock participation. The wealth of a household can be affected adversely and significantly by lack of participation in the stock market (Campbell 2006). This paper highlights the importance of increased enforcement of bribery violations, despite the fact that bribery can increase shareholders’ wealth. The remainder of the paper is organized as follows. Second section reviews the literature on households’ participation in the stock market. Third section explains the data and research methodologies. Fourth section presents the results, and Fifth section offers a concluding remarks.",1
43,2,Journal of Economics and Finance,16 November 2014,https://link.springer.com/article/10.1007/s12197-014-9307-1,RETRACTED ARTICLE: The role of circuit breakers in the oil futures market,April 2019,Nicholas Apergis,,,Male,Unknown,Unknown,Male,(PDF 747 kb),1
43,3,Journal of Economics and Finance,16 July 2018,https://link.springer.com/article/10.1007/s12197-018-9447-9,Are common stocks a hedge against inflation in emerging markets?,July 2019,Nassar S. Al-Nassar,Razzaque H. Bhatti,,Unknown,Unknown,Unknown,Unknown,,
43,3,Journal of Economics and Finance,10 August 2018,https://link.springer.com/article/10.1007/s12197-018-9453-y,"The growth of government, trust in government, and evidence on their coevolution",July 2019,Steven Gordon,John Garen,J. R. Clark,Male,Male,Unknown,Male,"The growth in government in the U.S. and other parts of the world has been examined extensively by the literature. Much of this research aims to understand this expansion, but scholars in economics, political science, and legal studies are often divided regarding the role of government, and its growth, in economic life. Two contrasting models, the “public interest” and “political economy” views of government, present simple characterizations of this divide and have different implications for social well-being and economic production as a result of increases in government activity (Mueller 2004). Another trend in the U.S. over the past 60 years that has gained attention is the decline in the public’s trust of government (Pew Research Center for the People and the Press 2010). This decline has caused great unease among many commentators, with the concern that trust is an important aspect of social capital and its weakening may detract from the efficacy of government, as well as in the ease of economic and social interactions.Footnote 1 This paper presents an empirical examination that relates these two phenomena, i.e., the mutual evolution of the growth of government on one hand, and the decline in trust of government on the other. Additionally, our findings speak to whether the public interest or political economy model is a more compelling viewpoint to understand the co-evolution of these trends. As discussed below, the political economy model implies a long-run outcome where growth in government erodes trust in government, thus growth in the former is associated with a decline in the latter. The public interest model suggests a positive long-run relationship of trust and government growth. Though the raw correlations are indicative of the political economy model, our empirical work looks for robust, long run associations in the data on the size of government, of trust in government, and other variables. Because we use time-series data that may be non-stationary, caution must be taken in the interpretation of correlations and we are careful to utilize the appropriate time series methods. While our analysis is based on the microeconomics of rent-seeking as well as on the literature on trust, reciprocity, and social capital, we do not examine the details of any particular economic or political market nor on the ebb-and-flow of historical events. Indeed, our approach abstracts from this level of detail and focuses on economy-wide trends of and associations among four variables: the size of government, trust in government, lobbying/rent-seeking activity, and labor productivity. We test if these variables maintain long-run associations to one another, i.e., if they are cointegrated, and whether the findings are consistent with either of the above viewpoints. Thus, the paper presents a novel approach that is relevant to the political economy literature, to studies of the growth of government, and to work on trust and social capital. Moreover, it provides new empirical evidence with which to interpret major economic and social trends. The remainder of the paper is organized as follows. Section 2 presents an overview of the literature of the growth of government, with discussion of its central themes which tie closely to the political economy and public interest models. Section 3 presents the theoretical discussion and empirical implications of the political economy model as well as the implications of the public interest view. The role of trust in government is discussed and incorporated here. Section 4 provides an overview of the data. Section 5 describes the econometric analysis and the findings. We specify an empirical model that allows us to examine the long run association of and the dynamics of the relationship between government size and trust in government. Using the vector error correction model (VECM), we test for and characterize the long run (cointegrating) relationship between trust, government size, and two other variables that we hypothesize play a role in the dynamic adjustment process: lobbying/rent-seeking and labor productivity. When measuring government activity with federal transfer spending or with pages in the Code of Federal Regulations, our estimates imply the existence of a long run association between trust and government size where higher levels of government lead to lower trust, greater lobbying/rent-seeking, and lower productivity. This is consistent with the political economy model. Use of other measures of government size do not support the political economy model, though there is no support for the public interest framework. Lastly, Section 6 concludes.",3
43,3,Journal of Economics and Finance,11 August 2018,https://link.springer.com/article/10.1007/s12197-018-9451-0,Keep calm and consume? Subjective uncertainty and precautionary savings,July 2019,Barbara Broadway,John P. Haisken-DeNew,,Female,Male,Unknown,Mix,,
43,3,Journal of Economics and Finance,30 August 2018,https://link.springer.com/article/10.1007/s12197-018-9452-z,Which sentiments do US investors follow when trading ADRs?,July 2019,Yaseen S. Alhaj-Yaseen,Dana Ladd,,Male,Female,Unknown,Mix,,
43,3,Journal of Economics and Finance,30 November 2018,https://link.springer.com/article/10.1007/s12197-018-9461-y,Ownership characteristics of Asian American banks,July 2019,Russ Kashian,Yuhan Xue,Rashiqa Kamal,Male,Unknown,Unknown,Male,"The Federal Deposit Insurance Corporation (FDIC)’s Policy Statement regarding the Minority Depository Institutions (MDIs) states that “Minority depository institutions often promote the economic viability of minority and under-served communities” (2002). The FDIC has historically taken steps to preserve and encourage minority ownership of insured financial institutions.”Footnote 1In August 1989, Congress enacted the Financial Institutions Reform, Recovery, and Enforcement Act of 1989 (“FIRREA”). According to Section 308 of FIRREA, “Minority” is defined as “Black American, Asian American, Hispanic American, or Native American”. Section 308 of FIRREA defines “minority depository institution” as any Federally insured depository institution where 51% or more of the voting stock is owned by one or more “socially and economically disadvantaged individuals.” Given the ambiguous nature of the phrase “socially and economically disadvantaged individuals,” for the purposes of FDIC’s Policy Statement, minority depository institutions are defined as any Federally insured depository institution where 51% or more of the voting stock is owned by minority individuals or where the board of directors is minority/majority and the bank serves a minority community (FDIC 2002). Whether MDIs enhance the economic viability of the communities that they serve has been a subject of research for a long time. Researchers like Brimmer (1971) and Kwast and Black (1983) among others show that the financial constraints of minority-owned banks limit their ability to encourage economic growth. However, a recent MDI research study by FDIC (2014) shows that MDIs tend to fulfill the objectives stated in the Policy Statement and provide services to high-poverty and minority communities. The study also points out that MDIs typically tend to be concentrated in geographic areas that reflect the communities they seek to serve. This is supported by Kashian and Drago (2016) who argue that minority-owned banks tend to operate in “population-matched” communities. This results in cultural affinity and altruism in the operations of the bank, which might have positive effects in terms of information gathering but negative effects in terms of costly attempts to protect clients against adverse circumstances (pp. 4). Black et al. (2003) contend that minority-owned banks may add value to the minority communities through cultural familiarity, which increases the likelihood that minority applicants will solicit minority-owned banks for mortgage credit. Using zip code clusters, their results show that overall lending and borrowing preferences exist between minority-owned banks and minority communities. As documented below, relatively little is known about the Asian American Owned Banks (AAOBs), much less the various sub-types within this group of banks, including Chinese American, Korean American and Indian American. This paper seeks to fill this gap in the literature by studying the characteristics of sub-types of AAOBs. Using data on all AAOBs in the United States, this paper compares these banks to mainstream banks in terms of profitability, risk in terms of non-performing loans, capital-asset ratios, commercial real estate (CRE) lending, and location in Asian American majority communities. The data is split into pre- and post-2008 financial crisis periods to analyze the changes in the performance of AAOBs over the two epochs. Overall, the results indicate that AAOBs tend to perform better than mainstream banks in the US and this can be attributed to their customers’ cultural affinity, and the AAOBs’ monopoly power. Even though the sample size of AAOBs is small, the results are based on the entire population, not just a sample. The findings of this paper are important because as is shown in the paper, the ethnic minorities that the AAOBs cater to are gradually playing a larger economic role in the US demography. These results also have implications for policy formulation and regulation of MDIs. The Federal Deposit Insurance Corporation’s Report to Congress for 2016 states that “The FDIC’s MDI Program is fully integrated into the supervision, consumer protection and receivership business lines. The FDIC retains a dedicated executive as National Director of Minority and Community Development Banking. In addition to the National Director at headquarters, the FDIC has designated regional MDI coordinators in each of its six regional offices.”Footnote 2 This demonstrates that the FDIC is dedicated to and is serious about the development of the MDIs in the US. Moreover, every two years, the federal banking regulatory agencies (FDIC, OCC, and Federal Reserve) host an interagency conference for FDIC-insured MDIs and Community Development Financial Institution (CDFI) banks to help preserve and promote their mission. The conference encourages interactive discussion among those who believe minority depository institutions and CDFI banks are uniquely positioned to create positive change in their communities. The conference also encourages a diverse array of participants to express their views.Footnote 3 We believe that the results of this, and similar types of research can make contributions to such discussions and help fulfill the mission of these regulatory authorities. The paper is organized as follows: Section 2 presents a brief overview of the existing literature, Section 3 develops and presents the hypotheses, Section 4 describes the data and methods, Section 5 presents the results and discussion, and Section 6 concludes.",
43,3,Journal of Economics and Finance,29 August 2018,https://link.springer.com/article/10.1007/s12197-018-9455-9,Stochastic dividend discount model: covariance of random stock prices,July 2019,Arianna Agosto,Alessandra Mainini,Enrico Moretto,Female,Female,Male,Mix,,
43,3,Journal of Economics and Finance,06 October 2018,https://link.springer.com/article/10.1007/s12197-018-9458-6,An evaluation of chapter 11 bankruptcy filings in a competing risks framework,July 2019,Sanjiv Jaggia,Satish Thosar,,Male,,Unknown,Mix,,
43,3,Journal of Economics and Finance,13 August 2018,https://link.springer.com/article/10.1007/s12197-018-9454-x,Cigarette smuggling: using the shadow economy or creating its own?,July 2019,Rajeev K. Goel,James W. Saunoris,,Male,Male,Unknown,Male,"The smuggling of cigarettes across jurisdictions has intrigued policymakers and researchers for quite some time (see ACIR (1985)). However, the problem persists and therefore the search for effective ways to combat smuggling is not over. The empirical literature has primarily focused on price or tax differences across jurisdictions, both within a country and across national borders, to capture the incentives for smuggling.Footnote 1 Lower prices in adjoining states induce both organized smugglers and consumers from a state to buy cigarettes there (and vice versa).Footnote 2 Cigarette smuggling has contributed to an ever widening of budgetary gaps through billions of dollars in lost tax revenues, forcing state governments to seek ways to overcome this problem.Footnote 3 The smuggling of tobacco products in general, and cigarettes in particular, creates dual problems for policymakers. On the one hand, there are lost tax revenues from smuggled cigarettes (e.g. cross-border shopping or organized smuggling); on the other hand, monitoring/controlling smoking (for health effects) is problematic as cigarettes are marketed in underground markets - where they might frequently be sold to underage individuals who cannot legally buy cigarettes. Moreover, cigarettes sold underground where regulation is lacking are conceivably unhealthier—e.g. they might lack filters or warning labels. Thus, policy makers’ attempt to curb smoking by raising taxes may have secondary consequences such as increased smuggling, with little offsetting effect on tax revenues (see Baltagi and Levin (1992)). For example, in 2009 excise taxes on cigarettes more than doubled, and although tax revenues initially increased, they subsequently continued to decline as a result of people quitting smoking or resorting to purchases in the underground.Footnote 4 This paper adds to this line of inquiry by considering an additional dimension that might significantly impact the incentives to smuggle cigarettes (see Chaloupka and Warner (2000), Gallet and List (2003), Goel (2004a) and Goel and Nelson (2008) for reviews of the literature; Apergis et al. (2014), Baltagi and Levin (1986), Bishop (2018), Sung et al. (1994) and Thursby and Thursby (2000) for some studies focusing on U.S. cigarette demand). Specifically, we consider the influence of the shadow or underground sector in a state on cigarette smuggling in that state and on smuggling in its neighboring states. Tax and regulation differentials across U.S. states promote spillovers of the shadow or underground market through cigarette smuggling (see Goel and Saunoris (2016)). As discussed by Cebula et al. (2014), tax hikes that reduce aggregate cigarette consumption do not account for the possible substitution from low nicotine to high nicotine cigarettes. Using similar reasoning, smokers may substitute away from the high cost legal cigarettes and instead purchase cigarettes underground. Incorporation of the shadow economy in a state uniquely enables us to capture intra-state smuggling (which would not be captured via inclusion of border cigarette prices, as much of the literature has done (see, for example, Baltagi and Levin (1986)). While cigarette smuggling forms a part of the underground sector, the overall shadow economy is much broader, including unlicensed taxis, unauthorized electricians, cash or barter transactions that are not recorded, etc. In fact, a state with a significant shadow sector might conceivably have insignificant levels of cigarette smuggling. Yet, the presence of an underground sector can influence/facilitate cigarette smuggling in a number of ways (Joossens and Raw (2012)). One, the presence of a significant shadow sector might signal to potential cigarette smugglers (both casual and organized smugglers) about the ease of operating underground (i.e., avoid paying taxes or breaking the law in general). Indeed, businesses (retail and wholesalers) are required to obtain a permit or license to sell tobacco, thus prompting firms underground to bypass these requirements. Second, some forms of shadow operators, e.g., unlicensed taxis, might make cigarette smuggling easier (as the opportunity costs of breaking the law are already low for someone who has broken the law in a different context). The transportation via shadow operators might lower transportation costs, making smuggling to more distant states economically feasible - this possibility would not be accounted for if solely border prices are considered. Third, a widespread shadow sector, within a state or among its neighbors, presents arbitrage opportunities in smuggled products - i.e., not necessarily including cigarettes. For instance, the presence of a substantial shadow sector in a state might enable the smuggling of cigarettes to a third state in exchange for another type of contraband. Finally, an income effect results from income earned underground that can be used to spend on cigarettes. In fact, according to Schneider and Enste (2000) as much as two-thirds of income earned underground is spent in the official sector. Thus, we see that the consideration of the shadow economy enables us to consider influences on cigarette smuggling that go beyond merely price considerations that have been the primary focus of the extant empirical literature. Viewed from a different angle, we are able to compare price-competition and inter-state externalities in a spatial context.Footnote 5 Is the presence of the shadow economy a substitute or complement to other (e.g., sometimes legal casual cross-border purchases by tourists) incentives for cigarette smuggling? The results, based upon the data across U.S. states for the years 1997–2008, show that border price effects are positive and statistically significant, and the average shadow economy in bordering states facilitates smuggling, with own shadow economy sometimes showing signs of facilitating intra- and cross-border smuggling. The other findings regarding the negative own-price elasticities and the presence of habit-forming effects are in line with the larger literature. Whereas this paper focuses on smuggling across U.S. states (similar to Thursby and Thursby (2000)), cigarette smuggling is also a significant problem in other cases, notably in Europe (see Calderoni (2014), Goel and Nelson (2008), Joossens and Raw (1995), Yürekli and Sayginsoy (2010)). According to Joossens et al. (2010), the size of the illicit trade in cigarettes varies across countries from 1% to about 40–50% of the overall market, with low income countries having greater illicit trade relative to high income countries. This trade leads to substantial losses in tax revenues and hundreds of thousands of premature deaths (due to smoking of lower priced smuggled cigarettes).Footnote 6 The structure of rest of the paper includes a discussion of the model and the data employed in the next section, followed by estimation and results, with conclusions forming the final section.",2
43,3,Journal of Economics and Finance,09 October 2018,https://link.springer.com/article/10.1007/s12197-018-9459-5,Schooling and income inequality in the long-run,July 2019,Tarkan Cavusoglu,Oguzhan Dincer,,Male,Unknown,Unknown,Male,"Income inequality, particularly the relationship between schooling and income inequality is among one of the more popular topics in development economics. There are several different theoretical arguments on how schooling affects income inequality. Schultz (1963), for example, argues that as long as human capital, which is generally measured as the average level of schooling grows faster than the physical capital, income inequality decreases. According to Chiswick (1968), on the other hand, the relationship between the two is positive. Chiswick’s (1968) argument nevertheless requires the level of schooling to be independent of the rate of return to schooling and empirical evidence does not support such independence (Winegarden 1979). Knight and Sabot (1983) argue that schooling has two different effects on income inequality: composition effect and compression effect. The composition effect increases the relative size of the educated people and tends to raise income inequality first, but eventually to lower it. Compression effect, on the other hand, lowers income inequality because as the relative supply of educated people increases, the rate of return to schooling decreases. Hence, the effects of schooling on income inequality depend on the strength of these two effects. The empirical evidence regarding the relationship between schooling and income inequality is conflicting as well. While some cross-country studies such as Winegarden (1979), Bourguignon and Morrison (1990), Park (1996), and De Gregorio and Lee (2002) find a negative relationship between the average level of schooling and income inequality, Ram (1984, 1989) does not find a statistically significant relationship between the two. Bergh and Nilsson (2010) find a positive relationship. Among the few within-country studies, Chiswick (1971) uses data from U.S. states and finds that an increase in the level of schooling causes income inequality to increase. Braun (1988), on the other hand, finds the opposite using, again, state level data.Footnote 1 A second group of cross-country studies which investigate the relationship between public spending on education and income inequality also find conflicting results. While Sylwester (2002a) and Keller (2010) find that public spending on education lowers income inequality, Bergh and Fink (2008) find that the relationship is not robust to different empirical specifications.Footnote 2 The conflicting results of the earlier studies mentioned above are partly due the differences in inequality and schooling measures used, and partly due to the samples and sample periods used. Their results are particularly sensitive to differences in specification as well, since all of them use Ordinary Least Squares (OLS) to estimate the relationship between schooling and income inequality. Moreover, none of these earlier studies correct for the possible endogeneity between schooling and income inequality. In this study, we use annual data from 48 contiguous U.S states over half a century between 1946 and 2000 to estimate the long-run cointegrating relationship between the average level of schooling and income inequality. Using state level data is quite advantageous for a variety of reasons. First, it minimizes the problems associated with data comparability often encountered in cross-country studies. Second, there are significant unobservable institutional differences across countries. U.S. states are much more similar than different countries regarding institutions and in other dimensions that are difficult to measure. Third, free flow of human capital across states means that human capital flows to the states that have the highest returns. We are likely to have different results if mobility of human capital is restricted. They are a group of democracies in which the confounding effects of different political regimes are absent. Finally, it allows us to work with a panel long enough to exploit the time series properties of the data. Following Pedroni (2000, 2001) and Pesaran (2006), we use Fully Modified OLS (FMOLS), Dynamic OLS (DOLS), and Common Correlated Effects (CCE) estimators to investigate the relationship between schooling and income inequality. Both FMOLS and DOLS correct for endogeneity and parameter heterogeneity, and CCE corrects for cross sectional dependence and all three estimators are superior not only to fixed effects (FE) estimator but also to Arellano and Bond (1991) and Blundell and Bond (1998) difference and system generalized method of moments (GMM) estimators for several reasons.Footnote 3 First, both estimators are designed for short and wide panels. Second, as argued by Roodman (2009) both estimators suffer from instrument proliferation, particularly in panels with long time dimensions like ours. Since the instruments are built from past values of the instrumented variables, difference and system GMM estimators generate too many instruments causing the coefficients of the endogeneous variables to be overestimated. Third, both estimators introduce dynamics to the regression equation by adding a lagged dependent variable. This causes the interpretation of the estimated coefficients to change significantly (Greene 2012). Without the lagged dependent variable, the “independent variables” represent the full set of information that explains the dependent variable. With the lagged dependent variable, we now have in the equation the entire history of the right-hand-side variables, so that any measured influence is conditioned on this history. In this case, any effect of a right-hand-side variable represents the effect of new information (Greene 2012, 496). Finally, both difference and system GMM estimators first-difference the data to eliminate the fixed effects. If, on the other hand, the regression equation is estimated on first differences rather than on levels, it loses its long-run interpretation. Using several different measures of income inequality, we find that an increase in the average level of schooling decreases income inequality in the long-run and (Granger) causality runs from schooling to inequality, not the other way around. The study is organized as follows. Section 2 describes the empirical methodology. Description of the data and the results are given in Section 3. Section 4 concludes.",1
43,3,Journal of Economics and Finance,29 July 2018,https://link.springer.com/article/10.1007/s12197-018-9450-1,The effects of role models on college graduation rates,July 2019,James V. Koch,Ziniya Zahedi,,Male,Unknown,Unknown,Male,"Many individuals believe that the presence of faculty role models not only influences the academic majors college students choose, but also stimulates their academic performance (Bayer and Rouse 2016). Decades ago, Verdugo (1995) offered a succinct version of the role model hypothesis through the lens of Hispanic students: “By role models is meant having Hispanics on campus who are in positions of status and power within and outside the institution. The belief is that if Hispanic students are able to see Hispanics in these kinds of positions, it will not only motivate them to remain in school and achieve academically, but it will also provide them with a group that is a natural sounding board for the many problems facing Hispanic students.” Ehrenberg (1995) organized a symposium that focused substantially on role models and concluded that the evidence in favor of these propositions was mixed. Boulware (2011) arrived at a similar conclusion: “the small body of studies on this topic contains methodological and conceptual inconsistencies and have not produced any consistent results or conclusions.” We offer fresh evidence on the influence of role models in higher education. However, in contrast to previous studies that have concentrated on the choices of individual students in specific academic majors, we adopt an alternative approach and focus upon institutional graduation rates. We address both racial/ethnic characteristics and gender. Our focus on institutions rather than individual students allows us to sidestep some of the methodological problems that have plagued previous work, most prominent among them the self-selection biases that have corroded the ability of researchers to discern role model effects. To wit, most students have some capacity to choose the specific courses they will take and the professors who teach those courses. Further, they can drop courses. This has clouded results reported in most prior studies. By no means do we provide the last word on the empirical validity of role model hypotheses. Nevertheless, our work pushes the ball down the field because we focus on the six-year graduation rates of specific racial groups (and women) and consider the possibility influences of faculty, student and citizen role models on those graduation rates. Our empirical work is based upon a diverse sample of 176 four-year public institutions of higher education. We examine the six-year graduation rate of undergraduate students at each institution and seek to determine if the presence or absence of role models at each has an impact upon its graduation rates. The results are thought-provoking. Where BlackFootnote 1 students are concerned, a 1.0% increase in the percentage of Black faculty on the typical campus evokes a .59% increase in that institution’s graduation rate of Black students. However, we find no evidence to support a role model/support effect for the relative presence of Black students on a campus, or for the Black citizenry within that state. We present comparable evidence for Asian, Hispanic, White and female students. In general, we uncover only mixed evidence of the existence of role model effects.",3
43,3,Journal of Economics and Finance,17 October 2018,https://link.springer.com/article/10.1007/s12197-018-9456-8,Information about the poor and support for redistributive policies,July 2019,Ngoc Phan,Sondra Collins,,,Female,Unknown,Mix,,
43,4,Journal of Economics and Finance,31 October 2018,https://link.springer.com/article/10.1007/s12197-018-9457-7,A residential mortgage bank lending channel during the financial crisis,October 2019,Salman Tahsin,Timothy J. Yeager,,Male,Male,Unknown,Male,"Between 2007 and 2009, home prices in the median U.S. county declined 10% while construction employment declined 20%. Much of the decline can be attributed to the bursting of the housing bubble fueled by aggressive subprime lending. The weakness in the jumbo market also played a significant role as jumbo interest rates surged with the collapse of the private label securitization markets. It is also possible that distressed banks rationed credit to creditworthy borrowers. Indeed, newspapers published numerous articles during the financial crisis about how the tight credit conditions in the jumbo market were inhibiting creditworthy households from obtaining loans, further depressing the housing market.Footnote 1 We empirically estimate the importance of the residential mortgage bank lending channel effect by examining a cross-section of county-level housing markets between 2007 and 2009. The transmission mechanism that we consider is purposefully narrow. Distressed banks deny loans to creditworthy households, which reduces housing demand and leads to lower home prices, residential investment, and construction employment. Our econometric challenge is to control for the endogeneity and spurious correlation between measures of bank distress and housing market performance while capturing the transmission mechanism of the bank lending channel. For example, a direct OLS regression of the change in county home prices on the change in bank distress provides no mechanism for bank distress to affect mortgage application denial rates. The regression is also biased if the deterioration of the county economy adversely affects bank health. In addition, the regression potentially captures spurious correlation between housing markets and bank health simply because both measures are declining sharply during the financial crisis. We use a two-stage least squares approach to model the residential mortgage bank lending channel. The first stage regresses a county’s overall change in home purchase loan acceptance rates between 2006 and 2008 on the change in the bank distress measure over the same period to identify the reduction in acceptance rates driven by bank distress. In the second stage, we regress changes in county housing measures on the predicted change in acceptance rates from the first stage. We minimize the endogeneity and spurious correlation concerns by using an instrumental variable for bank distress first explored by Loutskina and Strahan (2009), which is the relative change in the acceptance rates of jumbo loans (AJ) to nonjumbo loans (ANJ). Specifically, we compute the county-aggregated bank distress measure as Δ(AJ/ANJ) = (AJ/ANJ)2008 - (AJ/ANJ)2006.Footnote 2 Negative values indicate that banks accepted fewer jumbo loans relative to nonjumbo loans in 2008 compared with 2006. Because jumbo loans exceed the conforming loan size threshold of the mortgage Government Sponsored Enterprises (GSEs), banks that extend jumbo loans run the risk of expending precious liquidity by holding the loans in portfolio, especially when securitization markets are in turmoil. Most nonjumbo loans, in contrast, are conforming loans eligible for purchase by the mortgage GSEs. The key insight is that the jumbo acceptance rates should decline more than nonjumbo acceptance rates at distressed banks relative to healthy banks because distressed banks have strong incentives to contract credit to conserve liquidity and boost capital ratios. On the other hand, a deteriorating housing market in a county does not endogenously affect the instrumental variable nor introduce spurious correlation if declining housing conditions within a county reduce acceptance rates for jumbo and nonjumbo loans equally. The pre-crisis intensity of jumbo and subprime lending, however, are two powerful housing conditions that could affect the relative acceptance rates of jumbo and nonjumbo loans differently, and we control for these factors explicitly in our analysis. Our primary contribution to the literature is to estimate the importance of the residential mortgage bank lending channel during the financial crisis. We find that the channel is statistically significant but economically small. Counties with a one standard deviation decrease in the jumbo to nonjumbo acceptance ratio between 2006 and 2008 experience an additional 89 basis point decline in home prices, and a 1.25% decline in construction employment growth, which represent 7% and 6%, respectively, of their mean changes between 2007 and 2009. We run several robustness tests on our main specification. To ensure that the regression results are not driven by outlier counties, we conduct propensity score matching. We match counties with otherwise similar profiles in the 2006 pre-crisis year, where banks in one county but not the other subsequently experience a sharp reduction in the jumbo to nonjumbo acceptance ratio. The results are consistent with the two-stage least squares results. We also run threshold tests excluding applications well above and below the jumbo cutoffs. Again, results are similar. Finally, we run a placebo test using a post-crisis sample period when bank health was improving and the jumbo securitization market was functioning more normally. As expected, we do not find statistically significant results. A second contribution to the literature is to demonstrate the bias that results from alternative measures of distress. We run several tests using measures of bank distress computed at the bank level rather than the county level, which allows for spillover of distress across counties. Although this approach potentially reduces endogeneity between bank distress and county housing markets because many banks operate across several counties, it increases spurious correlation between the instrumental variable and housing market performance. Using a bank’s failure probability and expected default frequency (EDF), respectively, as measures of bank distress, we find a much stronger bank lending channel, accounting for about 20% of the decline in home prices between 2007 and 2009. We also compute the change in the jumbo to nonjumbo ratio at the bank level rather than the county level. Results from these bank-level distress measures consistently show that the bank lending channel explains a much larger share of the decline in county housing markets. The paper proceeds as follows. Section II states the conditions necessary for a residential mortgage bank lending channel. Section III introduces the methodology and the county-level instrumental variable. Section IV describes the data and summary statistics, and Section V presents the main two-stage least squares results. Section VI discusses a variety of robustness tests on the main results, and Section VII presents results using bank-level measures of bank distress. Section VIII concludes.",2
43,4,Journal of Economics and Finance,28 December 2018,https://link.springer.com/article/10.1007/s12197-018-9464-8,Long-term price overreactions: are markets inefficient?,October 2019,Guglielmo Maria Caporale,Luis Gil-Alana,Alex Plastun,Male,Male,Male,Male,"The Efficient Market Hypothesis (EMH) is one of the central tenets of financial economics (Fama 1965). However, the empirical literature has provided extensive evidence of various “anomalies”, such as fat tails, volatility clustering, long memory etc. that are inconsistent with the EMH paradigm and suggests that it is possible to make abnormal profits using appropriate trading strategies (Plastun 2017). A well-known anomaly is the so-called overreaction hypothesis, namely the idea that agents make investment decisions giving disproportionate weight to more recent information (see De Bondt and Thaler 1985). Clements et al. (2009) report that the overreaction anomaly has not only persisted but in fact increased over the last twenty years. Its existence has been documented in several studies for different markets and frequencies such as monthly, weekly or daily data (see, e.g., Bremer and Sweeney 1991; Clare and Thomas 1995; Larson and Madura 2003; Mynhardt and Plastun 2013; Caporale et al. 2018). There exist a significant number of studies on market overreactions but most of them analyse short-term price overreactions based on daily data (Atkins and Dyl 1990; Bremer and Sweeney 1991; Cox and Peterson 1994; Choi and Jayaraman 2009) and focus only on a single market/asset. By contrast, this paper analyses long-term overreactions and a variety of markets and frequencies by (i) carrying out various statistical tests to establish whether overreaction anomalies exist using both weekly and monthly data, and (ii) using a trading robot method to examine whether they give rise to exploitable profit opportunities, i.e. whether price overreactions are simply a statistical phenomena or can also be seen as evidence against the EMH. The analysis is carried out for various financial markets: the US stock market (the Dow Jones Index and 10 companies included in this index), FOREX (10 currency pairs) and commodity markets (gold and oil). A similar investigation was carried out by Caporale et al. (2018); however, their analysis focused on short-term (i.e., daily) overreactions, whilst the present study considers a longer horizon, namely a week or a month. The remainder of the paper is organised as follows. Section 2 reviews the existing literature on the overreaction hypothesis. Section 3 describes the methodology used in this study. Section 4 discusses the empirical results. Section 5 provides some concluding remarks.",12
43,4,Journal of Economics and Finance,03 January 2019,https://link.springer.com/article/10.1007/s12197-018-9467-5,Volatility relation between credit default swap and stock market: new empirical tests,October 2019,Miroslav Mateev,,,Male,Unknown,Unknown,Male,"Credit Default Swap (CDS) emerged in the late 1990s-to-early 2000s as an insurance instrument transferring risk from commercial banks’ balance sheets to third parties - mainly insurance companies and investors. Basically, there are two parties involved – the protection buyer and the protection seller, where the first one “shorts” the credit and pays a premium to the second one. The two parties involved in the transaction bet on credit events such as bankruptcy, default, and debt restructuring. The protection buyer pays a premium as a percentage of the notional value each quarter (denoted as an annualized spread in basis points), and receives payoff from the seller in case a credit-related event occurs. In addition to the technical features of the CDS contract, it is worth mentioning the following characteristics of CDS: 1) CDS is perceived as a purer credit risk measure than corporate bonds. According to Ötker-Robe and Podpiera (2010), the CDS spread is a more direct measure of credit risk than corporate bonds’ credit spreads. The credit spread is derived by subtracting a risk-free interest rate from the corporate bond’s yield. While selecting the risk-free rate and the term structure of the risk-free rate adds a significant degree of discretion to corporate bond spreads, CDS is a spread based on arbitrage-free pricing. 2) There are different arguments of whether CDS reflects liquidity risk premium or not. Longstaff et al. (2005) find that CDS is far less sensitive to liquidity compared to corporate bonds, whereas Fabozzi et al. (2007) find that CDS does not contain any liquidity premium.Footnote 1 3) CDS is based on standardized terms, unlike the corporate bond universe where spreads reflect differences in indenture characteristics - seniority, embedded options, guarantees, and others, and 4) CDS dominates corporate bonds in responding more quickly to changes in credit conditions (Zhu 2004; Blanco et al. 2004). The CDS market has witnessed a significant growth in the last decade. According to the International Swaps and Derivatives Association survey (ISDA 2009), the CDS notional amount outstanding totalled USD 30.4 trillion at year-end 2009, up from USD 2.2 trillion as of the end of 2002. The CDS market was one of the derivative market segments that suffered the most during the global financial crisis of 2008 - the CDS notional amount outstanding for the whole of 2008 was down 38% from $62.2 trillion at year-end 2007 (ISDA 2008). This amount was reduced in the post-crisis period to $36 trillion in 2011. A vast empirical literature investigates the relation between the volatility of CDS and stock prices. Volatility has a key role in the analysis and forecasting of risk. There are two basic concepts of volatility – conditional and unconditional. Under the conditional concept, volatility is a model-based volatility and updates regularly when new information is coming to the market, while the unconditional volatility is not changing over time. The long-term volatility is constant, while the model-based volatility changes. Financial time series (such as stock returns) are characterized by volatility clustering and time-varying volatility that tends to be serially correlated and cannot be modelled under the assumption of homoscedasticity (Schreiber et al. 2012). The serial correlation can be modelled via the Autoregressive Conditional Heteroscedasticity (ARCH) model, developed by Engle (1982), and its generalisation process (GARCH), developed by Bollerslev (1986). Usually, the serial correlation effects are captured by modelling conditional variance as a function of the squares of the previous returns and past variances. Hence, these processes can be used to model the nonlinearity effects in financial data. While univariate GARCH models are appropriate tools to address the problems of risk analysis and asset allocation, hedging requires that these models are extended to multivariate ones that analyse variances and covariances of the financial time series (Wang and Yao 2005; Minović 2010). This is due to the fact that all models related to risk analysis and risk management use covariances as inputs (Minović 2010). From a multivariate perspective, there are two well-known specifications, namely, the dynamic and the constant correlation GARCH (that is, DCC-GARCH and CCC-GARCH). The main advantage of multivariate GARCH models is that they are able to capture the important features of univariate GARCH models and, at the same time, to extend the analysis to more than just one asset. The previous analysis of the relation between CDS and stock prices is mainly focused on investigating the cointegration (that is, long-run relation) and the Granger causality (that is, short-term relation) between these two time series (at changes or returns level). While the changes/returns relation between CDS and stock prices reveals interesting patterns (that is, changes in one of the variables - CDS or stock prices, can cause changes in the second variable, or alternatively, one can define the long-term relation that can be used as a proxy for price discovery), the volatility modelling captures the impact of external shocks and/or external volatility on stock markets caused by changes in the CDS market, and vice versa – the impact of stock market changes on CDS. Hence, volatility modelling extends the framework of analysis of just changes or returns. Despite the relevance of this analysis, the existing evidence on the volatility relation between CDS and stock markets is still limited. There are several important contributions of this study. First, we analyse the time-varying characteristics of CDS in conjunction with stock price volatility of the European investment-grade companies. We bring new evidence that both correlations and covariances between CDS and stock prices exhibit a time-varying pattern. Second, we estimate the volatility spillover between CDS and stock markets, and find that volatility spillover is bi-directional, meaning that stock price changes impact on CDS volatility and vice versa – CDS changes influence stock price volatility. While prior research investigates the spillover effect between the two markets at price level (that is, cointegration analysis), this study estimates the co-movement between CDS volatility and stock price volatility. Third, while the majority of previous papers analyze volatility at broad-index levels (for example, the iTraxx level), we perform our analysis at the individual company level. Using a sample of 109 European investment-grade companies, during the period of January 2012-to-January 2016, we find strong evidence that the volatility spillover is bi-directional with the predominant leadership of the European CDS market over the stock market. This paper is organised as follows. Section 2 presents literature review and hypotheses derived from the analysis of the previous empirical findings, Section 3 introduces the methodology, and Section 4 presents data set and sample statistics. Section 5 illustrates the empirical results, and Section 6 concludes the paper.",7
43,4,Journal of Economics and Finance,04 January 2019,https://link.springer.com/article/10.1007/s12197-018-9466-6,Governance structure and performance of private family firms,October 2019,Tarun Mukherjee,Vighneshwara Swami,Wei Wang,Male,Unknown,,Mix,,
43,4,Journal of Economics and Finance,20 November 2018,https://link.springer.com/article/10.1007/s12197-018-9460-z,Investor reaction to simultaneous news releases: unemployment vs. earnings,October 2019,Neeraj J. Gupta,Vitaliy Strohush,Reilly White,,Male,,Mix,,
43,4,Journal of Economics and Finance,23 February 2019,https://link.springer.com/article/10.1007/s12197-019-09472-w,Estimating volatility transmission between oil prices and the US Dollar exchange rate under structural breaks,October 2019,Hassan Anjum,,,Male,Unknown,Unknown,Male,"Both oil prices and exchange rates are intrinsically linked with the economy. As a matter of fact, both of these series are closely followed by financial market participants and policy makers. There are different theoretical models that exist in the literature that provide a link between oil prices and exchange rates. Seminal work by Krugman (1983) and Golub (1983) provide theoretical models which form the basis of many empirical results. They argue that oil-exporting countries experience an increase in wealth in the short run when oil prices rises as wealth is transferred to oil-exporting countries which result in improvement of the current account balance in domestic currency terms. Consequently, they expect currencies of oil-exporting countries to appreciate and currencies of oil-importers to depreciate after a rise in oil prices. Another theoretical reasoning for the impact of exchange rates on oil prices was given by Bloomberg and Harris (1995) which is based on the law of one price for tradable goods. They argue that since oil is a homogeneous internationally traded commodity priced in US dollars, depreciation in the US dollar will reduce the oil price for foreigners which will increase the price of crude oil in US dollars. In addition to the above theoretical models, there is an indirect relationship between oil prices and exchange rates which is driven by common factors. These could include GDP, overall price level, interest rates or stock prices, as all these factors simultaneously affect exchange rates and oil prices. Ross (1989) argues that the rate of information flow affects the volatility in asset return. This implies that the rate of information flow from one market can contribute to the volatility generating process of another market. Thus, it is important to understand how shocks and volatility is transmitted between crude oil prices and exchange rate market over time. Understanding this volatility transmission is not only important for derivative valuation and hedging decisions but also has significant consequences for broader financial markets, the crude oil market, and the overall economy. Also many different financial assets are traded based on oil prices and exchange rates, so it is important that financial market participants correctly estimate the volatility transmission mechanism across these two series over time in order to make optimal decisions. One of the most popular methods to model market volatility dynamics is through GARCH models. However, these models assume that volatility is generated from a stable GARCH process. But recent evidence suggests that there are structural breaks in the unconditional variance of oil prices (Ewing and Malik 2010) and exchange rates (Malik 2003). Lamoureux and Lastrapes (1990) show that standard univariate GARCH models tend to overestimate the volatility persistence when structural breaks are ignored. Ewing and Malik (2005) show that accurate estimation of the volatility transmission dynamics requires endogenously detecting structural breaks and then incorporating them in the bivariate GARCH model. Thus, I use the model proposed by Ewing and Malik (2005). This paper studies the volatility dynamics of oil prices and US dollar exchange rate using the daily returns from January 2000 to December 2015. Using modified iterative cumulative sum of squares (ICSS) algorithm given by Inclan and Tiao (1994), I find several significant structural breaks in volatility of both series. I incorporated these structural breaks into the univariate GARCH models and find substantial reduction in estimated volatility persistence. Using BEKK parametrization of the bivariate GARCH model, I find no evidence of volatility transmission between oil prices and the US dollar exchange rate if structural breaks are ignored in the model. However, after accounting for structural breaks in variance in the bivariate GARCH model, I find significant volatility transmission between oil prices and the US dollar exchange rate. These findings are possibly driven by cross market hedging and sharing of common information among financial market participants. I compute dynamic risk minimizing hedge ratios and find that average hedge ratios change substantially after accounting for breaks.",14
43,4,Journal of Economics and Finance,28 February 2019,https://link.springer.com/article/10.1007/s12197-019-09471-x,On the effects of policy uncertainty on stock prices,October 2019,Mohsen Bahmani-Oskooee,Sujata Saha,,Male,Female,Unknown,Mix,,
43,4,Journal of Economics and Finance,22 June 2019,https://link.springer.com/article/10.1007/s12197-019-09482-8,Determinants of foreign portfolio investment in emerging markets: evidence from Saudi stock market,October 2019,Ahmed Badawi,Anas Al Qudah,Waleed M. Rashideh,Male,Male,Male,Male,"Stock market liberalization occurs when a country’s government decides to allow foreigners to purchase shares of domestic companies that are listed on its stock market. Numerous theories and empirical studies have explored the consequences of such stock market liberalization. So far, there is a general agreement on the positive impact of such liberalization on financial markets, in general (Choe et al. 1999; Errunza and Losq 1985; Stapleton and Subrahmanyam 1977). Saudi Arabia’s stock market recently opened up to foreign investors in an effort to promote market stability and reduce volatility. The Middle East has also seen comparable attempts in Qatar and the UAE (Alghamedi and Misfer 2012). The opening up of the Saudi Arabia stock market to foreign investors means that Middle Eastern markets will have a greater representation in emerging markets. This increases the need to study investment opportunities in emerging economies and the factors that determine why investors move to these markets in the first place. Financial markets in some emerging economies provide good incentives for growth due to the lower than average capital income in economies such as Thailand, Turkey, the UAE, the Philippines, Hong Kong, and Kuwait. This factor pushes foreign direct investors and multinational companies to move into these markets. However, when companies are planning whether to cross these countries’ borders and increase their investment there, it is important to evaluate the determinants that influence these decisions (Khanna and Palepu 2010). Traditional economies are generally noted for their over-reliance on agriculture and resource industries. This makes their markets susceptible to macro-level factors, such as demand and supply fluctuations on foreign markets. Additionally, the weakened comparative advantages of trading in some markets, specifically most of the Gulf Cooperation Council, increasingly affects these markets at both macro and micro levels (AlQudah et al. 2016). Over the years, numerous researchers have attempted to understand the determinants that lead to foreign investment and portfolio diversification. And although programmes in emerging markets have attempted to increase foreign inward investment, the foreign ownership of shares has not increased and still remains below expectations. A number of researchers have studied this phenomenon (French and Poterba 1991; Cooper and Kaplanis 1994); Tesar and Werner 1995; Unite and Sullivan 2003; Gul et al. 2010); and Coeurdacier and Gourinchas 2016), and different justifications for investment in portfolios have been put forward. To this point, however, no successful explanation has been generalized (Kang 1997). Numerous researchers have investigated the determinants of the foreign ownership of stocks in investment portfolios. Although each investigation uses a different approach, in general, they all pay specific attention to the firm-specific attributes that influence investment decisions. For example, Kang (1997) investigated foreign investment in Japanese stocks and found that investors favour firms in the manufacturing sector, large firms, and firms whose accounting practices are reliable and follow generally accepted accounting principles. Investors also prefer firms whose risk can be reduced through portfolio diversification (i.e., firms with low unsystematic risk), and firms with low debt-to-equity ratios. Dahlquist and Robertsson (2001) studied foreign ownership in firms. Their analysis shows that foreigners prefer large firms, firms that pay low dividends, and firms with large cash reserves. Several sets of determinants of foreign portfolio preferences have been suggested and tested in different countries. So far, there is no generally accepted list of exceptional attributes. This study aims to fill the gap left by previous research in terms of the variables that determine foreign entrance into emerging markets; thereby, adding to the existing literature in this field. This study analyses the factors that determined foreign investment in these markets following the Saudi government’s declaration of openness of the Saudi Arabian stock market to foreign investors. This paper aims to provide more insight to understanding the issues that any foreign investor would consider before deciding whether to enter this particular market. The study’s approach is a cross-sectional investigation of 125 non-financial institutions. It compromises four sections: the current section provides an overview of the topic, the motivation for the study, and the significance of its objectives. Section 2 presents the theoretical considerations and relevant previous studies on the topic. It also provides a brief discussion of the determinants of foreign portfolio investment in emerging markets, identifies various firm attributes that are common to foreign ownership, and discusses the trading behaviours of various investors and markets. It also lays out the research hypotheses and the theoretical framework that backs up these hypotheses. Section 3 describes the research data and explains the research methods that are used to demonstrate the preferences of foreign investors in relation to the Saudi stock market. Section 4 presents the results of the empirical test, provides some descriptive statistics, discusses the results that show support for the propositions and provides good reasons for inferring the likely significance of the hypotheses. The final section summarizes the research, sets out the implications of its empirical and normative findings, and proposes some recommendations that flow from these findings.",5
43,4,Journal of Economics and Finance,26 January 2019,https://link.springer.com/article/10.1007/s12197-019-9470-5,Celebrity attraction in the minors: the case of Tim Tebow,October 2019,Rodney J. Paul,Charles Garrett,Kyle Liotta,Male,Male,,Mix,,
43,4,Journal of Economics and Finance,29 May 2019,https://link.springer.com/article/10.1007/s12197-019-09479-3,Betting with house money: reverse line movement based strategies in college football totals markets,October 2019,James Francisco,Evan Moore,,Male,Male,Unknown,Male,"Over $90 billion is bet on college and professional football (both legally and illegally) in the United States annually (American Gaming Association 2016). In 2014 alone, more than 12 million individual bets were made on college football at the top offshore betting sites (Fuhrman 2015). That amount is likely to increase following the United States Supreme Court’s ruling in Murphy v. NCAA, which opened the door to more states allowing sports betting. Most college football wagering takes one of two types: “spread” betting, which relies on a handicap and relates to the outcome of the game, and “totals” betting, which is a wager on the combined final score of both teams. Bettors can wager on the over (under), which is that the teams’ combined score will be greater than (less than) the given total. This paper will concentrate on totals wagering.",2
43,4,Journal of Economics and Finance,11 June 2019,https://link.springer.com/article/10.1007/s12197-019-09487-3,Labor market freedom and geographic differentials in the percentage unemployment rate in the U.S.,October 2019,Richard J. Cebula,,,Male,Unknown,Unknown,Male,"Several scholarly empirical studies conclude that elevated levels of overall economic freedom tend to induce greater real economic growth, stability, and prosperity. Interestingly, this conclusion is predicated upon and has been verified using a wide variety of model specifications, econometrics techniques, and variables (Gatons and Cebula 1972; Gallaway and Cebula 1973; Tortensson 1994; Ali 1997; Doucoullagos and Ulubasoglu 2006; Cebula 2011; Wu 2011; Ashby et al. 2013; Shumway 2017; Shumway and Davis 2017; Berdiev et al. 2018). This stimulatory impact from higher levels of economic freedom, which has been applied to a number of nations, including China (Wu 2011) and the U.S. (e.g., Tortensson 1994; Ali 1997; Cebula 2011; Ashby et al. 2013; Shumway 2017; Shumway and Davis 2017), works through a variety of channels to augment productive efficiency, to promote efficient market transactions, and to stimulate increased entrepreneurship, investment, invention, and innovation, and in so doing acts to yield more efficiently operating markets. Indeed, it is perhaps noteworthy that using both cross-section analysis and panel data analysis for 82 nations, Doucoullagos and Ulubasoglu (2006) found that elevated economic freedom has acted to stimulate real economic growth in a variety of different economic, political, and social contexts around the globe. The present study inquires whether, at the state level in the U.S., a more elevated level of labor market freedom per se, in all three of its sub-index forms and in the form of an overall average thereof, acts to reduce the percentage rate of unemployment. This perspective of the present study argues that since unemployment is literally a phenomenon of the market for labor services, the dimension of economic freedom that logically should be most closely linked thereto is labor market freedom per se. This is because, by its very nature, labor market freedom directly influences the economic and institutional climate of and hence the functioning and efficiency of the labor market and the transactions therein undertaken. In this study, labor market freedom is first measured in three specific ways, namely, by three labor market freedom sub-indices that have been developed at the Frasier Institute and published in the Economic Freedom of North America (Stansel et al. 2018), each of which is described in some detail in the next section of this study. A fourth way of measuring labor market freedom is also considered here, namely, the unweighted arithmetic mean of these three labor market freedom sub-indices. Aside from focusing on the state-level civilian unemployment rate and, correspondingly, on labor market freedoms as indicated above, this study differs from most of the existing body of related research in at least two important ways. Firstly, unlike any of the published related literature to date, the study adopts a panel dataset covering the time period beginning with the year 2008 and ending with the year 2016; the year 2016 is the last year the necessary data are all currently accessible for. Thus, this study embodies the entirety of the so-called Great Recession period as well as more than six of the full years following the Great Recession, making the study period quite current and relevant from a policy perspective. Secondly, the model adopts the Panel 2SLS (Panel Two Stage Least Squares) technique for the 2008–2016 period; Panel 2SLS has not previously been used in the related literature. The context of this project is developed below in Part 2, following which the baseline estimating equations and the Panel 2SLS estimations thereof can be located in Part 3. In Part 4 of this study, an additional set of estimation findings is provided. Support for the fundamental hypothesis in the present paper, namely, that the state-level unemployment rate is a decreasing function of labor market freedoms, is shown in every one of the estimations. Lastly, a summary of the basic findings of this study along with potential relevant public policy implications of the analysis/empirical findings are provided in Part 5.",
44,1,Journal of Economics and Finance,06 February 2019,https://link.springer.com/article/10.1007/s12197-019-9469-y,"Monetary policy, social capital, and corporate investment",January 2020,Chad Kwon,Gongfu Zhang,Haiyan Zhou,Male,Unknown,Unknown,Male,"The classical irrelevancy theory by Modigliani and Miller (1958) states that capital structure would not influence firms’ investment decision. This indicates financial decisions by firms would remain unchanged in regard to which or how many projects firms pursue. However, relaxing a few assumptions that allow this framework to remain plausible leads researchers to question firms’ investment decisions may be time-sensitive. Specifically, depending on the economic climate and actions taken by a central bank may influence firms to consider how much capital to invest when the prevalent interest rate in the market place attracts or repels firms to raise capitals. Numerous publications have been devoted to a theoretical analysis of the firms’ corporate finance, investment behavior and monetary transmission in recent years. On the empirical side, there are a lot of micro evidence from developed economies, such as the USA (Vijverberg 2004), the UK(Mizen and Vermeulen 2005; Guariglia 2008), the Euro area(Bond et al. 2003; Haan and Sterken 2011) and Canada(Aivazian et al. 2005). However, the literature didn’t come to a consistent conclusion about the relationship between monetary policy and corporate investment. One of the possible reasons may be the different cultures of different countries. Among various cultures, the role of social capital in social life is different. While this field has been well explored regarding the link between monetary policy and corporate investment (e.g., Mojon et al. 2002; Bernanke and Blinder 1988), we introduce social capital in this study to deliver original evidence regarding the triangular interaction among monetary policy, social capital, and corporate investment behavior. We are motivated by potential fueling or hindering effect of social networking that contributes to leadership inside firms. We believe strong current and historical social connection that may not appear on financial statements, press releases, or government filings do not only influence the direction and magnitude of firms’ future project prospects directly, but also influence the relationship between the monetary policy and corporate investment. We explore this possibility by studying the effects of monetary policy status – either tight or easy – and social capital on investment behavior among firms. Therefore, we add to the literature on monetary policy and responsible changes in corporate investments by introducing a new dimension - social capital - that may contribute more toward decision process for corporate leadership.",2
44,1,Journal of Economics and Finance,02 April 2019,https://link.springer.com/article/10.1007/s12197-019-09475-7,Board leadership structure and corporate headquarters location,January 2020,Nilakshi Borah,Hui James,,Unknown,,Unknown,Mix,,
44,1,Journal of Economics and Finance,18 February 2019,https://link.springer.com/article/10.1007/s12197-019-9468-z,Analyzing the impact of workers’ remittances on household consumption in Latin American and Caribbean Countries,January 2020,Harri Ramcharran,,,Male,Unknown,Unknown,Male,"The impact of remittance flows on the economy of recipient countries continues to stimulate current research, for example, Grigorian and Kryshko (2017), Barajas et al. (2009), Fajnzylber and Humberto-Lopez (2008), and Goldberg and Levi (2008).Footnote 1 Recent studies focus on several issues: (i) Gabriela-Mundaca (2009) on economic growth, (ii) UNCTAD (2011) and Adams and Page (2005) on the poverty level (iii) Aggarwal and Demirguc-Kunt (2006) on financial sector development, (iv) Lueth and Ruiz-Arranz (2006) on the determinants of flows, (v) Neagu and Schiff (2009) on the stability, cyclicality and stabilizing impact, and (vi) Yang (2006) and Yang and Choi (2007) on consumption smoothing.Footnote 2 A topical issue is the impact of remittances on consumption, specifically as related to consumption augmentation, smoothing and volatility, and the potential Keynesian multiplier effect on the economy. The World Bank (2015) has examined ways that remittances can help promote consumption stability. In recent several countries have implemented economic liberalization policies that inter alia, target consumption driven growth. Previous studies, World Bank (2006a) and Adams (2006), are supportive of the consumption-increasing and poverty- reduction effects of remittances, these results, however, are based on survey data and the analysis of descriptive statistics. We extend the literature by using amore analytical methodology. We empirically estimating a consumption behavior model specified within the framework of the “permanent income hypothesis” (PIH), originally articulated by Friedman (1957) and Modigliani (1976), to analyze the impact of remittances on consumption pattern in eleven Latin American and Caribbean (LAC) countries for the period 2003–2013. The PIH relates consumption to permanent and transitory income. The theoretical model is justified on the basis of several analytical studies of the PIH, some include Willassen (1978), Hall and Mishkin (1982) and Kreuger and Perri (2008). They have applied (and tested the validity of) the PIH to analyze consumption behavior using different measurement of income (transitory and permanent). We use several panel data models (Restricted, Unrestricted-Fixed Effects, Fixed Effects and Random Effects) and perform diagnostic tests to validate the results. The independent variables are: (i) real per capita national income (exclusive of remittances) as the measurement of “permanent income”, (ii) remittances as “transitory income” and (iii) real interest rate (the opportunity cost of money). We justify the use of these variables within the framework of the PIH later in the paper. The interpretation of the results is as follows: (i) the coefficient of remittances (transitory income) measures the consumption augmentation and saving effects; (ii) the correlation between remittances (transitory income) and real per capita income (permanent income) indicates the cyclical effect; a low (or negative) correlation is considered counter cyclical and a positive (or high) correlation pro-cyclical; also a negative correlation is indicative of the consumption smoothing effect of remittances, and (iii) the significance of real interest rate indicates the ability of households (recipients) to make intertemporal substitution in consumption through savings and the accumulation of assets. We use data for the period 2003–2013 for eleven Latin American and Caribbean (LAC) countries Colombia, Costa Rica, Ecuador, Mexico, El Salvador, Guatemala, Honduras, Dominican Republic, Jamaica, Nicaragua, and Panama. The choice of these countries is based on several factors: (a) International Migration Outlook (OECD 2006) lists them as the largest recipients of remittances in the region, and (b) the relevant data are available for them, unlike some other countries of the region. The choice of the period has to do with the availability of published data on a country basis (a) the World Bank began publishing data in the early 2000 and on a country basis in 2003 (see Migration and Remittances Fact Book2011); (b) the latest edition (Migration and Development Brief, World Bank2015) and Migration and Remittances Factbook (World Bank 2006a, b) have only preliminary estimates of remittances for 2014 and 2015; and (c) International Financial Statistics (IMF2015), has many recent data missing on exchange rates, inflation rates, and interest rate for some countries. These countries constitute a group with different levels of GDP, consumption, population, and remittances Appendix Table 3 provides important ratios on consumption/GDP, per capita GDP, per capita remittances, and remittances/GDP over the same period. The high consumption/GDP and remittance/GDP ratios justify the importance of this study. Remittance flows to developing countries continue to increase after the current global recession; officially recorded flows are estimated to have reached $430 billion in 2014, an increase of 3.2% over 2013 (see Appendix Table 4). Flows to LAC countries reached $64 billion in 2014, this amount comprises about 15% of total flows to developing countries. The LAC region receives over 75 % of its remittances from the United States, thus these flows are susceptible to USA economic cycle and regulatory policies. The findings of this study have important policy ramifications regarding consumption stability and the leveraging of remittances to improve the economic and social development of recipient countries. This is consistent with the achievement of the Sustainable Development Goals (United Nations 2015) of eradicating extreme poverty and hunger. The Economic Commission for Latin America and the Caribbean (2014) notes that in recent years an increase in household income in the region has resulted in a striking rise in consumption, however, the consumption pattern is strongly pro-cyclical and volatile; this has exposed the economies of the region to greater vulnerability. There are also significant concerns regarding the volatility (risk factors) affecting remittance flows to the region: (a) the economic crisis in the USA had a dampening impact on migrants’ income, prompting them to decrease the frequency of their transfers, and (b) regulatory factors, for example, current immigration reform policies enacted by the Trump administration that could impact the number of immigrants from LAC in USA.Footnote 3 The rest of the paper includes the following: Section 2 reviews of the literature on the PIH; Section 3 analyzes the recent trend in remittance flows; Section 4 discusses the impact of remittances, economic, and consumption; Section 5 discusses the data and the statistical properties; Section 6 discusses the specification of the model; Section 7 discusses the empirical results and the ramifications; Section 8 provides the conclusion.",3
44,1,Journal of Economics and Finance,14 May 2019,https://link.springer.com/article/10.1007/s12197-019-09474-8,Real estate prices and banking performance: evidence from Canada,January 2020,Robert N. Killins,,,Male,Unknown,Unknown,Male,"It is well-known that bank lending and bank profits have been strongly knotted on frequent occasions to asset price fluctuations, at times culminating in banking crises. Among various key assets, real estate is of special interest for several reasons. The global financial crisis of 2007–2009 has illustrated the fact that problems in the real estate market can have a pronounced impact on the financial sector (Duca et al. 2010; Gimeno and Martinez-Carrascal 2010; Gerlach and Peng 2005; Hott 2011; Mian and Sufi 2011). When real estate prices grow substantially, the wealth of real estate owner’s increases and as an effect of the higher wealth, default rates decrease. These developments make it more profitable to invest in mortgages, and banks and other financial institutions tend to take advantage of the increased business (Quigley 2001; Hott 2011). Further, increasing real estate prices should reduce the riskiness of banks’ assets and decrease the likelihood of financial distress in the banking sector (Niinimäki 2009). The Canadian financial system withstood the 2007–2009 financial crisis better than most other major financial markets. The reasoning behind why the Canadian financial markets faired so well has been attributed to various factors, including strong regulatory regime, stringent capital requirements for banks, federal supervision, a concentrated banking system, strict mortgage market regulations, and a conservative appetite for risk (Bones 2009; Bordo et al. 2015; Calmes and Théoret 2013; Mohsni and Otchere 2018). In addition to the strong financial system in Canada, the real estate market has been one of the best performing markets over the last decade. According to the Economist, the Canadian real estate market is one of the most overvalued markets in the world.Footnote 1 Figure 1 provides a visual of the real housing price index for the Canadian market over the past two decades. Although prices did decline in the 2007–2009 period, the Canadian market did rather well when compared to most other developed real estate markets. The academic literature surrounding the housing market in Canada has suggested that the long lasting boom in housing prices over the past two decades is attributed to several factors including capital flows, immigration, and the cost of capital (Cerutti et al. 2017; Pavlov et al. 2015; Pavlov and Somerville 2019). The stable nature of the major players in the real estate lending market in Canada undoubtedly has provided some of the stability in the real estate market. Over the past several decades, the major Canadian banks have become the major lender in the real estate market. The post-war baby boom resulted in a lack of supply of mortgage funds, and the National Housing Act was subsequently amended to permit banks to make mortgage loans.Footnote 2 These changes have lead banks to capture market share from insurance companies and mortgage firms, and now the Canadian residential mortgage market is dominated by banks, which together hold approximately 75% of the value of outstanding mortgages (Crawford et al. 2013). Housing prices More recently, changes have been implemented via the Canadian federal government to slow the rapid growth in the housing market, and a recent report suggests that mortgage origination has been slowed across various lenders in Canada.Footnote 3 These changes, which are summarized in the Appendix (Table 9), have been targeted to slow the growth of household debt in Canada, in which Canadian regulators have highlighted as concerning and a risk to the financial system.Footnote 4 The purpose of this paper is to shed light on how real estate prices impact the profitability of the Canadian banking sector through traditional measures such as ROA and ROE along with risk-adjusted measures. With higher asset valuations leading to increases in lending and financing activities, theoretically one would expect an increase in returns to financial institutions, but this may come with increased risk. Using a comprehensive dataset of Canadian Schedule 1 and a variety of real estate measures, this study finds that positive changes in housing prices tend to increase bank profitability when evaluating profits through traditional measures of ROA and ROE. When using risk-adjusted measures of profitability, these positive impacts tend to become insignificant. In addition to measuring simple real estate price changes, this study indicates that real estate exuberance also tends to increase non-risk adjusted banking profits, but not risk-adjusted measures. These findings highlight the importance of using not only price changes in asset prices, but also deviations of asset prices from their underlying fundamental values when researching the impacts of real estate on profitably or returns. The findings of this research adds value to the current literature by providing investors, regulators, and policymakers with a robust understanding of how deviations in real estate prices can impact profitably in the banking sector. The remainder of the paper is structured as follows. Section 2 provides a literature review of the related literature. Section 3 discusses the data and variables for the study. Section 4 provides the methodology and empirical model. Section 5 provides the results of the estimations. Finally, the conclusion and implications are presented in Section 6.",7
44,1,Journal of Economics and Finance,03 May 2019,https://link.springer.com/article/10.1007/s12197-019-09477-5,Is the empirical relationship between hours and productivity effected by corporate profits?,January 2020,Kashif Zaheer Malik,Syed Zahid Ali,,Male,Male,Unknown,Male,"The seminal work of Kydland and Prescott (1982) documented a prodigious role of productivity shock in explaining variations in economic aggregates such as output, consumption, hours-worked, real wages, and average productivity etc.Footnote 1 These results were supported by Cooley and Prescott (1995), Prescott (1986), and King and Rebelo (1999). These studies in fact play an important role in sculpting the famous RBC theory (Real Business Cycle) (Rebelo 2005). The predictions of RBC models, however, are challenged by Keynesian, which strongly believe that most of the short-term economic fluctuations are essentially a result of demand-side shocks (Gali and Rabanal 2005). The controversy between the two schools of thoughts, in the recent past, boils down to the short-run dynamics of the labor input to the positive technology shock. The standard RBC models predict a positive comovement between hours-worked and an increase in productivity (see Christiano et al. 2003 and references therein). To the contrary, the standard Dynamic Stochastic General Equilibrium models (DSGE) which assume sticky prices, predict a fall in hours-worked. Gali (1999) tested the contradicting predictions of the two models with the help of U.S. data while estimating (Structural Vector Auto-regressive) SVAR model. Conditional on a technology shock, Gali find a negative correlation between hours-worked and productivity and that the contribution of productivity shock in explaining variations in economic aggregates are rather weak. He used this result as an evidence of sticky prices and proposed a sticky price version of DSGE models a better candidates to mimic the actual data. The later work of many authors such as Gali and Rabanal (2005) and Basu et al. (2006) find closed amenity with Gali (1999). Basu et al. (2006) attempted to calculate a growth accounting based-measure of technology (Solow residual) after controlling for non-technological effects in aggregate total factor productivity. The study find that technology improvements reduce total hours-worked within the year, however, hours-worked increases with a lag of up to two years. In the defense of RBC theory and its predictions, Christiano et al. (2003) argue that the results derived by Gali (1999) are sensitive to measurement of hours-worked series. Gali (1999) uses first-differenced log hours per capita in his SVAR model. Christiano et al. (2003) while using level of (log) hours worked per capita in Gali (1999) SVAR model generated a positive comovement between hours-worked and productivity, which is consistent with RBC model prediction. In short, Christiano et al. (2003) claimed that using first-differenced or detrended (log) hours per capita in SVAR model in fact makes the SVAR model misspecified, which in turn generates anomalous behavior of hours-worked to a positive productivity shock. Dotsey (1999) argued that the conditional correlation found by Gali (1999) cannot be taken as a sign of price stickiness. He further argued that delinking the conditional correlation between hours-worked and increase productivity from the nature of monetary policy is a perilous enterprise. He demonstrated that a sticky price model may generate a positive correlation between hours-worked and increase productivity, if the central bank respond to expected inflation and output deviation from their target levels optimally. In an open economy context Ali et al. (2012) show that in the presence of monetary and fiscal restraints, an improvement in productivity would cause an increase in employment. Altig et al. (2011) indorses Gali (1999) conclusion that technological progress account for a small variations in economic aggregates and that monetary policy has played a pivotal role in determining the nature of the transmission of technology shock. Following Lucas (1997) claim that business cycles are the result of aggregate shocks rather than sector-specific shocks, the relationship between productivity and hours-worked is also analyzed at the industry level by quite a few authors. Long and Plosser (1983) pioneer work, for example, find that comovement of different sectors of the economy in fact give rise to business cycles. In the context of U.S. economy, Rebelo (2005) find a strong comovement between employment in different industries which complement the findings of early study by Christiano and Fitzgerald (1998). Rebelo (2005) find a strong correlation between hours worked in major sectors (such as construction, durable goods producers, non-durable goods producer, and services) and aggregate private hours. The study find an average correlation is about 75%. Hornstein (2000) find a similar strong comovement between gross output, value added, and material and energy use. Francis (2001) apply the SVAR framework of Gali (1999) to manufacturing industries and find a negative correlation between hours-worked and output growth after a positive technology shock for most of the industries. Shea (1998) is another study based on industry level data, find that hours-worked and total factor productivity are negatively correlated to each other. Early study such as Kiley (1998) also find a negative correlation between productivity and hours in U.S. manufacturing sector. Basu et al. (2006), while using Dale Jorgenson’s annual KLEM data for 29 industries from 1949 to 1996, find that in 22 industries total hours worked falls after a positive total factor productivity shock. On the other hand, Chang et al. (2009) study which was based on 458 4-digit U.S. manufacturing industries over the period 1958–1996, find that a positive response of hours-worked to a positive productivity shock is more likely to occur in the presence of low cost of maintaining inventories, more elastic industry demand, and high degree of price flexibility. Conducting a similar study by Khan and Tsoukalas (2013) for 31 UK manufacturing and non-manufacturing industries, find that in 26 industries hours-worked fall in the short-run. As opposed to U.S., Khan and Tsoukalas (2013) found that due to relatively low adjustment cost for capital in UK, the fall in hours-worked were relatively less profound. The topic whether hours will increase or decrease in the event of increase productivity is important for two reasons. As Gali (1999) highlighted that if hours decrease in the event of increase in productivity then it may be taken as an evidence of friction in the economy. Particularly, if prices adjust sluggishly over time then it supports the views of Keynesians that variations in the nominal variables such as money supply have economic consequences and effect the real variables such as output and level of employment. The second reason which make this topic important is also linked with the first reason. If we agreed that prices adjust sluggishly overtime and that increase in productivity causes unemployment then this creates the scope of government intervention to protect the jobs of low skilled workers. In this paper we study the conditional correlation between hours-worked and productivity in the event of technology shock in two stages. In the first stage we derive a number of results while providing a closed form solution of a sticky price DSGE model where central bank managed the economy based on Taylor (1999) rule. In the second stage, we extend an otherwise a standard bivariate SVAR model, which account for corporate profits. Since corporate profits are often used as a proxy to gauge the health of the economy and deemed to be one of the cause of economic fluctuations, we find it interesting to analyze the role of corporate profits to determine the relationship between increased productivity and hours at the aggregate level. While collecting data at sectoral level (nonfinancial sector, manufacturing sector, whole trade sector, and retail trade sector) we also estimated and simulated SVAR model by incorporating corporate profits to study the behavior of output, profit, productivity, and hours-worked in the event of positive technology shock. Our closed-form solution results based on sticky price DSGE model, show that a surprised technology shock causes an increase in output and a decrease in inflation in the impact period. On the other hand, we note that the impact of the positive technology shock on average productivity of labor and hours-worked, are ambivalent. In the presence of high degree of price flexibility or when central bank systematically respond to technology shock, however, we find that hours-worked increase. Our SVAR model, on the other hand, depicts that the inclusion of corporate profit in Gali model solve the problem of anomalous behavior of hours in the event of productivity shock for the aggregate data. However, we find that sectoral results are more specific to sectoral dynamics. Non-financial sector mimics the aggregate findings, however, manufacturing, retail, and wholesale trade sectors represents contradictory picture. The rest of the paper is organized as follows. In section 2 we solve the DSGE model and derive few analytical results. In section 3 we discuss identifying restrictions, time series properties of data, and empirical framework to implement short-run and long-run restrictions. Section 4 represents empirical results. The last section conclude the paper and offer few policy recommendations.",
44,1,Journal of Economics and Finance,03 May 2019,https://link.springer.com/article/10.1007/s12197-019-09476-6,Learning advanced technology in easier ways from developed countries,January 2020,Inyong Shin,,,Unknown,Unknown,Unknown,Unknown,,
44,1,Journal of Economics and Finance,21 May 2019,https://link.springer.com/article/10.1007/s12197-019-09473-9,Converting remittances to investment: a dynamic optimal policy,January 2020,Nasreen Nawaz,,,Female,Unknown,Unknown,Female,"Workers’ remittances to developing countries have grown over time to be an important source of financing for household expenditures as well as investment. The funds are used for both consumption and investment in the home countries of the migrants. The increasing amount of inflows migrants remit to their home countries is of great importance. These inflows are increasingly substantial in terms of their stability, growth rates and as a share of GDP. In 2000, the world remittance inflow stood at $131 billion. By 2005, this figure more than doubled to $263 billion or 0.62% of the world’s GDP. This is a substantial amount compared to the Official Development Assistance (ODA) to developing countries which constitutes less than 0.24% of the world GDP. The World Bank estimates indicate that in terms of external inflows, remittances are second only to Foreign Direct Investment (FDI). It is also important to point out that for the very poor nations, remittances have surpassed even FDI. Among the Least Developing Countries (LDCs), remittance inflows in 2005 are 5.4% of GDP whilst FDI stood at 2.7% of GDP in the same period.Footnote 1 The extent to which remittances will stimulate investment and thereby also growth in developing countries is dependent on a number of circumstances. These circumstances will influence the use of remittances for consumption, savings or investment. They will also have an effect on the choice of transferring funds through channels of a formal transparent character rather than channels of an informal character that is not recorded in official statistics. To begin with, parts of remittances are not used for consumption by the receiver. Lucas and Stark (1985) present several hypotheses for motivations to remit, ranging from pure altruism to self-interest. Cox (1987) tests alternative hypotheses concerning motivation for inter vivos transfers. Adams (1998) studies the marginal effects of remittances on consumption and investment. Arif et al. (1999) investigates the question whether return migrants and their families succeeded in directing remittances into investment. Ilahi and Jafarey (1999) model informal loans between migrant and extended family for financing the costs of international labor migration. In Glytsos (2002), various channels transmitting the impact of remittances on development are investigated based on the experience of countries from both sides of the Mediterranean basin. León-Ledesma and Piracha (2004) analyse the effect of remittances on employment performance for Central and East European (CEE) economies. Adams (2005) studies the relationship of remittances, household expenditure and investment in Guatemala. Chami, Fullenkamp and Jahjah (2005) develop a model of remittances based on the economics of the family that implies that remittances are not profit-driven, but are compensatory transfers, and should have a negative correlation with GDP growth. Mundaca (2005) finds in a sample of selected Central America countries that financial development tends to increase the responsiveness of growth to remittances. In a review of the global evidence, Adams (2007) documents that remittance receiving households invest more on the average than households without remittance receipts. Adams (2007) also shows that remittance receiving households tend to save more than the average households. The ability of financial intermediaries to expand credit to the private sector and thus give additional impetus to investment is thereby increased. Adams, Cuecuecha and Page (2008) use a nationally-representative household survey from Ghana to analyze within a rigorous econometric framework how the receipt of internal remittances (from within Ghana) and international remittances (from African or other countries) affects the marginal spending behavior of households on a broad range of consumption and investment goods, including food, education and housing. Fayissa and Nsiah (2008) suggest that remittances boost growth in countries with an underdeveloped financial sector. Ang, Jha and Sugiyarto (2009) studies the relationship of remittances and the household behavior in the Philippines. Giuliano and Ruiz-Arranz (2009) study how local financial sector development influences a country’s capacity to take advantage of remittances. Mundaca (2009) analyzes the effects that both workers’ remittances and financial intermediation have on economic growth. Finally, there are effects of remittances on investment from the multiplier effect of remittance-induced expenses. Several studies have found the multiplier effect to be quite large (see Lucas (2005)). Munyegera and Matsumoto (2016) find a positive and significant effect of mobile money access on household welfare, measured by real per capita consumption. The mechanism of this impact is the facilitation of remittances; user households are more likely to receive remittances, receive remittances more frequently, and the total value received is significantly higher than that of non-user households. According to Ratha (2016), remittances remain a key source of funds for developing countries, far exceeding official development assistance and even foreign direct investment. It is important for developing countries to utilize the resources through remittances for investment purposes for the long term economic growth. In order to convert remittances into investment, the governments have to rely on some incentive mechanism for the senders of remittances to encourage them to send remittances as savings. If the governments want to spend some resources on encouraging the people to convert their remittances into savings and hence into investment, it is extremely important to spend those resources in an efficient manner so as to reap the maximum gains possible in terms of economic efficiency, and hence an optimal remittance-investment policy. However, following any remittance-investment measure, there is some transition period during which the market adjusts to arrive at the new equilibrium. This transition time is usually ignored when evaluating the effect of a policy, and the major reason being a lack of theoretical framework for such treatment. As governments continue their efforts to reduce investment costs by streamlining financial procedures and enhancing the quality of related services, it is important that officials in charge of developing future plans in this area be fully cognizant of the economic efficiency gains and losses during the adjustment process in order to devise an optimal policy and subsequent monitoring. The existing literature does not provide a theoretical framework for conversion of remittances into investment through an incentive mechanism by the government by adopting an optimal policy with regard to minimizing the efficiency losses after the adoption of the policy. There have been some empirical studies regarding the impact of remittances on investment, however, they lack the theoretical support, and hence could not be used for prediction and hence the proposals for future policies. If a government introduces some measure in the form of an incentive for the earner abroad to save more, the earner’s cost (to the extent of his/ her share of savings in the home country) jumps to the pre-policy cost minus the additional costs causing inefficiency prior to the remittance policy affecting the quantity of savings and hence pushing the domestic market out of equilibrium. The supply and the demand of the savings, then adjust over time to bring the new post-policy market equilibrium. The interest rate adjustment mechanism is based on the fact that when the remittance-investment policy leads the market out of equilibrium, the buyers’ and sellers’ decisions are not coordinated at the current interest rate. It is essential to take into account the efficiency losses during the adjustment process while computing the benefits of remittance-investment policy. This paper develops a dynamic model and derives an optimal remittance-investment policy minimizing the efficiency losses (output and/ or consumption of savings lost) during the dynamic adjustment process taking into account the gains from the post-policy market equilibrium subject to a policy cost constraint. The remainder of this paper is organized as follows: Section 2 explains how the individual components of the market system are joined together to form a dynamic market model. Section 3 provides the solution of the model with a remittance-investment policy. Section 4 derives an optimal remittance-investment policy minimizing the efficiency losses subject to a policy cost constraint in a specific time period. Section 5 summarizes the findings and concludes.",
44,1,Journal of Economics and Finance,29 May 2019,https://link.springer.com/article/10.1007/s12197-019-09481-9,Do more financing obstacles trigger tax avoidance behavior? Evidence from Indian SMEs,January 2020,Mohamed A. Elbannan,Omar Farooq,,Male,Male,Unknown,Male,"The purpose of this study is to examine whether firms facing financing obstacles (1) resort to tax avoidance to raise finance, (2) use funds obtained through tax avoidance in increasing their investment, and (3) find it difficult to engage in tax avoidance in jurisdictions enjoying stronger institutional environment. We focus on small and medium-sized enterprises (SMEs) in developing economies because those face the most financing obstacles compared to larger firms or peers in developed economies. SMEs are an engine of economic development, and making financing available to these entities furthers economic growth, reduces poverty and reduces income inequality (Beck et al. 2004). However, the small size of SMEs, which makes them vulnerable to market and institutional failures, creates risk for potential finance providers (Beck et al. 2005). Size may reflect not only profitability and financial collateral but also political connections and relationships. Traditional literature in finance assumes that, while considering financing alternatives, firms are indifferent between internal and external sources of capital in perfect capital markets (Modigliani and Miller 1958). However, in reality, capital markets are far from being perfect, and the short-run and long-run effects of economic uncertainty on stock prices can be distressing (Bahmani-Oskooee and Saha 2019). The presence of information asymmetries between firms and their capital providers inhibits investors from accurately identifying optimal investment opportunities and hence introduces inefficiencies into the functioning of capital markets. In considering financing alternatives, these inefficiencies motivate firms to show strong preference towards internal sources of capital.Footnote 1 Prior literature finds positive relations between firm reliance on internal sources of capital and the level of capital expenditures. Fazzari et al. (1988) and Almeida and Campello (2007) document that firm-level cash flow is a significant determinant of investment undertaken by firms. More recently, Brune, Lee, and Miller (2015) argue that capital-constrained entities attempt to maximize the utility from their cash holdings and as a result make better acquisition decisions that non-capital-constrained entities. One of the capital market imperfections that can lead firms to rely on internal sources of capital is the extent of difficulties that they face in obtaining external capital. In this paper, we argue that firms facing greater obstacles in obtaining access to finance mitigate their disadvantage by increase their reliance on internal capital. One such mechanism is tax avoidance – the reduction of explicit taxes paid by firms to governments (Hanlon and Heitzman 2010).Footnote 2 By avoiding taxes, firms are able to increase their internal capital. All else equal, firms with higher levels of tax avoidance pay less to the government and have higher internal capital than firms with lower levels of tax avoidance. Using the data from World Bank’s Enterprise Survey on Indian firms, we find that firms with higher obstacles to access finance are more likely to avoid taxes. Our results are robust across various sub-samples. We also address the concerns regarding endogeneity of access to finance. One can argue that there may be some unobserved characteristics that affect firm’s ability to access to finance and its decision to engage in tax avoidance. Using demand of audit as an instrument, we show (by using instrument variable regression) that our results not only hold, but also become more pronounced. Results also shows that, when firms with higher obstacles in access to finance engage in tax avoidance, they are more likely to spend the taxes saved in undertaking new capital investment projects. Our results show that these firms are more likely to invest not only in real assets, but also are more likely to spend on research and development. These results are consistent with prior literature that shows positive impact of tax avoidance on the firm’s level of capital expenditures. Grubert (2003) and Overesch (2009), for instance, provide evidence in favor of increase in investments for firms that shift income to low-tax jurisdictions. They attribute their findings to tax avoidance. Finally, the results also show that the relation between access to finance and tax avoidance is moderated by the environment in which the firm operates in. We show that firms headquartered in states/provinces with better institutional infrastructure (low level of crime rate, high level of GDP, high level of literacy rate, or high level of human development) are less likely to avoid taxes when faced with obstacles in accessing finance than firms headquartered in other states. We argue that improvement in institutional environment can reduce the adverse impact of obstacles in accessing finance on tax non-compliance. Our paper makes the following contributions to the literature. First, while empirical evidence is abound on the efficacy of SME policies in developed countries, comparable evidence is scant for developing countries. In these latter countries, SMEs do not receive as much attention, resources, or clout as larger companies. Hence, shedding light on some potentially negative macroeconomic consequences that may occur because of the barriers that curb the growth of SME is a much-needed research goal. Second, while extensive literature exists on potential drivers of tax avoidance, much less attention is devoted to access to finance as a potential driver and even less attention is devoted to smaller firms versus larger, more profitable entities. By arguing that tax avoidance can occur in an SME setting, we hope to bring these smaller entities into the same spotlight where larger entities, undertaking similar activities, are placed. Finally, firm investment is good news but not if (and when) it comes at the expense of the taxpayer. As we show, SMEs may use tax funds to make private investments. We believe our results have economic significance and should be of interest to regulators and accounting standard-setters. The remainder of paper is structured as follows: Section 3 introduces the sample, discusses variable measurements, and specifies the econometric models. Section 4 presents the empirical results. Section 5 concludes.",4
44,1,Journal of Economics and Finance,15 May 2019,https://link.springer.com/article/10.1007/s12197-019-09480-w,International movements of money and men: impact on the informal economy,January 2020,Rajeev K. Goel,Rati Ram,Ashley Potempa,Male,Male,,Mix,,
44,1,Journal of Economics and Finance,01 August 2019,https://link.springer.com/article/10.1007/s12197-019-09493-5,"Structural change in Asia, the real effective exchange rate, and agricultural productivity",January 2020,Richard Grabowski,Sharmistha Self,,Male,Female,Unknown,Mix,,
44,2,Journal of Economics and Finance,15 June 2019,https://link.springer.com/article/10.1007/s12197-019-09483-7,Forming appropriate peer groups for bank research: a cluster analysis of bank financial statements,April 2020,Ken B. Cyree,Travis R. Davidson,John D. Stowe,Male,Male,Male,Male,"Using appropriate bank peer groups enables more accurate performance evaluation and risk analysis. Appropriate peer groups are important for assessing investment performance, executive compensation, bank mergers or other expansion activities, and regulatory effectiveness. Previous analysis of banking firms in academic research has sorted banks into groups based on a variety of classification schemes. Our work adds to the banking literature by advocating another approach to create appropriate peer groups. Most prior banking studies form groups based on size or other characteristics on an ad hoc basis as a matter of convenience and not necessarily in order to achieve the best comparison. The multivariate analysis of bank financial statements that we describe provides an alternative way to derive banking peer groups. We use a complete set of common size variables, measured as percentage breakdowns of the balance sheet and income statement, to fully represent the structure of bank financial statements. With these 33 underlying variables, cluster analysis is used to form bank clusters (groups) without pre-specifying or assuming the purpose of the groups. Our classification system is elemental and should have informational value on its own, or importantly, could have incremental value if used as an adjunct to other bank classification systems. We implement the cluster procedure with 6444 U.S. banks using 2014 year-end financial statements and provide detailed information about the derived clusters. Our results show that bank groups are clustered around their lending business model and other factors. U.S. banks, even the largest banks, are distributed across different clusters because they have differentiated financial and operating strategies and occupy somewhat different spaces in the competitive banking industry. This suggests that grouping banks based on asset size alone can create erroneous inferences about performance and other factors being studied. Using regression models, we evaluate the ability of these financial cluster-based groups compared to size-based groups to explain eight different bank performance and risk measures. These measures include the return on assets, net interest margin, charge-offs to loans ratio, and common proxies for bank risk categories. For seven of the eight cases, the R-squares of the regression models are significantly greater (from double to forty-fold) for the cluster-based variables than for the size-only variables, and the incremental significance of cluster variables (when added to models using size variables) is highly significant and much greater than the reverse relation (adding size variables to cluster variables). For the one case where the cluster variables are not superior, their joint significance is roughly double the significance of either set of variables alone. Our results illustrate the usefulness of the financial based clusters for bank researchers and policymakers. A distinctive feature of this study is the employment of common size statement measures instead of a set of financial ratios. Financial ratios share some well-known methodological problems (Deakin 1976; Lev and Sunder 1979). For a number of financial ratios, the denominator can be zero or negative, creating discontinuities in their distributions. Empirical analysis of financial ratios must deal with their non-normal distributions, including skewness and extreme outliers. Researchers often discard observations when these “not meaningful” values appear. The analyst also uses judgment or discretion to choose the set of ratios to employ (and how to define the ratios), which can impact the empirical findings. For example, Elsas et al. (2010) relate bank diversification to bank performance, by measuring diversification as proportions of interest revenue, commission, trading revenue, and other revenue and find that diversification has no effect on bank value. However, Laeven and Levine (2007) find that adding profitability to pooled models changes this value result, and that “examining more elaborate measures of bank diversification is an interesting challenge for future research.” Often, the results of well-designed models hinge on such choices made by the researcher. Common size statement variables curtail some of this discretion. Using common size statement variables has additional advantages over financial ratios. One advantage is that the researcher has much less discretion in choosing the variables because the common size variables are a complete set of financial statement accounts expressed as proportions of assets or revenue. The simplicity and inclusiveness of common size variables make them attractive because we are forming groups based on the (dis)similarities in financial statements rather than for a specific purpose (such as predicting failures). Our study uses cluster analysis of a large number of common size statement variables that are logically complete (non-overlapping and exhaustive) to identify groupings of banks that are similar within groups (and distinct across groups). Although some bank clusters may be riskier and subject to distress, they were not formed for that sole purpose. Research on commercial banks has a rich history because of the high-quality databases on the industry and number of scholars and organizations devoted to this research and because of the intrinsic complexities and importance of the industry (Barth et al. 1992). This study can be contrasted with that literature based on its purpose, type of variables used, and statistical methods employed. Two major types of empirical studies have been on bank failures and banking efficiency. Bank failure prediction studies (Meyer and Pifer 1970; Sinkey Jr. 1975; Lane et al. 1986; Cole and Gunther 1998; Cole and White 2012; Cleary and Hebb 2016; Demyanyk and Hasan 2010; Ravi Kumar and Ravi 2007; Jin et al. 2011) employ a number of multivariate methods such as regression, logit and probit models, discriminant analysis, neural networks, and artificial intelligence on sets of particularized variables that would help identify banks that will become financially distressed. Econometric methods have been used in efficiency studies on economies of scale, interstate branching, and diversification activities (Berger 1995; Berger and Mester 1997; Demsetz and Strahan 1997; Diaz and Azofra 2009; Hughes et al. 1996; Hughes and Mester 1998; Hughes and Mester 2013; Laeven and Levine 2007). Bank asset size is used to identify community banks and to otherwise separate banks based on operational differences (Berger 1995; Demsetz and Strahan 1997; Hughes and Mester 2013; Kumar and Ravi, 2007; Berger and Bouwman 2013). Cluster analysis has rarely been used in prior bank research. These works have grouped banks based on stock returns (DeLong 2001; Dias and Ramos 2014), selected balance sheet variables (Diaz and Azofra 2009), and bank-specific financial ratios or risk measures (Sorensen and Gutierrez 2006; Dardac and Boitan 2009; Ercan and Sayaseng 2016; Ayadi et al. 2016). A related multivariate method, canonical correlation analysis, was used by Simonson et al. (1983) to find relationships between the investing, lending, and financing decisions of banks. Our study is more general than these studies because it is focused on bank taxonomy and because of its use of a complete set of common size financial statements variables rather than a set of financial ratios. Other classification schemes are common in previous bank studies. For example, using loan portfolio characteristics is shown to be important by Carlson et al. (2013) who investigate bank loan growth as related to bank capital and find that matching based on loan type creates differential effects. Hubbard et al. (2002) study the spread on bank loans and use a matched sample of loan characteristics but only consider assets and capital for bank-specific variables. Neither study explores other matching groups that could be appropriate. Thus, one of our contributions to the literature is the appropriate sample matching variables for bank research to help studies properly segment bank risk and return characteristics. The discussion of our study is organized as follows. Section 2 describes the statistical methods we employ and the data used in our analysis. Section 3 presents and interprets the empirical results. Section 4 summarizes and concludes.",3
44,2,Journal of Economics and Finance,08 June 2019,https://link.springer.com/article/10.1007/s12197-019-09484-6,"Remittances, market size, and foreign direct investment: a case of sub-Saharan Africa",April 2020,William A. Amponsah,Pablo A. Garcia-Fuentes,Joseph A. Smalley,Male,Male,Male,Male,"Foreign direct investment (FDI), official development assistance (ODA), private debt and portfolio equity, and remittances are important sources of financing for development in developing countries.Footnote 1,Footnote 2 Developing countries received 47% of the global FDI inflows in 2017, which is related to FDI being the largest component of external finance for developing countries during the previous decade (United Nations Conference on Trade and Development [UNCTAD] 2018). FDI inflows to developing countries increased from $19 billion in 1990 to $662 billion in 2014 (World Bank 2016). In the regional context, in 2017, FDI inflows to Africa, Asia, and Latin America and the Caribbean (LAC) were, $42 billion, $476 billion, and $151 billion, respectively (UNCTAD 2018). Domestic investment is also an important factor that contributes to economic development in developing countries. In Africa, investment as share of GDP for the periods 1990–1999 and 2000–2011 was 18% and 19%, respectively, which is considered low relative to the average investment rates of 24% for the period 1990–1999 and 26% for the period 2000–2011 for all developing countries (UNCTAD 2014). However, FDI inflows to Sub-Saharan Africa (SSA) increased from $0.3 billion in 1980 to $44.6 billion in 2014 (Fig. 1). Remittances, net FDI inflows, and net ODA inflows to SSA, 1970–2014 (nominal U.S. dollars). Source: Authors calculations using data from World Developments Indicators, online version, World Bank, 2017 Remittances are also an important source of external finance for developing countries. Even though remittances are less than FDI in developing countries, they are more than official development assistance and private debt and portfolio equity (World Bank 2016). In the case of SSA, remittances increased from $1.4 billion in 1980 to $36.9 billion in 2014 (Fig. 1). An important aspect of remittances is that they are not part of GDP, but they increase households’ disposable income and consumption in developing countries. Dornbush and Fischer (1994) argue that the money available for spending affects consumption demand, not just the level of output (GDP). Glytsos (2005) finds that a country’s disposable income (GDP plus remittances) has a positive effect on consumption. Thus, remittances may affect the demand for the goods produced by Transnational Corporations’ (TNC) subsidiaries in the host country.Footnote 3 Among the determinants of FDI, market size has been found to have positive effects on FDI (Bajo-Rubio and Sosvilla-Rivero 1994; Love and Lage-Hidalgo 2000; Filippaios et al. 2003; Lall et al. 2003). Given the importance of FDI and remittances to SSA (Fig. 1), it is important to explain whether FDI inflows to SSA are affected by remittances and market size. This paper assesses the effect of remittances through per capita GDP on net FDI inflows to SSA. It also compares SSA’s performance in attracting FDI with other developing regions. To our knowledge, there are no prior studies focusing on this issue for SSA. This study covers the period 1981–2014 and uses an unbalanced panel data set for 85 developing countries. The model derives a TNC’s optimal level of capital at the foreign plant based on the cost minimization approach developed by Bajo-Rubio and Sosvilla-Rivero (1994). The results show that the growth rate of the host country’s GDP is an important determinant of FDI. The main finding is that there is a threshold of host country per capita GDP that determines the positive effect of remittances on FDI. Given the threshold, an increase in remittances by one standard deviation increases FDI inflows by 0.09% a year. Remittances have a positive effect on FDI in 43 countries in our sample, of which eight of them are in SSA. In addition, a SSA country receives about 0.8% more FDI than the average country in ASIA, but there is no difference between SSA and LAC, and SSA and the Middle East and North Africa (MENA). The rest of the paper is organized as follows. The next section reviews the relevant literature, followed by the methodology and data section. The following section discusses the results. The last section presents conclusions and suggestions for further research.",2
44,2,Journal of Economics and Finance,15 June 2019,https://link.springer.com/article/10.1007/s12197-019-09485-5,An optimization model of retiree decisions under recursive utility with housing,April 2020,Asiye Aydilek,Harun Aydilek,,Female,Male,Unknown,Mix,,
44,2,Journal of Economics and Finance,03 July 2019,https://link.springer.com/article/10.1007/s12197-019-09488-2,Banking Competition and Bank Size: Some Evidence from Italy,April 2020,Paolo Coccorese,Laura Santucci,,Male,Female,Unknown,Mix,,
44,2,Journal of Economics and Finance,17 July 2019,https://link.springer.com/article/10.1007/s12197-019-09489-1,Is the flow-performance relationship really convex? - The impact of data treatment and model specification,April 2020,Alexander Schiller,René-Ojas Woltering,Steffen Sebastian,Male,Unknown,Male,Male,"The mutual fund literature provides strong evidence that investors are highly sensitive to past fund performance. However, most studies also find that investors tend to disproportionately chase top-performing funds, while showing little to no fund flow sensitivity to poorly performing funds (Ippolito 1992; Sirri and Tufano 1998; Berk and Green 2004; Del Guercio and Tkac 2002; Huang et al. 2007). This phenomenon has been dubbed the convexity of the flow-performance relationship. The flow-performance relationship has important implications for fund managers and asset management firms, because their fee income is usually tied to the amount of assets they have under management. Berk and Green (2004) even argue that the flow-performance relationship directly determines the degree to which fund volume is affected by past performance. According to Chevalier and Ellison (1997), a convex flow-performance relationship leads to an agency conflict between fund managers and investors. This is because managers are incentivized to manipulate risk to increase the probability of high returns and high inflows, while investors want managers to maximize risk-adjusted returns. Consistent with this “moral hazard” effect, Brown et al. (1996) empirically demonstrate that mid-year “loser” funds tend to increase fund volatility in the latter part of the annual assessment period. The authors also show that this effect increases over their sample period, during which industry growth and investor awareness of fund performance increased. These studies have some commonalities. First, most had to approximate fund flows using fund size and returns, because reported fund flow data were not accessible. Approximated flows rely on the assumption that all flows occur at the end of the month, and that any dividends or distributions are reinvested (Chevalier and Ellison 1997). Both assumptions are somewhat rigid, and raise the question of whether the usage of approximated fund flows may lead to biased regression results. Since 1996, the Securities and Exchange Commission (SEC) has required funds to report their exact U.S. dollar fund flow. However, although exact fund flows are definitely preferable to approximated flows, both are highly sensitive to data entry issues, which may lead to strong outliers and thus biases in the econometric estimation. This leads us to the issue of outlier treatment, which is the second potential problem these studies have in common. In order to avoid the regression results being biased by outliers in the sample, most papers winsorize their fund flows, i.e., they replace extreme outliers (positive or negative) with more plausible values, often the 1% to 99% values of the distribution. However, if the erroneousness is due to data entry problems, this approach may lead to serious biases. In this paper, we explain why extreme values should be trimmed instead (i.e., removed from the sample), and not winsorized. Finally, the majority of past fund flow studies have neglected the potential persistence of fund flows in their empirical models. More recent studies argue that investors tend to react to information at somewhat different time intervals (Del Guercio and Tkac 2002; Cashman et al. 2014). Therefore, a control for persistence appears to be essential. In this paper, we reexamine the flow-performance relationship of mutual funds by using reported instead of approximated flows and trimmed instead of winsorized extreme values. We also control for potential persistence of fund flows. Our empirical study is based on monthly data of U.S. mutual funds from 1999 through 2014. Our key finding is that the winsorization of outliers leads to serious biases. Investors only appear insensitive to poor past performance when outliers are winsorized. When they are trimmed instead, we find evidence of a significant investor reaction to poor past performance. The reaction becomes even more obvious when the regression model controls for the persistence of fund flows. Interestingly, even with winsorized fund flows, we find some evidence of investor reaction to poor performance if we control for fund flow persistence. The use of lagged fund flows therefore appears to counteract potential biases from the inappropriate use of winsorization. Moreover, we find that the reaction of investors to superior performance is slightly overestimated if approximated instead of exact flows are used. Our results challenge the convexity of the flow-performance relationship because we provide evidence that investors are sensitive to both poor and good past performance. Our findings have important implications for the potential moral hazard of fund managers. The lack of convexity implies that investors are withdrawing money for poor-performing funds. Contrary to previous research, fund managers and asset management companies should not be motivated to increase risk in order to benefit from potential strong performance. Our findings reveal that, ultimately, they would be penalized symmetrically for any underperformance that may accompany this risk-taking behavior. Our paper contributes to a growing strand of literature that challenges the convexity of the flow-performance relationship. Using market share-adjusted fund flows, Spiegel and Zhang (2013) find evidence that investors do flee poor performance. Furthermore, Clifford et al. (2014) document a linear flow-performance relationship for the subsample of the largest mutual funds. Interestingly, both findings rely on modifications of the original research setting. The authors of both papers highlight that their studies should not be taken as a critique of established articles for that reason. It is also important to note that both papers document evidence against a linear flow-performance relationship by focusing on an alternate setting compared to classical studies of the flow performance relationship. Our article is the first to demonstrate a linear flow-performance relationship in a classical setting. The remainder of this paper is organized as follows. Section 2 describes our data and variables and provides some descriptive statistics. Section 3 provides the regression results for the flow-performance relationship using our suggested approach. Section 4 concludes.",7
44,2,Journal of Economics and Finance,09 August 2019,https://link.springer.com/article/10.1007/s12197-019-09492-6,The intra-industry effects of proxy contests,April 2020,Fang Chen,Jian Huang,Han Yu,,,,Mix,,
44,2,Journal of Economics and Finance,11 September 2019,https://link.springer.com/article/10.1007/s12197-019-09496-2,Do directional predictions of US gasoline prices reveal asymmetries?,April 2020,Hamid Baghestani,Jorg Bley,,Male,Male,Unknown,Male,"There is a common belief that gasoline prices increase rapidly in response to a rise in crude oil prices, but decline rather slowly when crude oil prices fall. This belief is referred to as the “rockets and feathers” effect in the literature (Bacon 1991). Numerous studies have investigated the “rockets and feathers” effect with mixed results.Footnote 1 One of the often-cited reasons for mixed results is the parametric modeling approach. For instance, a common approach in detecting gasoline price asymmetry is the use of various forms of error-correction models (ECM), which combine the long-run equilibrium relation with the short-run adjustment process. Within such models, the “rockets and feathers” effect means that gasoline prices adjust at a faster (slower) speed to eliminate short-run deviations from the long-run equilibrium relation when oil prices are rising (falling). Balke et al. (1998) show that, unlike levels specifications, ECMs produces strong evidence in favor of the “rockets and feathers” effect. Grasso and Manera (2007) examine how the choice of different ECM specifications influences the findings. A more recent study by Martín-Moreno et al. (2019) uses instead a Markov-switching model in order to contrast the robustness of the results obtained with the threshold autoregressive (TAR)-ECM methodology. We differ from existing research by taking a directional forecasting approach to re-examine the possible “rockets and feathers” effect, which, to the best of our knowledge, has not been done before. Within this approach, we maintain that, the “rockets and feathers” effect means that oil prices have directional predictive power for gasoline prices but in an asymmetric fashion. That is, upward moves in oil prices accurately predict upward moves in gasoline prices, but downward moves in oil prices may not as accurately predict downward moves in gasoline prices. Our analysis uses monthly data and focuses on 1986–2018 and two distinct sub-periods of 1986–1999 and 2000–2018. The former (latter) sub-period is characterized by relatively low (high) oil and gasoline prices. Our findings indicate that, for both sub-periods, changes in oil prices accurately predict directional change in gasoline prices up to three months ahead. Further results for 1986–1999 do not support the “rockets and feathers” effect, since the upward and downward moves in oil prices predict the upward and downward moves in gasoline prices with a similar accuracy rate. The results for 2000–2018, however, support the “rockets and feathers” effect, since the upward (downward) moves in oil prices do (do not) accurately predict the upward (downward) moves in gasoline prices. There are three noteworthy aspects of this study. First, we take an out-of-sample directional forecasting approach and thus differ from many other studies, which rely on in-sample results for inferences. In-sample analysis is often augmented with out-of-sample forecasting for providing decisive answers (Salisu et al. 2019). As such, we maintain that our forecasting approach significantly contributes to the investigation of gasoline price asymmetry.Footnote 2 Second, the choice of various parametric model specifications is commonly cited as one of the reasons for the mixed results (see, among others, Grasso and Manera 2007; Ederington et al. 2018; Martín-Moreno et al. 2019). This is not a concern here, since we take a directional forecasting approach in our investigation. Third, directional forecasting provides useful insights about future directional changes in gasoline prices, which, in particular, are important to commodity market traders. For instance, in evaluating forecasts, directional accuracy is more important in maximizing expected economic profits than, say, the minimum mean square forecast error (Leitch and Tanner 1991). In addition, trading behavior is often driven by strategic incentives, and a profit-maximizing trader may prefer a directionally accurate prediction that involves asymmetric rather than symmetric loss (Cohen et al. 2015; Baghestani and Toledo 2017). The format of this study is as follows: Section 2 reviews the related literature. Section 3 describes both the data and the random walk forecasts of gasoline prices. Section 4 presents the empirical results. Section 5 concludes.",4
44,2,Journal of Economics and Finance,18 April 2019,https://link.springer.com/article/10.1007/s12197-019-09478-4,Corporate governance and employee treatment: evidence from takeover defenses,April 2020,Omer Unsal,Blake Rayfield,,Male,,Unknown,Mix,,
44,2,Journal of Economics and Finance,26 June 2019,https://link.springer.com/article/10.1007/s12197-019-09486-4,Correction to: Corporate governance and employee treatment: Evidence from takeover defenses,April 2020,Omer Unsal,Blake Rayfield,,Male,,Unknown,Mix,,
44,3,Journal of Economics and Finance,15 July 2019,https://link.springer.com/article/10.1007/s12197-019-09490-8,Risk factors explaining returns anomaly in emerging market banks – study on Indian banking system,July 2020,Sabyasachi Mohapatra,Arun Kumar Misra,Marimuthu Murali Kannan,Unknown,Male,Male,Male,"Financial Institutions like banks are often treated as outliers during the cross-sectional asset pricing studies owing to their higher leverage and stricter industry regulation when compared to other sectors. Even one of the most celebrated and influential study by Fama and French (1992 and 1993) excluded financial institutions and banks from their data sample. The 3-Factor model by Fama and French (1992, 1993) critically challenged and highlighted the insufficiency of the one-factor Capital Asset Pricing Model (CAPM) designed by Sharpe (1964) and Lintner (1965) while explaining the abnormal returns in case of a sample comprising of non-financial firms. The reason Fama and French felt the necessity to leave out non-financial firms in their analysis was due to the high degree of leverage more often than not associated in case of financial firms. While high degree of leverage is a normal observable phenomenon for financial firms, it indicates financial distress for firms belonging to other businesses. Citing the empirical validity of their results, Fama and French proposed that firm specific factors like book-to-market ratio and the firm’s size are the foremost conditions for explaining the returns. However, in contrast, for the sample constituting the non-financial firms, ‘beta’, which is considered to be a proxy for these market related factors fall short of elucidating the variations in cross-sectional returns. The divergent results led researchers to conduct frequent studies seeking explanations. While one set of researchers felt the phenomena could be arising due to the data set abnormality or the study period considered, hinting at the possibility of data snooping and survivorship bias, many others established the existence of a strong relationship between the 3-Factor model and the cross-sectional returns for both US and non-US equities. However, we find the results to be inconsistent corresponding to the literature focusing on banking sector. While Bessler et al. (2008) document firm size and book-to-market ratio as significant factors in their findings, Cooper et al. (2003) reported that future stock returns are a function of past performance. Moreover, previous studies focusing on banking sector explored the linkage between bank-specific fundamental variables and the cross-section of expected bank returns. The bank-specific variables usually considered in these studies were net loans to total assets ratios, equity to total assets ratios and ratio of non-performing loans to total assets reflecting the banks’ capital and income structures. Nevertheless, no consensus was reached on the empirical methodology and findings based on these variables (Thakor 1987; Grammatikos and Saunders 1990; Madura and Zarruk 1992; Kim and Santomero 1993; Docking et al. 1997). In particular, studies related to emerging market bank returns and the associated risk factors have been very limited. Studies before 1990s focused on interrelationship between debt crises and bank stock returns (Cornell and Shapiro 1986; Smirlock and Kaufold 1987 on the 1982 Mexican Debt Crisis, and Musumeci and Sinkey Jr 1990 on the 1987 Brazilian Crisis). During recent years Girard et al. (2010) analyzed the impact of both country risk factors as well as fundamental factors on the emerging markets bank returns. However, their study excluded the Asian Banks’ special characteristics and current changes owing to Global Financial Crisis period. Asian Banks have distinctive features. They are highly dependent on domestic market; their major source of income is Net Interest Income and; they are susceptible to external shocks (Mohanty and Turner 2010). Further, unlike other emerging market banks, following the 1997 Asian Currency Crisis, Asian banks are now become better capitalized and more effective in managing their credit crisis. Owing to these favourable macroeconomic conditions and improvement in bank-specific fundamentals, it becomes imperative to empirically design an asset pricing model directed at exploring the statistical relevance of the bank-specific variables and associated risk factors in explaining the cross-sectional returns of bank stocks for one of the growing emerging markets like India In the current context, we feel a strong urgency to spot the general risk factors amongst financial firms, specifically while explaining the pricing and returns for Banking stocks. The current study gains further relevance when we focus on an Emerging Market economy like India, where the financial sector constitutes close to 40% of the benchmark indices and the sector’s growth contribute significantly to the future economic development (Cole et al. 2008). Moreover, the introduction of the economic reforms and the follow-up market liberalisation led to far-reaching deregulation of the Indian banks. This unprecedented measure allowed the firms to have better control over their respective assets and liabilities, and in turn their risk-taking behaviour. Further, the continuous discussion involving the adoption of Basel III capital requirements stresses on the classification of the risk-weighted assets as the foremost regulatory contrivance. Thus, in the present regulatory set-up, the study becomes even more relevant while making an attempt to understand and explain the risk factors priced in the security prices of these banking stocks. Our study makes an attempt to bridge the gap in the active literature by presenting empirical evidence with respect to the bank specific factors like profitability, asset quality, capacity to leverage, and operating margin along with strength of loss provision for pricing the banking stocks utilizing the data available for the 35 Indian Banks over the period 2008–2018. We design and test multifactor model and observe that market and bank specific anomalies along with banks’ risk factors are priced in bank stock returns. Our results pertaining to the alternate models testing indicate that by the inclusion of risk-based factors like the banks’ asset quality, profitability metrics, adequacy ratio and operational efficiency alongside the Fama and French (1992 and 1993) firm-specific factors improve the explanation of returns anomaly in case of banking stocks. In Section 2, we briefly review the related literature. We discuss the methodology design and the data employed during the study followed by the empirical findings in Section 3 and 4 respectively. We conclude the paper by citing the policy related implications in Section 5.",1
44,3,Journal of Economics and Finance,31 August 2019,https://link.springer.com/article/10.1007/s12197-019-09491-7,Firm size proxies and the value relevance of predictive stock return models,July 2020,Gulraze Wakil,,,Unknown,Unknown,Unknown,Unknown,,
44,3,Journal of Economics and Finance,03 September 2019,https://link.springer.com/article/10.1007/s12197-019-09494-4,Stock returns and investor sentiment: textual analysis and social media,July 2020,Zachary McGurk,Adam Nowak,Joshua C. Hall,Male,Male,Male,Male,"As described in Malkiel and Fama (1970), the Efficient Market Hypothesis (EMH) predicts asset prices fully reflect all available information. Rational investors in response choose asset portfolios which diversify away idiosyncratic risk. As such asset prices are only a function of market fundamentals. When asset prices are mispriced through the actions of irrational investors, rational investors are able to use arbitrage to correct asset prices. In contrast to the EMH, behavioral finance theory suggests that the feelings of irrational investors (Investor Sentiment) drive a portion of asset prices. Due to the specific characteristics of some assets (small, hard to value, limited information, etc.), arbitrage by rational investors becomes costly and asset prices are perpetually mispriced.Footnote 1 Recent empirical studies have found Investor Sentiment to be related to stock returns.Footnote 2 While the empirical finance literature has found Investor Sentiment to be a valid predictor of the cross section and time series of stock returns, studies differ how the Investor Sentiment measure is estimated. As noted by Baker and Wurgler (2006, 2007) Investor Sentiment is difficult to directly measure. As a result, the literature has relied on proxies developed from market/investor surveys, data mining methods, and textual analysis from annual reports, commercial media, and social media. Due to data limitations, the market/investor survey and data mining methods literature focus on the impact of investor sentiment on returns over monthly or larger time horizons. While most of these studies show a relationship between asset returns and investor sentiment, these studies may not capture the full impact of investor sentiment. If asset markets are partially efficient (i.e. investor sentiment does not determine a portion of stocks), and information is randomly dispersed, then markets should be the least efficient in the very short run. Another critique of this literature is that these investor sentiment measures show overall market sentiment rather than asset specific sentiment. Baker and Wurgler (2006) discusses that due to imperfect information about smaller firms, any new information causes investors to engage in irrational speculative trading. Market sentiment may not necessarily capture this speculative feeling in smaller firms. While much of the textual analysis literature has been able to account for the preceding critiques, the estimation methods used may be not be able to fully capture investor sentiment. A portion of the previous literature has relied on dictionary based methods in determining Investor Sentiment (Loughran and McDonald 2011; Chen et al. 2014; Jiang et al. 2019). To estimate sentiment, these studies pre-define a dictionary of positive and negative finance words and determine overall investor sentiment as the net positive word counts. The limitation of this approach is that there may be important missing terms which show sentiment. This method also gives each word equal weight in determining sentiment and does not account for sentiment shown in multi-word phrases.Footnote 3 Other studies have utilized machine learning methods to estimate investor sentiment (Bartov et al. 2018; Ranco et al. 2015; Yang et al. 2015; Sun et al. 2016; Renault 2017; Behrendt and Schmidt 2018). These studies provide an improvement on dictionary-based methods as the created investor sentiment indexes allow for different weighting of textual terms. These papers focus on the extreme short run (5 to 30-minute intervals) impact of investor sentiment on returns and given data limitations are unable to create equity specific investor sentiment.Footnote 4 Given the limitations of the previous literature, we propose a new method for estimating Twitter based stock specific investor sentiment index utilizing as developed in Taddy (2013a). This method differs in that estimates of sentiment do not rely on a predefined dictionary, and individual words are not assumed to be related the same sentiment information. Further, given the data rich environment of Twitter, we are able to create equity specific investor sentiment indexes. In this method, a training set of posts by individual users on Twitter (tweets) are determined to either convey positive, neutral, or negative sentiment. These are then used to predict the sentiment information from all remaining tweets. For comparison, we also develop a dictionary based investor sentiment utilizing a similar method as Loughran and McDonald (2011). We further utilize our investor sentiment index to test the empirical validity of EMH and Behavioral Finance theories. We specifically determine the relationship between our investor sentiment measures (negative, neutral, and positive sentiment) and cross-section abnormal stock returns. For robustness, we test if this relationship is similar across firm size. Finally, we determine if investor sentiment is useful in forecasting abnormal returns at the market level and by firm size. The social media platform Twitter is used by over 320 million users who express opinions and thoughts on a number of different subject matters including equity prices.Footnote 5 Further, Twitter is unique in that an individual can reference specific stocks by affixing a ‘$’ before the stock symbol in a tweet. This allows all Twitter users to search for tweets discussing a particular stock. This allows researchers to collect tweets supplied by individuals specific to a stock. Anecdotal evidence has shown individual Twitter posts (tweets) to influence specific stock returns. On January 10, 2011, Business Insider reported hip hop artist, 50-Cent (Curtis Jackson), tweeted  HNHI is the stock symbol for TVG there launching 15 different products. they are no joke get in now. The article goes on to state (Weisenthal 2011, no page number):  In the three months to the end of September, the company was operating at a loss with cash of just $198,000 and a deficit of $3.3m. Then, on November 23, it said it would offer 180m shares to the public at a price of just 17 cents... trading under the stock name HNHI was worth just 4 cents each. Spurred by the tweet, the stock took off. It hit nearly 50 cents on Monday, before closing at 39 cents. By the end of the month, the stock was up to $1.68. This price increase was relatively short lived. In early May, 50-cent terminated his relationship with this company, and the stock dropped in value to $0.1. Overall, we find a relationship between abnormal stock returns and our estimated investor sentiment indexes. We find an increase in positive sentiment is related to an increase in abnormal returns while also finding that estimated negative estimated sentiment had a limited relationship with abnormal returns. These results are consistent across firm size. Using out-of-sample forecasting tests, we find investor sentiment is able to produce marginally more accurate forecasts compared to a constant only model. Gains in forecast accuracy, however, is limited to around one percent. Our results indicate that individuals on Twitter are relaying stale information as opposed to providing novel insights. The remainder of the paper proceeds as follows. Section 2 details the relevant literature on investor sentiment and textual analysis, Section 3 describes the methodology and data, Section 4 details the cross sectional analysis, Section 5 provides a discussion on forecasting method utilized and forecasting results, and Section 6 concludes.",33
44,3,Journal of Economics and Finance,15 October 2019,https://link.springer.com/article/10.1007/s12197-019-09495-3,U.S. Small Business Administration loans and U.S. state-level employment,July 2020,Paul E. Orzechowski,,,Male,Unknown,Unknown,Male,"For more than half a century, the U.S. Small Business Administration (SBA) has been managing two important flagship lending programs, known as the 7(a) and 504 Loan Programs, to assist small businesses, but few academic studies have evaluated the relationship of these programs to state employment. The only serious studies on SBA lending and its impact on the economy were conducted by Armstrong et al. (2014), Craig et al. (2007), Craig et al. (2008), and Brown and Earle (2017). Most of these studies, except Brown and Earle (2017), usually examined annual SBA lending in U.S. metropolitan statistical areas (MSA) and non-MSA areas with data before 2002. My study hopes to fill in the gap in the literature by examining state-level SBA lending over a longer period of time from 1990 until the end of 2013 by estimating the relationship between SBA lending and employment activity. By using a longer time period with quarterly data points that cover three recessions including the Great Recession of 2008–09, it is hoped that the quarterly frequencies will identify changes between employment and SBA lending that are not picked up in annual studies. The study was also extended to ascertain whether there is a difference in the relationship between SBA lending and employment among states according to personal income levels (i.e. high versus low personal income states). The motivation for the extended analysis comes from the findings of Craig et al. (2008), wherein lower-income areas benefited more from SBA lending than higher-income areas. At a minimum, the study hopes to confirm or refute the findings of the previous literature on the relationship between SBA lending and employment. The main questions of this study are: Does SBA lending have a statistically significant relationship with state-level employment? Are there differences between high- and low-income states and the relationship between SBA lending and employment? The remainder of this paper is organized as follows. The second section offers a background on the SBA, which is followed by a literature review in the third section. The empirical model, data, and hypothesis are described in the fourth section, which is followed by a section that discusses the results of the study. The last section concludes.",2
44,3,Journal of Economics and Finance,18 November 2019,https://link.springer.com/article/10.1007/s12197-019-09501-8,"Managerial optimism, CEO retention, and corporate performance: evidence from bankruptcy-filing firms",July 2020,Mao-Wei Hung,Wen-Hsin Tsai,,Unknown,,Unknown,Mix,,
44,3,Journal of Economics and Finance,19 November 2019,https://link.springer.com/article/10.1007/s12197-019-09499-z,Are earnings predictable?,July 2020,Shahram Amini,Vijay Singal,,Male,,Unknown,Mix,,
44,3,Journal of Economics and Finance,02 December 2019,https://link.springer.com/article/10.1007/s12197-019-09502-7,Long-run memory in ethical and conventional investments. Novel evidence from a VAR(1)-FIEGARCH model,July 2020,Athanasios Koulakiotis,Vassilios Babalos,Maria I. Kyriakou,Male,Male,Female,Mix,,
44,3,Journal of Economics and Finance,09 January 2020,https://link.springer.com/article/10.1007/s12197-019-09500-9,Real activities manipulation in stock-for-stock mergers,July 2020,Olukemi Fasipe,Huey-Lian Sun,,Unknown,Unknown,Unknown,Unknown,,
44,3,Journal of Economics and Finance,20 February 2020,https://link.springer.com/article/10.1007/s12197-020-09506-8,Market fragmentation and post-earnings announcement drift,July 2020,Justin Cox,,,Male,Unknown,Unknown,Male,"This paper investigates changes in market fragmentation around earnings announcements. First, the analysis examines if increases in informational asymmetries around an earnings announcement result in more dark or off-exchange trading. Second, the analysis determines if uncertainty and execution risks around an earnings announcement leads to market consolidation, resulting in less lit market fragmentation. Third, to the extent that market fragmentation changes around an earnings announcement, I analyze if the change in market fragmentation alters the price discovery mechanism around an earnings announcement, leading to more post-earnings announcement drift (PEAD). Thus, to the extent the analysis reveals that market fragmentation influences PEAD, then market structure innovations such as fragmented trading plays a role in the informational efficiency in the stock market. Dark trading or dark fragmentation refers to off-exchange trading that occurs on private exchanges, offering market participants both pre-trade transparency and lower execution cost. Lit fragmentation is the competition for order and trade flow across lit venues, which are quoting exchanges that display available liquidity to all market participants. As recent as July 2018, off-exchange trading accounts for roughly 35% of all executed trades while no single lit trading venue accounts more than 25% of the market share for executed trades (Fidessa 2018).Footnote 1 Due to the growth in market fragmentation, recent research analyzes which venues are preferred by investors around market-related events. Menkveld et al. (2017) suggest that U.S. equity markets follow a pecking order in which investors sort through trading venues via both execution costs and immediacy. Menkveld et al.’s pecking order hypothesis posits that investors prefer to transact in high-cost, high-immediacy venues (e.g., lit venues) over low-cost, low-immediacy venues (e.g., dark venues) around informational-related shocks (i.e., VIX shocks, macroeconomic news, and earnings surprises). Furthermore, Menkveld et al. document that midpoint (non-midpoint) dark pool market share decreases (increases) following an informational shock. Their findings are notable as previous studies (Ye 2011; Fleming and Nguyen 2013Bloomfield et al. 2015; and Foley and Putniņš 2016) argue that off-exchange trading venues may attract a larger market share during uncertain market conditions since trading in dark venues allows investors to potentially conceal information. While Menkveld et al. (2017) address market venue preference around earnings surprises they do not address whether changes in market venue preference hinders price discovery around an earnings announcement. This paper contributes to research by addressing whether changes in market fragmentation influence the price discovery mechanism around earnings surprises – resulting in greater post-earnings announcement drift (PEAD). Further, this study extends Menkveld et al. by examining whether lit markets fragment since the venue pecking order hypothesis posits that traders will prefer execution immediacy over cheaper execution costs. To the extent that a fragmented market presents investors with more search frictions in locating both the best price and lowest execution cost trading venue, theory posits that fragmentation across lit trading venues to change around earnings announcement. To motivate the empirical relation between market fragmentation and price discovery around earnings announcements, this paper follows theoretical literature that suggests dark trading may impact the level of informational efficiency and price discovery on the lit venue. First, Ye (2011, 2016) argues that increases in information risk leads to more informed traders to transact off-exchange. The increase in informed trading in off-exchange, dark venues leads to more uniformed trading in the lit venue and results in less informational efficiency and price discovery available in the lit venue. However, Zhu (2014) offers a counterargument that suggests that dark trading venues are more attractive to uninformed traders since informed traders tend to cluster on one side of the market in the dark venue, resulting in lower execution probabilities. Zhu theorizes that the dark venue traps mostly uninformed traders and leads to an increase in competition between informed traders on the lit venue. This increase in competition among informed traders in the lit venue improves both the informational efficiency and price discovery in the lit venue.Footnote 2 Thus, this paper examines if dark fragmentation affects price discovery around earnings announcements by analyzing how dark fragmentation contributes to post-earnings announcement drift (PEAD). PEAD is the tendency for the returns of a stock to persist or “drift” after an earnings announcement in the direction of an earnings surprise, either negative or positive (Ball and Brown 1968; Bernard and Thomas 1989). To the extent that market fragmentation improves the price discovery channel in the lit venue, theory posits that market fragmentation may reduce the level of PEAD. Using nearly 50,000 quarterly earnings announcements between 2012 and 2016, my analysis outlines the following results. The analysis reveals that overall dark fragmentation increases around earnings announcements, a finding which differs from the venue pecking order hypothesis in which traders prefer lit venues over dark venues. Moreover, the analysis discovers that executed lit trading volume fragments around earnings announcements. This finding demonstrates that multiple lit trading venues experience increases in market share as the primary trading venue is unable to service the order flow of immediacy-demanding investors – inconsistent with the venue pecking order hypothesis proposed by Menkveld et al. (2017). Analyzing the impact of dark fragmentation on the price discovery mechanism around earnings announcements, I find that dark fragmentation does not impede price discovery for positive earnings surprises. Sorting on the level of dark fragmentation and earnings surprise, the analysis shows that post-earnings cumulative abnormal returns (CARs) around positive earnings surprise announcements are lowest in the quintile with the highest level of dark fragmentation. This finding implies that increases in dark fragmentation around a positive earnings surprise increase the price discovery mechanism on the lit venue and reduces PEAD. Further, these results are consistent with Zhu (2014), who argues that price discovery on the lit exchange improves alongside the dark venue since uninformed traders migrate to the dark venue, while informed traders are forced to transact in the lit exchange, resulting in both higher informational efficiency and price discovery. While the results indicate that higher levels of dark fragmentation reduce post-earnings cumulative abnormal returns for stocks experiencing a positive earnings surprise, I also document that greater dark fragmentation leads to more PEAD for stocks experiencing a negative earnings surprise. Conditioning on the highest negative earnings surprise quintile, the analysis shows that 60-day CARs are 4.24% lower for stocks in the highest quintile of dark fragmentation relative to stocks in the lowest quintile of dark fragmentation. Similar results hold for stocks in the other negative earnings surprise quintiles. This paper contributes to the literature in several ways. First, this study finds that both aggregate dark and lit trade volume fragmentation increases around earnings announcements, which is inconsistent with the venue pecking order hypothesis. Second, this paper shows that dark fragmentation reduces the level of PEAD for positive earnings surprises. This finding is consistent with the premise that trading in dark venues aids in the price discovery mechanism on the lit venue (Zhu 2014). Third, the analysis documents that higher levels of dark fragmentation impede the price discovery process for negative earnings surprises, resulting in more PEAD. This finding is consistent with previous studies (Chae 2005; Park and Lee 2014; and Baruch et al. 2017) who report that the composition of informed and uninformed alters around negative earnings surprises. In the context of this paper, the results appear to indicate that uninformed traders delay their participation around negative earnings surprises, resulting in more informed trading off-exchange. The resulting increase in off-exchange informed trading reduces the price discovery mechanism, leading to greater PEAD. Finally, the analysis shows that despite an increase in lit fragmentation around earnings announcements, lit fragmentation has no significant impact on PEAD. This study differs from previous efforts that explore dark fragmentation around earnings announcements.Footnote 3 Several studies (Gkougkousi and Landsman 2017; Balakrishnan and Taori 2017; and Pan 2017) analyze dark trading activity around earnings announcements, finding conflicting results as to the effects of dark trading on PEAD. These studies, however, are limited to weekly data provided by the Financial Industry Regulatory Authority (FINRA) whereas this study uses high frequency data from Daily Trades and Quotes (DTAQ).Footnote 4 Further, this study differs by measuring PEAD using 60-day CARs whereas the studies of Balakrishnan and Taori as well as Pan are limited in measuring PEAD using only five-day CARs. Finally, this paper differs from a recent study by Thomas et al. (2019). Their study uses the SEC’s tick size pilot in late 2016 as an exogenous event that altered dark trading levels to analyze the impact on price efficiency around earnings announcements. However, their study does not necessarily disentangle the confounding effects of a higher minimum price increment (i.e., tick size) as several studies (Zhao and Chung 2006; Chung and Hrazdil 2011; and Chordia et al. 2014) demonstrate that changes in the tick size, especially a larger tick size, increase arbitrage costs and hinders price efficiency.",4
44,3,Journal of Economics and Finance,07 March 2020,https://link.springer.com/article/10.1007/s12197-020-09507-7,Efficiency in the madness? examining the betting market for the ncaa men’s basketball tournament,July 2020,Daniel C. Hickman,,,Male,Unknown,Unknown,Male,"The National Collegiate Athletic Association (NCAA) men’s basketball tournament begins in March of each year and is held over three (extended) weekends. This event, often referred to as “March Madness”, generates a large amount of interest and revenue. Like other prominent sporting events, the tournament also leads to a significant betting market. According to a press release from the American Gaming Association (AGA), Americans were expected to wager $8.5 billion on the 2019 event, with 4.1 million individuals placing a bet legally, and another 7.6 million estimated to be placing bets illegally.Footnote 1 These numbers may increase even further in the near future following the expansion of legalized sports gambling in states other than Nevada. There have been numerous studies that use sports betting markets to test the efficient market hypothesis (Fama, 1970).Footnote 2 Previous research examines whether certain markets are efficient overall, as well as looking for situations in which market prices may be biased. As they tend to draw the most action, much research has examined betting in the major professional leagues: the National Football League (NFL) (e.g., Borghesi 2007, Dare and Holland 2004, Nichols 2014, Shank 2018), the National Basketball Association (NBA) (e.g., Colquitt et al. 2007, Gandar et al. 1998), Major League Baseball (MLB) (e.g., Woodland and Woodland 1994, 2003), and the National Hockey League (NHL) (e.g., Woodland and Woodland 2011, Paul and Weinbach 2012). The two main revenue-generating NCAA sports – football and men’s basketball - have also been analyzed.Footnote 3 This study adds to this body of research by focusing on the gambling market for the postseason NCAA men’s basketball tournament, specifically. This offers an opportunity to examine a market that differs in meaningful ways from the regular season betting markets. To begin with, this market involves a greater number of casual participants and occurs less frequently. While many studies have examined betting in the NFL, for example, data limitations prevent a rigorous empirical study of gambling efficiency on the Super Bowl, specifically. The volume of games in the NCAA tournament makes such an analysis possible. How might efficiency differ in a more concentrated, less frequent market? On the one hand, we might expect the market to be highly efficient, given the large volume of activity. On the other hand, there are more market participants wagering on an NCAA tournament game than a regular season college basketball game, and these additional participants may be less informed. Humphreys et al. (2013) examine the possibility that NCAA basketball bettors derive consumption benefits (rather than financial benefits) from betting, and find evidence to support the idea. For example, games with major television coverage attract a larger volume of bets. Even in the case of informed, financially-motivated bettors, the market for tournament games may be harder to price accurately because of the unique matchups and locations. Many games involve two teams that rarely (if ever) have played each other previously, and all games are played on a neutral court, which introduces uncertainty. Colquitt et al. (2007) study coaching changes in the NBA and show that the uncertainty following the change does cause inefficiency in market prices. One of the reasons for the high-level of interest in the tournament is the prominent theme of the “David vs. Goliath” matchups that pit perennially strong teams against schools with much less name recognition. In a 2019 first-round game, for example, the University of Virginia Cavaliers (UVA) faced the Gardner-Webb University Runnin’ Bulldogs (GW). According to data from the U.S. Department of Education, during the 2017–18 academic year UVA had men’s basketball expenditures of $10.5 million, while GW’s were $1.2 million.Footnote 4 UVA went on to win the game by 15 points, and then proceeded to win the tournament overall. However, in the 2018 tournament, UVA was eliminated in the opening round by the University of Maryland-Baltimore County Retrievers, a program with resources similar to that of GW ($1.8 million in men’s basketball expenditures in 2017–18). This unpredictability is what earns the tournament its “March Madness” moniker, and perhaps signals that the correct price in the betting market may be difficult to settle upon. Studies of college basketball betting that focus primarily on the regular season often examine a similar effect and look for whether bettors have biases towards favorites or longshots in certain matchups. Paul and Weinbach (2005) find that, while the market appears to be efficient overall, the underdog tends to win in the betting market more than would be expected when there is a large point spread. Wolfers (2006) also finds that large-betting-market favorites tend to win the game but lose more often than expected relative to the gambling line and posits that this provides evidence of potential corruption (“point-shaving”) in the sport. Berkowitz et al. (2017) study the money-line, in which a bettor wagers on simply who will win the game, rather than needing to win relative to the point spread. They find that bettors in this case are biased against betting on the favorite. However, once accounting for the transaction costs (or “vigorish”) charged to wager, the market still operates efficiently. In a subsequent study (Berkowitz et al. 2018) the authors show that a strategy of betting on the underdog with the point spread and the favorite on the money-line can be profitable. In a recent study, Stone and Arkes (2018) include some analysis of the betting market on the NCAA tournament as part of a study of the decision-making process of the tournament selection committee. While not the primary focus of the paper, they find evidence that bettors may be biased when evaluating team performance leading into the tournament. In particular, the betting market seems to undervalue games won in the conference tournaments leading to the main national tournament. However, the betting market tends to place too much value on winning these conference tournaments. To my knowledge, this study is the first comprehensive analysis of betting market efficiency in the NCAA tournament. In addition to examining the overall efficiency of the market, I investigate whether there is evidence of bettor bias with respect to two prominent pieces of information: tournament seeding and the conference affiliation of the participants. The evidence points to the market being largely efficient, though there are some specific instances that are shown to win more than expected. A few of these situations are found to be profitable after accounting for the typical transaction costs of the market.",1
44,4,Journal of Economics and Finance,04 January 2020,https://link.springer.com/article/10.1007/s12197-019-09497-1,"Price Volatility, the Maturity Effect, and Global Oil Prices: Evidence from Chinese Commodity Futures Markets",October 2020,Jing Ao,Jihui Chen,,,Unknown,Unknown,Mix,,
44,4,Journal of Economics and Finance,22 January 2020,https://link.springer.com/article/10.1007/s12197-020-09505-9,Financial market risk and macroeconomic stability variables: dynamic interactions and feedback effects,October 2020,Agnieszka M. Chomicz-Grabowska,Lucjan T. Orlowski,,Female,Male,Unknown,Mix,,
44,4,Journal of Economics and Finance,14 April 2020,https://link.springer.com/article/10.1007/s12197-020-09508-6,Economic policy uncertainty spillover effects on sectoral equity returns of New Zealand,October 2020,Faruk Balli,Hatice O. Balli,Russell Gregory-Allen,Male,Female,Male,Mix,,
44,4,Journal of Economics and Finance,29 May 2020,https://link.springer.com/article/10.1007/s12197-020-09511-x,The U.S. term structure and return volatility in emerging stock markets,October 2020,Riza Demirer,Asli Yuksel,Aydin Yuksel,Male,Unknown,Male,Male,"Fueled by the liberalization of economies in developing regions and rise in the level of economic integration, cross-border capital flows have played an increasing role in driving return dynamics in emerging stock markets, often presenting challenges for policy makers in those nations (e.g. Henry 1998; Bekaert et al. 2002, among others). While one strand of the literature highlights the role of country-specific factors in attracting foreign flows (e.g. Chuhan et al. 1998), other studies point to the role of U.S. monetary policy as a major driver of capital flows into emerging economies (e.g. Taylor and Sarno 1997; De Vita and Kyaw 2008). Indeed, a growing number of studies argue the presence of a global financial cycle to describe patterns in global capital flows and prices across countries (Nier et al. 2014) for which the monetary policy by the U.S. Fed serves as a major driver (e.g. Passari and Rey 2015; Rey 2016, 2018). This evidence further supports the earlier findings that the Fed’s interest rate policies can be a determinant of financial conditions in foreign markets (Miranda-Agrippino and Rey 2015; Bruno and Shin 2015). Given the role of U.S. monetary policy as a major driver of global credit growth and financial cycles in capital flows (Miranda-Agrippino and Rey 2015) and the evidence of a significant U.S. monetary policy effect on financial and economic conditions in emerging markets (e.g. Anaya et al. 2017), a natural research question then is whether U.S. monetary policy can also serve as a driver of volatility in developing stock markets that are often exposed to large fluctuations due to hot money flows in and out of their financial markets. The goal of this paper is to contribute to the discussion on the effect of U.S. monetary policy on developing economies by examining the predictive power of the U.S. term structure over return volatility in emerging stock markets. Clearly, the analysis is of high interest to policy makers as they can provide a guideline to help devise effective policies in response to monetary policy signals from the U.S. The results can also provide valuable insight for investors in their market timing decisions to shift funds in and out of the stock market and to effectively manage stock market risk exposures via portfolio allocations based on interest rate signals from the Fed. More importantly, the findings can provide insight for domestic firms in developing markets in their hedging strategies to manage currency and stock market exposures that could help lower their cost of capital estimates. One of the novelties of our study is to distinguish between the two components of the U.S. Treasury yield curve that reflect future short-term rate expectations and a time-varying maturity premium, following Hamilton and Kim (2002). As Bernanke (2006) notes, the forward interest rates implicit in the term structure capture the market’s expectation of future short rates as well as compensation required by investors for holding longer-maturity instruments. Therefore, the predictive power of the yield curve over emerging stock markets can be driven by either one of these components (or both), depending on the country examined and accounting for this distinction could help enlarge our understanding of the effect the U.S. term structure over emerging financial markets. Thus, decomposing the term structure of U.S. Treasury bond yields into the expectations factor (EF) and the maturity premium (MP), we evaluate the informational content of each component over return volatility in a large number of emerging stock markets including Brazil, Chile, China, Colombia, Czechia, Greece, Hungary, India, Indonesia, Korea, Malaysia, Mexico, Peru, Philippines, Poland, Russia, South Africa, Taiwan, Thailand and Turkey. Our findings show that the U.S. term structure indeed contains predictive information over emerging stock market volatility, even after controlling for country specific factors. While we observe heterogeneous patterns across emerging markets in terms of their predictability due to the U.S. term structure, we find that the market’s expectation of the future path for short term rates, implied by the expectations factor, serves as a stronger predictor of stock market volatility compared to the maturity premium component of the yield spread. This finding for emerging markets supports earlier results in Hamilton and Kim (2002) that future expected changes in short-term rates possess greater predictive information over GDP growth in the U.S. than is captured by the term premium. Examining the possible effect of the 2007/2008 global financial crisis, we find that the U.S. term structure has gained further predictive value following the global financial crisis, particularly for the BRICS nations of China, Russia and S. Africa. This finding suggests that the short-term interest rate signals from the U.S. Treasury yield curve have captured greater informational value over stock return dynamics in emerging markets, possibly indicating concerns over global funding conditions (through the expectations component of the yield curve), which in turn affects fund flows in and out of these markets. Finally, we observe that the predictive power of the U.S. term structure is robust for most emerging markets, even after controlling for country-specific determinants of market volatility including turnover, market size, dividend yield and price-to-earnings ratio. Overall, our findings suggest that policymakers and investors can utilize interest rate signals from the U.S. Treasury yields to make projections over stock market volatility in global stock markets, however, distinguishing between the two components of the yield curve could provide additional forecasting power depending on the country of focus. The rest of the paper is organized as follows. Section 2 provides a brief review of the literature on the role of the term structure as leading indicator of economic activity. Section 3 presents the data and methodology. We discuss the empirical findings in Section 4 and conclude the paper in Section 5.",2
44,4,Journal of Economics and Finance,30 May 2020,https://link.springer.com/article/10.1007/s12197-020-09512-w,The effect of oil price shocks on economic activity: a local projections approach,October 2020,Mirko Abbritti,Juan Equiza-Goñi,Tommaso Trani,Male,Male,Male,Male,"Oil price shocks have received considerable attention in the economic and financial literature for their impact on macroeconomic and financial variables.Footnote 1 Kilian (2009), Baumeister et al. (2010) and Kang and Ratti (2013), for instance, study the effect of oil shocks on real GDP growth and inflation distinguishing between supply or demand shocks. Other papers have explored the presence of non-linear effects; see, for example, Hamilton (2011), Herrera et al. (2011) or Kilian and Vigfusson (2011). Although most of these papers focus on the U.S., there are many studies that analyze the macroeconomic impact of oil shocks in other countries. Examples in this case are Cunado and Perez de Gracia (2003, 2005), Cunado et al. (2015) and Herrera et al. (2015). Nonetheless, this question still raises a lot of policy interest and research discussion as shown by Baumeister and Kilian (2016) or Baumeister and Hamilton (2019). This paper examines the impact of oil price shocks on four macroeconomic variables of the U.S. from January 1974 to August 2016 using local projections (LPs). We contribute by showing how responses of relevant macroeconomic variables (production, real interest rate, unemployment rate, and credit spreads) differ depending on the level of oil prices (already high, normal, or already low). In particular, four are the main contributions of the paper. First, our empirical strategy is based on the LPs approach initially proposed by Jordà (2005) rather than the vector autoregressions (VARs) used by previous literature. Contrary to VARs, LPs do not require the specification and estimation of the unknown data generating process for analyzing the impulse responses (IRs). Therefore, relative to VARs, LPs are less affected by the issue of misspecification, do not suffer from the curse of dimensionality, and allow for non-linearities very easily.Footnote 2 In a recent paper, Equiza-Goñi and Perez de Gracia (2019) examine the impact of oil prices on both aggregate and industry-specific U.S. real stock returns using local projections. Second, we introduce non-linearities in the computation of the IRs following an oil price shock. Earlier literature, such as Mork (1989), Lee et al. (1995) and Hamilton (1996), employed non-linear transformations of oil prices to examine the negative relationship between increases in oil prices and real economic activity. Our paper takes advantage of the recent LPs method to newly study non-linearities in this easily adaptable framework.Footnote 3 The source of non-linearity we consider is the state of the oil price before the shock, as in Equiza-Goñi and Perez de Gracia (2019). We find that the effect of the shock on macroeconomic variables is remarkably different depending on the state, being particularly severe when oil prices are already high. Third, our work updates the empirical analysis of the effect of oil price shocks on three U.S. real macroeconomic variables previously studied (i.e., unemployment rates, real interest rates and production). The theoretical nexus between oil prices and these macroeconomic variables studied in the literature is the following. Rational economic agents consider that an unexpected increase in oil prices should reduce aggregate supply as it increases the cost of production (Rotemberg and Woodford 1996) and causes a costly sectoral reallocation of labor and capital (Hamilton 1988; Davis and Haltiwanger 2001). Higher input costs and reallocation should reduce economic activity and business investment, possibly through a postponement of production, increasing unemployment rates. Obviously, this has an impact on interest rates, which are also influenced by monetary policy. In particular, central banks respond to the inflationary consequences of an increase in the oil price but without preventing the real interest rate to fall (which is what happened in the 1970s). This is because of the need to balance the fight against inflation with the adverse effects on economic activity, while gaining credibility (see, e.g., Clarida et al. 2000, and Blanchard and Galí 2007). Finally, in the fourth place, our paper brings a new variable into the analysis: credit spreads. This is of particular interest as researchers have become increasingly aware after the financial crisis of the impact of financial intermediation in standard macroeconomic variables (see, for instance, Gertler and Karadi 2011, or, Gilchrist and Zakrajsek 2012). Following the idea that the equity markets is a key determinant of both corporate spreads (Collin-Dufresne et al. 2001) and sovereign credit spreads (Longstaff et al. 2011), we can expect that a higher crude oil price tends to be associated with increased corporate spreads. In turn, since wider credit spreads are related with higher level of financial risk, the increase in the oil price can have a negative impact on real economic activity. The remainder of this paper is structured as follows. In Section 2, we describe the methodology used. In Section 3, we present the dataset and discuss the empirical results including some robustness exercise. Finally, Section 4 concludes.",6
44,4,Journal of Economics and Finance,08 June 2020,https://link.springer.com/article/10.1007/s12197-020-09513-9,Does what happen in Vegas stay in Vegas? Football gambling and stock market activity,October 2020,Justin Cox,Adam Schwartz,Robert Van Ness,Male,Male,Male,Male,"Research finds that individual investors are attracted to financial assets with lottery-like payoffs. Kumar (2009) provides an identification of lottery-like stocks such as those with high idiosyncratic skewness and idiosyncratic volatility, limited dividend payouts, and a lower share price. Other studies show that lottery-like stocks tend to have low average returns, high return volatility, and high share turnover.Footnote 1 Campbell et al. (2001) and Meng and Pantzalis (2018) show that lottery-like stocks are typically preferred by speculative investors such as retail investors. Recent evidence provided by Chen et al. (2018) suggests that when overall gambling sentiment is high, investor demand for lottery-like securities increases. In this study, we examine if gambling sentiment surrounding football betting is relate to trading activity in lottery-like stocks. Recent empirical studies (Dorn et al. 2014; Gao and Lin 2014) analyze if gambling attitudes and sentiment resonate in changes in stock market participation. These studies argue that attention associated with gambling-related events may encourage more retail investor participation in the stock market as sentiment from gambling may invoke a positive reaction to investing in the stock market. Hence, both positive sentiment and payoffs from gambling may lead to investor overconfidence in selecting stocks in the financial markets. Additionally, research suggests that increases in gambling sentiment and payoffs may cause investors to defer or completely substitute their participation in the stock market. In this scenario, both gambling and financial investing are considered substitutes rather than complements. Thaler and Johnson (1990) put forth two competing behavioral effects associated investor risk-tendencies following prior gains and losses. Following prior gains, they posit that investors exhibit “house money” behavior – becoming more risk-seeking in their investment allocations. Conversely, following prior losses, Thaler and Johnson posit that investors may exhibit “break-even” behavior, becoming more risk-seeking in selecting their investment opportunities. We posit that these two behavioral effects are consistent with our analysis of how gambling sentiment influences trading behavior in the stock market, particularly for lottery-like stocks. We find evidence that lagged football betting win-loss imbalances correspond with lower levels of retail stock trading and buying behavior, particularly in lottery-like stocks. We suggest there are two potential interpretations for this finding. First, our finding that higher lagged betting imbalances (i.e., winning) are associated with less retail activity indicates that the two markets have a relation, in that gambling sentiment results in less trading activity in the stock market. Second, the result that lower betting imbalances (i.e., losing) are associated with subsequent higher retail buy-sell imbalances indicates possible “break-even” behavior in which investors try to offset football gambling losses by increasing their participation and buying preferences for lottery-like stocks. We extend our analysis to include over-the-counter (OTC) stocks since OTC stocks are typically traded by retail investors due to their lottery-like payoffs. Consistent with our earlier findings, we demonstrate that OTC trading is higher following lagged low betting imbalances. Figure 1 provides an early view of our results. Figure 1 details that retail trading imbalances are higher across various stock types following weekends of low football betting imbalances (i.e., losing weekends). This indicates that retail investors, subject to changes in gambling sentiment, are more active in buying stocks in the stock market – likely trying to offset losses from the previous weekend. The relation between retail buy-sell imbalances and lagged football betting payoffs is almost monotonically decreasing as the level of football gambling bet imbalances increases. Overall, the visual in Fig. 1 implies a possible “breakeven” pattern between football gambling markets and equity markets, at least among retail investors. Retail investor buy-sell imbalances. This figure reports the average weekly retail trading imbalances in various stock types sorted on the level of lagged football betting imbalances. Low Bet B_I (High Bet B_I) indicates that the lagged weekly football bet imbalances are the lowest (highest) in the sample. A high (low) bet imbalance implies betters were net winners (losers) over the football gambling weekend. Weekly bet imbalances are computed by taking the average bet imbalance across all games, NCAA and NFL. Stocks are classified as lottery-like, non-lottery-like, and others depending characteristics based idiosyncratic volatility, idiosyncratic skewness, and stock price We also address whether lagged stock market outcomes result in changes in weekend betting activity. We find mixed evidence that lagged retail trading behavior as well as lagged returns influence the level of subsequent football bets. Finally, we examine whether lagged OTC trading influences the level of football betting activity, finding a positive relation between lagged OTC trading and subsequent betting activity. This finding suggests that lottery-like stock markets such as OTC markets may have some spillover effects with gambling markets. Moreover, we also find that lagged OTC trading activity is associated with lower subsequent football win-loss betting imbalances. We suggest one explanation of this finding is that overinvesting and sentiment in the OTC market may result in overinvestment and noise in football betting the following weekend, resulting in lower bet imbalances. This study differs from previous research by exploring whether gambling sentiment associated with sport outcomes leads to changes in trading in lottery-like stocks. Kearney (2005) finds that household lottery spending is subsidized by a reduction in non-gambling expenditures. Cookson (2018) documents that a substitution effects exists between prized-link savings (PLS) accounts, which allocate interest distributions via lottery payments and household gambling. In a related study, Chen et al. (2018) show that when overall gambling sentiment as measured by internet search volume spikes, demand for lottery-like stocks increases. However, search volume spikes may not serve as a viable measure of sentiment as much as it proxies investor attention since Da et al. (2011) demonstrate that search frequency using SVI is simply a measure of investor attention since the SVI has little correlation with investor sentiment. Doran et al. (2012) document that New Year’s sentiment results in higher retail sentiment, leading to more trading in lottery-like stocks. Kumar et al. (2011) examine the propensity to gamble in the stock market conditioning on religious characteristics (i.e., Protestants vs. Catholics). Liao (2017) shows that sentiment induced by geographical proximity between investors and casinos results in increases in idiosyncratic portfolio risk. Consistent with other papers (Dorn et al. 2014; Gao and Lin 2014; and Kumar et al. 2016), our paper is similar in looking at ex-post reactions to gambling events. However, the studies of Dorn et al. (2014) as well as Gao and Lin (2014) are focusing on the role of investor attention rather than sentiment. For example, both studies look at the scale of lottery jackpots as a measure of investor attention and at best serves as a weak proxy of sentiment considering that 99.9% of participants would be classified as losers and thus lottery payoffs and sentiment are not distinctly proxied. Our paper is unique from the aforementioned studies because we examine an unstudied yet significant betting market that offers participants a higher likelihood of winning (as opposed to lotteries) and thus, we can better distinguish whether certain gambling events such as weekends with large gains (losses) are associated with changes in investor behavior (i.e., trading volume, buy-sell imbalances) in the stock market. Further, since we focus on both retail trading volume and buy-sell imbalances, we are likely focusing on the market participants who would are likely influenced by changes in sentiment (see Kumar and Lee, 2006). This study contributes to this stream of literature by examining the link between football gambling markets and stock markets and the potential spillover effects. Further, this study takes an additional step by analyzing whether gambling sentiment is induced by previous changes in lottery-like stock outcomes. Hence, by identifying the directional link between trading in lottery-like stocks and gambling sentiment in non-financial markets, we provide a unique contribution on how financial and gambling markets interact. We also contribute to the literature by showing how sports betting sentiment affects trading in lottery-like stocks. We confirm aspects of the “substitution” hypothesis in that gambling sentiment leads to decreases in trading in gambling, lottery-like securities. Alternatively, this finding implies that weeks associated with low gambling sentiment are associated with increases in retail trading and buying in lottery-like stocks – consistent with “break-even” behavior.",1
44,4,Journal of Economics and Finance,01 July 2020,https://link.springer.com/article/10.1007/s12197-020-09514-8,Efficient market hypothesis: a ruinous implication for Portugese stock market,October 2020,Farhang Niroomand,Massoud Metghalchi,Massomeh Hajilee,Unknown,Male,Unknown,Male,"Academicians in general and finance professors in particular believe in the Efficient Market Hypothesis (EMH) that asserts stock prices already reflect all information such as the history of past prices or trading volume. Therefore, the weak-form market efficiency hypothesizes that investors may not drive profits above a buy-and-hold strategy using any technical trading rule that depends on price and/or volume; implying that technical trading rules are useless. Advocates of EMH argue that the best strategy would be to buy the market index and keep it for the long term, or the famous buy and hold strategy (BH). In this paper, we will advocate two strategies for Portuguese investors that are superior to the BH strategy recommended by the EMH. The first strategy is based on Technical Analysis (TA), and the second is based on Gold Momentum Strategy (GMS) which is based on switching between the index and gold. Technical analysis is based on the idea that prices move in trends; according to Pring (1991, p. 3) TA is an art “to identify a trend reversal at a relatively early stage and ride on that trend until the weight of evidence shows or proves that the trend has reversed.” One of the most important indicators for trend identification is the moving average indicator. Our first strategy is based on some variations of moving average trend following techniques. Many technicians believe TA can help save an investor from extreme losses in the event of a severe bear market like the 2008 housing crisis or the 2020 Covid-19 crisis. For example, during the housing crisis of 2008, an investor would have lost 51.3% by following the BH strategy, however, this investor would have only lost 14.3% by following our recommended moving average rules. Section 2 provides a brief review of the literature. Section 3 describes the data and methodology. Empirical results are presented in Section 4. Trading strategies are discussed in Section 5. Section 6 provides concluding remarks.",5
44,4,Journal of Economics and Finance,03 July 2020,https://link.springer.com/article/10.1007/s12197-020-09516-6,Job Movement and Real Wage Flexibility in Eastern and Western Parts of Germany,October 2020,Fei Peng,Sajid Anwar,Lili Kang,,Male,Female,Mix,,
44,4,Journal of Economics and Finance,20 December 2019,https://link.springer.com/article/10.1007/s12197-019-09504-5,Manufacturing as a Growth Escalator in Low and Middle Income Countries,October 2020,Ummad Mazhar,Fahd Rehman,,Unknown,Male,Unknown,Male,"There is an emerging debate on the observed pattern of structural change in developing countries. The path followed by Europe, the US, and Japan is called manufacturing led economic development. The post-World War II successes of the East Asian economies also followed the same pattern. Does this historical pattern, with manufacturing possessing the key position in growth process, still hold the promise of prosperity for current developing countries? Contrary to the historical pattern, recently the pendulum of evidence seems to swing towards services as an engine of growth and structural change. Many practitioners point out the changing balance of sectoral shares in national products of developing countries to conclude that services offer greater potential for rapid economic growth. The modern knowledge based economies may depend on expanding services sector as a growth accelerator. The era of large scale industrialization under the patronage of state seems to be over or perhaps evolve into a new pattern in which the services sector takes the center stage. Using insights from the previous studies, this paper gathers favorable evidence for the hypothesis that manufacturing driven structural change increases per capita income growth while services sector slows it down. This evidence comes from a sample of 52 countries from 1990 to 2013. The writings of Adam Smith lends support to the evidence gathered here as he signifies the role of manufacturing in economic development. Smith’s notion of increasing returns made possible through specialization which means rising labor productivity and per capita income. Rising incomes increase the size of the market which, in turn, expands the market for industrial products, especially expansion in domestic manufacturing base reduces the burden on the external sector. On this basis, manufacturing plays a significant role in international trade and contributes to current account balance by improving terms of trade. Manufacturing is also considered an emblem of prosperity, therefore Felipe et al. (2014) state that most of the rich countries became prosperous owing to manufacturing employment. Furthermore, manufacturing plays a significant role in achieving successful industrialization which is manifested through a catch-up phenomenon. The countries usually achieve higher productivity growth when gap between a technology leader and a follower reduces. However, the pace of catch-up accelerated due to increased globalization, greater international technology transfer and ever-increasing advantages of backwardness (Naude and Szirmai 2012). The early industrializers follow a traditional pattern of structural change from agriculture to manufacturing and the rise of manufacturing precedes the rise of services (Naude and Szirmai 2012). However, the traditional pattern alters its course and a new pattern has emerged from agriculture to services for most of the late comer countries. Therefore, Naude and Szirmai (2012) highlight that statistics exaggerate the increase in the share of services in GDP on two grounds: outsourcing and price effects. Many manufacturing firms outsourced catering, consulting, warehousing and programming functions to service firms which are now counted in the services sector; price of services tended to increase more than that of manufacturing goods. There is a kind of complementarity between manufacturing and services and it becomes difficult to isolate the impact of manufacturing from services. For instance, manufacturing creates opportunities for wholesale, retail and transportation sub-sectors. On this basis, it is assumed that manufacturing generates positive externalities for services. In addition, manufacturing has backward linkages with the natural resource sector and forward linkages with the services sector and generates a large multiplier effect in an economy (Dadush 2015). Therefore, it is estimated that the US manufacturing sector has a higher multiplier effect than the other sectors; on value added basis, one dollar in manufacturing generates 1.4 dollars in other sectors (WEF 2012). Since more than half of the economic activity is performed in developing economies, the sustainability of their contribution calls for investigating their escalators of growth. Most of the manufacturing activities has been relocated to the developing countries since 1940s. As Naude and Szirmai (2012) show, the average share of manufacturing in developing countries’ GDP was 12.1% in 1950 and increased slightly to 15.2% in 2005, while the shares of advanced economies declined from 29% in 1950 to 16.1% in 2005. The stagnancy of average manufacturing shares reflects that services contribution to GDP has considerably increased in low and middle income countries with the exception of China where manufacturing to GDP ratio in 2012 is approximately the same as it was in 1990. The faster growth of services in the low and advanced income countries casts doubt about the historical significance of manufacturing as an engine of growth. The question among the development economists, professionals and practitioners is whether the escalator of growth is manufacturing or services. This paper attempts to address this question by collecting empirical evidence. The main contribution of the paper lies in its careful empirical estimates of the long term growth elasticities with respect to manufacturing and services for a diverse set of developing countries. The influences are explored both linearly and non-linearly and the possibility of endogeneity is also considered. The central result is a significant and positive (though inelastic) response of per capita growth to manufacturing share and a significant and negative (though inelastic) response of services share. However, considering the heterogeneous and representative nature of our sample which comprises __ according to UN World Economic Situation and Prospects (2014)Footnote 1 __ of 12 low income, 20 lower middle income, and 20 upper middle income countries ___ the findings are easily extendable to other contexts. The next section discusses the key theoretical insights from the literature. Section 3 describes data, model specification and discusses the results. Section 4 concludes with future directions.",4
44,4,Journal of Economics and Finance,21 August 2020,https://link.springer.com/article/10.1007/s12197-020-09517-5,"Gold market price spillover between COMEX, LBMA and SGE",October 2020,Xinyi Qian,,,Unknown,Unknown,Unknown,Unknown,,
45,1,Journal of Economics and Finance,22 August 2020,https://link.springer.com/article/10.1007/s12197-020-09522-8,EU accession: A boon or bane for corruption?,January 2021,Vincenzo Alfano,Salvatore Capasso,Rajeev K. Goel,Male,Male,Male,Male,"The formation of a trading and political block such as the European Union (EU) has had far-reaching ramifications for member nations (and their competitors); see Bickerton (2012). Many nations have joined the EU over time, with some others waiting to join (http://ec.europa.eu/). It may be argued that politicians in favour of joining the EU have espoused the economic benefits to motivate public opinion. However, some overall implications of joining, such as the impact on corruption, have not been analytically studied and are thus not very well understood. The composition of the EU has led to a formidable trading block with freedom of trade. An extensive literature studies the linkages between trade openness and corruption. Often, these studies identify a negative relationship between the degree of trade openness of a country and the level of corruption (Ades and Di Tella 1999; Krueger 1974; Larrain and Tavares 2007; Torrez 2002). Another common finding is that the size of the government is positively correlated with the level of corruption in a country (among others see Rose-Ackerman 1999; Svensson 2005). The explanation for these findings is intuitive: by nature, corruption captures economic rents, which are lower in the presence of greater trade openness. The notion is that trade restrictions shift resources from directly productive activities to rent-seeking activities, hence spurring corruption. For the same reason, the larger a government’s budget, the larger are the rent-seeking opportunities (i.e., when bribes are solicited either via the grant of favours or to reduce red tape, see Kaufmann and Wei 1999). The channels through which government resources can be exploited by corrupt bureaucrats depend on the nature and extent of regulations: in general, increased regulations heighten red tape and entail a rise in potential rent-seeking activities (see Guriev 2004; Shleifer and Vishny 1993, 2002). The aim of this work is to formally test the impact of EU entry and negotiation on the level of corruption. This is useful both to provide insights into whether one of the stated goals of the Union in terms of corruption-reduction has been accomplished, and to guide the decisions of governments and other stakeholders in deciding whether to join the Union and what policies to implement. For this reason, we believe it contributes to the literature on the effect of joining the EU, to the literature on determinants of corruption (see Dimant and Tosato 2018), and more generally to the effects of policy decisions on corruption. Given the complexity of the phenomenon, it is always difficult to find data to highlight the macroeconomic evidence on corruption. Indeed, the ceteris paribus condition, so needed in economic analyses, is hard to assume with regard to corruption, which is affected by so many factors. In this context, we believe that joining the European Union could be an interesting natural experiment to test the impact of the increase in both the number of regulations and the degree of free trade on the level of corruption. In fact, the countries that have joined the European Union have suddenly faced an increase in the number of regulations: European Union laws regulate several aspects of trade and commerce and to join the EU, the national governments enforced these regulations, often expanding the bureaucracy and government size. Moreover, the impacts of EU regulations on national laws and regulations are certainly not negligible, given that in member states they range between 6% and 84% of total regulations within a territory (Miller 2010). For instance, from 1994 to 2008 (the year it was repealed), a well-known EU regulation, (2257/94), set the minimum length a banana should have to be considered a first-class banana. A similar regulation was written on cucumbers, this time regulating their curvature. These examples suggest that a state joining the EU is going to impose a greater number of regulations, with implications for corruption and rent seeking. Viewed broadly, entry into the EU is likely to impact the quantity and the quality of government in a nation, with important consequences for corruption (La Porta et al. 1999; Rose-Ackerman 1999). The European Union itself declares that one of its main objectives is to fight corruption within its borders.Footnote 1 Starting with the European Council in Tampere in 1999, followed by The Hague programme in 2004, and finally, the Stockholm programme in 2010, the European Union institutions have been implementing different policies and programmes with the aim of reducing public (and private) corruption within the Union. Moreover, the empowering of competition and free trade that follows entry into the European Union should have a mitigating effect on corruption. Given the presumed linkages between the degree of free trade and the amount of bureaucracy and regulations on the one hand, and corruption on the other, a country joining the EU faces two opposite effects on its corruption. The first, e.g. the empowering of free trade and the opening of the markets, which is likely to start during the negotiation period before actually joining the EU, should have a negative impact on corruption; while the second, e.g. the increase in the amount of red tape that often follows the joining of the EU, should increase corruption via an increase in rent-seeking activities. Which of these opposite effects is stronger? Does EU membership have a positive or negative impact on corruption in member states? By focusing on the recent expansions that have taken place in the EU, this work’s main objective is to provide some answers to these questions and to shed new light on the effect of increased norms and regulations on a country’s corruption levels (see Tsanana et al. 2016). By exploiting insights from the literature on the determinants of corruption, our results clearly show that EU membership has not decreased the level of corruption in the last three rounds of joining countries (which are 13 of the 28 countries belonging to the Union, almost half of the total) and that soon after the countries had entered the single market, the level of corruption increased. This main and new finding is robust to alternative modelling formulations, albeit the fate of nations in the EU accession negotiations stage is different. The structure of the rest of the paper includes a literature review in the next section, followed by methodology and data, results, and conclusions.",3
45,1,Journal of Economics and Finance,18 August 2020,https://link.springer.com/article/10.1007/s12197-020-09518-4,On the performance of Bank-managed mutual funds: Canadian evidence,January 2021,Greg Hebb,,,Male,Unknown,Unknown,Male,"Mutual funds have experienced significant growth as a global investment tool over the last several decades. According to the Investment Company Institute, total net assets of all mutual funds grew globally from $US 9.6 trillion in 1998 to $US 46.7 trillion in 2018.Footnote 1 Similar growth has occurred in the Canadian mutual fund market with total net assets of $Cdn 398 billion in 2000 growing to $Cdn 1.4 trillion by the end of 2018.Footnote 2 Not surprisingly, commercial bank involvement in this market has also increased. Ferreira et al. (2018) find that approximately 40% of mutual funds globally are sponsored by commercial banks but this percentage varies significantly between countries. In the US, it is closer to 20% (Ferreira et al. 2018) and in France, it is over 80% (Dieu 2015). In Canada, the country of study in this paper, it is approximately 50% (O’Hara 2018). This range of market shares could have important implications for the behaviour and performance of bank-run funds. Although mutual funds were first introduced in Canada in 1932, the involvement of Canadian chartered banks in the mutual fund industry did not begin until the 1990s. Prior to that time, under Canadian banking laws, chartered banks were not allowed to participate in the securities industry in general and the mutual fund market specifically. This changed with revisions to the Bank Act in 1987 and 1992. By 2005, 31% of all mutual funds in Canada were managed by the Big Six banks and credit unions. This increased to 47.5% by 2015.Footnote 3 While much of this growth was internal, the banks have also been active in acquiring independent funds. For instance, National Bank acquired the Altamira family of funds in 2002, Royal Bank purchased PH&N funds in 2008, and Scotiabank bought DundeeWealth (Dynamic family of funds) in 2011. As a result, bank-managed funds are now in a majority position (47.5%) compared to independent funds (42%). The popular press has a generally negative perception of bank-managed mutual funds. “Bank-run mutual funds tend to perform poorly”,Footnote 4 “Bank-run funds are poor performers”,Footnote 5 and “All 29 funds run by banks fail to shine”Footnote 6 are just a few examples of headlines regarding bank-run mutual fund performance. Academic literature has generally found similar results. Frye (2001) studied the performance of US bonds funds and found that while performance of bank-run and independent funds was not significantly different, the bank-run funds were more conservative (less risky) in their investments. Dieu (2015) studies this issue using a sample of equity funds from France. In this market, banks dominate the mutual fund market. She finds that bank-run funds significantly underperform non-bank funds. Given the growth in both the mutual fund market and bank involvement in mutual funds, how bank-run mutual funds perform is an important question. Canada provides an interesting sample to test these issues for several reasons. First, Canadian banks have emerged over the last decade as some of the most respected financial institutions in the world. In 2017, the World Economic Forum ranked the Canadian banking system as the most sound in the world. This reputational factor may influence fund performance. Second, while banks are significant players in the Canadian mutual fund market, they do not dominate as they do in the French market, nor are they niche players as in the US. Previous studies of this topic have tended to focus on markets in which bank-run funds were either a very large or very small part of the overall mutual fund market. Studying this issue in a country where banks and non-banks have approximately equal market share, which removes the impact of market control on performance, may provide a purer measure of performance differences attributable to the fund manager. Finally, Canadian banks have been active acquirers of mutual fund companies over the last 20 years. This provides a unique opportunity to examine funds when they were independent and after they were acquired by a bank to determine what, if any, changes there have been to performance. This paper extends previous studies by examining the performance of bond, equity, and balanced funds – not just one type of fund. As discussed in Dieu (2015), the different investment opportunities for different classes of funds may cause performance differences. By examining all three major types of funds, the impact of class can be isolated. Our paper focuses on both risk and return differences of the mutual funds. Our model simultaneously allows for both risk and return differences between bank and non-bank funds. Finally, we use econometric techniques to account for the two-way clustering (both firm and time) of the data to better account for any possible dependencies in the data. Our results show that there are, in fact, differences in Canadian bank-managed mutual funds compared to non-bank-managed funds when properly controlling for double-clustering. These differences, however, tend to be in the risk of the portfolio rather than returns. They also depend on the type of fund and the time period examined. These results tend to contrast with those of other studies and thus suggest the need for further research on this topic. Section 2 of this paper examines past research on this issue. Section 3 provides the testable hypotheses. Section 4 describes the data and methodology for our study. Section 5 has our results, and conclusions are presented in Section 6.",
45,1,Journal of Economics and Finance,17 September 2020,https://link.springer.com/article/10.1007/s12197-020-09523-7,Efficiency on the dynamic adjustment path in a financial market,January 2021,Nasreen Nawaz,,,Female,Unknown,Unknown,Female,"Adam Smith’s invisible hand was a clever mechanism for describing how information, idiosyncratic and dispersed among individuals, is accumulated and combined by the market mechanism such that the overall market equilibrium is the same as that would be obtained by an all-knowing social planner. This result is the core principle of all modern Samuelsonion welfare economics. However, the existing literature inadequately addresses the efficiency issue on the dynamic adjustment path of the market after an economic shock before it arrives at a new efficient equilibrium, and rather an adjustment mechanism has never been defined clearly for a perfectly competitive market. There is an inherent limitation in the definition of a perfectly competitive market, which precludes the explanation of movement of the market price after an economic shock. In a perfectly competitive market, all producers are price takers and the prices are perfectly flexible, however, a central question to the discussion is: who changes the price after an economic shock to the market to lead to a new equilibrium? If no one changes the price, then supply will never equalize the demand after an economic shock, resulting in an infinite level of inefficiency. If the producers have to be assumed of changing the price when the market is out of equilibrium, it comes in direct conflict with the assumption of the producers being, “price takers.” No other market agent (responsible for the movement of prices) has been defined under the theoretical concept of a perfectly competitive market. In this scenario, it is impossible to provide a theoretical framework for some drifting mechansim for a perfectly competitive market from one equilibrium to the other, as the “price taking,” and the “price changing,” assumptions contradict each other. A producer cannot be a “price taker,” and a “price changer,” at the same time, however, with the only assumption of “price taking behavior,” the market will logically get stuck in an “out of equilibrium,” state for an infinite period of time leading to an infinite level of inefficiency, whereas, the assumption of a “price changer,” for a market agent (which is inevitable to explain the change in price from one equilibrium to the other) implies at least a minimal level of market power. This also implies that a perfectly competitive market has to incur some inefficiency after an economic shock, i.e., either it gets stuck in an “out of equilibrium,” state (under the agents’ price taking assumption) or a market agent changes the price to bring the new equilibrium, in which case, the transition cannot occur free of cost and has to come at the expense of some economic efficiency! The only way a perfectly competitive market can be efficient during the transition is that the economic shock itself instantly settles the price and quantity from one equilibrium to the other, i.e., the market jumps from one equilibrium to the other as a result of a shock through some magic wand. However, the basis of market mechanism is the self-interest by the economic agents, which acts as a driving force to push the market toward a new equilibrium. When the market is in equilibrium, demand equals supply, however, after an economic shock the market does not instantaneously settle at a new equilibrium, the market forces once again become operative to drive the market and the market follows some adjustment process to move to the new equilibrium. The demand does not equal the supply throughout the adjustment process and there is some efficiency loss before the market settles at an efficient equilibrium. An important question to consider is: Can the market mechanism itself minimize this loss or is there some other mechanism which may improve the efficiency during the adjustment process? In order to address this question, we first need to quantify the efficiency on the dynamic adjustment path, and then compare it with an ideal situation. Suppose there is only one producer in a market who produces a perishable good and sells it to the consumers living in a community. He sells a quantity exactly equal to the quantity he produces in each time period, and the market stays in equilibrium. If a demand shock decreases the demand of his product, some of his production will remain unsold and be wasted by the end of the time period in which the demand shock happened. Assuming that the producer can change the price and the production immediately, had he known the exact pattern of new demand, he would immediately pick the price and the quantity to maximize his profits and clear the market without wasting his production. However, he lacks this information, so he decreases the price based on his best guess about the new demand (based on the quantity of his unsold production), driving the market close to the new equilibrium. If in the next time period, his production is sold out, he will know that the new equilibrium has arrived, however, if a part of his production still remains unsold, he will reduce the price further (and production accordingly) to bring the market closer to the new equilibrium. The market will eventually settle at the new equilibrium after some efficiency loss. The resources wasted by the market mechanism are those which went into the unsold production in each time period during the adjustment process. In contrast, suppose that after a demand shock, a community coordinator conveys the information about the new demand at various prices to the producer. The producer would immediately pick the profit maximizing, market clearing price and quantity without an efficiency loss. The new equilibrium will be identical in both scenarios, however, coordination seems to improve upon the market mechanism during the adjustment process, through revealing the information of new demand to the producer, which is missing in the market where consumers only care about their self-interest without being bothered about the resources the producer will have to waste as unsold production on account of lack of this information. Therefore, coordination can improve upon the market mechanism in terms of efficiency during the adjustment process, even though the new equilibrium is going to be the same as that would be achieved by the market mechanism. In the above example, only one producer was assumed just to make the intuition simple and clear, however, the same reasoning goes through for a perfectly competitive market with a large number of buyers and suppliers. In a perfectly competitive market, the exact magnitude of an economic shock is not known to suppliers, and they also lack information on how the consumers are going to respond to a specific shock. They try to gauge the intensity of the shock through the increasing or decreasing size of their inventories and adjust their economic actions accordingly. However, if there is coordination among producers and the consumers, the producers have more information about the new economic scenario and the adjustment to the new equilibrium is smoother and quicker as compared to the adjustment in a free market. It would be too imaginative to believe that there can be such a perfect coordination among all buyers and suppliers which may result into a zero efficiency loss after an economic shock, however, coordination can certainly improve the efficiency during the adjustment process through mutual sharing of information among buyers and suppliers. There might arise issues of coordination costs, however, if the magnitude of shock is large enough and the adjustment process is associated with huge efficiency losses (which if saved could more than compensate the coordination costs) then coordination is superior to a free market, whereas if the magnitude of shock is too small and the coordination costs are comparatively too high, then free market might lead to a more efficient adjustment process. There could also be positive externalities of coordination. These issues need to be discussed separately in future research projects. The issue of economic efficiency has much been discussed in the previous literature. The Fundamental Theorems of Welfare Economics are generally stated in a form attributed to Arrow et al. (1952), Arrow and Debreu (1954) and Debreu (1959). Interpreting the conditions under which the First Theorem is true provides the basis of the Market Failure approach. Bator (1958) focuses on externalities, natural monopolies, and public goods. Akerlof (1970) explains the quality uncertainty as another source of market failure. Kaldor (1972) argues that the general economic equilibrium originally formulated by Walras and mathematically developed by Gerard Debreu is too over simplistic to depict the real world phenomena. Malkiel and Fama (1970) assembled a comprehensive review of the theory and evidence of financial market efficiency. Green (1977) discusses the non-existence of informational equilibria. Grossman and Stiglitz (1980) also model situations where a competitive equilibrium would not exist. Greenwald and Stiglitz (1986) argue that in general when risk markets are incomplete and information is imperfect, markets are not constrained Pareto optimal: the Invisible Hand does not work. In Stiglitz’s words (see Stiglitz (1991)): “The Welfare Theorems are just that: theorems, the conclusions of which follow inevitably from the assumptions. The research of the last two decades has not detected any major flaws of logic. The Theorems stand, as I have said, as one of the triumphs of modern mathematical economics. The question is not the logical status of these propositions, but their empirical relevance, the inferences which we make concerning how society should be organized and about the design of economic policy. These are, to be sure, matters of judgment. The earlier analyses of market failures basically agreed with the underlying conception of the market economy that was reflected in the assumptions of the Welfare Theorems. I am not so convinced.” Woodford (2002) mentions that improvements in information processing technology and deregulation, among other forces, are profoundly transforming the financial sector of the United States and other advanced economies. Many of these changes are likely to improve the efficiency of financial intermediation, in the sense that the dispersion of valuations of claims to future payments across different individuals and institutions is minimized. Ackerman, Nadal and Gallagher (2004) also call into question the assumptions in Arrow and Debreu’s analysis which make the Theorems of limited relevance to modern industrial economies. Cajueiro, Gogas and Tabak (2009) assess if the financial market liberalization introduced in the beginning of the 1990s in Greece has changed the degree of market development (efficiency) by studying time-varying global Hurst exponents. Their results suggest that changes in financial market liberalization have important positive implications on the degree of development of stock markets. There is also a huge literature discussing the efficiency of markets in the context of a society e.g., Arge and Hunt (1971) argue that for economic efficiency, what needs to be accomplished is a persuasion of the populace away from the doctrine of self-interest and toward a doctrine of cooperation and conservation. Sen (1993) argues that competitive market equilibria are weakly efficient in opportunity freedoms in terms of capabilities as well as commodity holdings. Oakley, Ashton et al. (1997) based on Titmus’ famous study on paid and unpaid blood donations (1971) explains the social repercussions of creating a market for blood, inviting an un-ending debate on the issue. Frey and Oberholzer-Gee (1997) develop the argument further to show how the market mechanism for certain entities undermines an individual’s sense of civic duty. Sunstein (1999) presents a new conception of the relationship between free markets and social justice. Folbre and Nelson (2000) elaborate that the market mechanism is unlikely to provide the quantum and quality of children, sick and elderly care. George (2004) challenges the view that free choice, as revealed by the preferences of individuals, maximizes the welfare of individuals given their income and market prices. Howell (2004) challenges the free market with regard to the labor markets around the world and persistent unemployment. Dolfsma, Finch and McMaster (2005) argue how the conceptualization of the market in relation to the society undermines the mainstream, Paretian perspective on welfare. Dolfsma (2005) highlights the importance of dynamic welfare. Satz (2010) delivers a comprehensive theory of the limits of the markets with regard to society. Hemsley-Brown (2011) portrays the challenges of a free market in higher education. As the major focus of this paper is the efficiency on the dynamic adjustment path, it will not be out of place to acknowledge the previous literature highlighting the importance of adjustment path to equilibrium. Diamond (1971) develops a model of price adjustment assuming that firms know the demands they face. Eden (1981) explains a theory of competitive price adjustment. Hahn (1984) explains why a theory of the economy out of equilibrium is required. David (2007) emphasizes the importance of path dependence in economics. Nerlove (1958) discusses the Cobweb model which explains why prices might be subject to periodic fluctuations in certain types of markets. Cobweb model is based on adaptive expectations and subject to criticism by the advocates of rational expectations. Herings (1996) gives a price adjustment process for an exchange economy that converges generically to a Walrasian equilibrium. Tuinstra (2004) provides a price adjustment process in a model of monopolistic competition. There have been a number of ways of modeling paths to equilibrium under different set of frictions, such as incomplete markets (see Angeletos and Calvet (2005), Talman and Thijssen (2006)), adjustment costs (see Lucas (1967), Bertola and Caballero (1990), Danziger (1999) and Chen, Feng and Seshadri (2014)), rational inattention (see Chen, Levy, Ray and Bergen (2008), Ma’ckowiak and Wiederholt (2009) and Mackowiak and Wiederholt (2010)), and search costs (see Wright (1986)), etc. The main contribution of this paper is that the commonly believed notion that if all the ideal conditions of a perfectly competitive market are maintained, the market is Pareto efficient has been challenged to the extent of dynamic adjustment path. It is shown that the market mechanism involves a huge efficiency loss during the adjustment process. It quantifies the efficiency loss on the dynamic adjustment path of a perfectly competitive financial market after an economic shock, and suggests how this efficiency level can be improved. The main source of inefficiency during the adjustment process is the agents’ lack of perfect information about the exact magnitude of the shock and the new patterns of supply and demand. In the existing literature, the imperfect information generally refers to the asymmetric information among buyers and suppliers regarding the quality of a product or other factors leading to an inefficient equilibrium or the non-existence of an equilibrium. This paper shows that even though an equilibrium exists, a free financial market cannot get rid of the ineffciency on the way to the equilibrium. In a financial market, a question of asymmetric information (regarding the quality of goods, which are just funds in a financial market) does not arise. The only way a perfectly competitive financial market can be Pareto efficient during the adjustment process after an economic shock is that if all the ideal conditions of a perfectly competitive market are maintained and also if all the economic agents have perfect information about the exact magnitude and direction of all future shocks and the exact patterns of supply and demand in the market as a result of shocks for all times, which would have been possible only if the economic agents were God, hence making the study of economics and finance needless! We develop a deterministic dynamic financial market model in continuous time framework. The model takes into account a supply curve, a demand curve, and an inventory curve (for funds) for the middleman/ financial intermediary. The model answers the following questions: How do the actions of three market agents, i.e., consumer, producer and the middleman/ financial intermediary in their own interest lead to the market equilibrium? Which path does the financial market follow as a result of an endogenous or exogenous shock in the steady state equilibrium? How efficient is the financial market on that path? Can a free financial market on its own minimize the efficiency losses during the adjustment process or is there any other mechanism which can help improve upon the market in terms of efficiency during the adjustment process? The model is based on mathematical formulations (in time domain) of practical behavior of key agents (without any over-simplistic assumptions) endogenously capturing the feedback effects of interest rate changes on the financial market system. A mathematical innovation in the model is the introduction of deviation variables, i.e., deviation from an equilibrium (before shock) instead of the absolute value of the variable. This makes the model free of initial conditions as the initial value of a deviation variable is always zero. The mathematical picture provides the concrete conditions for the existence of an equilibrium which is independent of the middleman/ financial intermediary’s response. This result is supported by the final steady state equilibrium (mathematical) expression which is dependent just on the responses of the consumer and the producer of funds. The parameter depicting the middleman/ financial intermediary’s response drops out of the final steady state equilibrium expression. However, the mathematical analysis shows that the middleman/ financial intermediary has a very strong role in the dynamic adjustment process. The response of the middleman/ financial intermediary (along with the consumer and the producer’s response) determines the adjustment path of the financial market after an economic shock. The middleman/ financial intermediary’s response also determines the quantum of inefficiency during the adjustment process. The concept of dynamic efficiency has been discussed in the paper. When an economic shock hits the market, the market remains out of equilibrium during the adjustment period before it arrives at a new equilibrium. The longer the adjustment period, the greater are the efficiency losses. The market certainly cannot and does not jump to a new equilibrium (an ideal dynamic efficiency condition which is not achievable). A financial market is said to be closer to an ideal dynamically efficient state if it follows the smoothest and the shortest possible route between equilibriums after an economic shock, that is, if it minimizes the present value of output (of funds) (and/ or consumption (of funds), utility) lost. The model provides a basic framework for a dynamic welfare analysis and scope for extensions and applications in various goods, services, and labor markets. The remainder of this paper is organized as follows: Section 2 explains how the individual components of the financial market system are joined together to form a dynamic financial market model. Section 3 provides a solution of the model in time domain. Section 4 summarizes the findings and concludes.",
45,1,Journal of Economics and Finance,18 September 2020,https://link.springer.com/article/10.1007/s12197-020-09526-4,Time series analysis of Cryptocurrency returns and volatilities,January 2021,Rama K. Malladi,Prakash L. Dheeriya,,,,Unknown,Mix,,
45,1,Journal of Economics and Finance,03 October 2020,https://link.springer.com/article/10.1007/s12197-020-09531-7,"Stock performance subsequent to combinations in quarterly revenue surprise, earnings surprise, guidance, valuation, and report time",January 2021,Jose I. Alvarado,Lindsay C. Clark,Jose A. Gutierrez,Male,,Male,Mix,,
45,1,Journal of Economics and Finance,25 October 2020,https://link.springer.com/article/10.1007/s12197-020-09532-6,Financing constraints and exports: evidence from India,January 2021,M. Padmaja,Subash Sasidharan,,Unknown,Unknown,Unknown,Unknown,,
45,1,Journal of Economics and Finance,16 November 2020,https://link.springer.com/article/10.1007/s12197-020-09535-3,"Derivative use, ownership structure and lending activities of US banks",January 2021,Benjamin A. Abugri,Theophilus T. Osah,,Male,Male,Unknown,Male,"In this paper we examine the impact of bank ownership structure on the relationship between derivative use and lending activities of U.S. banks. Interest in derivative use and its impact on economic activities heightened in the post 2007–2008 crisis period, and anecdotal evidence suggests that derivative use was a key driver of the crisis. Gorton (2008) attributes the financial panic to the opaque nature of derivatives while Paletta and Patterson (2010) point to banks’ use of derivatives predominately for potentially lucrative trading, as fueling the economic turmoil. Despite numerous studies on bank risk-taking and risk management, research on banks’ speculative trading in derivatives is limited. Geczy et al. (2007) suggest that balance sheets of companies are not adequate in identifying speculation. Thus, some studies for other industries, identify companies as speculators either through surveys (Geczy et al. 2007) or through indirect proxies for speculation (Beber and Fabbri 2012). On the role of ownership on lending growth, many studies have shown that ownership structure of firms can explain the activities managers and owners undertake (Fama and Jensen 1983a, b; Smith and Stutzer 1990; Mayers and Smith 1992; Doherty and Dionne 1993; Ben-Nasr et al. 2015; Nakabayashi 2019). Iannotta et al. (2007) as well as Illueca et al. (2008) also show that ownership is an important determinant of lending behavior, risk-taking and performance. Barry et al. (2011) argue that the issue of ownership structure is of particular interest for the banking industry because several factors interact with and alter governance in these institutions. They point out that as a result of financial deregulation and market integration, the scope of banks’ activities has been completely reshaped from traditional intermediation products to an array of new businesses. In the context of the U.S. as in many other countries, commercial banks have different ownership structures, and so an important question is whether the ownership structures of these banks have significant effects on the types of activities banks carryout (see Fama and Jensen 1983a, b). Our study empirically verifies whether ownership structure has a significant effect on the relationship between derivative use and lending activities of U.S. banks. The motivation to examine these relationships is partly based on the possible existence of stockholder-depositor agency conflict in banking institutions. Debtholders such as depositors cannot easily prevent bank shareholders from pursuing more risk due to the incomplete contract problem (Dewatripont and Tirole 1994). Furthermore, depositors have weak monitoring incentives due to the moral hazard problem caused by deposit insurance (Merton 1977; Boyle et al. 2015). Besides, regulators may not have sufficient incentives to monitor bank risk-taking optimally because of forbearance, which may result in taxpayers bearing the brunt when there is bank failure (Kane 1986, 1989). This moral hazard problem could be reflected in banks’ use of derivatives for speculative trading activities rather than for hedging purposes. Lel (2012) for example, finds that companies with strong governance use derivatives to mitigate their exposure to changes in exchange rates, whereas companies with weak governance tend to use derivatives in a manner that makes the companies excessively riskier. Brunnermeier (2009) reports that banks increase risk-taking by using derivatives for non-hedging activities such as speculation. In addition, Paletta and Patterson (2010) suggest that the lack of prudent government oversight may have led to some banks holding unusually large positions in derivatives for non-hedging activities before the 2007 financial crisis. We analyze two types of ownership structures, namely, mutual verses stock banks. According to Rasmusen (1988), mutual and stock banks are different in terms of who controls the bank and receives the profits because they differ in the incentives given to the three groups – owners, customers, and managers – whose interlocking contracts make up the firm. Rasmusen (1988) also argues that because the managers of mutual banks cannot claim the net assets of their firms, increased volatility of returns is of no benefit to them. Thus, the inherent differences between stock and mutual bank ownership make the former more risk-taking. Previous studies by Esty (1997a, b) also reveal that stock thrifts engage in riskier activities than mutual banks, and that risk-taking is increased after conversion from mutual to stock organizational form. In mutual banks, the managers have incentives to “play it safe” and reduce firm risk (Rasmusen 1988; Gormley and Matsa 2016). Specifically, we examine the following questions: (a) How does ownership structure of banks affect their derivative use activities? and (b) Is the relationship between derivative use and commercial bank lending significantly impacted by the ownership structure of a bank? We recognize that though derivative use affects lending, it is possible that a bank’s lending might influence its decision to use derivatives. We directly address this endogeneity concerns through the use of the dynamic GMM estimator. Our study contributes to the literature in a number of ways. First, we analyze how the ownership structure of commercial banks influences the postulated relationships between derivative use and bank lending in the US. The extant studies do not empirically verify the effect of ownership structure on that relationship.Footnote 1 In addition, we offer insights into the speculative activities of banks through our analysis of the substitution effect of derivative use on lending while controlling for ownership structure. Understanding banks’ speculation provides a broader perspective on how banks’ use of derivatives affects their lending activities. This line of investigation is an extension of the literature (Diamond 1984; King and Lipin 1994; Brewer et al. 2000; Purnanandam 2007) which studies the relationship between derivative use and lending. Our paper also relates to the literature on agency cost theory. Initial tests of differences in means of various bank-specific characteristics indicate that stock banks are significantly different from their mutual counterparts and the two bank types are also shown to have different risk profiles and appetite. Generally, stock banks are riskier as measured by their lower capital ratios and also have higher risk appetite. After controlling for other factors, we find that ownership structure is significant in explaining derivative use and lending behavior. Specifically, stock banks tend to engage more in derivative usage relative to mutual banks, and consistent with Diamond (1984), derivative use is positively associated with lending growth for both stock and mutual banks. However, for stock banks, this positive relationship is considerably mitigated, which is surprising given that derivative use is more intense and prevalent among stock banks. This suggest that stock banks are more likely to engage in speculative derivative activities. Our results also indicate that stock banks are riskier than mutual banks as evidenced by their low capital ratios and other related risk measures. After controlling for the 2007–2008 financial crisis, we determine that in both normal and crisis periods, the speed of adjustment of lending to derivatives use by stock banks lags that of mutual banks. We attribute the reduced response of lending to derivative use by stock banks to the higher level of speculative activities in stock banks versus mutual banks. Overall, we suggest that derivative use has a significantly lower impact on lending in banks that are characterized by higher agency problems. The reported results are subjected to robustness checks through the examination of model specifications and the use of an alternative proxy for derivative use. The remainder of this paper is organized as follows: Sect. 2 presents prior research and hypothesis development; Sect. 3 describes the data and presents descriptive analysis; Sect. 4 explains the estimation models; Sect. 5 discusses the empirical results; Sect. 6 reports the robustness checks and Sect. 7 concludes.",1
45,1,Journal of Economics and Finance,17 September 2020,https://link.springer.com/article/10.1007/s12197-020-09525-5,Social health insurance in the Philippines: do the poor really benefit?,January 2021,Salaheddine El Omari,Mahmoud Karasneh,,Male,Male,Unknown,Male,"Social health insurance is one possible way of financing health care services. It aims to reduce health inequalities by offering low-income households free access to medical services. According to standard economic theory, social health insurance improves the use of medical care by reducing the cost of care for indigent patients (Trujillo et al. (2005)). This theory is confirmed in developed countries, as shown by Buchmueller et al. (2005); Hadley (2003), among others. However, in the context of developing countries, there is no consensus among economists that health insurance coverage improves the use of health care services by indigents. Some empirical studies have reported favorable and statistically significant effects of social health insurance on the use of health care by indigent members (Galarraga et al. (2010); Panpiemras et al. (2011); Wagstaff and Pradhan (2005), among others), while other studies do not show any significant impact (Bauhoff et al. (2011); King et al. (2009); Thornton et al. (2010); Wagstaff and Moreno-Serra (2009); among others). In recent years, there has been an increasing interest in social health insurance programs and high-quality impact evaluation (see, for example, Savedoff et al. (2006) and Simon and Barmeier (2010)). Giedion et al. (2013), in their review of research published in the past 15 years on the impact of health insurance programs in developing countries, indicate that approximately 40% of all reviewed studies resort to either multivariate analysis or simple mean comparisons. The empirical evidence based on these evaluation methods generally suggests a positive effect of participation in health insurance programs on the use of health care by low-income households (Makinen and Konaté (2005); Scheil-Adlung and Jütting (2006), among others). However, these methods might not be appropriate for identifying the causal effect of an insurance health program in the absence of randomization. Indeed, the group of participating members and that of non-participants are likely to be distinguished by characteristics that influence recourse to healthcare, such as age, education, income, and health conditions. Evaluating the impact of health insurance is, methodologically, a challenging endeavor. It requires the use of rigorous econometric methods to tackle different issues. This paper evaluates the impact of the Philippine social health insurance program (hereafter PhilHealth) on the use of healthcare services by rural indigents by using a methodological protocol based on two econometric techniques: matching method and regression discontinuity design. The use of these two evaluation methods allowed us to evaluate rigorously this program and verify whether the results depend on the approach used. The first method allowed us to match indigent participants in PhilHealth to similar non-participants, based on their propensity score. This score, which is the probability of an indigent household head being accepted into this program, was estimated using a probit model with the following explanatory variables: the age of the head of household, their gender, their education level, their household wealth, their health status, the household size, the distance to health centers, and the number of children who had died from diseases. The last four variables are introduced for the first time in relation to the existing literature. Regression discontinuity design is infrequently used in economics as a source of identifying information in evaluating a program’s effects. The major advantage of this method is that it bypasses many of the questions concerning model specification (Hahn et al. 2001). Knowing that the Philippine program of social health insurance requires that the dependent (child of the participating indigent) be under the age of 21 to profit from the advantages of the program, we have taken advantage of this eligibility condition to use this precise evaluation technique. The paper is organized as follows. The first section provides a short description of the PhilHealth healthcare program. Section two sketches out the methodology used to evaluate the impact of this program on the use of health care by rural indigents. Section three presents the impact evaluation results. A final section expresses our conclusions and recommendations.",7
45,1,Journal of Economics and Finance,04 October 2020,https://link.springer.com/article/10.1007/s12197-020-09530-8,Keeping what you like: grandfathering and health insurance coverage take-up rates under the ACA,January 2021,Nour Kattih,Fady Mansour,Franklin G. Mixon Jr,,Male,Male,Mix,,
45,1,Journal of Economics and Finance,21 October 2020,https://link.springer.com/article/10.1007/s12197-020-09533-5,Dynamic linkages between US and Eurodollar interest rates: new evidence from causality in quantiles,January 2021,Kenneth A. Tah,Geoffrey Ngene,,Male,Male,Unknown,Male,"Investigation into the linkage between international interest rates has been an enduring topic in finance. Interest rates are an important component in asset pricing. Interest rates do not only comprise a significant portion of total returns of various types of bonds, but they affect the cost of raising debt capital and return on savings. Furthermore, certain interest rates such as the US 10-year Treasury rate afford valuable insight into future economic and financial market activity. According to Mougoue and Wagster (1997), international investors are able to maximize their returns if they have adequate discernment on the nature of the causal relationship between domestic and off-shore interest rates. Such an insight will help them identify and respond to exogenous shocks triggered by international interest rate linkages. For policy makers, the national monetary policy authorities can greatly benefit in designing monetary and fiscal policies if they can identify the effect of cross-market interest rate causal dynamics on domestic monetary policy (Swanson 1987). Lastly, finance managers must assess how sensitive the firm’s cost of debt (and overall cost of capital) is to cross-market causal influences, especially for multinational corporations operating in different countries and in different currencies. For business managers, accurate forecasting interest rates in a country is important for strategic business decisions (Lo et al. 1995). The building block for the literature on interest rate linkage is that global markets have become more integrated, making co-movements of international interest rates possible. Prior studies have heavily relied on least square (LS) models, such as the error correction model, to investigate the causal relationships between interest rate changes in the US domestic and European markets. However, the results have been inconclusive. For example, Hendershott (1967) and Kwack (1971) found no causal relation between the US interest rate market and the European markets. In contrast, Dufey and Giddy (1978), argue that the Eurodollar market should Granger cause the US domestic market because Eurodollar interest rate respond more efficiently to information. On the other hand, Yang et al. (2007) and Chan and Lee (1996) document causality that reverses over time. With a sample from 1983 to 2002, Yang et al. (2007) use a recursive co-integration test to show a stable long run relationship between US and Eurodollar interest rate after the early 1990s. They further show a unidirectional causality running from the Eurodollar rate to the US interest rate prior to 1991 and the bidirectional causality after 1991. Chan and Lee (1996) used the error correction model to investigate the causal relationships between the US domestic and Eurodollar interest rate. They found that the US domestic rate granger causes Eurodollar interest rate from 1973 to 1982 and Eurodollar interest rate granger cause US domestic rate from 1983 to 1992. Yet, other studies like Monadjemi (1998) and Bremnes et al. (2001) show causality that runs from the US domestic market to the European markets. Monadjemi (1998) examined the response of interest rates among the United States, the United Kingdom and the Netherlands. He found a long-run cointegrating relationship among the variables based on the Johansen’s cointegration test. However, their error correction term in the VECM model was hard to interpret. Using the variance decomposition and impulse response functions, he documents that the US interest rate explains variations in British and Dutch interest rates, but not vice versa. Bremnes et al. (2001) examined the short- and long-run relationships among interest rates in the US, Germany and Norway using Johansen cointegration, variance decomposition and impulse response methodologies. They documented that the US long-term interest rate influence both the Norwegian and German long-term rate, and German long-term interest rate influence the Norwegian interest rate. They interpreted their findings to suggest that shocks in the world’s major interest rates influence smaller interest markets. Additional findings by Ajayi and Serletis (2009) showed a bi-directional causality between interest rate changes in the US domestic and the Eurodollar markets. Ajayi and Serletis (2009) employed both the linear and nonlinear Granger causality tests and documented a linear unidirectional causality running from the US certificate of deposit interest rate to the Eurodollar interest rate and a bidirectional nonlinear causality between Eurodollar and US certificate of deposit interest rate. A common attribute with the above studies is that they investigated the causal linkages among interest rates across international money markets with the use of causality models. To the best of our knowledge, causal inferences from earlier studies did not allow for asymmetric quantile causal effects. The Granger causality test generates estimates that approximate the conditional mean of the response variable given certain values of the predictor variable. In order words, the Granger causality test model assumes that the causal relations between international interest rate changes are constant across different interest rate levels and frequencies. A causal relationship in a system of financial or economic time series has been widely studied using the Granger-causality test which assumed Gaussian distribution and focused on Granger-causality in mean. Ang and Chen (2002) asserted that it is common to find non-Gaussian relationship between economic variables or financial variables. Our summary statistics in Table 1 show that both the US interest rate and the Eurodollar interest rate have high kurtosis. This is similar to both Gray (1996) and Bali (2007) findings that US interest rate has high kurtosis. This implies that the distribution of US and Eurodollar interest rates levels have fat tail compared to the normal distribution. Our Jarque-Bera statistics in Table 1 suggest that both the US interest rate and the offshore Eurodollar interest rate are not Gaussian. Bali (2007), Jarque-Bera statistics indicated that the empirical distribution of US interest rate is far from Gaussian. Accordingly, with a non-elliptical distribution, causality among interest rates may matter differently in different moments. Therefore, it is more informative to test causality in quantiles to infer the causal relationship between the US and Eurodollar (London) interest rates. In this paper, we reinvestigate the causal relations between the US interest rate and Eurodollar (London) rate using the Granger causality test in quantiles. This permits us to examine causality over interest rate distributions not covered by the usual causality test. We therefore test causality in different quantile ranges. The rest of the paper is organized as follows: Section 2 discusses theoretical issues and econometric methodology. Section 3 explains the sources of data and the results of preliminary data analysis. Section 4 details the empirical results while section 5 concludes.",
45,2,Journal of Economics and Finance,13 December 2019,https://link.springer.com/article/10.1007/s12197-019-09503-6,ECB’s unconventional monetary policy and bank lending supply and performance in the euro area,April 2021,Dimitris Kenourgios,Despoina Ntaikou,,Male,Female,Unknown,Mix,,
45,2,Journal of Economics and Finance,03 December 2019,https://link.springer.com/article/10.1007/s12197-019-09498-0,The slowdown in trade: end of the “globalisation hype” and a return to normal?*,April 2021,Ansgar Belke,Daniel Gros,,Male,Male,Unknown,Male,"Trade and international financial transactions have grown massively in recent decades. This phenomenon, also called globalisation, has been subject to a huge amount of analysis (Michie 2019). Business and political leaders never tire of repeating that ‘globalisation’ is the future, that it delivers more jobs and higher incomes. The link between trade and growth, however, is much more subtle than the slogan ‘globalisation equals growth’. There is no general economic theorem that links more trade to growth and other economic benefits (IMF 2018). Economic theory implies only that, under most circumstances, lower trade barriers will generate welfare gains and lead to more trade. The simplification that more trade per se is thus always beneficial is not warranted. If trade increases for other reasons than the lowering of trade barriers, it is far from clear that this will benefit everybody. Over the last few years, trade growth has slowed down considerably. All the international financial institutions have noted this trend and have published empirical investigations of the phenomenon (IMF 2015; 2016); OECD 2016; ECB 2016). A general finding of these studies is that the slowdown in trade is difficult to explain. The explanations offered for declining trade to GDP ratios usually related to lower overall growth, lower investment and changes in the product composition of trade (KOF 2014). However, these ‘explanations’ seem rather contrived if one looks at the implications of standard trade models. The ‘Krugman Dixit Stiglitz’ model of intra-industry trade, in particular, is well suited to explain the exchange of differentiated manufacturing goods, which dominated world trade (along with raw materials). This model has strong implications for the global volume of trade and the ‘elasticity’ of trade with respect to GDP, if countries grow at different rates and relative country sizes change a lot. The model implies that both the expansion of trade over the last two decades, and the recent slowdown, can be readily explained by changes in relative country sizes. The key insight is that the emergence of new economies, in particular China, fosters trade as long as the newcomer is smaller than the established economies (Belke and Gros 2017), but this effect changes after a certain point. An immediate first implication is that the slowdown in China’s growth has little to do with the slowdown in trade. On the contrary, the model implies that once China is larger than the other large developed economies (EU, US), further growth of China leads to less trade as more and more components are produced at home instead of being imported. Given that China has already reached this point, one must conclude that faster growth in China would only accelerate the slowdown in trade. Additionally, the distinction between globalisation driven by lower trade barriers and increases in trade or financial transactions driven by other factors is not just an academic point. It is key to understanding why globalisation has become so unpopular in most advanced countries and why the recent slowdown in trade is not something to worry about. Next, in section 2, we shortly look at declining gains from trade liberalisation and the importance of commodity prices. Section 3 investigates the link between the global trade-to-GDP ratio the structure of the global economy, concentrating on the emergence of China. The following section 4 establishes a simple aggregate measure of differences in country sizes. Section 5 provides some basic data on Global Value Chains. Finally, we conclude on the lessons of this analysis.",1
45,2,Journal of Economics and Finance,06 May 2020,https://link.springer.com/article/10.1007/s12197-020-09510-y,Six decades of inflation and money demand,April 2021,Alexi Thompson,Henry Thompson,,Male,Male,Unknown,Male,"Inflation is expected to diminish demand for money as a store of value but not necessarily for transactions depending on available substitutes. Dollar inflation over recent decades must be contributing to the increasing popularity of gold, crypto currencies, and other international currencies. The present paper estimates the effects of a change in inflation on the growth rates of transactions, store of value, and total money demands. The money market model specifies money demand as a function of income, the opportunity cost stock market return, and inflation as a separate variable. Money supply is assumed under exogenous control by the central bank. The estimate of deflated real money demand as a function of deflated variables includes the price level in every variable introducing multicollinearity and conflicting the effect of inflation. The present approach is to combine the various effects of inflation into a single variable in the estimated money demand function. The annual data from 1960 to 2017 starts with availability from the Federal Reserve Economic Data FRED. Over these nearly six decades, the simple average inflation of 3.7% ranging from −0.4% to 12.6% lowered purchasing power of the dollar 88%. Even the relatively low 1.7% average inflation during the most recent decade lowered purchasing power 12%. Given the alternatives for store of value and increasingly for transactions as well, such decreased purchasing power would be expected to reduce dollar demand. The present difference stationary natural logs of the money series, income, and price level lead to reliable estimates in differences of growth rates. The stochastic stock market return enters all estimates directly. The series are cointegrated in differences leading to robust error correction estimates in double differences. Exogenous structural breaks for Volker money supply targeting in 1980 and bank bailout policy in 2009 are suggested by the data and improve properties of the estimates. The following section reviews the literature that treats inflation as a separate variable in money demand. Section 3 presents the novel money market model and reduction of the effect of inflation into a single coefficient on the right hand side of the estimated money demand equation. Section 4 develops the stationary properties of the series. Section 5 presents the estimated money demand models.",2
45,2,Journal of Economics and Finance,05 July 2020,https://link.springer.com/article/10.1007/s12197-020-09509-5,Can country-specific interest rate factors explain the forward premium anomaly?,April 2021,Efthymios Argyropoulos,Nikolaos Elias,Elias Tzavalis,Male,Male,Male,Male,"One of the well-known puzzles in the exchange rate literature is the failure of interest rate differentials across countries to forecast the correct direction of future exchange rate movements, predicted by the uncovered interest rate parity (UIRP). In fact, the slope coefficient of a regression of the one-month ahead exchange rate change between two countries on their one-month maturity interest rate differential (known as carry trade) predicts appreciation of the home currency per unit of the foreign currency, instead of depreciation. This means that countries with higher interest rates will face appreciation of their currency, compared to those with lower interest rates. This phenomenon is known in the literature as the forward premium anomaly (or puzzle). For a survey, see Lothian and Wu (2011). Many theories have been suggested in the literature to explain the anomaly. Among these theories, the existence of a time-varying exchange rate risk premium constitutes a natural one. According to this theory, investors in the FOREX market are not risk neutral, but they are covered for adverse exchange rate movements (see, e.g., Mark2001).Footnote 1 This can obscure investors’ expectations embodied in interest rate differentials about future changes of exchange rates, since these differentials are also determined by exchange rate risk premium movements in international bond and money markets. To model the time-variation of this risk premium, recent research relies on the factors and their associated risk-premium sources driving the domestic and foreign term structure of interest rates (see, e.g., Inci and Lu (2004), Ahn (2004), and Ang and Chen (2010), and Diez de los Rios (2011), among others). Based on affine international term structure models, this strand of research is mainly interested in examining whether there exist global and/or country-specific (local) factors that can explain the slope coefficient bias of the UIRP regressions and, hence, the forward premium anomaly. The global factors are often associated with macroeconomic changes in inflation and real activity, linked to world business cycle effects (see Diebold et al. 2006, or more recently Argyropoulos and Tzavalis 2016), while the country-specific ones are assumed to reflect asymmetries in the home (or foreign) economy, which are not common across countries. These asymmetries influence, separately, the asset and bond prices across countries and, as noted by Backus et al. (2001), they are necessary conditions for the explanation of the anomaly. In this paper, we add to the above literature in two ways: Firstly, we suggest an empirically attractive two-country affine term structure model which provides estimates of the exchange rate risk premium by modelling simultaneously interest rates and exchange rate dynamics. These estimates are then used to examine if time-variation in the risk premium can explain the forward premium anomaly. To this end, we suggest an extension of the UIRP regression model adjusted for time-varying risk premium effects. Secondly, we evaluate the performance of the model to forecast future exchange rate movements net of the risk premium effects. Using data from five developed economies (Germany, Japan, Canada, the US and UK), we show that the suggested regression model provides forecasts of the future exchange rate movements which are in accordance with the expectations hypothesis. We also find that, in terms of forecasting power, the suggested model compares favorably, in terms of the root mean square and absolute forecasting error metrics, to the random walk model of exchange rates which is considered as the benchmark model in the literature (see Rossi 2013 for a survey). The rest of the paper is organized as follows. In Section 2, we present the model. In Section 3, we provide estimation results of the model, as well as the UIRP regression model adjusted for the risk premium effects and we evaluate the forecasting performance of the model. Finally, Section 4 concludes the paper.",2
45,2,Journal of Economics and Finance,27 July 2020,https://link.springer.com/article/10.1007/s12197-020-09515-7,Public debt and economic growth: panel data evidence for Asian countries,April 2021,Dimitrios Asteriou,Keith Pilbeam,Cecilia Eny Pratiwi,Male,Male,Female,Mix,,
45,2,Journal of Economics and Finance,10 November 2020,https://link.springer.com/article/10.1007/s12197-020-09527-3,African equity markets’ exposure to oil and other commodities - implications for global portfolio diversification,April 2021,Imhotep Paul Alagidede,Gideon Boako,Bo Sjo,Unknown,Male,Male,Male,"Most developing economies depend on primary commodities for export revenues and foreign exchange for development (Deaton 1999). This dependency became pronounced between 2000 and 2010 with the broad-based surge in commodity prices, particularly precious metals. Gold prices for instance, quadrupled between 2001 and 2010 (Narayan et al. 2013). To this end, investment inflows attributed to most commodities’ future markets surged from US$15 billion to US$200 billion between 2000 and 2008 (CFTC 2008) and subsequently increased to US$210 billion in 2012 (Boako and Alagidede 2016). Baur and McDermott (2010) have explicitly shown that precious commodities exhibit low and positive best response function to negative economic disturbances. This is counterintuitive to the prevailing logic of equity price behavior. As a result, commodities attracted such huge investments in contrast to sharp deteriorations in the performance of global equity markets. The existence of low and negative correlation between unrelated asset classes have been ascribed to the different macroeconomic variables which influence their performance (Gorton and Rouwenhorst 2006). This relationship is very relevant for the transfer of price risk of commodities for diversification and portfolio allocation. Diversification into commodities has become more significant in the last two decades because of the high volatility and contagion risk emanating from high integration and interdependence among world financial markets. In addition, the increasing financialization of commodity markets expedites the distribution of same in huge multi-asset portfolios to achieve hedging demands and possible substitution (Bekiros et al. 2016). Embracing a diversified portfolio might raise total earnings, decrease risk and enhance sharp ratio (Batten et al. 2010). However, Africa and other emerging equity and commodity markets appear to be losing out of the advantages of this exposure to diversified portfolios which are likely to generate huge portfolio investment. Boako and Alagidede (2016) attributed this to post-crisis era poor performance of equity markets, as well as the uncertainty of the prospects of investing in Africa. They further argue that the prevailing state of knowledge on Africa is very limited, and this often leads to half-baked conclusions on the continents markets. That notwithstanding, the continuing turmoil in the advanced capitalist economies presents Africa and other emerging regions opportunities that can enable investors diversify their portfolio risk and reap superior returns. More importantly, Africa is a haven of safety. The continent has robust natural and human resources that can support literally any kind of investment contemplated by the human mind. For Africa and emerging markets to appropriately offer international investors a profitable alternative for investment portfolio diversification, there is the need to understand the joint movement between their equity markets and world prices of commodities. This is important because the risk of inadequate knowledge by investors on specific-market drivers of commodities might make them ascribe their performance to demand-side shocks. Given that the performance of financial markets in recent times has been affected by shocks attributed to business cycles, investors are most likely to believe that correlation between prices of commodities and equities will be high. The primary objectives of this study are two-fold. First, we address the issue of dynamic linkage between commodities and stock markets for Africa amid economic cycles. Second, we explore the relative potentials of African stock markets as vehicles for enhancing international portfolio diversification. We focus on African and emerging markets as the most favourable alternative to protect international investors from the frequent turmoil across advanced economies due to their likely decorrelation with global shocks (Boako and Alagidede 2016). In addition, African and emerging markets together constitute the largest producers of most precious metals in the world. For instance, in 2015 African and emerging markets of Asia, South and Central America produced about 59% of the global precious metals (U.S Geological Survey 2016). Cote d’Ivoire, Ghana, Nigeria and Cameroon account for more than 70% of global cocoa output. South Africa is among the top five gold producers, and it is number one for platinum, among a host other precious gems. Four African countries (namely, Algeria, Angola, Libya, and Nigeria) are part of the twelve-member OPEC group. With this in mind, there is no doubt that changes in the prices of these commodities can affect economic, social and financial fortunes of the African continent. Happenings in the commodities markets could therefore reflect the choices and selection of alternative asset classes by both local and international investors. Additionally, African countries depend on raw material and commodities in their production and exports. The net exposure and changing correlations among the asset classes are interesting to examine. The paper is particularly useful for Africa because at the center of Africa’s development agenda is the quest to attract high private capital flows (PCFs).Footnote 1 Although the absolute risk borne by international investors in their asset allocation and portfolio selection decisions may be irrelevant (as opined by Bekaert and Harvey 2014), it is important to note that investors may fail to include a particular asset class in their diversified portfolios if the ultimate risk exceeds the expected pay-off. Generally, emerging/frontier markets’ equity returns are characterized by higher risk and volatilities (see Moss and Thuotte 2013). At the same time, uncertainty, risk perceptions (such as extreme political strife), institutional underdevelopment and poor corporate governance structures which are common in developed markets have been exported to Africa and they remain critical hindrances to international investors seeking to diversify into the continent’s rather nascent markets – see also Alagidede (2008). Additionally, the uncertainty about earning higher expected pay-offs has been a major contributing factor to why the continent (Africa) appears not to be receiving large portfolio investment flows.Footnote 2 Meanwhile, recent crashes in the global economy and the increasing significance of developing economies in the globalization process have attracted the attention of fund managers to diversify across those economies. The ability of the African markets to identify and benefit from such potential international cross-border portfolio investment flows and diversification opportunities requires an understanding of the cross-market linkages between its financial markets and the global economy, as well as the risk-return trade-offs. Section 2 reviews the related literature. Section 3 outlines the methodologies. Detailed discussion of the results is executed in Section 4. Section 5 concludes the paper with some recommendations.",5
45,2,Journal of Economics and Finance,09 September 2020,https://link.springer.com/article/10.1007/s12197-020-09524-6,Assessing macro-prudential policies: the case of FX lending,April 2021,Michael Sigmund,,,Male,Unknown,Unknown,Male,"In recent years, many advanced and emerging countries have introduced macroprudential policies to limit the risks and costs of systemic crises.Footnote 1 Since the financial crisis in 2008, policymakers and market participants widely agree that idiosyncratic events may potentially trigger severe instability or collapse an entire industry or economy, also known as “systemic risk” (Iori et al. 2006). The materialization of systemic risk may lead to systemic crises endogenously, when markets, policy makers and/or individuals do not correctly anticipate the system risk or its costs. Hence, systemic risk poses a significant risk to financial stability. Well known policies such as microprudential, monetary and fiscal policy tools, even when conducted properly, do not always suffice to prevent financial instability or limit the costs of a crisis. Restoring financial stability after a financial crisis is undoubtedly very costly. For example, Eurostat (2015, 2018) report that the total costs of 241.3 bn euro for the general governments in the EU-28 to support financial institutions from 2007 to 2017. Following Galati and Moessner (2013), the overall key objectives of macroprudential measures (henceforth, MMs) are (1) limiting financial system-wide distress and (2) avoiding macroeconomic costs linked to financial stability. The 2008 financial crisis has uncovered the need to establish an “official” legally binding and harmonized macroprudential policy framework alongside microprudential supervision (ESRB 2013). Since 2014, this macroprudential framework is part of Basel III.Footnote 2 Before the introduction of Basel III in 2014, numerous examples of policy measures were also classified as “macroprudential”. For instance, (Galati and Moessner 2013, 2018; Dimova et al. 2016; Claessens 2015) provide an excellent overview on these earlier measures, focusing on their effectivenessFootnote 3. Thereby, Claessens et al. (2013) investigate how balance sheets of 2,800 banks in 48 countries over 2000-2010 changed due to macroprudential policies. They provide evidence that borrower-based measures (i.e. caps on debt-to-income (DTI) and loan-to-value (LTV) limits) are effective in reducing asset growth, while some measures are even counterproductive during downswings by exacerbating declines. Cerutti et al. (2017) focus on house prices and household credit for over 50 countries and find that housing finance regulations are most effective in handling mortgage market booms. Cerutti et al. (2017) do not only consider the real estate market but also examine the effectiveness of different macroprudential policies for 119 countries between 2000 and 2013. Their analysis covers both advanced and emerging markets and many different instruments, both borrower-based policies – which are more frequently employed in advanced economies – and foreign exchange related measures, in particular, implemented by emerging economies. Their paper finds that the effects are smaller in financially more developed and open economies. Also, they find evidence that macroprudential policies can help manage financial cycles, although they work less well during busts. Lim et al. (2011) and the IMF (2013) also investigate how macroprudential policy changes affect financial vulnerabilities (such as credit growth, pro-cyclicality, leverage, house prices) and the real economy. They conclude that LTV and DTI limits, reserve requirements (RR) and ceilings on credit growth have an effect on the real economy and are effective in reducing pro-cyclicality and leverage. In addition, using quarterly data of 16 CESEE countries, Vandenbussche et al. (2015) test whether housing price inflation is affected by macroprudential policy measures. They find that capital measures and nonstandard liquidity measures are most effective. Furthermore, Dumičić (2018) studies the effectiveness of macroprudential policies in Central and Eastern European Countries (CEE) and finds that MMs were more effective in slowing credit to households than credit to the non-financial corporate sector, as non-financial corporates had access to non-bank and cross-border credit in addition to domestic bank credit. Most recently, Revelo et al. (2020) look beyond the effects of macroprudential policy on credit growth by analyzing the interaction of macroprudential and monetary policy. They show that a restrictive monetary policy enhances the impact of macroprudential tightening on credit growth. Similar results are presented in Gambacorta and Murcia (2019) by evaluating the effectiveness of macroprudential tools in South America (Argentina, Brazil, Colombia, Mexico and Peru) and their interaction with monetary policy. Macroprudential tools have a greater effect on credit growth when reinforced by the use of monetary policy. Apart from analyzing the effectiveness of macroprudential policy measures, recently increasing efforts have been made to explore potential side effects of macroprudential policies. For example, Nier and Kang (2016) investigate the interactions between monetary and macroprudential policy as monetary policy can have severe unintended effects on financial stability. Alternatively Boar et al. (2017) explore the effects of macroprudential policy measures on long-run economic performance and find that countries that use macroprudential tools more frequently witness higher and less volatile output growth. In addition, measuring potential cross-border spillover effects from macroprudential policy measures are also important to understand as it might reduce or increase the effectiveness of macroprudential policies in containing systemic risk (Nocciola and Zochowski 2016). Altunbas et al. (2018) find a strong relationship between bank risk and macroprudential policy. Analyzing a large sample of banks from 61 countries, they present three main findings (1) macroprudential tools have a significant impact on bank risk, (2) the responses to changes in macroprudential tools differ among banks and (3) controlling for bank-specific characteristics, macroprudential policies are more effective in a tightening than in an easing episode. In this paper, we first contribute to the existing literature by proving a theoretical model and an empirical implementation for a commonly used MM aimed at restricting certain types of lending. We focus on measures reducing excessive FX lending by setting up a Cournot oligopoly model with two types of loans, intra-firm competition and horizontally differentiated products. We also highlight why FX lending poses a systemic risk to the Austrian banking system. In 2003, without a legally binding framework, Austrian authorities took their first steps to implement macroprudential tools to address excessive FX lending. Since the onset of the global financial crisis 2007/2008 (henceforth, GFC), further steps were taken to tighten the measures in 2008, 2010, 2013 and 2017. By using a supervisory, quarterly dataset on all Austrian banks from 1998Q1 to 2016Q4, we estimate these model parameters by employing a simultaneous equation panel approach. We aim to determine whether FX loans were substituted for euro loans and to what extent these MMs were effective in reducing FX lending. Our main finding is that Austrian banks substituted FX loans for euro loans, while only the later, more intrusive MMs of 2010 and 2013 were effective in reducing FX lending in Austria. The rest of the paper is organized as follows. In Section 2 we discuss the MMs used to curb excessive FX lending in Austria and why FX lending seemed to be beneficial for banks and borrowers from an individual point of view. In Section 3 we describe our unique supervisory data set. In Section 4, we develop a Cournot type model to look at and control for the trade-off between granting FX loans and euro currency loans. In Section 5 we describe how we estimate the parameters of the theoretical model of the previous section. Section 6 describes our results based on effectiveness and negative side effects of the macroprudentical measures. Section 7 concludes and critically reviews the policy actions taken.",
45,2,Journal of Economics and Finance,19 August 2020,https://link.springer.com/article/10.1007/s12197-020-09521-9,Empirical analysis of term structure shifts,April 2021,Joel R. Barber,,,Male,Unknown,Unknown,Male,"Traditional duration analysisFootnote 1 is based on the assumption that shifts in term structure are level (Fisher and Weil 1971). Surprisingly, until Litterman and Scheinkman’s 1991 ground breaking work, little attention was paid to rigorously analyzing and determining the types of shifts implied by actual changes in the term structure. Litterman and Scheinkman applied factor analysis to the excess weekly returns over the risk-free rate on zero coupon bonds from January 1984 through June 1988.Footnote 2 They discovered that three factors explain 96% of the variation of the excess returns of any zero coupon bond. By transforming the factors loadings on excess returns to loadings on spot rates, they observed that the first factor corresponds roughly to a level shift, the second factor is related to a change in steepness (a twist shift), and the third factor is related to a curvature change (a butterfly shift). Most subsequent papers analyzing term structure shifts dealt directly with changes in the spot rate curve, rather than implied changes in spot rates derived from returns on zero coupon bonds. Further, most subsequent papers employed principal component analysis (PCA), rather than factor analysis. Barber and Copper (2012) summarized the advantages of PCA versus factor analysis: “[I]t is computationally simpler and much faster; the components are observable in the sense they can be constructed as a linear combination of observed data; the explanatory power is higher because the form of the error covariance matrix is not restricted [to a diagonal form]; and the addition of an extra component does not change the components that are already present. Also, unlike maximum likelihood factor analysis, its implementation does not require the assumption that the underlying variables have a joint normal distribution.” PCA and factor analysis of term structure movements has shown that between 80 and 90% of shifts in term structure variation can be explained by a uniform change that is roughly level. For the most part, the remaining variation is explained by a twist and a butterfly-type shift. Litterman and Scheinkman’s observation is confirmed by a large number of other authors. See Barber and Copper (1996, 2012); Falkenstein and Hanweck (1997); Willner (1997); Golub and Tillman (2000); Barrett et al. (2004); Reisman and Zohar (2004); Novosyolov and Satchkov (2008); Roland and Nikitina (2011); Juneja (2012); Bauer and Hamilton (2018); Pan and Junhui (2018); and Hagenbjörk and Blomvall (2019). A naive interpretation of PCA, which supports traditional duration analysis, is that a very large percentage of term structure shifts are nearly level. This paper examines actual changes in term structure. We use a three-fold classification of term structure shifts, based on the number of zero crossings, into uniform, twist, and butterfly shifts. Each type of shift can further be classified as either positive or negative, for a total of six types of shifts. Our analysis of actual term structure changes from January 1986 to the end of 2016 reveals that only 57% of the shifts are uniform, 29% of the shifts are twists, and 10% butterfly-type shifts. The result is roughly the same for daily, weekly, and monthly changes. Evidently, PCA and factor analysis overstates the prevalence of uniform shifts and understates the prevalence of twists. This is true even if PCA is applied only to term structures that are empirically classified as twists. In this case, we find that 70% of the variation is explained by uniform shifts and only 23% explained by twists. In other words, only 23% of the variation is explained by twists in term structure shifts that are empirically classified as twists. The remainder of the paper is organized as follows: Section 2 first classifies term structure shifts as uniform, twists, and butterfly based on the number of zero crossings in the vector of an historical shift. We then count the number of each type of shift and examine the shape of each type in an historical sample using daily, weekly, and monthly changes over 28 years and over three subperiods. Section 3 develops the intuition behind PCA as a method to extract term structure shifts and measure their explanatory power. Section 4 contains the results of PCA and a comparison between the zero-crossing analysis and PCA. Finally, Section 5 concludes the paper.",1
45,2,Journal of Economics and Finance,20 August 2020,https://link.springer.com/article/10.1007/s12197-020-09520-w,Income convergence across the U.S. states: further evidence from new recent data,April 2021,Rati Ram,,,Male,Unknown,Unknown,Male,"Study of income convergence across countries or regions is of substantial interest because it can shed light on temporal evolution of income inequality and indicate whether there is support for the recent view about increasing income inequality in the U.S. and other Western countries. Traditionally it has also been used to form an opinion on the relative merits of Solow-type neoclassical growth theories, which imply convergence, and endogenous growth models which carry no such implication.Footnote 1 In addition to numerous studies of cross-country convergence, there has been considerable research on income convergence across the U.S. states.Footnote 2 The early work by Barro and Sala-i-Martin (1991, 1992, 2004) concluded that ""overall evidence weighs heavily in favor of convergence"" and that ""U.S. states provide clear evidence of convergence."" These studies led to a kind of ""stylized fact"" of convergence across U.S. states at a rate of about 2% per year.Footnote 3 Since then several researchers have investigated the presence or absence of income convergence across the U.S. states, and there is some variability across the studies in terms of the methodology and the conclusions. Among the recent studies, Heckelman (2013) reported tests of beta-, sigma- and stochastic-convergence along with a consideration of the determinants of convergence, particularly the change in capital-labor ratio. Even more recently, Ganong and Shoag (2017) indicated that there has recently been a large decline in the rate of income-convergence across the US states, and provided an explanation in terms of rising housing prices in high-income areas deterring low-skill migration and slowing income convergence. The main purpose of this paper is to revisit the question about income convergence across the U.S. states so as to make a contribution toward filling the following gaps in the existing studies. First, it covers the most recent period 1997-2018, which no other researcher has considered, and which is important because of the belief about increasing income inequality during the last two decades. Second, it uses official BEA data on state-level real GDP per capita, which has also not been done in any other study. While use of real-income data is obviously appropriate conceptually, it is also empirically important. For example, Ram (2018) has shown that interstate income inequality is much smaller in real income than in nominal income, and Izraeli and Murphy (1997) reported opposite convergence-patterns in nominal and real income for the US states. Third, it documents opposite convergence-patterns in 1997-2018 relative to 1977-1997. Fourth, it places the observed patterns in the context of Piketty-type propositions about high returns to capital and increasing income inequality in the U.S. during the last few decades.",7
45,2,Journal of Economics and Finance,06 November 2020,https://link.springer.com/article/10.1007/s12197-020-09534-4,Impact of green bond policies on insurers: evidence from the European equity market,April 2021,Petr Jakubik,Sibel Uguz,,Male,Female,Unknown,Mix,,
45,2,Journal of Economics and Finance,21 August 2015,https://link.springer.com/article/10.1007/s12197-015-9335-5,Erratum to: RETRACTED ARTICLE: The role of circuit breakers in the oil futures market,April 2021,Nicholas Apergis,,,Male,Unknown,Unknown,Male,,
45,3,Journal of Economics and Finance,10 April 2021,https://link.springer.com/article/10.1007/s12197-021-09546-8,An examination of the liquidity of equity carve-out parents,July 2021,Thomas H. Thompson,,,Male,Unknown,Unknown,Male,"Bali et al. (2014) report that stock liquidity reflects a security’s ability to be traded without incurring an adverse price impact. We extend the literature by examining several variables and their impact on three liquidity measures: bid-ask spread, turnover, and volatility. Although there have been other equity carve-out (ECO) studies, this is the first study to test hypotheses related to the liquidity of ECO parents. We investigate the traded parents of 234 equity carve-outs (ECO) during the period from 1993 to 2018. Since liquidity can be time variant (Bali et al. 2014) we examine liquidity measures over seven timeframes: the carve-out announcement, the carve-out ex-date, the post carve-out period, the second event announcement period, the second event ex-date, and the five-day post second event period. The ECO has gained prominence since the Klein et al. (1991) study reported that carve-outs are a part of two stage process. In their sample period (1966–1983), 25 of 52 (48.08%) were reacquired by the parent, 16 (30.77%) were acquired by third parties, eight (15.38%) had no second event and only two carve-outs (3.85% of the sample) were subsequently spun off.Footnote 1 For our sample period (1993–2018), 94 of 234 carve-outs (40.17%) were acquired by third parties, 65 (27.78%) were spun-off from their parents, 34 (14.53%) were reacquired by their parents, 22 (9.40%) were delisted, and 19 (8.12%) had no second event. We observe that liquidity varies from the carve-out period through the post-second event period. Liquidity measures peak at the lock-up date and the second event announcement date. Also, we find that variables such as trade volume and market size can influence liquidity measures.",
45,3,Journal of Economics and Finance,01 September 2020,https://link.springer.com/article/10.1007/s12197-020-09519-3,Persistence in the market risk premium: evidence across countries,July 2021,Guglielmo Maria Caporale,Luis A. Gil-Alana,Miguel Martin-Valmayor,Male,Male,Male,Male,"The capital asset pricing model (CAPM), in particular its one-factor version, has been for decades the most commonly used framework to analyse the relationship between risk and return. For instance, Fama and MacBeth (1973) estimated this model for NYSE stocks and found a positive relationship between average return and market volatility in the period 1926–1968. The standard approach to calculating the cost of equity is also based on the CAPM (Fernandez 2015): in a survey of the Association for Financial Professionals (AFP) 90% of the respondents said that they use the CAPM for estimating the cost of capital and making investment decisions (Jacobs and Shivdasani 2012). This paper focuses on a key component of the CAPM, namely the market risk premium (MRP), which is defined as the difference between the expected return on a market portfolio and the risk-free rate, and is also the slope of the security market line (SML), a graphical representation of the CAPM. The aim of the analysis is to provide evidence about some of its statistical properties as well as those of its volatility, in particular their degree of persistence, by applying fractional integration techniques to a set of data from the US, Germany and Japan, namely the biggest economies in America, Europe and Asia respectively in terms of nominal GDP over most of the time period considered in our study. The tests are carried out at different frequencies (weekly and monthly) and over different time horizons (2, 5, and 10 years) in order to check the robustness of the findings. The possible existence of breaks and changes in persistence is then investigated in the case of the US. The layout of the paper is the following: Section 2 reviews the relevant literature; Section 3 describes the data and the econometric framework; Section 4 discusses the empirical findings; Section 5 offers some concluding remarks.",2
45,3,Journal of Economics and Finance,07 February 2021,https://link.springer.com/article/10.1007/s12197-021-09540-0,What Can explain catering of dividend? Environment information and investor sentiment,July 2021,Hadfi Bilel,Kouki Mondher,,Unknown,Male,Unknown,Male,"The financial literature widely support the role played by behavioral finance covers some topics such as representativeness bias, overconfidence, party took selfish, the gambler’s fallacy, back, panic, breeding behavior, status quo, survival bias, money illusion, loss aversion, attachment, disposition effect, recovery, familiarity, illusion of control, bias at home on the investor attitude (see Kent Baker et al. (2017), Hirshliefer (2008) and Hirshliefier and Hong Toeh (2009), Malmendier and Tate (2015), Filbeck et al. (2017), Wang and Jianfeng (2014), Mcarthy (2016), Filbeck et al. (2017). Further, the investor’s sentiment plays an important role in determining his decision towards the dividend. This result suggests that investor with negative (positive) feeling increase (decrease) their preference for dividend. The investor asks for more dividends when their sentiment is negative. This is explained by the fact that the latter believes that the share price of the company cannot be changed over time, which anticipates that it can achieve a gain on its speculation. Further, if the investor is purely rational like in the classic finance, the behavioral variables should have no influence on dividend policy choices. Besides, the investor psychology plays a crucial role in their decision to demand dividend. Sherfrin and Statman (1984) indicate the self-control bias suggesting that investor prefer demand to their self-control and for personal issue. Baker and Wurgler theory (2004b) indicate that some investors have time-varying demand for dividend-paying stocks for institutional or behavioral reasons. Breuera et al. (2014) related the dividend payout to three based ideas on the behavioral finance. The loss aversion indicated that the most adverse investors at the loss prefer the biggest dividends. The investor makes the choice between a future gain and another smaller but immediate. The mental accounting bias indicated that the most patient investors (low beta) prefer the smaller dividends. Finally, the ambiguity aversion bias indicates the most patient investors to ambiguity prefer the biggest dividends “one holds you better than two you’ll have”. If the investor is purely rational like in the classic finance, the behavioral variables should have no influence on dividend policy choices. Further, we conclude that investor sentiment play an important role to influence their decision to demand or not dividend from managers of the firms. Furthermore, the investors’ preference for dividend should be related also to some institutional environment and market development situation factors controlling the market sentiment. Besides, from our table we conclude the absence of relationship between our institutional factor and catering proxies of dividend. According to La Porta et al. (2000), countries with higher levels of legal protection have lower dividend payout ratios than countries with low levels of legal protection. Fuller and Goldstein (2011) they suggest that investors prefer dividends more in when the market sentiment is low, because managers tend to cater more in poorly performing market. Neves (2014), indicate in their study from fifteen countries around the world that the institutional context plays a key role in explaining managers’ catering behavior and consequently firms’ dividend policy. Pascareno and Siringoringo (2016), Vindha Priya and Mohanasundari (2016), showed that that corporate dividend policy needs more focus as a modern and flexible way of developing market performance. Vieira (2011) indicates that investor sentiment has some influence on the market’s reaction to dividend announcements. With regard to dividend policy, Sherfrin and Statman (1984) present a framework for explaining investors’ preferences for dividends versus capital gains based on the theory of self-control. Thaler and Shefrin (1981) choice under uncertainty theory of Kahneman and Tversky (1979). Their theory suggests that some investors may be required to pay a premium for companies that pay cash dividends because of psychological considerations of self-control, the desire to be placed in a separate portfolio, and the desire to avoid or reduce the feeling of regret. Ghosh (1993) offers an explanation of the dividend policy pursued on the basis of regret theory. This theory implies that when individuals are faced with a risky choice among several strategies, they express regret (pride) if the result of the chosen action is inferior (superior) to the results associated with the neglected strategies. This feeling of regret or pride is perceived only in the case where the chosen action represents a deviation from the conventional or standard practice observed. In this context of dividend policy, Ghosh (1993) shows that the feeling of regret or pride that manifests after the decision of the managers, implies a reluctance of the latter to reduce or eliminate the distributed dividend. Twenty years later, the work of Baker and Wurgler (2004a) gave new impetus to this research. These authors develop the idea of a dividend demand in case of strong equity value, especially because the markets are depressed; in this case, the shareholders have the feeling of winning something, or losing less. Baker and Wurgler (2004b) confirmed this hypothesis, called “catering incentives”, in the United States. Companies pay dividends when demand is strong, meaning when investors value companies that pay in a “depressed” or “bearish” market environment. Li and Lie (2006) enriched previous results, focusing on increases and reductions in dividend payments by companies. They note in particular that the decision to change the amount of the payments depends on the demand of the investors, and the market premium which results from the payment of dividends. Denis and Osobov (2008) examine the dividend policy on time series data for six financial markets developed between 1982 and 2002. They test Baker and Wurgler (2004a) catering theory, and show that “Common law” (Canada and the United Kingdom) - where shareholders put strong pressure on managers - the theory of catering is verified but in a very insignificant way, while in the countries of “civil law” (Germany, France and Japan) - where the shareholders exert a weak pressure on the leaders - the theory of satisfaction is rejected and finds no empirical validation. The importance of our topic is to offer a new explanation of the catering theory of dividend proposed by Baker and Wurgler (2004). Moreover, related to Fuller and Goldstein (2011) suggest that investors prefer dividends more in a bearish (poorly performing) market than in bullish (performing) markets. Investors demand more dividends when their sentiment is negative. Our objective is to examine the importance of the market situation and investors sentiment on the decision to demand dividend from firms.",4
45,3,Journal of Economics and Finance,11 January 2021,https://link.springer.com/article/10.1007/s12197-020-09537-1,The role of institutional investors in pension risk transfers,July 2021,Mary McCarthy,Elisabeta Pana,Andrew Weinberger,,Female,Male,Mix,,
45,3,Journal of Economics and Finance,02 October 2020,https://link.springer.com/article/10.1007/s12197-020-09528-2,Currency risk exposure and the presidential effect in stock returns,July 2021,Samar Ashour,David Rakowski,Salil K. Sarkar,Female,Male,Male,Mix,,
45,3,Journal of Economics and Finance,19 February 2021,https://link.springer.com/article/10.1007/s12197-020-09536-2,Determinants of defined-contribution corporate pension adoptions in Japan,July 2021,Yutaka Horiba,Kazuo Yoshida,,,Male,Unknown,Mix,,
45,3,Journal of Economics and Finance,26 February 2021,https://link.springer.com/article/10.1007/s12197-021-09538-8,Additionality of government guaranteed loans for SMEs in Israel,July 2021,Tzameret H. Rubin,Nir Ben-Aharon,,Unknown,Male,Unknown,Male,"The Israeli Government Loans Foundation (GLF) for small and medium businesses (SMEs) has been implemented in its current format since April, 2012, by the Ministry of Economy and Industry and the Ministry of Finance, with a number of changes made in 2016. The Fund’s main objective is to assist SMEs with economic justification to access a loan that can support its growth potential in terms of sales and employability. Assisting those businesses is conceivable by providing Government guarantees (to reduce the risk that banks take upon themselves) and reducing the costs of the bank’s financial assessment. The Government’s guarantees for a business loan remove a critical barrier for businesses which are economically viable, but for various reasons, are considered risky and therefore, cannot raise capital from commercial banks. A secondary objective of the Foundation is to contribute to improving the business loans structure by transferring part of the credit from short-term to medium-term (more than three years). The Foundation provides a credit committee which decides loans applications. The committee is composed of one bank representative, one GLF representative and one representative of the public. The committee is advised by a co-ordination body in its role to supervise the execution of the whole process of the loan from the application stage, through providing the loan, to managing the payments. This co-ordination body also provides the committee with a financial and market review of the business, with a recommendation of whether to provide the loan for the applicant’s business. Businesses which apply for a GLF loan must obtain the co-ordinating body’s recommendation and the credit committee’s recommendation for the loan amount. In the second stage, the businesses approach the bank with GLF guarantees for the specific credit conditions. Each loan can amount to up to NIS500,000 for businesses with a turnover of up to NIS6.25 Million per annum, and up to 8% of total turnover for businesses with NIS6.25 – NIS100 Million turnover. The loan is provided for a period of five years, with an additional half a year’s grace period. Typically, a business with turnover of up to NIS25 Million would pay a loan commission of 1%; 1.5% for a business with NIS25-NIS50 Million turnover, and 2% for businesses with turnover above NIS50 Million. The banks would also pay the Government a commission. Businesses which applied for a refinancing loan, or for investment purposes, would need to provide guarantees for up to 25% of the loan. Early stage businesses are required to have only 10% guarantees for loans up to 300,000, while the Government will guarantee 85% of the loan. For bigger loans, guarantee terms are similar for larger and established businesses. In Israel, like in many countries, SMEs play an important role in the economy. In 2015, SMEs accounted for 99.5% of 518,135 businesses and 61.1% of the business sector employees (Small Medium Business Association (SMBA) 2015). The Small and Medium Business Association (SMBA) definition for an SME is a business with 20 to 99 employees. Using the OECD definition for SMEs which includes companies with up to 249 employees, the results show they account for an even larger share of the economy, with SMEs accounting for 99.8% of all employer businesses in Israel, 68.7% of the business sector workforce and 62.3% of business economy value added (OECD 2016) (Table 1).
 In Israel, bank credit is the main instrument used to fund SMEs and over 80% of SME credit is derived from this source. The remaining 20% tends to come from sources such as suppliers, institutes, funds, private credit companies, angel investors, VC funds, credit card companies and the emerging crowd funding platforms (Small Medium Business Association (SMBA) 2015). In general, the leading source is credit card companies which, until recently, were owned by the commercial banks. VC funds target a small number of high-tech companies. Crowd funding is not yet sufficiently developed to be considered a significant source. However, in 2017, the Government’s ‘Economic Committee’ approved regulation allowing crowd fund raising platforms to not be listed as credit rating companies, aimed to ease their ability to operate (‘The Knesset’, The Israeli Parliament website, June 2017). The banking sector provides half of total business credit (411 out of 800 Billion ILS in 2015). Unlike large businesses which receive half of their credit from non-banking sources, SMEs depend mostly on bank credit. There is information asymmetry between SMEs and the banking system which causes difficulties with risk assessment and providing adequate loans. According to SMBA, the private sector reacted to the public sector initiative to significantly increase Government guaranteed loans from 27 to 257 ILS millions, and from 2007 to 2016, the outstanding business loans for SMEs rose by 52% from 169,300 to 256,820 NIS millions accordingly, and their share of total outstanding business loans rose from 40.9% to 61.6%. The main purpose of this paper is to estimate the additionality of the GLF for SMEs. In particular, estimating the rate of businesses that were eligible for GLF loans which would not have been offered a loan by commercial banks, or at least, would not have received the credit conditions which the GLF offered. A second purpose of this paper is to examine the loan condition improvements that the businesses received because of Government guarantees for their loan. Our examination includes not only businesses which took a GLF loan, but also, businesses that were guaranteed a loan but decided to retrieve their application from the GLF and apply for a loan from a commercial bank, trying to match the loan Terms and Conditions (T & C). By capturing this, we can demonstrate that the GLF scrutiny process for SME financing and business plans has provided Indirect Additionality for SMEs. This is the first attempt to evaluate the GLF performance and we hope it will give policy makers a better, more concrete picture of the programme on a micro business level.",
45,3,Journal of Economics and Finance,20 March 2021,https://link.springer.com/article/10.1007/s12197-021-09545-9,Sukuk and bond spreads,July 2021,Faruk Balli,Hassan Ghassan,Essam H. Al Jeefri,Male,Male,Male,Male,"The increase in the need of Islamic finance leads new financial instruments. From debt financing perspective, sukuks are introduced since early 2000s as an alternative for bonds.Footnote 1 There is a greater international recognition of sukuk that the total value of global sukuk outstanding is expected to reach $395 billion by the year of 2020 (Reuters 2016). The Islamic investment market allows to a Shariah-compliant financial institution to issue and offer sukuk. The increasing demand for sukuk has engendered a prosperous Islamic investment market financial system. While, the oil price volatilities can negatively impact the Islamic investment markets, but it still be that, for instance, the Gulf Cooperation Council (GCC) countries can decide for sukuk to achieve their economic development plans. According to Reuters sukuk market outlook (Reuters, sukuk revival 2018), the historical trend of global sukuk issuance indicates a revival in the issuing value from the third quarter 2017, due partly to the latest Saudi Arabia international and domestic sovereign sukuk issuances. In 2014 the Malaysian sukuk issuances reached $26.4 billion and $38.9 billion in 2016 (Reuters 2017, pages 54–55), indicating that the Malaysian sukuk market was and remains the highest ever value of recorded issuances.Footnote 2 In 2016, the UAE was among the top countries with $6.9 billion of issued sukuk after Malaysia and Indonesia with $38.9 and $14.3, respectively. The sukuk issuance grows quickly because the investors prefer to invest in the Islamic financial market that allows to own a part of the corporate asset. Generally, in comparison to sukuk, the issuance of bonds may be less sensitive to economic growth and market liquidity indicators. The decrease in international sukuk transactions is mostly due to economic uncertainties such as decrease in the economic growth, the devaluation in currency, inflation and crisis in the global financial system. According to IIFM 2016, Reuters 2017 (page 11) the global sukuk market record huge growth as the sukuk issuances augmented from around $1 billion (2001) to $138 billion (2013) and around $345 billion (2016). Sukuk can be specified as an issuance within the purview of Islamic investment markets. It permits to proclaim a share in the investment project and its value at maturity would indicate the current market value, while bonds represent a nominal debt that the issuer has to pay upon maturity. According to the Islamic financial services board (IFSB 2009, pages 3, and 20), sukuk are “certificates with each sakk representing a proportional undivided ownership right in tangible assets, or a pool of predominantly tangible assets, or a business venture”. Sukuk certificates, corresponding to mechanisms of liquidity management, set up an original asset-based securities (ABS) structure. The success of sovereign sukuk issues incite the corporate issuances. They cover over 90% of the total volume of global sukuk issuances (Tariq and Dar 2007). The main features distinguishing sukuk from bonds are the prohibition of a fixed interest payments and of excessive uncertainty in transactions, arbitrage, and extreme speculation (Aloui et al. 2015). In December 2005, IFSB issued its capital adequacy standard for institutions offering Islamic financial services (IIFS). Exposure to risks of IIFS may be similar to that of the conventional securitization. The risk exposures of sukuk are from various sources: risks of the originator or obligor, default risk of the issuer i.e. special purpose entity (SPE), bankruptcy managed by SPE’s instruments. Also, for investors there is liquidity and return risk in sukuk secondary markets.Footnote 3 This is because investment-account holders (IAH) expect returns as by a float return rate benchmark, and impairment of assets even if there is no negligence of the lessee (for more details see IFSB 2009, pages 6 and 7; Tariq et al. 2007). Our paper fills the literature gap on the topic of the sukuk yield spreads (such as: Fathurahman and Fitriati 2013; Safari et al. 2013; Rauf and Ibrahim 2014; Ayturk et al. 2017; Hassan et al. 2018; Saad et al. 2019), through our focus on the yield spread by comparing the sukuk yield to bond yield through their main determinants using weekly data from 2013 to 2018 of 123 firm sukuks (corporate). Such difference is a sign of the risk premium for investing in sukuk over bonds. Following the bibliometric review of Paltrinieri et al. (2020), our paper would contribute to the literature on the sukuk yield spreads. By definition, the greater risk, an asset class is, the greater its yield spread. We explore the GCC countries, Turkey, Malaysia, Indonesia and Singapore bond markets in a broader sense to observe how the global conventional bond and sukuk markets affect the countries’ financial markets. We investigate the sukuk and bond spreads taking into account the global economic policy uncertainty (EPU) index, money price and stocks index in the US. To test the effects of global shocks on sukuk and bond markets in each country, we use the data of financial and non-financial firms at weekly level. Mainly, we investigate the reactions of the sukuk and bond spreads to global risk factors. For the sake of minimizing the structural disparities between sukuk and bonds, we build sukuk and bond spreads database and other main determinants by collecting data from the same companies. We find that bonds are more integrated with the global markets and they react to global uncertainties including like economic policy uncertainty, volatility index of stocks (VIX) or US interest rates more significantly, compared to sukuk. By using the quantile analysis, we observe that for higher or lower bond spreads the results do not change. However, for higher sukuk spreads we detect that sukuk react more to the global uncertainties. In Section 2 of the paper, we present a detailed literature review that focuses on the yield spreads to exhibit our contribution to such literature. Section 3 presents data and points out the methodology used. Section 4 displays and discusses some empirical findings. Section 5 concludes.",7
45,3,Journal of Economics and Finance,22 February 2021,https://link.springer.com/article/10.1007/s12197-021-09539-7,Is fund performance driven by flows into connected funds? spillover effects in the mutual fund industry,July 2021,Bing Zhu,René-Ojas Woltering,,,Unknown,Unknown,Mix,,
45,3,Journal of Economics and Finance,13 April 2021,https://link.springer.com/article/10.1007/s12197-021-09548-6,Correction to: Is fund performance driven by flows into connected funds? spillover effects in the mutual fund industry,July 2021,Bing Zhu,René-Ojas Woltering,,,Unknown,Unknown,Mix,,
45,4,Journal of Economics and Finance,25 May 2021,https://link.springer.com/article/10.1007/s12197-021-09555-7,Capital structure adjustment behavior of listed firms on the Mexican stock exchange,October 2021,Carlos Omar Trejo-Pech,NyoNyo A. Kyaw,Wei He,Male,Unknown,,Mix,,
45,4,Journal of Economics and Finance,27 February 2021,https://link.springer.com/article/10.1007/s12197-021-09542-y,"Discipline, risk, and the endogeneity between financial decisionmaking and health",October 2021,Stefani Milovanska-Farrington,Stephen Farrington,,Female,Male,Unknown,Mix,,
45,4,Journal of Economics and Finance,14 May 2021,https://link.springer.com/article/10.1007/s12197-021-09550-y,The clientele effect around the turn of the year: evidence from the bond markets,October 2021,Vladimir Kotomin,,,Male,Unknown,Unknown,Male,"Researchers have studied calendar-time regularities in capital asset prices at least since the 1970s. The most thoroughly examined of these regularities is the turn-of-the-year effect, sometimes separated into the December effect and the January effect. Abnormally high January returns following abnormally low December returns were first detected in small-cap US equities (Rozeff and Kinney 1976; Keim 1983; Reinganum 1983; Roll 1983). In bond markets, the turn-of-the-year effect has been found in high-yield corporate bonds (Chang and Pinegar 1986; Cooper and Shulman 1994; Maxwell 1998). This study is the first one to examine the turn-of-the-year effect in the high-yield municipal (muni) bond market. With about $0.5 trillion in outstanding securities, high-yield munis is a large but understudied market. Being understudied aside, why would such an investigation be of interest? Professional investors and arbitrageurs surely are aware of the forces that can cause the turn-of-the-year effect in various asset classes because numerous research papers have been published on this phenomenon. Building on an argument advanced by Cochrane (1999), unless a predictable return pattern that appears anomalous reflects risk, it should not persist after being uncovered because arbitrageurs can trade against it. Consistent with it, McLean and Pontiff (2016) find that stock return predictability diminishes after publications of research identifying predictable return patterns. However, the aforementioned research focuses on stock markets, which are much more liquid than most bond markets. Within the bond markets, many US municipal bond issues are less liquid than typical corporate bonds because they are issued by small local governments and trade very infrequently. And within the muni markets, high-yield (speculative-grade) munis are even less liquid than investment-grade issues of similar size and maturity. This is because many institutional investors – banks, various investment funds, insurance companies – face limitations when investing in below investment-grade securities. Overall, muni bonds are illiquid, opaque, and dominated by highly tax-sensitive investors – high-net-worth households. These market characteristics, discussed in detail in Section 3, make arbitrage difficult. In addition, the higher credit risk of high-yield munis, compared to their investment-grade counterparts, increases the potential for and size of capital losses. Households engage in December tax-loss selling more than institutions (Sias and Starks 1997; Lynch et al. 2014). Based on limited arbitrage opportunities, high tax-sensitivity of muni investors, and greater potential for capital losses for high-yield munis, I propose and test the following three hypotheses: Hypothesis 1: Municipal bonds (both investment-grade and high-yield) are more likely to exhibit elements of the turn-of-the-year effect – abnormally low December returns, abnormally high January returns, or both – than corporate or Treasury bond indices; Hypothesis 2: The turn-of-the-year effect is stronger in high-yield municipal bonds than in investment-grade munis; Hypothesis 3: The turn-of-the-year effect in municipal bonds is stronger following years in which yields increase (and thus capital losses are more likely). To test these hypotheses, I examine monthly returns of U.S. Treasury, Credit (corporate bond), High-yield Corporate, Muni, and High-yield Muni Bloomberg Barclays bond indices over the 2004-2020 period. The contribution is two-fold. First, I find a strong turn-of-the-year effect – low returns in December followed by high returns in January –in the high-yield muni index. Its average monthly total return in January is 1.5 percent higher than in December, an economically and statistically significant difference. This difference is greater in years in which the index incurs capital losses over the first eleven months – its yield increases between January and November preceding the turn of the year. The Muni index, dominated by investment-grade bonds, has a similar but less pronounced turn-of-the-year effect – its December returns are not abnormally low even in years with capital losses, while its January returns are abnormally high regardless of the preceding yield changes. The High-yield Muni is the only one of the five indices whose December returns are negatively correlated with the yield change in the first eleven months of the year, highlighting tax sensitivity of high-yield muni investors. Two studies examine the turn-of-the-year effect in municipal bonds. Starks et al. (2006) find abnormally high January returns in muni bond closed-end funds but not in the underlying municipal bond index. Chen et al. (2011) find that the Bloomberg Barclays Municipal Bond Index returns in January are economically but not statistically higher than returns in other months in 1990–2009. They show that net flows of muni bond mutual funds tend to reverse from negative in December to positive in January. The present study finds a similar turn-of-the-year pattern in mutual fund flows for both investment grade and high-yield muni mutual funds. Both Starks et al. (2006) and Chen et al. (2011) focus on investment-grade munis. The second contribution is the finding that the previously documented depressed December returns in high-yield corporate bonds (Chang and Pinegar 1986; Cooper and Shulman 1994; Maxwell 1998) have disappeared, or at least do not show up at the index level. Moreover, the High-yield Corporate Bond Index exhibits abnormally high December returns over 2004-2020. The December return is even higher when the index experiences a capital loss (i.e., an increase in yield) over the first eleven months of the year. The investment-grade corporate bond index has abnormally high December returns in years with capital losses. These results suggest that corporate bond investors exhibit contrarian behavior in December and highlight the differences between muni and corporate bond investor clienteles. Highly tax-sensitive households, who dominate the muni market, engage in December tax-loss selling, depressing December returns, while institutional investors that collectively dominate the corporate bond market – insurance companies, mutual funds, pension funds, ETFs – appear to search for opportunities to acquire undervalued securities prior to the year-end. The paper proceeds as follows. Section 2 reviews the turn-of-the-year effect literature. Section 3 provides details about the municipal bond market. Data are described in Section 4, and empirical results are presented and discussed in Section 5. The last section concludes.",
45,4,Journal of Economics and Finance,04 March 2021,https://link.springer.com/article/10.1007/s12197-021-09541-z,Fixed income mutual fund performance during and after a crisis: a Canadian case,October 2021,Laleh Samarbakhsh,Meet Shah,,Female,Unknown,Unknown,Female,"Mutual funds are portfolios of various financial securities selected by professional fund managers. These funds ensure that the risk factor of their portfolios matches the risk preferences of the investors. A fixed income balanced fund must invest a minimum of 70% of total net assets in Canadian equity and fixed income securities. Of that 70%, fund managers must allocate no more than 40% of assets into equity holdings. As this portfolio combines both equity and debt securities, it is intended for investors with low-to-medium risk preferences. As all mutual funds aim to yield positive returns on investments, mutual fund managers try to use strategies that can outperform standard indexes over specific periods. Various equity funds include other index funds or a combination of small/large capitalization stock. By contrast, debt funds include corporate or government bonds as well as fixed income and gilt funds.Footnote 1 The strategies vary according to the individual risk preferences. Investors around the world have different beliefs about risk. Risk-seeking investors invest in riskier assets with a higher betaFootnote 2 than the market, whereas risk-averse investors may invest in safer securities with a lower but guaranteed rate of return (e.g., GICs). As part of a diversification strategy, managers often invest in different asset classes and different geographic locations to reduce the overall risk of the portfolio, often referred to as “systematic risk.” The literature has analyzed the contributions made by various parameters of international funds, such as asset and fund size, management expenses, strategies, investment performance, and fees (Ippolito 1989; Cumby and Glen 1990; Droms and Walker 1994). Many U.S. mutual funds that invest abroad deal extensively with equity funds in developed nations but also invest a lesser share in transitional economies because of their higher default risk (Kaminsky et al. 2001). Investing in international mutual funds offers a practical way to diversify and earn the highest risk-adjusted return (Droms and Walker 1994). The mutual fund industry averaged positive returns during the 2008 financial crisis (Bello 2008). The crisis affected the lives of many individuals around the world due to the collapse of the stock market, the burst of the housing bubble, the increased unemployment rates, and the significant drop in interest rates (Helppie 2011). Researchers were led to conclude that specific predatory trading techniques contributed to the crisis. Nevertheless, investors still managed to invest their funds in different markets domestically as well as internationally to diversify their risk and boost portfolio returns. Research dating back to the inception of mutual funds has examined a variety of angles concerning the mutual fund industry, from those who manage the funds (Aggarwal and Jorion 2010) to fund performance with respect to market indexes (Cumby and Glen 1990, Blake et al. 1993, Leite and Armada 2017). While research on equity funds has been accumulating, there remains little practical or academic knowledge on debt funds, such as fixed income funds in Canada. Fixed income funds are investments that pay a fixed rate of return on government bonds, corporate bonds, and other grade bonds. Debt funds have been growing at a higher rate than equity funds.Footnote 3 Various global events have impacted financial markets and the lives of many, the most recent being the COVID-19 pandemic. Zenteno and Aquino (2020) claim that balanced and fixed-income funds suffered the most amongst all fund classes. Based on that observation, this study poses the following research question: How did the financial crisis impact the performance of Canadian fixed income funds? This question can help determine how fixed income funds respond to crisis periods. Institutional investors can use this research to focus on allocation strategies during moments of distress. Determining how fund flow varies for fixed income funds throughout recessionary periods allows this study to offer a good indication of the economic environment and depict levels of variation in returns that provide an accurate snapshot of the debt funds in the industry. Having clearer insight into fixed income funds in crisis periods allows investors, practitioners, and fund managers to construct portfolios that minimize risk during economic downturns. Identifying performance variations in a specific segment of the mutual fund industry requires that we first understand the overall nature of the mutual fund industry, so that we can better assess how the recession impacted the funds under examination. The government of Canada weathered the global financial crisis because it prevented top banks from engaging in risky behavior. Due to such policies and the creation of strict regulatory reforms, banks in Canada were spared a total market collapse. For example, in the United States, the collapse of the Lehman Brothers was due primarily to mortgage-backed securities depreciating below their book value, which dried up liquidity in financial markets. Unlike in the United States and Europe, the Canadian economy was not at risk of insolvency. Our research focuses on Canadian mutual funds—specifically, fixed-income mutual funds—which are presumably protected from endogenous crisis factors due to government measures designed to help banks endure the crisis, leading to the stability of the economy during the post-crisis period. This study examines the impact of flow-induced mutual funds trading in the fixed income market to see how changes in fund size differ between the pre- and post-crisis periods. The rest of the paper is organized as follows. Section 2 reviews the related literature. Section 3 describes the methodology used to conduct the empirical analysis. Section 4 examines the study’s dataset and interprets the results of the empirical analysis. Finally, Section 5 concludes the paper by discussing the results and avenues for future research.",
45,4,Journal of Economics and Finance,02 April 2021,https://link.springer.com/article/10.1007/s12197-021-09544-w,Dynamic volatility spillover and network connectedness across ASX sector markets,October 2021,Ki-Hong Choi,Ron P. McIver,Sang Hoon Kang,Male,Male,,Mix,,
45,4,Journal of Economics and Finance,21 May 2021,https://link.springer.com/article/10.1007/s12197-021-09549-5,Empirical analysis of bitcoin price,October 2021,Yuanyuan (Catherine) Chen,,,Unknown,Unknown,Unknown,Unknown,,
45,4,Journal of Economics and Finance,03 June 2021,https://link.springer.com/article/10.1007/s12197-021-09551-x,COVID-19 internet vaccination information and vaccine administration: evidence from the United States,October 2021,Rajeev K. Goel,Michael A. Nelson,,Male,Male,Unknown,Male,"The fight against the COVID-19 pandemic has recently seen a glimmer of hope with the arrival and regulatory approval of a number of vaccines around the world. Many nations, especially wealthy nations, have started vaccinating their populations, using various guidelines to prioritize access to this scarce resource (Goel and Nelson 2021; Persad et al. 2020). Even within nations, such as states across the United States, regulations and vaccination delivery to populations vary. A common reason cited has been a lack of supply chain and administration logistics, coupled with the unique characteristics of some vaccines (e.g., short shelf life, transportation at certain temperatures, etc.) that has challenged the pace of vaccine delivery (Baldwin and Weder di Mauro 2020; Farzanegan et al. 2020; Goel et al. 2020, 2021). Yet, there are two sides to effective delivery and the demand-side bottlenecks of vaccine delivery have been ignored. There have only been anecdotal pieces of evidence of eligible recipients not showing up for their vaccine appointments, leading to waste or arbitrary (out of turn) vaccinations. The issue of lagging vaccine administration has profound implications for controlling the pandemic, saving lives, and return on public investments (since COVID-19 vaccine development was paid for by taxpayer dollars in the United States).Footnote 1 This paper tries to address the shortcomings on demand-side vaccination bottlenecks by examining the role of the internet. In general, the internet has been a game-changer in many markets, impacting the demand and supply not only of information but also related transaction costs (see Goel and Hsieh 2002). The internet is being used as a key, often primary, means to provide information about the coronavirus, vaccine availability and safety information, and information about the scheduling and delivery (the last mile of the vaccination process). Two unique questions addressed in this research are: Does greater information on the internet about COVID-19 vaccine access increase the number of vaccines administered? Does greater information about COVID-19 vaccine reliability facilitate vaccine administration? To address these questions, we use internet search results (total number of search hits) on Google across U.S. states for a recent month (February 2021), with the following alternative keywords: search A (vaccine access search): “state name COVID-19 vaccine coronavirus appointment schedule location provider”, with “COVID-19 vaccine” being the exact phrase in the search search B (vaccine reliability search): “state name COVID-19 coronavirus adverse health effects vaccine side illness”, with “COVID-19” being the exact phrase in the search. These searches capture the relevant information available on the internet regarding the availability and reliability of coronavirus vaccines. Since Google’s search algorithm lists the sites most accessed first,Footnote 2 there is some inherent quality distinction in the searches (also see appendices A and B). These internet searches yielded information by government agencies, public health groups, international organizations, industry groups (health and insurance providers), special interest groups, etc. Appendices A and B show the top 25 hits for searches A and B, respectively. Although the searches yielded hundreds of thousands of hits (see Table 1), the samples of top search hits in both cases show useful and pertinent information that would empower potential consumers and seekers of vaccines. Since internet search results can change rapidly, all the searchers for a set of keywords were done in a single sitting, i.e., 2-3 hour window with no breaks in the last week of February 2021 (Search B) and the first week of March 2021 (Search A). With some similarities in the names of some states, a couple of adjustments were made to avoid double-counting. These included: (i) in the case of the state of Virginia, taking out the word “West” to exclude sites with West Virginia; and (ii) in the case of the state of Washington, excluding the words “DC” and “District of Columbia” to avoid overlap with Washington, DC. In spite of these refinements and the case about the timing of searches, internet searching remains imperfect, especially for the general public and non-computer specialists. For example, there could be mirror sites where the same information is displayed on multiple sites (hosted by the same or different organizations), fake sites selling dubious products with text/title relevant to coronavirus to generate hits/traffic, etc. Nevertheless, the application of internet searches to gauge information/awareness used in this paper is novel in the context of coronavirus vaccines. This methodology, however, has been successfully used in other contexts (Goel et al. 2012; Goel and Nelson 2014). One of the first applications in economics was by Goel et al. (2012) to measure cross-country corruption awareness. Thankfully, the language disparities experienced across different nations are less of an issue when searching across states in the United States (also see Section 3.3 for a consideration of the language aspect).Footnote 3 The estimation results showed that greater availability of the relevant information on the internet increased vaccine administration rates, and this was true for both types of internet searches. In contrast, the diffusion of internet access and the digital divide across states did not have a significant impact on vaccination rates. The structure of the rest of the paper includes the model and data in the next section, followed by results and conclusions.",10
45,4,Journal of Economics and Finance,17 June 2021,https://link.springer.com/article/10.1007/s12197-021-09557-5,Patience is a virtue: exploiting behavior bias in gambling markets,October 2021,Kevin Krieger,Justin L. Davis,James Strode,Male,Male,Male,Male,"Market efficiency is an integral underlying assumption in sports gambling markets. It is generally accepted that these markets operate similar to traditional financial markets in that information asymmetry will be reflected in prices as informed and uninformed actors ultimately create equilibrium (Sauer 1998). Similar to traditional financial markets, there is a constant search by investors (bettors) to gain informational advantages and exploit existing inefficiencies. One of the main streams of literature in the area of sports gambling markets focuses on this identification of market asymmetry or other informational advantages. Findings in this stream of literature have been mixed, at best, with most studies identifying short-term inefficiencies or historical inefficiencies that cannot be practically applied. A common practice in examining sports betting markets is studying whether contextual factors can lead to a profitable betting strategy. Studies have explored ideas such as the impact of home-field advantage (Dare and Dennis 2011), travel distance for teams (Nichols 2014), and even the weather (Borghesi 2007), to name a few. Shank (2019) found that the home-field advantage is weaker in intradivision games and point-spreads are set such that informed bettors can profit accordingly. Continuing research in this area seeks to identify new or unexplored situations that might provide profitable betting opportunities. A second stream of research in this area has focused more on betting strategies that are independent of contextual factors but, instead, identify market trends and inefficiencies in price/line setting or how other individual behavioral variables influence bettor decision-making. For example, Paul and Weinbach (2012) found profitable betting opportunities by using broad betting approaches such as simply betting against big favorites. Their study identified a trend in point-spread line setting that was inefficient. Davis et al. (2015) looked at how bettor knowledge of teams from the previous season in the NFL can lead to early season betting market inefficiencies. Paul and Weinbach (2005) found that, in general, bettors have a clear preference to bet the over in totals markets across all sports examined in their study. Studies such as these are representative of a growing body of research embracing the factors of psychology and cognition and how these can influence bettor decision-making and, ultimately, betting market efficiency. Typically, such strategies involve acting quickly to exploit available inefficiencies. In contrast, we seek to identify how these psychological and cognitive biases can result in exploitable market asymmetries for the patient and informed bettor. The current study extends research in this stream by examining how certain heuristics and biases an influence betting behavior, resulting in a novel exploitable market inefficiency. Specifically, we consider how recent performance in NFL regular season games influences many bettors. We consider that recent impressive (unimpressive) performance by a team might cause some bettors to overreact due to recency bias. We find support for this conjecture and, surprisingly, we find bettor perception often helps create greater inefficiency in the days leading up to a game.",2
45,4,Journal of Economics and Finance,13 March 2021,https://link.springer.com/article/10.1007/s12197-021-09543-x,Religiosity and corporate risk-taking: evidence from Italy,October 2021,Richard J. Cebula,Fabrizio Rossi,,Male,Male,Unknown,Male,"Recently, some studies have emphasized the role that social values, local culture, and religious beliefs play in explaining economic growth, corporate decisions, and organizational behavior, including agency costs, dividend policy, and corporate risk-taking (e.g., Chintrakarn et al. 2017; Diez-Esteban et al. 2019). Guiso et al. (2006, p. 23) define culture “… as those customary beliefs and values that ethnic, religious, and social groups transmit fairly unchanged from generation to generation.” La Porta et al. (1999) find that, beyond the institutional factors, there are other cultural factors, such as religiosity, that can influence corporate behavior. McGuire et al. (2012) find that religiosity can discourage managers from engaging in unethical business practices. Du (2013) and Chintrakarn et al. (2017) find that religious piety substitutes effectively for corporate governance in alleviating the agency problem. Furthermore, Diez-Esteban et al. (2019) find that religiosity has an impact on corporate risk-taking. Using a sample of 155 public firms based in Italy, for 2,382 firm-year observations, over the period 2000–2016, this empirical study investigates for the case of Italy the relationship between corporate risk-taking and local culture, as measured by religiosity. The study contributes to the existing literature in two ways. First, Italy has a history of being a locus of people with strong religious beliefs and arguably thereby provides us to test a “natural environment” for investigating the potential impact of religiosity on corporate risk taking. Indeed, the population of Italy has been especially strongly associated with the Catholic religion and its highest authorities as represented by the Papacy for the Roman Catholic Church. Catholic Social Teaching (CST) takes the position that a firm is a ""common good"" and has the right to earn a profit. Nevertheless, the owners must manage the firm using ethical behavior. Catholic Social Teaching (CST) recognizes the firm's goal to profit on real investments based on the right risk without highly speculative financial activities. Therefore, religiosity actually could have a strong impact on corporate risk-taking behavior in Italy. We analyze data from ISTAT (The National Institute for Statistics) on the percentage of the population aged 6 years or older that attended a formal religious congregation at least once a week over the previous 12 months in the region where the firms’ headquarters are located. On average, data indicate that approximately 30% of the Italian population attend such religious congregations. Therefore, we hypothesize that Italian firms' religious beliefs are likely to affect firms' organizational behavior and be an essential factor in explaining an aversion to extensive corporate risk-taking. Second, this study examines a single bank-based French Civil Law country that offers less overall shareholder protection and fewer creditor rights (La Porta et al. 1999) than typically provided in Common Law countries. Italian companies have a higher ownership concentration than typically found in other countries, and it is in theory potentially easier for the largest shareholder(s) to extract private benefits of control from the minority stockholders. Indeed, several previous empirical studies find that agency costs are more severe for Italian firms and that the probability of expropriation of wealth to the detriment of minority shareholders, through firms’ debt levels, pyramidal groups, dividend policies, and dual-class shares, is higher than other countries (e.g., Faccio et al. 2010; Dyck and Zingales 2004). Previous empirical studies also find that religious piety substitutes effectively for corporate governance in alleviating agency issues in countries other than Italy (Chintrakarn et al. 2017). While a growing body of literature has investigated the relationship between religiosity and risk-taking in US and other countries, to our best knowledge few studies have examined civil law countries and no study has investigated Italy. Therefore, the goal of our paper is to measure if there is a relationship between religiosity and corporate risk-taking in Italian listed firms. Following other studies (e.g., Harjoto and Rossi 2019; Rossi et al. 2019), we hypothesize that firms located in more religious areas exhibit lower levels of risk-taking. This study consists of four more Sections. Section 2 discusses related literature and develops the hypotheses, while Section 3 describes the data sample and survey methodology. Section 4 provides the empirical results and a discussion of the findings, whereas Section 5 provides a brief summary of the estimation findings and the conclusions of this study.",3
45,4,Journal of Economics and Finance,14 April 2021,https://link.springer.com/article/10.1007/s12197-021-09547-7,Information asymmetry in fire insurance: a frontier approach,October 2021,Donald F. Vitaliano,,,Male,Unknown,Unknown,Male,"The notion that the insured party may possesses relevant information regarding insurable risk, which information is unavailable to the insurer, is known as asymmetrical information. Relevance refers to the fact that the insurer would alter the terms of the insurance contract if it was also in possession of this information. Two types of information asymmetry are adverse selection and moral hazard, both of which have long been facts of life in the insurance industry. The former refers to the risk category of the insured, and the latter refers to his risk behavior. In the case of fire insurance, explosive or inflammatory materials might be used on the insured premises without knowledge of the insurance company (adverse selection).Footnote 1 Moral hazard might involve carelessness in preventing fires, such as failure to regularly remove accumulated rubbish that might catch fire. The modern academic interest in information asymmetry dates from the influential paper by Rothschild and Stiglitz (1976). Although highly stylized, their model leads to several inferences about potential insurance ‘market failure’ that have gained currency in economics, the legal profession and amongst policymakers. Siegelman (2004) discusses the various ways in which the theory of asymmetrical information has influenced policymaking, the courts and the path of economic research. Among the predictions of the theory is that insurance may be rationed or unavailable (the famous ‘death spiral’ effect), or that no market equilibria may be exist. All of the potentially bad outcomes are predicated on the insured having superior information vis a vis the insurer. This paper applies the stochastic frontier regression model to fire insurance losses incurred by 275 stock companies in 1917. This data set is noteworthy because it reports losses, premiums and related information for each company across 48 states, two territories and the District of Columbia, with almost six thousand observations. In spite of the ubiquity of fire insurance, there are no published empirical papers related to information asymmetry. And the use of the frontier model is a novel application of this modern econometric technique.",
46,1,Journal of Economics and Finance,11 June 2021,https://link.springer.com/article/10.1007/s12197-021-09554-8,Does economic policy uncertainty matter to explain connectedness within the international sovereign bond yields?,January 2022,Noureddine Benlagha,Wael Hemrit,,Unknown,Male,Unknown,Male,"There has been growing interest in analysing spillover and dynamic connectedness across international financial markets, especially after the emergence of the US subprime mortgage and European sovereign debt crises (Meegan et al. 2018; Kim et al. 2015; Jung and Maderitsch 2014). In these studies, much attention has been placed upon how the financial crises affected dynamic spillovers among international financial markets, and their findings have suggested a significant increase in spillovers during a period of financial turmoil. However, despite the importance of sovereign bonds for institutional investment portfolios, and for individual investors, a review of the existing literature revealed few studies that have investigated the spillovers and connectedness among this particular asset class. Spillover effects are highly relevant to regulators, financial professionals and investors investing in portfolios consisting of sovereign bonds. Handler and Jankowitsch (2018) suggest that sovereign bonds represent the most directly affected financial instruments and understanding their price reactions offers significant insides, enriching the results presented for stock and option markets. Prior studies that have identified shocks transmission between bond markets, which generally focus on the effects of the benchmark term structure of interest rates on bond risk premia, spread the first moment and assume a non-informational interaction between sovereign bond volatilities (Cepni et al. 2019; Presbitero et al. 2016a). Another branch of literature has relied exclusively on isolated studies of target counties and regions or a very small group of economies, most of which have operated under very special circumstances. Accordingly, they did not take into consideration the very serious multilateral linkage between countries. As a result, this can lead to weak predictive looseness and robustness of empirical testing. This study, therefore, sets out to assess the dynamic spillovers and connectedness among sovereign bond markets of the G7 countries (US, Canada, UK, France, Germany, Italy and Japan) over the period from January 2015 to December 2019. As opposed to most of the previous work that focuses on the exploration of the aggregated spillovers among markets, we employ the methodology of Diebold and Yilmaz (2012) to investigate the dynamic net connectedness among the considered bond yields. To assess the sensitivity of dynamic connectedness for a specific investment horizon, we also investigate simultaneously the spillovers among bonds that take two years and thirty years to mature. In addition to attempting to measure the degree of connectedness and their sensitivity to time horizons, this paper examines how macroeconomic factors such inflation rates, the real interest rate and the economic growth influence the dynamic of net connectedness among the considered sovereign bond yields. Historically, research investigating the factors associated with dynamic spillovers between assets has focused on the standard macroeconomic variable (see, for example, Capelle-Blancard et al. 2019; Vácha et al. 2019; Costantini et al. 2014; Ghosh et al. 2013; Benlagha 2020). Unlike these studies, further to the standard macroeconomic factors, this paper pays special attention to the impact of EPU on the patterns of dynamic connectedness between the G7 sovereign bond yields. During the last several decades, the world has become full of uncertainty as a result of financial crises, wars and the current COVID-19 pandemic. Against this background, spillovers and connectedness have increased sharply among several assets. Thus, a natural question is raised: does economic policy uncertainty (EPU) affect the dynamic connectedness between sovereign bond yields? To the best of our knowledge, this paper is the first to offer and answer to the preceding inquiry. The need to understand and measure the effects of uncertainty on economic policy and the receiving country’s characteristics on the net connectedness across sovereign debt markets is an important topic in finance research. Decisions that rely on this understanding include whether to take advantage of arbitrage opportunities, whether to combine hedging operations or whether to share risks rather than sharing a ‘common’ sovereign bond. Understanding the determinants of connectedness can help to predict changes in Sovereign Bond Yields (SBY), which can affect government and borrowing costs and, consequently, affect the financial sector. Not only does EPU affect interest rate levels, but it also explains the level and shape of the term structure of bond yield volatilities. Analysts and investors alike place great value in the yield spread. Investors think that EPU leads to a worsening deficit by putting bond yields under pressure in the short term and believe they can get a higher return on investment with a two-year bond than with a thirty-year bond market (Leippold and Matthys 2015). Thus, it is important to identify both near-term aspects (e.g., when the government adjusts its policy rate and regulates the issuance of government bonds) and longer-term aspects (e.g., how to implement entitlement programs). In this respect, the main objective of this study is to bridge the literature examining the impact of EPU with the literature on spillovers between sovereign bond markets at various maturities in countries around the world. First, we explore the dynamic patterns of connectedness between sovereign bond yields of the G7 countries (the US, Canada, France, the UK, Germany, Italy and Japan). We employ the methodology proposed by Diebold and Yilmaz (2009). Second, we assess the influence of EPU and several macroeconomic variables on dynamic net spillovers and net connectedness between bond yields of the selected countries. Empirically, we estimate and analyse several panel data models by regressing the net connectedness of each sovereign bond yields on macroeconomic variables affecting bond yields, namely, the inflation and interest rate along with EPU index. The remainder of the paper is structured in the following way: Section 2 reviews the literature. Section 3 describes the data and the summary statistics. Section 4 presents the models and the estimation method. Section 5 presents and discusses the empirical results. Section 6 concludes.",6
46,1,Journal of Economics and Finance,14 June 2021,https://link.springer.com/article/10.1007/s12197-021-09552-w,Stock market reaction and adjustment speed to multiple announcements of accounting restatements,January 2022,Kyung-Chun Mun,,,Unknown,Unknown,Unknown,Unknown,,
46,1,Journal of Economics and Finance,28 June 2021,https://link.springer.com/article/10.1007/s12197-021-09556-6,Anatomy of intraday volatility at the Chilean stock exchange,January 2022,A. Can Inci,Andres Ramirez,Hakan Saraoglu,Unknown,Male,Male,Male,"It is important to study the market microstructure of a stock market as it can provide investors with an understanding of how specific trading mechanisms have an impact of the price formation process (O’Hara 1995, p.1). Motivated by this premise, we examine intraday volatility as a measure of the quality of price discovery on the Chilean Stock Exchange and make use of a policy decision by market regulators on mechanism design to evaluate the relationship between a recently introduced closing call auction and the observed intraday volatility patterns.Footnote 1 In today’s world of fast algorithms, advanced technology for financial transactions, artificial intelligence, fintech environment and high-speed trading, the only way to understand the efficiency of an equity exchange is to examine the market microstructure, intraday price movements and intraday volatility in that exchange. As Nowak et al. (2011) has documented, the price discovery and volatility dynamics analysis of financial markets including those in Chile require intraday data as the immediate reaction of markets to new information can be discovered only at such high frequencies. Accordingly, our primary motivation of our study is to analyze the market efficiency characteristics of one of the three largest stock exchanges in Central and South America, the Santiago Stock Exchange. Our investigation provides guidelines to different types of investors about (1) whether to invest in the Chilean stock market or not from an efficiency point of view, (2) what time of the day would be best for trading purposes, given the different risk aversion characteristics of different investors, and (3) whether the exchange regulators’ initiatives, such as the January 2019 closing call auction to improve the exchange efficiency, are justified. This is the first study to our knowledge that addresses these issues for the Chilean stock market. These perspectives altogether constitute the motivation of our study. Founded in 1893, the Chilean Santiago Stock Market (Bolsa de Comercio de Santiago) is one of the fastest growing equity markets in the world. It is currently the second largest in South America after that of Brazil, the third largest in Latin America behind the Brazilian and Mexican stock exchanges, and 30th largest in the world. The current total market capitalization is roughly around US$235 billion with about 270 companies traded at the exchange (as a comparison, Argentinian stock market has a market capitalization of about US$40 billion). Open economy policies and numerous bilateral trade agreements have helped Chile’s economy to take significant strides in growth, and these have been reflected in the growth and popularity of the Santiago Stock Exchange. The data related to trade executions, price movements, and company related financial statements have become widely available, including the recent availability of intraday tick prices of the stocks traded at the exchange. Intraday price movements and intraday volatility of the stocks are very important to understand the risk and efficiency of a stock exchange. Heightened volatility is an indication of uncertainty and market price deviations from the true values of the stocks. Mispricings may lead to discrimination of markets participants and to unfair and unethical treatment of small investors. Low volatility, on the other hand, is indicative of a stable and efficient trading environment with fairFootnote 2 and accurate prices. Consistent patterns in intraday volatility of stocks traded at a stock exchange would inform investors about how efficient the market is and when would be a better time to trade during the day. Therefore, it is important to understand how volatility evolves over the trading day. In this paper, we examine the characteristics of intraday volatility at the Santiago Stock Exchange. There is a rich literature on intraday volatility patterns in developed country stock markets and numerous studies on emerging country stock markets. However, this is the first paper, to our best knowledge, which investigates the intraday volatility at the Chilean Stock Market. Using a long sample period from June 2010 to January 2019 and focusing on the most popular and highly liquid stocks, we document consistent patterns of accentuated volatility during the opening minutes and the first half-hour. We provide explanations for the volatility configurations, provide evidence of the evolving and increasing nature of volatility over time, and comment on the appropriateness of the policies implemented by exchange regulators to keep intraday volatility in check. All our results and interpretations for the Chilean stock exchange, which are unique contributions to the literature, can serve as a leading guide for the other regional economies and financial sectors in Central and South America. The rest of the paper is organized as follows. Section 2 reviews the literature on the market microstructure of intraday volatility. Section 3 presents the data, the characteristics of the Chilean stock exchange, and the methodology. Section 4 provides discussion of the results. Conclusion follows.",
46,1,Journal of Economics and Finance,30 June 2021,https://link.springer.com/article/10.1007/s12197-021-09553-9,The impact of policy uncertainty on the M&A exit of startup firms,January 2022,Carmen Cotei,Joseph Farhat,Indu Khurana,Female,Male,Female,Mix,,
46,1,Journal of Economics and Finance,05 August 2021,https://link.springer.com/article/10.1007/s12197-021-09558-4,Fuelling fire sales? Prudential regulation and crises: evidence from the Italian market,January 2022,Alessandro Leardi,,,Male,Unknown,Unknown,Male,"In April 2020, the banking supervision of the ECB announced a temporary reduction of capital requirements for market risk to respond to the extraordinary levels of volatility recorded in financial markets since the outbreak of the coronavirus (Covid-19). The decision is aimed at smoothing pro-cyclicality and maintaining banks’ ability to provide market liquidity and to continue market-making activities (ECB – European Central Bank, 2020). The policy stance of the supervisor was clear; however, no reference to theoretical model or empirical evidence was published along with the decision. From a policy point of view, the opinion about regulatory measures based on risk management techniques (such as VaR) has changed over time. In 2006, the Chairman of the Federal Reserve Bank (Fed), Ben Bernanke, highlighted the importance of modern risk management as a central element of good supervisory practice and encouraged the industry to push forward the risk management frontier (Bernanke 2006). Just a few years later, in 2010, after the beginning of the financial crisis, Janet Yellen, Chair of the Fed from 2014, said that “methods of modern risk management may have intensified the cycle…because of their reliance on metrics such as value at risk that are highly sensitive to recent performance, especially volatility. In good times, volatility declines, and value at risk along with it. This pattern generated a pro-cyclical willingness to take on risk and leverage, amplifying and propagating the boom and bust cycle. The vicious cycle of a collapse of confidence, asset fire sales, evaporation of liquidity, and a deleveraging free fall was the mirror image of the manic mortgage market that preceded it” (Yellen 2010). Hence, in the four years between Bernanke’s speech and Yellen’s one, the opinion on risk management techniques, including the use of quantitative measures of risk such as VaR, changed radically. From crucial methods for economic stability and the main contributors to the decline of volatility (Panetta et al. 2006), they became a quasi-evil mechanism of depression which contributed to the worsening and deepening of the crisis (Bernanke 2008, Financial Stability Forum 2008, Senior Supervisors Group 2008). From the theoretical side, academic literature has underlined the pro-cyclical effects of VaR constraints, which amplify the impact of shocks and affect market volatility (Adrian and Shin 2010, 2013, Danielsson 2010). Furthermore, Danielsson et al. (2001) claimed that the use of value at risk could have induced crashes when they would not have otherwise occurred. For some researchers, the main channel of transmission of the amplification effect, in a VaR-constrained framework, is the adjustments of the expected returns and covariances of the investors and the related increase to risk aversion caused by the VaR constraint (Danielsson et al. 2004). From the empirical point of view, as mentioned above the evidence of the pro-cyclical effects of capital requirements for market risk is poor and is in favour of the existence of the amplification effect described in the theoretical literature (Adrian and Shin 2010). This work provides empirical evidence of pro-cyclicality of market risk prudential measures, by using a unique dataset of value at risk (VaR) values for Italian banks. In details, I show empirically that the increasing tightness of the value at risk constraint amplifies the instability of the financial market; therefore, the recent decision of the ECB to reduce capital requirement for market risk goes in the proper direction of containing pro-cyclicality. To the best of my knowledge, no empirical proof of this pro-cyclical relation has been given so far. The main difficulties of such empirical proof are related to data availability and possible reverse causality or endogeneity in the analysis of data. About the former, to measure the tightness of the VaR constraint I use a dataset of daily VaR and VaR limits of Italian banks, retrieved from supervisory reporting data. For the latter, to take into account any potential endogeneity of the data, a vector autoregression model (VAR) without contemporaneous components has been employed. In addition, the measures of banks’ VaR (based on historical worst 1% of losses) and market risk (volatility of returns) have been based on two neatly distinct variables (historical simulation VaR versus standard deviation of returns). Lastly, I do not use as a variable a simple measure of market risk, such as VaR, but a measure of tightness of the regulatory constraint (“VaR ratio”, which is the ratio between VaR and internal risk limit), which has low correlation with the market risk measure (volatility of the stock exchange index). With this framework in mind, I find that market volatility has a significant, positive relation with lagged VaR ratio (one lag), in line with the amplification effect described in literature. Furthermore, I find also evidence of the overshooting effect described by Shin (2010), since the VaR ratio at lag two has a negative impact on market volatility. Despite the relevant number of theoretical articles on risk management and pro-cyclicality, this is the first empirical investigation that uses jointly internal data from bank and financial markets to prove the existence of the macro-impacts of VaR. The results provide additional keys to interpret the recent reaction of the European supervisor to the pandemic crisis, further suggestions to study the unintended consequences of individual regulatory instruments, and evidence of the mechanism that takes place when a measure used to control and contain the risk of individual financial agents has an impact on the whole system and on market variables (such as prices, returns and volatility), via the homogenization of behaviours. The article is organized as follows: Sect. 2 provides a literature review, of theoretical and empirical articles, Sect. 3 explains the main methodological choices, Sect. 4 describes the data used, Sect. 5 presents the major results and proves their robustness. At last, Sect. 6 concludes.",1
46,1,Journal of Economics and Finance,28 August 2021,https://link.springer.com/article/10.1007/s12197-021-09560-w,Multifrequency network for SADC exchange rate markets using EEMD-based DCCA,January 2022,Anokye M. Adam,Kwabena Kyei,Emmanuel N. Gyamfi,Unknown,Unknown,Male,Male,"The Article 3 of the Africa Union Constitution adapted from the Article II of the disbanded Organisation for African Unity Charter has a bigger agenda for economic and financial integration with the goal is to introduce a common currency for Africa by the year 2021. To developing regions, economic and financial integration are seen as a panacea to harnessing resources and capabilities of individual countries toward economic development (Kreinin and Plummer 2002). This is corroborated by Jefferis (2007) who classified benefits of economic and financial integration into four main areas. Firstly, it provides an “agency of restraint” that will reduce the ability of governments to pursue irresponsible and destabilising macroeconomic policies. Secondly, economic integration act as a bulwark against currency speculation and contagion effects that could add to exchange rate volatility. Thirdly, it supports the exploitation of economies of scale in the financial sector, with accompanying efficiency benefits. Lastly, it exploits the traditional optimum currency area (OCA) benefits, i.e., the potential gains to trade from reduced transaction costs and exchange rate uncertainty, a net of potential losses resulting from reduced national policy autonomy and constrained ability to react and adjust to economic shocks. The economic and financial integration process in Africa was envisioned to follow a two-way approach in which each economic community has been encouraged to form a monetary union as financial integration with intention to bring to together to form Africa-wide financial integration with single currency (Alagidede et al. 2008). This has caused several economic blocs vigorously pursuing currency union of which the Southern African Development Community (SADC) is no exception (Adam et al. 2021). Southern African Development Community (SADC), the largest regional economic grouping in Africa, has vigorously pursued an integration agenda with the aim of becoming a monetary union with a common currency. The formation of SADC was aimed at promoting regional cooperation and integration, economic growth, socio-economic development, and durable peace and security among its member states. The SADC has over the years been successful in promoting regional peace and security and economic development for the betterment of the SADC region’s most important resources—its people. These include the negotiation of vital government reforms and peaceful transition of political power in Lesotho; resolution of the border dispute between Zambia and the Democratic Republic of Congo; mobilization of resources to address energy shortages that threaten regional development and economic integration; and many others. Exchange rate market integration is a particular aspect of the broader issue of financial integration needed for coordination of policy indicators among member countries of a monetary union to achieve a stable monetary union as postulated by the Optimum Currency Area (OCA) hypothesis (Mundell 1961; McKinnon 1963 and Kenen 1969). Accordingly, exchange rate markets integration in various economic communities have been studied and cited as a key indicator for stable monetary union (Adam et al. 2010; Coulibaly and Gnimassoun 2013; Zehirun et al. 2015, 2016). The readiness of SADC to form a monetary union has been studied within the OCA hypothesis and conclusions from these studies suggest that SADC as a whole is not ready yet (Tipoy 2015; Kumo 2011; Zerihun et al. 2014). Rose (2008) posits that OCA convergence can be achieved ex-post than ex-ante, however, macro-economic coordination is required. Fritz and Mühlich (2010) corroborated this view and assert that uncoordinated macroeconomic policies in south-south economic integration have been a root cause of unsuccessful attempts towards monetary integration. The exchange rate has been cited as central to economic activity, as it affects and is being affected by all other policies, making policy coordination and harmonisation essential for the success of a common currency (Zehirun et al. 2015). In this regard, several studies have delved into the implications of exchange rate coordination on the possible monetary union in SADC area (Khamfula and Huizinga 2004; Agdeyegbe 2009; Zehirun et al. 2015; Asongu et al. 2015; Zehirun et al. 2016). Putting Rational Expectation Theory (RET) and the Efficient Market Hypothesis (EMH) under one umbrella and based on the assumptions of the EMH and RET, participants in the exchange rate market act rationally and homogeneously. This is because EMH suggests that market prices reflect all available information, news and events that come to the market are normally distributed leading to a lack of asymmetry of information (Fama 1970). However, Shiller (2000) argues that most market participants are not smart but rather follow ‘trends and fashion’ in their decision making. Therefore, the participants of the exchange rate market (speculators, central banks, dealers, individuals, etc.) are heterogeneous with different information, objective interest and investment behaviour as explained by the Heterogeneous Market Hypothesis (HMH) (Müller et al. 1993). Thus, the price and data generation of the exchange rate are mixed and noisy. In addition, the physical measurements of exchange rate data have been found to suffer from one or more of the following problems: short data span, nonstationarity, nonlinearity, and long memory (Xu et al. 2016; Ferreira et al. 2019), limiting its usage in research and practice. This implies that the use of symmetric models in analysing exchange rate data could lead to spurious results and conclusion. The introduction of empirical mode decomposition (EMD) by Huang et al (1998) presents a new way of analyzing nonlinear and nonstationary data. EMD method is intuitive, direct, posteriori and adaptive. EMD performs a time-adaptive decomposition of a complex signal into elementary, almost orthogonal components that do not overlap in frequency. By decomposing a time series into a small number of independent and concretely implicational intrinsic modes based on scale separation, EMD explains the generation of time-series data from an alternative perspective. This would be an improvement in the analysis of exchange rate market data over detrended fluctuation analysis (DFA) (Stošić et al. 2015; Ferreira et al. 2019) and wavelet transform (Owusu et al. 2017; Meng and Huang 2019) employed in recent literature. As useful as EMD is, it suffers from the problem of mode-mixing. The ensemble empirical mode decomposition (EEMD) proposed by Wu and Huang (2009) correct the issue of mode-mixing. Exchange rate markets integration has been studied using several approaches, with the use of correlations and cointegration tests probably being the most common. As noted by Pereira et al. (2019), the evolution of methodology and data availability has led to multiple types of studies, with linear and nonlinear methodologies but also in different countries and regions. The commonest among these methods are the coreeletion based approaches such as dynamic correlation analysis (Engle 2002), asymmetric dynamic correlation (Cappiello and Engle 2006; Toyoshima et al. 2012; Tamakoshi and Hamori 2013), cross-correlation function (Cheung and Ng 1996; Nakajima and Hamori 2012) and detrended cross-correlation analysis (DCCA) (Podobnik and Stanley 2008). The DCCA has become the most extensively adopted methods to measure cross-correlation among nonstationary financial time series. In this paper, we propose an EEMD-based DCCA model to build a multifrequency network of exchange rate markets in SADC at different frequency scales. The DCCA is based on Zebende (2011) method to investigate the cross-correlation power laws between two simultaneous time series, called Detrended Cross-Correlation Analysis (DCCA). The EEMD-based DCCA model provides two innovations in examining exchange coordination over the existing studies. First, it offers the opportunity to understand the extent of cross-correlation at different frequency scales. Second, it delineates the influence of noise from the cross-correlation of the exchange rate market to provide information on fundamental independence. The study contributes to the literature on exchange rate dependencies by introducing a new approach to the analysis of multifrequency interdependence. The findings allow us to gain new insight into the cross-correlations of exchange rate markets. The analysis of the high-, medium- and low frequencies together with the residue and the original series show that the observed series mimic the behaviour of the high frequency. The results from the residue, representing the deterministic trend, showed that SADC countries’ long-run economic fundamentals are linked. These findings suggest the possibility of currency union formation in SADC, albeit policy direction, to address the difference in business cycle synchronization. The innovation of this paper is to combine EEMD with DCCA to study the multifrequency cross-correlations of exchange rate markets, which can provide policymakers a deeper understanding of the dynamics of exchange rate markets toward the formation of currency unions. The rest of the paper is structured as follows. Section 2 presents the literature review; Sect. 3 introduces the methods employed in the study ad Sect. 4 describes the exchange rate data of SADC used in the study. Section 4 presents the results and analysis, Sect. 5 highlights the policy implications and concludes in Sect. 6. Following Rose (2008) proposition that group of countries proposing to form a currency union need not meet the OCA convergence criteria ex-ante but can be achieved ex-post with considerable macro-economic coordination, several studies have emerged to assess the readiness of various economic groupings from this perspective. According to Zehirun et al. (2015), exchange rate is key and linked to all economic activities, making its policy coordination and harmonisation essential for the success of a common currency. This corroborates the assertion of Inci and Lu (2004) that the exchange rate is sensitive to many economic factors such as money supply, inflation rates, economic growth rates, and trade variables in domestic and foreign economies. Owing to this, a number of studies seeking to assess the readiness of the regional bloc. have focused on the coordination of exchange rates across different blocs (see Mai et al. 2018; Owusu Junior et al. 2017; Abdalla 2012; Baig 2001; Baxter and Stockman 1989; Ghosh et al. 2002; Hsing 2007; Lin 2012; Orlov 2009; Reboredo and Rivera-Castro 2013). Mai et al. (2018), for example, employed a correlation network to analyse the exchange rate among Asian currencies. Owusu Junior et al. (2017) on the other hand employed wavelet analysis to examine exchange rate coordination in the West African Monetary Zone. Within the SADC region, Khamfula and Huizinga (2004), Agdeyegbe (2009) Asongu et al. (2015), and Zehirun et al. (2014, 2015, 2016) have examined the extent of exchange coordination and implication for monetary union. Zehirun et al. (2014, 2015, 2016) employed panel cointegration, unit root and pool mean group to examine the coordination of exchange rates in SADC and made interesting conclusions about the readiness of SADC for the single currency. Similarly, the GARCH framework and other frequency invariant methods have been utilized to understand the extent of exchange rate independence in the SADC area (see Khamfula and Huizinga 2004); Asongu et al. 2015). Unfortunately, these techniques are unable to reveal the dynamic structure of the data and cover the frequency domain of comovements associated with exchange rate markets. The nature of exchange rate data is such that using earlier methodologies is insufficient for a detailed understanding. We need to decompose the exchange-rate data into various frequencies to fully appreciate the dynamic interdependence of the data. Bailliu and King (2005) emphasised the need to use a model that can help economists to extract better high-frequency signals about the economy from apparently noisy exchange rate movements to provide a well-specified model of exchange rate movements over all time horizons.",10
46,1,Journal of Economics and Finance,20 September 2021,https://link.springer.com/article/10.1007/s12197-021-09563-7,Inflation and growth: the role of institutions,January 2022,Hakan Yilmazkuday,,,Male,Unknown,Unknown,Male,"Price stability is the main concern of monetary authorities (e.g., see Fischer (1996)), although benefits of economic growth are much larger than those of eliminating macroeconomic instability (e.g., see Lucas (1987)). Therefore, knowing the relationship between inflation and growth is essential to have an optimal balance between monetary and growth policies. The theoretical literature provides mixed evidence on this subject, where the effects of inflation on growth have been explained by using the positive relationship between capital accumulation and economic growth. Studies such as by Tobin (1965) have shown that additional money can be used as substitute for capital and thus higher inflation results in higher capital accumulation (and growth), whereas studies such as by Stockman (1981) have shown that higher inflation results in less money to purchase capital goods and thus lower capital accumulation (and growth). Other studies have shown alternative mechanisms through which inflation can hurt growth; e.g., McKinnon (2010) has considered the reducing efficiency of the financial system due to high inflation, Tommasi (1994) has considered the diminishing value of acquiring information with inflation, and Beaudry et al. (2001) have considered increasing relative price variability with inflation that results in investment misallocation. To contribute more to this discussion, superneutrality of money implies that there is no relationship between inflation and growth (e.g., see Sidrauski (1967) or Lucas (1973)). Evidence provided by the empirical literature is also mixed. While studies such as by Fischer (1993) have shown a negative relationship between inflation and growth, those by Levine and Renelt (1992) or Ericsson et al. (2001) have challenged the existence of such a relationship. This paper contributes to this discussion by investigating the causal relationship between inflation and per capita income growth. This is achieved by using the implications of a structural vector autoregression (SVAR) model, because it is an important tool to capture causal relationships in a time-series framework as indicated in studies such as by Rubin (1974) and Imbens (2014). The main advantage of using a SVAR model in a time-series framework (compared to the cross-sectional or panel models in the literature) is having a structure identifying shocks that can be interpreted as the randomly-assigned treatments to capture the dynamic causal effects on each variable of interest (e.g., see Stock and Watson (2018)). In technical terms, these dynamic causal effects are captured by impulse response functions, and they are robust to the consideration of endogeneity by construction. The empirical investigation is achieved for 36 countries over the period between 1970-2017, where control variables such as trade openness, financial development and government size are also used. Since the estimations are achieved for each country individually, the initial conditions of countries (e.g., initial human capital, initial development, initial institutions, etc.) are also controlled for (by estimated constant terms). All variables are represented as moving five-year averages to focus on the long-run relationship between inflation and income growth. The estimation results are further used to estimate the inflation elasticity of growth over time, which is defined as the cumulative response of growth divided by the cumulative response of inflation, both following an inflation shock. In order to investigate the effects of inflation on growth, we consider alternative inflation elasticities of growth measured one year, five years, ten years and twenty years after an inflation shock. Such an approach (especially when longer horizons are considered) corresponds to having a long-run investigation between inflation and income growth. The estimated inflation elasticity of growth measures are highly heterogeneous across countries, providing evidence for significantly positive, significantly negative or insignificant relationships between inflation and growth. Consistent with the mixed evidence suggested by the literature (as discussed above), it is implied that the effects of inflation on growth depend on the country investigated. To have an explanation for this heterogeneity across countries, in a secondary analysis, we investigate the relationship between country-specific measures for inflation elasticity of growth and country-specific strength of institutions. The corresponding results show that the effects of inflation on growth are negative and significant in countries with stronger institutions, whereas they are positive and significant in countries with weaker institutions. Regarding the economic intuition behind the results, on one hand, the positive effects of inflation on growth for countries with weaker institutions are consistent with studies such as by Porta et al. (1998) who have shown that weak institutions can result in poorer access to direct capital; therefore, additional money (and thus higher inflation) can be used as substitute for capital in these countries as suggested by Tobin (1965). Such a positive effect, for example, can be achieved through borrowing of governments from their (non-independent) central banks in countries with weak institutions to increase real investment (e.g., see Cukierman et al. (1992)). The negative effects of inflation on growth for countries with stronger institutions, on the other hand, are consistent with studies such as by Jung and Marshall (1986) who have shown that inflation can hurt growth due to political power of urban workers in countries with strong institutions, where governments can impose price controls to fight against inflation that would lead into shortages and thus lower growth. The rest of the paper is organized as follows. The next section introduces the estimation methodology and data. Section 3 depicts the empirical results. Section 4 discusses the results by connecting them to the existing literature. The last section concludes by also depicting the corresponding policy implications.",1
46,1,Journal of Economics and Finance,08 October 2021,https://link.springer.com/article/10.1007/s12197-021-09562-8,Advertisement-financed credit ratings,January 2022,Heidrun Hoppe-Wewetzer,Christian Siemering,,Female,Male,Unknown,Mix,,
46,1,Journal of Economics and Finance,08 September 2021,https://link.springer.com/article/10.1007/s12197-021-09561-9,An empirical test for bubbles in cryptocurrency markets,January 2022,George A. Waters,Thuy Bui,,Male,,Unknown,Mix,,
46,1,Journal of Economics and Finance,16 October 2021,https://link.springer.com/article/10.1007/s12197-021-09564-6,Structural Change in the Investment Function,January 2022,Russell E. Triplett,Nilufer Ozdemir,Paul M. Mason,Male,Unknown,Male,Male,"Business investment was notoriously sluggish in the aftermath of the Great Recession, and although recent indicators suggest a long-awaited uptick in investment growth, sustainability questions remain. Given the importance of investment as a contributor to aggregate demand, and its critical role in fostering productivity growth and fueling innovation and technological progress on the supply side, many economists and policy makers have identified strong growth in business investment as central to a comprehensive economic recovery (e.g., Fernald et al. 2017). Arguably, the tax policies of the Trump administration were aimed at stimulating greater business investment in the U.S., though recent evidence on the effectiveness of similar tax changes is not encouraging (Yagan 2015). The theoretical and empirical literature on business investment has a long history, but there is relatively little recognition that the determinants of investment have changed over time in fundamental ways. Using quarterly data from 1953–2016, we estimate an augmented accelerator function of U.S. business investment and present evidence of a structural change in the key determinants in the year 2000. Splitting the sample, we then show that in the most recent time period investment growth has become more volatile, and that the accelerator and crowding-out effects have weakened, while the cost of capital, risk spread, cash flow and economic policy uncertainty have grown more important. Finally, we highlight and evaluate several recent hypotheses regarding structural changes in the U.S. macroeconomy in light of our estimation results. In particular, we argue that our results are not consistent with the secular stagnation hypothesis, but are more closely connected to long-term trends in demographics, market concentration, financialization, and the inter-industry reconfiguration of firms away from traditional manufacturing.",1
46,2,Journal of Economics and Finance,07 January 2022,https://link.springer.com/article/10.1007/s12197-021-09559-3,Asymmetric Impact of Oil Price Changes on Stock Prices: Evidence from Country and Sectoral Level Data,April 2022,Sujata Saha,,,Female,Unknown,Unknown,Female,"The link between oil price and stock prices is of interest to both investors and researchers because it plays an important role in the development of an economy through investment and business decisions. Stock prices fluctuate daily and due to the interrelated impacts of globalization, it is important to explore the several macroeconomic determinants of stock prices. Variation in oil price is one of the important factors that affect stock prices. Oil price is known to have both positive and negative effects on stock prices depending on the type of the firm. Oil price crisis leads to an increase in oil price volatility. Jones and Kaul (1996) emphasized the cash flow hypothesis to explain the relationship between oil price and stock prices. There could be a few channels to explain a negative relationship between the two variables. Firstly, oil is used as one of the major inputs for production of several goods. Increase in oil price raises the cost of production of these firms which in turn, reduces the future cash flows of the firms. This leads to lower earnings, less dividends and hence, a decrease in stock prices. The second channel could be that a higher oil price in the present can lead to an overestimation of a higher oil price in the future. This could lead to a decrease in the discounted future cash flows which causes a decrease in stock prices. A positive relationship between stock prices and oil price can also exist. Sometimes investors associate increase in oil price with the growth of an economy which leads to an increase in stock prices. This is true for an oil-rich country since increase in oil price attracts investment to the country. If increase in oil price raises confidence in businesses, then it might lead to an increase in stock prices in the future. There are a few other macroeconomic variables such as exchange rates, money supply, inflation, economic activity, interest rates, risk premia, etc. which affect stock prices as well. Theoretically, the value of a firm’s stock should be equal to the expected present value of the firm’s future cash flow, and the future cash flow is dependent on the performance of the firm. Furthermore, the performance of the firm is dependent on the changes in different macroeconomic variables of a country. Hence a change in the macroeconomic variables could potentially affect stock prices. One of the earliest studies to explore the relationship between economic activity and oil prices is Hamilton (1983). The author found that the seven recessions in the U.S. between 1948 and 1980 were preceded by a spike in the price of crude oil. Increase in the price of crude oil appeared to be linked to strikes by oil, coal, and steel workers and the price of coal itself. There is a negative correlation between economic activity and price of oil. This downturn in economic activity can lead to a decrease in stock prices. Huang et al. (1996) considered the interest rate on one-month T-bills and daily closing prices for oil futures contracts and found no correlation between stock returns and oil futures returns but they found that oil futures Granger-cause stocks. Both studies considered data from the U.S. only. There are other studies which have focused on analyzing the effects of different macroeconomic variables such as exchange rate, money supply, inflation, interest rates, industrial production index, GDP, oil prices, exchange rates, etc. on stock prices. Among the recent ones, Driesprong et al. (2007), Kilian and Park (2009), Degiannakis et al. (2013) and Bastianin et al. (2016) also found relationship between stock prices and oil price. But more recently researchers have shifted the focus to the asymmetric relationship between stock price and oil price. By using monthly data, Sadorsky (1999) found that one standard deviation shock in oil prices has a negative effect on stock prices which is significant for about the first three months following the shock. The author also found that oil price volatility shocks have asymmetric effects on stock returns for the U.S. A strong evidence of effect of oil price on stock price returns was found by Basher and Sadorsky (2006) for emerging countries. Cong et al. (2008) and Park and Ratti (2008) studied the asymmetric effect of positive and negative oil price shocks on stock returns. But none of the above work focused on both the short run and long run asymmetric relationship between oil price and stock price by considering data from both developed and emerging economies. The contribution of this paper is two-fold. First, it studies the asymmetric effect of oil price changes on stock prices by considering both developed and emerging countries. Most of the existing literature has focused on the assumption of symmetric effects of oil price changes on stock prices, which means, assuming a negative relationship, if an increase in oil price hurts the country’s stock prices, then a decrease in oil price will improve the stock prices (with the magnitude of change remaining the same in both cases). But this might not be true always. This is because the amount of an increase in stock prices due to a decrease in oil price might not match the amount of a decrease in stock prices due to an increase in oil price, hence, the effect of changes in oil price on stock prices can be asymmetric. The asymmetry can arise in two forms: asymmetry in terms of the sign; which means that different countries or sectors might respond to increases in oil prices and decreases in oil prices differently, and asymmetry in terms of magnitude; which means that different countries or sectors respond differently to the degree of changes (large or small) in oil prices. To be specific, a decrease in oil price decreases the cost of production (considering oil is used as an input to production) which leads to an increase in profit, and this might increase the stock price of the firm. However, for the same firm, when oil price increases, the cost of production might increase. But in order to maintain the market share, the firm can either keep the price of the goods the same and absorb the increased cost by lowering its profit margin or can increase the price of the goods by a fraction of the increase in the cost of oil (the input good), thereby transferring a little bit of the cost to the consumer, without significantly decreasing the market share. In both cases, profits will decline but the decline in magnitude will not be the same as that of the increase in magnitude as it is the case for a decrease in oil price. Also, an increase in oil price can lead to a more increase in stock prices considering the future growth prospect of an economy from an increased production or, it could be that an increase in oil price signal uncertainty in the future which can cause an increase in demand and an increase in future stock prices. Second, to avoid aggregation bias, this paper considers eleven sectoral stock prices for the U.S. to study how sectors are affected differently due to changes in oil price. The other variables that are considered are nominal effective exchange rate (EX), industrial production index (IPI), consumer price index (CPI) and money supply (MS). Monthly data over the period of 1986 to 2019 is used for Brazil, Canada, Chile, Japan, S. Korea, Malaysia, Mexico, U.K. and U.S. Whereas, to explore the asymmetric effect of oil price changes on different sectors of the U.S., data on the Dow Jones Industrial Average Index, Dow Jones Transportation Average Index, Dow Jones Utility Average Index, NASDAQ Bank Index, NASDAQ Biotechnology Index, NASDAQ Computer Index, NASDAQ Industrial Index, NASDAQ Insurance Index, NASDAQ Telecommunications Index, NASDAQ Transportation Index and PHLX Semiconductor Sector have been considered over the period of 1986 to 2020:M5.Footnote 1 The rest of the paper is organized as follows: Section 2 provides a review of the literature, Section 3 describes the model and methodology, the empirical results are discussed in Section 4 and Section 5 concludes the paper. Data definition, data sources and details on the data period are provided in Appendix 1.",4
46,2,Journal of Economics and Finance,08 February 2022,https://link.springer.com/article/10.1007/s12197-022-09569-9,"Drivers of intermediation costs, financial repression and stability",April 2022,Sadia Afrin,Ilias Skamnelos,Waheduzzaman Sarder,Female,Male,Unknown,Mix,,
46,2,Journal of Economics and Finance,11 November 2021,https://link.springer.com/article/10.1007/s12197-021-09565-5,Learning to trade on sentiment,April 2022,Cuiyuan Wang,Tao Wang,Jane Yihua Rong,Unknown,,Female,Mix,,
46,2,Journal of Economics and Finance,28 January 2022,https://link.springer.com/article/10.1007/s12197-021-09567-3,Tapping of the crowd: The effect of entrepreneur engagement on equity crowdfunding success,April 2022,Sarah Borchers,Lee M. Dunham,,Female,,Unknown,Mix,,
46,2,Journal of Economics and Finance,07 January 2022,https://link.springer.com/article/10.1007/s12197-021-09566-4,Effect of COVID-19 on ETF and index efficiency: evidence from an entropy-based analysis,April 2022,Kunal Saha,Vinodh Madhavan,G. R. Chandrashekhar,Unknown,Unknown,Unknown,Unknown,,
46,2,Journal of Economics and Finance,22 January 2022,https://link.springer.com/article/10.1007/s12197-021-09568-2,Bayesian Estimation of the Hierarchical SLX Model with an Application to Housing Markets,April 2022,Joshua C. Hall,Donald J. Lacombe,James Young,Male,Male,Male,Male,"Hedonic models have been extensively used in real estate and urban economics to value geographical amenities that are associated with a house. One of the most studied features of local housing markets has been the impact of schools, given that every house is embedded in a public school district and the United States has a great diversity of school districts (Fischel 2009). Brasington (1999), for example, uses a hedonic model to explore the measures of school quality that households value. He finds using Ohio data that households are willing to pay a premium for high test score levels and spending per pupil, but not for value-added.Footnote 1 Using Texas data, Dills (2004) finds similar results in that Texas households do not seem to value changes in test scores. In addition to the value of test scores, other studies related to public school district characteristics have focused on race (Dougherty et al. 2009; Brasington et al. 2015), city-school district border congruence (Hall 2017), district size (Brasington 2001), and attendance zone uncertainty (Turnbull et al. 2018). One important issue in papers looking at housing and school districts is that houses are nested within school districts and a standard OLS specification does not take into account the nested nature of the data. On the other hand, hierarchical or multilevel models are designed to accommodate data that are nested in nature. The nesting of data occurs quite naturally in certain settings such as houses that are nested within the same school district. For example, homes in the same school district would be exposed to the same policies of that district, the changes in which could affect all home prices in that same district. Shipan and Volden (2008, 2012) discuss how these local district policies may spillover to neighboring districts. This policy diffusion, thus, should be taken into account as districts can compete, learn and imitate one-another, which could affect home prices nested within these school districts. To this end, we expand the traditional hierarchical econometric model to include spatial spillover effects estimating a hierarchical Spatial Lag of X (SLX) model. Hierarchical spatial econometric model have been used in transportation research (Alarifi et al. 2018), epidemiology (Gu et al. 2018), and ecology (Ver Hoef et al. 2018). Despite their increased use, there are almost no studies that use a hierarchical SLX model, the lone exception being (Lacombe and Flores 2017). We utilize the model of Lacombe and Flores (2017) in our empirical exercise and other details regarding hierarchical spatial econometric models can be found in Lacombe and McIntyre (2017) and Lacombe and McIntyre (2016). There are a number of advantages to using hierarchical linear models and we review some of the more common ones here. The first advantage of hierarchical or multilevel models is that standard estimation techniques such as OLS ignore the nesting of the data and thus violate the independence assumption. For example, homes that are nested within school districts are not likely to be independent of one another and thus OLS would be an inappropriate estimation strategy. Multilevel models take into account this non–independence in the data generating process. Another major advantage of hierarchical or multilevel models is that they allow for the inclusion of separate dummy or indicator variables for each level 2 entity as well as the inclusion of a level 2 covariate. In the current study, we have homes that are nested within each school district. The idea here is that one can include a dummy variable for each school district to control for unobserved heterogeneity and also include a school district level variable in the model. It is well known that this is not possible in a standard fixed effects model that attempts to control for unobserved heterogeneity using indicator or dummy variables. A final major advantage is that the estimates of the intercepts in hierarchical models are “shrinkage estimates” or Empirical Bayes estimates in that the estimates of each intercept is “shrunk” towards the overall mean. The empirical Bayes or shrinkage estimates work as follows. If the number of level 1 observations within an individual level 2 group is small, then the estimate of the intercept for that group will be “shrunk” towards the overall intercept as in a standard OLS non–fixed effects model with a single intercept. On the other hand, if the number of level 1 observations in within a level 2 group is large, the estimate of the intercept will be closer to the estimate one would obtain in a “fixed–effect” regression model. In either case, the estimate of the intercepts are not only informed by the observations within the group but by observations by all other groups as well. Mathematically, this can be expressed as follows (Subramanian 2010; Luke 2004): where \(\hat{\alpha }_j^{NP}\) is the “no–pooling” estimate of the intercept, i.e. the intercept one would obtain if each level 2 group had its own indicator variable (commonly referred to as a “fixed–effects” model); \(\hat{\alpha }_j^{FP}\) is the value of the intercept from a “fully–pooled” model, i.e. a model with a single intercept for all level 2 groups, and \(\hat{\alpha }_j^{EB}\) is the “empirical Bayes” or “shrinkage” estimate of the intercept in the multilevel or hierarchical model, which is a linear combination of the “no–pooled” and “fully pooled” models. The weights assigned to the “no–pooled” and “fully pooled” are given by \(\lambda _j\) and are a function of the level 2 and level 1 error variance as well as the number of level 1 observation in each level 2 unit, i.e. \(n_j\). The main idea is that level 2 groups that have relatively few observation will be shrunk more to the overall intercept while level 2 groups that have many observation within that group will experience less shrinkage in the estimate of their intercepts. Additional details regarding all aspects of hierarchical/multilevel modeling is contained in Gelman and Hill (2006). We proceed as follows. In Section 2 we provide an overview of the hierarchical SLX model. Section 3 describes our data on Ohio homes and school districts, while Section 4 presents our empirical results. Section 5 concludes with a discussion of our results and additional areas where the hierarchial SLX mode can be applied.",
46,2,Journal of Economics and Finance,22 March 2022,https://link.springer.com/article/10.1007/s12197-022-09571-1,Dimensions of size and corruption perceptions versus corruption experiences by firms in emerging economies,April 2022,Rajeev K. Goel,Ummad Mazhar,Rati Ram,Male,Unknown,Male,Male,"This paper adds a new dimension to the empirical determinants of corruption across countries by focusing on selected categories of the “size” of the main determinants. While the literature has considered a number of factors to potentially impact the level and prevalence of corrupt activities and found some of those influences to be more robust than others (see Dimant and Tosato (2018); Lambsdorff (2006); Seldadyo and de Haan (2006); Serra (2006); Treisman (2000, 2007)), the impact of several major types of the size of these has received scant attention. In our context, size has different types or categories. It can be related to the size of firms who are asked to pay bribes (see, related, Beck and Maher (1989)), the size of the government where the firms or respondents affected by corruption operate, and the size of localities where firms are situated. All these could be potentially related to the prevalence of corrupt activities. The prevalence of corruption is examined for both corruption perceptions and corruption experience. These categories of size differ along a number of fronts, perhaps most pertinent in the present context is their ability/propensity to change in the short term. For instance, while the firm size is endogenous to firms, their location would be less so, especially when substantial sunk costs are involved – as is generally the case for firms in the manufacturing sector. On the other hand, government size is largely exogenous for respondents (bribe payers) in the short term, and prone to change gradually even by policymakers. These qualitative dimensions of size could have different impacts on corruption. Larger firms might be better equipped to negotiate bribe solicitations, but may be “sitting ducks” for corrupt officials with their relative inability to move to less corrupt areas. Large firms may be able to devote greater resources to lobbying, which has nexus with corruption (Campos and Giovannoni (2007); Goel (2014)). Further, large firms may be visible targets for rent-seekers, but they might also have greater media focus, which might act as a deterrent (Dutta and Roy (2016)). While large firms have traditionally been the targets of antitrust regulations, especially in mergers and acquisitions, their spillovers on corrupt activities do not seem to figure prominently in regulatory considerations. The role of the government is multifaceted, with implications for various institutions and corruption. Government size might be related to bureaucratic rent-seeking on the one hand, and better governance on the other (Rose-Ackerman (1999)). Bureaucratic red tape increases corruption, while greater vigilance mitigates it. Large cities might attract corrupt, rent-seeking, bureaucrats to seek postings there due to relatively lower transaction costs of finding sources of rent (potential bribe payers). There might, however, be some countervailing, corruption-reducing, forces at work in large cities – for instance, there might be greater media scrutiny and disproportionate policing in large cities. On the other hand, patronage politics in cities makes corruption more likely by insulating politicians from voter displeasure, but the ability of the tax base to exit the city provides some constraints on rent-extraction. We consider whether these different dimensions of size have similar effects on corruption. Other contributions of this work relate to a comparison of the determinants of corruption perception and corruption experience and using data from a large survey of firm-level respondents. There is some debate about what exactly do the aggregate, country-level corruption indices, such as the Corruption Perceptions Index by the Transparency International, capture (Donchev and Ujhelyi (2014); also see Goel and Nelson (2011)). Our firm-reported indicators of corruption perceptions and corruption experience can be used to shed light on this point. Corruption perceptions can be seen as potential entry barriers for firms, whereas corruption experience resembles transaction costs (see Belousova et al. (2016); Olken (2009)). Not all respondents who perceive corruption to be high might have had a first-hand experience with it. Furthermore, local and global perceptions of corruption might differ (Liu and Mikesell (2014)), and some scholars have noted the differing impacts of tax administration practices on corruption perceptions and experience (Ponomariov et al. (2018)). Our work measures both perceptions and experience using actual responses of the formal sector firms. A number of studies have pointed out the disconnect between perceived and actual business practices (e.g., Besley (2015)). This study overcomes this weakness by focusing on firm managers for both perceptions and experience. We also consider the role of “size” in affecting corruption perceptions and experience. The following key questions are addressed in this research, using cross-country survey data at the firm level: Are the drivers of corruption perceptions similar to those of corruption experience? How do the different dimensions of size, including firm size, city size, and government size, impact corruption perceptions and corruption experience? The structure of the rest of this paper includes the literature and the model in the next section, followed by data and estimation, results, and conclusions.",
46,2,Journal of Economics and Finance,09 April 2022,https://link.springer.com/article/10.1007/s12197-022-09575-x,Social media sentiment and the stock market,April 2022,Amir Fekrazad,Syed M. Harun,Naafey Sardar,Male,Male,Unknown,Male,"The efficient market hypothesis states that stock market prices incorporate all publicly available information at any given point in time. It is important—and interesting—to see how the market evaluates the information contained in social media, which may have a low signal-to-noise ratio. Furthermore, it is worth exploring if the credibility of the poster, i.e., whether the account is verified, changes how the market perceives such information. On the other hand, when the price of a stock moves notably in any direction, social media content may be created by users to inform others of the movement. Therefore, social media and the stock market can mutually affect each other. This paper examines whether such a link between the two variables exists. There is considerable literature analyzing the relationship between media (news or social) and stock returns. While papers like Dougal et al. (2012) find that the news media has significant predictive power over the stock market, the empirical evidence regarding the impact of social media is mixed, with the eventual outcome depending on the choice of specification and sentiment. The social media literature can be classified into two groups characterized by their respective conclusions. The first group of studies emphasizes that social media has a significant impact on stock returns and volatility. Bollen et al. (2011) find that overall sentiment (not just tweets related to the stock market) helps predict movement in the stock market by using daily data on the Dow Jones Industrial Average (DJIA) between February 28, 2008, and December 19, 2008. While there has been skepticism surrounding their results, as expressed in Lachanski and Pav (2017), an abundance of other studies have also come to similar conclusions. These papers include Mittermayer and Knolmayer (2006); Luo et al. (2012); Chen et al. (2014); Checkley et al. (2017); Jiao et al. (2020); Rakowski et al. (2020), and McGurk et al. (2020). In fact, Rakowski et al. (2020) observe that Twitter influences stock trading, especially among small, less visible securities traded primarily by retail investors. It must be noted that all these studies measure sentiment by transforming the words employed in social media or messages posted to the internet into structured data that quantifies emotion and sentiment. The second group of studies, however, find no evidence that sentiment helps forecast future stock returns. Examples of such studies are Lachanski and Pav (2017); Nassirtoussi et al. (2014), and Kim and Kim (2014). One defining feature of most of these studies, barring Checkley et al. (2017) and Rakowski et al. (2020), is that they rely on daily data or consider aggregate stock market indexes, i.e. Dow Jones, as opposed to individual stocks. It is worth noting that when we talk about the interaction between the stock market and social media, we are trying to establish a Granger causal relationship between the two variables. Granger causality refers to the idea that if the change in social media sentiment happens before movement in the stock market, and sentiment helps predict the stock market returns in a statistically significant way, then sentiment “Granger causes” stock market returns. Bollen et al. (2011); Lachanski and Pav (2017); Luo et al. (2012); Kim and Kim (2014); Checkley et al. (2017), and Jiao et al. (2020) also carry out a variety of Granger causality tests, ranging from single-equation to multiple-equation models (such as VARs). For the purposes of our analysis, we rely on single-equation models to test for a Granger causal relationship between social media sentiment and the stock market. Furthermore, we are interested in whether tweets posted by verified accounts have a different impact on the market compared to those posted by ordinary users. Related to this topic, Ge et al. (2019) show that when President Trump’s official Twitter account mentioned the name of a publicly-traded company, the company’s stock price, volume, and volatility moved. While there is an abundance of research studying the interaction between social media and the stock market, our research distinguishes itself in multiple ways. First, we use more granular data, i.e., 15-minute data, as opposed to the majority of other papers that only use daily data. Our use of higher frequency data provides a much richer and more appropriate dataset for estimating the response of the stock market to social media. Lower-frequency data may lead to considerable bias, as pointed out by Geweke (1978). Hence, in our case, the use of minute-level data rather than daily data results in more accurate estimates, especially if, as expected, the stock market is quick to respond to social media because of algorithmic/bot trading. To the best of our knowledge, the only other paper that uses minute-by-minute data is Checkley et al. (2017). However, their sample of companies is distinctly different from ours. Checkley et al. (2017) analyze the response of only five household-name tech companies, namely, Apple, Amazon, Goldman Sachs, Google, and IBM, to social media, while we consider the returns from twenty-five companies listed on the S&P 100. Second, as pointed out by Dougal et al. (2012), the news media has significant predictive power over the stock market. As a result, we analyze the impact of social media while controlling for the effect of traditional or news media on the stock market. Third, we use a more comprehensive data set of tweets that were collected using both the company name and symbol, as opposed to the majority of other papers that use only symbols (known as “cash tags” on Twitter). Fourth, we test for reverse causality, i.e., whether social media responds to the stock market (for instance, by discussing the stocks that have displayed a distinctively large return in any direction), and find strong evidence for this hypothesis at the hourly and daily level. Last, we consider the response of another aspect of the stock market, i.e., short sales, to social media sentiment. Using 15-minute stock price data for twenty-five companies in 2017 matched to approximately two million tweets (the universe of tweets posted in 2017 containing the name or symbol of the companies), this paper examines the interaction between the stock market and social media. We estimate panel regression models with both time and market fixed effects and find that social media sentiment aggregated into 15-minute intervals does not have a significant impact on the stock market. This is followed by an analysis of the response of the stock market in hourly and daily intervals, from which we find that social media sentiment does have an effect. More specifically, we find that a higher proportion of negative tweets (relative to positive and neutral tweets) leads to a decrease in stock returns. These results are consistent with the concept of loss aversion in behavioral economics, which posits that people tend to react more strongly to possible losses. Moreover, we find no evidence of positive or negative tweets posted by verified accounts having an additional impact. Even though such accounts belong to public figures, it is plausible that their tweets do not contain any information that the rest of the social media platform does not already know. Additionally, we also collect thirty-five thousand news headlines from the traditional news media in 2017 related to all companies present in the sample to control for the effect of non-social media information on the stock market. Our previous results hold, with negative tweets affecting stock returns at the hourly and daily intervals. Moreover, our results are also robust to the use of an alternative method of extracting sentiment from tweets. We also find evidence of reverse causality and conclude that social media responds to price fluctuations in the stock market. This result is important because it establishes the presence of a feedback loop mechanism in the stock market: social media sentiment about a stock can impact that stock’s return, which in turn can result in more tweets reflecting the same sentiment in the next period. Lastly, we observe that social media sentiment impacts short sales only over daily intervals. This points to the fact that investors might not act impulsively following a change in social media sentiment, and instead choose to wait to see if the sentiment persists before taking a short position. This paper is organized as follows. Section 2 discusses the data and its preparation. Section 3 presents the econometric methodology used to estimate the response of the stock market to social media. Section 4 presents the results. Section 5 discusses the various robustness checks. Section 6 concludes.",
46,2,Journal of Economics and Finance,07 April 2022,https://link.springer.com/article/10.1007/s12197-022-09572-0,Adjustable consumption model for retirees to balance spending and risk,April 2022,Barry R. Cobb,Tim Murray,Jeffrey S. Smith,Male,Male,Male,Male,"Throughout their working lives, people are faced with the challenge of balancing their income with household expenditures. Once a person retires, however, establishing a sustainable level of consumption becomes a more complex decision. The Life-Cycle Hypothesis (LCH) suggests that people will smooth their consumption over their life-cycle by saving during their working years and living off those savings in retirement (Modigliani and Brumberg 1954). Two recent studies suggest a sustainable constant spending on consumption for retirees, while also investigating the decision regarding when to begin Social Security payments. Alderson and Betker (2017) establish a constant real consumption spending rate as a function of a retiree’s nest egg and Social Security benefits, then simulate the probability that the individual exhausts their savings prior to death for various beginning Social Security payment ages. Cobb and Smith (2020) use Monte Carlo simulation to determine the maximum possible constant consumption for an individual retiring at age 62 and taking Social Security payments starting at ages between 62 and 70. The retiree’s risk tolerance is incorporated by requiring that the person “fails” on only a small percentage of simulation trials. Failure in each study is defined as exhausting one’s savings and living solely on Social Security income; we adopt the same definition in this paper. Alderson and Betker (2017) and Cobb and Smith (2020) assume that the investor maintains liquidity in their investment portfolio. Other strategies for producing constant real consumption involve conversion of assets to an annuity at some point during retirement. For example, Fullmer (2007) devises a plan to establish a stable retirement income and adjust investment allocation over time to provide this level of consumption. This continues until the remaining wealth falls to a level that will fund an annuity paying outflows equal to the desired consumption level. Sexauer and Cassidy (2015) describe a two-part annuity plan that includes a 20-year investment in laddered Treasury-Inflation Protected Securities (TIPS) combined with a purchase of a deferred life annuity. Waring and Siegel (2015) devise a “virtual annuity” where the investor withdraws an amount each year that could be produced by an annuity with the remaining investment balance, where the interest rate is the real return on a ladder of TIPS, and thus their model adjusts consumption. However, the extreme risk aversion (the authors assign the probability of failure at 0%) of the strategy may limit consumption, particularly early in retirement. Although studies cited above assume constant real consumption spending over the retiree’s life-cycle, a decline in real spending in retirement has been observed in empirical studies (Hamermesh 1984; Banks et al. 1998) and can be seen in survey data from the Health and Retirement Study. However, a decline in real spending is not synonymous with a decline in real consumption, as real consumption is smoothed by an increase in household production in retirement (Aguiar and Hurst 2005; Atalay et al. 2020; Been et al. 2020; Schwerdt 2005; Velarde and Herrmann 2014). A higher level of spending early in retirement has not been captured in previous studies that seek to determine optimal spending on consumption in retirement, as previous studies have assumed constant real consumption spending in retirement. An individual receiving regular Social Security benefits may supplement this regular income with withdrawals from retirement savings. People’s choice of saving and how much to withdraw from savings each year is a choice that is affected by differences in time preferences, risk tolerance, work and leisure preferences, exposure to uncertainty (Bernheim et al. 2001; Harris 2019), psychological traits (Asebedo and Browning 2020), type of retirement savings (taxable or not), and their tax bracket. Additionally, many older households intend to leave a bequest (Ameriks et al. 2011), which also impacts the savings and withdrawal decision. As a result of these differences, many households that operate with a risk-averse low-probability of failure have a large amount of wealth remaining at death (Cobb and Smith 2020). This may be the fulfillment of the bequest motive or it could be longevity risk aversion, which is the fear of prematurely depleting retirement savings, in this case caused by overconsuming early in retirement. This paper seeks to develop a strategy that allows retirees to spend more of their wealth, both earlier in their retirement and prior to death, without exposing them to an additional probability of failure. The strategy allows for an adjustable rate of consumption spending in retirement, as compared to constant real consumption strategies that have been used in previous studies. Our model shows that households can increase their spending in retirement using an adjustable consumption strategy, even if households have a bequest motive. This is a novel finding because those who restrain from desired consumption out of fear experience sub-optimal utility from this decision. We show increased consumption with considerable wealth remaining at death, which satisfies both the motives of increased consumption and the possibility of bequeathing assets at death. We present results for various levels of wealth and Social Security income that can be utilized by retirees or financial planners so the strategies can be directly applied. The remainder of this paper is organized as follows. Section 2 describes the data used from the Health and Retirement Study and the other model inputs. Section 3 describes results from the Health and Retirement Study (HRS) that indicates empirical spending patterns for retired persons. Section 4 defines the model that will be used to test various consumption patterns and determine the maximum consumption for individuals. Section 5 discusses the results of the model and implications for retired persons who are establishing their consumption levels. Section 6 further discusses general conclusions that can be drawn from this research, while Section 7 presents some limitations and directions for further research.",
46,2,Journal of Economics and Finance,29 March 2022,https://link.springer.com/article/10.1007/s12197-022-09573-z,Cash Holdings along the Supply Chain: The Downstream Evidence,April 2022,Fang Chen,Jian Huang,Jianjun Jia,,,Unknown,Mix,,
46,3,Journal of Economics and Finance,04 April 2022,https://link.springer.com/article/10.1007/s12197-022-09577-9,U.S. monetary policy and the predictability of global economic synchronization patterns,July 2022,Mehmet Balcilar,Riza Demirer,,Male,Male,Unknown,Male,"The role of uncertainty as a driver of business cycle fluctuations has been examined in a large strand of the literature with inconclusive findings on the causal direction and transmission channels between uncertainty and real economic activity. While some studies including Bloom (2009) and Bloom et al. (2012), among others, document a significant productivity uncertainty effect on economic growth and employment, others including Bachmann and Bayer (2013) and Bekaert et al. (2013) argue otherwise, reporting little to no uncertainty effect on business cycle proxies. Focusing on the role of policy uncertainty specifically, numerous studies point to a significant monetary policy effect on the macroeconomy (see Aastveit et al., 2013 for a detailed literature review), while Born and Pfeifer (2014) argue that the role of policy risk in explaining business cycles is overstated although their findings show that the effects of uncertainty regarding fiscal and monetary policy are seven times larger than those of technological uncertainty. Despite the multitude of studies in this large strand of the literature, however, the focus has primarily been on developed economies without much interest in the role of uncertainty as a driver of synchronization patterns in business cycles across the world economies including emerging market economies. Clearly, given the rise in the level of integration across global capital markets, one can expect the uncertainty about financial and macro conditions as well as monetary and fiscal policies in local markets (or regions) to have far-reaching implications in other economies, suggesting the presence of some form of synchronization in business cycle patterns across nations. Indeed, Bordo and Helbling (2001) note a secular trend towards increased synchronization across the world economies during much of the twentieth century, with global shocks serving as the main driver. Similarly, Ha et al. (2019) find that inflation patterns at a global level have become increasingly synchronized over time with a common global factor accounting for about 22 percent of variation in national inflation rates. Focusing on emerging markets, studies including Kose and Prasad (2010) and Kose et al. (2012) show that emerging market economies have experienced a greater degree of synchronization in their business cycles, especially after the global financial crash of 2008. Against this background, our goal is to provide insight to the drivers of commonality in real economic activity across the world economies by building on the output gap measure of Cooper and Priestley (2009) as a business cycle indicator. This paper contributes to the literature from a novel angle by (i) proposing output gap dispersion as a measure of global economic synchronization; and (ii) examining the causal interactions between various uncertainty proxies and global economic synchronization patterns using a novel, multivariate quantile causality testing methodology and data from a set of 45 advanced and emerging nations. The multivariate causality model employed in our analysis allows us to disentangle the causal effects of monetary policy measures on synchronization patterns from that of risk/uncertainty proxies, while quantile-based tests provide insight to the uncertainty-synchronization relationship at various levels of synchronization captured by its quantiles. By doing so, the paper enlarges our understanding of the role of uncertainty on business cycle fluctuations at a global level by addressing possible nonlinearity in the relationship between uncertainty and economic activity. Clearly, this is a matter of interest for the implementation and coordination of monetary policies across nations, particularly considering the finding by Yeyati and Williams (2012) that business cycles across emerging economies have become more synchronized in the 2000s. The issue is also of interest to investors and corporations who time their international investments conditional on economic growth expectations and various uncertainty proxies. Our proposed measure of global economic synchronization, termed output gap dispersion, builds primarily on Cooper and Priestley (2009) who document a link between the output gap and the business cycles, implied by a positive relationship with various highly procyclical variables like earnings growth rates, real GDP, industrial production, inventories, and a negative relationship with the term structure of interest rates. Recently, Atanasov (2018) shows that the world output gap, computed as the simple equal-weight average of the national output gaps, serves as a global business cycle indicator, capturing significant predictive information for aggregate stock market returns. Our measure of global economic synchronization builds on this line of work and uses the cross-sectional dispersion of national output gaps around the world output gap as a measure of directional similarity in economic activity such that a low value for gap dispersion implies greater similarity in national output gaps around the world output gap, reflecting synchronized economic activity across global economies. To that end, the quantile-based focus adopted in our empirical approach provides an appropriate framework to examine the drivers of synchronization patterns by targeting the quantiles of the conditional distribution of gap dispersion, thus allowing us to model extreme high/low and typical synchronization regimes in a formal setting. The choice for the predictors for global economic synchronization patterns is largely motivated by a number of studies that argue the presence of a global financial cycle that governs patterns in global capital flows and prices across world economies (e.g. Nier et al., 2014; Ali et al., 2022). A growing number of studies identify U.S. monetary policy actions as well as changes in risk appetite as primary factors that drive return and volatility patterns via a global financial cycle across global equity (De Vita and Kyaw (2008), Passari and Rey (2015), Miranda-Agrippino and Rey (2015), Rey (2018)) as well as commodity and currency markets (Salisu et al., forthcoming, Demirer et al., 2021). While Kose et al. (2017) argue that U.S. financial conditions in part driven by U.S. monetary policy could have significant effects on emerging and developing economies that rely heavily on external financing, Anaya et al. (2017) suggest that international portfolio flows serve as a key channel of transmission between the U.S. and the emerging market economies. These findings on the effect of U.S. monetary policy decisions on the global economy are further supported by Colombo (2013) and Balcilar et al. (2017) who report significant spillover effects of U.S. monetary policy uncertainty on global economies. Against this background, we focus on various U.S. monetary policy measures as well as risk/uncertainty proxies that relate to macroeconomic, financial, real uncertainty and risk aversion, and explore their causal effects on the economic synchronization patterns across the advanced and emerging economies via quantile-based multivariate causality tests. Our empirical analysis yields significant evidence of causality running from U.S. monetary policy measures to the synchronization patterns across the advanced and emerging economy business cycles, even after controlling for various risk and uncertainty proxies. While U.S. monetary policy actions cause synchronized economic activity across advanced economies (implied by low quantiles of output gap dispersion), we also find evidence of causal effects at extreme high quantiles, suggesting that U.S. monetary policy actions drive dispersion in their economic activity, contributing to uncertainty in industrial production patterns across advanced economies. In the case of emerging market economies, the analysis yields significant evidence of causality from U.S. monetary policy actions to the dispersion of emerging market output gaps at low quantiles, suggesting that policy actions by the U.S. Fed drive synchronized economic activity across emerging nations as well. Interestingly, our multivariate tests show that the predictive power of risk/uncertainty measures over emerging market synchronization patterns is absorbed by U.S. monetary policy proxies, highlighting the dominant role of interest rate decisions of the U.S. Fed on emerging market economies. Overall, our findings suggest that output gap dispersion could serve as a reasonable proxy for synchronization in economic activity across the world economies and provide clear evidence of a U.S. monetary policy effect on the synchronization patterns across global economies, even after controlling for changes in macro and financial uncertainty as well as risk appetite. The remainder of the paper is organized as follows. Section 2 presents the methodological details regarding the computation of the gap dispersion measure and the multivariate quantile causality model employed in our empirical analysis. Section 3 reports the empirical findings for the linear and nonlinear causality tests. Finally, Sect. 4 concludes the paper with policy implications of the findings.",1
46,3,Journal of Economics and Finance,05 April 2022,https://link.springer.com/article/10.1007/s12197-022-09576-w,Inflation in the G7 countries: persistence and structural breaks,July 2022,Guglielmo Maria Caporale,Luis Alberiko Gil-Alana,Carlos Poza,Male,Male,Male,Male,"Measuring inflation persistence is of interest to both academics (to establish the empirical relevance of different theoretical models, such as the Phillips curve or DSGE models) and monetary authorities (to anchor expectations in order to lower persistence and reduce the output costs of disinflation). High persistence might result, for instance, from price and wage rigidities (Galí and Gertler, 1999), or from the lack of transparency of monetary policy (Walsh, 2007). There is plenty of evidence suggesting that inflation has been highly persistent in most developed countries since WWII (Miles et al., 2017). However, an equally important issue is whether or not its degree of persistence has changed over time, possibly as a result of the adoption of different monetary policy frameworks such inflation targeting. Pivetta and Reis (2007) and Stock and Watson (2007, 2010) do not find any significant changes in the US in the post-WWII period when accounting for uncertainty around point estimates or distinguishing between persistent and transitory changes in inflation. Similarly, Caporale et al. (2018) conclude that inflation persistence has been lower in the UK in the twentieth century compared to earlier ones but has not changed significantly since WWI. The present paper aims to provide more extensive evidence on this issue by analysing the stochastic behaviour of inflation in all G7 countries in the last five decades and testing for possible breaks. The motivation for the analysis is therefore to investigate more thoroughly inflation persistence in the most developed countries by allowing for long memory, nonlinearities and breaks, as well as interpreting the empirical evidence to draw policy implications. More specifically, our study uses a fractional integration framework that is much more general and flexible than the AutoRegressive-(Integrated)-Moving Average (AR(I)MA) models most commonly adopted in the literature since it is not based on the classical I(0) versus I(1) dichotomy and allows instead the order of integration d to take fractional as well as integer values. Having ruled out non-linearities in the series of interest, it then carries out endogenous break tests and re-estimates d over the corresponding sub-samples, thereby obtaining evidence of significant changes in persistence across countries and subsamples, which is an important piece of information for policy makers. Therefore the contribution of this study to the literature is fourfold: (i) it improves upon earlier works by following a modelling approach that encapsulates a much wider range of dynamic behaviours and therefore is better suited to capturing the properties of inflation; (ii) it explores the issues of possible nonlinearities, which is typically overlooked by studies on inflation; (iii) it addresses the issue of changing persistence, which again is normally not addressed in this area of the literature, virtually all papers on the topic of inflation only considering average persistence over the entire sample; (iv) it draws some policy implications by interpreting the results in terms of the reputation and credibility of monetary authorities. The layout of the paper is as follows. Section 2 briefly reviews the relevant literature. Section 3 outlines the methodology and describes the data. Section 4 presents the empirical results. Section 5 offers some concluding remarks.",
46,3,Journal of Economics and Finance,01 April 2022,https://link.springer.com/article/10.1007/s12197-022-09570-2,Is interest rate uncertainty a predictor of investment volatility? evidence from the wild bootstrap likelihood ratio approach,July 2022,Godwin Olasehinde-Williams,Oktay Özkan,,Male,Male,Unknown,Male,"Sustained investment is required for economic growth. Investment however often experiences severe volatility, making spending plans difficult to formulate and diminishing growth potentials. Thus, for most countries, investment volatility dampening is a key macroeconomic policy. Also, issues related to interest rate uncertainty are controversial in policymaking. Arguments often arise on whether central banks should use their influence over interest rates to limit fluctuations that lead to uncertainty or to focus chiefly on other macroeconomic objectives while leaving interest rates to freely respond to market pressures. This is of great importance because interest rate uncertainty is an indicator of the level of economic uncertainty present within an economy. It therefore exerts some influence on the consumption and investment decisions of economic agents. This therefore suggests that interest rate uncertainty may be an important trigger for investment volatility across economies. This paper explores, on the basis of the experience of nine selected developed economies, the interconnectedness of interest rate uncertainty and investment volatility. It specifically investigates and confirms interest rate uncertainty as a significant predictor of volatility in investments. The paper adds to literature in several ways. First, it extends the empirical research on the interest rate-investment nexus by examining the ability of interest rate uncertainty to predict investment volatility. While numerous studies have examined the impact of interest rate uncertainty on investment, especially at the microeconomic level, none has examined the macroeconomic implication for investment volatility. Second, this study is the first to employ the novel wild bootstrap likelihood ratio approach developed by Kim and Shamsuddin (2020) in examining the interest rate-investment nexus. Third, the study employs the newly developed subjective interest rate uncertainty index of Istrefi and Mouabbi (2018). Conventional growth models such as those of Harrod (1939), Domar (1946), Solow (1956) and Romer (1986) emphasize the importance of investment for economic growth. By driving innovation as well as physical and human capital accumulation, investment acts as the lifeblood of economic growth (Bano 2018). The implication of this is that economic growth requires sustained investment. For any economy, the level of investment at a point in time connects the present with the future in terms of issues such as business cycles, current and expected future profits, savings rate, infrastructure, financial institutions and the cost of investment, which is the interest rate (Griliches and Wallace 1965; Amir et al. 2012; Bano 2018). Investment volatility can thus significantly affect the economic well-being of nations by making spending plans difficult to formulate and diminishing growth potentials. Extant literature shows that of all the components of gross domestic product, investment along with net exports is the most volatile, especially in terms of economic expansion (Olasehinde-Williams 2021). Most of the volatility noticeable in gross domestic product is generally due to volatilities in investment spending and net exports. Interest rate movements generally reflect the state of the macroeconomy and also affect macroeconomic variables such as investment (Khurshid 2015). Real interest rate is the main variable of concern to investors as it is the core for stimulating investment because of its role as the cost of capital (Coleman 1997). As stated by Berk (1999), the current value of any investment fluctuates in response to interest rate movements. This suggests that investment rate is not only dependent on interest rate level, but on the degree of uncertainty in interest rate movements as well. Interest rate uncertainty is thus an important factor to be considered in investment analysis. As a matter of fact, interest rate uncertainty has the capability to impact investment even more than the general level of interest rates (Carmichael and Bustamante 2014). This implies that unexpected interest rate movements can significantly induce volatility in investment. For a long time, issues related to unexpected changes in interest rate and its effects have been generating controversies in central bank policymaking. While the functions of central banks often involve nonfinancial economic activities such as inflation control and economic growth, in executing policies designed to achieve these objectives, central banks operate almost exclusively in financial markets (Friedman 1982). Interest rates are thus significantly impacted by the actions of central banks. Whether to use this influence on interest rates to dampen fluctuations that result in uncertainty, or to solely focus on other macroeconomic objectives while leaving interest rates to freely respond to market pressures is thus a major policy concern. In particular, the possibility that rising interest rate uncertainty may distort the market mechanism for raising long-term capital and thus introduce volatility into investment decisions is a key concern for policymakers. The remaining sections of the paper are arranged in the following manner: Sect. 2 reviews relevant literature, Sect. 3 discusses the data and methodological approach followed, Sect. 4 presents and discusses the empirical findings, Sect. 5 details the robustness check conducted, while Sect. 6 provides the conclusion alongside some policy implications.",1
46,3,Journal of Economics and Finance,06 April 2022,https://link.springer.com/article/10.1007/s12197-022-09574-y,Determinants of Interest rate swap spreads: A quantile regression approach,July 2022,Kenneth A. Tah,,,Male,Unknown,Unknown,Male,"The determinants of interest rate swap spreads have attracted a vast array of theoretical and empirical literature. Previous literature has focused on the factors that have contributed to the dynamics of interest rate swap spreads. To understand the determinants of interest rate swap spreads, it is central to decompose the components of spreads, i.e., the swap rate and the yield on a government bond of the same maturity. The first component of the interest rate swap spreads, the interest rate swap is linked to a reference rate (LIBOR) which is determined by the demand and supply of funds in the interbank market and includes a component of default risk, among other reasons. Accordingly, the swap spread is determined by the time variation in the short-term riskless rate and risk premium demanded by market participants in this market. The second component of the interest rate swap spreads,Footnote 1 the yield to maturity of government debt, is driven by the corresponding variation in the short-term riskless rate, associated inflation, and the real rate risk premium demanded by market participants in the government bond market. Apparently, previous research shows that the variation in swap spreads is driven by the relative temporal variation in default risk (Sorensen and Thierry 1994, Liu et al. 2006), liquidity (Grinblatt 2002, Huang and Neftci 2003, Liu et al. 2006), and the associated risk-premia like credit risk and shock to swap spreads (Sun et al. 1993, Brown et al. 1994, Duffie and Singleton 1997, Lang et al. 1998, Lekkos and Milas [2001; 2004], Fehle, 2003, In et al. 2003). It is expected that the interest rate swap spreads will always be positive since the swap rate at any maturity is always higher than the yield on treasury security with an equal and corresponding maturity. However, it is important to note that following the 2007/2008 financial crises, spreads tightened, and market participants noticed a swap spread anomaly. The 10-year (7-year) interest rate swap spreads tightened, recording -0.04% (-0.049%) on January 21, 2009 (March 24, 2010). Studies that show variation between long-term and short-term interest rate swap spreads include Brown et al (1994) and Lekkos and Milas (2001). Brown et al. (1994) demonstrate that long-term (5-, 7-, and 10-year) swaps are priced differently than short-term (1- and 3-year) swaps. Additionally, they show that 5-year swap pricing dynamics experienced a substantial change from January 1985 to May 1991. Lekkos and Milas (2001) use the determinants of swap spreads to assess the stochastic evolution of the term structure of swap spreads. They document a procyclical behavior for short-term U.S. swap spreads, and a countercyclical behavior for long-term U.S. swap spreads. They document that the significance of liquidity and corporate bond spreads varies with maturity. In investigating the determinants of interest rate swap spreads, the above studies have focused on the means of the conditional interest rate swap distributions. This paper investigates whether the relationship between interest rate swaps and their determinants is subject to quantile parameter heterogeneity. We analyze whether the effects of default, slope, ted and volatility on interest rate swap spreads are heterogeneous along quantiles of conditional interest rate swap spread distributions and if such heterogeneity (if any) varies with long-term and short-term interest rates swaps. To the best of our knowledge, no previous study has attempted to investigate the quantile effects on the relationship between interest rate swaps and their determinants. Indeed, the relationship between interest rate swap spreads and their determinants might affect the conditional interest rate swap spread distribution, which goes beyond a simple location shift. For example, there might be a link between default and the dispersion of the conditional interest rate swap spread distribution. If the effect of default on interest rate swap spreads is increasing (decreasing) along quantiles of conditional interest rate swap spread distributions, then higher default is associated with higher (lower) dispersion of conditional interest rate swap spread distributions. This suggests higher (lower) conditional interest rate swap spread inequality. Accordingly, default might be a source (break) of (on) conditional interest rate swap spread inequality. We extend previous literature by considering the impacts of default, slope, ted, and volatility, not only on the mean but on the shape of the conditional interest rate swap spread distribution. We use 10- and 2-year swaps to observe whether our results vary with long-term and short-term interest rate swaps, respectively. We document that the effect of default risk on 10- and 2-year spreads increases at a decreasing rate as spread quantile increases (concave down quantile effect). There is a weak slope effect on the 10-year interest rate swap spread at the middle quantiles (U-shape) but strong slope effects on the 2-years interest rate swap spreads at the middle quantiles (inverted U-shape). Additionally, the effect of ted on 10- and 2-year spread increases at an increasing rate as spread quantile increases (concave up quantile effect). Lastly, there is a strong volatility effect on the 2-year interest rate swap spreads at the middle quantiles (inverted U-shape) and weak volatility effects on the 10-year interest rate swap spread at the middle quantiles (U-shape). We use a coefficient equality test to document that, in most cases, the estimated coefficients at both the lower and the higher quantiles are significantly different from the median coefficient. Our findings may be beneficial to corporate issuers of fixed-rate bonds with intentions to swap the debt into “floaters” and usually observe the evolution of interest rate swap spreads relative to the corporate bond spread to estimate the funding cost of debt. Our study also provides a better insight to policymakers as they pay careful attention to interest rate swap spreads to assess the health of the financial system and the economy in general. The rest of the paper is organized as follows: Sect. 2 presents the econometric methodology. Section 3 presents the data. Section 4 presents the empirical evidence. Section 5 concludes.",
46,3,Journal of Economics and Finance,24 April 2022,https://link.springer.com/article/10.1007/s12197-022-09579-7,Evaluating measures of dependence for linearly generated nonlinear time series along with spurious correlation,July 2022,Christos Agiakloglou,Anil Bera,Emmanouil Deligiannakis,Male,Male,Male,Male,"Modeling dependence between two time series is very often a challenging issue for many economic and financial applications not just due to the nature of the data but also due to the fact that several other factors can alter an existing relationship throughout a period of time. Moreover, and apart from issues of causality and spuriosity, the model specification is indeed another important concept in modeling dependence. Analysts tend to believe that the selected model, which eventually will be used for statistical inference, is the most suitable model driven by the data, but not necessarily the “right” one, ignoring in a sense any miss-specifications issues. Things become even more complicated when researchers tend to assume that the relationship between two variables remains constant over time or that the generating mechanism of many time series simply follows a linear pattern. And yet, any study of a co-movement between two variables generally starts from correlation analysis, knowing in advance that there will be some concerns regarding the reliability of the obtained results. Indeed, Fisher (1915), who has revealed some of the properties of the sample correlation coefficient, r, showed that r is not an unbiased estimator of the population correlation coefficient, ρ, for normal populations, as also can be seen in Kenny and Keeping (1951) and Kendall (1954). However, the bias itself is not usually considered as an important issue in correlation analysis, since its magnitude is typically a small number for a coefficient that takes values from − 1 to + 1. Contrary, what seems to be very important is that the bias affects the accuracy of the relevant t statistic for testing the null hypothesis of zero association, especially for small absolute values of the sample correlation coefficient, leading to ambiguous results. Misleading statistical results regarding the relationship between two variables can also be obtained in correlation analysis using time series that have nothing in common, a phenomenon that was discovered by Yule (1926) and it is known in our days as spurious behavior. Yule (1926) noticed by studying the frequency distribution of the correlation coefficient of two independent processes that their correlation has no meaning when the shape of the distribution looks like a U, a result that can also be found in Banerjee et al. (1993). Banerjee et al. (1993), using a Monte Carlo analysis, showed that the frequency distribution of the sample correlation coefficient has a U shape for two independent non-stationary I(2) processes with values of -1 and + 1 most likely to occur, while for two independent non-stationary I(1) processes, the frequency distribution of the sample correlation coefficient looks like a semi-ellipse, contrary to the case of two independent white noise processes, where the frequency distribution of the correlation coefficient is symmetric around zero behaving as a normal distribution. It turns out though that even for the simplest case of two independent stationary processes, i.e., for two independent AR(1) processes, the spurious behavior does exist as Agiakloglou and Tsimpanos (2012) have indicated, although the frequency distribution of their correlation coefficient is symmetric around zero, but it does not behave exactly as a normal distribution, while Agiakloglou and Agiropoulos (2016) showed that the performance of the power of the classical test for zero correlation was not as satisfactory as it should have been for these two series. The spurious findings presented by Yule (1926), through correlation analysis for time series data, showed evidence of a linear relationship among variables that had nothing in common. This unique result motivated several analysts to further investigate the behavior of this phenomenon in time series econometrics, based initially on the pioneering work of Granger and Newbold (1974) that dealt with two independent random walk processes without drift and it was extended to two independent stationary processes by Granger et al. (2001). Actually, most of the research on spurious behavior is conducted on linear relationships based on linearly generated processes, knowing that it is difficult to formulate acceptable nonlinear conditions, since nonlinearity can occur in a variety of ways, regardless whether linearity may not be the most appropriate approximation of an existing relationship nor the most suitable way to describe the evolution of a volatile process. Likewise, the correlation coefficient, which measures linear dependence between two variables, cannot identify other types of relationships. According to Embrechts et al. (1999) the correlation coefficient is not an adequate measure of dependence unless the series are jointly Gaussian, something that does not often appear in financial studies. For this reason, other techniques have been established in the literature to capture more complex types of association. Copula, for example, is one of them, which is a multivariate distribution function that can completely describe the dependence between two (or more) variables, while at the same time it is independent of their marginal distributions. Thus, it will be very interesting to investigate the behavior of several dependence measures, along with the correlation coefficient in lieu with the test of zero association, based on nonlinear time series generated by linear dependence. Using a Monte Carlo analysis for series generated by the family of AR(1) – ARCH(1) in variable models, presented by Bera et al. (1992 and 1996), where the stationary AR(1) process is a special case of them, this study finds that copulas do perceive dependency better than the correlation coefficient based on the generated level of dependency and not on the structure of the series. The correlation coefficient produced unreliable results for several cases strongly affected by the nonlinearity parameter and the sample size. Meanwhile, this study discovers that the spurious correlation phenomenon can be eliminated for these series according to the magnitude of the nonlinearity parameter, while the power of the test of zero association is very low. The remaining of the paper is organized as follows. Section 2 presents the nonlinear family of models, known as the AR(1) – ARCH(1) in variable models, presented by Bera et al. (1992 and 1996) and the most commonly used copulas. Section 3 describes the design of the simulation process and discusses the results, while the concluding remarks are presented in Sect. 4.",
46,3,Journal of Economics and Finance,16 June 2022,https://link.springer.com/article/10.1007/s12197-022-09585-9,Ex-date variables and DIPO parent returns,July 2022,Thomas H. Thompson,,,Male,Unknown,Unknown,Male,"The recent announcements by General Electric and Johnson and Johnson to divest their subsidiaries reflect opportunities for others to do likewise. These divestitures can be spin-offs, equity carve-outs, or divestiture IPOs (DIPOs).Similar to Bradley and Jordan (2002) who examined the partial price adjustment of IPOs and Thompson (2011) who extended their study to equity carve-outs, we examine the returns of 57 DIPO parents that conducted DIPOs during the period from 1988 to 2020. A DIPO is a corporate divestiture where the parent sponsors or issues shares in a previously wholly owned subsidiary and retains less than one-third ownership (on average 11.89%) in the subsidiary after the DIPO. Unlike regular IPOs that receive all of the offering proceeds, DIPOs parent receive on average 78% of the offering proceeds. DIPOs are similar to equity carve-outs (ECOs) as a traded parent offers shares of its subsidiary to the public, but the ECO parents retain one-third or more ownership (on average 72.75%) in the subsidiary after the ECO. We observe, but do not report that DIPO subsidiary returns and variables are marginally significant at best. Thus, we extend the ECO literature to DIPOs and examine what variables influence DIPO parent returns. Like equity carve-outs, DIPOs can provide funds that can be used to retire debt, to increase cash for investment, or to cash-out insiders. In addition, Klein et al. (1991) observe that carve-outs allow parents to showcase their subsidiaries to prospective buyers. Carve-outs reflect insiders’ preferences to hold the shares to be divested later and the potential market price increases (Leland and Pyle 1977; and Loughran and Ritter 2002). The low levels of secondary offerings and reported insider selling result in an overhang of future share sales. These measures can apply to DIPO parents. We evaluate four hypotheses related to equity carve-out underpricing: partial price adjustment, prospect theory, managerial discretion, and leaning against the wind. Also, we investigate the extent to which carve-out (partial IPO) underpricing can be predicted based on public information available before the announcement and offer dates, for a sample of 57 DIPO parents during the period 1988–2020. Although Loughran and Ritter (2002), Thompson (2011), and others study carve-outs or their parents, the specific contributions of this study are that we examine results from different perspectives in our hypothesis tests. Unlike the referenced and other carve-out studies, we contrast initial period DIPO parent returns. Also, we examine the change in wealth over the short term. In addition, we examine results during low and high market volatility periods and for increased, decreased, and unchanged initial filing adjustments. Moreover, we show that 9.59%-27.91% of the variation in market-adjusted DIPO parent returns can be predicted using public information known prior to the offer date. Furthermore, we show that due to the overhang theory, the increase in share values retained by carve-out parents offsets the impact of underpricing. Consistent with Bradley and Jordan (2002) and Thompson (2011) we conclude that many factors available prior to an equity carve-out can predict underpricing. The implications are that models with public information reduce the need for information asymmetry models. There are several reasons that parents conduct DIPOs or ECOs. Allen and McConnell (1998) and Madura and Nixon (2002) observe that distressed parents receive debt repayments or dividends as part of the subsidiary IPO proceeds. Schipper and Smith (1986) find that because the ECOs are reported as part of existing firms the ECO provides reduced information asymmetry at the offering date. Klein et al. (1991) report that parents use carve-outs to showcase their subsidiaries to prospective buyers. Also, Schipper and Smith (1986) find that because the ECOs are reported as part of existing firms the ECO provides reduced information asymmetry at the offering date. Thus, DIPO parents with less than one-third ownership can be motivated by proceeds from the DIPO and potential sales of the subsidiary. Prezas et al. (2000) observe that prior to the ECO, parent market prices reflect the combined firms’ value. Similarly, for DIPOs, after the partial IPO and similar to ex-dividend price adjustments, parent prices reflect the divested ownership in subsidiaries. Hulbert et al. (2002) and Vijh (2002) observe that pure carve-outs have lower parent company announcement period returns and lower subsidiary initial period returns than IPOs. Madura and Nixon (2002) report that distressed parents have negative long-term returns and other parents have positive long-term returns. Below we examine four hypotheses and their impact on DIPO parent returns.",
46,3,Journal of Economics and Finance,18 May 2022,https://link.springer.com/article/10.1007/s12197-022-09581-z,Shareholder response to pension deficit: evidence from the COVID-19 pandemic,July 2022,Amanjot Singh,Harminder Singh,,Unknown,,Unknown,Mix,,
46,3,Journal of Economics and Finance,02 June 2022,https://link.springer.com/article/10.1007/s12197-022-09583-x,The impact of global economies on US inflation: A test of the Phillips curve,July 2022,Hany Guirguis,Vaneesha Boney Dutra,Zoe McGreevy,Male,Unknown,Female,Mix,,
46,3,Journal of Economics and Finance,07 April 2022,https://link.springer.com/article/10.1007/s12197-022-09578-8,Mortgage rate predictability and consumer home-buying assessments,July 2022,Hamid Baghestani,,,Male,Unknown,Unknown,Male," This study aims to examine whether US consumer home-buying assessments can potentially help reduce the random walk prediction errors of mortgage rates. The reason for employing the random walk benchmark follows Pesando (1979, p. 458), who theoretically shows, “long-term interest rates in an efficient market will exhibit random walk characteristics in the absence of time-varying term premiums, but cautions that this result is only approximate.” Despite the inherent difficulty in beating the random walk, professional forecasters and market participants are constantly engaged in predicting long-term interest rates (by, perhaps, questioning the time-invariant term premium assumption). In general, as noted by Diebold and Lopez (1996, p. 252), “the random walk is not necessarily a naïve competitor, particularly for many economic and financial variables.” In particular, Mitchell and Pearce (2007, p. 840) find, “Most economists’ forecast accuracy is statistically indistinguishable from a random walk model in forecasting the Treasury bill rate, but many are significantly worse in forecasting the Treasury bond rate and the exchange rate.” Following Pesando (1979; 1981), Reichenstein (2006, p. 116) points out, “According to term structure theory, the optimal near-term forecast of a long-term rate is essentially today's rate.” Baghestani (2009) shows that the Blue Chip survey forecasts of bond rates fail to outperform the random walk predictions.Footnote 1 As for the 30-year mortgage rate, Baghestani (2008a, p. 225) shows that the random walk prediction accuracy improves with a reduction in lead time but deteriorates with an increase in the forecast horizon. Baghestani (2008b) questions the time-invariant term premium assumption and shows that the information in the long-run trend in inflation helps produce accurate forecasts of mortgage rates. In this study, however, we investigate whether US consumer home-buying assessments can potentially help reduce the random walk prediction errors of the 15- and 30-year mortgage rates. In particular, we maintain that an improvement (a deterioration) in consumer home-buying attitudes results in higher (lower) home buying and demand for mortgage financing, which, in turn, lead to higher (lower) mortgage rates. The assessment data come from Surveys of Consumers (https://data.sca.isr.umich.edu). One survey question asks, “Generally speaking, do you think now is a good time or a bad time to buy a house?” The follow-up questions are concerned with the price, interest rate, good or bad investment, affordability, and uncertain future. There is some evidence that consumer opinion about interest rates helps produce accurate forecasts of the 30-year mortgage rates (Baghestani 2017).Footnote 2 However, to the best of our knowledge, no study has investigated whether consumer home-buying attitudes and consumer responses to other follow-up questions contain potentially useful predictive information for mortgages rates. This is the knowledge gap, which we aim to fill in this study. In evaluating the accuracy of the random walk forecasts of the 15- and 30-year mortgage rates, we allow for the possibility of asymmetric loss (Elliott et al. 2005). We show that these forecasts, on average, over-predict and imply asymmetric loss. As such, they are of value to a user who assigns higher cost to over-predictions than to under-predictions. However, the null hypothesis of orthogonality is rejected for only “the difference of opinion of consumers about whether now is a good or a bad time to buy a house”, and “the difference of opinion of consumers about whether the interest rate is low or high.” Using such survey information, one can potentially reduce the random walk prediction errors of the 15- and 30-year mortgage rates. Section 2 describes the data. Section 3 presents the results. Section 4 concludes.",
46,3,Journal of Economics and Finance,09 June 2022,https://link.springer.com/article/10.1007/s12197-022-09582-y,Predicting distress: a post Insolvency and Bankruptcy Code 2016 analysis,July 2022,Paras Arora,Suman Saurabh,,Unknown,,Unknown,Mix,,
46,4,Journal of Economics and Finance,13 July 2022,https://link.springer.com/article/10.1007/s12197-022-09593-9,"On the uncertainty-global bank linkage nexus: The moderation of crises, financial regulations, and institutional quality",October 2022,Tien Van Nguyen,Dung Phuong Hoang,Thang Ngoc Doan,,,,Mix,,
46,4,Journal of Economics and Finance,26 July 2022,https://link.springer.com/article/10.1007/s12197-022-09588-6,Why has the median real income of lawyers been declining?,October 2022,James V. Koch,Barbara Blake-Gonzalez,,Male,Female,Unknown,Mix,,
46,4,Journal of Economics and Finance,06 July 2022,https://link.springer.com/article/10.1007/s12197-022-09589-5,The role of domestic and foreign economic uncertainties in determining the foreign exchange rates: an extended monetary approach,October 2022,S. M. Woahid Murad,,,Unknown,Unknown,Unknown,Unknown,,
46,4,Journal of Economics and Finance,07 May 2022,https://link.springer.com/article/10.1007/s12197-022-09580-0,Persistence in ESG and conventional stock market indices,October 2022,Guglielmo Maria Caporale,Luis Gil-Alana,Inna Makarenko,Male,Male,Female,Mix,,
46,4,Journal of Economics and Finance,11 July 2022,https://link.springer.com/article/10.1007/s12197-022-09592-w,Oil price uncertainty and real exchange rate in a global VAR framework: a note,October 2022,Abdullahi Musa,Afees A. Salisu,Chinecherem D. Okoronkwo,Unknown,Unknown,Unknown,Unknown,,
46,4,Journal of Economics and Finance,14 July 2022,https://link.springer.com/article/10.1007/s12197-022-09591-x,Banks’ financial soundness during the COVID-19 pandemic,October 2022,Dung Viet Tran,M. Kabir Hassan,Nam Dau,,Unknown,Male,Mix,,
46,4,Journal of Economics and Finance,09 July 2022,https://link.springer.com/article/10.1007/s12197-022-09594-8,Time–frequency return co-movement among asset classes around the COVID-19 outbreak: portfolio implications,October 2022,Seyed Alireza Athari,Ngo Thai Hung,,Male,Unknown,Unknown,Male,"The relationship between various asset classes has received substantial attention from investors, portfolio managers, and policymakers, and numerous scholars have been focused to unveil the return and volatility connectedness among different asset classes especially by increasing global financial uncertainty and also market integration (e.g., globalization, the openness of markets, financialization and technological developments) (Bouri et al. 2018a). In the empirical literature, several studies have investigated the connectedness between commodity and equity markets in developed and developing economies (Mollick and Assefa 2013). Narayan and Narayan (2010) highlighted that oil shocks have a short and long-term effect on equity returns. Shahzad et al. (2020) revealed that gold is a hedge and a safe haven for most of the G7 stock indices. Furthermore, another strand of literature on rising global financial uncertainty (e.g., global financial crisis (208–2009), European debt crisis of (2010–13)) and also the emergence of the digital asset of Bitcoin has been shifted to investigate the co-movement between conventional (such as stocks, bonds, and currencies) and digital assets (e.g., Brière et al. 2015; Bouri et al. 2017a). Specifically, several empirical studies have attempted to examine the hedging ability of digital assets under various market conditions since commodities were less likely considered safe-haven assets and behaved as risky assets, particularly after the global crisis period 2008–2009 (Bekiros et al. 2017). This is crucial since detecting any significant spillover connectedness between Bitcoin and other asset classes provide important implications for portfolio managers, risk management decisions, and also for policymakers. Some studies found a weak relationship between Bitcoin and commodities (e.g., gold, crude oil), bonds, and equities (Bouri et al. 2018a; Ji et al. 2018), and the Bitcoin market is not influenced by those asset classes. The weak correlation implies diversification and hedging benefits of Bitcoin against traditional asset classes (Bouri et al. 2017b; Corbet et al. 2018). Consistently, the study by Brière et al. (2015) showed that the inclusion of Bitcoin can be enhanced the risk-return trade-off of well-diversified portfolios and Bitcoin is an effective diversifier for the main world asset classes. The weak connectedness can be explained as Bitcoin prices do not mainly determine by the price factors of other asset classes (Bouoiyour et al. 2016), Bitcoin prices are less probable to depend on financial and economic factors (Ciaian et al. 2016), and Bitcoin has a specific set of characteristics including user anonymity (Ober et al. 2013), attractiveness (Kristoufek 2015), computer programming interest and prohibited activity (Yelowitz and Wilson 2015), and also energy prices (Li and Wang 2017). Remarkably, Baur et al. (2018) showed that Bitcoin is not linked with other asset classes during both calm and stress periods. The finding of prior studies also underscored that Bitcoin is a hedge against global risk (Bouri et al. 2017a) and there is a negative relationship between Bitcoin returns and economic policy uncertainty, implying a hedging ability of Bitcoin (Demir et al. 2018). Besides, Bouri et al. (2020a) revealed that Bitcoin is the least dependent on global stock markets, and Bitcoin has dominance diversification benefits compared to gold and commodities. Bouri et al. (2020b) also supported cryptocurrencies as a possibly valuable digital asset class against down movements in the US stock market. However, several empirical studies highlighted a significant relationship between the commodity and the cryptocurrency markets (e.g., Hayes 2017). Bouri et al. (2018a) provided significant evidence that the Bitcoin market is not isolated fully and Bitcoin returns are linked closely to those of most of the asset classes (e.g., equities, stocks, bonds). Bouri et al. (2018b) found that there is a likelihood to estimate Bitcoin price movements according to the aggregate commodity index and gold price information. Maghyereh and Abdoh (2020) found that there is a long-term relationship between Bitcoin returns and the S&P 500 while the association between Bitcoin and commodity significantly decreases in the short term. Overall, they found that Bitcoin can provide financial diversification in certain return quantiles and time frequencies. Recently, spreading the COVID-19 outbreak that resulted in rising economic and financial uncertainties globally, the interest of some scholars triggered to investigate the connectedness across various asset classes for providing insight to investors, portfolio managers, and regulatory bodies. For example, Bouri et al. (2021) by using the TVP-VAR investigated the linkage across various assets, and they found that the total relationship spikes and the structure of the relationship changes, which coincides with the COVID-19 pandemic. Siddique et al. (2021) by using continuous wavelet transform revealed that green bonds, gold, and bitcoin have the least relationship with the equity market during the COVID‐19 outbreak period, suggesting the hedge and safe haven ability of these financial assets. Thus far, the majority of studies reviewed above focused to investigate the return and volatility connectedness between Bitcoin, commodities (e.g., oil and gold), stocks, and bonds by employing various approaches before occurring the COVID-19 outbreak. While a few only studies examined the connectedness of the inclusion of COVID-19 in their studies (e.g., Bouri et al. 2021; Siddique et al. 2021), it remains unclear what relationship exists in terms of time and frequency horizons between the various asset classes in the global market, especially during the COVID-19 outbreak. Hence, this study fills this gap and significantly contributes by selecting the unique various asset classes indices in the global market during the COVD-19 episode using the daily data between 2017: M02 to 2021: M09. More specifically, we compare the co-movement between using asset classes by splitting data before the COVID-19 crisis period (28/02/2017 to 10/03/2020) and during the COVID-19 period (11/03/2020 to 30/09/2021). Besides, the present study sheds light by answering the following questions: (i) is there any causal link between equity, digital assets, commodity, and fixed income asset classes, especially during the COVID-19 period? (ii) if yes, in which direction(s)? (iii) Does the co-movement of using asset classes help reduce portfolio risk? To do so, this study follows the previous studies (e.g., Alam et al. 2019; Bouriet al. 2020a) and employs the wavelet coherence approach,Footnote 1 which is rarely used in this field and has some advantages over other methods. Employing the wavelet approach has an advantage and allows us to capture the causality direction from both time and frequency dimensions. Besides, this study contributes by examining the causal linkage between asset classes using the wavelet-based Granger causality test. Moreover, the present study performs the wavelet-VaR to investigate hedging properties among using asset classes. To the best of our knowledge, this may be the first study that conducts this relationship comprehensively using wavelet analysis during the COVID-19 period. The present study yields some consistently remarkable highlights. First, in contrast to the pre-COVID-19 period, strong relationships are found at different frequencies during the COVID-19 pandemics, multiple directions are unveiled with timescale differences, which uncovers that the co-movement and the causal association among the pairs are manifested by the difference in scale. Second, the wavelet-based Granger causality test corroborates the wavelet results and underscores there is a significant causal link between the variables during COVID compared to pre-COVID, indicating the lack of hedging opportunities. Third, the results of the value at risk (VaR) highlight that portfolio diversity advantages are variable with respect to time and frequency, which can be so useful for investors, portfolio managers, and policymakers. The rest of this study is organized as follows. Section 2 describes the data, models, and methodologies. Section 3 is followed by presenting empirical results. Section 4 concludes the paper.",10
46,4,Journal of Economics and Finance,05 July 2022,https://link.springer.com/article/10.1007/s12197-022-09584-w,Testing Effects of the Treasury single account system on the cost of borrowing in the OECD Countries,October 2022,Saliha Çınar,Aygül Anavatan,Fatih Deyneli,Female,Female,Male,Mix,,
46,4,Journal of Economics and Finance,02 July 2022,https://link.springer.com/article/10.1007/s12197-022-09590-y,"Trilemma of pandemic-related health emergency, economic policy uncertainty and partisan conflict in the United States: A time-varying analysis evidence",October 2022,Seyi Saint Akadiri,Andrew Adewale Alola,Ahdi Noomen Ajmi,Unknown,Male,Unknown,Male,"With the heightened partisan conflict in the United States, particularly between the two political parties of Democrats and the Republicans, there has been a growing concern regarding how the economic policies coupled with some uncertainties of this giant nation. More importantly, there is growing concern on how the government and policymakers are reacting to shocks the economy is facing in terms of COVID-19 pandemic which has claimed about 310,000 lives as at 15 December 2020.Footnote 1 It is paramount to state here that, the level of interconnectedness and interaction among nations affect political and economic outlooks, which somehow directly or indirectly create tension and a perception of political and economic instabilities. For instance, an economy that is associated with increasing economic policy uncertainty alongside heightened partisan conflict and unresolved pandemic issues, could be overwhelmed such that constrained policymakers in making appropriate, corrective and productive economic policies. Consequently, the question at the back of the authors mind is, are these variables (economic policy uncertainty, partisan conflict, and health-related issue such as pandemic) mutually exclusive? Is it possible for an economy to experience this situation simultaneously or is the interaction among the variables particular to a certain economic, political or financial era? If they were not mutually exclusive, what would be the impact on the economy? These questions among others motivate the authors to examine the topic under investigation. For better understanding of the interconnectedness and interaction between these variables, this study provides an example of mutual exclusivity situation pertaining to the events associated with the variables, and followed by the potential impacts on the economy. First, we discuss a situation where interaction between the variables is mutually exclusive, i.e. when economic policy uncertainty, partisan conflict and pandemic-related health emergency cannot happen simultaneously. In this situation, we assume either, economic policy uncertainty predict pandemic, pandemic predict partisan conflict or partisan conflict predict economic policy uncertainty and vice versa at a particular period. During a pandemic situation as aggravated by economic uncertainty, stable and less political divide between the parties (both in the Congress and in Senate) could proffer immediate solution. On the other hand, should pandemic-related health emergency heightened partisan conflict, the situation might not necessarily lead to economic policy uncertainty. Lastly, when partisan conflict causes economic policy uncertainty this situation could delay the policymakers’ immediate response to curtail pandemic as seen presently in the United States. Thus, we are of the opinion that, these variables might not necessarily occur at the same specific period. However, in a complex situation where the interactions between the variables occur at the same time (not mutually exclusive) then critical and immediate agreement actions should be taken. It requires, adopting a more balanced and less politically bias economic policy to mitigate the heightened partisanship, scientific experts that are non-partisan to implore health or pandemic-related regulations that could further heighten partisan conflict. We are of the position that, the services of such experts are to be optimized especially when there is a health emergency as that of COVID-19 pandemic. The aim of this current study is as follow: (i) to test if there is an existence of Granger causality between the variables and whether its date-stamped or not (ii) whether the interaction and interconnectedness between the variables under observation are mutually exclusive or not (iii) if they are or not, what are the potential impacts on economic stability and hence economic growth of an economy that is experiencing heightened partisan conflict, economic policy uncertainty and pandemic-related health emergency at the same time, which in this case, United States. To achieve our research objectives, we start by estimating a lag augmented VAR model suggested by Dolado and Lütkepohl (1996), Toda and Phillips (1994) and Toda and Yamamoto (1995) to conduct Granger causality test, using the Bayesian information criteria (BIC) with a maximum of 12 lags to implement the time-varying causality (TVC) tests. The time-varying causality employed is based on the forward window, recursive evolving and rolling-window algorithms, using a monthly frequency time series dataset between the periods January 1996 to June 2020 based on availability of data. This study is an addition to literature on economic policy uncertainty-pandemic-partisan conflict relationships. This current study establishes the fact that first; there is an existence of Granger causality relationship running from economic policy uncertainty to partisan conflict, from pandemic-related health emergency to partisan conflict and vice versa. Second, this study also establishes that, the causality relationships between the variables are date-stamped. Third, we found that, the causality nexus between the variables is mutually exclusive. Thus, policy suggestions from this study would be of interest to policymakers, government, authors, scholars and interested individuals most especially of economies with high partisan conflict, pandemic and economic policy uncertainty to come up with workable policies that would facilitate social, political and economic stability for an all-inclusive economic stability and hence growth both in the short- and long run. The study is broken into four different parts: Section one covers the introduction, where we discuss the motivation and contributions of the study. We proceeded in section two on the discussions of the data and econometric methodology adopted for empirical analysis. We present and discuss results in Section 3, while conclusion and potential policy suggestions is render in Section 4 of the study.",1
46,4,Journal of Economics and Finance,30 June 2022,https://link.springer.com/article/10.1007/s12197-022-09587-7,Examining the spillover effects of volatile oil prices on Iran’s stock market using wavelet-based multivariate GARCH model,October 2022,Siab Mamipour,Sanaz Yazdani,Elmira Sepehri,Unknown,Female,Female,Female,"The oil market is one of the main markets in the world, usually impacting other markets. In other words, oil price fluctuations cause upheavals in other markets, including the stock market, though the reverse is not typically true. As a major oil-exporting country, Iran’s political, economic, and other sectors are significantly affected by oil price fluctuations. This is due to oil revenues as a principal component of the government budget indirectly affecting other economic activities. For oil-importing countries, oil price shocks negatively impact economic activities. Increases in oil prices lead to higher production costs since oil is one of the most critical factors of production (Backus and Crucini 2000; Arouri et al. 2011). The transfer of these higher costs to consumers increases consumer expenditures and, eventually, leads to a reduction in demand. Lower consumption will result in lower production and, consequently, higher unemployment (Lardic and Mignon 2006). Stock markets react negatively to any rise in crude oil prices resulting in a decline in the stock exchange index in these countries (Sadorsky 1999). Empirical studies have shown the negative relationship between oil prices and stock market returns based on the fact that oil prices are assumed as a risk factor for the stock market (Jones and Kaul (1996)). Researchers such as Filis et al. (2011), Chen (2010), Miller and Ratti (2009), and O’Neill et al. (2008) have provided evidence proving this negative relationship, although, for oil-exporting countries, there seems to be a direct relationship between oil price shocks and stock market efficiency (Arouri and Rault 2010). Many scholars argue that oil prices indirectly affect the stock market, and macroeconomic indicators confirm this relationship. Bjørnland (2000) believes that an increase in oil prices and, consequently, an increase in the income of oil-exporting countries will positively affect their economic performance. Increased incomes are expected to lead to higher spending and investment and the injection of financial resources into the economy, which, in turn, will increase production and reduce unemployment. Hence, stock markets tend to react positively to such events. The spilling over of oil market price fluctuations into the stock market can vary depending on the length of time (short, medium, and long term). Also, the intensity of fluctuations or shocks from the oil market on stock markets varies over the various time scales. Given these issues and the lack of research on this matter in Iran, the current paper explores the volatility spillover effects between the oil and stock markets over different time frames. It investigates the issue over the short, medium, and long-term periods (different frequencies) via wavelet transformation, followed by an examination of volatility spillover in each time scale. The main advantage of this method is that spillover effects are studied in different time scales, and that volatility spillover is investigated thoroughly and in detail in each time scale. The subsequent sections are organized as follows: First, the literature review is presented, followed by the methodology used in the model, the definitions of variables, and the various method stages. Then, the model’s estimations and analysis of the findings are presented. Finally, the results obtained are reported together with their policy implications.",
47,1,Journal of Economics and Finance,23 September 2022,https://link.springer.com/article/10.1007/s12197-022-09604-9,A synthetic control analysis of U.S. state level COVID-19 stay-at-home orders on new jobless claims,March 2023,John Gibson,Xiaojin Sun,,Male,Unknown,Unknown,Male,"The spread of COVID-19 across the U.S. during Spring 2020, caused many states to enact unconventional policies as a means of limiting the impact of the virus, a process that has become colloquially known as “flattening the curve.” Of these new policies, the most controversial has been statewide stay-at-home (SAH) orders, which effectively shutters non-essential businesses and severely limits social interaction. Between mid-March and early April 2020, 43 states implemented statewide SAH orders, while 7 never implemented such orders. The goal of this paper is to estimate the treatment effects of SAH orders on economic conditions, focusing attention on the change in weekly new jobless claims in the period immediately following the adoption of the policy.Footnote 1 The connection between statewide SAH orders and the labor market is natural. Many opponents of the policy express concerns that such restrictions may severely hurt the labor market, increase unemployment, and result in economic costs that out-weight the health benefits.Footnote 2 If this is true, then we should observe large spikes in new jobless claims around the time SAH orders are enacted. However, it is also possible that other factors associated with the spread of COVID-19, such as a rapid increase in the number of infections and a general reduction in consumer confidence, may also result in a sharp uptick in new jobless claims around the same time. As such, determining the treatment effects of SAH orders on the labor market requires careful consideration of both effects. In this paper, we use the synthetic control method (our primary specification) to determine the impact of SAH orders on new jobless claims. The synthetic control method developed by Abadie and Gardeazabal (2003) and Abadie et al. (2010), by combining elements from matching and difference-in-differences (DiD) techniques, is able to provide reliable estimates of treatment effects in much more general contexts.Footnote 3 The main advantage of the synthetic control method is that it estimates the treatment effect by creating a hypothetical or “synthetic” control unit, a weighted average of all other untreated units (donor pool), that resembles the treatment unit in the pre-treatment period. This avoids choosing any individual unit from the untreated group to match the treated unit. As for our research question, the presence of 7 states that never implemented SAH orders provides a natural donor pool from which we can construct a synthetic control unit to best match each of the 43 states with SAH orders. Our findings suggest that the negative impact of SAH orders on the labor market may be overstated. Specifically, while we do observe a spike in new jobless claims following the start of SAH orders, we observe a similar spike in the synthetic counterfactual states where no policy was implemented. In short, our findings suggest that over two-thirds of the spike in new jobless claims observed in the data is a direct result of the virus, not the policy response. We also re-estimate our treatment effects using DiD and find consistent results. Given the importance and topical nature of the research question, there are several recent working and published papers that directly relate to our present work. While our paper focuses on new jobless claims as the variable of interest, several previous works have examined the economic impact of COVID-19 and the subsequent policy responses using output as a primary measure (e.g., Luo and Tsang 2020 and Makridis and Hartley 2020). One exception is the work of Baek et al. (2021), which also estimates the causal effects of SAH orders on unemployment insurance claims. The authors construct an employment-weighted measure of the duration of exposure to SAH orders for each state using county-level data and estimate the marginal effect of an additional week of SAH exposure on state-level weekly initial new jobless claims. While the present paper and Baek et al. (2021) share a similar research question, the methodologies employed differ in three important ways: (1) (Baek et al. 2021) estimate the marginal effect of an additional week of SAH exposure, while we focus on the implementation of the SAH order as an event or treatment. (2) While (Baek et al. 2021) include important controls in their analysis, their main empirical specification still relies on a state-level cross-sectional regression. In contrast, we use the synthetic control method, which provides substantial advantages as a research design in social sciences with the aim to estimate the causal effects of aggregate interventions; see Abadie (2021). (3) Baek et al. (2021) use an employment-weighted measure of the duration of exposure to SAH orders constructed from county-level data, which allows them to consider the impact of both county-level and state-level SAH orders. In contrast, we focus on state-level policy interventions that are universal for all counties in a state. Interestingly, while the two papers approach the research question in very different ways, both reach a similar conclusion. Specifically, our results suggest that SAH orders account for only about 32 percent of the increase in new jobless claims during the early stages of the COVID-19 pandemic, while Baek et al. (2021) find that SAH orders account for only 24 percent of the increase. The fact that two studies using very different empirical strategies, reach similar conclusions regarding the impact of SAH orders on the labor market provides additional support to the individual findings.",1
47,1,Journal of Economics and Finance,12 December 2022,https://link.springer.com/article/10.1007/s12197-022-09609-4,Volatility and dependence in energy markets,March 2023,Jinan Liu,Apostolos Serletis,,Female,Male,Unknown,Mix,,
47,1,Journal of Economics and Finance,09 August 2022,https://link.springer.com/article/10.1007/s12197-022-09597-5,Asymmetric effects of exchange rate volatility on trade flows: evidence from G7,March 2023,Mohsen Bahmani-Oskooee,Huseyin Karamelikli,Farhang Niroomand,Male,Unknown,Unknown,Male,"When international monetary system changed from fixed to relatively more flexible exchange rates in 1971, proponents of the fixed rate arguted that flexible rates will introduce uncertainty into international markets that could hurt trade. After ten years when enough data were available, reserahcers began testing the impact of exchange rate uncertainty measured by volatility of exchange rates on trade flows. Since data were available mostly for industrial countries, they estimated trade models by using data from industrial countries. The first study was by Akhtar and Hilton (1984) who considered exports and imports of Germany and the U.S. and found negative effects of exchange rate volatility on all trade flows except U.S. imports. Similar negative effects were also reported by Arize (1995a, 1995b, 1997) who only considered export volume of the Netherlands, Sweden, Denmark, Switzerland, and G7. However, when Arize (1998) considered import volume of the same industrial countries, some positive effects of exchange rate volatility on imports were observed. Positive effects were also reported by Arize and Malindretos (1998) for Australian exports. Other studies that have reported mixed effects are: Kenen and Rodrik (1986), Asseery and Peel (1991), Chowdhury (1993), Arize and Shwiff (1998), De Vita and Abbott (2004), Medhora (1990), Bahmani-Oskooee and Ltaifa (1992), Bahmani-Oskooee (1996), Doroodian (1999), Arize et al. (2000), Sauer and Bohara (2001), Hall et al. (2010), Olayungbo et al. (2011), Serenis and Tsounis (2014), Asteriou et al. (2016), and Senadza and Diaba (2017). However, even before discovery of positive effects of exchange rate volatility on trade flows, De Grauwe (1988) and Peree and Steinherr (1989) argued and theoretically demonstrated that exchange rate volatility could indeed have positive effects on trade flows, depending on the degree of risk aversion by traders. Risk averse traders will trade less to avoide uncertain future prices. Risk tolerant traders may trade more to generate more revenue today in order to cover any income loss in the future due to uncertain prices. Bahmani-Oskooee and Hegerty (2007) is the latest review article that shows the negative and positive effects are supported by empirical literature that use data from developed as well as developing countries. If we label the above studies as the old literature, they all have a common feature in that they have all assumed that the impact of exchange rate volatility on trade flows is symmetric. However, this need not be the case if traders reactions and expectations change when an exchange rate becomes more volatile versus when it becomes less volatile, hence possibility of asymmetric effects. These asymmetric effects of exchange rate volatility on trade flows were recently demonstrated by Bahmani-Oskooee et al. (2020) using aggregate trade flows of eight Asian countries, i.e., Pakistan, Japan, China, Korea, Singapore, Malaysia, the Philippines, and India who showed that in most countries, the effects are asymmetric. In search of additional support, Bahmani-Oskooee and Arize (2020) considered experiences of 13 African countries (i.e., Algeria, Cameroon, Ethiopia, Ghana, Kenya, Mauritius, Morocco, Nigeria, South Africa, Tanzania, Tunisia, Uganda, and Zambia) and again, reported asymmetric effects in most cases. Since asymmetric analysis in this new literature requires estimating nonlinear models versus the linear models in old literature, it appears that nonlinear models yield relatively more significant outcomes compared to linear models which is the direction for future research.Footnote 1 We add to this new literature on asymmetric effects of exchange rate uncertainty on trade flows by considering the experiences of G7 countries (i.e. Canada, Germany, France, Italy, Japan, U.K., and the U.S.).Footnote 2 To that end, in Section 2 we introduce the linear and nonlinear models as well as estimation methods. We then present empirical results in Section 3 that is followed by our summary in Section 4. Finally, data sources and definition of variables are cited in an Appendix.",2
47,1,Journal of Economics and Finance,17 September 2022,https://link.springer.com/article/10.1007/s12197-022-09598-4,"Non-linear structures, chaos, and bubbles in U.S. regional housing markets",March 2023,Benjamas Jirasakuldech,Riza Emekter,Thuy Bui,Unknown,Male,,Mix,,
47,1,Journal of Economics and Finance,18 August 2022,https://link.springer.com/article/10.1007/s12197-022-09600-z,The Predictive Power of Financial Stress on the Financial Markets Dynamics: Hidden Markov Model,March 2023,Hayet Soltani,Mouna Boujelbène Abbes,,Female,Female,Unknown,Female,"Following the succession of financial crises, stock market and oil crashes, the financial markets experienced a period of major financial shocks called “Financial Stress” which appears to be an important factor in determining the dynamics of stock prices during stressful periods. Indeed, setting the definition of financial stress is a daunting task. It is the same as financial instability (Borio and Lowe (2002), Bloom (2009)). Moreover, on the financial markets, the stress index is a purely financial indicator with a double objective: to be an indicator of stress measurement and to be a leading indicator of economy, in order to predict macroeconomic trend reversals. Therefore, financial stress can be defined as “being the force exerted by uncertainty and the change in expectations of losses on the economic agents of markets and financial institutions” (Zayati and Gaaliche 2014). In fact, Sum and Brown (2014) focused on the relationship between financial stress and the performance of the US real estate market proxied by Real Estate Investment Trust returns. They conclude that, depending on the spike in financial stress, the response of returns on the CRSP Ziman REIT indices and sub-indices is negative in the first few months. Moreover, Apostolakis and Papadopoulos (2014) investigated financial stress co-movements and spillovers among the G7 economies for the period 1981–2009. They show a positive association of financial stress co-movements and spillovers with both financial crises and uncertainty and financial stress play an important role in the financial markets. Further, Das et al. (2018) focused on the relationship between stocks, crude oil, gold and financial stress during the period of 1993–2017. They have found evidence of a significant bi-directional causality in mean and variance of financial stress with gold and crude oil. More specifically, over the past two decades, the stock markets have experienced various episodes of crises and crashes, such as the great global recession of (2007-2009), the political turmoil (2010-2011), the oil crisis (2014-2015), the Covid-19 pandemic (2019-2021).In fact, the outbreak of the COVID-19 pandemic has affected significantly the global financial markets (Al-Awadhi et al. 2020; He et al. 2020; Okorie and Lin 2021). Thus, the stock markets reacted more proactively to the increase of the confirmed COVID-19 cases as compared to the increase in the number of deaths. Further, Liu et al. (2020) indicate that the novel coronavirus had a significant negative impact on stock market returns across all affected countries, and the numbers of confirmed COVID-19 cases significantly hit the major stock indices performances, particularly in Asia, where they suffered a greater decline. In this scenario, the proliferation of economic, financial, political, social, oil and health crises is referred to as a source of systematic risk and increases uncertainty on the financial markets, which will in turn increase the level of financial stress. As a result, this relationship prompts us to investigate the impact of financial stress regime change on stock market returns dynamics. To this end, we utilize the Hidden Markov Model (HMM), which is a probabilistic process that uses the current state to predict the next one and allow us to predict the state transition that depends on the current state and not on the past states. On the financial markets, the use of this model is becoming more and more important, especially in improving investment decisions. In this context, the financial stress effect has been mainly studied in the literature, but so far, its impact on stock market dynamics has not been fully explored for the MENA financial markets. To our knowledge, this is the first study among the current literature that uses Hidden Markov Model to explore the predictive effect of financial stress on regime change of the financial market returns and implement further study on the financial effects of the Covid-19 outbreak on the MENA region. We considered three financial market states: bullish stress (S1), bearish stress (S2), and calm stress (S3). We proposed a Hidden Markov Model based on the transition matrix to capture not only financial stress but also uncertainties in the MENA region. The outline of the paper proceeds as follows: Section 2 reviews the relevant literature on the predictive effect of financial stress on stock market performance. The data and methodology will be described in section 3 and 4. Section 5 provides the research findings and discusses the possible implications of the findings. Finally, we conclude.",
47,1,Journal of Economics and Finance,17 August 2022,https://link.springer.com/article/10.1007/s12197-022-09602-x,Stock and oil price returns in international markets: Identifying short and long-run effects,March 2023,Theophilus Teye Osah,Andre Varella Mollick,,Male,Male,Unknown,Male,"Despite different perspectives in examining the relationship between oil price and the stock market, the evidence has been mixed. This conundrum continues to engender new studies that attempt to shed light on this relationship. In this vein, we combine dynamic ordinary least squares (DOLS), a long-run econometric technique that has received little or no attention in stock market-oil price research, with a short-run method to investigate the association between oil price and the aggregate stock market. Research on the economic link between stock returns and oil price returns is well-documented. One strand of literature employs econometric techniques such as cointegration methods to focus on the long-run perspective (Sahu et al. 2014; Alamgir and Bin Amin 2021). Additionally, researchers consider the stock return—oil price nexus for oil-importing and oil-exporting countries with contrasting findings (Bouri 2015; and Gupta 2016). Besides, there are papers that use a panel set of countries (Arouri and Rault 2012; Westerlund and Sharma 2019). Most studies concentrate on the response of stock price to oil prices in developed economies such as the U.S., UK, Australia, Canada, and other European countries (Am and Shanmugasundaram 2017). On the other hand, Sharma et al. (2018), Alamgir and Bin Amin (2021), among others, focus on emerging markets. Using Vector Autoregressive (VAR) models, some authors consider the response of the stock market to shocks in oil prices (Sadorsky 1999; Papapetrou 2001; Kang and Ratti 2013). For instance, Kilian and Park (2009) apply Kilian (2009) to the U.S. stock market and report that stock market returns do not respond to supply-side shocks, while positive (negative) responses are observed after aggregate demand (precautionary demand) shocks. Arampatzidis et al. (2021) find results, consistent with Kilian and Park (2009), for the aggregate stock market and for forty-nine U.S. industry-specific portfolios. In contrast, Cunado and de Gracia (2014) suggest that oil supply shock is the main driver of a negative and significant impact of oil price changes on most European stock market returns, whereas the effect of oil price change triggered by demand shock can be either positive or negative. Others examine the time-varying relationship between oil shocks and stock market performance using either multivariate Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models, Time-Varying Parameter Vector Autoregressive models, and others (Lee and Zeng 2011; Moya–Martínez et al. 2014). The measure of the stock market also varies from aggregate indices to sectors and to firm-level evidence. Notwithstanding these various perspectives that shed light on the stock market—oil price nexus, the findings are inconclusive while the relationship also misses evidence combining short and long-run approaches. In this paper we complement long-run DOLS with evidence on return predictability more commonly associated with short-run effects to examine the impact of changes in West Texas Intermediate (WTI) oil price on aggregate stock markets. We also use monthly data from 1990 to 2020 which provides a sufficiently long time-span to better ascertain the properties of cointegrated variables by implementing the DOLS methodology. The long length of the sample period is important given that some authors document a time-varying oil-stock market relationship (Filis et al. 2011; Sadorsky 2012). Lothian (1990) argues that the difference in the length of a data series may be crucial. Our sample countries include six oil-exporting and six oil-importing countries, with equal representation of emerging and advanced economies in each set. For our short-run specification, we first follow Narayan (2019) and include a measure of uncertainty, namely market volatility index (VIX) or geopolitical risk (Gpr). In other analyses, we augment our models with discount rates to examine the effects of monetary policy in our set-up. Starting with country-by-country OLS regressions with fixed or flexible lag-lengths, we document that the effects of oil price returns vary in the short-term: from changing signs in Brazil (formerly an oil importer and more recently a major oil producer) to negative effects for 3 oil exporters to no effects for 2 oil exporters. Thus, the relationship is ambiguous for oil-exporters using the Narayan (2019) model. By contrast, we observe negative oil price effects on stock markets for all 6 oil importers, though the relationship is not significant in 2 of these markets. We also find that Johansen cointegration tests confirm the existence of long-run trends between stock market indices, oil prices, VIX, and long bond yields; the result of this tests justify the use of DOLS to generate estimates for the long-run. The distinctive result with the DOLS is that the oil price effect on stock markets now becomes positive and significant for all oil-exporting economies with coefficients varying from 0.077 in the U.S to 0.736 in Brazil. Interest rate changes have negative effects in 10 (almost one to one: -1.048 in Mexico, -1.101 in Russia, -0.909 in the U.S., and 0.920 in Germany) out of 12 countries. VIX has mixed effects in the long-run, in contrast to its always negative effects in short-run modelling. In further analysis, we employ fixed effects panel regressions which are known to yield more robust findings compared to individual-specific studies. Panel analysis increases the precision of estimators and provides more reliable inferences, which leads us to pool the data into two sets comprising our sample of oil-exporting and oil-importing countries. The panel analysis corroborates our findings with country-by-country regressions. While the effect of oil price on the stock markets of net oil exporters may appear to be negative or unclear in predictive regressions, long-run econometric methods uncover significant gains in these stock markets when oil prices go up. This study adds to debate on the heterogeneity of the effect of oil price movements on stock markets of oil-exporting countries versus oil-importing countries. Studies for oil-exporting countries suggest a positive relationship between oil and stock prices, while studies of oil-importing countries report a decline in stock prices as oil price rises (Sadorsky 1999; Park and Ratti 2008; Bjornland 2009; Cunado and de Gracia 2014). There is contradictory evidence, however. Using Nonlinear Autoregressive Distributed Lag, Alamgir and Bin Amin (2021) document a positive relationship between the world oil price and stock market index for 4 selected South Asian countries. They specifically note that the “tendency for stocks to move along with world oil prices is entirely unexpected, especially in countries like Bangladesh, India, Pakistan, and Sri Lanka, net importers of oil” (Alamgir and Bin Amin 2021, p. 699). Similarly, Prabheesh et al. (2020) find a positive co-movement between oil price returns and stock price returns during the COVID-19 period for China, India, Japan, and Korea. Cong et al. (2008) employ a multivariate vector auto-regression and report that oil price shocks do not have statistically significant effects on the real stock returns of most Chinese stock market indices, except for the indices of manufacturing and some oil companies. Our paper complements these studies by providing evidence that DOLS techniques clearly identify positive effects of oil price changes on stock markets of oil exporters relative to the negative oil price effects for oil importers. This observation is important, particularly given that even studies that decomposed oil price shocks, such as Cunado and de Gracia (2014), document findings that contradict Kilian and Park (2009). Moreover, in spite of various econometric techniques (including SVAR, GARCH, Vector Error Correction Models, wavelength analyses, among others) employed by other studies, the lack of consensus in findings remains unresolved. We also provide evidence based on panel data which supports time series studies, such as Mollick and Assefa (2013), who document that VIX exerts a negative impact on stock markets. The rest of this paper is organized as follows. Section 2 lists the data sources and reviews the data properties; Section 3 introduces the empirical models; Section 4 presents the results and Section 5 concludes this article.",2
47,1,Journal of Economics and Finance,07 October 2022,https://link.springer.com/article/10.1007/s12197-022-09603-w,Firm specific pass through and heterogeneity,March 2023,Zahra Sheidaei,,,Female,Unknown,Unknown,Female,"A large number of empirical and theoretical studies document the fact that exchange rate variations are not fully passed through to prices of traded goods, The literature on incomplete pass-through started with Dornbusch (1985); Feenstra et al. (1996); Engel (1999) and Goldberg and Verboven (2001) and received a lot of attention in the literature (For example: Amiti et al. 2014; Goldberg and Campa 2006; Gopinath and Rigobon 2008; Parsley and Wei 2001). There also has been an increasing interest in the identification of the factors that determine incomplete pass-through, factors such as the elasticity of demand and market structure (Atkeson and Burstein 2008; Bergin and Feenstra 2001; Dornbusch 1985), Local cost and price rigidity (Bacchetta and Wincoop 2003; Devereux and Engel 2002; Kasa 1992; Nakamura and Zerom 2010). How the degree of pass-through is different across firms has become one of the main question in the literature of pass-through. In this regard, Berman et al. (2012) using French firm level data conclude that firms increase their markups as a result of an exchange rate depreciation and high performance firms do so more. Chatterjee (2013) with a model of multiproduct firms finds the similar results and check the results on Brazilian data. Studies such as Auer and Schoenle (2016) and Garetto (2016) imply that there is a U-shaped function between firm level pass-through, firm productivity and size. Martin and Rodriguez (2004) using Spanish firm level data show that Spanish firms increase their markup following an exchange rate depreciation. While Ravn et al. (2010) find that firms decline their markups about on average 20 percent in response to a cost shock. Firms may also choose the strategy to set different markups over different destination on exchange rate changes (Atkeson and Burstein 2008). Corsetti et al. (2007) focused on market structure that firms are vertically integrated and market-specific nominal rigidities exist in downstream level, while upstream producers discriminate pries across countries, exacerbating the distortions from monopoly power. Berman et al. (2012) extend the model with distribution costs of Corsetti and Dedola (2005), allowing for firm heterogeneity where single-product firms differ in their productivity. One of the features that missing in the literature of pass-through is the firm’s cost structure which is discussed in this paper, most of the factors affecting incomplete pass through can best be analyzed by the firm’s cost function, as most existing theoretical literature used in applied analysis of pass-through restrict the marginal cost as being constant, which means that the unit-cost of production remains constant as the scale of production increase. Since pass-through is determined by the elasticity of supply and demand under imperfect competition, we reduce the role of supply when we only consider returns to scale as constant in our analysis. To develop this new theoretical explanation, this assumption is reduced, keeping in mind that supply condition also play important role in the strategic behavior of firms. In the literature (Fabinger and Wey 2012) consider this issue by proposing an Adjustable pass-through (Apt) class of demand functions to avoid this restriction, this paper introduces variable marginal cost in the supply side to contain its role in the pass-through analysis. The central element of the proposed theory is to consider the firm’s cost function as a source of heterogeneity reflecting its technology of production, therefore impose no restrictions on their terms of returns to scale which let us test different hypothesis concerning the technology of production. This paper presents a simple model of firm heterogeneity, featuring different kind of returns to scale and international price setting where the market for each good has the characteristics of monopolistic competition. The main aim is to examine how idiosyncratic marginal cost affect the pricing policy of individual firms producing differentiated goods, thus linking up more closely with the empirical literature on cost pass-through. The model is based on the work of Melitz and Ottaviano (2008) on linear demand systems and endogenous markups while features the variable marginal cost as the main driver of heterogeneity of price responses. First single-product firms are analyzed and then examined how better performance firms react differently from weaker performances. For a wide range of parametrization, it is concluded that high performance firms (firms who experience diminishing marginal cost as output increases) tend to charge higher prices over their marginal cost relative to firms with constant marginal cost and have higher markups. conversely, low performance firms, (firms with lower technology of production which result in increasing marginal cost) charge lower prices than firms with constant marginal cost. The proposed model demonstrates that following an exchange rate depreciation, higher performance firms absorb more this variation in their markups relative to weaker performance firms. The intuition behind that may be as follows that better performance firms as already set higher prices can absorb more exchange rate variations in their markups, but weaker performance firms since they set lower prices over their marginal cost and have tiny markups as well as less room for absorbing exchange rate variations, then in response to an exchange rate depreciation pass more these changes and increase more their markups. The predictions of the model are tested using the Iranian firm level data, observations from non-petroleum manufacturing firms over 2000-2007. As the exchange rate is highly volatile in Iran, it can be a good sample of exploring the theory. The estimates confirm the prediction of the proposed model and provide some evidence in support of the fact that firms behavior to an exchange rate variations depends most on the degree of firm’s technology of production. The remainder of the paper is organized as follows. In Section 2, the closed economy version of the model is presented. In Section 3, the model extended to characterize the heterogeneity in the open economy model which provides the implications for different pass-through among firms. Section 4, tests the model prediction using firm level data from Iranian manufacturing. Section 5 concludes.",
47,1,Journal of Economics and Finance,10 August 2022,https://link.springer.com/article/10.1007/s12197-022-09599-3,Growth opportunities and earnings management by cross-listed and U.S. firms,March 2023,Shrikant P. Jategaonkar,Linda M. Lovata,Xiaoxiao Song,Unknown,Female,Unknown,Female,"Existing earnings management studies have largely focused on identifying suspect firms based on the timing of earnings reporting or a corporate event.Footnote 1 We take a different approach and argue that some firms routinely manage earnings due to a particular firm characteristic, viz, growth opportunities. Another important dimension of our approach is the inclusion of foreign firms that are cross-listed in the U.S. The objective of our study is twofold: (i) to examine whether a cross-listed firm’s growth opportunities are associated with its earnings management and (ii) to compare and contrast the relation between growth opportunities and earnings management exhibited by cross-listed firms versus the U.S. firms. The indicators and motivations behind earnings management have been a topic of interest in academic research for decades, but how a firm’s growth opportunities are associated with earnings management remains unclear. While some (Collins and Kothari 1989; Matsumoto 2002; Skinner and Sloan 2002; Jensen 2005) argue that growth firms, relative to value firms, are more likely to use earnings management to report higher earnings, others have reported mixed empirical findings. Using a sample of U.S. firms, Madhogarhia et al. (2009) document significantly higher discretionary accruals earnings management (AEM) by growth firms. Similarly, Houmes and Skantz (2010) find that ‘glamour’ firms exhibit higher AEM. On the other hand, Chung et al. (2005) show that low-growth firms with high free cash flows exhibit more AEM. Charitou et al. (2011) investigate earnings quality of firms with different health characteristics and growth prospects and conclude that value firms manage earnings toward a positive target more frequently than growth firms.Footnote 2 AlNajjar and Riahi-Belkaoui (2001) conclude that firms with high growth opportunities use income decreasing accruals to reduce political costs and political risks and Nabar and Song (2017) find that growth firms exhibit lower real activities earnings management (REM).Footnote 3 In short, the relation between growth opportunities and earnings management is not fully understood in the literature. In addition, these studies have ignored the group of firms cross-listed in the U.S. Our goal is to fill this gap in the literature and provide further insight into the relationship between earnings management and growth opportunities. A better understanding of this relationship can enable investors and analysts to better assess the quality of earnings of these companies. We contrast earnings management by cross-listed growth and value firms versus U.S. growth and value firms. While cross-listed firms are expected to abide by the U.S. Securities and Exchange Commission (SEC) regulations, there are environmental factors resulting in different motivations and enforcement mechanisms (Lang et al. 2006; Srinivasan et al. 2015).Footnote 4 Therefore, our study empirically tests whether cross-listed and U.S. firms exhibit a similar relation between growth opportunities and earnings management. We compare AEM and REM by: (i) cross-listed growth (CLG) versus cross-listed value (CLV) firms, (ii) CLG versus USG firms, and (iii) CLV versus USV firms. A major contribution of this study is the comparison of earnings management by CLG versus CLV firms. Our sample consists of total 11,468 firm-year observations from 1990 to 2019. Half of the observations are cross-listed firms and the other half are matching U.S. firms. We use the Tobin’s Q ratio as a proxy for growth opportunities to differentiate between firms with high and low growth opportunities. Unlike many other studies, the focus of our analysis is not centered on the timing of reports or specific corporate events. Instead, our goal is to examine which type of firms engage in earnings management over time. Therefore, we follow prior studies and use the absolute value of abnormal discretionary accruals and the absolute value of abnormal real activities to measure the magnitude of AEM and REM.Footnote 5 Our findings can be summarized as follows: (i) CLG firms exhibit significantly more AEM and REM than CLV firms, (ii) CLG firms exhibit significantly more AEM and REM than USG firms (except for the cash flow measure), but (iii) there is no significant difference in earnings management between CLV and USV firms. Accordingly, there are two main takeaways from the findings. First, both cross-listed and U.S. growth firms engage in more earnings management than the value firms, consistent with the notion that growth firms have stronger incentives to manage earnings. Second, while previous studies have concluded that cross-listed firms tend to manage earnings more than U.S. firms, we find that this difference between cross-listed and U.S. firms is limited to growth firms. Therefore, analysts and investors should be most skeptical of the quality of earnings for CLG firms. For robustness checks, we use two other proxies (the market-to-book ratio and sales growth) for growth opportunities and find consistent results.Footnote 6 We also verify that our results are not influenced by observations from a specific country or a specific period in our sample. Our study contributes to the literature by separating the cross-listed and U.S. firms into growth and value firms and comparing their earnings management behavior. There is still disagreement in the literature as to how growth opportunities are associated with AEM and REM. We provide valuable insight into this issue. The findings of the study have implications for investors, analysts, and policy makers. Investors and analysts can better assess the quality of earnings presented in financial statements by knowing the tendencies of each category of company. Also, policy makers, such as SEC, can determine efficient strategies to constrain earnings management by focusing on the most flagrant offenders. The remainder of the paper is organized as follows: “Review of Literature and Hypotheses Development” discusses the relevant literature and develops our hypotheses; “Data and Methodology” describes sample selection process, variables, and our models; “Empirical Results” describes the findings; “Additional Tests” explains alternative tests and robustness checks; and the “Conclusion” summarizes the study.",2
47,1,Journal of Economics and Finance,05 November 2022,https://link.springer.com/article/10.1007/s12197-022-09605-8,"The trilogy of economic policy uncertainty, earnings management and firm performance: empirical evidence from France",March 2023,Ines Kahloul,Jocelyn Grira,Khawla Hlel,Female,Female,Unknown,Female,"Managers and investors have different targets when making decisions, due to their different horizons. This is likely to lead managers to use reporting discretion to present a situation that is not consistent with the firm’s true performance (Khuong et al. 2019). Hence, managers exercise different methods to reach personal objectives, some of which are legal and some of which are illegal. This phenomenon is commonly referred to as earnings management (Khuong et al. 2019). Earnings management is defined as the deliberate altering of financial information in order to either mislead investors about firm performance, or to obtain contractual profits that depend on reported accounting figures (Shackelford and Shevlin 2001). Financial economists and accountants have recognised for years three different categories of earnings management: accrual earnings management (Dechow et al. 1995), real earnings management (Roychowdhury 2006) and classification shifting (McVay 2006). The existing literature has extensively highlighted the relationship between earnings management and firm performance (Teoh et al. 1998; Krishnan 2003; Tang and Chang 2015; Chen et al. 2016). However, there is mixed evidence in the literature regarding the association between firm performance and earnings management practices. Indeed, managers may increase or decrease their reported earnings to exhibit a firm’s performance and obtain greater compensation. Hence, earnings management can be beneficial or harmful for firms based on how managers use the practice (Holthausen 1990). From the agency perspective, conflicts of interests between managers and external shareholders lead the former to abuse accounting discretion in order to increase their own wealth at the expense of external shareholders. Then, earnings management has a negative impact on firm performance (Teoh et al. 1998). In contrast, the efficient contracting theoretical perspective argues that earnings management is beneficial when managers use accounting choices to reduce agency costs and create a net increase in aggregate wealth for shareholders (Jiraporn et al. 2008). Likewise, the informational perspective suggests that accounting methods are likely to reveal managerial expectations about the firm’s future cash flows (Holthausen 1990). The effect of earnings management on firm performance is then ambiguous. The purpose of this study is to examine how earnings management influences firm performance in the French context, where firms mostly feature concentrated ownership structures and are held by families. In the meantime, firm performance can be affected by economic policy uncertainty (EPU). This is because the environment in which a company operates is dynamic and is shaped by the decisions of politicians and institutions. Therefore, EPU changes will influence the economic environment of the company (Kang et al. 2014). Moreover, the potential effects of uncertainty about taxes, future government policies, and regulatory and monetary policies are likely to impact corporate financial decisions. EPU is referred to as a major reason for the sluggish economic recovery following the global financial crisis of 2008–2009 (Baker et al. 2016). In Europe, numerous events have boosted doubts about the future of the Euro and economic policies. For instance, Russia’s annexation of Crimea and the refugee crisis have led to an increase in right-wing political ideologies and raised terror threats and have disrupted relationships within and among nations (Kang et al. 2014). Economic policy uncertainty is relevant to investors and companies. It has been studied frequently in economics literature in regard to its impacts on society and economic growth (Handley and Limao 2017; Luttmer and Samwick 2018). Yet, there is increasing evidence that uncertainty is a crucial factor that impacts economic outcomes. A growing number of recent literature investigates the consequences of the EPU on firms’ decisions. For instance, the literature focuses on how economic policy uncertainty affects different aspects of managerial decision making, such as R&D spending, capital investment, cash holding, advertising and hiring (e.g.,Pindyck 1993; Bloom 2009; Gulen and Ion 2016; Phan et al. 2019). EPU has always played a crucial role in shaping economic results, as evidenced by the recent slow economic growth in many countries that are currently experiencing policy uncertainty (Luttmer and Samwick 2018). We then investigate the moderating role of economic policy uncertainty on the relationship between earnings management and firm performance. Recently, Yung and Root (2019) found that EPU positively affects earnings management. The authors also found that policy uncertainty induced by earnings management harms firm value. This study makes several contributions to the literature. First, we aim to expand the earnings management literature for French firms by using two alternative techniques regarding earnings management. The first is real earnings management, which occurs when managers intentionally make operating decisions that have actual cash flow consequences, with the goal of altering reported profits. The second is classification shifting, in which core expenses are shifted to special items in the income statement to increase earnings before extraordinary items. Second, we shed the light on the role of economic policy uncertainty as a harmful economic factor that increases opportunistic earnings management practices and decreases firm performance. Finally, our study extends the work of Stein and Wang (2016), Yung and Root (2019) and Iqbal et al. (2020) by providing new evidence that, when financial markets are less certain about their future prospects, managers behave opportunistically in order to privilege their own interests at the expense of external shareholders, leading to a decrease in firm performance. Specifically, we extend Yung and Root’s (2019) work, which focuses on the effect of EPU on earnings management and how it could affect a stock market-based measure of firm performance. We highlight how EPU influences the relationship between earnings management practices and accounting firm performance. Our study is based on a sample of 323 French listed firms between 2010 and 2017. We show that earnings management (proxied by accrual earnings management, real earnings management and classification shifting) positively affects firm performance, suggesting that managers manipulate earnings to increase shareholders’ wealth or to perceive contractual outcomes linked to accounting performance. However, high EPU encourages managers to engage more in opportunistic earnings management practices, leading to a decrease in firm performance. This finding suggests that EPU is a major risk factor that harms economic outcomes and firm behaviour. The remainder of the paper is organised as follows. Section 2 reviews the literature and develops the hypotheses. Section 3 outlines the sample, describes the data and defines the variables used in the empirical analysis. Section 4 reports and discusses the main empirical findings. Section 5 concludes the paper.",
47,1,Journal of Economics and Finance,06 August 2022,https://link.springer.com/article/10.1007/s12197-022-09595-7,Economic and Institutional Determinants of Corruption: The Case of Developed and Developing Countries,March 2023,Ali Acaravci,Seyfettin Artan,Sinan Erdogan,Male,Male,Male,Male,"Corruption is one of the most severe problems in contemporary societies. Its origin can be traced back to the times of the ancient Greeks. Corruption is also one non-negligible threat against economic development, limiting the effectiveness of government and the economy’s operation (Todaro and Smith 2012). Even though it has a long history, until the 1980s, corruption was examined in conversation with political, historical, and sociological dimensions. With the acceleration of globalization in the 1980s, the notion of corruption has drawn the attention of international institutions and economists, primarily because of the increased rate of economic, political, and social integration. Within this context, corruption affects states' economic, social, and political affairs. Because it is an interdisciplinary concept, corruption has no precise definition. The World Bank (1997: 8) describes corruption as “abusing the public authority towards private gains and interests.” However, this definition falls short, given that public officers are not the only ones engaging in corruption. For this reason, in the literature, the broad definition provided by the World Bank has been restated to describe the notion as “abuse of the power and authority by the decision-maker to secure personal interests and gains” by Transparency International (Transparency International 2019a). Ivanyna et al. (2018) note that corruption involves populist and rent-seeking behaviors and practices that can potentially worsen economic productivity by leading to legal but excessive spending for the sake of winning an election, monopolization, and changing legal regulations. Noting that bureaucrats and politicians are exercising public authority, Lambsdorff (2007) adds that this authority is applicable in a wide range of sectors, including judicial affairs, public procurement, business regulations and granting of permits, privatization, foreign exchange (customs, trade permits, and international financial transactions), taxes, police, subsidies, public utilities (water, telephone, garbage collection and disposal, health services), and government services (health, education). Becker and Stigler (1976) emphasized that corruption is not unique to a specific group or occupation. For this reason, corruption appears to be an essential element that should be considered regarding economic decisions. Today, most countries grapple with corruption problems. Primary factors for the growing incidents of corruption include poor education, low public salaries, income disparities among different social classes, lack of transparency in the public sector, political instability, democracy deficit, lack of right to know, constrained information sharing (especially in social media), red tape, and centralized authority. Other reasons, specific to particular countries or certain periods, could also affect corruption's frequency, nature, and scope (Olsson 2014). However, corruption has a feedback effect. For instance, a lack of transparency in the public sector may lead to a high level of corruption, thereby lowering the public sector’s accountability. Such a lack of transparency shifts government spending away from inclusive education, resulting in poor education. Even though there are opposing views, it can be said that corruption might be a retarding factor in economic development. Former World Bank President James Wolfensohn said, “Corruption is like cancer, retarding economic development.” Wei (1999), echoing Wolfensohn, says, “Corruption is a major obstacle to economic development.” Therefore, controlling and diminishing corruption levels is crucial to promoting economic development, particularly in less-developed economies. Accordingly, there is a solid emphasis on incorporating a requirement for transparent regulations, which reduce corruption levels, to achieve Sustainable Development Goals (United Nations 2019). Treisman (2000) also explains why corruption is more prevalent in some countries than others. Accordingly, the phenomenon of corruption, shaped by the nations' political, economic, and social structure, is related to the legal system, cultural and institutional tradition, democracy, and level of economic development. For example, an efficient legal system reduces the level of corruption as it increases the risk of being caught and punished due to corruption. Similarly, corruption is a less common phenomenon in democratic countries where the freedom of association and the press are developed. In addition, economically developed countries are more likely to see instances of abuse being noticed and reported because economic development increases education, literacy, and the spread of interpersonal relationships, which leads to a corruption-reducing effect. Acemoglu and James (2012) argue that corruption emerges from injustice in the global income distribution. They state that those living in emerging countries can achieve the living standards and opportunities of those in rich countries only through illegal practices. On the other hand, Leff (1964) emphasizes that corruption might be beneficial. Leff (1964) considers corruption as a phenomenon that facilitates the work of entrepreneurs in the case where the government and bureaucracy are unresponsive to entrepreneurs’ initiative in economic activities and innovations. Secondly, corruption can stimulate economic growth by eliminating excessive bureaucracy. Corruption can also help for increasing investment and economic development through decreased uncertainty. Furthermore, competition will emerge among entrepreneurs in less developed countries to take advantage of a limited number of licenses and privileges. In this case, corruption might have a positive effect on the productivity of capital since the most efficient firm is to pay the highest bribe. Finally, corruption can reduce losses from an adverse public policy because there might be a better way via corruption when the government implements a policy. Likewise, Huntington (1968) states that corruption may help for promoting economic development. Accordingly, corruption could be an option to overcome traditional laws and bureaucratic regulations hindering economic expansion. Even though it applies to all countries, corruption is mainly associated with developing countries. As a result, most studies have focused on the economic foundations of corruption in those countries. These studies paid attention to the impact of corruption on economic growth, openness, investment, the public sector, and other macroeconomic variables. Although some argue that corruption has a positive effect on economic growth (Acemoglu and Verdier 1998; Paul 2010; Swaleheen 2009), most studies focus on the economic and social costs and repercussions (Acemoglu and Verdier 1998; Charoensukmongkol and Moqbel 2014; Drury et al. 2006; Giavazzi and Tabellini 2004; Heckelman and Powell 2008; Huang 2016; Mistry and Jalal 2012; Shim and Eom 2008). Some studies argue that corruption leads to a decline in investment and economic growth (Del Monte and Papagni 2001; Mauro 1995; Tanzi and Davodi 1997), increase in foreign trade restrictions, and public expenditure deterioration (Del Monte and Papagni 2001; Tanzi and Davodi 1997). In addition, it makes the financial system fragile and vulnerable to risks and uncertainties, which eventually causes the emergence of the informal economy. However, corruption is also a problem in developed countries, so it cannot be explained solely by using economic factors. Literature also confirms that a society’s moral values, customs, traditions, socio-cultural structure, legal system, and judicial and economic institutions further shape corruption and economic dimensions. Therefore, regardless of a country’s development level and institutional structure, corruption affects all countries. The primary objective of this paper is to investigate relationships between corruption, economic growth, institutional quality, and internet use alongside other institutional and economic variables in 65 developed and developing countries for the period from 1999 to 2016 by using dynamic panel data methods providing robust results under cross-sectional dependence. This study makes four distinct contributions to the literature. First, corruption is challenging for developing and developed countries, creating displeasure in economic activities and distorting market mechanisms in developed countries. Hence, it is crucial to include developed countries in any analysis of corruption. Second, unlike the existing literature, we formed a quality of governance index based on government stability and bureaucracy quality. Such a strategy may enable us to measure the quality of governance comprehensively. Third, unlike previous studies on this issue, the panel data approaches employed in this study consider possible cross-section dependence, frequently occurring in panel data analysis and allowing for heterogeneity across cross-sections. Ignoring possible cross-section dependence may lead to the exclusion of mutual dependencies stemming from unobserved common effects among panel members. For instance, African leaders declared 2018 “the African Year of Anti-Corruption,” showing their commitment to reducing corruption. Therefore, ruling cross-section dependence out may ignore the effects of international treaties. Such an approach may inevitably lead to a loss of information and biased estimations. Therefore, considering cross-section dependency makes it possible to generate consistent inferences about estimations and inclusive policy proposals among individual panel members (Chudik and Pesaran 2013). Fourth, this study covers a more extensive period than the existing literature. It is widely known in panel data analysis that the size and power of panel data methods increase once data's time dimension (T) mounts up. Hence, we can say that using an expanded period could contribute to obtaining more consistent results.",1
47,1,Journal of Economics and Finance,05 August 2022,https://link.springer.com/article/10.1007/s12197-022-09596-6,The level of African forex markets integration and Eurobond issue,March 2023,Lord Mensah,Charles Andoh,Eric Boachie-Yiadom,Unknown,Male,Male,Male,"African countries have been battling with exchange rate volatility over the years and this has presented challenges to central banks across the continent. Especially, when many of the emerging frontier economies in Africa are highly dependent on a consistent supply of G10 currencies, dominated by the US dollar and increasing Chinese Yuan to facilitate the import of capital goods, fuel, and machinery. Africa’s economies continue to invest in ambitious new infrastructure projects, which require foreign capital inputs, and therefore high demand for foreign exchange. Much of these projects may be hampered as a result of high demand for foreign exchange making imports more expensive and price volatility. Though individual countries have headaches in dealing with country-level volatilities, it is naturally wise to question the interdependencies and spillovers among the forex markets in Africa, which is much less researched but attracts quite a substantial foreign inflow (Ahmed and Zlate 2014). It has been established that the interdependencies and volatility spillovers among developed forex markets are determined by the decisions of central bank interventions, and international trade impacts (Menkhoff 2013). To deepen the understanding of interdependencies and volatility spillovers across frontier markets, it is worth investigating how African forex markets are integrated. Secondly, the motivation behind this paper may also be attributed to the exposure of African countries to the international capital market for external financing. This started in 2006, where African countries developed an appetite for issuing Eurobonds one after the other. Despite the history of default and over-indebtedness, international investors have in turn developed an appetite for the relatively high yield of African bonds. The coupons and principal payment of these bonds are all serviced in dollars, which tends to impact the dollar buffer of the various countries. This is likely to build up into the volatility of the dollar to the local currencies of the country. The interdependence may not be overlooked, since all the countries are sourcing from the same pot in the Eurobond market. Apart from international trade, the currencies on the African continent have gained attention as they have become more integrated with the global markets through Eurobonds. Further, forex trade in Africa has been growing over the years with South Africa leading the way. Recently, the industry has been developing with an ever-growing number of traders, investors demanding interested brokers, and a well-regulated banking sector. South Africa has the Financial Sector Conduct Authority (FSCA) which is the license issuer for forex trading. The FSCA also regulates the sector in a well-organized and trading-friendly manner. The second-largest retail FX market in Nigeria has no regulations for the sector. Currently, investors choose brokers who are regulated by the FSCA. In 2019, the global daily forex trading volume was about 6.6 trillion of which the BRICS takes 261.653 Billion USD, South Africa trades about 20.3 billion, 314 million to Nigeria, and 192 for Kenya. This paper will help the forex market by analyzing the extent and evolution of interdependence and connectedness among the forex markets in Africa. Specifically, we will (i) analyze the time-varying comovements among the currencies of countries that have issued Eurobonds since 2006 and (ii) study how volatility spillovers propagate among them. We will also compare the connectedness of the forex markets among Eurobond issued African markets and non-euro-bond issued African Markets. The development of the African forex market serves as one of the motivations for this paper, However, we take a keen interest in the dynamics of the African Forex markets by assessing various empirical and theoretical trends found in the developed forex markets. In these markets, it has been established that investors tend to follow the bandwagon effect when the market is uncertain. This behavior which is described as a herding mentality has been observed in several markets including the forex market (Tsuchiya 2015) and the stock market (Bohl et al. 2017). Investors may ignore or override their own beliefs and run-in herds during bull markets and the growth of asset bubbles. This phenomenon can be observed in the US market through rising correlations between financial assets. The investigations on the time variation in the correlation among the African forex will have a practical implication in managing risk and asset allocations, as weak interlinkages will reveal potential gains from diversification in the African forex markets. Therefore, we investigate the degree of co-movements among currencies in Africa based on the Dynamic Conditional Correlation (DCC) model developed by Engle (2002). The theory that underpins this paper and supports the hypothesis we are testing is the financial contagion theory. As indicated by Atenga and Mougoue (2021), and Masson (1998) papers on contagion theory can be divided into two, that is, non-crisis-contingent theory and non-crisis contingent theory. The non-crisis contingent theory deals with a contagion that is premised on the transmission mechanism across markets on the interdependence of the international trade, financial linkages, political linkages, and the monsoonal effects. Effectively, the theory indicates that the size and the effect of the financial contagion are the same devoid of whether in a crisis period or not. On the other hand, the crisis-contingent theories represent a contagion that occurs through different crises. Per the crisis contingent theory, the impact of contagion is not the same across crisis and stable periods. Our work falls under the non-crisis contingent theory and fills in the gap of replacing international trade with the Eurobond issue. We can comfortably replace the Eurobond with international trade as they all serve the purpose of increasing the US dollar buffer of countries. We study the extent and nature of volatility spillovers on the African forex markets because Kanas (2001) indicated volatility and its spillovers across currencies affect decisions about hedging open forex positions and may worsen the idiosyncratic risk that reduces the gains from international diversification. African economies are mostly open economies and as indicated by Hau (2002) open economies exhibit less volatile real exchange rates. This study assesses the volatility of the currencies in Africa about the US dollar by showing the nature and the extent of volatility spillovers among them. Our analysis also presents the first of its kind in assessing the interdependencies and risk spillovers on the African forex markets. We find different correlation dynamics and volatility spillovers across the period of our study. There were significant comovements and volatility spillovers in the Light Eurobond Issues and the Global Financial Crisis Period. The conditional correlation and the volatility spillovers are not significant in the Heavy Eurobond Issue Period. The rest of the paper is organized as follows: we review the literature about the topic understudy in the next section. In Section 3, we describe the data, methodology, and the test of the hypothesis. Empirical results and their economic implications are presented in Section 4. The conclusion is provided in Section 5.",
47,1,Journal of Economics and Finance,02 July 2022,https://link.springer.com/article/10.1007/s12197-022-09586-8,Covid-19 pandemic and stock returns in India,March 2023,Munusamy Dharani,M. Kabir Hassan,Mohammad Zoynul Abedin,Unknown,Unknown,Male,Male,"The Covid-19 pandemic has caused distress around the world since December 2019, gaining great momentum in number of reported cases and deaths globally. According to a World Bank report, around 10 million people have been infected and 2 million people have lost their life due to Covid-19. The World Health Organization (WHO) has suggested that governments follow uniform guidelines to control the disease. Governments around the world have implemented precautionary measures of travel restriction, social distancing, home quarantine, working from home, and shutting down businesses, among others. Covid-19 has brought about unprecedented challenges to people’s social, economic, and financial activities. Accordingly, global stock markets have quickly processed information regarding these upheavals and integrated it into stock prices, resulting in an over 30% decline in stock prices around the world. Some studies argue that the Covid-19 pandemic effect is homogeneous and stock returns have fallen across markets [Topcu and Gulal (2020); Ashraf (2020a, b; Phan and Narayan (2020); Erdem (2020); Alexakis et al. (2021)]. However, Covid-19 does not affect all stocks across sectors equally, with some stocks in the information technology, health, and consumer staples industries actually earning positive returns during the pandemic period. Therefore, the Covid-19 pandemic has a heterogeneous effect in the markets [Liu et al. (2020), Narayan et al. (2021), Mazur et al. (2020), Baek et al. (2020)]. Further, stock markets are affected negatively in the initial period of the pandemic and later react positively due to the relaxation of lockdowns and adoption of government stimulus packages [Narayan et al. (2020a, b; David et al. (2021); Ashraf (2020a, b; Fernandez-Perez et al. (2021)]. Of course, the Covid-19 pandemic also adversely affects people’s health. It incites fear in investors in the market. At the same time, the pandemic leads to heightened volatility in the global stock markets [Alexakis et al. (2021), Haroon and Rizvi (2020), Zhang et al. (2020), and Albulescu (2020)]. Moreover, to understand the psychology of people in the stock market, Isen and Simmonds (1978) provide evidence that people in a good mood make optimistic decisions, whereas people in a bad mood make pessimistic decisions. Similarly, Zhai et al. (2021) document that major events such as the global financial crisis of 2008, terrorist attacks of 2001, Ebola outbreak of 2014, and current Covid-19 pandemic induce pessimistic investment decisions. Thus, these adverse events affect investor psychology and lead to antagonistic changes in stock prices. Moreover, investor psychology increases the pressure to sell by both institutional and retail investors in the markets, hence increasing volatility. Fascinatingly, investors pay constant attention to Covid-19 health news and prices immediately respond to new information daily. This leads to information discovery and market efficiency (Vozlyublennaia 2014). Most existing studies document that such adverse events have homogeneous effects on stock prices [Topcu and Gulal (2020); Ashraf (2020a, b; Phan and Narayan (2020); Erdem (2020); Alexakis et al. (2021); Zhai et al. (2021); Dharani et al. (2022)]. Therefore, the present study examines whether the Covid-19 pandemic has homogeneous effects on stock returns in India. In this study, we examine the impact of the Covid-19 pandemic on stock returns using panel data. This study contributes to the literature from three different perspectives. First, we investigate the relationship between the daily growth rate in Covid-19 cases (Gcases) and the daily growth rate in Covid-19 deaths (Gdeaths) on the one hand, and stock returns on the other. Secondly, we consider the overall lockdown period and four different phases of the lockdown (see Table 1) to examine the impact of lockdown on stock returns. Finally, we classify the stocks into different industries and examine which industry provides a higher return during the Covid-19 pandemic period. The rest of the paper is formatted as follows. Section 2 explains the review of literature and research. Section 3 describes the data sources and methodology of the study. Section 4 discusses and interprets the empirical results. The summary and concluding remarks are presented in Sect. 5.",1
47,1,Journal of Economics and Finance,01 August 2022,https://link.springer.com/article/10.1007/s12197-022-09601-y,Publisher Correction: Is fund performance driven by flows into connected funds? spillover effects in the mutual fund industry,March 2023,Bing Zhu,René-Ojas Woltering,,,Unknown,Unknown,Mix,,
47,2,Journal of Economics and Finance,09 March 2023,https://link.springer.com/article/10.1007/s12197-023-09621-2,Using property rights to fight crime: the Khaya Lam project,June 2023,Kerianne Lawson,,,Unknown,Unknown,Unknown,Unknown,,
47,2,Journal of Economics and Finance,11 January 2023,https://link.springer.com/article/10.1007/s12197-022-09614-7,Does a CEO’s ability to hedge affect the firm’s payout policy?,June 2023,Lee M. Dunham,Sijing Wei,Jiarui (Iris) Zhang,,Unknown,Unknown,Mix,,
47,2,Journal of Economics and Finance,27 December 2022,https://link.springer.com/article/10.1007/s12197-022-09611-w,Oil price uncertainty and climate risks,June 2023,Apostolos Serletis,Libo Xu,,Male,Unknown,Unknown,Male,"The relationship between oil price uncertainty and the level of economic activity has been investigated in a series of recent papers by Elder and Serletis (2010), Elder and Serletis (2011), Bredin et al. (2011), Elder and Serletis (2011), Rahman and Serletis (2011), Rahman and Serletis (2012), Pinno and Serletis (2013), Jo (2014), Elder (2018), Serletis and Mehmandosti (2019), and Serletis and Xu (2019). They appeal to the real options theory — see, for example, Bernanke (1983) and Brennan and Schwartz (1985) — and use structural VAR models that accommodate an independent role for the effects of oil price uncertainty. These studies use mostly quarterly data and find that uncertainty about the price of oil has a negative (and statistically significant) effect on several measures of real economic activities. They also find that accounting for the effects of oil price uncertainty tends to exacerbate the negative dynamic response of economic activity to negative oil price shocks, while dampening the response to positive ones. In this paper we take advantage of a recently developed monthly real GDP series to investigate the relationship between oil price uncertainty and the level of economic activity in the United States over the period from 2000:1 to 2019:10. We follow (Elder and Serletis 2010) and use their (identified) structural GARCH-in-Mean VAR in real output growth and the change in the real price of oil as the benchmark model. As in Elder and Serletis (2010), we associate the oil price change VAR residual with exogenous oil price shocks and use the conditional standard deviation of the forecast error for the change in the real price of oil as a measure of oil price uncertainty. With the higher frequency data we find that oil price uncertainty has a negative but statistically insignificant effect on real output growth. In an expanded version of the Elder and Serletis (2010) structural GARCH-in-Mean VAR, we allow oil price uncertainty to interact with the climate risk factors, recently constructed by Faccini et al. (2021). They are the U.S. climate policy textual factor, the international summits textual factor, the global warming textual factor, the natural disasters textual factor, and the U.S. climate policy narrative factor. These factors capture natural disasters and climate change and their impacts on the economy and also reflect the risks stemming from government intervention via carbon taxation and incentives to develop green technologies. In the context of the expanded model, we test whether the impact of oil price uncertainty on real output is enhanced by its interaction with environmental policies and climate risks. With the monthly data, we find that the main result of a negative but statistically insignificant effect of oil price uncertainty on real output growth does not change, except for the case when oil price uncertainty interacts with the international summits textual factor; that interaction leads to a negative and statistically significant effect of oil price uncertainty on the growth rate. The rest of the paper is organized as follows. Section 2 presents the monthly data and the Faccini et al. (2021) five climate risk factors. Section 3 presents results using the Elder and Serletis (2010) bivariate structural GARCH-in-Mean VAR as the benchmark model. In Section 4, we modify the Elder and Serletis (2010) methodology by incorporating the Faccini et al. (2021) climate risk factors, considering the interaction between oil price uncertainty and climate risks. We present the empirical results and investigate the responses of real output to oil price increases and decreases in the context of impulse response functions. The final section concludes the paper.",
47,2,Journal of Economics and Finance,20 December 2022,https://link.springer.com/article/10.1007/s12197-022-09608-5,"Economic policy statements, social media, and stock market uncertainty: An analysis of Donald Trump’s tweets",June 2023,Daniel Perico Ortiz,,,Male,Unknown,Unknown,Male,"Social networks have rapidly gained relevance as a source of information for stock market participants when forming expectations about future events, especially when the government is involved. Enikolopov et al. (2018) show how firms mentioned in Alexey Navalnys blog posts, which uncovered corruption scandals in Russian state-controlled companies, exhibit a later negative market valuationFootnote 1. Further research based on Twitter data, such as Yang et al. (2015) and Piñeiro-Chousa et al. (2016), and Schnaubelt et al. (2020), show that the continuous flow of information through this social network coincides with comovements in market indicators and asset prices. Economic and financial media coverage of Donald Trump’s social media behavior reflects the general concern about the adverse effects of the misuse of official social media channels, especially when used to disclose information regarding future national economic policiesFootnote 2. This case provides an opportunity to understand the increasing role of social networks in real-time economic policy communication and its immediate effect on stock market uncertainty. To uncover this relationship, I retrieve tweet and retweet from Donald Trump’s Twitter account for the period between December 31, 2015, and October 21, 2019, with their respective metadata.Footnote 3 Text data is aggregated in five-minute intervals to match the frequency of the market uncertainty measure, here given by the closing value of the Chicago Board Options Exchange (CBOE) Volatility Index (VIX). Thereafter, I developed a double unsupervised machine learning approach, similar to Bybee et al. (2020), to identify and cluster policy-related statements from Trump’s tweets and retweets. This approach is based on two algorithms. The first one retrieves a set of possible topics using the Latent Dirichlet Allocation (LDA). The second algorithm clusters topics hierarchically based on a measure of semantic distance. I aggregate similar economic-related topics into cluster topics and label them according to their implicit economic policy issue. The high frequency effect of policy-related tweets on market uncertainty is estimated in an event study context, such as in Beechey and Wright (2009). For this, I generate two sets of indicator variables: the first set is based on policy-related cluster topics, hereafter policy statements events; the second set is based on content-independent measures, such as posting or retweeting frequency. Finally, I estimate the effect of selected events on the change in the VIX over an estimation window covering from 15 minutes before the event up to five hours afterward. The results for policy statement events suggest that tweets about foreign policy, trade, and immigration have a statistically significant uncertainty-promoting effect. Tweets regarding monetary policy, with high levels of sharing (retweeting), show the highest estimated impact on stock market volatility. Tweets about fiscal and health care policies did not increase perceived uncertainty. Content-independent events regarding unexpected changes in disclosure frequency and sharing levels have a statistically significant positive impact on market volatility. The intensity of these effects increases as the effect becomes more unusual. Most of the estimated effects are short-lived: significant responses to statement and content-independent events response appear between one and four hours after the occurrence of the event, except for immigration, which becomes significant at the end of the estimation window. This paper complements and adds evidence to similar studies based on Trump’s tweet data, such as Colonescu et al. (2018), who studies the effect of tweets on foreign exchange markets; Bianchi et al. (2019), who provides evidence on the impact of tweets on Fed funds futures; and Fan et al. (2020), who studies firm-level exposure around political events by using a (dis)agreement among social media users who jointly mention firms from the S&P 500 composite and Trump. In terms of data frequency level, this paper follows (Kinyua et al. 2021), who documents the intra-day of the S&P 500 and DJIA indexes to Trump’s tweets using sentiment analysis. Studies concerning uncertainty measures, such as Baker et al. (2019) and Burggraf et al. (2020a), and Burggraf et al. (2020b), also demonstrate the direction of the causal relationship between Trump’s announcements regarding trade policy and an increase in stock market volatility. Klaus and Koser (2020) show a similar effect for European financial markets. However, this paper deviates from previous related literature in three aspects. First, the estimated uncertainty effect is not based on a particular set of tweets including a word or single estimated topic. Instead, similar topics are clustered in broad but recognizable policy categories. Second, the high-frequency evolution of the uncertainty response to policy statements and to different levels of disclosure and sharing are described. Finally, it combines content-dependent and content-independent information to identify particular scenarios in which policy statements generate more uncertainty. It is possible to circumscribe this paper within two similar strands of literature: financial market reaction to news or announcements and financial market reaction to policy uncertainty. Authors that have provided evidence on the sensitivity of asset prices to the disclosure of unexpected macroeconomic indicators and FOMC statements include (Beechey and Wright 2009) for treasury inflation-protected securities, Lapp and Pearce (2012) for federal funds rate futures prices. More recently, Gilbert et al. (2017) showed that the heterogeneity in asset price responses depends on the forecasting power of the announcement. The literature on asset volatility, such as Graham et al. (2003), suggests that announcements regarding employment, NAPM (manufacturing), producer price indices, import and export price indices, and the employment cost index have a significant impact on implied volatility and thus on stock valuation. In terms of index volatility, Clements et al. (2007) shows that the VIX falls significantly on FOMC meeting days. Bomfim (2003) and Lee and Ryu (2019) reaffirm the importance of the timing of the announcements for volatility dynamics. Clements et al. (2007) links FOMC preannouncement periods to relatively calm levels of conditional volatility. Finally, Lee and Ryu (2019) suggests that the effect of announcements, especially monetary policy ones, are also more pronounced in crisis and postcrisis periods than in the precrisis period. Early literature on policy uncertainty, such as Bittlingmayer (1998) and Voth (2002), provide evidence of financial market reactions to political and policy uncertainty from a historical perspective by using German and U.S. data, respectively. Liu and Zhang (2015) and Baker et al. (2016) documents the predictive power of the Economic Policy Uncertainty (EPU) Index when forecasting realized and implied volatility of the S&P 500 index. The positive relationship among political uncertainty, economic policy, and options volatility is documented in Pástor and Veronesi (2013), Kelly et al. (2016) for political events,Footnote 4 and Amengual and Xiu (2018) for monetary policy uncertainty. Finally, it is important to consider how firm’s behavior change in high economic policy uncertainty scenarios. Goel and Nelson (2021) and William and Fengrong (2022) documented a negative effect on technological innovation at the country and industry level respectively. This effect is heterogeneous and depends on country and firms characteristics; for example (Goel and Nelson 2021) finds that R&D oriented firms may introduce innovations to hedge against economic policy uncertainty. William and Fengrong (2022) and Liu and Ma (2020) shows that in countries with high levels of trade and financial market liberalization the negative effect of policy uncertainty on technology innovation is milder This paper is structured as follows. Section 2 presents the data. Section 3 describes the policy statements identification approach and the event study design. Section 4 summarizes the results. Section 5 concludes.",
47,2,Journal of Economics and Finance,03 December 2022,https://link.springer.com/article/10.1007/s12197-022-09610-x,Productivity-conditioned market reaction of US Bank acquisitions during regulation-deregulation eras,June 2023,Jamal Ali Al-Khasawneh,Naceur Essaddam,Benito A. Sanchez,Male,Unknown,Male,Male,"The 1990s witnessed one of the most massive deregulations in the US banking industry's history. This deregulation had the main objective of enhancing the banking industry's productivity and efficiency by opening up new markets and creating new investment opportunities (DeYoung et al. 2009; Kowalik et al. 2015). The 1994 Riegle-Neal Interstate Banking and Branching Efficiency Act and the 1999 Graham-Leach-Bliley Act were the two primary influences on that deregulation. The 1994 act gave banks freedom to acquire banks in other states. The 1999 Act gave banks more freedom to combine with other financial entities. Implementing these acts resulted in a surge in acquisitions, which led to a severe decrease in the number of operating banks. However, the deregulation also led to the opening of many bank branches that reduced the wealth of acquiring firms' stockholders (Wang 2003; Moeller et al. 2005). Nevertheless, after the 2008 economic crisis, some regulations returned to the banking industry. This regulation era started with the creation of an early-warning systemic risk council that intended to impose more restrictions on using derivatives and to stabilize large banking institutions and ended with the passage of the Dodd-Frank Wall Street Reform and Consumer Protection Act of 2010 that created new regulatory agencies and gave new power to the Federal Reserve Bank to regulate financial institutions. Several articles in the literature have addressed the issue of productivity performance during regulation and deregulation of banking system in countries and regions. For example, Sengar et al. (2021) study the post-merger effect on the performance of Indian banks and conclude that “the mergers of banks are successful and beneficial for the new amalgamated entity as well as shareholders and customers.” Furthermore, Zhao et al. (2021) propose a model to study the efficiencies of Chinese banks to adapt to regulatory change in the era of big data and find that there exist significant disparities in inefficiencies between natural disposability (bank decreases its inputs to decrease undesirable outputs) and managerial disposability (bank considers a regulation change as an opportunity by utilizing big data technology). Using a non-parametric bootstrapped analysis, Al-Khasawneh et al. (2020) investigate the productivity and cost efficiencies dynamics of US acquiring banks during the fifth merger wave (1993–2003) and find that large merging banks tend to have the same productivity scores compared with peers, but small merging banks experience lower productivity than their peers. From a sample of 1530 banks operating in 88 countries during 1999–2011, Tanna et al. (2017) find that the net effect of financial liberalization on banks’ total factor productivity growth is positive despite the higher propensity to banking crisis. Degl’Innocenti et al. (2017) study bank productivity growth for the 28 European Union countries during three main phases of the financial crisis, the US subprime crisis (2007–2008), the global financial crisis (2009–2010), and the sovereign debt crisis (2010–2012) and find productivity growth during the first phase, but a decline the second and third phases. Chaffai (2021) compares productivity changes between domestic and foreign banks in North Africa using the stochastic cost frontier approach and finds that “foreign banks are not more productive than their counterparts” and are not the driver of technology transfer from their more advanced banking system. Finally, using a short-run profit Luenberger indicator, Chen and Wu (2020) investigate the performance of Chinese city banks and find that, although China’s eastern banks are more profit efficient, they have had a higher productivity decline during the period 2010–2014. However, given the deregulation and regulation swings of the last 30 years, very few studies have examined the effect of deregulations / regulations on US banks’ productivity, which was the intended objective of the deregulation of the 1990s, or whether the regulation after the 2008 economic crisis harmed productivity gains from deregulation. Furthermore, and more importantly, no one –to our knowledge– has examined whether the stockholders of acquirers benefited from either deregulation or regulation. This article fills the gap in the literature by investigating whether the market detects and compensates (penalizes) productivity increase (decline). We use a unique combination of the non-parametric Malmquist Productivity Index (MPI), event study methodology, and multivariate regression analysis to answer whether the market can detect the productivity changes in acquiring banks distinctively in the regulation (1992–2006) and deregulation (2008–2015) eras. This investigation has two motivations. First, we investigate the link between productivity changes and the market reaction of productivity pre-classified mergers following acquisitions in the US pre and post 2008 crisis (i.e., does the market keep distinguishing value creative acquisitions after the crisis as before regardless the size of the acquirer?). Moreover, this study may have some critical policy implications because the US government is reassessing the Dodd-Frank Act, which might revert the US banking industry to greater liberalization. This review is based on the idea that regulation after the 2008 economic crisis was too onerous for businesses and the economy and assessing the US acquiring banks' performance during the past deregulation and regulation eras will be critical in re-structuring the future of the US banking industry. This study proceeds as follows: Section 2 presents the literature review. Section 2 describes the method. Section 3 presents samples and data. Section 4 gives the results. Section 5 concludes.",
47,2,Journal of Economics and Finance,17 November 2022,https://link.springer.com/article/10.1007/s12197-022-09607-6,Regulation of data breach publication: the case of US healthcare and the HITECH act,June 2023,Lorenz Bohn,Dirk Schiereck,,Male,Male,Unknown,Male,"The US healthcare sector is growing yearly, and Covid-19 even fosters this growth. The Centers for Medicare and Medicaid Services (2020) estimate that in 2019 the overall share of the U.S. gross domestic product related to health care spending was 17.7 percent. Similarly, the healthcare sector outperformed the S&P500 over the last ten years (S&P Dow Jones Indices 2021), underlining the growth potential and the size and heft of large healthcare companies and their impact on the healthcare sector in general. Compared to other industries, the advantages of leveraging data and the importance of protecting data became apparent early. The Health Insurance Portability and Accountability Act (HIPAA) of 1996 kickstarted the creation of national standards to protect sensitive patient health information and define permitted uses (Centers for Disease Control and Prevention 2018), while the Health Information Technology for Economic and Clinical Health Act (HITECH) of 2009 aimed at promoting and expanding the adoption of health information technology (U.S. Department of Health & Human Services 2017). But with the increasing adoption of electronic health and medical records, the criminal exploitation of patient data has increased correspondingly (Li et al. 2019). Due to the sensitive nature of the affected data, it is not surprising that the Ponemon Institute (2017) estimates the costs per breached record to be the highest in the healthcare sector. The susceptibility to data breaches and the enormous digital potential encapsulate the healthcare sector's current state. Previous studies made key contributions by exploring this subject from a variety of angles. Tanriverdi et al. (2020) show how different types of complexity within multihospital systems drive and mitigate the risks of data breaches while Angst et al. (2017) underline the importance of cohesive, deeply integrated IT security practices, by showing the persisting susceptibility of hospitals to IT risks if their efforts are merely symbolic. Building on these findings, Li et al. (2019) report that the deviation of security investments from the industry norms plays an important role in assessing the likelihood of security breaches. This study aims to contribute to that area of research by analyzing the financial implications arising from data breaches, specifically stock market reactions to the announcement of data breaches. Previous studies of this type have typically assessed market reactions across multiple sectors or were heavily skewed towards IT firms. To the authors’ knowledge, there has not been a single comprehensive study focused on the healthcare sector. Given the importance of the US healthcare sector and the enforcement of data breach announcements in this sector, this analysis provides an important new viewpoint on IT and data in general in the healthcare sector. Due to federal disclosure requirements, the sample can be considered the most comprehensive sector-specific analysis of data breaches on firm value to date.",
47,2,Journal of Economics and Finance,09 February 2023,https://link.springer.com/article/10.1007/s12197-023-09616-z,Predicting the contribution of artificial intelligence to unemployment rates: an artificial neural network approach,June 2023,Mihai Mutascu,Scott W. Hegerty,,Male,Male,Unknown,Male,"In an era of unprecedented technological advances, innovation has found its way into nearly every conceivable production process. Whereas in recent decades, manufacturing workers may have found themselves replaced by robots, white-collar professionals such as lawyers might have to worry about text-analysing software. This trend is only expected to continue as more tasks that are specialized can be automated; customer-service tasks might be performed by bots, for example. Even seemingly “safe” professions that require higher-level thinking and organization might be at risk if Artificial Intelligence (AI) becomes sufficiently advanced. As a result, unemployment might increase for a new cause that has not previously been included in empirical analyses. During the last few decades, the prediction of unemployment rates has become a favourite subject of investigation for many researchers. Over time, the topic gradually grew in importance, especially with improvements in modern computational technology. Techniques far more sophisticated than simple Ordinary Least Squares (OLS) have come to the fore, which have expanded the universe of results available in the literature. Many researchers have intensively targeted the unemployment rates of many countries, using econometric methodologies of varying degrees of complexity, and with linear or non-linear approaches. In this vein, Karlsson and Javed (2016, p. 5) appreciate that the unemployment prediction is “often related to stationary time series, seasonality and trend analysis, and exponential smoothening to the simple OLS technique including autoregressive integrated moving average (ARIMA) models.” Irrespective of the chosen forecasting methodology, the set of unemployment determinants includes different combinations of variables including labour market conditions, productivity, economic growth, price levels, monetary aggregates, interest rates, population, government size, and quality of institutions. During this time, additional inputs have been included. Out of them, artificial intelligence has arisen a particular interest in recent decades. The implications of artificial intelligence on unemployment are significant, especially in developed economies, with a strong automation process because of advanced technologies and the use of AI in many sectors. Almost all vanguard sectors are strongly affected, from engineering and electronics to control engineering and operational research. That is why the growth of artificial intelligence started to be an important challenge in those countries, creating many worries in terms of occupation. Such changes open the door for more complex approaches regarding unemployment forecasting by considering the contribution of artificial intelligence as well. In this context, the aim of this paper is to predict countries’ unemployment rates based on a set of input parameters, with the most prominent being artificial intelligence. The forecasting follows the ANN methodology by considering a sample with 23 most high-tech and developed countries, over the period from 1998 to 2016. The theoretical ground is given by the Phillips curve (Phillips 1958) and Okun’s law (Okun 1962) as they are amended by Dornbusch et al. (2017) and Folawewo and Adeboje (2017), but controlling for artificial intelligence (Mutascu 2021). The contribution of the paper is threefold. First, to the best of our knowledge, this is a pioneering work in unemployment forecasting, which uses artificial intelligence as the core determinant in these rates. Second, this paper provides one of the first contributions considering ANN for predicting unemployment by using artificial intelligence as an input parameter. As a socio-economic phenomenon, unemployment generally follows non-linear dynamics with a large pallet of determinants. In this context, ANN fits better in analysing such phenomenon as this method can be easily adapted to any arbitrary type of function, working with all categories of inputs. Unlike classical time-series methods, ANN has the ability to learn and model non-linear processes, can be generalized by predicting unseen data, and does not impose any restrictions or special statistical treatments of input variables. Third, compared with the existing literature, this paper offers an extended dataset by including many countries over a generous period. The rest of the paper is organized as follows: Section 2 reviews the literature, Section 3 presents the data and methodology, and Section 4 shows the empirical results. Finally, Section 5 concludes.",
47,2,Journal of Economics and Finance,24 January 2023,https://link.springer.com/article/10.1007/s12197-022-09612-9,Impact of futures’ trader types on stock market quality: evidence from Taiwan,June 2023,Ya-Wen Lai,,,,Unknown,Unknown,Mix,,
47,2,Journal of Economics and Finance,21 January 2023,https://link.springer.com/article/10.1007/s12197-023-09615-0,"CO2, SO2 and economic growth: a cross-national panel study",June 2023,T. Daniel Coggin,,,Unknown,Unknown,Unknown,Unknown,,
47,2,Journal of Economics and Finance,09 November 2022,https://link.springer.com/article/10.1007/s12197-022-09606-7,Profitability of private equity: mean reversion and transitory shocks,June 2023,Luis Alberiko Gil-Alana,Francisco Puertolas-Montanes,,Male,Male,Unknown,Male,"The aim of this paper is to study the evolution of profitability of private equity across time and geographically, and more specifically, whether the shocks in the series over time feature transitory or permanent effects, looking at both aggregated and disaggregated data by region. We focus on four specific areas: US, Europe, Asia/Pacific and Rest of the World, along with the “Total” data. For this purpose, we use methodologies based on the concept of fractional integration, employing updating techniques in time series analysis. Assessment of the type of shock is normally performed by unit root tests (Dickey and Fuller 1979; Phillips and Perron 1988 and others). In the case that the process has no unit roots, it is supposed to be stationary and hence exhibiting reversion to the mean (the lagged level, pre-shock, will drive the reversion to the mean). However, should it have unit roots, the process does not revert to the mean, the shock having a permanent effect on the series. In this article, we depart from these classical methods by using fractional integration, which is more flexible and general in the sense that it allows fractional degrees of differentiation and mean reversion takes place as long as the differencing parameter is significantly smaller than 1. As later explained in the manuscript, this will allow us to consider flexible approaches, including, for example, nonstationary though mean reverting processes if the differencing parameter is in the range [0.5, 1). Thus, the main objective of the paper is to determine if shocks in profitability of private equity have transitory or permanent effects, and we determine this by estimating the degree of differentiation of the series from a fractional viewpoint. The structure of the paper is as follows: Sect. 2 presents a historical context of the profitability of private equity. Section 3 deals with the methodology employed in the paper. Section 4 displays the dataset and the main empirical results, while Sect. 5 concludes the paper.",
47,2,Journal of Economics and Finance,02 February 2023,https://link.springer.com/article/10.1007/s12197-023-09618-x,Bank performance before and after the subprime crisis: Evidence from pooled data on big US banks,June 2023,Christian Calmès,Raymond Théoret,,Male,Male,Unknown,Male,"Before the introduction of the Glass-Steagall Act in 1933, the share of bank non-interest income (in net operating income) exceeded 40% while the loans-to-assets ratio was as low as 30% (Fig. 1). The severe restrictions imposed by the Glass-Steagall Act on US bank behavior paved the way to an era of traditional banking characterized by a low share of non-interest income (20%) and growing loans-to-assets ratio and net interest margin. With the advent of securitization, market-oriented banking reemerged gradually and was consolidated by the passage of the Financial Services Modernization Act (Gramm-Leach-Bliley Act) in 1999, which gave to commercial banks access to the financial services industry. The share of non-interest income resumed its rise, and exceeded its historic peak right before the subprime crisis. At the same time, bank return on assets (ROA), which was quite stable from 1935 until the start of the 1990s, then increased significantly, with a concomitant decrease in net interest margin. However, the subprime crisis and stricter regulation imposed by Basel IIIFootnote 1 at the international level and by the Dodd-Franck ActFootnote 2 in the US reversed the bank diversification landscape once more. Source: Federal Deposit Insurance Corporation (FDIC) Share of non-interest income, loans-to-assets ratio, net interest margin and ROA for all US banks, 1934–2016.  Consistent with the arguments which led to the imposition of the Glass-Steagall Act, many studies conclude that bank diversification in off-balance sheet activities (OBS) at the end of the 1990s and the beginning of 2000s is a source of instability and does not improve bank risk-return trade-off (e.g., Boyd and Graham 1988; Boyd and Gertler 1994; Demsetz and Strahan 1997; DeLong 2001; DeYoung and Rice 2004; Stiroh 2006; Stiroh and Rumble 2006; Laeven and Levine 2007; Calmès and Liu 2009; Wagner 2010; Calmès and Théoret 2010, 2014). The issue which is not addressed in most of these studies and which could be at the root of their negative diagnostics on bank performance is that the variable used to gauge bank diversification, which is frequently the share of noninterest income in net operating income as in our paper, could actually be endogenous. Indeed, since the decision to diversify in OBS activities is the by-product of a classical dynamic profit optimization problem, it should be considered as endogenous (Campa and Kedia 2002; Beale et al. 2007; Busch and Kick 2009; De Jonghe 2010). For instance, US banks got involved in diversification by increasing the share of noninterest income in reaction to the structural decrease in their net interest margin.Footnote 3 The feedback effect of this share on bank net interest margin makes it endogenous. Neglecting this endogenous issueFootnote 4 may entail an underestimation of its beneficial impact on bank performance. Diversification in OBS activities, by providing a room of manoeuver, also induces banks to take more risk with increased leverage—implicit or explicit (Mishkin and Serletis 2013; Brunnermeier and Sannikov 2014). In this regard, banks could engage in regulatory capital arbitrage to dodge the capital requirement constraints by transferring a portion of their activities off-balance-sheet and thus ipso facto increasing their implicit leverage (Calomiris and Mason 2004; Ambrose et al. 2005; Brunnermeier 2009; Cardone Riportella et al. 2010; Nijskens and Wagner 2011; Bergevin et al. 2013; Kling 2015). Moreover, in the environment of exceptionally low interest rates since the subprime crisis, many studies show that these rates depress bank net interest margins, and thus ROA (Borio et al. 2017; Bikker and Vervliet 2018; Claessens et al. 2018; Brei et al. 2020; Hanzlik and Teply 2019) and even change banks’ behavior by inducing them to shift their activities from loans to fee-based business lines (Altavilla et al. 2018; Coleman and Stebunovs 2019; Brei et al. 2020). In this article, we analyze the impact of the subprime crisis on bank performance and the effect of depressed interest rates prevailing after the crisis. We pay a particular attention to the effect of the tightening of bank regulation on bank profits after the crisis which went against the Financial Services Modernization Act put in place only ten years ago. We focus on the nineteen biggest US banks with assets exceeding $100 billion because they dominate fees business lines and hold more than 60% of the assets of the US banking system.Footnote 5 We separate our sample in two parts: the four biggest banks (Big 4)—i.e. universal banks—and the remaining 15 banks (Big 15). It is indeed relevant to study the Big 4 separately because they are highly interconnected (Diebold and Yilmaz 2014) but also because the new tighter bank regulation—e.g., the Dodd-Franck Act and the new rules on bank liquidity and capital put in place in Basel III after the subprime crisis— has particularly constrained the behavior of universal banks, especially their funding operationsFootnote 6 (Adrian et al. 2018). In order to analyze the full list of fees business lines available since 2001, our sample stretches from the fourth quarter of 2001 to the fourth quarter of 2016. Our estimation period is divided in three parts: the period before the subprime crisis, the subprime crisis per se, and the post-crisis period. One of our contributions is to explicitly account for the endogeneity issue related to the share of noninterest income by resorting to a GMM (Generalized Method of Moments) procedure using robust instruments—i.e., the higher moments of the explanatory variables in our models (Fuller 1987; Dagenais and Dagenais 1997; Lewbel 1997).Footnote 7 Our results are quite different from those obtained in many studies performed before the subprime crisis because they relied instead on simple OLS (Ordinary Least-Squares) or to rudimentary instrumental variable procedures to analyze the incidence of diversification in OBS activities—as measured by the share on noninterest income—on bank performance. We find that fee-based income has a significant positive impact on the risk-adjusted return on assets (ROA) of the biggest US banks, and that the Big 4, with their greater diversification power and economies of scope, benefit the most from this kind of income. Trading, securitization, and loan sales are particularly important for bank performance and contribute to add flexibility in their activities. In contrast to previous studies (Gallo et al. 1996; Vander Vennet et al. 2004; Busch and Kick 2009), we do not find that fee-based business lines most related to traditional activities—especially charges on deposits—enhance bank performance. Net interest income even negatively impacts the ROA of the Big 4, suggesting that unusually low interest rates are damageable for the net interest margins of the Big 4, and that universal banks might actually subsidize their loans through cross-selling. Overall, accounting for endogeneity related to bank diversification seems to significantly reverse several results found in the literature. Endogeneity is a dynamic process which cannot be grasped by simple estimation methods like OLS. Our paper shows that GMM, which accounts for the nonlinearities embedded in this process, is quite potent to study it effectively.Footnote 8 The behavior of the biggest US banks differs markedly before and after the subprime crisis. In this respect, we identify a structural break regarding their performance over these two subperiods. Indeed, the pre-crisis period is characterized by moderate interest ratesFootnote 9 and a loosening of regulatory rules. In contrast, the post-crisis period is characterized by very low interest ratesFootnote 10 and a tightening of rules, two deterrents to bank performance. In this context, we find that banks have altered their business model and geared toward riskier activities to compensate the decrease in their profits after the subprime crisis. Our study has important policy implications. The new regulatory guidelines regarding bank liquidity have a detrimental impact on big banks’ profitability since they reduce their involvement in loans and especially in fee business lines. The same is true regarding regulations depressing fee-based income directly. According to our findings, fee business lines are nevertheless more lucrative than suggested by the past literature and are also a source of liquidity and flexibility in bank risk management (e.g., Jiangli and Pritsker 2008; Loutskina 2011). These regulations have partly contributed to the drop in securitization and to a decrease in trading, two valuable bank activities according to our study. Not explicitly taking into account the endogeneity of the diversification process when putting in place regulatory constraints may thus lead to misleading policy directions since it could disturb bank engineering counter-productively. This article is organized as follows. Second 2 is devoted to the background literature. Section 3 presents our methodology based on pooled regressions relying on GMM with robust instruments and SUR (seemingly unrelated regressions) to account for the interaction between the innovations of banks’ regressions. Section 4 provides an analysis of our quarterly database while Sect. 5 reports our empirical results. Section 6 concludes.",
47,2,Journal of Economics and Finance,06 January 2023,https://link.springer.com/article/10.1007/s12197-022-09613-8,An empirical investigation of COVID-19 effects on herding behaviour in USA and UK stock markets using a quantile regression approach,June 2023,Richard T. Ampofo,Eric N. Aidoo,Daniel Sasu,Male,Male,Male,Male,"The world is currently hit by the COVID-19 pandemic, leading to a significant distortion in economic activities. This global crisis has impacted financial markets in many ways (Goodell 2020), even though it is complicated to estimate its impact on economic and social life activities. The COVID-19 pandemic has infected over 624 million people and killed about 6.55 million individuals as of October 24, 2022 (WHO 2022). In addition to the loss of lives, the pandemic has affected the performance of public and private sector businesses, declining the economic growth rate of most countries worldwide (Verma et al. 2021). The impact of the pandemic has forced governments across different countries to implement unprecedented interventions to slow the spread of the virus, including closing international borders, instituting national and regional quarantine, and enacting lockdown measures. A study conducted in the USA indicated that the effectiveness of these non-pharmaceutical interventions rapidly affected the labour market negatively; however, the implementation of economic support measures to reduce the negative impact of these interventions became effective shortly but partially eased unemployment in the country (Dergiades et al. 2022). This problem has made the situation a “black swan” following the negative impact of the pandemic on daily human activities and the poor performances of economies worldwide (Yarovaya et al. 2021). Almost all countries in the world are fighting the COVID-19 disease. However, the expectancy of an ending COVID-19 pandemic is unpredictable as the virus mutates. This undesirable crisis has generated global fear and economic shock to financial activities worldwide. The COVID-19 pandemic has significantly increased US dollar volatility prices, resulting in the broad selling of financial stocks and the generation of a severe and vicious cycle (Chang et al. 2020). According to Ali et al. (2020), countries like the United States of America (USA), United Kingdom (UK), Germany, and South Korea experienced a massive increase in stock market volatility between December 2019 to March 2020, and afterward, due to the pandemic. Economic sectors such as the healthcare system, agriculture, information technology stocks and natural gas have earned very encouraging returns, unlike equity prices for over 1,450 Standard and Poor performing firms in real estate, hospitality management, petroleum and entertainment at the early stages of the pandemic (Mazur et al. 2021). It is interesting to wonder if COVID-19 has a significant effect on herding behaviour, influencing the performance of stock prices in financial markets. Globally, the behaviour of investors has been widely studied (Alhaj-Yaseen and Ladd 2019; Cakan et al. 2019; Chang et al. 2000; Christie & Huang 1995; Weisberg 2013), especially following current financial market anomalies and resulting volatility and market spill-overs across various financial markets. Due to market stress periods and their impact on stock market movements, one fascinating area that has attracted international research is the assessment of herd behaviour. Herd behaviour is experienced when many act in the same direction at a given period. Nofsinger and Sias (1999) have defined the term as having a group of investors transacting in the same way, in the same direction for a given period (long or short). In financial markets, herd behaviour is observed when investors transact in the same direction or deny their own information and views and are influenced by collective behaviour, despite the possibility of being misled by this group. This paper investigates the behaviour of investors before and during the COVID-19 pandemic in the USA and UK financial stock markets under different market periods or conditions: bullish, bearish, and normal market periods. Literature has proved that herding is expected not to occur in well-established markets, unlike emerging markets where the presence of inexperienced agents, speculative trading, weak regulations and low liquidity are common and plausible explanations to influence overreaction and herd behaviour (Chang et al. 2000; Pochea et al. 2017). Significant studies on herding have also shown that investors tend to herd during market stress periods which can lead to market volatility (Angela-Maria et al. 2015; Bogdan et al. 2022; Clements et al. 2017; Jirasakuldech and Emekter 2021). During periods categorised by increasing market volatility, herding becomes very persistent. In view of this, a positive relationship could be derived between pandemics and market volatility. In analysing volatility levels in the USA, it was observed that the volatility measured in the early months of 2020 was more than those experienced in October 1987, December 2008, or the ending periods of 1929, attributing the effect to many reasons including, but not restricted to behavioural and policy reactions to the COVID-19 pandemic (Baker et al. 2020). Therefore, an increase in market volatility is experienced during pandemics and financial crises (Diamandis 2008; Ferreruela and Mallor 2021). This study uses the quantile regression model to comprehensively study investors behaviour in the USA and UK stock markets. The quantile regression model is a robust approach that produces estimates conditioned on the median and can easily account for outliers (Yu et al. 2003). In quantile regression, curves represent the relationship between dependent and independent variables at specified quantiles. Hence, a modified herding behaviour detector as suggested by Chang et al. (2000) is illustrated on several quantiles. The approach provides a more detailed result of investors behaviour as it thoroughly examines the whole market return distribution. Unfortunately, the ordinary least square (OLS) regression model can be limited in explaining investors behaviour as it produces a single model, which may not provide very good information about investors behaviour at some locations of the market distribution. Several studies have been conducted in recent times where the Cross Sectional Standard Deviation (CSSD) and Cross Sectional Absolute Deviation (CSAD) approaches have been widely used to detect herding behaviour among investors (Duygun et al. 2021; Espinosa-Méndez and Arias 2021b; Ferreruela and Mallor 2021; Susana et al. 2020). In Europe, Espinosa-Méndez and Arias (2021a) investigated whether the COVID-19 pandemic affected capital markets in France, Germany, Italy, United Kingdom and Spain. Results revealed a strong indication that the COVID-19 pandemic potentially drives herding behaviour. The pandemic influenced agents with less information to follow the decision of agents with more information due to fear and uncertainty. Similar results were evident in Australia (Espinosa-Méndez and Arias 2021b) and India (Dhall and Singh 2020), confirming how crises and pandemics cause investors to react or follow the decisions of other investors. Furthermore, the COVID-19 pandemic offers an exclusive opportunity to explore the behaviour of investors in cryptocurrency and energy sector markets. For instance, Yarovaya et al. (2021) examined whether the COVID-19 pandemic amplifies herding behaviour in four most traded cryptocurrency markets (USD, EUR, JPY and KRW). Their findings showed that herding was experienced on bullish and bearish market days; however, herding did not get stronger during the COVID-19 pandemic periods. On the contrary, in energy stocks, Chang et al. (2020) documented that herding is more likely to occur during extremely low oil price returns. The study attributed the effect to investors unwisely selling their assets as a result of panic in taking risks during the COVID-19 pandemic, creating a vicious cycle. The ongoing literature on COVID-19 impact on investors’ behaviour has been studied across financial stocks, cryptocurrencies and energy stocks. However, there have been limited studies focusing on the COVID-19 effects on well-established markets such as the USA and UK. Herding impacts financial markets in diverse ways. Typically, the behaviour of investors causes the value of assets to move away from economic principles, therefore making the value of assets biasedly priced. This has primarily been observed to be the cause of financial bubbles throughout economic and financial history. Some of these phenomena are equity bubbles such as The Dutch Tulip Mania in 1634, Japan’s Bubble Economy in 1980s, The Dot-com Bubble in the Late 1990s, The Debt of the Roaring Twenties, which caused the Great Depression, and The US Housing Bubble that led to the Great Recession, making the study of this behaviour very significant. The behaviour of investors cannot be determined based on mere theoretical evidence, hence, there is a need for an empirical analysis of this behaviour. This effect is critical in financial decision-making as it establishes behavioural patterns and trends which can tell a particular market’s direction. The remainder of this study is structured as follows: the second section outlines the methodologies and model estimation techniques used to detect herding behaviour. The third section presents the data description and preliminary analysis of the study. The fourth and fifth sections present the main empirical results and discuss possible reasons for herding behaviour in the selected markets, respectively. The conclusion section summarizes the study and provides some suggestions for future research.",

Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055563,Editors' note,March 1988,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055564,Status quo bias in decision making,March 1988,William Samuelson,Richard Zeckhauser,,Male,Male,Unknown,Male,,2887
1.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055565,Theory and individual behavior of first-price auctions,March 1988,James C. Cox,Vernon L. Smith,James M. Walker,Male,Male,Male,Male,,234
1.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055566,Risk aversion in bargaining: An experimental study,March 1988,J. Keith Murnighan,Alvin E. Roth,Francoise Schoumaker,Unknown,Male,Unknown,Male,,69
1.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055567,The law of large numbers and the attractiveness of compound gambles,March 1988,Chew S. H.,Larry G. Epstein,,Unknown,Male,Unknown,Male,,12
1.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056165,Editor's introduction,June 1988,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
1.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056166,Measuring values: A conceptual framework for interpreting transactions with special reference to contingent valuation of visibility,June 1988,Baruch Fischhoff,Lita Furby,,Male,Female,Unknown,Mix,,
1.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056167,The marginal value of job safety: A contingent valuation study,June 1988,Shelby Gerking,Menno De Haan,William Schulze,,Male,Male,Mix,,
1.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056168,Consumer processing of hazard warning information,June 1988,Wesley A. Magat,W. Kip Viscusi,Joel Huber,Male,Unknown,Male,Male,,34
1.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056169,Learning about radon's risk,June 1988,V. Kerry Smith,William H. Desvousges,F. Reed Johnson,Unknown,Male,Unknown,Male,,25
1.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056138,Expected utility: An anniversary and a new era,September 1988,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,54
1.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056139,The value of changes in life expectancy,September 1988,Sherwin Rosen,,,,Unknown,Unknown,Mix,,
1.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056140,"Rank-dependent, subjective expected-utility representations",September 1988,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
1.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056141,Non-expected utility risk premiums: The cases of probability ambiguity and outcome uncertainty,September 1988,Uzi Segal,Avia Spivak,,Male,Female,Unknown,Mix,,
1.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00117640,Publisher's announcements,December 1988,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00117641,Ordinal independence in nonlinear utility theory,December 1988,Jerry R. Green,Bruno Jullien,,Male,Male,Unknown,Male,,80
1.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00117642,How a certain internal consistency entails the expected utility dogma,December 1988,Paul A. Samuelson,,,Male,Unknown,Unknown,Male,,1
1.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00117643,Aversion to one risk in the presence of others,December 1988,John W. Pratt,,,Male,Unknown,Unknown,Male,,82
1.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00117644,The effect of OSHA records-check inspections on reported occupational injuries in manufacturing establishments,December 1988,John W. Ruser,Robert S. Smith,,Male,Male,Unknown,Male,,20
2.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055709,"Risk, ambiguity, and insurance",April 1989,Robin M. Hogarth,Howard Kunreuther,,,Male,Unknown,Mix,,
2.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055710,Preferences for information on probabilities versus prizes: The role of risk-taking attitudes,April 1989,Paul J. H. Schoemaker,,,Male,Unknown,Unknown,Male,,26
2.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055711,An experimental test of several generalized utility theories,April 1989,Colin F. Camerer,,,Male,Unknown,Unknown,Male,,317
2.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055712,Decision analysis using lottery-dependent utility,April 1989,Joao L. Becker,Rakesh K. Sarin,,Unknown,Male,Unknown,Male,,12
2.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055713,Erratum,April 1989,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056134,Retrospective on the utility theory of von Neumann and Morgenstern,June 1989,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,75
2.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056135,Probability and juxtaposition effects: An experimental investigation of the common ratio effect,June 1989,Chris Starmer,Robert Sugden,,,Male,Unknown,Mix,,
2.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056136,Deterministic transformations of random variables and the comparative statics of risk,June 1989,Jack Meyer,Michael B. Ormiston,,Male,Male,Unknown,Male,,39
2.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056137,Adjusting risky situations: A theoretical framework and empirical test,June 1989,Donald A. Wehrung,Kam-Hon Lee,Ilan B. Vertinsky,Male,Unknown,Male,Male,,13
2.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00209388,The impact of risk sharing on efficient decision,September 1989,John W. Pratt,Richard J. Zeckhauser,,Male,Male,Unknown,Male,,17
2.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00209389,Prospective reference theory: Toward an explanation of the paradoxes,September 1989,W Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
2.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00209390,Experimental markets for insurance,September 1989,Colin Camerer,Howard Kunreuther,,Male,Male,Unknown,Male,,36
2.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00209391,Laboratory experiments with a finite-horizon job-search model,September 1989,James C. Cox,Ronald L. Oaxaca,,Male,Male,Unknown,Male,,80
2.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00356860,The effect of airline pilot characteristics on perceptions of job safety risks,December 1989,Leon N. Moses,Ian Savage,,Male,Male,Unknown,Male,,8
2.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00356861,Risk aversion and the relationship between Nash's solution and subgame perfect equilibrium of sequential bargaining,December 1989,Alvin E. Roth,,,Male,Unknown,Unknown,Male,,18
2.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00356862,"Adaptive learning, adaptive utility, and rational behavior in a repeated prisoner's dilemma",December 1989,Michael J. Moore,Marian Chapman Moore,,Male,Male,Unknown,Male,,4
2.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00356863,Welfare analysis under uncertainty,December 1989,John M. Marshall,,,Male,Unknown,Unknown,Male,,2
2.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00356864,Choice of purchasing arrangements in insurance markets,December 1989,Kevin D. Cotter,Gail A. Jensen,,Male,,Unknown,Mix,,
2.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00356865,More on insurance as a Giffen good,December 1989,Eric Briys,Georges Dionne,Louis Eeckhoudt,Male,Male,Male,Male,,39
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213257,Editorial announcement,March 1990,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213258,Inducing risk-neutral preferences: An examination in a controlled market environment,March 1990,James M. Walker,Vernon L. Smith,James C. Cox,Male,Male,Male,Male,,25
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213259,Testing between alternative models of choice under uncertainty: Some initial results,March 1990,Raymond C. Battalio,John H. Kagel,Komain Jiranyakul,Male,Male,Unknown,Male,,156
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213260,The cost of accidental death: A capital market approach,March 1990,Ivy E. Broder,,,Female,Unknown,Unknown,Female,,15
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213261,A tale of two tails: An alternative characterization of comparative risk,March 1990,Michael Landsberger,Isaac Meilijson,,Male,Male,Unknown,Male,,18
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213262,The value of information in anticipated utility theory,March 1990,Edward Schlee,,,Male,Unknown,Unknown,Male,,25
3.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00213263,Ross' measure of risk aversion and portfolio selection,March 1990,Josef Hadar,Tae Kun Seo,,Male,,Unknown,Mix,,
3.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056367,The logic of partial-risk aversion: Paradox lost,June 1990,John W. Pratt,,,Male,Unknown,Unknown,Male,,18
3.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056368,An experimental evaluation of the descriptive validity of lottery-dependent utility theory,June 1990,Richard L. Daniels,L. Robin Keller,,Male,Unknown,Unknown,Male,,12
3.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056369,The role of risk preferences in bargaining when acceptance of a proposal requires less than unanimous approval,June 1990,Joseph E. Harrington Jr.,,,Male,Unknown,Unknown,Male,,35
3.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056370,Dynamic processes in risk perception,June 1990,George Loewenstein,Jane Mather,,Male,Female,Unknown,Mix,,
3.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056371,The Becker-DeGroot-Marschak mechanism and nonexpected utility: A testable approach,June 1990,Zvi Safra,Uzi Segal,Avia Spivak,Male,Male,Female,Mix,,
3.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056372,The impact of self-protection and self-insurance on individual response to risk,June 1990,Jason F. Shogren,,,Male,Unknown,Unknown,Male,,53
3.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00116781,The effect of probabilistic demands on the structure of cost functions,September 1990,Gregory M. Duncan,,,Male,Unknown,Unknown,Male,,16
3.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00116782,"Loss volatility, bankruptcy, and the demand for reinsurance",September 1990,Thomas J. Hoerger,Frank A. Sloan,Mahmud Hassan,Male,Male,Male,Male,,48
3.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00116783,"A “Pseudo-endowment” effect, and its implications for some recent nonexpected utility models",September 1990,Drazen Prelec,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00116784,Information evaluation under nonadditive expected utility,September 1990,Irving H. Lavalle,Yongsheng Xu,,Male,Unknown,Unknown,Male,,2
3.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00116785,"Expected utility, μ-σ preferences, and linear distribution classes: A further result",September 1990,Hans-Werner Sinn,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00116786,OSHA enforcement and workplace injuries: A behavioral approach to risk assessment,September 1990,John T. Scholz,Wayne B. Gray,,Male,Male,Unknown,Male,,97
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353343,Long-term environmental risks,December 1990,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353344,Understanding long-term environmental risks,December 1990,Baruch Fischhoff,,,Male,Unknown,Unknown,Male,,16
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353345,Risk communication and attitude change: Taiwan's national debate over nuclear power,December 1990,Jin Tan Liu,V. Kerry Smith,,Female,Unknown,Unknown,Female,,23
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353346,Environmental markets in the year 2000,December 1990,Robert W. Hahn,Roger G. Noll,,Male,Male,Unknown,Male,,24
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353347,Discounting and the evaluation of lifesaving programs,December 1990,Maureen L. Cropper,Paul R. Portney,,Female,Male,Unknown,Mix,,
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353348,Models for estimating discount rates for long-term health risks using labor market data,December 1990,Michael J. Moore,W. Kip Viscusi,,Male,Unknown,Unknown,Male,,77
3.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353349,Discounting statistical lives,December 1990,John K. Horowitz,Richard T. Carson,,Male,Male,Unknown,Male,,67
4.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057884,Preference and belief: Ambiguity and competence in choice under uncertainty,January 1991,Chip Heath,Amos Tversky,,Male,Male,Unknown,Male,,951
4.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057885,Rank- and sign-dependent linear utility models for finite first-order gambles,January 1991,R. Duncan Luce,Peter C. Fishburn,,Unknown,Male,Unknown,Male,,209
4.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057886,Risk reduction or risk compensation? The case of mandatory safety-belt use laws,January 1991,William N. Evans,John D. Graham,,Male,Male,Unknown,Male,,84
4.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057887,"Compensating wage differentials for fatal injury risk in Australia, Japan, and the United States",January 1991,Thomas J. Kniesner,John D. Leeth,,Male,Male,Unknown,Male,,78
4.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057888,Evidence of a new violation of the independence axiom,January 1991,Graham Loomes,,,Male,Unknown,Unknown,Male,,36
4.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056121,Nontransitive preferences in decision theory,April 1991,Peter C. Fishburn,,,Male,Unknown,Unknown,Male,,127
4.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056122,Motorist use of safety equipment: Expected benefits or risk incompetence?,April 1991,Glenn C. Blomquist,,,Male,Unknown,Unknown,Male,,20
4.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056123,Indirect methods for valuing changes in environmental risks with nonexpected utility preferences,April 1991,A. Myrick Freeman III,,,Unknown,Unknown,Unknown,Unknown,,
4.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056124,Incentives and government relief for risk,April 1991,Louis Kaplow,,,Male,Unknown,Unknown,Male,,79
4.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056125,Ambiguity aversion in the small and in the large for weighted linear utility,April 1991,Gordon B. Hazen,Jia-Sheng Lee,,Male,,Unknown,Mix,,
4.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056126,Altruism and the value of other people's safety,April 1991,M. W. Jones-Lee,,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114153,Editor's introduction,July 1991,,,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114154,Lotteries in the real world,July 1991,Charles T. Clotfelter,Philip J. Cook,,Male,Male,Unknown,Male,,31
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114155,Market uncertainty and the process of belief formation,July 1991,Bertrand R. Munier,,,Male,Unknown,Unknown,Male,,8
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114156,Lexicographic state-dependent subjective expected utility,July 1991,Irving H. Lavalle,Peter C. Fishburn,,Male,Male,Unknown,Male,,17
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114157,Measures of risk aversion with expected and nonexpected utility,July 1991,Aldo Montesano,,,Male,Unknown,Unknown,Male,,10
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114158,"Ambiguity, probability, preference, and decision analysis",July 1991,Robert L. Winkler,,,Male,Unknown,Unknown,Male,,29
4.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00114159,Cognitive rationality and alternative belief measures,July 1991,Antoine Billot,,,Male,Unknown,Unknown,Male,,4
4.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056159,Paying to improve your chances: Gambling or insurance?,December 1991,Martin McGuire,John Pratt,Richard Zeckhauser,Male,Male,Male,Male,,30
4.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056160,Comparative statics for rank-dependent expected utility theory,December 1991,John Quiggin,,,Male,Unknown,Unknown,Male,,86
4.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056161,Editor's introduction liability insurance symposium: Part I,December 1991,,,,Unknown,Unknown,Unknown,Unknown,,
4.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056162,The once and future crisis,December 1991,Kenneth S. Abraham,,,Male,Unknown,Unknown,Male,,3
4.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056163,McCarran-Ferguson Act reform: More competition or more regulation?,December 1991,Paul L. Joskow,Linda Mclaughlin,,Male,Female,Unknown,Mix,,
4.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056164,"Uncertainty, information and resolution of medical malpractice disputes",December 1991,Frank A. Sloan,Thomas J. Hoerger,,Male,Male,Unknown,Male,,13
5.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00208784,Where does subjective expected utility fail descriptively?,February 1992,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
5.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00208785,The effect of information on health risk valuations,February 1992,Alan J. Krupnick,Maureen L. Cropper,,Male,Female,Unknown,Mix,,
5.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00208786,Status-quo and omission biases,February 1992,Ilana Ritov,Jonathan Baron,,Female,Male,Unknown,Mix,,
5.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00208787,A note on Savage's theorem with a finite number of states,February 1992,Thorsten Hens,,,Male,Unknown,Unknown,Male,,5
5.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00208788,Violations of dominance in pricing judgments,February 1992,Barbara Mellers,Robin Weiss,Michael Birnbaum,Female,,Male,Mix,,
5.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00208789,A subjectivist approach to consecutive conflict,February 1992,John G. Wilson,,,Male,Unknown,Unknown,Male,,1
5.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057566,Subjective probabilities and utility with event-dependent preferences,May 1992,Edi Karni,,,Male,Unknown,Unknown,Male,,8
5.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057567,An intransitive expectations-based bayesian variant of prospect theory,May 1992,Robert F. Bordley,,,Male,Unknown,Unknown,Male,,11
5.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057568,Evaluating the injury risk associated with All-Terrain Vehicles: An application of Bayes' rule,May 1992,Daniel L. Rubinfeld,Gregory B. Rodgers,,Male,Male,Unknown,Male,,8
5.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057569,Different frames for the independence axiom: An experimental investigation in individual decision making under risk,May 1992,Michele Bernasconi,,,Female,Unknown,Unknown,Female,,8
5.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057570,The value of job safety for railroad workers,May 1992,Michael T. French,David L. Kendall,,Male,Male,Unknown,Male,,8
5.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057571,"Reference points, loss aversion, and contingent values for auto safety",May 1992,Timothy L. Mcdaniels,,,Male,Unknown,Unknown,Male,,45
5.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057878,Multidimensional bargains and the desirability of ex post inefficiency,July 1992,John W. Pratt,Richard Zeckhauser,,Male,Male,Unknown,Male,,2
5.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057879,State-independent subjective expected lexicographic utility,July 1992,Irving H. Lavalle,Peter C. Fishburn,,Male,Male,Unknown,Male,,16
5.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057880,The pioneer's curse: Selection bias and agricultural land degradation,July 1992,John Quiggin,,,Male,Unknown,Unknown,Male,,7
5.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057881,Propitious selection in insurance,July 1992,David Hemenway,,,Male,Unknown,Unknown,Male,,33
5.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057882,Reinsurance and the liability insurance crisis,July 1992,Lawrence A. Berger,J. David Cummins,Sharon Tennyson,Male,Unknown,Female,Mix,,
5.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057883,Adverse selection and equilibrium in liability insurance markets,July 1992,Lawrence A. Berger,J. David Cummins,,Male,Unknown,Unknown,Male,,5
5.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00122574,Advances in prospect theory: Cumulative representation of uncertainty,October 1992,Amos Tversky,Daniel Kahneman,,Male,Male,Unknown,Male,,8782
5.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00122575,Recent developments in modeling preferences: Uncertainty and ambiguity,October 1992,Colin Camerer,Martin Weber,,Male,Male,Unknown,Male,,1129
5.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00122576,Bayesian decisions with ambiguous belief aversion,October 1992,W. Kip Viscusi,Wesley A. Magat,,Unknown,Male,Unknown,Male,,65
5.0,4.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00122577,Ambiguity and decision modeling: A preference-based approach,October 1992,Rakesh K. Sarin,Robert L. Winkler,,Male,Male,Unknown,Male,,14
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065347,Preference reversals and the measurement of environmental values,January 1993,Julie R. Irwin,Paul Slovic,Gary H. McClelland,Female,Male,Male,Mix,,
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065348,The valuation of contingent claims markets,January 1993,Edward E. Schlee,Harris Schlesinger,,Male,Male,Unknown,Male,,1
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065349,Subjective utility with upper and lower probabilities on finite states,January 1993,Yutaka Nakamura,,,,Unknown,Unknown,Mix,,
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065350,Determinants of risk-taking: Behavioral and economic views,January 1993,Paul J. H. Schoemaker,,,Male,Unknown,Unknown,Male,,81
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065351,An empirical investigation into the effect of psychological perceptions on the willingness-to-pay to reduce risk,January 1993,Lan Savage,,,,Unknown,Unknown,Mix,,
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065352,Counterexamples to Segal's measure representation theorem,January 1993,Peter Wakker,,,Male,Unknown,Unknown,Male,,13
6.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065353,The measure representation: A correction,January 1993,Uzi Segal,,,Male,Unknown,Unknown,Male,,28
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065354,Is choice the correct primitive? On using certainty equivalents and reference levels to predict choices among gambles,April 1993,R. Duncan Luce,Barbara A. Mellers,Shi -Jie Chang,Unknown,Female,,Mix,,
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065355,Framing effects in the evaluation of multiple risk reduction,April 1993,Ilana Ritov,Jonathan Baron,John C. Hershey,Female,Male,Male,Mix,,
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065356,Testing between alternative models of choice under uncertainty—Comment,April 1993,John Quiggin,,,Male,Unknown,Unknown,Male,,7
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065357,The effect of 1980s tort reform legislation on general liability and medical malpractice insurance,April 1993,W. Kip Viscusi,Richard J. Zeckhauser,Glenn Blackmon,Unknown,Male,Male,Male,,46
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065358,Insurance with undiversifiable risk: Contract structure and organizational form of insurance firms,April 1993,Neil A. Doherty,Georges Dionne,,Male,Male,Unknown,Male,,75
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065359,Anouncement,April 1993,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065360,Call for papers,April 1993,,,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01072612,Embedding effects: Stimulus representation and response mode,June 1993,Baruch Fischhoff,Marilyn Jacobs Quadrel,Patrick Stroh,Male,Female,Male,Mix,,
6.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01072613,Testing for juxtaposition and event-splitting effects,June 1993,Chris Starmer,Robert Sugden,,,Male,Unknown,Mix,,
6.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01072614,The utility of gambling,June 1993,John Conlisk,,,Male,Unknown,Unknown,Male,,137
6.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01072615,Estimating the demand for risky assets via the indirect expected utility function,June 1993,Ardeshir J. Dalal,Bala G. Arshanapalli,,Unknown,Female,Unknown,Female,,9
6.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01072616,Similarity and preferences in the space of simple lotteries,June 1993,J. M. Aizpurua,T. Ichiishi,J. R. Uriarte,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065311,Making decisions about liability and insurance: Editors' comments,August 1993,Colin Camerer,Howard Kunreuther,,Male,Male,Unknown,Male,,2
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065312,Intuitions about penalties and compensation in the context of tort law,August 1993,Jonathan Baron,Ilana Ritov,,Male,Female,Unknown,Mix,,
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065313,"Framing, probability distortions, and insurance decisions",August 1993,Eric J. Johnson,John Hershey,Howard Kunreuther,Male,Male,Male,Male,,492
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065314,Transaction analysis: A framework and an application to insurance decisions,August 1993,Baruch Fischhoff,,,Male,Unknown,Unknown,Male,,12
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065315,Insurer ambiguity and market failure,August 1993,Howard Kunreuther,Robin Hogarth,Jacqueline Meszaros,Male,,Female,Mix,,
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065316,Ambiguity and risk taking in organizations,August 1993,Zur Shapira,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065317,Insurance for low-probability hazards: A bimodal response to unlikely events,August 1993,Gary H. McClelland,William D. Schulze,Don L. Coursey,Male,Male,Male,Male,,128
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065318,The risky business of insurance pricing,August 1993,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065319,First announcement and call for papers,August 1993,,,,Unknown,Unknown,Unknown,Unknown,,
7.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065812,An axiomatization of cumulative prospect theory,October 1993,Peter Wakker,Amos Tversky,,Male,Male,Unknown,Male,,273
7.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065813,Valuing environmental resources: A constructive approach,October 1993,Robin Gregory,Sarah Lichtenstein,Paul Slovic,,Female,Male,Mix,,
7.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065814,Implementing and testing risk-preference-induction mechanisms in experimental sealed-bid auctions,October 1993,Thomas A. Rietz,,,Male,Unknown,Unknown,Male,,20
7.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065815,Impersonal probability as an ideal assessment based on accessible evidence: A viable construct?,October 1993,Rex V. Brown,,,Male,Unknown,Unknown,Male,,10
7.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065816,The Peltzman hypothesis revisited: An isolated evaluation of offsetting driver behavior,October 1993,Thomas L. Traynor,,,Male,Unknown,Unknown,Male,,18
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079626,Decision making with belief functions: Compatibility and incompatibility with the sure-thing principle,December 1993,Jean -Yves Jaffray,Peter Wakker,,Male,Male,Unknown,Male,,30
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079627,Two-parameter decision models and rank-dependent expected utility,December 1993,Michael B. Ormiston,John Quiggin,,Male,Male,Unknown,Male,,11
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079628,On matrix probabilities in nonarchimedean decision theory,December 1993,Peter C. Fishburn,Irving H. Lavalle,,Male,Male,Unknown,Male,,5
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079629,Delayed compensation for lost income,December 1993,John W. Pratt,Richard Zeckhauser,,Male,Male,Unknown,Male,,1
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079630,On a lottery pricing anomaly: Time tells the tale,December 1993,Nathaniel T. Wilcox,,,Male,Unknown,Unknown,Male,,
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079631,The economics of adding and subdividing independent risks: Some comparative statics results,December 1993,Louis Eeckhoudt,Christian Gollier,Michel Levasseur,Male,Male,Male,Male,,2
7.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01079632,Call for papers,December 1993,,,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064083,Risk-risk analysis,January 1994,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064084,The fatality and injury costs of expenditures,January 1994,W. Kip Viscusi,Richard J. Zeckhauser,,Unknown,Male,Unknown,Male,,24
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064085,Health-health analysis: A new way to evaluate health and safety regulation,January 1994,Randall Lutter,John F. Morrall III,,Male,Male,Unknown,Male,,46
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064086,Cross-country analyses don't estimate health-health responses,January 1994,V. Kerry Smith,Donald J. Epp,Kurt A. Schwabe,Unknown,Male,Male,Male,,
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064087,Controlling for causality in the link from income to mortality,January 1994,Kenneth S. Chapman,Govind Hariharan,,Male,Male,Unknown,Male,,
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064088,Mortality risks induced by the costs of regulations,January 1994,Ralph L. Keeney,,,Male,Unknown,Unknown,Male,,
8.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064089,Regulatory review of environmental policy: The potential role of health-health analysis,January 1994,Paul R. Portney,Robert N. Stavins,,Male,Male,Unknown,Male,,
8.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065369,A variational model of preference under uncertainty,March 1994,Peter Fishburn,,,Male,Unknown,Unknown,Male,,7
8.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065370,Regret theory with general choice sets,March 1994,John Quiggin,,,Male,Unknown,Unknown,Male,,149
8.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065371,Violations of the betweenness axiom and nonlinearity in probability,March 1994,Colin F. Camerer,Teck-Hua Ho,,Male,Unknown,Unknown,Male,,468
8.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01065372,Expected utility with lower probabilities,March 1994,Ebbe Hendon,Hans JØrgen Jacobsen,Torben TranÆs,Male,Male,Male,Male,,19
8.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064043,Discriminating between preference functionals: A preliminary Monte Carlo study,May 1994,Enrica Carbone,John D. Hey,,Female,Male,Unknown,Mix,,
8.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064044,Preferences for life saving programs: how the public discounts time and age,May 1994,Maureen L. Cropper,Sema K. Aydede,Paul R. Portney,Female,Female,Male,Mix,,
8.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064045,Hope: An empirical study of attitude toward the timing of uncertainty resolution,May 1994,Soo Hong Chew,Joanna L. Ho,,,Female,Unknown,Mix,,
8.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064046,Absolute and relative risk aversion: An experimental study,May 1994,Haim Levy,,,Male,Unknown,Unknown,Male,,123
8.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064047,A test of the gambler's fallacy: Evidence from pari-mutuel games,May 1994,Dek Terrell,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064048,Errata,May 1994,V. Kerry Smith,Donald J. Epp,Ralph L. Keeney,Unknown,Male,Male,Male,,
9.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01073401,Determinants of stated willingness to pay for public goods: A study in the headline method,February 1994,Daniel Kahneman,Ilana Ritov,,Male,Female,Unknown,Mix,,
9.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01073402,An empirical test of ordinal independence,February 1994,George Wu,,,Male,Unknown,Unknown,Male,,70
9.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01073403,Compensating wage differentials for workplace accidents: Evidence for union and nonunion workers in the UK,February 1994,W. S. Siebert,X. Wei,,Unknown,Unknown,Unknown,Unknown,,
9.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01073404,Risk seeking with diminishing marginal utility in a non-expected utility model,February 1994,Alain Chateauneuf,Michéle Cohen,,Male,Unknown,Unknown,Male,,97
9.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01073405,On a statistical approach to choice under uncertainty,February 1994,Vladimir I. Rotar,,,Male,Unknown,Unknown,Male,,1
9.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064181,Imprecise preferences and the WTP-WTA disparity,October 1994,W. R. Dubourg,M. W. Jones-Lee,Graham Loomes,Unknown,Unknown,Male,Male,,83
9.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064182,"Making social trade-offs among lives, disabilities, and cost",October 1994,Robert F. Bordley,,,Male,Unknown,Unknown,Male,,2
9.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064183,Generalized similarity judgments: An alternative explanation for choice anomalies,October 1994,Jonathan W. Leland,,,Male,Unknown,Unknown,Male,,88
9.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064184,"Moral hazard, risk seeking, and free riding",October 1994,Lawrence A. Berger,John C. Hershey,,Male,Male,Unknown,Male,,20
9.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064200,Comonotonic independence: The critical test between classical and rank-dependent utility theories,December 1994,Peter Wakker,Ido Erev,Elke U. Weber,Male,Male,Female,Mix,,
9.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064201,Unbounded behaviorally consistent stopping rules,December 1994,Edi Karni,Zvi Safra,,Male,Male,Unknown,Male,,1
9.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064202,Observing different orders of risk aversion,December 1994,Graham Loomes,Uzi Segal,,Male,Male,Unknown,Male,,24
9.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01064203,The preservation of multivariate comparative statics in nonexpected utility theory,December 1994,Edward E. Schlee,,,Male,Unknown,Unknown,Male,,5
10.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01211525,The impact of testing errors on value of information: A quality-control example,January 1995,Anil Gaba,Robert L. Winkler,,Male,Male,Unknown,Male,,1
10.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01211526,Decision making under ignorance: Arguing with yourself,January 1995,Robin M. Hogarth,Howard Kunreuther,,,Male,Unknown,Mix,,
10.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01211527,Time and risk,January 1995,John Quiggin,John Horowitz,,Male,Male,Unknown,Male,,22
10.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01211528,"Unions, employment risks, and market provision of employment risk differentials",January 1995,Michael J. Moore,,,Male,Unknown,Unknown,Male,,16
10.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01211529,Coherent decision analysis with inseparable probabilities and utilities,January 1995,Robert F. Nau,,,Male,Unknown,Unknown,Male,,24
10.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01083555,Numerical simulation as a complement to econometric research on workplace safety,March 1995,Thomas J. Kniesner,John D. Leeth,,Male,Male,Unknown,Male,,2
10.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01083556,Utility theory with probability dependent outcome valuation: Extensions and applications,March 1995,Edi Karni,Edward E. Schlee,,Male,Male,Unknown,Male,,7
10.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01083557,Guaranteed renewability in insurance,March 1995,Mark V. Pauly,Howard Kunreuther,Richard Hirth,Male,Male,Male,Male,,72
10.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01083558,Optimal insurance without expected utility: The dual theory and the linearity of insurance contracts,March 1995,Neil A. Doherty,Louis Eeckhoudt,,Male,Male,Unknown,Male,,73
10.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207550,On the nonexistence of Blackwell's theorem-type results with general preference relations,May 1995,Zvi Safra,Eyal Sulganik,,Male,Male,Unknown,Male,,13
10.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207551,Do risk information programs promote mitigating behavior?,May 1995,V. Kerry Smith,William H. Desvousges,John W. Payne,Unknown,Male,Male,Male,,39
10.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207552,Willingness-to-pay and willingness-to-accept for risky and ambiguous lotteries,May 1995,Roselies Eisenberger,Martin Weber,,Female,Male,Unknown,Mix,,
10.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207553,The value of a statistical life: A comparison of two approaches,May 1995,Paul Lanoie,Carmen Pedro,Robert Latour,Male,Female,Male,Mix,,
11.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01132727,Editor's note,July 1995,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01132728,A note on deriving rank-dependent utility using additive joint receipts,July 1995,R. Duncan Luce,Peter C. Fishburn,,Unknown,Male,Unknown,Male,,77
11.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01132729,A theory of coarse utility,July 1995,Liping Liu,Prakash P. Shenoy,,Unknown,,Unknown,Mix,,
11.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01132730,"Do bettors prefer long shots because they are risk-lovers, or are they just overconfident?",July 1995,Joseph Golec,Maurry Tamarkin,,Male,Unknown,Unknown,Male,,20
11.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01132731,Inducing risk-neutral preferences: Further analysis of the data,July 1995,James C. Cox,Ronald L. Oaxaca,,Male,Male,Unknown,Male,,13
11.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01067679,Regulation and insurer competition: Did insurers use rate regulation to reduce competition?,September 1995,Anne Gron,,,Female,Unknown,Unknown,Female,,5
11.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01067680,Demand for risky assets and the monotone probability ratio order,September 1995,Louis Eeckhoudt,Christian Gollier,,Male,Male,Unknown,Male,,57
11.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01067681,Ambiguity aversion in first-price sealed-bid auctions,September 1995,Ahtia Salo,Martin Weber,,Unknown,Male,Unknown,Male,,50
11.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01067682,Risk perception and smoking behavior: Empirical evidence from Taiwan,September 1995,Jin -Tan Liu,Chee -Ruey Hsieh,,Female,,Unknown,Mix,,
11.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01067683,Reversals of preference between compound and simple risks: The role of editing heuristics,September 1995,Rob Ranyard,,,Male,Unknown,Unknown,Male,,15
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207785,Scale and context effects in the valuation of transport safety,December 1995,M. W. Jones-Lee,G. Loomes,,Unknown,Unknown,Unknown,Unknown,,
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207786,Risk-taking activities and heterogeneity of job-risk tradeoffs,December 1995,Joni Hersch,Todd S. Pickton,,Female,Male,Unknown,Mix,,
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207787,Insurance supply with capacity constraints and endogenous insolvency risk,December 1995,Julie A. B. Cagle,Scott E. Harrington,,Female,Male,Unknown,Mix,,
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207788,Performance of the similarity hypothesis relative to existing models of risky choice,December 1995,David Buschena,David Zilberman,,Male,Male,Unknown,Male,,13
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207789,Regret aversion or event-splitting effects? more evidence under risk and uncertainty,December 1995,Steven J. Humphrey,,,Male,Unknown,Unknown,Male,,69
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207790,A correction and its genesis,December 1995,Irving H. Lavalle,Peter C. Fishburn,,Male,Male,Unknown,Male,,2
11.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF01207791,Call for papers,December 1995,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353328,The comonotonic sure-thing principle,January 1996,Chew Soo Hong,Peter Wakker,,Unknown,Male,Unknown,Male,,38
12.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353329,Moral hazard in insurance claiming: Evidence from automobile insurance,January 1996,J. David Cummins,Sharon Tennyson,,Unknown,Female,Unknown,Female,,88
12.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353330,Do poor people have a stronger relationship between income and mortality than the rich? Implications of panel data for health-health analysis,January 1996,Kenneth S. Chapman,Govind Hariharan,,Male,Male,Unknown,Male,,24
12.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353331,Long-term care insurance and bequests as instruments for shaping intergenerational relationships,January 1996,Peter Zweifel,Wolfram Strüwe,,Male,Male,Unknown,Male,,37
12.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00353332,"Moral hazard, monitoring costs, and optimal government intervention",January 1996,Neil Bruce,Kar-Yiu Wong,,Male,Unknown,Unknown,Male,,1
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055786,Introduction: Social treatment of catastrophic risk,May 1996,Kenneth E. Scott,,,Male,Unknown,Unknown,Male,,1
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055787,Opening remarks: Stanford conference on social treatment of catastrophic risk,May 1996,,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055788,The theory of risk-bearing: Small and great risks,May 1996,Kenneth J. Arrow,,,Male,Unknown,Unknown,Male,,45
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055789,The economics of catastrophes,May 1996,Richard Zeckhauser,,,Male,Unknown,Unknown,Male,,58
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055790,The complex politics of catastrophe economics,May 1996,Roger G. Noll,,,Male,Unknown,Unknown,Male,,9
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055791,Alternative institutional responses to asbestos,May 1996,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055792,Mitigating disaster losses through insurance,May 1996,Howard Kunreuther,,,Male,Unknown,Unknown,Male,,348
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055793,Difficulties in making implicit government risk-bearing partnerships explicit,May 1996,Edward J. Kane,,,Male,Unknown,Unknown,Male,,4
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055794,A rational approach to pricing of catastrophe insurance,May 1996,Weimin Dong,Haresh Shah,Felix Wong,Unknown,Unknown,Male,Male,,24
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055795,"The government, the market, and the problem of catastrophic loss",May 1996,George L. Priest,,,Male,Unknown,Unknown,Male,,70
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055796,Global risk management,May 1996,M. Elisabeth Paté-cornell,,,Unknown,Unknown,Unknown,Unknown,,
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055797,Union carbide's Bhopal incident: A retrospective,May 1996,Michael J. Fischer,,,Male,Unknown,Unknown,Male,,9
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055798,"Global financial markets, derivative securities, and systemic risks",May 1996,Myron S. Scholes,,,Male,Unknown,Unknown,Male,,17
12.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055799,Catastrophic responses to catastrophic risks,May 1996,Richard A. Epstein,,,Male,Unknown,Unknown,Male,,40
13.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055335,Options traders exhibit subadditive decision weights,July 1996,Craig R. Fox,Brett A. Rogers,Amos Tversky,Male,Male,Male,Male,,115
13.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055336,A test of rank-dependent utility in the context of ambiguity,July 1996,Hein Fennema,Peter Wakker,,Male,Male,Unknown,Male,,17
13.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055337,Is the insurance aspect of producer liability valued by consumers? Liability changes and childhood vaccine consumption,July 1996,Richard L Manning,,,Male,Unknown,Unknown,Male,,4
13.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055338,An experimental investigation of the impact of ambiguity on the valuationof self-insurance and self-protection,July 1996,Carmela Di Mauro,Anna Maffioletti,,Female,Female,Unknown,Female,,29
13.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00055339,Increasing claims for soft tissue injuries in workers' compensation: Cost shifting and moral hazard,July 1996,Richard J. Butler,David L. Durbin,Nurhan M. Helvacian,Male,Male,Female,Mix,,
13.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057863,Structuring and assessing linear lexicographic utility,September 1996,Irving H. Lavalle,Peter C. Fishburn,,Male,Male,Unknown,Male,,4
13.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057864,Structuring and assessing matrix-probability distributions,September 1996,Irving H. Lavalle,Peter C. Fishburn,,Male,Male,Unknown,Male,,4
13.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057865,A model of comparative statics for changes in stochastic returns with dependent risky assets,September 1996,Georges Dionne,Christian Gollier,,Male,Male,Unknown,Male,,9
13.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057866,"To be, or not to be, that is the question: An empirical study of the WTP for an increased life expectancy at an advanced age",September 1996,Magnus Johannesson,Per-Olov Johansson,,Male,Unknown,Unknown,Male,,58
13.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00057867,Learning by accident? Reductions in the risk of unplanned outages in U.S. nuclear power plants after Three Mile Island,September 1996,Paul A. David,Roland Maude-Griffin,Geoffrey Rothwell,Male,Male,Male,Male,,14
13.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056153,"The likelihood of various stock market return distributions, part 1: Principles of inference",November 1996,Harry M. Markowitz,Nilufer Usmen,,Male,Unknown,Unknown,Male,,24
13.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056154,"The likelihood of various stock market return distributions, part 2: Empirical results",November 1996,Harry M. Markowitz,Nilufer Usmen,,Male,Unknown,Unknown,Male,,34
13.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056155,Probabilities and beliefs,November 1996,Edi Karni,,,Male,Unknown,Unknown,Male,,29
13.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056156,The value of private safety versus the value of public safety,November 1996,Magnus Johannesson,Per-Olov Johansson,Richard M. O'Conor,Male,Unknown,Male,Male,,74
13.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056157,The pain of road-accident victims and the bereavement of their relatives: A contingent-valuation experiment,November 1996,Nathalie G. Schwab Christe,Nils C. Soguel,,Female,Male,Unknown,Mix,,
13.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/BF00056158,Table of contents: Volumes 12/13 1996,November 1996,,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007754402585,Estimating Fatalities Induced by the Economic Costs of Regulations,January 1997,RALPH Keeney,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007717719423,The Interaction Between the Demands for Insurance and Insurable Assets,January 1997,Louis Eeckhoudt,Jack Meyer,Michael Ormiston,Male,Male,Male,Male,,22
14.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007769703493,Reasons for Rank-Dependent Utility Evaluation,January 1997,Elke Weber,Britt Kirsner,,Female,Female,Unknown,Female,,51
14.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007721820332,Fragile Redistribution Choices Behind a Veil of Ignorance,January 1997,Ed Bukszar,Jack Knetsch,,Male,Male,Unknown,Male,,12
14.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007773804402,An Experimental Test of a General Class of Utility Models: Evidence for Context Dependency,January 1997,Richard Chechile,Alan Cooke,,Male,Male,Unknown,Male,,19
14.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007719626543,Increasing Risk: Some Direct Constructions,March 1997,MARK MACHINA,JOHN PRATT,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007773311522,"Proof by Certainty Equivalents That Diversification-Across-Time Does Worse, Risk Corrected, Than Diversification-Throughout-Time",March 1997,PAUL SAMUELSON,,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007725428360,On the Inefficiency of Bang-Bang and Stop-Loss Portfolio Strategies,March 1997,Christian Gollier,,,Male,Unknown,Unknown,Male,,9
14.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007721327452,The Impact of Incentives Upon Risky Choice Experiments,March 1997,JANE BEATTIE,GRAHAM LOOMES,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007769210613,Optimal Incentive Contracting with Ex Ante and Ex Post Moral Hazards: Theory and Evidence,March 1997,ROBERT PUELZ,ARTHUR SNOW,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007723931551,Dynamically Consistent Preferences with Quadratic Beliefs,March 1997,JÜRGEN EICHBERGER,SIMON GRANT,,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007799508646,Measures of Mortality Risks,May 1997,W. VISCUSI,JAHN HAKES,ALAN CARLIN,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007740225484,Explaining the Identifiable Victim Effect,May 1997,KAREN JENNI,GEORGE LOEWENSTEIN,,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007792209554,Bad Deaths,May 1997,CASS SUNSTEIN,,,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007744326393,Insensitivity to the Value of Human Life: A Study of Psychophysical Numbing,May 1997,DAVID FETHERSTONHAUGH,PAUL SLOVIC,JAMES FRIEDRICH,Unknown,Unknown,Unknown,Unknown,,
14.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007796310463,Confusion of Relative and Absolute Risk in Valuation,May 1997,JONATHAN BARON,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007799303256,Probabilistic Insurance,October 1997,PETER WAKKER,RICHARD THALER,AMOS TVERSKY,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007785820094,Discriminating Between Preference Functionals: A Monte Carlo Study,October 1997,ENRICA CARBONE,,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007737904164,Availability Crises in Insurance Markets: Optimal Contracts with Asymmetric Information and Capacity Constraints,October 1997,NEIL DOHERTY,LISA POSEY,,Unknown,Unknown,Unknown,Unknown,,
15.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007789921003,Genetic Risk Factors and Offsetting Behavior: The Case of Skin Cancer,October 1997,MARK DICKIE,SHELBY GERKING,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007727016095,The Value of Life: Editor's Introduction,November 1997,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
15.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007726117003,Characterizing QALYs by Risk Neutrality,November 1997,Han Bleichrodt,Peter Wakker,Magnus Johannesson,,Male,Male,Mix,,
15.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007730201074,Individuals' Estimates of the Risks of Death: Part I—A Reassessment of the Previous Evidence,November 1997,Daniel Benjamin,William Dougan,,Male,Male,Unknown,Male,,28
15.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007782217912,Mortality Risk Perceptions: A Bayesian Reassessment,November 1997,Jahn Hakes,W. Kip Viscusi,,Male,Unknown,Unknown,Male,,36
15.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007734401982,Characterizing QALYs under a General Rank Dependent Utility Model,November 1997,Han Bleichrodt,John Quiggin,,,Male,Unknown,Mix,,
15.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007786418821,Saving Lives in the Present Versus Saving Lives in the Future—Is There a Framing Effect?,November 1997,Magnus Johannesson,Per-Olov Johansson,,Male,Unknown,Unknown,Male,,14
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007710023656,How Best to Flip-Flop if You Must: Integer Dynamic Stochastic Programming for Either-Or,January 1997,Paul A. Samuelson,,,Male,Unknown,Unknown,Male,,3
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007796907726,Estimating Probabilities Relevant to Calculating Relative Risk-Corrected Returns of Alternative Portfolios,January 1997,Paul A. Samuelson,,,Male,Unknown,Unknown,Male,,1
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007749008635,"Adverse Selection, Bequests, Crowding Out, and Private Demand for Insurance: Evidence from the Long-term Care Insurance Market",January 1997,Frank A. Sloan,Edward C. Norton,,Male,Male,Unknown,Male,,109
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007705309543,On the Value of Changes in Life Expectancy: Blips Versus Parametric Changes,January 1997,Magnus Johannesson,Per-Olov Johansson,Karl-Gustaf Löfgren,Male,Unknown,Unknown,Male,,72
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007757326382,The Role of Social Distrust in Risk-Benefit Analysis: A Study of the Siting of a Hazardous Waste Disposal Facility,January 1997,Peter A. Groothuis,Gail Miller,,Male,,Unknown,Mix,,
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007753225473,Workers' Compensation Costs When Maximum Benefits Change,January 1997,Richard J. Butler,B. Delworth Gardner,Harold H. Gardner,Male,Unknown,Male,Male,,10
15.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1017175006955,Estimating Probabilities Relevant to Calculating Relative Risk-Corrected Returns of Alternative Portfolios,January 1997,Paul A. Samuelson,,,Male,Unknown,Unknown,Male,,1
16.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007717224343,Amos Tversky and the Ascent of Behavioral Economics,April 1998,David Laibson,Richard Zeckhauser,,Male,Male,Unknown,Male,,69
16.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007710408413,Shared Outrage and Erratic Awards: The Psychology of Punitive Damages,April 1998,Daniel Kahneman,David Schkade,Cass Sunstein,Male,Male,,Mix,,
16.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007762425252,"Coalescing, Event Commutativity, and Theories of Utility",April 1998,R. Luce,,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007714509322,Common Consequence Conditions in Decision Making under Risk,April 1998,George Wu,Richard Gonzalez,,Male,Male,Unknown,Male,,62
16.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007730226688,Stochastic Dominance and Prospect Dominance with Subjective Weighting Functions,May 1998,Haim Levy,Zvi Wiener,,Male,Male,Unknown,Male,,89
16.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007734327597,Bandwagon Effects and Two-Party Majority Voting,May 1998,Chew Soo Hong,Kai Konrad,,Unknown,Male,Unknown,Male,,14
16.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007786328505,Lloyd's Financial Distress and Contagion within the US Property and Liability Insurance Industry,May 1998,Joseph Fields,Linda Klein,Edward Myskowski,Male,Female,Male,Mix,,
16.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007742529414,Another Tale of Two Tails: On Characterizations of Comparative Risk,May 1998,Alfred Müller,,,Male,Unknown,Unknown,Male,,6
16.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007798818929,Guaranteed Renewability with Group Insurance,May 1998,Mark Pauly,Andreas Nickel,Howard Kunreuther,Male,Male,Male,Male,,6
16.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007703002999,Revealed Likelihood and Knightian Uncertainty,May 1998,RAKESH SARIN,PETER WAKKER,,Unknown,Unknown,Unknown,Unknown,,
16.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007755019837,Decomposing Hindsight Bias,May 1998,Mark Kelman,David Fallas,Hilary Folger,Male,Male,Female,Mix,,
16.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007707103908,Consumer Capital Market Constraints and Guaranteed Renewable Insurance,May 1998,Kevin Frick,,,Male,Unknown,Unknown,Male,,12
16.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007759120746,Time Insensitivity for Protective Investments,May 1998,Howard Kunreuther,Ayse Onculer,Paul Slovic,Male,Unknown,Male,Male,,35
17.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007711416843,On the Contingent Valuation of Safety and the Safety of Contingent Valuation: Part 1-Caveat Investigator,October 1998,JANE BEATTIE*,JUDITH COVEY,ANNE SPENCER,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007791217751,Reference Wealth Effects in Sequential Choice,October 1998,WILLIAM S. NEILSON,,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007739200913,Testing Descriptive Utility Theories: Violations of Stochastic Dominance and Cumulative Independence,October 1998,Michael H. Birnbaum,Juan B. Navarrete,,Male,Male,Unknown,Male,,103
17.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007769628257,Dynamic Choice and NonExpected Utility,November 1998,RAKESH SARIN,PETER P. WAKKER,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007767512327,Risk Premiums and Benefit Measures for Generalized-Expected-Utility Theories,November 1998,JOHN QUIGGIN,ROBERT G. CHAMBERS,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007719629165,Risk Aversion and Self-Insurance-cum-Protection,November 1998,KANGOH LEE,,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007771613236,Biases in Assessments of Probabilities: New Evidence from Greyhound Races,November 1998,DEK TERRELL,,,Unknown,Unknown,Unknown,Unknown,,
17.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007723730074,Risk Adjustment for Health Insurance: Theory and Implications,November 1998,THOMAS M. SELDEN,,,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007782800868,"On the Contingent Valuation of Safety and the Safety of Contingent Valuation: Part 2 - The CV/SG ""Chained"" Approach",December 1998,TREVOR CARTHY,SUSAN CHILTON,ANNE SPENCER,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007734917706,Price Versus Quantity: Market-Clearing Mechanisms When Consumers are Uncertain about Quality,December 1998,ANDREW METRICK,RICHARD ZECKHAUSER,,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007786901776,An Analysis of the Distribution of Combinations Chosen by UK National Lottery Players,December 1998,JONATHAN SIMON,,,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007739018615,Measuring the Utility of Losses by Means of the Tradeoff Method,December 1998,HEIN FENNEMA,MARCEL VAN ASSEN,,Unknown,Unknown,Unknown,Unknown,,
18.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007706009628,Elicitation of Subjective Probabilities when the Initial Endowment is Unobservable,April 1999,Jean-Yves Jaffray,Edi Karni,,Unknown,Male,Unknown,Male,,8
18.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007708326467,Calibrating Hypothetical Willingness to Pay Responses,April 1999,Magnus Johannesson,Glenn C. Blomquist,Richard M. O'Conor,Male,Male,Male,Male,,84
18.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007760327375,Willingness to Pay for Health Protection: Inadequate Sensitivity to Probability?,April 1999,James K. Hammitt,John D. Graham,,Male,Male,Unknown,Male,,234
18.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007764411446,Are Groups More (or Less) Consistent Than Individuals?,April 1999,John Bone,John Hey,John Suckling,Male,Male,Male,Male,,63
18.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007716528284,Why the Young Do Not Buy Long-Term Care Insurance,April 1999,Volker Meier,,,Male,Unknown,Unknown,Male,,22
18.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007877728961,Exercising Property Rights to Pollute: Do Cancer Risks and Politics Affect Plant Emission Reductions?,August 1999,James T. Hamilton,,,Male,Unknown,Unknown,Male,,24
18.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007834413032,Lottery Acquisition versus Information Acquisition: Prices and Preference Reversals,August 1999,Gordon B. Hazen,Jayavel Sounderpandian,,Male,Unknown,Unknown,Male,,13
18.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007886529870,An Axiomatization of Cumulative Prospect Theory for Decision Under Risk,August 1999,Alain Chateauneuf,Peter Wakker,,Male,Male,Unknown,Male,,68
18.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007838613940,"Diabetic Risk Taking: The Role of Information, Education and Medication",August 1999,Matthew E. Kahn,,,Male,Unknown,Unknown,Male,,17
18.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007890630779,Is Workers' Compensation a Substitute for Unemployment Insurance?,August 1999,Bernard Fortin,Paul Lanoie,Christine Laporte,Male,Male,Female,Mix,,
18.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007842714849,The Effect of Legal Rules on the Value of Economic and Non-Economic Damages and the Decision to File,August 1999,Mark J. Browne,Robert Puelz,,Male,Male,Unknown,Male,,34
18.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007899711255,Analyzing the Demand for Deductible Insurance,October 1999,Jack Meyer,Michael B. Ormiston,,Male,Male,Unknown,Male,,13
18.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007845328093,"The Impact of Institutional Change on Compensating Wage Differentials for Accident Risk: South Korea, 1984–1990",October 1999,Seung-Wook Kim,Price V. Fishback,,Unknown,Male,Unknown,Male,,20
18.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007897312164,The Aversion to the Sequential Resolution of Uncertainty,October 1999,Ignacio Palacios-Huerta,,,Male,Unknown,Unknown,Male,,21
18.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007849529002,Pretrial Bargaining in the Face of a Random Court Decision: Evidence from Laboratory Games,October 1999,Alison F. Delrossi,Owen R. Phillips,,Female,Male,Unknown,Mix,,
18.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007801613072,Cost Perceptions and Voter Demand for Environmental Risk Regulation: The Double Effect of Hidden Costs,October 1999,Laurie Tipton Johnson,,,Female,Unknown,Unknown,Female,,3
18.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011156710779,Reanalysis of the Chechile-Cooke Experiment: Correcting for Mismatched Gambles,October 1999,Richard A. Chechile,R. Duncan Luce,,Male,Unknown,Unknown,Male,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007892121059,Editors' Introduction: Elicitation of Preferences,December 1999,Baruch Fischhoff,Charles F. Manski,,Male,Male,Unknown,Male,,5
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007850605129,The Effects of Financial Incentives in Experiments: A Review and Capital-Labor-Production Framework,December 1999,Colin F. Camerer,Robin M. Hogarth,,Male,,Unknown,Mix,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007802721967,Commentary on “The Effects of Financial Incentives in Experiments: A Review and Capital-Labor-Production Framework”,December 1999,David V. Budescu,,,Male,Unknown,Unknown,Male,,1
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007854706038,Commentary on “The Effects of Financial Incentives in Experiments: A Review and Capital-Labor-Production Framework”,December 1999,Catherine Eckel,,,Female,Unknown,Unknown,Female,,16
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007806822876,Analysis of Choice Expectations in Incomplete Scenarios,December 1999,Charles F. Manski,,,Male,Unknown,Unknown,Male,,52
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007858806946,Commentary on “Analysis of Choice Expectations in Incomplete Scenarios”,December 1999,Kenneth I. Wolpin,,,Male,Unknown,Unknown,Male,,8
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007811023785,"Commentary on “Analysis of Choice Expectations in Incomplete Scenarios,”",December 1999,Elke U. Weber,,,Female,Unknown,Unknown,Female,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007863007855,Rationality for Economists?,December 1999,Daniel McFadden,,,Male,Unknown,Unknown,Male,,313
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007815124693,A Challenge To The “Econoclasts”,December 1999,Mark J. Machina,,,Male,Unknown,Unknown,Male,,2
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007867108764,What's the Problem? A Commentary on “Rationality for Economists?”,December 1999,Jonathan Baron,,,Male,Unknown,Unknown,Male,,1
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007819225602,Anchoring and Acquiescence Bias in Measuring Assets in Household Surveys,December 1999,Michael D. Hurd,,,Male,Unknown,Unknown,Male,,32
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007871209672,Commentary on “Anchoring and Acquiescence Bias in Measuring Assets in Household Surveys”,December 1999,Arie Kapteyn,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007823326511,Construal Processes in Preference Assessment,December 1999,Baruch Fischhoff,Ned Welch,Shane Frederick,Male,Male,Male,Male,,31
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007875310581,"Commentary on Fischhoff et al., “Construal Processes in Preference Assessment”",December 1999,Jeff Dominitz,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007827427419,(Mis)Construal Processes for Contingent Valuation Questions: A Commentary on “Construal Processes in Preference Assessment”,December 1999,Timothy L. Mcdaniels,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007879411489,Choice Bracketing,December 1999,Daniel Read,George Loewenstein,Matthew Rabin,Male,Male,Male,Male,,361
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007831528328,"The Explanatory Power of Choice Bracketing: A Commentary on Read et al., “Choice Bracketing”",December 1999,Gideon Keren,,,Male,Unknown,Unknown,Male,,2
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007883512398,"Commentary on “Choice Bracketing” by Read, Loewenstein and Rabin",December 1999,David Laibson,,,Male,Unknown,Unknown,Male,,2
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007835629236,Economic Preferences or Attitude Expressions?: An Analysis of Dollar Responses to Public Issues,December 1999,Daniel Kahneman,Ilana Ritov,David Schkade,Male,Female,Male,Mix,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007887613307,"Commentary on “Economic Preferences or Attitude Expressions?: An Analysis of Dollar Responses to Public Issues” by Kahneman, Ritov and Schkade",December 1999,Steven J. Sherman,,,Male,Unknown,Unknown,Male,,
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007839714215,Commentary on Economic Preferences or Attitude Expressions: An Analysis of Dollar Responses to Public Issues by Kahneman et al.,December 1999,Hal Varian,,,Male,Unknown,Unknown,Male,,5
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007843931054,Measuring Constructed Preferences: Towards a Building Code,December 1999,John W. Payne,James R. Bettman,David A. Schkade,Male,Male,Male,Male,,263
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007895915124,"Defensible Preferences and the Public: Commentary on “Measuring Constructed Preferences Towards a Building Code” by Payne, Bettman and Schkade",December 1999,Norbert Schwarz,,,Male,Unknown,Unknown,Male,,2
19.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007848031962,"Commentary on “Measuring Constructed Preferences: Towards a Building Code” by Payne, Bettman and Schkade",December 1999,Robin Gregory,,,,Unknown,Unknown,Mix,,
20.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007822718954,Utility Functions for Wealth,January 2000,David E. Bell,Peter C. Fishburn,,Male,Male,Unknown,Male,,19
20.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007862603025,Do Wages Compensate for Risk of Unemployment? Parametric and Semiparametric Evidence from Seasonal Jobs,January 2000,Enrico Moretti,,,Male,Unknown,Unknown,Male,,35
20.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007814719863,"Generalized Expected Utility, Heteroscedastic Error, and Path Dependence in Risky Choice",January 2000,David Buschena,David Zilberman,,Male,Male,Unknown,Male,,43
20.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007866720772,Workers' Compensation and Family and Medical Leave Act Claim Contagion,January 2000,Harold H. Gardner,Nathan L. Kleinman,Richard J. Butler,Male,Male,Male,Male,,7
20.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007885423198,The Construction of a Simple Book,March 2000,Michael Cain,David Law,Dennis Lindley,Male,Male,Male,Male,,4
20.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007876907268,The Affection Effect in Insurance Decisions,March 2000,Christopher K. Hsee,Howard C. Kunreuther,,Male,Male,Unknown,Male,,95
20.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007829024107,Which Error Story is Best?,March 2000,Enrica Carbone,John D. Hey,,Female,Male,Unknown,Mix,,
20.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007881008177,Just Who Are You Calling Risk Averse?,March 2000,R. Mark Isaac,Duncan James,,Unknown,Male,Unknown,Male,,133
20.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007833125015,Is “Generic Utility Theory” a Suitable Theory of Choice Behavior for Gambles with Mixed Gains and Losses?,March 2000,Richard A. Chechile,Susan F. Butler,,Male,Female,Unknown,Mix,,
20.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007848013750,Do Asset Market Prices Reflect Traders' Judgment Biases?,May 2000,Ananda R. Ganguly,John H. Kagel,Donald V. Moser,Male,Male,Male,Male,,37
20.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007819530588,Stability of Estimates of the Compensation for Danger,May 2000,G. R. Arabsheibani,A. Marin,,Unknown,Unknown,Unknown,Unknown,,
20.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007871514658,Buying Insurance for Disaster-Type Risks: Experimental Evidence,May 2000,Philip T. Ganderton,David S. Brookshire,Hale Thurston,Male,Male,Female,Mix,,
20.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007823631497,The Demand for Flood Insurance: Empirical Evidence,May 2000,Mark J. Browne,Robert E. Hoyt,,Male,Male,Unknown,Male,,306
21.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1026561108963,Editor's Introduction,July 2000,,,,Unknown,Unknown,Unknown,Unknown,,
21.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1026565225801,"An Iterative Choice Approach to Valuing Clean Lakes, Rivers, and Streams",July 2000,Wesley A. Magat,Joel Huber,Jason Bell,Male,Male,Male,Male,,31
21.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1026517309871,Action Bias and Environmental Decisions,July 2000,Anthony Patt,Richard Zeckhauser,,Male,Male,Unknown,Male,,85
21.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1026569410780,Do Painless Environmental Policies Exist?,July 2000,V. Kerry Smith,Randy Walsh,,Unknown,,Unknown,Mix,,
21.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1026573527618,Valuation of Multiple Environmental Programs,July 2000,John W. Payne,David A. Schkade,Chris Aultman,Male,Male,,Mix,,
21.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1026525611689,Public Choices Between Life Saving Programs: The Tradeoff Between Qualitative Factors and Lives Saved,July 2000,Uma Subramanian,Maureen Cropper,,Female,Female,Unknown,Female,,32
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007841906187,Smoking Risk Policy: A Special Issue of the Journal of Risk and Uncertainty,November 2000,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007807323025,Smoking Risks in Spain: Part I—Perception of Risks to the Smoker,November 2000,Fernando Antoñanzas,W. Kip Viscusi,Iirineu Carvalho,Male,Unknown,Unknown,Male,,34
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007859307096,Smoking Risks in Spain: Part II—Perceptions of Environmental Tobacco Smoke Externalities,November 2000,Joan Rovira,W. Kip Viscusi,Irineu Carvalho,Female,Unknown,Unknown,Female,,19
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007811423934,Smoking Risks in Spain: Part III—Determinants of Smoking Behavior,November 2000,W. Kip Viscusi,Irineu Carvalho,Fabiola Portillo,Unknown,Unknown,Female,Female,,21
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007863408004,How Good a Deal Was the Tobacco Settlement?: Assessing Payments to Massachusetts,November 2000,David M. Cutler,Arnold M. Epstein,Elizabeth Richardson Vigdor,Male,Male,Female,Mix,,
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007815524843,"Gender, Income Levels, and the Demand for Cigarettes",November 2000,Joni Hersch,,,Female,Unknown,Unknown,Female,,59
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007867508913,Passive Smoking and Health Care: Health Perceptions Myth vs. Health Care Reality,November 2000,Michael J. Moore,Carolyn W. Zhu,,Male,Female,Unknown,Mix,,
21.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1007819625751,Racial Difference in the Determinants of Smoking Onset,November 2000,Philip DeCicca,Donald Kenkel,Alan Mathios,Male,Male,Male,Male,,27
22.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011180805925,The Firm Under Uncertainty with General Risk-Averse Preferences: A State-Contingent Approach,January 2001,John Quiggin,Robert G. Chambers,,Male,Male,Unknown,Male,,8
22.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011149422763,Lottery Decisions and Probability Weighting Function,January 2001,Yves Alarie,Georges Dionne,,Male,Male,Unknown,Male,,12
22.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011101506833,Individuals' Estimates of the Risks of Death: Part II—New Evidence,January 2001,Daniel K. Benjamin,William R. Dougan,David Buschena,Male,Male,Male,Male,,28
22.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011153523672,Generalized Disappointment Models,January 2001,Jianmin Jia,James S. Dyer,John C. Butler,Unknown,Male,Male,Male,,38
22.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011105607742,Are Event-Splitting Effects Actually Boundary Effects?,January 2001,Steven J. Humphrey,,,Male,Unknown,Unknown,Male,,11
22.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011105524935,Dynamic Decision-Making Under Uncertainty: An Experimental Investigation of Choices Between Accumulator Gambles,March 2001,Robin P. Cubitt,Robert Sugden,,,Male,Unknown,Mix,,
22.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011157509006,Comparative Ignorance and the Ellsberg Paradox,March 2001,Clare Chua Chow,Rakesh K. Sarin,,Female,Male,Unknown,Mix,,
22.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011161709914,Rush and Procrastination Under Hyperbolic Discounting and Interdependent Activities,March 2001,Isabelle Brocas,Juan D. Carrillo,,Female,Male,Unknown,Mix,,
22.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011109625844,Estimating Risk Attitudes using Lotteries: A Large Sample Approach,March 2001,Bas Donkers,Bertrand Melenberg,Arthur Van Soest,Male,Male,Male,Male,,242
22.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011144500928,The Benefits of Reducing Gun Violence: Evidence from Contingent-Valuation Survey Data,May 2001,Jens Ludwig,Philip J. Cook,,Male,Male,Unknown,Male,,70
22.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011131017766,The Relation Between Probability and Evidence Judgment: An Extension of Support Theory,May 2001,Lorraine Chen Idson,David H. Krantz,Nicolao Bonini,Female,Male,Unknown,Mix,,
22.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011183001837,The Medium Prizes Paradox: Evidence From a Simulated Casino,May 2001,Ernan Haruvy,Ido Erev,Doron Sonsino,Unknown,Male,Male,Male,,28
22.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011135118675,Self Selection Bias in the Estimates of Compensating Differentials for Job Risks in India,May 2001,K. R. Shanmugam,,,Unknown,Unknown,Unknown,Unknown,,
22.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011187102745,Premium Risk and Managed Care,May 2001,Mathias Kifmann,,,Male,Unknown,Unknown,Male,,5
23.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011198414683,Is Time-Discounting Hyperbolic or Subadditive?,July 2001,Daniel Read,,,Male,Unknown,Unknown,Male,,260
23.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011112631522,Measurement Error and the Effects of Unions on the Compensating Differentials for Fatal Workplace Risks,July 2001,Robert Sandy,Robert F. Elliott,Xiangdong Wei,Male,Male,Unknown,Male,,20
23.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011164615592,A μ-σ-Risk Aversion Paradox and Wealth Dependent Utility,July 2001,Andreas Löffler,,,Male,Unknown,Unknown,Male,,8
23.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011116732430,Recurrence in Workers' Compensation Claims: Estimates from a Multiple Spell Hazard Model,July 2001,Michele Campolieti,,,Female,Unknown,Unknown,Female,,15
23.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011111601406,Making Low Probabilities Useful,September 2001,Howard Kunreuther,Nathan Novemsky,Daniel Kahneman,Male,Male,Male,Male,,154
23.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011180018244,The Value of a Statistical Life in Transport: Findings from a New Contingent Valuation Study in Sweden,September 2001,Ulf Persson,Anna Norinder,Katarina Gralén,Male,Female,Female,Mix,,
23.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011132102314,Ranked-Weighted Utilities and Qualitative Convolution,September 2001,A. A. J. Marley,R. Duncan Luce,,Unknown,Unknown,Unknown,Unknown,,
23.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011184119153,Valuing Mortality-Risk Reduction: Using Visual Aids to Improve the Validity of Contingent Valuation,September 2001,Phaedra S. Corso,James K. Hammitt,John D. Graham,Female,Male,Male,Mix,,
23.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011136203223,Probability Weighting in Choice under Risk: An Empirical Test,September 2001,Han Bleichrodt,,,,Unknown,Unknown,Mix,,
23.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011833407458,Payoff Kinks in Preferences over Lotteries,November 2001,Mark J. Machina,,,Male,Unknown,Unknown,Male,,11
23.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011825824296,Background Risks and the Value of a Statistical Life,November 2001,Louis R. Eeckhoudt,James K. Hammitt,,Male,Male,Unknown,Male,,84
23.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011877808366,On the Intuition of Rank-Dependent Utility,November 2001,Enrico Diecidue,Peter P. Wakker,,Male,Male,Unknown,Male,,128
23.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1011829925205,"Improved Risk Information, the Demand for Cigarettes, and Anti-Tobacco Policy",November 2001,Peter Zweifel,,,Male,Unknown,Unknown,Male,,3
24.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1013234309490,Living Healthy and Living Long: Valuing the Nonpecuniary Loss from Disability and Death,January 2002,Krista M. Perreira,Frank A. Sloan,,Female,Male,Unknown,Mix,,
24.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1013225326328,A Further Examination of Cumulative Prospect Theory Parameterizations,January 2002,William Neilson,Jill Stowe,,Male,Female,Unknown,Mix,,
24.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1013277310399,Effects of Outcome and Probabilistic Ambiguity on Managerial Choices,January 2002,Joanna L. Y. Ho,L. Robin Keller,Pamela Keltyka,Female,Unknown,Female,Female,,62
24.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1013229427237,Consumer Preferences for Food Irradiation: How Favorable and Unfavorable Descriptions Affect Preferences for Irradiated Pork in Experimental Auctions,January 2002,John A. Fox,Dermot J. Hayes,Jason F. Shogren,Male,Male,Male,Male,,155
24.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1014094209265,A Microeconometric Test of Alternative Stochastic Theories of Risky Choice,March 2002,Graham Loomes,Peter G. Moffatt,Robert Sugden,Male,Male,Male,Male,,151
24.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1014015926103,Comparative Risk Sensitivity with Reference-Dependent Preferences,March 2002,William S. Neilson,,,Male,Unknown,Unknown,Male,,26
24.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1014067910173,Examining Predictive Accuracy Among Discounting Models,March 2002,L. Robin Keller,Elisabetta Strazzera,,Unknown,Female,Unknown,Female,,34
24.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1014020027011,"Age, Health and the Willingness to Pay for Mortality Risk Reductions: A Contingent Valuation Survey of Ontario Residents",March 2002,Alan Krupnick,Anna Alberini,Martin Heintzelman,Male,Female,Male,Mix,,
24.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1015697417916,Horizon Length and Portfolio Risk,May 2002,Christian Gollier,Richard J. Zeckhauser,,Male,Male,Unknown,Male,,47
24.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1015683401986,Replacement Cost Endorsement and Opportunistic Fraud in Automobile Insurance,May 2002,Georges Dionne,Robert Gagné,,Male,Male,Unknown,Male,,36
24.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1015635518824,Do Life-Saving Regulations Save Lives?,May 2002,Ulf-G. Gerdtham,Magnus Johannesson,,Unknown,Male,Unknown,Male,,31
24.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1015687502895,Testing Theories of Choice Under Risk: Estimation of Individual Functionals,May 2002,Serge Blondel,,,Male,Unknown,Unknown,Male,,12
24.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1015639619733,Greater Downside Risk Aversion,May 2002,Donald C. Keenan,Arthur Snow,,Male,Male,Unknown,Male,,48
25.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1016340413134,Explicit Versus Implicit Income Insurance,July 2002,Thomas J. Kniesner,James P. Ziliak,,Male,Male,Unknown,Male,,17
25.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1016315329973,Tacit Coordination in Choice Between Certain Outcomes in Endogenously Determined Lotteries,July 2002,Amnon Rapoport,Darryl A. Seale,Lisa Ordóñez,Male,Male,Female,Mix,,
25.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1016367314043,Compensation for What? An Analysis of Insurance Strategies for Repairable Assets,July 2002,Pierre-François Koehl,Bertrand Villeneuve,,Unknown,Male,Unknown,Male,,1
25.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1016319430881,Simultaneous Over- and Underconfidence: Evidence from Experimental Asset Markets,July 2002,Erich Kirchler,Boris Maciejovsky,,Male,Male,Unknown,Male,,69
25.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1016371414952,"The Endowment Effect, Status Quo Bias and Loss Aversion: Rational Alternative Explanation",July 2002,Dominique Y. Dupont,Gabriel S. Lee,,,Male,Unknown,Mix,,
25.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020642912445,Pitfalls in Drawing Policy Conclusions from Retrospective Survey Data: The Case of Advertising and Underage Smoking,September 2002,John Geweke,Donald L. Martin,,Male,Male,Unknown,Male,,6
25.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020691629284,Risk and Self-Protection: A State-Contingent View,September 2002,John Quiggin,,,Male,Unknown,Unknown,Male,,17
25.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020643713354,Propagation of Individual Bias through Group Judgment: Error in the Treatment of Asymmetrically Informative Signals,September 2002,William P. Bottom,Krishna Ladha,Gary J. Miller,Male,,Male,Mix,,
25.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020695730192,Risk Perceptions and Alcohol Consumption among Young People,September 2002,Petter Lundborg,Björn Lindgren,,Male,Male,Unknown,Male,,46
25.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020647814263,Stretching the Truth: Elastic Justification and Motivated Communication of Uncertain Information,September 2002,Maurice E. Schweitzer,Christopher K. Hsee,,Male,Male,Unknown,Male,,169
25.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020661025652,Errata,September 2002,Serge Blondel,,,Male,Unknown,Unknown,Male,,1
25.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020962104810,Public Perceptions of Risk and Preference-Based Values of Safety,November 2002,Susan Chilton,Judith Covey,Anne Spencer,Female,Female,Female,Female,,63
25.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020923921649,An Experimental Test of Loss Aversion,November 2002,Ulrich Schmidt,Stefan Traub,,Male,Male,Unknown,Male,,138
25.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020975905719,On the Definition and Age-Dependency of the Value of a Statistical Life,November 2002,Per-Olov Johansson,,,Unknown,Unknown,Unknown,Unknown,,
25.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1020928022557,"Arrow-Pratt Risk Aversion, Risk Premium and Decision Weights",November 2002,Haim Levy,Moshe Levy,,Male,Male,Unknown,Male,,25
26.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1022299422219,Helping a Victim or Helping the Victim: Altruism and Identifiability,January 2003,Deborah A. Small,George Loewenstein,,Female,Male,Unknown,Mix,,
26.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1022246206289,On the Representation of Beliefs by Probabilities,January 2003,Edi Karni,,,Male,Unknown,Unknown,Male,,15
26.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1022298223127,Measuring Intergenerational Time Preference: Are Future Lives Valued Less?,January 2003,Shane Frederick,,,Male,Unknown,Unknown,Male,,55
26.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1022250307197,Reassessing the Testing of Generic Utility Models for Mixed Gambles,January 2003,Richard A. Chechile,Susan F. Butler,,Male,Female,Unknown,Mix,,
26.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1022202424036,Perceived Hazard and Product Choice: An Application to Recreational Site Choice,January 2003,Paul M. Jakus,W. Douglass Shaw,,Male,Unknown,Unknown,Male,,25
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024111622266,Sacrificing Civil Liberties to Reduce Terrorism Risks,March 2003,W. Kip Viscusi,Richard J. Zeckhauser,,Unknown,Male,Unknown,Male,,103
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024111006336,Terrorism and Probability Neglect,March 2003,Cass R. Sunstein,,,,Unknown,Unknown,Mix,,
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024163023174,Judged Terror Risk and Proximity to the World Trade Center,March 2003,Baruch Fischhoff,Roxana M. Gonzalez,Jennifer S. Lerner,Male,Female,Female,Mix,,
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024115107245,"Catastrophic Events, Parameter Uncertainty and the Breakdown of Implicit Long-Term Contracting: The Case of Terrorism Insurance",March 2003,J. David Cummins,Christopher M. Lewis,,Unknown,Male,Unknown,Male,,48
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024161808231,Insuring September 11th: Market Recovery and Transparency,March 2003,Neil A. Doherty,Joan Lamm-Tennant,Laura T. Starks,Male,Female,Female,Mix,,
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024167124083,The Ecology of Terror Defense,March 2003,Nathaniel O. Keohane,Richard J. Zeckhauser,,Male,Male,Unknown,Male,,76
26.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1024119208153,Interdependent Security,March 2003,Howard Kunreuther,Geoffrey Heal,,Male,Male,Unknown,Male,,405
27.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025598106257,The Value of a Statistical Life: A Critical Review of Market Estimates Throughout the World,August 2003,W. Kip Viscusi,Joseph E. Aldy,,Unknown,Male,Unknown,Male,,1067
27.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025576823096,Decision-Making Under Scientific Uncertainty: The Economics of the Precautionary Principle,August 2003,Christian Gollier,Nicolas Treich,,Male,Male,Unknown,Male,,123
27.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025662307166,Local Utility Functions and Local Probability Transformations,October 2003,John Quiggin,Robert G. Chambers,,Male,Male,Unknown,Male,,3
27.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025680924004,Optimal Insurance With Divergent Beliefs About Insurer Total Default Risk,October 2003,J. David Cummins,Olivier Mahul,,Unknown,Male,Unknown,Male,,56
27.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025633008075,Preference Reversals and Induced Risk Preferences: Evidence for Noisy Maximization,October 2003,Joyce E. Berg,John W. Dickhaut,Thomas A. Rietz,Female,Male,Male,Mix,,
27.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025685024913,Did We Overestimate the Value of Health?,October 2003,Rafael Lalive,,,Male,Unknown,Unknown,Male,,14
27.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025816308983,Introduction: Making Sense of Safety,December 2003,Thomas J. Kniesner,,,Male,Unknown,Unknown,Male,,
27.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025889125822,On the Measurement of Job Risk in Hedonic Wage Models,December 2003,Dan A. Black,Thomas J. Kniesner,,Male,Male,Unknown,Male,,81
27.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025841209892,Saving Lives: A Review of the Record,December 2003,John F. Morrall III,,,Male,Unknown,Unknown,Male,,30
27.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025893226730,Racial Differences in Labor Market Values of a Statistical Life,December 2003,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
27.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025845310801,Compensating Wage Differentials for Fatal and Nonfatal Injury Risk by Gender and Race,December 2003,John D. Leeth,John Ruser,,Male,Male,Unknown,Male,,61
27.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/A:1025897327639,The Clean Air Act of 1970 and Adult Mortality,December 2003,Kenneth Chay,Carlos Dobkin,Michael Greenstone,Male,Male,Male,Male,,92
28.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000009433.25126.87,Neglecting Disaster: Why Don't People Insure Against Large Losses?,January 2004,Howard Kunreuther,Mark Pauly,,Male,Male,Unknown,Male,,180
28.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000009434.18807.bd,A Simple Risk-Sharing Experiment,January 2004,John Bone,John Hey,John Suckling,Male,Male,Male,Male,,18
28.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000009435.11390.23,Effects of Risk and Time Preference and Expected Longevity on Demand for Medical Tests,January 2004,Gabriel Picone,Frank Sloan,Donald Taylor Jr.,Male,Male,Male,Male,,120
28.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000009436.14961.6a,A Note on Luce-Fishburn Axiomatization of Rank-Dependent Utility,January 2004,Liping Liu,,,Unknown,Unknown,Unknown,Unknown,,
28.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000009437.24783.e1,Effects of Disease Type and Latency on the Value of Mortality Risk,January 2004,James K. Hammitt,Jin-Tan Liu,,Male,Unknown,Unknown,Male,,93
28.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000016139.79886.3e,"The Fatality Risks of Sport-Utility Vehicles, Vans, and Pickups Relative to Cars",March 2004,Ted Gayer,,,Male,Unknown,Unknown,Male,,48
28.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000016140.72468.f7,"A Simple Tool for Qualitatively Testing, Quantitatively Measuring, and Normatively Justifying Savage's Subjective Expected Utility",March 2004,Veronika Köbberling,Peter P. Wakker,,Female,Male,Unknown,Mix,,
28.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000016141.88127.7c,Is Transport Safety More Valuable in the Air?,March 2004,Fredrik Carlsson,Olof Johansson-Stenman,Peter Martinsson,Male,Male,Male,Male,,38
28.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000016142.47743.5a,Sequential Parimutuel Betting in the Laboratory,March 2004,Anthony Ziegelmeyer,Marie-Hélène Broihanne,Frédéric Koessler,Male,Unknown,Male,Male,,4
28.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000026095.75015.e0,The Ecology of Risk Taking,May 2004,François Degeorge,Boaz Moselle,Richard Zeckhauser,Male,Male,Male,Male,,9
28.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000026096.48985.a3,Is Probability Weighting Sensitive to the Magnitude of Consequences? An Experimental Investigation on Losses,May 2004,Nathalie Etchart-Vincent,,,Female,Unknown,Unknown,Female,,95
28.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000026097.72268.8d,The Swing of the Regulatory Pendulum in Europe: From Precautionary Principle to (Regulatory) Impact Analysis,May 2004,Ragnar E. Löfstedt,,,Male,Unknown,Unknown,Male,,
28.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000026098.84109.62,Do They Know What They are Doing? Risk Perceptions and Smoking Behaviour Among Swedish Teenagers,May 2004,Petter Lundborg,Björn Lindgren,,Male,Male,Unknown,Male,,48
29.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000031515.28779.12,Introduction: Risk and Uncertainty in Environmental and Resource Economics,July 2004,Christian Gollier,Hans-Peter Weikard,Justus Wesseler,Male,Unknown,Male,Male,,2
29.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000031442.60436.ea,"Social Willingness to Pay, Mortality Risks and Contingent Valuation",July 2004,Olivier Armantier,Nicolas Treich,,Male,Male,Unknown,Male,,8
29.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000031443.39763.f0,Robust Control in Water Management,July 2004,Catarina Roseta-Palma,Anastasios Xepapadeas,,Female,Male,Unknown,Mix,,
29.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000031444.37497.26,A Bird in the Hand is Worth Two in the Bush? When Do We Prefer Something Certainly Dirty to Something Perhaps Clean?,July 2004,Andreas Lange,Ulf Moslener,,Male,Male,Unknown,Male,,
29.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000031445.13939.e4,"Stable International Environmental Agreements with a Stock Pollutant, Uncertainty and Learning",July 2004,Alistair Ulph,,,Male,Unknown,Unknown,Male,,56
29.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000038939.25355.d8,"Data Mining Mining Data: MSHA Enforcement Efforts, Underground Coal Mine Safety, and New Health Policy Implications",September 2004,Thomas J. Kniesner,John D. Leeth,,Male,Male,Unknown,Male,,24
29.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000038940.62992.1b,"How Do People Take into Account Weight, Strength and Quality of Segregated vs. Aggregated Data? Experimental Evidence",September 2004,Carlo Kraemer,Martin Weber,,Male,Male,Unknown,Male,,16
29.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000038941.44379.82,Strategic Choice of Variability in Multiround Contests and Contests with Handicaps,September 2004,Ilia Tsetlin,Anil Gaba,Robert L. Winkler,Male,Male,Male,Male,,17
29.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000038942.18349.88,"Changes in the Value of Life, 1940–1980",September 2004,Dora L. Costa,Matthew E. Kahn,,Female,Male,Unknown,Mix,,
29.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000038943.63610.16,Incorporating Framing into Prospect Theory Modeling: A Mixture-Model Approach,September 2004,Mei Wang,Paul S. Fischbeck,,,Male,Unknown,Mix,,
29.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000046143.10752.0a,Are Preference Reversals Errors? An Experimental Investigation,December 2004,Ulrich Schmidt,John D. Hey,,Male,Male,Unknown,Male,,36
29.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000046144.89048.9e,How Much Internalization of Nuclear Risk Through Liability Insurance?,December 2004,Yves Schneider,Peter Zweifel,,Male,Male,Unknown,Male,,13
29.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000046145.25793.37,The Utility of Gambling Reconsidered,December 2004,Enrico Diecidue,Ulrich Schmidt,Peter P. Wakker,Male,Male,Male,Male,,46
29.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000046146.97495.9e,Comparative Mixed Risk Aversion: Definition and Application to Self-Protection and Willingness to Pay,December 2004,Kaïs Dachraoui,Georges Dionne,Philippe Godfroid,Unknown,Male,Male,Male,,42
29.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1023/B:RISK.0000046147.20779.70,Valuing International Safety Externalities: Does the “Golden Rule” Apply?,December 2004,Michael Jones-Lee,,,Male,Unknown,Unknown,Male,,6
30.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5831-x,It is Whether You Win or Lose: The Importance of the Overall Probabilities of Winning or Losing in Risky Choice,January 2005,John W. Payne,,,Male,Unknown,Unknown,Male,,103
30.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5832-9,Ranked Additive Utility Representations of Gambles: Old and New Axiomatizations,January 2005,R. Duncan Luce,A. A. J. Marley,,Unknown,Unknown,Unknown,Unknown,,
30.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5833-8,Updating Subjective Risks in the Presence of Conflicting Information: An Application to Climate Change,January 2005,Trudy Ann Cameron,,,Female,Unknown,Unknown,Female,,74
30.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-6561-9,Testing Prospect Theories Using Probability Tradeoff Consistency,March 2005,George Wu,Jiao Zhang,Mohammed Abdellaoui,Male,,Male,Mix,,
30.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-6562-8,An Empirical Investigation of the Assumptions of Risk-Value Models,March 2005,John C. Butler,James S. Dyer,Jiammin Jia,Male,Male,Unknown,Male,,13
30.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-6564-6,What is Loss Aversion?,March 2005,Ulrich Schmidt,Horst Zank,,Male,Male,Unknown,Male,,99
30.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-6565-5,An Experimental Comparison of Induced and Elicited Beliefs,March 2005,Terrance M. Hurley,Jason F. Shogren,,Male,Male,Unknown,Male,,29
30.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-1153-2,The Gambler’s Fallacy and the Hot Hand: Empirical Data from Casinos,May 2005,Rachel Croson,James Sundali,,Female,Male,Unknown,Mix,,
30.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-1154-1,The Value of Safety as Revealed in the Swedish Car Market: An Application of the Hedonic Pricing Approach,May 2005,Henrik Andersson,,,Male,Unknown,Unknown,Male,,56
30.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-6563-7,Informational Asymmetries and Observational Learning in Search,May 2005,Liran Einav,,,Male,Unknown,Unknown,Male,,11
30.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-1155-0,"Risk Perceptions and Value of a Statistical Life for Air Pollution and Traffic Accidents: Evidence from Bangkok, Thailand",May 2005,Sujitra Vassanadumrongdee,Shunji Matsuoka,,Unknown,Male,Unknown,Male,,62
31.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-2927-2,Smoking Restrictions as a Self-Control Mechanism,July 2005,Joni Hersch,,,Female,Unknown,Unknown,Female,,33
31.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-2928-1,The Value of a Statistical Life and the Coefficient of Relative Risk Aversion,July 2005,Louis Kaplow,,,Male,Unknown,Unknown,Male,,63
31.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-2929-0,The Diversification Theorem Restated: Risk-pooling Without Assignment of Probabilities,July 2005,Göran Skogh,Hong Wu,,Male,,Unknown,Mix,,
31.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-2930-7,Heterogeneous and Correlated Risk Preferences in Commercial Fishermen: The Perfect Storm Dilemma,July 2005,Martin D. Smith,James E. Wilen,,Male,Male,Unknown,Male,,57
31.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-2931-6,A Choice Experiment Approach to the Valuation of Mortality,July 2005,Takahiro Tsuge,Atsuo Kishimoto,Kenji Takeuchi,Male,Male,Male,Male,,60
31.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-3551-x,How Many Balance Functions Does it Take to Determine a Utility Function?,September 2005,John W. Pratt,,,Male,Unknown,Unknown,Male,,1
31.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-3552-9,Explaining Diversities in Age-Specific Life Expectancies and Values of Life Saving: A Numerical Analysis,September 2005,Isaac Ehrlich,Yong Yin,,Male,,Unknown,Mix,,
31.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-3553-8,How Do Information Ambiguity and Timing of Contextual Information Affect Managers’ Goal Congruence in Making Investment Decisions in Good Times vs. Bad Times?,September 2005,Joanna L. Y. Ho,L. Robin Keller,Pamela Keltyka,Female,Unknown,Female,Female,,11
31.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-3554-7,Investigating Risky Choices Over Losses Using Experimental Data,September 2005,Charles F. Mason,Jason F. Shogren,John A. List,Male,Male,Male,Male,,22
31.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-3555-6,Individual and Household Values of Mortality Reductions with Intrahousehold Bargaining,September 2005,Jon Strand,,,Male,Unknown,Unknown,Male,,8
31.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5102-x,Relative Risk Aversion: What Do We Know?,December 2005,Donald J. Meyer,Jack Meyer,,Male,Male,Unknown,Male,,156
31.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5103-9,A Comparison of Five Models that Predict Violations of First-Order Stochastic Dominance in Risky Decision Making,December 2005,Michael H. Birnbaum,,,Male,Unknown,Unknown,Male,,38
31.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5104-8,Fear of Ruin,December 2005,Jérôme Foncel,Nicolas Treich,,Male,Male,Unknown,Male,,21
31.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-005-5105-7,Loss Averse Behavior,December 2005,Peter Brooks,Horst Zank,,Male,Male,Unknown,Male,,98
32.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s10797-006-6663-6,Behavioral Probabilities,January 2006,W. Kip Viscusi,William N. Evans,,Unknown,Male,Unknown,Male,,32
32.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s10797-006-6664-5,"The Effects of Spousal Health on the Decision to Smoke: Evidence on Consumption Externalities, Altruism and Learning Within the Household",January 2006,Ahmed Khwaja,Frank Sloan,Sukyung Chung,Male,Male,Unknown,Male,,21
32.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s10797-006-6665-4,Estimates of Own Lethal Risks and Anchoring Effects,January 2006,Olivier Armantier,,,Male,Unknown,Unknown,Male,,13
32.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s10797-006-6666-3,Assessing the Accuracy of Self-Reported Data: an Evaluation of the Toxics Release Inventory,January 2006,Scott de Marchi,James T. Hamilton,,Male,Male,Unknown,Male,,112
32.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-8288-7,An exploration of the offset hypothesis using disaggregate data: The case of airbags and antilock brakes,March 2006,Clifford Winston,Vikram Maheshri,Fred Mannering,Male,Male,Male,Male,,102
32.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-8289-6,Cumulative prospect theory's functional menagerie,March 2006,Henry P. Stott,,,Male,Unknown,Unknown,Male,,273
32.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-8290-0,A theoretically-consistent empirical model of non-expected utility: An application to nuclear-waste transport,March 2006,Mary Riddel,W. Douglass Shaw,,,Unknown,Unknown,Mix,,
32.0,2.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-8291-z,The value of improved road safety,March 2006,Lars Hultkrantz,Gunnar Lindberg,Camilla Andersson,Male,Male,Female,Mix,,
32.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-9518-8,Ambiguity seeking as a result of the status quo bias,May 2006,Mercè Roca,Robin M. Hogarth,A. John Maule,Female,,Unknown,Mix,,
32.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-9519-7,Lottery qualities,May 2006,Yves Alarie,Georges Dionne,,Male,Male,Unknown,Male,,
32.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-9520-1,Discount rates and risky sexual behaviors among teenagers and young adults,May 2006,Harrell W. Chesson,Jami S. Leichliter,Kenneth H. Fife,Male,Female,Male,Mix,,
32.0,3.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-9521-0,Willingness to pay for mortality risk reductions: Does latency matter?,May 2006,Anna Alberini,Maureen Cropper,Nathalie B. Simon,Female,Female,Female,Female,,40
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0168-7,Natural disaster risks: An introduction,September 2006,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0169-6,"National survey evidence on disasters and relief: Risk beliefs, self-interest, and compassion",September 2006,W. Kip Viscusi,Richard J. Zeckhauser,,Unknown,Male,Unknown,Male,,75
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0170-0,Adjusting to natural disasters,September 2006,V. Kerry Smith,Jared C. Carbone,Michael E. Darden,Unknown,Male,Male,Male,,88
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0171-z,The catastrophic effects of natural disasters on insurance markets,September 2006,Patricia Born,W. Kip Viscusi,,Female,Unknown,Unknown,Female,,48
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0172-y,Private investment and government protection,September 2006,Carolyn Kousky,Erzo F. P. Luttmer,Richard J. Zeckhauser,Female,Unknown,Male,Mix,,
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0173-x,Rules rather than discretion: Lessons from Hurricane Katrina,September 2006,Howard Kunreuther,Mark Pauly,,Male,Male,Unknown,Male,,108
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0174-9,Planning for natural disasters in a stochastic world,September 2006,Lester B. Lave,Jay Apt,,Male,Male,Unknown,Male,,9
33.0,1.0,Journal of Risk and Uncertainty,,https://link.springer.com/article/10.1007/s11166-006-0175-8,Analyzing disaster risks and plans: An avian flu example,September 2006,Baruch Fischhoff,Wändi Bruine de Bruin,Larry Brilliant,Male,Unknown,Male,Male,,41
34.0,1.0,Journal of Risk and Uncertainty,29 December 2006,https://link.springer.com/article/10.1007/s11166-006-9001-6,"Counting casualties: A framework for respectful, useful records",February 2007,Baruch Fischhoff,Scott Atran,Noam Fischhoff,Male,Male,,Mix,,
34.0,1.0,Journal of Risk and Uncertainty,10 January 2007,https://link.springer.com/article/10.1007/s11166-006-9002-5,"The value of mortality risk reductions in Delhi, India",February 2007,Soma Bhattacharya,Anna Alberini,Maureen L. Cropper,Female,Female,Female,Female,"Reductions in risk of death in the context of road safety have been valued using both revealed and stated preference approaches. Studies in high income countries have used expenditures on safer automobiles, child safety seats and bicycle helmets to infer the value placed on reductions in risk of death. Hedonic studies of automobile prices (Atkinson and Halvorsen, 1990; Dreyfus and Viscusi, 1995; Andersson, 2005) decompose automobile price into the price of various vehicle characteristics, including the probability of a fatal accident. The marginal cost of a risk reduction should equal the value of the reduction to the purchaser at the margin. Studies involving bicycle helmets and car seats (Jenkins et al., 2001; Blomquist et al., 1996) are based on the assumption that, for the marginal buyer, the value of the risk reduction achieved equals the cost of buying it. This allows the researcher to infer the value of safety from purchases of such safety equipment. Studies have also attempted to infer the value of safety from seatbelt usage (Blomquist, 1979, 1991) and vehicle speeds (Ghosh et al., 1975; Ashenfelter and Greenstone, 2002). To accomplish the former, the time cost of using a seatbelt (assumed equal at the margin to the benefits of using the belt) must be monetized. Likewise, the time saving associated with faster speeds must be monetized to infer the rate at which people are trading money for higher risk of death at faster speeds. Revealed preference studies are difficult to implement in a developing country context. The data required to implement a hedonic pricing study of the automobile market would be difficult to obtain in India. Even if such data existed, they would apply to a small segment of the population. (Only 13% of households in Delhi own cars.) More importantly, revealed preference studies have a serious drawback even in a developed country setting. These studies measure the risk reductions associated with safety equipment or safer vehicles by the objective risk reductions achieved. The studies implicitly assume that consumers’ risk perceptions match objective risks—that consumers think they are buying the risk reduction that is measured by objective methods. Studies have, however, cast doubt on laypersons’ abilities to accurately estimate small probabilities (Viscusi and O’Connor, 1984). If this is the case, the values from revealed preference studies correspond to risk changes of an unknown magnitude. This has led to the use of stated preference studies. In a pioneering study to value mortality risks in a transport context, Jones-Lee et al. (1985) asked respondents what they would pay to travel on a safer bus, i.e., what they would pay to reduce their risk of death on a bus trip from (e.g.) 8 in 100,000 to 4 in 100,000 and from 8 in 100,000 to 1 in 100,000. Other studies have asked respondents about their WTP for living in a city with lower risk of mortality from road accidents (Guria et al., 2005; Viscusi, 1995), or what they would pay to install an optional safety device in their car (Dubourg et al., 1997; Corso et al., 2001; Persson et al., 2001).Footnote 2
 The vast majority of WTP studies in the context of road safety have been conducted in high income countries; few have been conducted in developing countries. Exceptions include stated preference studies in Chile, Thailand and Malaysia (Ortuzar et al., 2000; Vassanadumrongdee and Matsuoka, 2005; Melhuish et al., 2005). In spite of the very different pattern of road traffic deaths in developing countries, these studies have relied on the same scenarios as studies in high income countries.Footnote 3 We have attempted to construct scenarios that reflect the profile of road accidents in developing countries, where pedestrians and motorcyclists bear the brunt of road fatalities.",71
34.0,1.0,Journal of Risk and Uncertainty,30 December 2006,https://link.springer.com/article/10.1007/s11166-006-9003-4,Can ranking techniques elicit robust values?,February 2007,Ian Bateman,Brett Day,Robert Sugden,Male,Male,Male,Male,"There is now a substantial literature reporting evidence of apparently systematic violations of key axioms of expected utility (EU) theory—see, for example, Camerer (1995). One ‘anomaly’ which is particularly troublesome, not only for standard theory but also for applied work in various important areas of public policy, is the preference reversal (PR) phenomenon, first reported by Lichtenstein and Slovic (1971) and Lindman (1971). Although the PR anomaly can take a variety of forms (Tversky and Thaler, 1990; Seidl, 2002), the best known and most frequently replicated occurs when the preference ordering inferred from the values an individual separately attaches to two different items is contradicted by the choice that individual makes when considering them together. Such reversals have been replicated within gains (Reilly, 1982) and losses (MacDonald et al., 1992); have been found in both individual and group responses (Mowen and Gentry, 1980); have been observed both in real-world lotteries (Bohm and Lind, 1993) and in those constructed in the laboratory (Knez and Smith, 1987); occur across lotteries which differ in expected value (Cox and Epstein, 1989) and are priced using differing formats (Berg et al., 1985). Perhaps most tellingly PR phenomena have been shown to be robust against explicit attempts to design them out of responses (Grether and Plott, 1979). Such is the frequency, persistence and robustness of the anomaly that, in his review of the literature, Seidl (2002, p. 621) describes PR as “one of the most spectacular violations of procedure invariance.” As noted, the most frequently replicated form of PR is that which elicits certainty equivalent (usually, selling price) values for two lotteries and also asks respondents to make a straight choice between the two. Normally in these experiments, one lottery gives a relatively small chance of a high payoff (and has come to be referred to as the $-bet) while the other offers a much more modest payoff, but with a high probability of receiving it (and is often referred to as the P-bet). Transitivity requires that an individual who (strictly) prefers lottery X to lottery Y will both place a higher certainty equivalent value on X than on Y and also select X in a straight choice between the two. But the PR phenomenon shows that many people behave otherwise: a substantial proportion place a higher value on the $-bet, but choose the P-bet in a straight choice between the two. In some experiments, this is the modal pattern and we shall refer to it as the regular form of reversal. The opposite violation—placing a higher value on the P-bet but choosing the $-bet—is relatively rarely observed, and we shall call it the counter reversal. One interpretation of anomalies such as PR is that people's preferences are essentially well-behaved, but operate according to different principles than underpin EU (for a review, see Starmer, 2000). However, a radically different interpretation of such evidence is that, when presented with different types of tasks, many people are liable to use different cognitive processes or heuristics with which to construct their preferences (see Tversky and Kahneman, 1986). There is more than one variant of the heuristic explanation for the PR phenomena, but the essence of most of them is that the valuation and choice tasks bring somewhat different cognitive processes into play. The valuation task asks for a monetary response and therefore tends to focus respondents’ attention on the money payoffs, and in particular, evokes a tendency to anchor on the positive payoff and adjust downwards, but insufficiently, so that the valuation response for the higher-anchor $-bet is liable to end up greater than the valuation of the lower-anchor P-bet. By contrast, the choice task encourages relatively more weight to be attached to the probability of getting some positive payoff, which favours the P-bet. The claim is that this difference between the ways the two tasks are processed is liable to lead to the disparity so frequently observed. Might there be some elicitation procedure which is less susceptible (perhaps invulnerable) to such influences? And if so, will the standard PR pattern be attenuated (even eliminated)? To investigate this issue, our basic strategy, described in more detail in the next section, is to move away from the standard presentation of isolated valuation and choice tasks and adopt a format which encourages respondents to consider a variety of lotteries—including some that have the characteristics of $-bets and others with the characteristics of P-bets—as well as a number of different sure amounts, and ask them to produce a single ranking of the full set of alternatives. This allows valuations (within some range) to be inferred for each lottery from its location between two sure amounts. The intuition is that engaging respondents in a task where there is a wide spectrum of probabilities as well as numerous payoffs spread across a broad range, and where there is implicit encouragement to strike balances between the two dimensions, might make them less susceptible to anchoring on any particular component and may thereby greatly attenuate the disparity between choice and valuations. However, even if that turned out to be the case, it would not, by itself, be sufficient to establish that the ranking procedure is immune from distortions or anomalies of its own. The most obvious candidate for a ranking anomaly is that the ordering of some particular subset of lotteries may be systematically affected by changing some of the other lotteries being ranked. An indication of the sort of thing that has been observed to happen in other contexts is reported in Robinson et al. (2001). In that study, respondents were asked to rank a number of descriptions of road accident injuries in order of how bad they were, and then score these on a scale where an index of 100 was assigned to ‘Normal Health’ while ‘Worst Outcome/Death’ was assigned a score of 0. Two sets of nine injury descriptions were compiled, of which Normal Health, Death and three injuries (labeled R, S and X) were common to both sets, while the other four descriptions differed between sets. In Set A, three of these other four were all injuries of a less serious nature, involving no permanent after-effects, whereas in Set B three of the four were permanently disfiguring and/or disabling. These differences in the ‘other’ injuries did not affect the ranking over R, S and X. But there were significant differences in the scores assigned to those injuries: the inclusion of the milder ‘other’ injuries in Set A pushed the scores for R, S and X down relative to those in Set B where the inclusion of several very unpleasant injuries made R—and even more so, S and X—seem less severe. Of course, those scores were not certainty equivalent values of the kind being considered in the present study. But what those results illustrate is the potential for a ranking procedure to induce a form of ‘range-frequency (R-F) effect’Footnote 1 (Parducci and Weddell, 1986), and it is important to check whether something analogous may come into play in the ranking of lotteries. The present study reports the results of two experiments designed to explore whether there might be some ranking procedure that could reduce or even eliminate PR without generating some other subversive anomaly of its own.Footnote 2 The next two sections report the two experiments we conducted. The first showed that there was indeed a strong procedural effect associated with the ranking task, and that this was liable to confound the preference reversal data. The second experiment attempted to both explore and control for that procedural effect. This showed that in cases where the effect was controlled for, the preference reversal phenomenon was greatly attenuated. The final section discusses possible interpretations and implications.",24
34.0,1.0,Journal of Risk and Uncertainty,03 January 2007,https://link.springer.com/article/10.1007/s11166-006-9004-3,Perception of own death risk,February 2007,Henrik Andersson,Petter Lundborg,,Male,Male,Unknown,Male,"Individuals’ perception of risk has been given a lot of attention in academic literature in recent decades (Slovic, 2000). There is plenty of empirical evidence that objective risk measurements, experts’ risk estimates, and lay people's perceptions differ (Sunstein, 2002).Footnote 1 Whereas experts are often better informed and rely on sophisticated tools in order to evaluate hazards, lay people (who have been found to have difficulties judging small probabilities (Kahneman and Tversky, 1979; Kahneman et al., 1982)) are influenced to a larger extent by their own experience of the hazards, how they perceive the risk (dread, controllable, etc.), and media coverage, when forming their risk perceptions (Slovic, 1987). A widely cited study on mortality risk comprehension is Lichtenstein et al. (1978), where it was shown that individuals overassessed small fatality risks and underassessed large fatality risks. The pattern found in Lichtenstein et al., also obtained in several other studies, has come to be regarded as an “established fact” (Morgan et al., 1983; Benjamin and Dougan, 1997; Viscusi et al., 1997; Hakes and Viscusi, 2004; Armantier, 2006). When Benjamin and Dougan (1997) reexamined the data in Lichtenstein et al., and controlled for age cohorts, they could not reject the hypothesis that the risk estimates were unbiased. Benjamin and Dougan (1997) suggested that individuals would be able to more accurately perceive the risk of their own age group, since this is the risk most relevant to them. This hypothesis was supported by the findings in Benjamin et al. (2001), where respondents were asked about mortality risks of the population and of their own age group, especially for larger risks. Armantier (2006) suggested, however, that the results obtained in Benjamin et al. (2001) were due to an anchoring effect and that the pattern in Lichtenstein et al. is a “salient and robust phenomenon” (p. 54). Armantier also found evidence, however, that individuals perceive the risk of their own age group more accurately, as was suggested by Benjamin and Dougan (1997). Most of the previous literature has examined differences in average values between perceived and objective risks for accident groups. Hakes and Viscusi (2004) further contributed to the analysis of mortality risk perception by collecting extensive data on individuals’ mortality risk perception, which enabled them to study how demographic factors influenced perception. They examined how individuals perceive the risk of the population (i.e. the “risk of others”). Our study further contributes to the literature by examining individuals’ perception of their own mortality risk, using individual-level data, which (in line with Hakes and Viscusi) enables us to examine how socio-economic and demographic factors affect mortality risk perception and the corresponding bias. The analysis is done for two mortality risks, overall and road-traffic. Road-traffic risk is assumed to be more voluntary and controllable compared with overall risk.Footnote 2 Data on risk perception originates from a Swedish contingent valuation survey (Persson et al., 2001). The aim of this study is fourfold, to examine if: (i) perceived risks differ from objective risks, (ii) the probability of underestimation varies in terms of demographic characteristics, (iii) there is any correlation between the magnitude of bias and individual characteristics, and (iv) the risk perception formation of own risk follows the pattern found in Lichtenstein et al. (1978), and whether it differs between road-traffic and overall mortality risks. Objective risk in this study is defined as the risk of the respondents’ peers (their own gender and age group). In the following section we present empirical findings from previous research on mortality risk perception, and briefly outline the Bayesian learning model for risk assessment. In Section 2 the data used is described and in Section 3 we discuss the empirical models. The results are shown in Section 4. We find that road mortality risk follows the same pattern found in Lichtenstein et al. (1978), that men are more likely to underestimate their own risk, and that there is a positive correlation between the perception of own health and a lower perception of own risk. Finally, Section 5 offers a summary and a discussion of the results, and some concluding remarks about the policy relevance of the findings in the study.",45
34.0,1.0,Journal of Risk and Uncertainty,06 January 2007,https://link.springer.com/article/10.1007/s11166-006-9000-7,Wage compensation for job-related illness: Evidence from a matched employer and employee survey in the UK,February 2007,Xiangdong Wei,,,Unknown,Unknown,Unknown,Unknown,,
34.0,2.0,Journal of Risk and Uncertainty,06 March 2007,https://link.springer.com/article/10.1007/s11166-007-9005-x,Intertemporal choice under timing risk: An experimental approach,April 2007,Selçuk Onay,Ayse Öncüler,,Male,Unknown,Unknown,Male,"This paper focuses on preferences over timing lotteries (x;t
1, p
1;...;t

n
, p

n
), where x denotes a sure outcome (e.g. a monetary payoff, a consumption good, etc.), t

i
’s denote the possible delays (in any unit of time) and p

i
’s denote the respective probabilities of these delays, where \( {\sum\limits_i {p_{i} = 1} } \).Footnote 3 A timing lottery gives a sure outcome in exactly one of the possible time periods. A stream of outcomes distributed over time is generally evaluated by a discounted utility model in economics. Typically, this model combines a discount function that captures time preferences and a utility function that represents preferences over outcomes. We will employ a similar discounted utility framework to evaluate timing lotteries. We assume that individuals’ preferences over consumption streams (c
0, c
1, ..., c

T
) can be represented by the following general discounted utility model, with the total utility of an outcome profile represented in an additively separable manner across time periods: 
 where u(c

t
) is a real-valued utility function, the direct utility derived from consuming c

t
 in period t, and D(t) is the discount function, the weight given to the consumption in period t. Under the assumption of positive time preferences (i.e. preferring to consume earlier than later), D(t) is a positive and declining function of delay with D(0) = 1. In the classic discounted utility model (Koopmans 1960; Samuelson 1937), the discount function is modeled as an exponential function, \( D{\left( t \right)} = {\left[ {1 \mathord{\left/ {\vphantom {1 {{\left( {1 + r} \right)}}}} \right. \kern-\nulldelimiterspace} {{\left( {1 + r} \right)}}} \right]}^{t} \), where the discount rate r is constant. The constant discounting assumption has been challenged by empirical studies which propose other discount functions with more descriptive power, among which is the hyperbolic discount function (Loewenstein and Prelec 1992; Mazur 1987). In hyperbolic discounting, the discount rate is decreasing over time. Two of the most widely-cited versions of hyperbolic discounting are the one-parameter function proposed by Mazur (1987), formulated as \( D{\left( t \right)} = 1 \mathord{\left/ {\vphantom {1 {{\left( {1 + kt} \right)}}}} \right. \kern-\nulldelimiterspace} {{\left( {1 + kt} \right)}} \) where k is a constant and the two-parameter hyperbolic discounting function of Loewenstein and Prelec (1992), given by \( D{\left( t \right)} = {\left[ {1 \mathord{\left/ {\vphantom {1 {{\left( {1 + \alpha t} \right)}}}} \right. \kern-\nulldelimiterspace} {{\left( {1 + \alpha t} \right)}}} \right]}^{{\raise0.7ex\hbox{$\beta $} \!\mathord{\left/ {\vphantom {\beta \alpha }}\right.\kern-\nulldelimiterspace} \!\lower0.7ex\hbox{$\alpha $}}} \) where α and β are hyperbola coefficients. Note that all the discount functions proposed in the intertemporal choice literature so far, such as exponential, hyperbolic and quasi-hyperbolic functions, are strictly convex. In the following analysis, we will use a convex discount function, without any specific assumptions on its functional form.Footnote 4
 Given this brief background, we return to our question, that is, how an individual whose preferences over consumption streams can be represented by Eq. 1 would choose between the timing lottery L:(x;t
1, p;t
2, (1−p)) and the sure timing alternative S:(x;s) which offers the same payoff x at the sure delay s where t
1 < s < t
2. Let us assume that the baseline consumption is c in all time periods t
1, s and t
2. The timing lottery offers the consumption stream (c + x,c,c) with probability p and (c,c,c + x) with probability (1−p); and the sure consumption profile is (c,c + x,c) with certainty. The discounted expected utility of the timing lottery would be: 
 Likewise, the discounted utility of the sure consumption stream, denoted by V(S), would be: 
 Subtracting Eq. 3 from Eq. 2 would give us the additional utility that the individual would derive from having chosen the risky consumption plan over the sure one: 
 Which alternative would the individual prefer? Assuming u(.) is an increasing function of x, the individual would be indifferent between the risky consumption plan and the certain one if and only if: 
 Equation 4 also implies that in the gain domain, i.e. when x > 0, the timing lottery is preferred to the sure alternative if and only if: 
 since \( u{\left( {c + x} \right)} - u{\left( c \right)} > 0 \), for all x > 0. Likewise, in the loss domain, when x < 0, the timing lottery is preferred to the sure alternative if and only if: 
 In this formulation, what determines the preference between a timing lottery and a sure timing alternative is the value that the discount function takes at given delays and the probabilities attached to those delays. Since both options yield the same outcome x, the shape of the utility function does not play any role in such decisions. In order to see which alternative would be preferred, all that is needed is to check under which conditions the inequality \( pD{\left( {t_{1} } \right)} + {\left( {1 - p} \right)}D{\left( {t_{2} } \right)} > D{\left( s \right)} \) is satisfied. Since the sure delay is assumed to be between the two possible delays of the timing lottery, we can always write s as a linear combination of t
1 and t
2. In other words, we can always find an α, 0 < α < 1, such that \( s = \alpha t_{1} + {\left( {1 - \alpha } \right)}t_{2} \). Then, we can rewrite Inequality (7) as: 
 Inequality (8) would be satisfied when D(.) is convex and if p ≥ α.Footnote 5 We summarize the predictions of the DEU model and formulate our first experimental hypothesis as follows: A decision maker, whose intertemporal utility function can be written as in Eq. 1, with a convex discount function, would always prefer the timing lottery L: (x;t
1
,p;t
2,(1−p)) to the sure timing option S: (x,s), where \( s = \alpha {\text{ }}t_{1} + {\left( {1 - \alpha } \right)}t_{2} \) in the gain domain (i.e., when x > 0) and would always prefer the sure timing option to the timing lottery in the loss domain (i.e., when x < 0), for all p ≥ α. A special case is when p = α, that is, when the sure timing is the expected time of the timing lottery. In this case, we would expect the decision maker to prefer the timing lottery in the gain domain and the sure timing in the loss domain. For instance, a timing lottery that gives 1,000€, either in 1 month with 50% probability or in 11 months with 50% probability, should be preferred to the sure timing option that gives 1,000€ for sure in 6 months simply because of the time preferences implied by convex discounting. Intuitively, this is due to the fact that delaying the reception of the payoff from 1 to 6 months causes a bigger decrease in the discounted utility of the prize than delaying it from 6 to 11 months. As the discount rate of the decision maker increases, her preference for the timing lottery would become stronger, since the discount function would become even more convex. If we relax the assumption that the individual’s discount function is convex, the DEU model would still predict that the decision maker would switch her preference between the certain timing option and the timing lottery when the sign of the payoff is changed (see Eqs. 6 and 7 above). Hypothesis 2 below summarizes this prediction: A discounted expected utility maximizing decision maker would always reverse her preference between a timing lottery L: (x;t
1,p;t
2,(1−p)) and a sure timing alternative S: (x,s) whenever the sign of the payoff x is changed. Table 1 shows a summary of the predicted preferences between the two alternatives under different conditions. Another prediction of the DEU model is that if an individual prefers the timing lottery to its expected timing alternative for a given probability distribution, then she should have the same preference ordering for any probability distribution. This prediction will constitute our third hypothesis: A discounted expected utility maximizer’s preference ordering between a timing lottery L: (x;t
1
,p;t
2,(1−p)) and its expected timing alternative S: \( {\left( {x;pt_{1} + {\left( {1 - p} \right)}t_{2} } \right)} \) should remain the same for all 0 ≤ p ≤ 1. Note that none of these predictions depends on the shape of the individual utility function, i.e. the results would hold for any increasing utility function. We will test the descriptive validity of these predictions in the following sections.
",27
34.0,2.0,Journal of Risk and Uncertainty,07 March 2007,https://link.springer.com/article/10.1007/s11166-007-9006-9,Scope insensitivity in health risk reduction studies: A comparison of choice experiments and the contingent valuation method for valuing safer food,April 2007,Isabell Goldberg,Jutta Roosen,,Female,Female,Unknown,Female,"When valuing WTP for health risk reductions, the theoretical background has to be clarified to link economic theory with survey design, scope tests, and survey results. In the following we aim at linking our survey approach to economic theory, give an insight into the debate about scope insensitivity, and highlight some methodological aspects of CVM and CEs. In this section we derive hypotheses of scope sensitivity to be tested using data from the CVM and CEs. The hypotheses are often found in the literature and based on the expected utility maximization paradigm. We adapt the expected utility model of Cook and Graham (1977) already employed in the context of food safety evaluations by Hayes et al. (1995). Assume state-dependent preference is represented by a von-Neumann-Morgenstern utility function U(W,H), where W denotes the individual’s wealth and H his health state, where H = 0 if the individual is sick and H = 1 if the individual is healthy. Let U

S
(W) = U(W,0) and U

H
(W) = U(W,1), and assume that U

S
(W) < U

H
(W) for all W and that \( U^{\prime }_{i} > 0\) and\(U^{{\prime \prime }}_{i} \leqslant 0\) for i = S, H. Given a baseline risk of food safety disease, π, the expected utility results as 
 If the individual is offered the opportunity to reduce the health risk π by an amount r, the WTP for this health risk reduction R is defined by the reduction of wealth that leaves the individual indifferent between the lottery before and after the health risk reduction, such that 
 Taking the total differential of Eq. 2 with respect to r and R (Jones-Lee 1974; Weinstein et al. 1980) yields the WTP in response to a change in risk: 
 WTP for a health risk reduction is always positive and increasing in the risk reduction. We formulate this result in a first proposition and refer to this hypothesis as a hypothesis of weak scope sensitivity. 
 
Willingness to pay for a reduction in health risk increases in the amount of risk reduction (weak scope sensitivity). To describe the curvature of WTP with respect to the risk reduction r, we differentiate Eq. 3 and obtain 
 The sign of Eq. 4 is indeterminate. Using Eq. 3 to simplify we obtain 
 Given a concave utility function with \(U^{{\prime \prime }}_{i} \leqslant 0\) for i = S, H and given that \({\left( {U^{\prime }_{H} - U^{\prime }_{S} } \right)}\) cannot be signed a priori, the sign of Eq. 5 is not known. Jones-Lee (1974) made the assumption that \( U^{\prime }_{H} \geqslant U^{\prime }_{S} \) and could hence derive a WTP concave in risk reduction. For the more general result that corresponds to ours see Weinstein et al. (1980). Though expected utility theory does not allow predicting concavity or convexity of WTP in health risk reductions, it is often assumed in the literature that for sufficiently small changes in health risks, it should be possible to approximate WTP for health risk reductions by a linear function (Hammitt and Graham 1999). This result holds for any function, and the extent to which the linear approximation is satisfactory depends on the one hand on the size of risk change and on the other hand on the wealth effect in the utility function. We summarize this proposition and refer to it as one of strong scope sensitivity: 
Willingness to pay is almost proportional in health risk reductions for small changes in risk (strong scope sensitivity). While scope sensitive WTP estimates should adhere to the two propositions, they often do not. In the following, we discuss scope insensitivity and embedding while we use either the one or the other term depending on which one is used in the original article. Scope insensitivity occurs when respondents do not sensitively react to different magnitudes of health risk reductions but state an amount more as a general approval in favor of “voting” for a risk reduction. It does not imply that these respondents would behave in a similar way in a real market setting and are willing to pay the same if they are really ask to pay the stated amount. The reasons for scope insensitivity and embedding are multifarious. Since the WTP values and the quality of the results are contingent upon the scenario, the representation of the stimulus and the response mode used in the task influence the valuation (Fischhoff et al. 1993). Loomis et al. (1993), for example, conclude that embedding is not always pervasive in CV surveys if the context is clearly communicated. In contrast, Heberlein et al. (2005) argue that economists regard scope failures as evidence for bad questionnaire design. They argue that respondents should not depend too much on information that is provided to them in the questionnaire. The values should be rooted in respondents’ attitudes, beliefs, and experiences. Otherwise the values may be unstable and sensitive to the information given to the subjects. The authors cast doubt on the assumption that scope issues can be resolved by providing more information to the respondents. These conclusions are drawn on the results of a CV study where a scope test was conducted for four environmental goods. Heberlein et al. (2005) found that possible lack of scope sensitivity in valuations of goods can be explained by differences in behavioral, affective and cognitive scope, thus questioning the requirement of economic scope sensitivity for consistency of preferences. Also, moral satisfaction is an often cited phenomenon occurring in CV surveys resulting in embedding. Kahneman and Knetsch (1992) argue not to misinterpret individuals’ WTP to acquire moral satisfaction for a measure of the economic value of a public good. The degree of embedding that might be traced back to moral satisfaction depends possibly on the good/program being evaluated. Scope insensitivity might also be the result of a lack of interest and of the fact that people do not care about the program’s outcome they are being asked to value (Olsen et al. 2004). Also the valuation of a (food) safety issue might be different from that of an environmental issue. But there are also overlaps or interdependencies possible. For instance, Hamilton (1985) mentioned that when valuing concern about toxic waste, the contamination that affects one’s own household turns into a safety rather than an environmental issue for the respondent. When embedding or scope insensitivity occurs, one question is if the degree of embedding depends on public goods having use value or non-use value. Kahneman and Knetsch (1992) stated that the magnitude of embedding does not depend on use or non-use value of public goods. They suggest that it might be more important to consider if private purchase is conceivable to the public good or not. However, embedding is not just a problem of non-market valuation. Randall and Hoehn (1996) pointed out that theory predicts that embedding may be observed with market and non-market valuation and is a routine economic phenomenon. Explanations for the part-whole bias or embedding, respectively, have revolved around partial budgeting and have been argued to apply to private, market-valued goods as well. In a study about a private consumption good, namely vouchers for parts of a restaurant meal or a voucher for a complete meal, clear evidence of part-whole bias was found. Embedding might therefore not be attributable simply to the CV method. It suggests that it may be traced back to a property of individuals’ preferences that is not allowed for in conventional consumer theory (Bateman et al. 1997). Also Hayes et al. (1995) conduct an auction experiment to measure WTP for safer food. They found bids to be largely nonproportional to risk changes and conclude that the measured WTP values should be seen as a WTP for safe food in general and not for a specific health risk reduction, although part of the scope insensitivity may also be explained by Bayesian updating of probability estimates. If scope insensitivity or embedding is identified, it does not invalidate the WTP measures or the CVM in general. However, it is possible that the CVM exacerbates this effect (Randall and Hoehn 1996). Even if the scope test is closely linked to economic theory, researchers call the usefulness of scope tests into question. If a study passes or fails a scope test is neither an indicator for validating nor invalidating a CV study (Heberlein et al. 2005). Nonetheless, a scope test is an appropriate instrument to investigate whether or not respondents sufficiently distinguish between different levels of health risk reductions. Both methods, the CVM and CEs, rely on stated preferences. While economists generally prefer revealed preference methods to stated preference methods, stated preference methods have some advantages. A salient feature is that preferences for non-existing attributes can be elicited. In cases such as ours, product safety characteristics that cannot be readily observed by consumers in the marketplace can be explicitly valued by survey methods. However, there is a risk that the respondents could misinterpret or ignore an attribute if the attribute level does not appear realistic. Furthermore, respondents could use the questionnaire as an opinion statement for their own benefit or ignore situational constraints. Nevertheless, trade-off relationships among major attributes are assumed to be common to both revealed and stated preferences, which validates the use of stated preference methods (Morikawa et al. 2002). The CVM is widely used to determine willingness to pay for public and non-market goods. Usually a CV survey contains an introduction section to set the general context where respondents receive information about the good being valued, the institutional circumstances in which the good is made available, and the payment vehicle. The hypothetical market is constructed and the respondent becomes familiar with the good. Using different elicitation formats used in CV studies (e.g. payment cards, bidding games, or dichotomous choice questions), respondents’ WTP for the good in question is valued. The additional information that is gained from socio-demographic questions (e.g. age, income, educational background) and from questions eliciting respondents’ preferences and attitudes helps to estimate a valuation function and is used in regression equations (Carson 1999; Mitchell and Carson 1989). In CEs, respondents are asked to make repeated choices between different consumption bundles which are described by different attributes. Typically, one of these attributes is price. The respondents’ utility depends on attribute levels of the choices made from these sets. This procedure enables the researcher to obtain different pieces of information. The attributes that significantly influence choice are determined, ranking of these attributes can be constructed, and the marginal WTP for an increase or decrease in the significant attributes can be inferred (Hanley et al. 1998). CEs are, as the CVM too, sensitive to information presented to the respondent. In the survey, we balanced that drawback by presenting the same information about health risks to the respondents in both the CE and the CVM part. Other possible sources for misspecifications are, for instance, ignoring interactions between attributes or excluded attributes which might be important for the CE. On the one hand, the experimental design is probably too simple in these cases. On the other hand, managers and policy makers may be more interested in the marginal value when changing certain attributes and including all possible attributes might not be of interest. CEs enable the policy researcher to determine and value the individual characteristics of a policy (Hanley et al. 1998). Just some selected attributes can be taken into account and the experimental design significantly determines the reliability of the obtained data.",56
34.0,2.0,Journal of Risk and Uncertainty,08 March 2007,https://link.springer.com/article/10.1007/s11166-007-9008-7,Dual process theories: A key for understanding the diversification bias?,April 2007,Christoph Kogler,Anton Kühberger,,Male,Male,Unknown,Male,"The following experiment is based on the card task of Rubinstein (2002) and on findings of Kogler and Kühberger (2006), where we showed that people have a strong tendency to emit the probability matching response in this task. Specifically, we found that only about 5% of participants predicted according to probability theory (i.e., they uniformly predicted the most frequent color). This pattern, which is termed probability maximizing, was clearly less common than the probability matching pattern, which was found in about 35% of participants (the rest being patterns resembling probability matching). In different versions of the task we investigated the robustness of probability matching and found that probability matching was robust across stimuli (symbols and colors) and incentives (real and hypothetical payoffs). In sum, participants withstood our aim to raise the percentage of probability maximizers at least up to the level of probability matchers (Kogler 2006). The following experiment is another try at accomplishing this. Ninety-seven undergraduates (78 females and 19 males; mean age = 22.6 years) from six introductory statistics courses at the University of Salzburg participated voluntarily. Participants had basic knowledge about probabilities and statistical independence. The experiment was designed to support the corrective functions of System 2. Kahneman (2003) suggests three procedures to strengthen System 2: (1) provide strong cues to the relevant rules; (2) increase the vigilance of the monitoring activities; and (3) extensive training in statistical reasoning. We implemented (1) and (2), keeping (3) constant. Participants were instructed on the card-task. They learned that five cards were to be randomly drawn from a deck of 100 colored cards and that each of the five cards had to be put into a separate envelope. The deck of cards consisted of 36 green, 25 blue, 22 yellow, and 17 red cards. Thus, each of the envelopes labelled 1 to 5 was to contain either a green, a blue, a yellow, or a red card. The participants’ task was to predict the color of the card in each particular envelope. Two different groups were formed and instructed according to Kahneman’s (2003) advice to strengthen System 2. First, we provided cues to the relevant rules. This was done by referring to the task as a “lottery task” (group A), or as a “statistical test” (group B). In addition, participants in Group B were informed that the task was a test to find out about their level of statistical competence, and participants were encouraged to show the best of their ability in this test, presumably increasing the vigilance of the monitoring activities. Furthermore we advised them to take their time (low time pressure), and to carefully reconsider a second time their predictions (hint at correction). Generally speaking, participants in both groups received the same task, but the instruction for Group B represents an attempt to support the corrective functions of System 2. Thus, in accordance with dual process theories, less probability matching is predicted for Group B compared to Group A.",20
34.0,2.0,Journal of Risk and Uncertainty,16 March 2007,https://link.springer.com/article/10.1007/s11166-007-9007-8,Paying for permanence: Public preferences for contaminated site cleanup,April 2007,Anna Alberini,Stefania Tonin,Aline Chiabai,Female,Female,Female,Female,"Hazardous waste site programs purport to eliminate or reduce threats to public health and to reduce mortality risks. The VSL—the marginal rate of substitution between income and risk—is a summary measure of the WTP to pay to reduce these risks, and is generally deemed as the appropriate construct for ex ante policy analyses, when the identities of the people whose lives are saved by the policy are not known yet. The mortality benefits of a policy that saves L lives are equal to (VSL×L). In the U.S., the Superfund statute spells out cleanup criteria to be adopted at the most egregious contaminated sites in the nation, which are placed on the so-called National Priorities List (NPL) and may qualify for publicly financed cleanup. The 1986 Superfund Amendments and Reauthorization Act (SARA) specifically directed EPA managers to select target risk reductions to protect human health and meet any “legally applicable” or “relevant and appropriate” standards (e.g., maximum contaminant limits in groundwater), regardless of cost (Revesz and Stewart 1995). EPA guidelines have interpreted SARA to warrant cleanup where excess lifetime cancer risks to an individual based on reasonable maximum exposure are greater than 10−4, and to give discretion to project managers where risks are between 10−4 and 10−6 (U.S. Environmental Protection Agency 1991).Footnote 2 SARA also contains an explicit preference for permanent remediation, as opposed to simple containment to prevent migration of pollutant and to limit exposure. Permanent remedies are generally more expensive, but Gupta et al. (1996) find that the EPA has indeed heeded this preference for permanent cleanups in its remediation decisions. Recent state programs, however, seem to be reversing this preference for permanence. State voluntary cleanup programs, for example, offer a variety of incentives in exchange for site cleanup, including simplified or variable cleanup standards linked to land use, engineering and institutional controls, in place of (more stringent) cleanups (Meyer 2000). The U.S. General Accounting Office (1997) surveyed 17 state voluntary cleanup programs and found that over 50% of the cleanups entailed non-permanent remedies and/or adopted industrial land use standards. Several European countries face similar dilemmas. In Italy, the first piece of legislation addressing hazardous waste sites—the Waste Act—was passed in 1997 (Gazzetta Ufficiale 1997). The statute requires cleanup if the concentrations of certain pollutants exceed the maximum contaminants limits set by the law for soil and water. A subsequent law (Legislative Decree 152/2006) required that risk assessments be conducted at sites where pollutants exceed the maximum concentration limit, and that remedial plans be based on such risk assessments. Remediation is recommended when excess lifetime cancer risk exceeds 10−5 (Gazzetta Ufficiale 2006). The Waste Act provides for only limited funding for cleanup,Footnote 3 places the burden of remediating orphan sites on the municipalities, and contains an explicit preference for permanent remediation and for on-site treatment of contaminated media. Recent analyses conducted by the Italian Environmental Protection Agency and environmental organizations point out that the majority of actions at NPL and non-NPL contaminated sites have, thus far, been short-term and impermanent (Agenzia per la Protezione dell’ Ambiente e per i Servizi Tecnici (APAT) 2004; Legambiente 2005). Conjoint choice experiments are a survey-based technique frequently used to place a value on a good or estimate the benefits of a public program (see Hanley et al. 2001). The approach asks individuals what they would do under hypothetical circumstances, rather than observing actual behaviors. An advantage of this approach is that it is flexible and can span goods/programs, levels of risk reductions and other aspects of environmental quality that do not currently exist. In a conjoint choice survey, a good or public program is described in a stylized fashion by a vector of attributes. Respondents are shown K ≥ 2 alternative variants of this good or program obtained by taking combinations of the possible values of the attributes, and are asked to choose the most preferred. The alternatives differ from one another in the levels taken by two or more of the attributes. If a “do nothing” or status quo option is included in the choice set, choice experiments can be used to estimate the WTP for each alternative. We asked respondents to consider hypothetical public programs that would clean up sites where the responsible parties are no longer in existence or do not have the means to pay for remediation. Respondents were told that the government would be in charge of the remediation programs, and that the programs would be guaranteed to be effective. The specifics of the programs are described using five attributes: (1) the risk reduction per year, expressed as the number of lives saved per million people, (2) the size of the population living in the areas with the contaminated sites targeted by the program, (3) the delay until the risk reduction begins, (4) the number of years over which the risk reduction would be observed, and (5) the cost of the program to the respondent, which would be incurred as an immediate, and one-time, tax. Clearly, attribute (3) gets at the heart of the latency issue, and attribute (4) captures the degree of permanence of the risk reductions. The respondents were shown a total of four pairs of hypothetical programs constructed in this fashion. They were first asked to indicate which of the two programs—A or B—they prefer, and then indicate which they would choose out of program A, program B, or neither. This results in a total of eight conjoint choice questions where the size of the choice set is 2 (when choosing between A and B) or 3 (when choosing between A, B, and the status quo). An example of the conjoint choice questions is reported in the Appendix, and a summary of attributes and levels is reported in Table 1. 
 That risk reductions will be realized no earlier than two years from now (attribute (2) or “Delay” in Table 1) is consistent with the notion that the pollutants at most contaminated sites are carcinogens or cause long-term health effects, and with the fact that it takes some time to complete even the most efficient government remediation program.Footnote 4 To facilitate the respondents’ task, we held the delay the same for all hypothetical pairs shown to a respondent, and employed a split-sample design where half the respondents were given D = 2 and the other half D = 10. It is also reasonable to assume that no remediation program can reduce risks forever: hence, we set the duration of the risk reductions at 20, 30 or 45 years. These may be interpreted as time to failure of the remedies. The delay and duration attributes provide variation in the timing of the mortality risk reductions across and within respondents, which we exploit for the purpose of estimating the rate at which people discount future risks. We chose a one-time tax to be incurred immediately for two reasons. First, since risk reductions are incurred in the future, this allows U.S. to estimate the rate at which people discount risks. Second, in focus groups and during the survey development work, people voiced strong opinions against new taxes and against committing to pay annual taxes over a long period of time. We certainly did not want people to dismiss our scenarios outright, and a one-time tax was the most appealing option. The one-time tax amounts ranged between €50 and €950.Footnote 5 We chose these bid amounts because they cover a broad range of possible VSL values: Using the model described by equations (1) and (2) below, and assuming discount rates between 0% and 10%, our bid amounts correspond to VSL figures ranging between €37,000 and €11 million.Footnote 6
 We also vary the size of the population living in the areas with the contaminated sites that would be addressed by the program, and hence potentially affected by the risk reductions. We chose hypothetical populations of 0.5, 1 and 2 million because these levels were judged credible by focus group participants, especially when compared with the total population living in areas with NPL sites (7 million; see Section 3), and because we felt that respondents could easily form a sense of the size of these populations by comparing them with those of the cities they live in. We created a total of 32 sets with four pairs of programs each. We began this task by creating all of the possible alternative programs (i.e., all possible combinations of the levels of the attributes). We then formed all of the possible pairs, but excluded pairs that contained dominated alternatives.Footnote 7 The 32 sets we used for the survey were obtained by selecting four pairs at random (without replacement) out of this universe of non-dominated pairs. Respondents were randomly assigned to one of the 32 sets. We assume that in the conjoint choice questions respondents choose the alternative with the highest indirect utility, and that the indirect utility depends on the discounted stream of risk reductions and on residual income. Formally, 
 where \( \overline{V} _{{ij}} \) denotes the deterministic component of the indirect utility function, DR is the discounted flow of risk reductions delivered by program j, y is income and C is the cost of the program to the respondent. Coefficients α and β denote the marginal utility of the discounted flow of risk reductions and the marginal utility of income, respectively. We assume constant exponential discounting and define DR as 
 where ΔR is the annual risk reduction (which is varied to the respondents but constant over the years), δ is the discount rate, A is the number of years one must wait before the risk reductions are observed, and T is the number of years over which lives are saved. Expression (2) shows the effect of a delay in the beginning of the risk reduction (captured by the term \( e^{{ - \delta A}} \)) and the effect of more or less permanent risk reductions (captured by term in brackets). On appending an error term ɛ

ij
, Eq. 1 becomes a random utility model, which in turn results in a conditional logit model if we further assume that the error terms ɛ

ij
 are independent across alternatives within the same respondent and follow the standard type I extreme value distribution. The probability that option k is selected out of K alternatives when answering a choice question is thus 
 and the log likelihood function of our sample is 
 where y

imk
 is a binary indicator that takes on a value of 1 if the respondent i selects alternative k in choice question m, and 0 otherwise, K

m
 is the number of the alternatives the respondent is faced with in choice question m (so K

m
 = 2 for m = 1, 3, 5, and K

m
 = 3 for m = 2, 6, and 8), and M is 8, the total number of choice questions asked of the respondent. Equation 4 thus describes a non-linear conditional logit. It assumes that the choice responses are independent within and across respondents. The maximum likelihood estimates of the coefficients can be used to compute the Willingness to Pay (WTP) for any given program: 
 The VSL, i.e., the willingness to pay for a marginal risk reduction to be incurred in the current year, is equal to \( {\left( {{\widehat{\alpha }} \mathord{\left/ {\vphantom {{\widehat{\alpha }} {\widehat{\beta }}}} \right. \kern-\nulldelimiterspace} {\widehat{\beta }}} \right)} \). Clearly, the model described by equations (1) and (2) assumes that the VSL is constant with respect to the size of the risk reduction and the size of the population that would benefit from the cleanup. In other words, according to this model people look at individual risks. In this paper, we wish to test if the VSL does indeed vary with the number of beneficiaries of the program. To do so, we amend Eq. 1 to obtain: 
 where DR0.5 = DR if the size of the population affected by the program is 0.5 million and 0 otherwise, DR1 = DR if the size of the population affected by the program is 1 million and 0 otherwise, and DR2 = DR if the size of the population affected by the program is 2 million and 0 otherwise. We then test the null hypothesis that α
1 = α
2 = α
3. Failure to reject the null implies that Eq. 6 is simplified to Eq. 1, i.e., the marginal utility of a risk reduction is not affected by the size of the population of beneficiaries of the program, N. If the above null is rejected, we further wish to test the null hypothesis that α
2 = 2α
1 and α
3 = 2α
2. This null hypothesis implies that what enters in the utility function is the discounted number of lives saved, rather than discounted individual risk. The indirect utility function would thus be 
 where L is discounted lives saved: 
 Equations 7 and 8 mean that the VSL is strictly proportional to N, the size of the population living in the areas targeted by the hypothetical program. Equation 7 is similar to the social net benefit of cleanup (expected number of cancers avoided, minus cleanup cost) posited, and empirically rejected, by Viscusi and Hamilton (1999) as the objective function for the U.S. EPA. Viscusi and Hamilton concluded that the agency was consistent neither with its own mandate requiring exclusive focus on maximum individual risks, nor with social efficiency. We are also interested in testing whether the marginal utility of risk reductions and the marginal utility of income depend on individual characteristics. To see if this is the case, we amend Eq. 1 (or Eq. 6) to allow for heterogeneity among the respondents.Footnote 8 Specifically, we posit that the marginal utility of risk reduction for respondent i is \( \alpha _{i} = \alpha _{1} + x_{i} \alpha _{2} \) and that the marginal utility of income is \( \beta _{i} = \beta _{1} + \beta _{2} P_{i} \), where x

i
 is a vector of individual characteristics such as age, gender, education, own health, familiarity with contaminated sites and remediation, acceptance of government policies addressing hazardous waste sites, etc., and P is a low-income dummy. In other words, we form interaction terms between the arguments of Eq. 2—DR and residual income—and x

i
 and P, respectively, and add these interactions in the right-hand side of the indirect utility function: 
 Finally, it is possible to replace δ with a function of individual characteristics z

i
 of the respondent, such as age, whether he or she is married and has young children, etc.: \( \delta _{i} = z_{i} \pi \).",46
34.0,3.0,Journal of Risk and Uncertainty,04 May 2007,https://link.springer.com/article/10.1007/s11166-007-9011-z,Eliciting decision weights by adapting de Finetti’s betting-odds method to prospect theory,June 2007,Enrico Diecidue,Peter P. Wakker,Marcel Zeelenberg,Male,Male,Male,Male,"This section presents rank-dependent utility and the new version of prospect theory in an elementary manner so as to highlight the central role of rank dependence. Tversky and Kahneman’s (1992) explanation is more complex. Given the importance of prospect theory, a simple explanation, accessible to a wide audience, is desirable. The three uncertain events in our experiment (U, D, and R) are related to the performance of the Dow Jones industrial average and the Nikkei 225. U denotes the “Up” event that both stock indexes will go up tomorrow, D the “Down” event that both will go down, and R the “Rest” event that either one will go up and the other one will go down or at least one will remain constant. A prospect (u,d,r) yields $u if U obtains, $d if D obtains, and $r if R obtains. The outcomes u,d,r, are always positive in this paper. In applications, outcomes usually depend on stock-index changes in more complex manners. For the sake of exposition, and to be consistent with the experiment described later, we confine our attention to the three-outcome prospects as just described. Generalizations to more outcomes are straightforward. Outcomes x are sometimes equated with constant (riskless) prospects (x,x,x). 
Subjective expected utility holds if there exists a utility function v and subjective probabilities πU, πD, and πR that are nonnegative and sum to 1, such that a prospect (u,d,r) is evaluated by \( \pi _{{\text{U}}} {\text{v}}{\left( u \right)} + \pi _{{\text{D}}} {\text{v}}{\left( d \right)} + \pi _{{\text{R}}} {\text{v}}{\left( r \right)} \). Prospect theory generalizes subjective expected utility by allowing the πs to depend not only on the subjective beliefs about the occurrence of the event, but also on the “rank” of the events. Formally, the rank of an event is defined through the event that is ranked better in the sense of yielding better outcomes. The term rank dependence refers to this dependence. We use the term decision weight instead of subjective probability to reflect this dependence. To what extent decision-based quantities such as subjective probabilities and decision weights reflect beliefs or other factors has been a topic of many debates and speculations (Fox and Tversky 1998; Karni 1996; Nau 1995). At any rate, decision weights are relevant to decisions, and are the focus of this paper. To illustrate the above evaluation, consider the prospect (5,9,7). Event D yields the best outcome and has the best rank. Event R has the middle rank, and event U has the worst. The prospect theory value of the prospect is
 where the superscript w reflects the worst rank where all other events are better, b the best one where no other event is better, and m the middle one where in this case event D yields a better outcome. The middle decision weight can depend on which of the other events is best, indicated by the superscript D in this case. In this manner, there are four decision weights for event U, \( \pi ^{b}_{U} ,\pi ^{w}_{U} ,\pi ^{{m,D}}_{U} ,{\text{and }}\pi ^{{m,R}}_{U} \), and, similarly, there are four decision weights for the events D and R. The general formula for prospect theory is
 where superscripts are to be added to the π’s according to the ranks of events, described in Table 1. For events that yield the same outcomes, such as events D and R that both yield outcome 1 in prospect (0,1,1), the ranking can be chosen arbitrarily. Observation 1.1 below will ensure that each possible ranking leads to the same evaluation; we do not elaborate on this point. An example of a prospect-theory evaluation is
 Schmeidler (1989) and Tversky and Kahneman (1992) stated their theories in terms of a weighting function. This function assigns to each event E the decision weight \( \pi ^{{\text{b}}}_{{\text{E}}} \) (when E has the best rank). Our presentation in terms of decision weights is equivalent. Decision weights for a single prospect should sum to 1. All rows in Table 1 sum to 1. □ Because of this observation, the new version of prospect theory avoids the violations of stochastic dominance that hampered the developments of original prospect theory. Decision weights for middle ranks only show up for prospects with three or more outcomes. Consequently, such prospects are needed for direct elicitations of such decision weights, and for direct tests of such decision weights. Earlier elicitations of middle decision weights were indirect, deriving them from nonadditive measures elicited from two-outcome prospects (Abdellaoui 2000; Bleichrodt and Pinto 2000; Fox and Tversky 1998; Gonzalez and Wu 1999; Tversky and Kahneman 1992). We will follow the assumption of linear utility underlying de Finetti’s betting-odds system and discussed elsewhere, i.e., we set v(x) = x.",13
34.0,3.0,Journal of Risk and Uncertainty,18 May 2007,https://link.springer.com/article/10.1007/s11166-007-9012-y,The sensitivity of subjective probability to time and elicitation method,June 2007,Graham Loomes,Judith Mehta,,Male,Female,Unknown,Mix,,
34.0,3.0,Journal of Risk and Uncertainty,25 April 2007,https://link.springer.com/article/10.1007/s11166-007-9010-0,"Smoking, information sources, and risk perceptions—New results on Swedish data",June 2007,Petter Lundborg,,,Male,Unknown,Unknown,Male,"Whereas mandatory on-product warnings on cigarette packages were introduced in the US in 1965, it was not until 1977 that similar regulations were introduced in Sweden. Each cigarette package now had to include one out of a series of 16 warnings. Typical warnings were: “Smokers are more often sick than non-smokers,” “Smokers face an increased risk of vascular diseases and some diseases in the blood-vessels,” “Smoking when pregnant may harm your baby,” and “Smoking damages the lungs! It starts with coughing and may end with lung-cancer or another lung disease.” The warnings were later modified on four occasions during the period 1977–1994. In 1979, warnings such as “Lung cancer causes more deaths than traffic. Most cases of lung cancer are caused by smoking,” “What cigarettes are most dangerous? Those with the highest levels of carbon monoxide, tar, and nicotine. Compare the declaration of contents of different brands” appeared. 1982 saw the introduction of warnings such as “9 out of 10 patients with throat cancer are smokers.” In 1987, the number of different warnings in place was reduced to 13, since three were not accepted by the government. Some new messages introduced were: “In 1983, 779 persons died in traffic and at least 8,000 died because of smoking,” and “Smoking increases the risk of inflammation in the gums. This may lead to periodontitis.” The overwhelming majority of warning texts during the period 1977–1994 concerned smoking-related morbidity and mortality to the smoker him/herself. To a lesser extent the texts concerned the risks of smoking during pregnancy and passive smoking. Interestingly, none of the texts during the period concerned the addictiveness of smoking. In 1994 a new tobacco law, the Swedish Tobacco Act, was introduced. The act contained provisions for restrictions on smoking in certain indoor premises and indoor areas, smoke-free work environments, warning texts and contents declarations on the packaging of tobacco products, and restrictions on the marketing of tobacco products. This law replaced and strengthened earlier legislation and new amendments have been added over time. The Swedish Tobacco Act of 1994 stipulated that two separate warning texts now had to be included on every pack. On one side of the package, the text “Tobacco seriously endangers your health” had to be included. On the other side, the package had to contain one of the following eight warnings: “Smoking causes cancer,” “Smoking causes cardiovascular diseases,” “Smoking causes life-threatening diseases,” “Smoking kills,” “Smoking when pregnant harms your baby,” “Protect children: don’t make them breathe your smoke,” “Smoking harms those around you,” and “Smoking causes addiction.” Thus, the risk of addiction was now included in one of the warning texts. The warnings should cover at least 4% of the surface of the package and should be easy to read. Moreover, it was now required that all cigarette packages should be provided with a declaration of contents, covering at least 4% of the side of the package. In 2001, the warnings were changed again and each package now had to include one of the two messages “Smoking kills” or “Smoking seriously harms you and others around you.” The message now had to cover at least 30% of the surface. In addition, a second warning text, covering 40% of the surface had to be included, containing one of the following 12 texts: “Smokers die younger,” “Smoking clogs the arteries and causes heart attacks and strokes,” “Smoking causes fatal lung cancer,” “Smoking when pregnant harms your baby,” “Protect children: don’t make them breathe your smoke,” “Smoking is highly addictive. Don’t start,” “Stopping smoking reduces the risk of fatal heart and lung diseases,” “Smoking can cause a slow and painful death,” “Smoking may reduce the blood flow and causes impotence,” “Smoking causes aging of the skin,” “Smoking can damage the sperm and reduces fertility,” and “Smoke contains benzene, nitrosamines, formaldehyde and hydrogen cyanide.” Regarding the school environment, the tobacco law of 1994 forbids smoking in all school facilities as well as smoking in the schoolyard, but smoking may be permitted in designated rooms or areas, provided that children and young people do not have access to them. Similar rules apply for other public premises, e. g. those in which cultural or sporting events take place. Prior to 1997, no age limit for the buying or selling of cigarettes was in place and an attempt to introduce it in 1991 was rejected by the parliament. In 1997, however, the tobacco law was modified and the selling of tobacco products to anyone below 18 years of age was forbidden. The age limit only concerned selling, not buying. The observance of the law has been limited, however, and many young people are still able to buy cigarettes. In 2003, for instance, 60% of smokers aged 15–16 stated that they bought the cigarettes themselves, mainly at the local corner shop (CAN 2004). The law of 1997 also required that all sales of tobacco products to consumers shall be conducted in such a way that it is possible to determine the age of the recipient, which also applies to vending machines etc. The tobacco law further restricted the possibilities to market tobacco products. All marketing in periodical publications and radio or television was forbidden. In all other cases the law stipulated that marketing should exercise especial moderation, meaning that advertising may not be obtrusive or soliciting, or encourage the use of tobacco products. In practice, this meant a ban on outdoor advertising campaigns and direct marketing through mail etc. In 2002, a number of additional restrictions were adopted. Tobacco products were no longer allowed to include any text or name, such as “light,” that communicates the message that the product is less harmful than others. Indirect advertising—for example, the sales of clothing and shoes that include a tobacco-product trademark—was also forbidden. In addition, all restaurants were required to have designated smoke-free areas. The law was further strengthened in 2004, with the decision to ban smoking in all types of restaurants, bars, and cafés starting June 2005. Smoking is still allowed however in special rooms set aside for smoking, where no food or drink is allowed, and in outdoor service areas. Education about alcohol, narcotics, and tobacco (ANT) is a compulsory subject in Swedish schools. Traditionally, the education has been focused on facts, risks, and medical and social damage created by alcohol, narcotics, and tobacco (Skolverket 2000). In addition, both government bodies and independent organisations have over the years conducted various campaigns aimed at reducing smoking among teenagers. In 2004, for instance, a large quit-smoking campaign was launched by the National Cancer Society, with financial support from, among others, the National Institute of Public Health. The most controversial campaigns have been launched by the independent organisation A Non-Smoking-Generation. In 1994 they launched a large advertising campaign against Philip Morris, with pictures of gravestones accompanied by the text “Welcome to Marlboro Country.” Other campaigns by the organisation that yielded attention were the “Raped by a Prince” campaign, alluding to the effect of smoking on the bodies of young women, and a campaign showing dead bodies and cigarette packs, coupled with the message “Why?” Additional campaigns have been directed at girls and young women. Between 1996 and 2000, for instance, Miss Sweden contestants were involved in non-smoking educational campaigns for young girls in co-operation with popular women’s magazines.",28
34.0,3.0,Journal of Risk and Uncertainty,19 April 2007,https://link.springer.com/article/10.1007/s11166-007-9013-x,Preferences and decision errors in the winner’s curse,June 2007,Ellen Garbarino,Robert Slonim,,Female,Male,Unknown,Mix,,
34.0,3.0,Journal of Risk and Uncertainty,19 May 2007,https://link.springer.com/article/10.1007/s11166-007-9009-6,Stochastic expected utility theory,June 2007,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"Notation L(x
1, p
1;...x

n
, p

n
) denotes lottery L delivering a monetary outcome x

i
 with probability p

i
, i∈{1,...,n}. Let x
1 be the lowest possible outcome and let x

n
 be the highest possible outcome. The expected utility of lottery L according to deterministic preferences of an individual is \( \mu _{L} = {\sum\nolimits_{i = 1}^n {p_{i} u{\left( {x_{i} } \right)}} } \). A subjective non-decreasing utility function u:R→R is defined over changes in wealth rather than absolute wealth levels, as proposed by Markowitz (1952) and later advocated by Kahneman and Tversky (1979). An individual makes random errors when calculating the expected utility μ

L
 of a risky lottery.Footnote 1
 Random errors are assumed to be additive on the utility scale, similar to Hey and Orme (1994, p.1301) and Gonzalez and Wu (1999). Thus, instead of maximizing deterministic expected utility μ

L
, an individual behaves as if he or she maximizes stochastic expected utility
 For simplicity it is assumed that an error term ξ

L
 is independently distributed across lotteries. In other words, the error which occurs when an individual calculates the expected utility of one lottery is not correlated with an error when calculating the expected utility of another lottery. The stochastic expected utility (1) of a lottery is assumed to be bounded from below and above. It cannot be less than the utility of the lowest possible outcome for certain (see, however, Gneezy et al. 2006). Similarly, it cannot exceed the utility of the highest possible outcome for certain. Formally, the internality axiom holds i.e. \( u{\left( {x_{1} } \right)} \leqslant \mu _{L} + \xi _{L} \leqslant u{\left( {x_{n} } \right)} \), which imposes the following restriction on the cumulative distribution function \( \Psi _{L} {\left( v \right)} = {\text{prob}}{\left( {\xi _{L} \leqslant v} \right)} \) of a random error ξ

L
:
 Assumption (2) implies that there is no error in choice between “sure things.” A degenerate lottery delivers one outcome for certain, which is simultaneously its lowest possible and its highest possible outcome (x
1 = x

n
). In this case, Eq. (2) immediately implies that prob(ξ

L
 = 0) = 1 i.e. the utility of a degenerate lottery is not affected by random errors. For non-degenerate lotteries, the random errors are assumed to be symmetrically distributed around zero as long as restriction (2) is not violated i.e. \( {\text{prob}}{\left( {0 \leqslant \xi _{L} \leqslant v} \right)} = {\text{prob}}{\left( { - v \leqslant \xi _{L} \leqslant 0} \right)} \) for every \( v \in {\left[ {0,\min {\left\{ {\mu _{L} - u{\left( {x_{1} } \right)};{\text{ }}u{\left( {x_{n} } \right)} - \mu _{L} } \right\}}} \right]} \). Formally, this corresponds to the restriction
 where \( \Gamma _{L} {\left( v \right)} = {\text{prob}}{\left( {\xi _{L} \geqslant v} \right)} \). Intuitively, random errors are non-systematic if they are within a reasonable range so that a lottery is not valued less than its worst possible outcome or more than its best possible outcome. In general, the cumulative distribution function of random errors for risky lotteries is unknown and it is likely to be lottery-specific (Hey 1995). Equations (1)–(3) complete the description of StEUT. Obviously, when prob(ξ

L
 = 0) = 1 for every lottery L, StEUT coincides with the deterministic EUT. StEUT resembles the Fechner model of stochastic choice e.g. Becker et al. (1963). Both models introduce an error term, which is additive on the utility scale. However, they differ in two important aspects. First, the error term in the Fechner model is a continuous random variable that is symmetrically distributed around zero and unbounded. In practical applications, it is typically assumed to be normally distributed (Hey and Orme 1994; Loomes et al. 2002). In contrast, the error term in StEUT is bounded from below and above by a basic rationality requirement of the internality axiom. For practical estimations, such an error term can be drawn from a truncated normal distribution (see Section 3). Second, the error term in the Fechner model affects the difference in the expected utilities of two lotteries that are compared. We can think of it as a compound error equal to the difference between two computational errors that occur separately when an individual evaluates the expected utility of lotteries. Moreover, if computational errors are normally distributed, their difference is also normally distributed. In contrast, the error term in StEUT is a genuine computational error that affects the expected utility of a lottery. When two lotteries are compared, two corresponding computational errors are taken into account.",64
35.0,1.0,Journal of Risk and Uncertainty,11 July 2007,https://link.springer.com/article/10.1007/s11166-007-9014-9,Conspicuous conservatism in risk choice,August 2007,Boaz Moselle,François Degeorge,Richard Zeckhauser,Male,Male,Male,Male,"We assume that agents have private information regarding their quality. Agents care about the lottery outcome because they stand to gain from the reputation they acquire among outsiders, based in part on the lottery that they choose, and in part on the lottery outcome. Agents choose a level of risk for their activity. We assume the risk choice to be continuous. Formally, each agent chooses from a one-dimensional family of random variables indexed by its variance, V, where \( \underline{V} < V < \overline{V} \). The choice of V is common knowledge. That is, outsiders who are drawing inferences about the agent’s type, see V, and update accordingly. By contrast, when the risk choices by agents are unobservable, the situation considered earlier by Degeorge et al. (2004), hereafter DMZ, the results are quite different. The prime concepts in this paper, such as signaling strategies, and pooling and separating equilibria, do not apply. To facilitate contrasts, we follow the features of the DMZ model, with one crucial difference: In our setting risk choice is observable. The random variable \( \widetilde{x}_{V} \) represents first-period performance (say, the test score, or company earnings). It is distributed normally with variance V and mean \( \mu {\left( {V,\theta } \right)} = \mu {\left( V \right)} + \theta \), where θ indexes the agent’s type. We will refer to the θ = 0 type agent as bad and the θ = Δ type as good, where Δ > 0. The prior probability that an agent is of the good type, denoted by p, is common knowledge, as is the mean-variance schedule μ(V). Each agent chooses a point on the mean-variance schedule given by μ(V) and for given V an agent of type θ has performance 
 We posit that μ(V) is single-peaked, with its maximum at internal point V*, and concave, i.e., the marginal benefit of adding variance decreases throughout, turning negative beyond V*. In fact, the mere existence of an interior maximum is sufficient for most of our theoretical results.Footnote 1 The existence of such an interior maximum follows naturally from the usual assumption that agents have only a finite amount of favorable lotteries.Footnote 2 Our theoretical results, apart from Claim 3, do not assume concavity. Single-peakedness of μ(V) is assumed for the proof of Claim 1 and Claim 3.Footnote 3
 It is convenient (though by no means essential) to assume that \( \mu {\left( V \right)} \to - \infty {\text{ }}as{\text{ }}V \to \underline{V} {\text{ }}or{\text{ }}V \to \overline{V} \), and we maintain this assumption throughout. At the beginning of period 0, an agent learns his type, and then chooses his desired variance V. His random performance x is then drawn according to the equation above, reaped by him, and observed by all, and the period ends. At the beginning of period 1, outsiders draw inferences, and the agent reaps the rewards to his reputation.
 All agents are assumed to be risk-neutral, so that in a full-information setting agents would choose the mean-maximizing level of variance V*.Footnote 4 We assume risk neutrality to facilitate exposition and because we are confident the same qualitative results would apply—namely agents tilting toward conservatism in risk choice—if agents were risk averse, as is normally assumed. We investigate how asymmetric information leads to departures from this optimum. Choosing a variance less than V* represents risk-reducing behavior; choosing a variance greater than V* would increase risk. We can formulate the last step in the game as the sale by the agent of his capital—human, organizational or intellectual—to a new long-term owner. It will clearly be optimal for the new owner to choose the level of variance V* that maximizes expected performance (we assume risk-neutrality throughout), so the expected return to an agent of type θ from next period onward is \( {\sum\limits_{t = 1}^\infty {\frac{{\mu {\left( {V^{*} } \right)} + \theta }} {{{\left( {1 + r} \right)}^{t} }}} } = \frac{{\mu {\left( {V^{*} } \right)} + \theta }} {r} \) where r is the discount rate. Thus, the expected present value (performance plus expected price) of an agent with performance x this period is
  where buyers use Bayesian analysis to compute the expectation term. Agents choose V to maximize their expected total payoff, which is given by their present value, namely
  If θ were observable, this problem would be trivial; we would have
 and agents would always set V = V*. We consider the case where θ is private information, so the agent is faced with a trade-off in maximizing its expected total payoff: clearly, setting V = V* maximizes the expected performance; however, deviating from V* will change the information that flows to outsiders, and has the potential to increase the expected reputation.",1
35.0,1.0,Journal of Risk and Uncertainty,04 July 2007,https://link.springer.com/article/10.1007/s11166-007-9016-7,"Environmental disasters as risk regulation catalysts? The role of Bhopal, Chernobyl, Exxon Valdez, Love Canal, and Three Mile Island in shaping U.S. environmental law",August 2007,Matthew E. Kahn,,,Male,Unknown,Unknown,Male,"The five environmental shocks that I study in this paper received extensive media coverage. These events were not simply environmental shocks. Human error helped cause each of these shocks. When “man made” disasters such as nuclear plant accidents, chemical releases or oil spills take place, people often blame “greedy” corporations for negligence. The probability of an industrial disaster is a decreasing function of costly safety investments by firms (Ehrlich and Becker 1972). If firms such as Exxon could have anticipated the public relations nightmare caused by their oil spill, then such firms would have invested more ex-ante to reduce the likelihood of a spill.Footnote 2
 Media coverage is a necessary condition for such events to be regulatory catalysts. A number of recent papers have emphasized the role played by the media in determining economic and political outcomes (Gentzkow and Shapiro 2006; Hamilton 2004; DellaVigna and Kaplan 2006; Eisensee and Stromberg 2007; Mullainathan and Shleifer 2005; Besley and Burgess 2002). Appendix A provides details about five major environmental shocks that took place between 1978 and 1989.Footnote 3 The New York Times historical search engine provides a consistent method for counting news coverage of each of these shocks. In Figure 1, I graph story counts from the New York Times mentioning “oil spills,” “nuclear accidents,” and “hazardous waste” over the years 1971 to 1998. The figure highlights that Love Canal, Three Mile Island, Chernobyl and the Exxon Valdez shock all had large effects on shifting the composition of news coverage. In the aftermath of the Exxon Valdez spill, there was a disproportionately high quantity of media coverage relative to the number of gallons of oil spilled.Footnote 4 These facts support Zeckhauser’s (1996) claim that concentrated losses are substantially overweighted by the public relative to other day to day threats to health and well being where the costs are dispersed and it is difficult to identify the key polluter (i.e health damage from coal fired power plants).
 New York Times event coverage In the aftermath of the environmental disasters, media attention does decay over time. To document this, I report a monthly count of articles in the newspaper for the 36 months after the shock.Footnote 5 Figure 2 presents these time series figures documenting media coverage. The count of articles devoted to the environmental disaster declines to zero three years after the event.Footnote 6
 New York Times coverage of four shocks Stoked by ample media coverage, disasters may shift the balance of power between interest groups. A standard pressure group theory would posit that tightly organized producer groups would have success fending off the demand for environmental regulation against diffused, free riding voters. The typical voter has little incentive to invest time in being well informed about low probability events. Salient events may change this equilibrium. Such media coverage can encourage the Congress to take action. When the media are actively covering a news story, this reduces the cost for people to learn about the issue.Footnote 7 For low probability rare events such as chemical spills or accidents at nuclear plants, people may never have thought about the particular risk before. Big shocks can “educate” the rationally ignorant voter, leading to pressure for legislative or regulatory reform (Kane 1996; Sunstein 2003).Footnote 8 This catalytic effect would be even larger if people believe in a “law of small numbers” (Noll and Krier 1990; Rabin 2002). People tend to overestimate low probability events and highly publicized events (Viscusi and Hamilton 1999). For low probability events such as nuclear accidents, chemical releases and oil spills, voters and congressional representatives might observe the new shock and increase their posterior probability assessment of the likelihood of future shocks. This paper focuses on the consequences of five shocks that became national media sensations. I do not attempt to offer an explanation for why these events “snowballed.” There have been other environmental shocks that received relatively little media coverage, such as the 1986 chemical spill in Basel, Switzerland that created a massive fish kill in the Rhine River.",45
35.0,1.0,Journal of Risk and Uncertainty,21 June 2007,https://link.springer.com/article/10.1007/s11166-007-9015-8,Statistical vs. identified lives in benefit-cost analysis,August 2007,James K. Hammitt,Nicolas Treich,,Male,Male,Unknown,Male,"We begin with a simple example that illustrates how the outcome of a BCA depends on information about heterogeneity of a policy-induced change in risk. Consider an economy with two people, H and L. Each has initial wealth w and probability p of surviving the single period we consider. Each is risk-neutral in wealth and has no bequest motive (i.e., the utility of wealth conditional on death is zero). Hence each individual has expected utility (EU)
 Consider a project that will impose mortality risk on both people. The average incremental mortality risk is e. The monetary benefit of the project to each person is s. If people do not know their individual incremental risks (or if the incremental risks are identical) then each has expected utility
 Defining social welfare as the mean of the two individuals’ expected utilities (Harsanyi 1953, 1955), social welfare is also given by Eq. 2. Now consider the effect of information about individual risks. For simplicity, assume H (high) faces incremental risk 2e and L (low) faces no incremental risk. Social welfare given information is
 Social welfare with information is the same as in the no-information case. In this example, information about individual changes in risk does not affect social welfare, as Broome (1978) suggests. Moreover, observe that the project will enhance social welfare if and only if
 Consider how BCA would handle the situation. BCA involves comparing the monetary values of social benefits and costs. The per-capita benefit is s. The costs are the total compensation required to induce the people to accept the mortality risks. The compensating variation to accept a risk x, C(x), is defined byFootnote 7
 which implies
 In the case without information about individual risks, the per-capita cost is
 With information about individual risks, the per-capita cost is
 Since C/info > C/no info, the decision recommended by BCA may depend on whether or not the individual risks are known.Footnote 8 Specifically, if
 then BCA will suggest the project should proceed if there is no information about individual risks but should be rejected if individual risks are known. Another approach to BCA is to ask if there is any lump-sum transfer y such that the project with transfer is Pareto superior to rejection of the project. In the present case with information, the question is whether there is an amount y that could be transferred from L to H such that H prefers to adopt the project, i.e.,
 and L prefers to adopt the project, i.e.,
 Equation 11 implies that y < s, i.e., L cannot provide compensation greater than s if he is to prefer the project with transfer to its absence. So the maximum expected utility of H cannot exceed that obtained when y = s. It is clear that Eq. 10 may not be satisfied since the survival probability (p − 2e) can be arbitrarily close (or equal) to zero and so with finite s the inequality is violated.Footnote 9
 How is it possible that information about individual risk has no effect on social welfare but may determine whether the project is adopted? The BCA criterion aggregates the monetary values of welfare gains and losses across people. The relationship between survival probability and the monetary value of utility increments can be highly non-linear. In our example, as survival probability approaches zero the monetary value of a unit of utility approaches infinity. In contrast, the expected-utility criterion aggregates increments of expected utility, which are linear in probability. Measuring in utility units, changes in individual risk (holding population risk constant) and information about individual risk have no effect on social welfare (when individuals are otherwise identical). Measuring in monetary units, information may have a large effect since the monetary value of the harm increases more rapidly for the person whose risk is increased than it decreases for the person whose risk is decreased (i.e., the indifference curve for wealth and risk and the compensation function for risk are both convex, as will be shown below). This example illustrates a well known point: adding compensation is not the same as adding utility.Footnote 10 If one wishes to use monetary compensation as an accurate proxy for adding utility, one must adjust for differences in the marginal utility of consumption between people.Footnote 11 It is well known that adding unweighted compensation is valid in a first-best economy in which a social planner is able to implement lump-sum transfers to equalize marginal utility of consumption across people (Samuelson 1954). But it is also well known that implementing optimal lump-sum transfers, or weighting compensation by the inverse of the marginal utility of consumption, confronts the problem of non-observability of utility which BCA avoids by seeking to determine only whether a proposed change is a potential Pareto improvement (i.e., satisfies a Kaldor–Hicks compensation test). The logical point conveyed by the example is that, without information, individuals are identical and there is no need to weight their compensation for any difference in marginal utilities. In this case, the unweighted sum of compensation is a correct indicator of social welfare. This is not true with information about differences in individual risks. Indeed, with information the situation is asymmetric and marginal utilities may differ between individuals. Hence, whether lives are statistical or identified may affect economic valuation. In the remainder of the paper, we study the effects of individual-specific information about risks on the outcome of BCA, i.e., on the sum of unweighted individual compensation.",41
35.0,1.0,Journal of Risk and Uncertainty,30 June 2007,https://link.springer.com/article/10.1007/s11166-007-9017-6,Risk aversion and expected-utility theory: A calibration exercise,August 2007,Laura Schechter,,,Female,Unknown,Unknown,Female,"Binswanger (1981) found that choices made over modest stakes by rural Indians lead to implausible coefficients of relative risk aversion 25 years ago, although he did not incorporate individual income or wealth. His calculations of partial relative risk aversion (the coefficient of absolute risk aversion multiplied by the size of the gamble rather than wealth, a concept developed by Menezes and Hanson 1970 and Zeckhauser and Keeler 1970) yield reasonable coefficients. On the other hand, if he assumes asset integration he derives coefficients of (Arrow–Pratt) relative risk aversion four to six digits long. He does not use data on individual wealth to calculate these coefficients, but instead assumes that all players have a wealth of 10,000 rupees (while modal wealth in the village is 13,000). He calculates that, holding the player’s choice constant, an increase in certain gains in the gamble of 100 rupees decreases the coefficient of absolute risk aversion as much as would an increase in wealth of 10,000 rupees. This causes him to reject asset integration. At first these results may seem in contrast with much recent experimental research calculating coefficients of relative risk aversion which claims to find single-digit coefficients. This research ignores the fact that players may save their winnings or may come into the game with some level of initial wealth. One of the most well-known of these is work by Holt and Laury (2002) finding average coefficients of approximately 0.4 when defining utility over gains, which is equivalent to assuming wealth is zero. Cardenas and Carpenter (2007) review coefficients calculated ignoring wealth from risk experiments in developing countries and find estimates between 0.32 and 1.25. When using mean annual income as wealth (assuming people cannot save from year to year) double-digit coefficients are found from deductible choice in the Israeli car insurance market (Cohen and Einav 2006) and high single-digit coefficients from play in a TV game show (Gertner 1993). An innovation in this paper is that we use data on both income and choices over risky prospects to calculate players’ coefficients of relative risk aversion. Economists generally think that wealthier people are less risk averse. Holt and Laury (2002) find that “income has a mildly negative effect on risk aversion” and Schechter (2007) finds wealthier people are less risk averse. Can the fact that wealthier people may choose more risky gambles lead to more plausible coefficients when taking into account individual wealth than the quadruple-digit coefficients found by Binswanger (1981) when he assumed every player had the same wealth level? In this paper, even after matching individuals’ incomes with their moderate-stakes bets, we still calculate unreasonably large risk aversion parameters.Footnote 5
 If accounting for individual income does not lead to reasonable coefficients, then perhaps players are evaluating risky decisions in isolation. This is sometimes called ‘narrow bracketing’ or ‘narrow framing.’ Using choices made by contestants in multiple rounds of a TV game show, Gertner (1993) gives evidence that players segregate risky decisions. Read, Loewenstein and Rabin (1999) discuss the many ways choice bracketing may affect decision making in daily life. Kahneman and Lovallo (1993) posit that people are overly timid in their choices because they evaluate risky prospects one at a time rather than pooling risks. Even the equity premium puzzle brought to light by Mehra and Prescott (1985) has been explained by ‘myopic loss aversion’ (Benartzi and Thaler 1995) in which investors are both more sensitive to losses than to gains and evaluate their portfolios frequently. More recently, Fudenberg and Levine (2006) have proposed a dual-self self-control model in which a short-run self is risk averse only over pocket cash while a long-run self is risk averse over wealth.",46
35.0,1.0,Journal of Risk and Uncertainty,19 June 2007,https://link.springer.com/article/10.1007/s11166-007-9018-5,"Valuing publicly sponsored research projects: Risks, scenario adjustments, and inattention",August 2007,Daniel R. Burghart,Trudy Ann Cameron,Geoffrey R. Gerdes,Male,Female,Male,Mix,,
35.0,2.0,Journal of Risk and Uncertainty,01 September 2007,https://link.springer.com/article/10.1007/s11166-007-9021-x,Whom should we believe? Aggregation of heterogeneous beliefs,October 2007,Christian Gollier,,,Male,Unknown,Unknown,Male,"We consider an economy of N heterogeneous agents indexed by θ = 1,...,N. Agents extract utility from consuming a single consumption good. The model is static with one decision date and one consumption date. At the decision date, there is some uncertainty about the state of nature s that will prevail at the consumption date. There are S possible states of nature, indexed by s = 1,...,S. Agents are expected-utility maximizers with a state-independent utility function u(.,θ):R→R where u(c,θ) is the utility of agent θ consuming c. We assume that \(u_{c}=\partial u/\partial c\) is continuously differentiable and concave in c. As in Calvet et al. (2001), we focus on interior solutions. To guarantee this, we assume that \(\lim_{c\rightarrow 0}\partial u/\partial c=+\infty \) and that \(\lim_{c\rightarrow +\infty }\partial u/\partial c=0.\)We also assume that each agent θ has beliefs that can be represented by a vector (p(1,θ),...,p(S,θ)), where p(s,θ) > 0 is the probability of state s assumed by agent θ, with \(\sum_{s=1}^{S}p(s,\theta )=1\). There is an aggregate risk in this economy, which is characterized by the state-contingent endowment z(s) per capita. The crucial assumption of this paper is that the group can allocate risks efficiently among its members. An allocation C is Pareto-efficient if it is feasible and if there is no other feasible allocation that raises the expected utility of at least one member without reducing the expected utility of the others. A special case is the competitive solution, which will be examined in Section 4. In this paper as in Wilson (1968), we characterize the properties of all Pareto-efficient allocations. For a given vector of positive Pareto weights Λ = (λ(1),...,λ(N)), normalized in such a way that \(N^{-1}\sum_{\theta =1}^{N}\lambda (\theta )=1,\) the group would select the allocation of risk that maximizes the weighted sum W of the members’ expected utility under the feasibility constraint: 
 Obviously, this problem can be decomposed into a sequence of S cake-sharing problems. Consider a specific state of nature s with wealth per capita z(s) = z and with a vector P(s) = P = (p(s,1),...,p(s,N)) of individual-specific state probability. For this pair (z,P), define the following cake-sharing problem: 
 The solution of this program is denoted x
 ∗ (.) = c(z,P,.). The interpretation of this program is straightforward. A cake of size Nz must be shared among the N members of the group. The cake-sharing rule is selected in order to maximize a weighted sum of the individual utility functions. In this well-behaved cake-sharing problem, z represents the consumption per capita, and v(z,P) is the maximum sum of the members’ utility weighted by the product of the Pareto weights (λ(1),...,λ(N)) and the vector P. Notice that by construction, v is homogeneous of degree 1 with respect to P. The following proposition, whose proof is skipped, states that for any vector Λ, there exists a representative agent associated to this economy.  Consider a given Pareto-weight vector Λ. The corresponding efficient risk allocation that solves Eqs. 1–2 is such that C(s,θ) equals c(z(s),P(s),θ) for all (s,θ), where c(z,P,.) solves Eq. 3. The social welfare W equals \( \sum_{s=1}^{S}v(z(s),P(s)),\) where v(z,P) is the maximum of Eq. 3. The representative agent’s welfare ex ante is measured by the sum of the v(z(s),P(s)). The characterization of v would be very useful since, from the above proposition, it would allow us to compare and to rank different aggregate risks on which we could build either collective risk policy recommendations or asset pricing formulas. We will link the properties of the v function to the primitive characteristics of individual preferences and beliefs. This will be done by focusing on the cake-sharing problem that defines the v function. The collective valuation function v is linked to the individual-specific utility functions and beliefs through the cake-sharing program (3). Its first-order condition is written as 
 for all (z,P), and for all θ = 1,...,N, where ψ is the Lagrange multiplier associated to the feasibility constraint of program (3). The second equality comes from the envelop theorem. The remainder of the paper focuses on the characterization of function v

z
, where v

z
(z(s),P(s)) is referred to as the willingness to consume in state s. The willingness to consume is central for the determination of optimal collective choices under uncertainty. For example, if society has the opportunity ex ante to transform one unit of wealth in state s into π units of wealth in state s
′, it would be socially efficient to do so at the margin if \(v_{z}(z_{s^{\prime }},P_{s^{\prime }})\) is larger than v

z
(z

s
,P

s
)/π. It would be nice if the willingness to consume would be multiplicatively separable, as in the standard expected utility model. Indeed, if there would exist two functions p
v:R
S→R
 +  and h:R→R
 +  such that v

z
(z,P) would equal p
v(P)h(z) for all (z,P), we could refer to (p
v(P(1)),...,p
v(P(S))) as the vector of state-probabilities of the representative agent (up to a normalizing constant). In the following proposition, we show that this separability does not hold in general, and there exists no aggregate beliefs in the classical sense. Define the absolute risk tolerance of agent θ as \(T^{u}(c,\theta )=-u_{c}(c,\theta )/u_{cc}(c,\theta ).\) We say that the economy has the Identically Sloped Harmonic Absolute Risk Aversion (ISHARA) property if T
u is linear in c with a slope identical for all consumers.Footnote 3
  The willingness to consume v
z
(z,P) is multiplicatively separable in (z,P) if and only if consumers have ISHARA preferences:  Because the proof of this proposition relies on results proved in the next section, it is relegated to Appendix. This implies that in the special case of ISHARA, the representative agent is an expected-utility-maximizer with a well-defined vector (p
v(P(1)),...,p
v(P(S))) of subjective probabilities, and a well-defined marginal utility function h(.). In all other cases, things are more complex, but the complexity is linked to the terminology to be used rather than to some more fundamental aspects of the problem. Following Karni and Schmeidler (1993) and Nau (1995), the collective probabilities p
v could be defined by the following conditions: 
 together with \(\Sigma _{s}p^{v}(s)=1.\)
",27
35.0,2.0,Journal of Risk and Uncertainty,11 September 2007,https://link.springer.com/article/10.1007/s11166-007-9020-y,Individual and group decision making under risk: An experimental study of Bayesian updating and violations of first-order stochastic dominance,October 2007,Gary Charness,Edi Karni,Dan Levin,Male,Male,Male,Male,"Our basic premise is that all the tasks presented to the participants in our experiment are choice problems that have right and wrong answers. Specifically, choices violating monotonicity with respect to first-order stochastic dominance are bad, representing decision-making errors. Furthermore, these errors are more likely to occur when the choice is accompanied by affect, when the stochastic order is less transparent and when the choices are made in isolation. To test these hypotheses, we began by confronting the subjects with a two-stage process. In the first stage, the subjects acquired information, and in the second stage they were asked to choose between two lotteries (risky prospects). If utilized correctly—that is, by the application of Bayes’ rule—the information acquired in the first stage permits the subject to order the lotteries that figure in the second stage, by first-order stochastic dominance. The presence of the information-acquisition stage simultaneously introduces several factors that may contribute to decision making errors. First, if the information acquired is considered to be “good news” or “bad news” it might be accompanied by an emotional response (affect) blurring the difference between the subsequent alternatives and biasing the choice in a predictable direction, resulting in violations of monotonicity with respect to first-order stochastic dominance. Second, to the uninitiated, the need to update the beliefs using Bayes’ rule is confusing and blurs the stochastic order of the risky prospects encountered in the second stage. In the experiment described below we start by confronting the subjects with tasks that confound all these issues. We then disentangle the issues by first removing affect and then, successively, increasing the transparency of the task. Finally, to test for the effect of social interaction, we introduce group decisions and observe and compare the choices made by groups consisting of two and three individuals to those made by solitary decision makers. We conducted Web-based sessions on the UCSB campus, with students recruited by e-mail from the same general student population (but with different students) as in Charness and Levin (2005); approximately 65% of the participants were female. Sessions lasted about 45 minutes on average. Participants met in the lab and were given a handout explaining the experimental set-up; detailed, hands-on instructions were provided on the Web site, and participants were required to correctly answer questions testing comprehension. In our design, there are two equally likely states of the world, Up and Down, and two lotteries (Left and Right) consisting of two different sides of the screen, from which the individual can draw face-down ‘cards’ that may be either black or white. There is always a mix of colors on the Left side, while the Right side has cards of only one color. Throughout the experiment,Footnote 4 only black cards have value, that is, black cards are assigned positive payoff, while white cards pay nothing. In the state Up, the Left side has four black cards and two white cards, while the Right side has six black cards. In the state Down, the Left urn has two black cards and four white cards, while the Right urn has six white cards. 
 Our design includes four different treatments. We refer to the first, most complex, treatment as ABCD, where A signifies the potential presence of affect, B the need for Bayesian updating, C the presence of compounding effect, and D the ranking by first-order stochastic dominance. In this treatment, subjects are asked to select one of six cards on either the left or right side of the screen. The card is then revealed and replaced, with the face-down cards on the screen then ‘shuffled’. The subject then selects a second card, knowing that the state (Up or Down) remains the same as for the first card, and this card is then revealed; people were paid one unit of experimental currency for each black card drawn. We are primarily interested in the choice behavior when the first draw is from the Left urn, as previous evidence indicates that the error rate is very low in all cases when the first draw is from the Right urn. To ensure receiving relevant observations for each individual, during the first 20 (out of 60) periods in the session we required the first draw to be from the Left side in the odd-numbered periods and from the Right side in the even-numbered periods. After drawing a black card from the Left side, a second draw from the Right side stochastically dominates a second draw from the left side. Therefore, the subject should switch to the Right side for the second and final draw of the period. (Updating his prior according to Bayes’ rule, the subject’s probability of success, that is, winning a unit of the experimental currency, from the second draw are: 5/9 if he draws again from the Left side, and 6/9 if his second draw is from the Right side). By the same logic, after having drawn a white card from the Left side, the decision maker should stay with the Left side for the second draw of the period. (Updating his prior by Bayes’ rule, the subject’s probability of success in the second draw are 4/9 if he draws again from the Left side, and 3/9 if he draws from the Right side).Footnote 5
 A subject’s choice for the second draw may be biased by his emotional response to the observed outcome. A black card in the first draw from the Left represents success (and an award), and may induce a sensation of optimism reinforcing the Left side choice. In other words, having just won a prize drawing from the Left, the subject may consider this to be his lucky side, and be reluctant to switch. On the other hand, drawing a white card first may instill a sensation of disappointment (failure) with the first Left draw and “push” the subject to switch sides for the second draw. To the extent that such heuristics exist and influence decisions, they may exacerbate the violations of monotonicity with respect to first-order stochastic dominance. In order to mitigate the potential influence of such sensations, we implemented a treatment, dubbed BCD (the Affect being removed), in which the first draw from the Left was not associated with a payoff. Moreover, to counteract the possible presence of a more subtle emotional response that might accompany success or failure when observing the first draw, we did not tell the subject whether it is the black or white card that has a positive payoff in the second draw until after the first card was drawn. This was feasible due to the symmetry of the distribution of black and white cards across the Up and Down states. People were required to make their initial draws from the Left side, observing the color of the card before replacement; there was no payment for the first draw. At the bottom of the screen, the subject was then informed as to the color that would pay on the second draw. In our third treatment, CD, we eliminate the need for Bayesian updating while preserving the compound-lottery structure involving combining continuation probabilities after different initial outcomes. In this treatment prizes were awarded for drawing black cards during 80 periods. As before, after a first black draw from the Left the correct Bayesian posterior probability of Up is 2/3 and that of Down is 1/3. In order to eliminate the need for Bayesian updating, we simplified the procedure so that there is only one draw in a period. We offer subjects a choice between the Left and Right side, informing them that the probability of state Up is 2/3 (the same as in the BCD treatment after a would-be successful first draw); this matches the case when a would-be-successful color is drawn first in the BCD treatment. Note that to compute overall probabilities a subject must analyze, for the Left side, a compound lottery involving different states of Nature. The fourth treatment, D, focuses on dominance. In this treatment, done on paper, subjects were offered a reduced lottery instead of the compound lottery mentioned in the preceding paragraph. Subjects were presented with a choice of choosing the Left or Right side after being informed that we would roll a nine-sided die (actually ten-sided, but if 0 came up we rolled again); if a subject chose Left, a prize of $2 would be awarded if numbers 1-5 came up, while if the decision maker chose Right, this prize would be awarded if numbers 1-6 came up. We handed out a sheet of paper with this choice at the end of BCD or CD sessions, rather than repeating the choice multiple times. This procedure is equivalent to the case (in the more complex treatments) where the first draw has been made from the Left side and was successful, as there is the same 5/9 probability of success by drawing again from the Left side and a 6/9 chance of success by instead drawing from the Right side. The procedure that we chose to follow in these experiments, that is, starting with the ABCD treatment rather than the simpler ones, reflects our view that decision making under risk and under uncertainty, typically involves all, or some, of these factors. One makes a choice and experiences an emotional response to the outcome; this outcome also generates information that necessitates updating the priors, and the calculations may also involve compound lotteries. The second part of our study examines the social-interaction effects on decisions. Our interest in this issue stems from the obvious fact that most real-life decisions take place in environments in which individuals can, and do, consult and seek advice before making decisions, thereby benefiting from the experience and expertise of others. We hypothesize that the opportunity to exchange ideas and opinions improves the decision making ability of individuals. To test this hypothesis, we administered some of the treatments with groups instead of individuals acting as decision making units. Specifically, we repeated treatments BCD/CD/D and CD/D with pairs of subjects making joint decisions, and treatments BCD/CD/D with groups of three subjects making decisions.Footnote 6 The subjects in each group were permitted to speak to each other (quietly) and could reach decisions in any manner, while experimenter intervention (e.g., a coin flip) was available if needed. In practice, no group ever failed to reach a decision on its own. To grasp our hypotheses, consider the case of the BCD treatment. In a population of subjects an unknown proportion, say x, of the subjects are Bayesians (that is, subjects who know and are able to apply Bayes’ rule). We hypothesize that, without fail, Bayesians choose the dominating option. The rest of the subjects, whose proportion in the population is \(\left( 1-x\right) \), employ other means to assess the alternative risks and make their choices. Among these subjects, a certain proportion, say \(\left( 1-\alpha _{1}\right) ,\) may actually “guess” the right choice and pick the dominating alternative, without using Bayesian formula. Thus, \(\left( 1-\alpha _{1}\right) (1-x)\) is the proportion of non-Bayesian subjects in the population that, by chance or by intuition, made the right choice. Clearly, α
1(1 − x) is the proportion of non-Bayesian subjects that made the wrong selection. We are reluctant to speculate on what might be reasonable values for x or α
1. In particular, if the non-Bayesian participants are as likely to make the right decision as to make the wrong decision, then α
1 = 0.5. However, it is conceivable that, even if they do not know how to apply Bayesian rule, subjects may nevertheless intuitively choose correctly more often than not. Thus, we hypothesize that α
1 < 0.5. We propose here an hypothesis that would allow us to estimate the values of x and α
1, and test them. To illustrate how this is done, suppose that Bayesian decision makers do not err. Then observing the proportion of wrong choices, say k
1, in the individual experiments, we learned that: 
 Turning next to decisions by groups, we hypothesize that whenever matched with non-Bayesians, the Bayesians are able to convince them of the right way of looking at the (posterior) alternatives and, consequently, choose the right alternative—this would be the case if the truth-wins norm applies. We also hypothesize that, even if Bayesians are not represented in a group, the mere deliberation of the alternatives among members of a group, tends to increase the number of right choices. Formally, let k
2 be the proportion of wrong choices by groups consisting of pairs of individuals. Then, 
 where it might seem a natural presumption that α
2 ≤ α
1; we test this presumption later. That is, k
2 is the proportion of non-Bayesians that were randomly matched with other non-Bayesians and who jointly chose the wrong answer. If we alternatively insist that α
2 = α
1 = α, these two equations allow us to estimate (solve) simultaneously for x and α. Call these estimates x
* and α
*. But if in fact, α
2 < α
1 ≤ 0.5, then x
* overestimates the true value of x, and α
* overestimates α
1 and α
2, where the overestimation of α
2 is relatively larger than that of α
1.
Footnote 7
 Observe next that in groups of three subjects, the proportion of wrong choices should satisfy \(k_{3}=\alpha _{3}\left( 1-x\right) ^{3}\), where x is the true proportion of Bayesians in the population and \(\alpha _{3}\leq \alpha ^{\ast }\leq \alpha _{2}.\) Our hypothesis does not indicate unambiguously the relationship between the magnitude of k
3 and \(\alpha ^{\ast }\left( 1-x^{\ast }\right) ^{3}.\) To see why, note that 
 Thus, solving α
 ∗  from Eq. 3, gives 
  where we used the inequality α
3 ≤ α
2 and that x
 ∗  overestimates x. However, \(k_{3}\leq \alpha ^{\ast }\left( 1-x^{\ast }\right) ^{3}\) implies that α
3 < α
2. Hence, the last inequality, that non-Bayesian trios make better guesses than non-Bayesian pairs, is a testable hypothesis. In the CD and D treatments, the need for Bayesian updating is removed. That is, it can be modeled along the same lines as above assuming that x = 0. Note, however, that because the treatments are less complex, it is not necessarily true that the proportions of subjects that make the wrong selection remain the same across treatments. It seems reasonable to suppose that this proportion declines when the stochastic order becomes more transparent. Formally, let \(\alpha _{i}^{CD}\) and \(\alpha _{i}^{D},i=1,2,3\) denote the proportion of subjects that make the wrong selection in the CD and D treatments, respectively, rather than in the BCD treatment. We hypothesize that \(\alpha _{i}>\alpha _{i}^{CD}>\alpha _{i}^{D}\), i = 1,2,3. We summarize our treatments with individual decision makers in Table 1, and those with groups in Table 2. 
 Participants were paid $0.30 for each successful draw in the ABCD,BCD, and CD treatments. In order to pay reasonably similar amounts across treatments, we had 80 periods in the individual BCD and CD treatments rather than the 60 periods in the ABCD treatment (80 decisions counted, compared to 120) and also included the D treatment at the end of the session. No person could participate in more than one session or treatment (with the exception of the BCD/D and CD/D sessions). Group decisions took longer, so we reduced the number of periods to 30 and increased the show-up fee (to $8 from $5) in these sessions.Footnote 8 On average, our participants earned approximately $16–17 for a 1-h session.",112
35.0,2.0,Journal of Risk and Uncertainty,01 September 2007,https://link.springer.com/article/10.1007/s11166-007-9022-9,A Bayesian examination of information and uncertainty in contingent valuation,October 2007,David M. Aadland,Arthur J. Caplan,Owen R. Phillips,Male,Male,Male,Male,"Assume a continuum of agents indexed on the unit interval. Representative agent i ∈ (0,1) maximizes utility 
 by choosing a vector of private goods, z

i
. Each agent’s valuation of the public good, G, depends on a stochastic component η

i
 (discussed below). θ

i
 is a vector of individual-specific characteristics excluding income level. The agent’s budget constraint is 
 where m

i
 is income, p is a vector of prices corresponding to z, and g

i
 ≥ 0 is an exogenously determined lump-sum payment toward the provision of G. We assume that the sum of the total private contributions, \({\textstyle\int\nolimits_{i}} g_{i}di\), results in provision of the public good at level G. We invoke the standard assumption that utility is strictly increasing and quasi-concave in both the private and public goods. The term η

i
 reflects the notion that agents are not always capable of accurately assessing the value of the public good due to a lack of experience with the good or potential bias in a hypothetical setting. In particular, agents with η

i
 > 0 tend to overestimate their WTP for G, agents with η

i
 < 0 tend to underestimate their WTP for G, while agents with η

i
 = 0 accurately assess their WTP for G. As we show below, although agents attempt to correct for the bias via their interactions with the interviewer in a hypothetical assessment, they do not necessarily have adequate information to completely eliminate it. As is common in the literature, we refer to hypothetical bias as the tendency to misstate true WTP when in a hypothetical rather than a real market environment. This definition is sufficiently broad to accommodate many of the different biases associated with nonmarket valuation, such as strategic bias, awareness bias, nay-saying, yeah-saying, etc. In particular, our theory assumes that hypothetical bias is stochastic and involves a respondent continually updating and estimating the degree of personal bias using a Bayesian approach. In our view, this is the most natural way to think about hypothetical bias but note that it could instead result deterministically, e.g., as a “purchase of moral satisfaction” (Kahneman and Knetsch 1992) or as a desire to conform socially (Bernheim 1994). In these cases, one might think of cheap talk as an updating mechanism that instills guilt in the respondent (for knowingly reporting an untruthful WTP) rather than a mechanism that reduces uncertainty. Let \(z_{i}^{\ast}=z(p,m_{i}-g_{i},G(\eta_{i});\mathbf{\theta}_{i})\) represent the agent’s optimal choice of the private good vector, implying indirect utility level \(u_{i}^{\ast}=u(z_{i}^{\ast},G(\eta_{i});\mathbf{\theta}_{i})\). The corresponding minimum expenditure function, defined with respect to net income, m

i
 − g

i
, is 
 Using Eq. 3, the agent’s WTP for G is derived as 
 which is the difference between the minimum expenditure required to achieve utility level \(u_{i}^{\ast}\) without and with the public good. Due to the presence of η

i
, Eq. 4 reflects the agent’s perceived, rather than true, WTP for the public good. Accordingly, we characterize perceived \(\textrm{WTP}_{i}\) as: 
 where \(\textrm{WTP}_{i}(\eta_{i}=0)\) is “true” WTP and δ

i
 is a random variable with density function p(δ

i
) and population mean 
 We assume that δ

i
 reflects the agent’s innate tendency to incorrectly estimate WTP for the public good. While agents do not know p(δ

i
), they do hold prior beliefs regarding the distribution for δ

i
. Based on this subjective probability distribution for δ

i
, they form a corresponding expectation denoted by E

i
 (δ

i
). This expectation represents the agent’s initial evaluation of personal bias. For example, if δ

i
 > E

i
(δ

i
) = 0, then the agent does not recognize that he is overvaluing the public good and thus a positive bias exists. Another possibility is that δ

i
 > E

i
 (δ

i
) > 0, in which case the agent suspects that he is overvaluing the public good, but only partially corrects for the bias. We refer to the agent’s initial perceived WTP as \(\textrm{WTP}_{i}^{0}\), which is given by Eq. 5. However, as the agent receives information (assumed from the interviewer), the agent revises \(\textrm{WTP}_{i}\) in an attempt to reduce the influence of δ

i
 and bring perceived WTP closer to the true WTP. The agent thus forms 
 where \(E_{i}(\textrm{WTP}_{i}|s_{i})\) is agent i’s expectation of \(\textrm{WTP}_{i}\) conditional upon the information contained in the signal vector s

i
. From Eqs. 5 and 7, we see that clear signals provided by the interviewer regarding the population mean of δ

i
 are, on average, likely to bring perceived WTP closer to the true WTP. Each agent faces a Bayesian-updating problem with a subjective prior distribution for δ

i
, h

i
(δ

i
). Henceforth, we assume E

i
(δ

i
) = 0 so that the agent initially perceives no bias in valuing the public good. After receiving the signal s

i
 from the interviewer, the agent then uses Bayes’ formula to form the posterior distribution for δ

i
: 
 where g

i
(s

i
|δ

i
) is the distribution for s

i
 conditional on δ

i
. The function g

i
(s

i
|δ

i
) captures the essence of the revisions to beliefs about δ

i
 by directly accounting for the interaction between δ

i
 and s

i
. Assuming a quadratic loss function, the agent then responds “rationally” to s

i
 by forming an updated expectation of δ

i
 usingFootnote 7
",19
35.0,2.0,Journal of Risk and Uncertainty,14 September 2007,https://link.springer.com/article/10.1007/s11166-007-9019-4,The relationship between individual expectations and behaviors: Mortality expectations and smoking decisions,October 2007,Ahmed Khwaja,Frank Sloan,Sukyung Chung,Male,Male,Unknown,Male,"We use data on subjective beliefs about mortality outcomes, actual mortality, and smoking decisions from the Health and Retirement Study (HRS). The HRS is unique in that it contains self-reported data on survival expectations of individuals and a longitudinal follow-up of their mortality outcomes. The HRS is a national panel study of birth cohorts 1931 through 1941 and their spouses, if married (http://www.hrsonline.isr.umich.edu). The HRS oversamples blacks, Hispanics and residents of Florida. Participants in the HRS have been interviewed every 2 years since 1992. The main respondent to the first wave of the HRS was between 50 to 61 years of age; spouses received an identical interview and could be of any age. Table 1 shows summary statistics. Briefly, nearly 28% of the sample smoked at the time of the baseline survey; 36% were former smokers, most of whom had quit for 10 or more years, and 37% had never smoked. On average, sample persons were 56 years old; 73% were white, 53% were female. Seventeen percent were college graduates while 57% were high school graduates. Twenty-two percent of the sample was in fair or poor health. The most frequent chronic conditions were hypertension, arthritis and limitations in lower body activities; 11% were hospitalized in the last 2 years.Footnote 5
",58
35.0,3.0,Journal of Risk and Uncertainty,20 November 2007,https://link.springer.com/article/10.1007/s11166-007-9025-6,Fair (and not so fair) division,December 2007,John W. Pratt,,,Male,Unknown,Unknown,Male,"A group of n agents jointly owns a collection of N items that are to be divided up among them. The items may be indivisible, but agents may be allocated probability shares of an item. The agents’ initial entitlements (ownership fractions) are specified; they will usually be assumed equal for convenience in exposition. Side-payments are not allowed, although a monetary value may be assigned to each item which the recipient pays into a kitty to be divided among the agents in proportion to their entitlements, as in PZ. The agents are neither altruistic nor spiteful, that is, they care only about their own receipts. Agent j’s private value for item i, denoted u

ji
, is revealed, at least to a central administrator or arbitrator. Our concern is fair division, not incentive compatibility. Our results apply to the values as revealed. With well-intentioned agents, the revelation problem in practice is not strategic misrepresentation but conceptual misunderstanding (cf. the Appendix of PZ). Except in Sections 7–10, we assume that the agents’ values are interpretable as cardinal utilities and that they have additive utility functions. Thus agent j’s expected utility gain, which we shall often call simply utility or gain, is ∑
i

p

ji

u

ji
 where p

ji
 is agent j’s probability of receiving item i. We assume that an item is removed from the collection if no one values it positively (free disposal). Efficiency then requires that someone receive each item, that is, ∑
j

p

ji
 = 1 for every i. We shall sometimes say that an allocation procedure or solution is unique if it determines the agents’ expected utilities uniquely. Besides seeking efficiency, we assume that no agent is forced to participate (individual rationality), and of course that the solution is unaffected by the numbering of the agents (symmetry) and the scale used to express their utilities (utility invariance). The only other axiom suggested here is the absence of spite (see above and Section 6).",8
35.0,3.0,Journal of Risk and Uncertainty,17 November 2007,https://link.springer.com/article/10.1007/s11166-007-9023-8,Predicted risk perception and risk-taking behavior: The case of impaired driving,December 2007,Georges Dionne,Claude Fluet,Denise Desjardins,Male,,Female,Mix,,
35.0,3.0,Journal of Risk and Uncertainty,18 October 2007,https://link.springer.com/article/10.1007/s11166-007-9024-7,"Gender, risk and stereotypes",December 2007,Dinky Daruvala,,,Unknown,Unknown,Unknown,Unknown,,
35.0,3.0,Journal of Risk and Uncertainty,25 October 2007,https://link.springer.com/article/10.1007/s11166-007-9026-5,Behavior towards health risks: An empirical study using the “Mad Cow” crisis as an experiment,December 2007,Jérôme Adda,,,Male,Unknown,Unknown,Male,"On March 20, 1996, the British Minister of Health informed the House of Commons that scientists had established a link between the “Mad Cow” disease, and the new variant Creutzfeldt–Jacob disease, a fatal brain disease which affects humans. The disease is triggered by the accumulation of a prion protein in the brain. The prion passes from cows to humans, by consumption of infected beef. At that date, eight people out of the ten diagnosed with the nvCJD in the UK had died. By 1996, a large number of cows had been infected with BSE and, given the incubation time, a number of them had entered the food chain undetected. Before March 1996, nvCJD was totally unknown in the wider public and BSE was still a specific bovine disease, not unlike scrapie, which had affected sheep for more than a century, without effects on humans. The “Mad Cow” crisis made the headlines of most newspapers for several months and came as a shock, as over 98% of French households had consumed beef prior to that date. An embargo on British beef was imposed shortly after, but the media reported numerous cases of frauds. At that time, BSE had also been diagnosed in French cattle. At the time of the crisis, few scientific facts were known for certain. Consumers were informed that the consumption of infected beef was the determinant of nvCJD, as those who had been diagnosed had on average consumed large amounts of beef. The exact incubation period in humans was not known, but was thought to be around a couple of years. The exact dose-response relationship was also unknown at the time and has not been firmly established in the medical literature yet. Alarmist (but imprecise) forecasts of the future death toll were published. Predictions as high as 500,000 deaths in the UK were put forth (The Economist, March 30, 1996, p. 25). The panel data set has been collected by SECODIP, a French firm which gathered data for marketing purposes on a sample of representative households. It recorded all expenditures for a representative sample of 2,798 French households, week by week, between January 1, 1995 and June 24, 1996 (76 weeks).Footnote 2 The data was recorded using bar code scanners and measurement error is likely to be small. Each week, the household reported each item bought with a detailed description of the product, the quantity and the expenditure. The items under consideration are all purchases of meat, fish, eggs and dairy products. The information about the product is quite detailed, describing the particular cut of meat or the type of fish (18 different cuts of beef are reported). The data has been aggregated up at a quarterly frequency, in order to avoid zero expenditure due to infrequent purchases. This leaves six periods, the crisis starting at the beginning of the last quarter. In addition, in 1995 only, the data set recorded all purchases of alcohol on a weekly basis. The data set also reports details on the composition of the family, the age of all the members, their occupation and education level, the household income, the region of residence and the size of the city. The data set also reports anthropometrical measures for all the household members such as height and waist circumference. All these household characteristics are reported at the start of the period, so we observe no variation during these 76 weeks. We construct a measure of alcohol consumption by averaging all alcohol purchases over the 52 weeks of 1995 and by scaling it by the number of adults in the household. We then break down this variable into three dummies at the 33rd and 66th percentile. We also construct a ratio between waist circumference and height to measure whether any household members are overweight. We break down this variable into three dummies in the same way as alcohol consumption. Tables 1 and 2 present summary statistics. On average, the consumption of beef fell sharply by about 26% in quantities and by 22% in expenditures when households learned about the crisis. During that period, the relative price of beef fell slightly by about 2%. From the aggregate price index, the crisis is barely noticeable. This may be the effect of the European Union policy of price stabilization, which allows storage and destruction of surplus. These facts imply that consumers changed to more expensive types of meat, a fact that we investigate in more detail in Section 3. Households did not change their total expenditure on animal protein (which includes all meat and fish expenditure), which is slightly higher during the last period. This means that they substituted towards other types of meat or fish.
 Tables 1 and 2 only give an aggregate view of the behavior as a result of new health information. We now study the heterogeneity in the response to new information, using the cross-section dimension of the data set. Figure 1 displays a measure of the change in quantities between two periods (quarters). If c

Bt
 is the per capita quantity of beef consumed in quarter t, the measure is 100*(c

Bt
 − c

Bt − 1)/(c

Bt
/2 + c

Bt − 1/2), which is bounded between − 200 and 200. Two distributions are displayed, before and after the announcement.
 Change in beef consumption, before and during the announcement. Note: Data source: SECODIP panel data Before the announcement, the change in consumption is centered around zero with a roughly symmetric distribution. The distribution after the announcement is different and interesting for two reasons. First, the distribution is centered to the left and asymmetric, because most households have decreased their consumption. Second, there is a strong heterogeneity in households’ responses in terms of consumption changes. There is a continuous distribution over the consumption changes. Some households have decreased their consumption by 20, 50 or 80%. This means that the households were not faced with a discrete decision: either stop risky behavior or ignore new health information. This result is not due to the aggregation of different behavior within the household, as it holds also for single person households. The decrease in beef consumption is the result of consumers purchasing less often and fewer quantities. Even the average quantity of beef purchased, conditional on purchasing some, has decreased significantly. About 8% of the sample stopped consuming beef altogether. This figure is higher than the 3.5% in the preceding periods, but still relatively low with regard to the crisis, as the households could substitute to other types of meat or animal protein. Note that some households have increased their consumption, despite the crisis. Part of the increase could be explained by relative price variations, as the price of beef slightly decreased after March 1996. The distribution of total expenditure on animal protein did not change as a consequence of the crisis. In particular, there is no evidence that any households became vegetarian. In this section, we analyze the heterogeneity in changes in beef consumption as a function of past behavior. We correlate changes in quantities consumed as a result of the crisis with the average consumption calculated up to the crisis (weekly average per quarter). Figure 2 displays a non parametric regression using a grid with fifty points. The figure plots the response of all households in the sample as well as single person households.Footnote 3
 Effect of prior exposure to beef on change in consumption. Note: Data source: SECODIP panel data There is a correlation between changes in health behavior (as indicated by further consumption of beef) and disease susceptibility (proxied by past consumption). The response is U-shaped. Households with either small or large consumption prior to the crisis demanded less beef. This fact is even more pronounced for individual consumption as measured for single person households. A formal statistical test shows that both minor and substantial consumers of beef reduced their consumption statistically less than moderate consumers of beef (the associated F tests are equal to 40 and 43, respectively). The previous graph assumed that consumers could only respond to new health information along one dimension, the consumed quantities. In reality, individuals can also choose better quality products. Analyzing only quantities could lead to misleading conclusions.Footnote 4
 We now analyze how consumers responded to the crisis in the quality dimension and how the demand for quality differed with prior behavior. In France, at the time of the crisis, the main quality differentiation was the cut of the beef and not the origin.Footnote 5 The data on quantities and expenditure per item were used to compute unit prices, i.e. the price per gram. The variation in prices paid by a household reflects both time and regional variations but also the quality of the product. We therefore compute a quality index by computing the residuals from a regression of unit prices (prices per gram) on time and regional indicators. As seen in Table 1, the relative price of beef fell by 1.6%. In the data set, we find the same pattern for a given cut of beef. However, the average price paid by households in the data set for beef increased sharply as a result of the crisis, by about 10%. This indicates that the households went out for more expensive cuts of beef after March 1996. From the data, the market share of low quality cuts of beef did sharply decrease during the crisis, while the demand for high quality cuts of beef increased.Footnote 6
 The crisis triggered both a decrease in the consumption of beef and an increased demand for higher quality. This increased demand for quality regarding beef consumption was not uniform across households. Figure 3 displays the change in the demand for quality between the quarter preceding the crisis and the quarter after the announcement as a function of the previous exposure to beef, both for all households and for single person households. The response is hump shaped as both households with low and high exposure to beef did not change their behavior in terms of quality.
 Effect of prior exposure to beef on change in quality. Note: Data source: SECODIP panel data The responses in Figs. 2 and 3 do not condition on observable characteristics which could potentially confound these results. The next section presents a model which summarizes different explanations and then shows how to discriminate between them.",23
36.0,1.0,Journal of Risk and Uncertainty,05 January 2008,https://link.springer.com/article/10.1007/s11166-007-9027-4,Correcting expected utility for comparisons between alternative outcomes: A unified parameterization of regret and disappointment,February 2008,Carlos E. Laciana,Elke U. Weber,,Male,Female,Unknown,Mix,,
36.0,1.0,Journal of Risk and Uncertainty,04 January 2008,https://link.springer.com/article/10.1007/s11166-007-9028-3,The effect of the background risk in a simple chance improving decision model,February 2008,Jinkwon Lee,,,Unknown,Unknown,Unknown,Unknown,,
36.0,1.0,Journal of Risk and Uncertainty,04 January 2008,https://link.springer.com/article/10.1007/s11166-007-9030-9,Measuring revisions to subjective expectations,February 2008,Adeline Delavande,,,Female,Unknown,Unknown,Female,"This section presents the ERS metric that can be used to characterize the process of revising subjective expectations about a probability. Consider an outcome B that is experienced by agent i with probability P

i
, i.e., 
  where \(\left\{ b_{\!i}=1\right\} \) is the binary event “i experiences outcome B.” Suppose that the objective probability P

i
, although specific to agent i, is not known to her and that she holds subjective expectations about it. Let f

i,1 ∈ Γ denote i’s prior subjective distribution of beliefs about the probability P

i
, where Γ denotes the set of all probability distribution functions defined in [0,1]. In a similar way, let f

i,2 ∈ Γ denote i’s posterior subjective distribution of beliefs after i has received a new piece of information o

i
 ∈ I about the binary outcome B, where I represents the set of all pieces of information that an agent can receive. The revision process, or learning, is modeled by positing that agent i has an updating function \(U_{i}(.,.):\Gamma \times I\longrightarrow \Gamma \), which specifies a posterior subjective distribution about P

i
 given any prior subjective distribution f

i,1 and any information received o

i
. Thus, we can write 
 It may be difficult to identify the function U

i
(.).Footnote 5 A convenient way to characterize this revision of beliefs can be provided by the random sample of binary events that generates the observed revision from prior to posterior beliefs according to Bayes’ rule. More specifically, a given revision process can be represented by backing out the random sample of \(\left\{ 0\right\} s\) and \(\left\{ 1\right\} s\) drawn from the probability distribution P

i
 that an individual i using Bayes’ theorem should have seen to execute an identical revision of beliefs. This representation is general and incorporates many types of updating rules; Bayes’ theorem serves only as a device for the purpose of normalization. I formalize this representation of the revision to subjective expectations in the following definition. The Equivalent Random Sample
π

i
(o

i
) of individual i to the data o

i
 is the random sample of binary events drawn from P

i
 that would have generated the Bayesian updating from f

i,1 to f

i,2 = U

i
(f

i,1,o

i
). Hereinafter, I restrict the distributions of beliefs f

i,1 and f

i,2 to be beta distributions.Footnote 6 When the prior and posterior are beta distributions, there is a simple condition under which the ERS exists and is unique.Footnote 7 The following proposition formalizes this claim. 
Consider two beta distributions f
1
and f
2
characterized by the integer parameters n
1
and r
1
and n
2
and r
2
, respectively. Let f
1
be the prior distribution of beliefs about the probability P
of a Bernoulli process. If n
2 ≥ n
1
and r
2 ≥ r
1
, then there exists a unique random sample of binary events drawn from the probability P
that generates the Bayesian updating from f
1
to
f
2. From the fact that f
1 is a beta distribution with parameters n
1 and r
1, we can write 
  where \(c_{1}=\int_{0}^{1}z^{n_{1}-1}\left( 1-z\right) ^{r_{1}-1}dz\). In a similar way, from the fact that f
2 is a beta distribution with parameters n
2 and r
2, it follows that 
  where \(c_{2}=\int_{0}^{1}z^{n_{2}-1}\left( 1-z\right) ^{r_{2}-1}dz\). Let n and r be the number of \(\left\{ 1\right\} s\) and \(\left\{ 0\right\} s\) respectively drawn from a Bernoulli process with probability P, observed by an agent with prior distribution f
1 about P . After observing this sample, the agent’s posterior distribution of beliefs f
post about P, updated according to Bayes’ theorem, is given by 
 where \(c_{3}=\int_{0}^{1}\dfrac{z^{n_{1}+n-1}\left( 1-z\right) ^{r_{1}+r-1}}{ c_{1}}dz\) and \(c_{4}=\int_{0}^{1}z^{n_{1}+n-1}\left( 1-z\right) ^{r_{1}+r-1}dfz.\)
 Thus, f
2(p) = f

post
(p) if and only if 
  for all \(p\in \left] 0,1\right[ \), which is equivalent to 
  and 
 Hence, when r
2 ≥ r
1 and n
2 ≥ n
1 a sample of observations that generates the updating from prior to posterior according to Bayes’ theorem exists, is unique and is given by n = n
2 − n
1
\(\left\{ 1\right\} s\) and r = r
2 − r
1
\(\left\{ 0\right\} s\). This completes the proof.□ Analyzing the revision process reduces to analyzing the individual-specific ERS π

i
(.). The revision process U

i
(f

i,1,o

i
) can thus be represented as follows: 
 In Section 5, I estimate systematic variation in ERS as a function of observable characteristics X

i
, in which case the revision process U(f

i,1,o

i
,X

i
) can be represented as follows: 
 While other methods could be used to describe the revision process,Footnote 8 the ERS enjoys the advantage of providing an updating rule that can be compared to both Bayes’ theorem and the heuristic rules that have been identified in the literature (e.g., the representativeness or conservative heuristics of Tversky and Kahneman 1974). In the empirical application that follows, I focus on women learning about the effectiveness of a contraception method M. A sexually active woman i faces the objective probability of getting pregnant \(P_{i}\in \left[ 0,1 \right] \) if method M is used for a year: \(P_{i}=\Pr (b_{\!i}=1),\) where \( \left\{ b_{\!i}=1\right\} \) is the event “i gets pregnant within a year while using method M.” The objective probability P

i
 is not known to woman i, so she possesses a subjective distribution of beliefs \(f_{i,t \text{ }}\) about P

i
 at time t. The time index reflects that beliefs about P

i
 may change as i receives new information o

i,t
 ∈ I between time t and t + 1. Let F

i,t
 denote the cumulative distribution of belief of agent i at time t, and A

i,t
 denote the first moment of f

i,t
, i.e., 
 
A

i,t
 is the (overall) subjective probability of getting pregnant while using method M for a year.",22
36.0,1.0,Journal of Risk and Uncertainty,08 January 2008,https://link.springer.com/article/10.1007/s11166-007-9029-2,Prospect theory for continuous distributions,February 2008,Marc Oliver Rieger,Mei Wang,,Male,,Unknown,Mix,,
36.0,2.0,Journal of Risk and Uncertainty,05 February 2008,https://link.springer.com/article/10.1007/s11166-008-9033-1,Self-protection and insurance with interdependencies,April 2008,Alexander Muermann,Howard Kunreuther,,Male,Male,Unknown,Male,,19
36.0,2.0,Journal of Risk and Uncertainty,06 February 2008,https://link.springer.com/article/10.1007/s11166-008-9034-0,Valuing lives equally: Defensible premise or unwarranted compromise?,April 2008,Rachel Baker,Susan Chilton,Hugh Metcalf,Female,Female,Male,Mix,,
36.0,2.0,Journal of Risk and Uncertainty,31 January 2008,https://link.springer.com/article/10.1007/s11166-008-9031-3,Risk preferences and changes in background risk,April 2008,Donald C. Keenan,Donald C. Rudow,Arthur Snow,Male,Male,Male,Male,"We assume utility u(z), where final wealth z is a random variable \(\tilde{z} = \tilde{x} + \tilde{y}\), with \(\tilde{x}\) representing foreground risk and \(\tilde{y}\) representing background risk. We also assume that the derivatives of utility, beginning with u′(z) > 0, are continuous and alternate in sign up to the relevant degree. We let r(z) = − u′′(z)/u′(z) denote the Arrow–Pratt measure of absolute risk aversion, p(z) = − u′′′(z)/u′′(z) the measure of absolute prudence, and t(z) = − u′′′′(z)/u′′′(z) the measure of absolute temperance. Let F
1(x) (respectively F
2(x)) be the initial (final) cumulative distribution function for \(\tilde{x},\) and let G
1(y) (respectively G
2(y)) be the independent, initial (final) distribution function for \(\tilde{y}\). We shall denote the random variables \(\tilde{x}\) and \(\tilde{y}\) by \(\tilde{x}_i\) and \(\tilde{y}_{j}\) when their cumulative distribution functions are F

i
(x) and G

j
(y). We restrict all foreground risks to one compact interval and all background risks to another. For notational purposes, let R

j
(x) denote the measure of absolute risk aversion for the derived foreground utility function \(\upsilon_j(x)= Eu(x+\tilde{y_{j}}) = \int u(x + y)dG_{j}(y)\), and let \(Eu(\tilde{x_i}+\tilde{y_{j}})\) denote \(\int\int u(x \!+\! y)\)
\(dG_{j}(y)dF_i(x)=\int \upsilon_j(x)dF_i(x)\). As Gollier and Pratt make clear, the thought experiments that lead to greater aversion to bearing a foreground risk after the introduction of a background risk stipulate that an undesirable foreground risk must remain undesirable after the addition of a particular type of background risk. To extend these thought experiments to deteriorations of an existing background risk in the presence of a foreground risk, we need to show that any change in a given foreground risk that is undesirable for υ
1 is undesirable for υ
2 when \(\tilde{y}_2\) is a particular type of deterioration in an existing background risk \(\tilde{y}_1\). To that end, we first establish the following preliminary result showing that any change in a given foreground risk that is undesirable for υ
1 is undesirable for υ
2 if, for all x, υ
2(x) has a higher index of absolute risk aversion than υ
1(x).Footnote 2
 
 Given F
1(x), any change to F
2(x) that induces an SSD deterioration in the distribution of utility 
\(\upsilon_1(\tilde{x})\)
 reduces the expected value of utility 
\(\upsilon_2(\tilde{x})\) (i.e. 
\(Eu(\tilde{x_2}+\tilde{y_2}) \leq Eu(\tilde{x_1}+\tilde{y_2}))\)
 if 
 
 Given that F
1(x) is strictly monotonic, this condition is necessary in order that any such change reduces the expected value of utility 
\(\upsilon_2(\tilde{x})\)
. 
 This result is related to Theorem 3 of Diamond and Stiglitz (1974) that characterizes greater risk aversion by reference to changes in F
1(x) that induce mean-preserving spreads in the distribution of utility. By contrast, we are concerned with the broader class of changes that are undesirable, so as to include introductions of foreground risk. Note that the lemma is quite independent of the specific natures of the background risks G
1(y) and G
2(y). We are interested in restrictions on utility function u that are sufficient for inequality 1 to hold when υ
2 differs from υ
1 as a result of particular types of changes in the background risk G
1(y).",11
36.0,2.0,Journal of Risk and Uncertainty,26 February 2008,https://link.springer.com/article/10.1007/s11166-008-9035-z,What impacts the impact of rare events,April 2008,Ido Erev,Ira Glozman,Ralph Hertwig,Male,Female,Male,Mix,,
36.0,2.0,Journal of Risk and Uncertainty,05 February 2008,https://link.springer.com/article/10.1007/s11166-008-9032-2,Cancer premiums and latency effects: A risk tradeoff approach for valuing reductions in fatal cancer risks,April 2008,George Van Houtven,Melonie B. Sullivan,Chris Dockins,Male,Female,,Mix,,
36.0,2.0,Journal of Risk and Uncertainty,27 February 2008,https://link.springer.com/article/10.1007/s11166-008-9036-y,"Generalized expected utility, heteroscedastic error, and path dependence in risky choice",April 2008,David Buschena,David Zilberman,,Male,Male,Unknown,Male,,6
36.0,3.0,Journal of Risk and Uncertainty,14 May 2008,https://link.springer.com/article/10.1007/s11166-008-9040-2,Third-generation prospect theory,June 2008,Ulrich Schmidt,Chris Starmer,Robert Sugden,Male,,Male,Mix,,
36.0,3.0,Journal of Risk and Uncertainty,03 May 2008,https://link.springer.com/article/10.1007/s11166-008-9038-9,Causes of ambiguity aversion: Known versus unknown preferences,June 2008,Stefan T. Trautmann,Ferdinand M. Vieider,Peter P. Wakker,Male,Male,Male,Male,"A central point in the explanation of ambiguity aversion concerns the perceived informational content of the outcome generating process. People shy away from processes about which they think they have insufficient information (Frisch and Baron 1988). This happens in particular if an alternative process with a higher perceived informational content is available (Chow and Sarin 2001; Fox and Tversky 1995; Fox and Weber 2002). The effect appears to be particularly strong when somebody with a higher knowledge of the outcome generating process may serve as a comparison (Heath and Tversky 1991; Taylor 1995) or observes the decision (Chow and Sarin 2002). In Ellsberg’s (1961) example the effect leads to preference for the urn with a known probability of winning, about which subjects feel more knowledgeable. A preference for the more informative process may be explained by fear of negative evaluation, which is driven by the expectation that one’s actions or judgments may be difficult to justify in front of others. When the audience’s views on an issue are unknown and no prior commitment to one course of action exists, people have been found to make the decision which they deem most easily justifiable to others rather than the one that is intrinsically optimal (Shafir et al. 1993; Simonson 1989; Lerner and Tetlock 1999). In this way they minimize the risk of being judged negatively by others on their quality as decision makers. Choosing the unfamiliar process entailed by the ambiguous urn may lead to embarrassment if a losing outcome should obtain (Ellsberg 1963; Fellner 1961; Heath and Tversky 1991; Roberts 1963; Tetlock 1991; Toda and Shuford 1965). The risky prospect is perceived as more justifiable than the ambiguous one because potentially available probabilistic information is missing from the ambiguous urn (Frisch and Baron 1988). This is consistent with people’s preference for betting on future events rather than on past events, given that information about past events is potentially available whereas the future has yet to materialize (Brun and Teigen 1990; Rothbart and Snyder 1970). It is also consistent with people’s unwillingness to act on the basis of ambiguous information (van Dijk and Zeelenberg 2003). A decision based on more information is generally perceived as better (Tetlock and Boettger 1989), and it has been shown that a risky prospect is generally considered preferable to an ambiguous one by a majority of people (Keren and Gerritsen 1999). Kocher and Trautmann (2007) find that people correctly anticipate these negative attitudes towards ambiguity. If a bad outcome were to result from a prospect about which an agent had comparatively little knowledge, her failure may be blamed on her incompetence or ‘uninformed’ choice (Baron and Hershey 1988). A bad outcome resulting from a risky prospect, on the other hand, cannot be attributed to poor judgment. All possible information about the risky prospect was known, and a failure is simply bad luck (Heath and Tversky 1991; Toda and Shuford 1965). FNE is difficult to eliminate completely, because people naturally expect to make their choices in a social context. This may explain the pervasiveness of ambiguity aversion. Curley et al. (1986) found that letting more people observe the decision increased ambiguity aversion. To determine to what extent ambiguity aversion can exist beyond FNE, however, FNE should be completely eliminated. This will be achieved in our main experiment (Experiment 2). First, however, we present an experiment that replicates the findings of Curley et al. (1986) in a slightly different setup, and that shows that FNE also can arise with hypothetical choice.",85
36.0,3.0,Journal of Risk and Uncertainty,07 May 2008,https://link.springer.com/article/10.1007/s11166-008-9039-8,A tractable method to measure utility and loss aversion under prospect theory,June 2008,Mohammed Abdellaoui,Han Bleichrodt,Olivier L’Haridon,Male,,Male,Mix,,
36.0,3.0,Journal of Risk and Uncertainty,17 May 2008,https://link.springer.com/article/10.1007/s11166-008-9037-x,On the intensity of downside risk aversion,June 2008,David Crainich,Louis Eeckhoudt,,Male,Male,Unknown,Male,"From Menezes, Geiss and Tressler (1980) and its presentation in Eeckhoudt and Schlesinger (2006) (from now on E-S) we know that in the expected utility model: 
 where x is initial wealth, k is a positive constant and \(\widetilde{ \epsilon }\) a zero mean risk. In E-S’s terminology the left hand side (LHS) of Eq. 1 is preferred to the right hand side (RHS) because for the LHS term, the pains (− k and \(\widetilde{\epsilon }\,\)) are “better apportioned” than for the RHS term. More precisely, on the LHS, the pains are disaggregated while it is not the case on the RHS where they are concentrated on a single state of nature. Since with U
″′ > 0 welfare is higher on the LHS of Eq. 1 we can then search for the amount of money that compensates for the misallocation of the pains on the RHS of Eq. 1. If this amount of money — denoted m — is received in the best state (x) on the RHS of Eq. 1, it is the solution of:Footnote 3
 For small risks \(\widetilde{\epsilon }\) and applying a second order approximation à la Arrow-Pratt we obtain: 
 Multiplying by 2 on each side and simplifying the common term U(x − k), we are left with: 
 or, after a first order approximation around x: 
  so that: 
 Of course as expected U
″′(x) > 0 implies m > 0 and \( \frac{U^{\prime \prime \prime }(x)}{U^{\prime }(x)}\) is the strength of the D.R.A. motive which confirms from another point of view the interest of the measure proposed by Modica and Scarsini. Of course, the higher the coefficient of D.R.A. (\(\frac{U^{\prime \prime \prime }(x)}{U^{\prime }(x)}\) ) the larger the compensation required to accept a misallocation of the pains. Many readers may rightly wonder why skewness does not appear in the approximation formula since it involves third order terms.Footnote 4 In fact, as shown in the Appendix, the expression \(\frac{ \sigma ^{2}}{2}k\) is proportional to the change in skewness induced by the misapportionment of the risk. Keeping this in mind, the approximation of m, the cost of the misallocation of the pain, appears to be determined by two factors:
 the importance of the change in skewness (a statistical property). the intensity of the D.R.A. coefficient expressed by \(\frac{U^{\prime \prime \prime }(x)}{U^{\prime }(x)}\) (a preference property). Seen from this point of view, our result about the approximation of m exactly parallels the approximation of the risk premium in Arrow-Pratt.",85
37.0,1.0,Journal of Risk and Uncertainty,08 July 2008,https://link.springer.com/article/10.1007/s11166-008-9044-y,Resource allocation when projects have ranges of increasing returns,August 2008,Catherine Bobtcheff,Christian Gollier,Richard Zeckhauser,Female,Male,Male,Mix,,
37.0,1.0,Journal of Risk and Uncertainty,17 June 2008,https://link.springer.com/article/10.1007/s11166-008-9042-0,One-reason decision-making: Modeling violations of expected utility theory,August 2008,Konstantinos V. Katsikopoulos,Gerd Gigerenzer,,Male,Male,Unknown,Male,"Assumptions 1 to 3 are commonplace. Why do we propose they be reconsidered? In this section, we discuss two reasons: empirical evidence for people’s use of heuristics that violate these assumptions and prescriptive reasons for why these heuristics can make quick and accurate predictions. People often do not evaluate options in isolation but instead relative to at least one other option. For instance, when judging the value or size of objects, ratings are more inconsistent—both within and between individuals—when objects are evaluated independently rather than in comparison to other objects (e.g., Gigerenzer and Richter 1990). Different choices are made depending on the other options in the choice set (Shafir et al. 1993) and on the other options preceding an option when these are sequentially presented (Schwarz 1999; Schwarz et al. 1985). Regret theory (Loomes and Sugden 1987) and range-frequency theory (Parducci 1965) both model the relative evaluation of options. Based on this and other evidence, Luce and von Winterfeldt (1994, p. 267) conclude that “no theory that is based on separate evaluations of gambles can possibly work.” Thus, psychologically, a class of situations exists in which an option has a value only relative to other options. The second assumption is that the value of an option is calculated by searching for all available information. This assumption is unrealistic in many contexts, such as on the Internet, where there is too much information and limited search is necessary. Similarly, experimental research has shown that people do not search exhaustively but employ limited search, both in internal search (in memory) and in external search (e.g., in libraries; Payne et al. 1993). A number of theories have modeled limited search, both within the framework of optimization (Stigler 1961) and satisfying (Simon 1955, 1956). In the extreme, search could terminate after the first reason that allows for a decision, thus making no trade-offs. Bröder (2000, 2003; Bröder and Schiffer 2003) report that under various conditions (e.g., time pressure, high information costs) a majority of people rely on lexicographic heuristics that look up one reason at a time and stop as soon as a reason allows them to do so. Rieskamp and Otto (2006) and Rieskamp and Hoffrage (1999) show that people adapt the length of search to the structure of the problem. Third, the experimental evidence shows that people often do not make trade-offs but base their decisions on heuristics that are “non-compensatory,” which means that low values on one attribute (value or probability) cannot be compensated by high values on others. These no-trade-off heuristics include lexicographic models, conjunctive rules, disjunctive rules, and elimination-by-aspects (see also Lilly 1994). Consider this classic review of 45 studies in which the process of decision-making was investigated by means of Mouselab, eye movement, and other process tracking techniques (Ford et al. 1989). Varying between studies, the choices to be made included apartments, microwaves, and birth control methods: “The results firmly demonstrate that non-compensatory strategies were the dominant mode used by decision makers. Compensatory strategies were typically used only when the number of alternatives and dimensions were small or after a number of alternatives have been eliminated from consideration.” (p. 75). Consistent with this result, most subsequent studies that reported trade-offs have used only a small number of attributes (typically only 2 to 4) and have fitted the data by means of conjoint analysis or other linear models without testing lexicographic or other no-trade-off models. Studies that investigated consumer choice on the Internet and in other situations with large numbers of alternatives and cues—and that tested models with stopping rules—concluded that a majority of participants followed non-compensatory processes. For instance, Yee et al. (2007) reported that when people had a choice between 32 SmartPhones that varied on six cues, non-compensatory models predicted their choices better than Bayesian and other models that assume trade-offs did. Similarly, when people chose between cameras varying on seven attributes with two to six levels each, non-compensatory strategies again provided the best prediction: 58% relied on one attribute only, 33% relied on two attributes, and only 2% used three attributes (Gilbride and Allenby 2004). Experiments in which participants chose a product (such as an answering machine or toaster) from the Web sites CompareNet and Jango showed the same result: The larger the number of alternatives offered, the more customers relied on a no-trade-off strategy (Jedetski et al. 2002). Bröder and his colleagues (Bröder 2000; Bröder and Schiffer 2003) conducted 20 studies, concluding that a lexicographic heuristic, Take The Best, is used under a number of conditions such as when information is costly and the variability of the validity of the attributes is high. Bröder and Gaissmaier (2007) and Nosofsky and Bergert (2007) showed that Take The Best predicts response times better than weighted additive and exemplar models. Thus, the experimental evidence strongly suggests that heuristics that rely on limited search and do not make trade-offs are in people’s “adaptive toolbox” (Gigerenzer and Selten 2001), and that these heuristics are selected in a sensitive way according to the structure of the problem (Gigerenzer et al. 1999; Lopes 1995; Payne et al. 1993). The empirical evidence cited above shows that people proceed differently from assumptions 1 to 3. Relying on limited search and foregoing trade-offs, however, does not generally imply that these decisions are inferior or irrational. First, institutions routinely apply lexicographic rules in designing environments in order to make them safer and more transparent and allow human minds to operate in an efficient way. Which vehicle has the right of way at a crossing is defined by a lexicographic rule, not by a trade-off between the police officer’s hand signal, the color of the traffic light, the traffic sign, and where the other car is coming from. Similarly, in soccer and hockey, the national and international associations agreed on lexicographic rules to determine the final standing within a group of competing teams. The Arabic number system allows using a simple lexicographic rule to decide which of two numbers is larger, employing order and limited search unavailable in other systems. Second, lexicographic heuristics can also be accurate. Under certain conditions, they are more accurate than multiple regression and other linear models that make tradeoffs (Gigerenzer et al. 1999; Martignon and Hoffrage 2002; Hogarth and Karelaia 2005, 2006; Baucells et al. 2008), as well as nonlinear models such as neural networks and classification and regression trees (Brighton 2006). Lexicographic heuristics can even be optimal (Katsikopoulos and Martignon 2006). We would like to emphasize these results, given that ever since lexicographic rules were first proposed in economics by Carl Menger, decision researchers have often dismissed them as a form of irrationality (see also Fishburn 1974). But how can it be that heuristics are accurate? In fact, there are good mathematical reasons for their accuracy. First, the heuristics tend to be robust. That is, they do not lose much of their accuracy between fitting known data and predicting new data. In contrast, models with numerous free parameters tend to over-fit the data and lose accuracy in prediction (Roberts and Pashler 2000). Second, lexicographic heuristics can exploit a number of structural properties, such as the presence of cumulatively dominating options in the choice set (Baucells et al. 2008) or large differences in the statistical informativeness of attributes (Martignon and Hoffrage 2002; Hogarth and Karelaia 2005, 2006; Katsikopoulos and Fasolo 2006; Katsikopoulos and Martignon 2006). Simulation studies have shown that these properties are relatively common. A major unresolved problem in the tradition of revising EVT and EUT by using free parameters is that none of the estimated sets of parameters in models such as cumulative prospect theory can simultaneously account for buying lottery tickets, buying insurance policies, the Allais paradox, and other choice patterns observed in the literature (Neilson and Stowe 2002). For instance, the functions estimated by Camerer and Ho (1994) and Wu and Gonzalez (1996) imply that people will purchase neither lottery tickets nor insurance policies. Moreover, Neilson and Stowe (2002) concluded that the troubles run deeper; they showed that no parameter combinations allow for these two behaviors and a series of choices made by a large majority of participants and reasonable risk premia. Similarly, Blavatskyy (2005) showed that the conventional parameterizations of cumulative prospect theory do not explain the St. Petersburg paradox. Overall, the parameter values fitted to one set of data are unlikely to be robust, in the sense of generating accurate predictions for new sets of data. On the other hand, simple heuristics such as the priority heuristic have no free parameters and tend to be robust (Gigerenzer et al. 1999; Martignon and Hoffrage 2002).",59
37.0,1.0,Journal of Risk and Uncertainty,09 May 2008,https://link.springer.com/article/10.1007/s11166-008-9041-1,Myopic risk-seeking: The impact of narrow decision bracketing on lottery play,August 2008,Emily Haisley,Romel Mostafa,George Loewenstein,Female,Male,Male,Mix,,
37.0,1.0,Journal of Risk and Uncertainty,11 July 2008,https://link.springer.com/article/10.1007/s11166-008-9043-z,An experimental investigation of violations of transitivity in choice under uncertainty,August 2008,Michael H. Birnbaum,Ulrich Schmidt,,Male,Male,Unknown,Male,"The decision-maker chooses between alternatives, A and B, which yield different consequences depending on the state of the world. Assume there are n mutually exclusive and exhaustive states, E

i
, and that consequences may be contingent on both the choice and the state of the world. Let A = (E
1, a
1; E
2, a
2;..;E

n
, a

n
) and B = (E
1, b
1; E
2, b
2;..;E

n
, b

n
) represent the contingency between E

i
 and the consequences for the alternatives. If B were chosen, the consequence under E

i
 would be b

i
, instead of a

i
. Regret theory and majority rule are both special cases of an integrative contrast model, which can be written as follows:
 Where A ≻ B denotes A is preferred to B, where a

i
 and b

i
 are the consequences of A and B for state of the world E

i
, ϕ (E

i
) is the subjective probability of this state of the world E

i
, and ψ maps pairs of consequences (psychological contrasts between a

i
 and b

i
 for a given state of the world) into psychological preferences. It is assumed that ψ(a, b) = −ψ(b, a) and ψ(a, b) = 0 ⇔ a = b. When probabilities are known, it is assumed that ϕ (E

i
) = p

i
, where p

i
 is the probability of E

i
. See Fishburn (1982, 1991) for analysis of closely related nontransitive representations. According to regret theory (Loomes and Sugden 1982; Bell 1982), people compare the prizes for each state of the world and make choices in order to minimize regret. It is assumed that regrets are particularly large for large contrasts in consequences. For all a ≻ b ≻ c, it is assumed that ψ(a, c) > ψ(a, b) + ψ(b, c). This assumption is called “regret aversion.” The following special case of regret theory can be used to illustrate its implications:
 where p

i
 is the probability that the corresponding state of the world occurs; x

i
 and y

i
 are the cash payoffs of choosing A and B in state of the world, E

i
, respectively. Note that in this case, large differences in payoff produce extra large regrets (i.e. the regret function is convex), as proposed by regret theory. Note as well that the cubic function retains the signs (directions) of the regrets. Consider the first three choices in Table 1 (Choices 11, 5, and 13). These choices were defined with respect to an urn containing 100 tickets numbered from #1 to #100, which were otherwise identical, from which one ticket would be drawn randomly to determine the prize. The first gamble of the first row (Choice 11) indicates that if the ticket drawn were #1 to #30, the prize is $3, #31–60, the prize is $3, and if it is #61–100, the prize is $10. For the second alternative, these ticket ranges yield prizes of $1, $7.50, and $7.50, respectively. Loomes, Starmer, and Sugden (1991) reported that these choices produced the greatest percentage of intransitive cycles (28%, see p. 437). In addition, this set was chosen because the observed incidence of this intransitive cycle exceeded the frequency of the most common transitive preference pattern that differed from it by only one choice. According to Eqs. 1 and 2, \(\sum\nolimits_{i = 1}^n {p_i \left( {x_i - y_i } \right)^3 } = - 18.7\), so B ≻ A; \(\sum\nolimits_{i = 1}^n {p_i \left( {x_i - y_i } \right)^3 } = - 8.3\), so C ≻ B; however, \(\sum\nolimits_{i = 1}^n {p_i \left( {x_i - y_i } \right)^3 } = 45.2\), so A ≻ C, violating transitivity. This special case model of regret thus reproduces the violation of transitivity in this case.
 The majority rule model (sometimes called the most probable winner model) is also a special case of Eq. 1 in which the contrast functions are as follows:
 According to this model applied to the first three choices of Table 1, people should prefer A to B because it has higher values on two of the three dimensions. Similarly, they should prefer B to C, and C to A, for the same reasons. Thus, majority rule also predicts violations of transitivity, but of the opposite pattern from that predicted by regret theory. [In this case, \(\sum\nolimits_{i = 1}^n {p_i } \psi \left( {a_i ,\;b_i } \right) = 0.4\), \(\sum\nolimits_{i = 1}^n {p_i } \psi \left( {b_i ,\;c_i } \right) = 0.4\), yet \(\sum\nolimits_{i = 1}^n {p_i } \psi \left( {a_i ,\;c_i } \right) = - 0.2\); therefore, A ≻ B, B ≻ C, but C ≻ A.] A problem in previous empirical tests of regret theory is that certain confounds were present in those studies (Humphrey 2001; Starmer and Sugden 1998). Probably the most important problem was that different forms of the gambles were used in different choices. A and B were presented for comparison as three-branch gambles: A = ($10, 0.4; $3, 0.3; $3, 0.3), B = ($7.5, 0.4; $7.5, 0.3; $1, 0.3). However, the so-called choice between B and C was actually presented in a form in which the two upper branches of B and C were coalesced, creating two new gambles, B′ = ($7.5, 0.7; $1, 0.3), C′ = ($5, 0.7; $5, 0.3). The choice between C = ($5, 0.4; $5, 0.3; $5, 0.3) and A was presented with the two lower branches coalesced, creating two other new gambles, C′′ and A′′ where C′′ = ($5, 0.4; $5, 0.6), and A′′ = ($10, 0.4; $3, 0.6). According to the transitive TAX model, with parameters taken from previous data (see Birnbaum and Navarrete 1998, p. 57), splitting and coalescing of branches could account for the apparent violations of transitivity. According to this TAX model we have U(A) = 4.33, U(B) = 5.33; U(C) = 5.00; U(B′) = 3.79; U(C′) = 5.00, U(A′′) = 5.01; U(C′′) = 5.00. Thus, this TAX model implies that, A ≻ B, B′ ≻ C′ and C′′ ≻ A′′, so it reproduces the results that were called evidence of intransitivity with the assumption that the results are due instead to violations of coalescing. In this paper, we keep all gambles in the same three-branch form to avoid this confound with coalescing. Starmer and Sugden (1998) and Humphrey (2001) recognized this confound and controlled for it by presenting choices in fully split forms or by using a different format for display (“strip”) in which gambles were presented in fully coalesced form. But those articles had a second problem; namely, they used asymmetry of different types of intransitivity as evidence of intransitivity. As we show in the next section, such asymmetry is entirely compatible with an error model in which people make occasional “errors” in determining or reporting their preferences, even if they are truly transitive.",39
37.0,1.0,Journal of Risk and Uncertainty,29 July 2008,https://link.springer.com/article/10.1007/s11166-008-9046-9,Resource allocations when projects have ranges of increasing returns,August 2008,Catherine Bobtcheff,Christian Gollier,Richard Zeckhauser,Female,Male,Male,Mix,,
37.0,2.0,Journal of Risk and Uncertainty,25 September 2008,https://link.springer.com/article/10.1007/s11166-008-9055-8,Discounting dilemmas: Editors’ introduction,December 2008,Richard J. Zeckhauser,W. Kip Viscusi,,Male,Unknown,Unknown,Male,"Many of the most important societal problems will play out over extremely long time horizons. What happens tomorrow is uncertain. What happens over 15,000 tomorrows is massively uncertain, and so it is with climate change, the dangers of killer asteroids, and year 2050 energy sources and cancer cures. The normative analyses presented in this issue, which in some cases are informed by behavioral studies, include contributions by Samuelson, Summers and Zeckhauser, Dasgupta, Gollier, and Fels and Zeckhauser. These articles develop the general economic principles for discounting, particularly when there are risks to distant generations. The central role of the discount rate in environmental policy assessments is well illustrated by simple calculations of the value of $1 in benefits in 50 years and in 100 years. With a time discount rate of 3%, which is consistent with the lower value in OMB guidelines and calculations of climate change effects by economists such as Nordhaus (1994), the present value of $1 equals $0.23 in 50 years, and $0.05 in 100 years. The alternative OMB discount rate of 7% leads to a present value of $1 equal to $0.03 after 50 years and $0.001 in 100 years. The famed Stern Report (Stern et al. 2007) on climate change utilized a time discount rate of 0.1% for tallying benefits, which is close to not discounting at all, and is a low outlier in the debate.Footnote 8 Using that rate, the present value of $1 in benefits is $0.95 if they are received in 50 years and $0.90 if they are received in 100 years. Stern’s positive, albeit miniscule, rate comes not from time preference, nor because we value future generations less than our own. Such preferences he finds ethically indefensible, and draws support from Ramsey, Pigou and later distinguished writers. Rather, it allows for the possibility that human life may not exist in the future, perhaps smitten by a meteorite (Stern et al. (2007), pp. 35–37, 53). If we exclude discount rates for benefits such as 7% as being too high, which most economists believe is the case, then the main intellectual battleground is whether we should be using discount rates in the range of 3% or discounting by only a token nonzero amount, such as 0.1%. The potentially dominant role of discounting in long-term policy analyses, in conjunction with the embodied substantial uncertainties, is strikingly illustrated in the U.S. Environmental Protection Agency’s (EPA) analysis of nuclear waste storage at Yucca Mountain, a site about 100 miles northwest of Las Vegas, Nevada.Footnote 9 EPA initially prepared a regulatory analysis providing for the sufficient safety of the nuclear waste storage for 10,000 years, but in response to a Federal court decision it extended the period for analysis demonstrating safe storage to 1 million years. To put these time frames in perspective, recorded human history spans only 5,000 years, and homo sapiens have walked the earth for a mere 120,000 years. Even at the more immediate 10,000 year time period, a $1 benefit valued at a 3% discount rate would have a present value of only $4.2 × 10−129. The present economic value of exposing the entire current U.S. population to a lethal dose of radiation in 10,000 years is so small that it would drop out of the analysis at any reasonable discount rate. Perhaps because discounting would obliterate the value of such remote benefits, EPA didn’t provide a present value calculation of the 1 million years of secure storage. The EPA analysis assures us that the nuclear waste stored underground at Yucca Mountain will expose nobody in the next 10,000 years to a radiation level of over 15 millirems of radiation. Even more striking is its assurance that nobody in the next million years will be exposed to more than 350 millirems. That is half the average background radiation risk currently faced by Colorado residents. EPA avoided the question, which a skeptical economist might pose, of what probabilities should be attached to such terms as “nobody” and “secure” when applied to million-year periods. As we move from 100 years to 10,000 years, the crystal ball for safety goes from cloudy to pitch black. And million year projections surely transform regulatory impact analyses from bureaucratic documents into science fiction fantasies. The articles in this issue stop well short of fantasia; they reside in plausibilia. Thus, they address time frames going from days to decades and from decades to centuries, but never get close to millennia. Challenges raised by greenhouse gas emissions are the prime policy issue addressed in these papers. Many policy problems have long-term consequences, but few have crystallized intellectual examination of the far future, particularly by economists, as strongly as has the potential for climate change. It is quite possible that actions taken today, notably the emission of greenhouse gases, will have significant consequences on people living 100 or 200 years hence. Here you will find Summers and Zeckhauser (2008), drawing on both psychological and philosophical principles, who suggest that the weights put on the welfares of far future generations should be much closer to those of nearby generations than any traditional discounting formulation would produce. Gollier (2008), employing a neoclassical model, shows how appropriate concern for the distribution and persistence of future growth rates will lead to a systematic decline in discount rates as the future stretches forward. Fels and Zeckhauser (2008) put a philosophical nail in the constant-discount-rate coffin. They show that perfect altruism (treating all pairs of successive generations alike) is inconsistent with total altruism (valuing other generations’ altruistic preferences, not merely their felicities from consumption). These three papers, and most notably that of Dasgupta (2008), should be viewed alongside the Stern Report. Dasgupta reviews the climate change discount rate controversy the Stern Report raised, and then wades into the subsequent swirl of economic debate. We now review our normative papers in more detail. Samuelson conducts an historical tour of discounting concepts. He observes that behavior that shortchanges savings and other excessively impatient or Pollyannish actions had been observed, bemoaned and exploited eons before credit cards entered our wallets or myopia the economist’s vocabulary. He observes that patience and prosperity will be bed partners, as will their opposites, and then generalizes the lesson, identifying shortsighted choices in areas ranging from sex to smoking. He then derides laws against usury, taking kings, the Old Testament, a couple of religions, and Karl Marx to task. Chicago economics gets poked along the way. Samuelson—capitalizing on his multiple-decade perspective—closes by identifying optimal life-cycle investing strategy: keep steady allocations as you go. Summers and Zeckhauser take on the problem of policymaking for the long-term future, what they label posterity. They employ a variety of paradigmatic choice problems to illustrate the problems and insights into methods for valuing future consequences. Thus, they ask such questions as: (1) If given a choice would you destroy a threatening comet to return in 100 years, or one in 300 years, if the latter represented a 10 times greater threat? (2) Why do you treat dropping litter so differently from failing to pick it up? (3) What fraction of your income would you give up to avoid a 1% chance of a cataclysm that would kill you as it made the human species extinct? (4) If your predecessors set a noble precedent by saving more for you, will you follow it, hoping that strengthens the precedent for your descendants, or would you free ride by consuming some of your inheritance? They use such questions, traditional economics and behavioral economics to address policymaking for posterity, first for a certain world and then for an uncertain world. The example of actions against climate change suffuses their discussion. Under certainty, they find a case for giving more weight to the distant future, taking more actions on its behalf, than would be implied by choosing “reasonable” parameters for the fundamental discounting equation. Their conclusions from the uncertainty analysis push in opposite directions from conventional wisdom, primarily because they find that in a von Neumann-Morgenstern framework logic does not support assigning infinite negative value to even the extinction of human life. Their model of uncertainty and learning, developed for the climate change example, shows that greater uncertainty may increase or decrease the optimal percentage of total expected mitigation undertaken in the first period. Summers and Zeckhauser close with a discussion of reaction function issues. That is, they assess how current efforts to help the future, say to curb greenhouse gas emissions, will affect alternative investments of benefit to the future, actions by other generations and other nations, and technological developments. Dasgupta develops a model of how time preference and societal preferences for income distribution across time should influence the valuation of such deferred policy impacts. His framework is driven by key ethical assumptions, relating in particular to intergenerational inequality in consumption. In Dasgupta’s model, the consumption discount rate, which determines the weights placed on consumption levels at different times, plays a key role. In any year t, it is the sum of the time discount rate and a term that is the product of the predicted growth in consumption from year t to t + 1 and the elasticity of what he calls marginal felicity, i.e., instantaneous marginal utility of consumption. Dasgupta favors a time discount rate of approximately zero, which accords better with the Stern Report’s 0.1% value than the U.S. government-recommended time discount rates. However, Dasgupta departs from the Stern Report approach in that he favors a higher value of the elasticity of marginal felicity than used in past climate change studies. Therefore, he would place a very high social weight on avoiding the risk of inequality of consumption levels for future generations. Dasgupta’s belief that there should be a very strong concern with possible future inequality seems initially inconsistent with the current generation’s willingness to tolerate substantial current income inequality across nations. However, he discusses this observation and suggests that our willingness to reduce inequality for future generations should be greater than our apparent willingness to fund efforts to eliminate dire poverty, say in African countries. His elegant formulation requires economists to concern themselves not merely with estimating the highly uncertain climate change effects, but also with the temporal trajectory of income inequality, a theme echoed by the next article. Gollier follows the classic discounting approach, which posits that a dollar today is worth more than a dollar tomorrow, because the marginal utility of consumption is decreasing when consumption grows over time. If consumption growth follows a Markov process, everything is copasetic: constant discounting does an excellent job given an appropriate CRRA utility function. Gollier observes that prosperity may be just around the corner, and many corners after that. But history is also filled with long slumps and slowdowns, often associated with afflictions such as wars and plagues. He then develops the theory of appropriate discounting when there are future uncertain shocks to growth. With only this slight but realistic departure from standard assumptions, Gollier is able to show that as time stretches forward the discount rates we should apply at present will drop. Basically, that is due to an asymmetry between the effects of high and low growth rates. Call the average long-term growth rate middle. Looking to the far distance, an uptick in growth rates will matter little; a dollar will be worth virtually nothing with either rate. But if the growth rate oozes to low over a long period, and sticks there, a future dollar will gain significant value, in part because of risk aversion and disappointing consumption. Gollier divides discount rates into two components, a wealth effect and a precautionary effect. The latter protects against low growth rates. Alternative specifications have proper discount rates falling from 4.3% to 3.4% over 100 years, and from 3.5% to 1% over a millennium. Fels and Zeckhauser take altruism seriously. They posit that generations, or for that matter altruistic individuals, should respect the preferences of those for whom they have altruism. Thus, if those generations are altruistic, their full utility function should be considered, not just the felicity they get from own consumption. They show that this concept, total altruism, can yield a functional form that depends only on each generation’s felicity of consumption. They show, however, that what has been labeled perfect altruism, namely that a generation value itself relative to a successor the way it values any two successive generations, is incompatible with total altruism. They close with a philosophical pitch for having altruism stretch backward as well as forward, and claim that the real world at times respects such backward looking preferences.",24
37.0,2.0,Journal of Risk and Uncertainty,07 August 2008,https://link.springer.com/article/10.1007/s11166-008-9047-8,Asymmetric or symmetric time preference and discounting in many facets of economic theory: A miscellany,December 2008,Paul A. Samuelson,,,Male,Unknown,Unknown,Male,"The busy bees work hard all summer long to fill their winter honeycombs. Grasshoppers, however, consume all they can consume. (Maybe not so dumb if grasshoppers are scheduled to die before autumn is over.) I once asked my mother-in-law, who was omniscient about her Wisconsin hometown neighbors, whether a certain local was a tightwad. She replied, “Yes, indeed. He saves every year even if in a lean year he has to borrow to save.” By contrast there once was a legendary New England village pariah who had done the most awful thing—namely, he had trenched on family capital. The early utilitarian Jeremy Bentham contrasted “impure pleasures” and “pure pleasures.” Among the latter he was not judgmental, agreeing that doggerel and epic poetry were equally valuable if you thought them to be. An impure pleasure he defined as one that pleased you in the beginning but during subsequent times brought you even weightier dis-utilities.",5
37.0,2.0,Journal of Risk and Uncertainty,30 September 2008,https://link.springer.com/article/10.1007/s11166-008-9052-y,Policymaking for posterity,December 2008,Lawrence Summers,Richard Zeckhauser,,Male,Male,Unknown,Male,"Economists’ standard approach to comparing future costs and benefits is reflected in what might be labeled the fundamental discounting equation:
 where ρ is the discount rate to be applied in valuing future per-capita consumption dollars, δ is taken as a measure of pure time preference, g is the rate of growth of per capita income, and η is minus the elasticity of marginal utility with respect to consumption. In essence the equation says that a dollar in the future is worth less than a dollar for two reasons: First, the future increments to consumption are discounted because of pure impatience. The δ term would also reflect the fact that we give less weight to those alive in the future than we give to ourselves. The second term shows that increments to future consumption are reduced in value because the future will be richer, implying that an increment to consumption will be worth less in terms of marginal utility. There has been much discussion of the appropriate assumptions to make regarding each of these parameters, and of consequences of uncertainty regarding them. The discussions quickly become philosophical. For example—is δ to be thought of as reflecting the value that those alive today place on the welfare of future generations or to represent the values that an imagined impartial ethical observer, with no particular generational connection, places on different generations? Weitzman (2007) suggests as an easy-to-remember triad of values δ = .02, g = .02 and η = 2 which together imply a discount rate of 6% a year, a sufficiently high value to render anything that happens a century from now almost irrelevant as a dollar then is worth less than $0.03 today [0.03 > 0.0029472262 = 1/(1.06)100] (see also Cowen 2008). Others, such as Stern (2007), argue for alternative and much lower parameter values. We do not believe that an argument about these alternative parameter values will resolve our policymaking concerns. Hence, we focus on several issues that we regard as important in making very long-run policy judgments that do not fit naturally into the standard approach. The fundamental discounting equation simply ignores questions relating to the size of the population. We are inclined to believe that the weight attached to the utility of a given generation should be related to its size, and probably proportional. In this case it is necessary to subtract the rate of population growth in discounting future levels of consumption. At the global level or for the USA such a correction would not be inconsequential, as population may well grow at close to 1% a year for the next half century. Population issues raise further critical questions. When making policy for the very long run, how should a country think about those now living abroad who will immigrate and become citizens in the future? How about those whose descendants will become citizens? How about those nationals whose descendants will marry and have children with noncitizens? In an increasingly open world it seems reasonable to suppose that the longer the horizon the more cosmopolitan the perspective needs to be, and therefore the size of the population one should care about should increase. To see this point think about a Boston city councilor who cares only about the welfare of Bostonians. She would give little weight to some benefit Boston might provide non-Boston residents of Massachusetts next year. On the other hand if considering benefits that would only be realized a half century from now, the distinction between Boston and the state seems much less consequential, making the case for a broader perspective and other things equal a lower discount rate. When this argument is extrapolated to the world at large, the issue is not merely that people will move around and intermarry. Some populations will grow much faster than others. How will Europeans or Japanese, or indeed most currently prosperous ethnic groups, weight the future if they conclude (as at least present evidence would suggest) that their percentage of the far-future world population is likely to be much smaller than it is today? We provide no ethical answer, but we would observe that this factor would lead them to weight the future lower. The argument is sometimes made that the future will be far richer than are we, and that therefore providing for them by providing a less tarnished planet makes little sense; it is redistribution in the wrong direction. In essence, this is giving attention to the second term in the discounting equation above. Of course the argument is not that nothing should be done to provide for the future. If this were done it is unlikely that growth would be positive. Rather the argument is that the distant future is so great a beneficiary of the spillover from investments made for the near term future that no special efforts on its behalf are appropriate. When the Greatest Generation made enormous sacrifices to fight World War II, they were preserving liberties in the twenty-first century. But they had sufficient motivation to preserve liberty in the 1940s and 1950s. When Thomas Edison pursued his extraordinary array of inventions, he effectively moved all successive worlds years ahead in their technological development. Thus, the Internet might have been delayed for a decade were it not for him. But Edison was substantially pursuing his personal interests, and we are just a lucky beneficiary. And so it is with all commercial R&D work today. The future benefits because much of the information generated is not appropriable. The standard discounting approach provides the right way to think about the trade offs involved in helping the distant future if what is being traded off is current consumption that is perfectly substitutable with consumption in the distant future. In fact, in most policy contexts the benefits to the future are provided in kind, for example in the form of a protected rather than a despoiled environment. In the environmental case, the benefit to the future of costly actions that we take today needs to be measured in terms of the future’s willingness to pay for the amenity, a factor that is usually overlooked. And this will rise with their income, thereby counterbalancing the diminishing marginal utility of consumption as a valuation of consumption. In general, the effect of income growth on the value of providing a future amenity will be ambiguous. Consider the particular special case where amenity value is separable, so that a generation’s utility can be written as U(c) + a, where c is consumption and a is the amenity. Then policymakers can consider amenity values by using current willingness-to-pay measures and then discounting only at the pure rate of time preference whether coming from impatience or generational selfishness.Footnote 1
 How important is this amenity elasticity issue? Potentially quite important. We would claim on the basis of casual empiricism that not only are societies prepared to pay more for environmental amenities or good health, for example, as they get richer but they are also more willing to sacrifice growth itself. More generally issues of health and the environment certainly loom much larger in rich than in poor countries. As we roll into the future, if the past provides any prologue, production goods will be far cheaper. Looking at what might be called terms of trade, amenity goods will be far more valuable, and doubly so if—as we might expect—the income elasticity of demand for such goods is high. We could see people in the far future paying many dozens of times as much as people would today in terms of dishwashers or televisions sacrificed for an authentic wilderness experience, or a magnificent 70° spring day in New York. When considering time preference in the context of altruistic preferences, it is hardly obvious that the standard discounting approach applies. Following philosophers, we will consider a hypothetical ethics problem to help get our thinking straight. There are two giant comets that will be coming near to the Earth in the next decade, and will return again in the far future. Astronomers calculate that comet A has one chance in 100 of hitting the Earth on its next swing by, which will come in 100 years. Comet B has one chance in 10 of hitting the Earth on its next swing by, which will come in 400 years. If either comet is on course, there will be nothing that can be done at its next coming. Either comet will wipe out all sentient life if it hits. In preparation for killer comets, the government has prepared a single missile-weapon combination that can knock a comet significantly off course. Should it be fired at comet A or comet B? Which comet would you target? Our intuition, and we suspect that of most of our readers, is that we would target Comet B. Yet a policymaker applying any realistic discount rate in the standard framework would choose Comet A using any discount rate even close to or above 1%.Footnote 2 But we would argue that it is not clear that we care about people 300 years from now more than modestly less than those alive 100 years from now. True, destroying A would protect an additional increment of people, namely those living from 100 to 300 years from now, but that hardly would take care of a 10 to 1 disparity in risk. We regard this as at least a tough conundrum. Moreover, unlike many discounting problems, it would not matter much to us how rich these various groups would be. Talk of great future affluence is a dodge often used when not providing for the future. The comet problem convinces us that when we think about people in the far future, we should try to think hard about something like a weighting factor on their welfare. Traditional discounting analysis does not help much. And that weighting factor, at least for many of us, will be far greater than any discounting analysis would provide. Survey results from Cropper et al. (1994) indicate that individuals do attach weights to the welfares of far future generations well above what a discounting approach with any plausible interest rate could accommodate. However, such generous weighting leads to a new conundrum when we recognize that the future potentially involves an infinite number of generations. If beyond some point weights to future generations are not going to decline, then a loss that continues forever, whenever it starts and however small, will count infinitely against benefits today. As noted above, we are willing to give far higher weights to far future generations than would a typical discounting analysis, but we think it essential to have in place some system where a finite loss to each generation over an infinite future counts far less than infinity.Footnote 3
 Such an approach might involve discounting, or thinking about a representative future generation. Enthusiasm for giving nontrivial weighting to a specific far future generation, it seems, may represent some variant of the embeddedness phenomenon well known in the contingent valuation literature: “Different but similar samples of respondents are asked about their willingness to pay for prevention of environmental damage scenarios that are identical except for their scale: different numbers of seabirds saved, different numbers of forest tracts preserved from logging, etc. It is reported that average willingness to pay is often substantial for the smallest scenario presented but is then substantially independent of the size of the damage averted, rising slightly if at all for large changes in size” (Arrow et al. 1993, p. 26). In our context, asking about valuing a cost to a single generation at a far future date may lead to a valuation not much less than asking about that same cost to all generations starting from that far future date. Many observers will argue that climate change is special, that we have more of an obligation to avoid destroying something that has been bequeathed to us by nature than to provide the future with say enhanced intellectual capital (from R&D, say). There are three strong elements to this argument: reference points, loss aversion, and errors of commission. Prospect theory (Kahneman and Tversky 1979) tells us that utility does not attach to the state space, rather to changes from the state space. It tells us as well that individuals treat losses from some reference point as being much more consequential than gains. Every time a climate-change agreement is formulated, there is a profound debate on where should we start, what the reference point should be. Should it be current emissions, 2006 emissions (the baseline for the July 2008 Harvard University plan to curtail greenhouse gases by 30% by 2016), or some other date. When the G-8 countries agreed to a 50% cut by 2050, Japan wanted to start from 2008 levels; the Europeans favored 1990. The agreement specifies no date. Most discussions set a reference point for greenhouse gases, but if significant economic sacrifices are at stake, there are likely to be reference points for those losses as well. Prime ministers and presidents cannot hope to sacrifice GNP for climate control without hearing strong cries of protest from those experiencing loss aversion on their incomes. In growth economies, matters will be easier, because the cuts will come against what would have been their incomes, a moving and hence more fuzzy reference point. The “Save More Tomorrow” plan of Thaler and Benartzi (2001) increases employee savings by having them commit funds out of their future salary increases, presumably something that is less noticed, that serves less as a reference point. Our strong suspicion is that expenditures for climate change will be far easier to make in economies where per capita income is growing. Environmental problems often involve an additional element: One’s purposeful action determines the change. That is where the contrast between errors of omission and commission play such a strong role. Consider the everyday problem of litter in a park. We all would prefer to encounter three rather than a dozen pieces of litter. The omission-commission distinction pushes further. Most citizens would never consider picking up some of those dozen littered pieces. But they would also never litter, whatever the current level. Thus, if a citizen inadvertently dropped a piece of paper she would pick it up, to avoid an act of commission. We thus observe a hierarchy of utility states: Littered partly due to me, littered more than usual, littered the usual amount, clean. For most citizens, the benefit of exiting the lowest state makes it worthwhile to bend over, pick up, and find a trash can. But they would not make that effort to transit upward from other states. The same underlying motivation, we believe, motivates many citizens’ thoughts about climate change: Their normal altruism for the future is reinforced by their guilt in contributing to the problem of greenhouse gases. Consider another example, where concern about an act of commission plays a major role. It is a variant of the famous trolley problem proposed by Foot (1967): A trolley is running out of control down a track, on course to kill five people. You can push a fat man in front of the trolley. He will be killed, but the trolley will be stopped and the five will be saved. Would you push the man? Few people would push the man to his death.Footnote 4 (Informal surveys suggest that economists are more willing to do so.) Commission makes a big difference, but it is not a trump card. If the numbers were 20 and 1, there would be more pushers. The same factors would influence policymaking for posterity. Actions that avoid imposing harm will get extra weight, but the magnitudes of costs and benefits will surely matter as well. The implication of these examples is that in doing policy analysis for the very long run we should give greater weight to damage done to those alive in the distant future as a consequence of our actions than to damage caused by external events. While the issue of what is the appropriate reference point arises—is it not emitting any greenhouse gases? not increasing emissions? business as usual?—the implication would seem to be that a higher weight should be attached to damages caused by our emissions than to changes in consumption arising from other causes. Moreover, there would seem to be no obvious reason why this “caused damage” penalty should be felt less strongly as the length of time involved increases. The considerations adduced here all make a case for giving more weight to the distant future, taking more actions on its behalf, than would be implied by choosing “reasonable” parameters for the fundamental discounting equation. They also point in the direction of applying lower discount rates at longer horizons—a conclusion urged by others, notably Weitzman (1999) and Gollier (2008), on grounds related to uncertainty.Footnote 5 We leave to future research the question of how the appropriate magnitudes can best be gauged.",28
37.0,2.0,Journal of Risk and Uncertainty,04 September 2008,https://link.springer.com/article/10.1007/s11166-008-9049-6,Discounting climate change,December 2008,Partha Dasgupta,,,Unknown,Unknown,Unknown,Unknown,,
37.0,2.0,Journal of Risk and Uncertainty,27 August 2008,https://link.springer.com/article/10.1007/s11166-008-9050-0,Discounting with fat-tailed economic growth,December 2008,Christian Gollier,,,Male,Unknown,Unknown,Male,"We consider a standard utilitarian welfare function 
 The preferences of the representative agent in the economy are represented by her utility function u and by her rate of pure preference for the present δ. In a model with multiple generations, the current generation integrates the welfare of the next generation as if it was its own one. Thus, there is pure altruism in model Eq. 1. The utility function u on consumption is assumed to be three times differentiable, increasing and concave. Let c

t
 denote consumption at date t. Consider a marginal risk-free investment at date 0 which generates a single benefit e
rt at date t per euro invested at date 0. At the margin, investing in this project has the following impact on welfare: 
 The first term in the right-hand side is the welfare benefit that such investment yields. Consumption at date t is increased by e
rt, which yields an increase in expected utility by \(Eu^{\prime }\left( c_{t}\right) e^{rt}\), which must be discounted at rate δ to take account of the delay. The second term, \(u^{\prime }(c_{0}),\) is the welfare cost of reducing consumption today. Because ΔW is increasing in r, there exists a critical rate of return denoted r

t
, such that ΔW = 0 for r = r

t
. Obviously, r

t
 is the socially efficient discount rate, which satisfies the following standard pricing formula: 
 If financial markets would be frictionless and efficient, r

t
 would be the equilibrium interest rate associated to maturity t. This formula is the standard asset pricing formula for riskfree bonds (See for example Cochrane 2001). Suppose that u
′(c) = c
 − γ, where γ represents the constant relative risk aversion of the representative agent.Footnote 1 Suppose also that 
  normally distributed. As is well-known,Footnote 2 the Arrow-Pratt approximation is exact for an exponential function and a normally distributed risk. This implies that 
  It implies in turn that 
 or equivalently, that 
 where \(g_{t}=t^{-1}\ln (Ec_{t}/c_{0})=t^{-1}(EX_{t}+0.5Var(X_{t}))\) is the annualized growth rate of mean consumption.Footnote 3 This formula states that the socially efficient discount rate has three determinants. The first one is the rate of pure preference for the present, δ, which we put equal to zero in this paper. The second one is the wealth effect, which is measured by γg

t
, the product of relative risk aversion and the annualized growth rate of mean consumption between 0 and t. The third determinant is the precautionary effect. We see that it has an effect that is equivalent to a sure reduction of the growth rate of consumption by \(0.5(\gamma +1)t^{-1}Var(X_{t}),\) which is the precautionary premium defined by Kimball (1990). Indeed, γ + 1 is the index of relative prudence − cu
″′(c)/u
″(c). The uncertainty on the wealth available to the generation living at date t tends to reduce the discount rate associated to that date, which implies that more sacrifice must be endured today to increase wealth at that date. Stern (2006) considers the following specification: δ = 0.1%, g

t
 = 1.3%, Var(X

t
) = 0 and γ = 1. Equation 5 applied with these values of the parameters implies that r

t
 = 1.4% per year. Actually, Stern does not use explicitly a discount rate, because the investment project is not marginal. Therefore, the marginalist approach presented in this section cannot be used in his context. Rather, Stern estimates the sure immediate and permanent loss in consumption that yields the same effect on welfare W as the impacts of climate change. However, in Stern (2006), the certainty equivalent loss does not exceed 15% of GDP in 2200, which is not far of being “marginal”. In order to evaluate this point, let us evaluate in a non-marginal way the maximum share y of current GDP that one should be ready to sacrifice to eliminate a sure loss of 15% of GDP in 200 years, where y is the solution of the following iso-welfare condition: 
  Under the calibration of Stern, we obtain y = 12.46%. This corresponds to discounting the future loss of \(0.15\exp (200g)\) at a rate of 1.39% per year. Using a non-marginalist approach to valuing efforts to mitigate global warming does not noticeably change the conclusion compared to using a standard cost-benefit analysis of certainty equivalent impacts with a discount rate of 1.4%.",78
37.0,2.0,Journal of Risk and Uncertainty,15 October 2008,https://link.springer.com/article/10.1007/s11166-008-9054-9,Perfect and total altruism across the generations,December 2008,Stephen Fels,Richard Zeckhauser,,Male,Male,Unknown,Male,"The traditional formulation of the altruism model has altruistic terms in a generation’s utility function that relate solely to other generations’ felicity terms. We would argue that a truly altruistic generation would let other generations evaluate their own welfares. If they, like the present generation have utility functions that include altruistic terms, these terms should not be neglected in the altruistic considerations of the present generation. In our formulation, therefore, we use total utilities rather than felicities as the basis for altruistic evaluations. It is evident that if altruism is a major factor driving current policies, say in curbing greenhouse gases to save the future from severe climate change, the impact of that altruism would be much greater if it incorporated other generations’ total utilities. Although we think that altruistic preferences should and do relate directly to total utilities, we would really admit that felicities from own consumption are the original source of all utilities. In this essay we are interested in what might be called the basic structure of a formulation using total utilities. We will think of this structure as a derived formulation that employs felicities as the primordial argument of altruistic terms. If we discover that some apparently reasonable formulations using total utilities imply unreasonable basic structures, we will think further before employing these formulations. In some sense, the former is only truly reasonable if the latter makes sense as well.Footnote 3
 It is equally interesting to know whether traditional formulations relating to felicities are consistent with, and are the basic structure for, acceptable models with total utilities. If the answer is affirmative, we can think of these formulations as shorthand forms of writing what we regard to be the true utility functions. This is much in the way we might think of structural equations in relation to the behavioral equations of an economic model.",8
37.0,2.0,Journal of Risk and Uncertainty,09 September 2008,https://link.springer.com/article/10.1007/s11166-008-9045-x,Estimating discount rates for environmental quality from utility-based choice experiments,December 2008,W. Kip Viscusi,Joel Huber,Jason Bell,Unknown,Male,Male,Male,"Our study uses an original survey in which each respondent considered a policy choice task such as that presented in Fig. 1. The general research strategy is to elicit respondents’ valuations of environmental improvements that would begin after different periods of delay. Before considering the choices, respondents receive detailed information on three dimensions of the choices: water quality, cost, and time. Respondents make five choices among three policy options, which are defined along these three dimensions. Respondents indicate their most preferred choice among the different policy alternatives.
 Water quality survey policy choicea
 The environmental dimension is the amount of water quality improvement, which is the percentage of lakes and rivers in the respondent’s region that the U.S. Environmental Protection Agency (EPA) rates as being “good” for fishing, swimming, and quality of the aquatic environment.Footnote 2 The percentage improvement ranges from 5 percent to 20 percent. Each of the policies generates costs ranging from $100 to $400. The time dimension in Fig. 1 is the year when improvement begins, which we will refer to below as time delay. The amount of time delay before the improvement is realized is 0, 2, 4, or 6 years. Because of the relatively short time delays, the results should not be influenced by how people discount effects that occur after one’s death.Footnote 3 The policy choice decisions involved four different levels of cost, four different levels of water quality improvement, and four different periods of time delay. The survey design makes it possible to identify the individual’s rate of time preference for quality improvements.Footnote 4 For each different time delay, the survey permits an estimate of how much the respondent would have been willing to give up in terms of lower water quality or higher cost to remain just as well off. The cost dimension of the policy choice is not needed to estimate this intertemporal tradeoff rate. However, including cost makes the policy choice more realistic and leads to estimates of the cost-water quality improvement tradeoff that can be compared with estimates using a different survey methodology as an additional validity check. The costs and water quality improvements had comparable time dimensions, with each lasting for 5 years. However, costs begin immediately while the benefits begin after a period of 0 to 6 years. Thus the time discounting considers the value in present dollars of the level of improvement or of having the improvement come sooner. To see the tradeoff between the present value of costs and improvements, consider the standard exponential discounting case with a constant annual discount factor δ. There is a delay of t years before the improvement begins. With a delay of t years, the discount factor is δ
t. Let the person’s utility function be additively separable and linear in cost c and water quality improvement w, and let the time period of delay be t.Footnote 5 Then the present value of the 5-year imposition of costs beginning immediately, as described in the survey text, is \(c\left[ {1 + \delta + \delta ^2 + \delta ^3 + \delta ^4 } \right]\). Similarly, the present value of water quality benefits after a t year delay is given by \(w\delta ^t \left[ {1 + \delta + \delta ^2 + \delta ^3 + \delta ^4 } \right]\). Because the bracketed terms are identical, the person’s decision reduces to ascertaining whether the value of c is greater than wδt. The value of \(\delta = {1 \mathord{\left/ {\vphantom {1 {\left( {1 + r} \right)}}} \right. \kern-\nulldelimiterspace} {\left( {1 + r} \right)}}\), where r is the rate of interest. Thus, 1/(1 + r)t units of water quality that will result from improvements begun immediately will be equivalent to a unit of water quality improvement begun after a period of t years. The cost imposition will be worthwhile if the utility of the water quality improvement in year t is at least as great as (1 + r)t multiplied by the utility of the annual cost. The fact that the costs and improvements occur over a 5-year period drops out of the analysis, as the bracketed terms in the present value formulas above cancel out when comparing costs and benefits. We test for the possibility that respondents use hyperbolic discounting rather than exponential discounting. Hyperbolic discounting rates place a greater weight on immediate compared to deferred payoffs, inducing patterns of time inconsistency. Concerns with time inconsistency and hyperbolic discounting date back to Strotz (1956). The widely used quasi-hyperbolic discounting (hereafter merely denoted as “hyperbolic”) approach employed by Laibson (1997) is useful because of its analytic simplicity and clear-cut contrast with the exponential model. The hyperbolic formulation for discrete time periods yields discount factors given by \(\left\{ {1,\beta \delta ,\beta \delta ^2 ,\beta \delta ^3 , \ldots } \right\}\), where 0 < β < 1, and δ < 1. The discount factor terms involving δ are all multiplied by a parameter β except in the initial period. The discussion here and below employs discrete rather than continuous time because our survey focuses on discrete periods of time. Matters become a bit more complicated based on the hyperbolic discounting model. The survey scenario pertains to costs and benefits over a 5-year period. The present value of the cost stream becomes \(c\left[ {1 + \beta \delta + \beta \delta ^2 + \beta \delta ^3 + \beta \delta ^4 } \right]\). If the benefits begin immediately, the present value is \(w\left[ {1 + \beta \delta + \beta \delta ^2 + \beta \delta ^3 + \beta \delta ^4 } \right]\). The policy is attractive if the utility of the annual water quality improvement w exceeds the disutility of the annual cost. If there is a time delay of t years, benefits are \(w\beta \delta ^t \left[ {1 + \delta + \delta ^2 + \delta ^3 + \delta ^4 } \right]\). The bracketed expression and the β term are present for all nonzero periods of delay. Consider the 5-year stream of water quality improvement deferred by t years that is equivalent to the disutility of the 5-year cost stream that begins immediately. Let costs be multiplied by −1 to reflect the fact that cost c has a negative utility value. The value of w must satisfy
 or
 Compared to the exponential discounting case, hyperbolic discounting boosts the water quality improvement needed to achieve indifference with the utility of the immediate cost stream. This relationship reflects the general phenomenon that hyperbolic discounting differentially reduces the value of all deferred payoffs by a multiplicative parameter β in the hyperbolic discounting model. In 2004 a group of almost three thousand respondents participated in our Web-based valuation survey. The sample participants were members of the Knowledge Networks panel, which is a probability-based panel so that the composition closely parallels the U.S. Census statistics. People who do not have computers are given free internet access. The response rate to our survey is over 75 percent. As documented in Appendix Table 6, the demographic profile of our respondent group is remarkably similar to the mix of the age 18 and over U.S. population. We describe the properties of the sample and present tests of the survey methodology elsewhere.Footnote 6
 Although this conjoint survey is not a contingent valuation survey, it is in the general family of stated preference surveys. In the Appendix we report the requisite validity tests that have been established for such studies.Footnote 7 Chief among these tests is a series of scope tests to ascertain whether subjects consistently prefer more water quality improvement to less and, similarly, whether they prefer lower values of costs and shorter delays to higher costs and longer delays.Footnote 8 The survey included an additional series of rationality tests to determine whether subjects made decisions that did not lead to the choice of a dominated alternative. Overall, 95 percent of the original sample, or 2,914 individual respondents, passed the dominated choice test and therefore constitute the sample considered here.Footnote 9
 The computer-based survey lasted an average of 25 minutes and included detailed information pertaining to the meaning of water quality ratings and financial costs. Each respondent considered a series of five policy choice tasks, such as that in Fig. 1, so that there are 14,570 decisions among the three policies.",66
37.0,2.0,Journal of Risk and Uncertainty,03 September 2008,https://link.springer.com/article/10.1007/s11166-008-9051-z,Conflicting motives in evaluations of sequences,December 2008,Shane Frederick,George Loewenstein,,Male,Male,Unknown,Male,,62
37.0,2.0,Journal of Risk and Uncertainty,16 October 2008,https://link.springer.com/article/10.1007/s11166-008-9053-x,Individual laboratory-measured discount rates predict field behavior,December 2008,Christopher F. Chabris,David Laibson,Dmitry Taubinsky,Male,Male,Male,Male,"Dozens of studies have demonstrated a relationship between laboratory measures of discounting and various behaviors and traits, including smoking (e.g., Bickel et al. 1999; Mitchell 1999), gambling (e.g., Dixon et al. 2003), cognitive ability (e.g., Benjamin et al. 2006; for a review see Shamosh and Gray 2008), heroin use (e.g., Kirby et al. 1999), age (e.g., Green et al. 1994), alcoholism (e.g., Petry 2001), and brain injury (e.g., Dixon et al. 2005).Footnote 3 In general, this literature shows that higher rates of discounting are associated with problematic and impulsive behaviors, drug use being one particularly well-studied domain (e.g., Reynolds 2006). Many discounting studies target a clinical population (alcoholics, pathological gamblers) and infer discount rates from a series of choices between a smaller, immediate reward (SIR; usually real or hypothetical money) and a larger, delayed reward (LDR), which are then compared to the discount rates of control subjects who do not have the diagnosis in question. To our knowledge, no previous paper has used a wide range of behaviors to estimate and compare the predictive strength of the discount rate and the predictive strength of other demographic variables. The present study utilizes data from three independent, relatively large, diverse samples, which feature numerous traits and field behaviors. Intertemporal choice data were collected as part of three independent studies. For all three samples, we used the 27–choice delay-discounting task created by Kirby et al. (1999), and in all studies we administered the task by computer. Choice trials were presented to subjects in a fixed order. On each trial, subjects indicated their preference for either an SIR or a LDR. The trial order and specific reward amounts and delays are shown in Table 1. Table 2 shows summary descriptive statistics for the samples we use in our analysis. Each of the three samples is described below. 
 One hundred and forty-six individuals participated in the study.Footnote 4 All subjects were recruited from the greater Boston area via advertisements appearing in a free community newspaper and postings in various public spaces. Because the study was primarily concerned with the relationship between body mass index (BMI) and measures of impulsivity, separate advertising targeted normal-weight, overweight, and obese subjects, offering $20 and a chance at earning additional money in return for one hour of participation in the laboratory and willingness to fast for 12 hours prior to the session. All subjects were high school graduates, English speakers, and reported having no family history of serious mental illness. BMIs of the subjects ranged from 18.7 to 60.5 (M = 29.0, SD = 7.1). Current U.S. government standards specify that a BMI of 18–25 is labeled “normal weight,” a BMI between 25 and 30 is labeled “overweight,” and a BMI 30 or greater is labeled “obese.” We exclude subjects who always chose either the delayed or the immediate reward (N=5), because for those subjects we cannot estimate a discount rate based on the data. In addition, we do not include subjects whose discount rates were estimated to be in the top 5% (N=7) of the empirical distribution of the estimated discount rates. We exclude the top 5% because our data are extremely right-skewed (before the exclusion): the minimum discount rate is 4.08 ×10 − 6, the 5th percentile is 3.48 ×10 − 4, the 10th percentile is 0.001, the 50th percentile is 0.007, the 90th percentile is 0.044, the 95th percentile is 0.102, and the maximum is 0.241. As a consequence of this skewed distributional pattern, regressions are dominated by a few outliers in the right tail of the data. To explore the robustness of our results, we have redone all of our analysis using the percentile of the discount rate (instead of the raw discount rate) without excluding the top 5%. This nonlinear transformation of the discount rate data prevents the right tail observations from dominating the analysis and none of the qualitative results of this study change. Finally, we exclude subjects who have missing data in any of the variables we study (N=8). The three types of exclusions leave us with a final sample for the Weight Study of 126 subjects. Kirby et al. (1999) 27-item delay-discounting task was administered using Psyscope 1.2.5 software (Cohen et al. 1993) on Apple Macintosh computers running OS 9. To make the task incentive compatible, each participant had a 1-in-6 chance of receiving an additional $11 to $85 monetary reward (in addition to the promised $20 show-up fee), reflecting their indicated preference on one randomly selected trial. At the end of the laboratory session, the participant rolled a six-sided die; if a 6 was rolled, the participant blindly selected one card from a box containing cards numbered 1–27 to determine the trial that would be paid off. If, on that trial, an immediate reward was selected, the amount was added to the $20 and a check request for the total was submitted on the same day (or the next business day) to the research administration office. If a delayed reward was selected, a $20 check request was made immediately, and a second request for the reward amount was submitted after the specified number of days had elapsed. Checks were mailed to subjects approximately two weeks after requests were submitted. Subjects reported their age (in years), sex (male or female), and highest level of education (which was used to estimate a total number of years of education; e.g., high school = 12 years, college degree = 16 years, law school = 19 years, etc.). Subjects reported on a variety of health-related behaviors, including smoking, dieting status, and physical exercise (see Appendix 2a for exact item wording). BMI was used to assess each participant’s body weight adjusted for height. Heights and weights were measured in the laboratory using measuring tape and a Taylor 300 portable scale. Each participant’s BMI was calculated using the standard formula: weight (kg)/[height (m)]2. In addition, the Beck Depression Inventory-II was used to measure depression symptoms. The BDI-II (Beck et al. 1996) is a 21-item self-report questionnaire that assesses the presence and severity of depression symptoms during the previous 2 weeks. Questions correspond to the criteria for major depressive disorder outlined in the fourth edition of the Diagnostic and Statistical Manual of Mental Disorders. Each question is multiple-choice and each response is scored on a 4-point (0–3) scale. Responses are summed into a total score that ranges from 0 to 63, with higher scores suggesting more severe depression: A total score between 0 and 13 indicates minimal depression, 14–19 indicates mild depression, 20–28 indicates moderate depression, and 29–63 indicates severe depression.Footnote 5
 Subjects were tested individually in private rooms around mid-day.Footnote 6 After giving informed consent, each participant completed the BDI-II and then was measured for height and weight (in each case with shoes removed). Each participant was next administered a questionnaire that contained some of the health-related measures and the demographic measures mentioned above, as well as additional questions not analyzed here. Subjects then completed the delay-discounting task. The task’s instructions were closely adapted from Kirby et al. (1999) and explained that each participant would be asked to choose between receiving a SIR or a LDR on 27 choice trials. The instructions emphasized that subjects should take the task seriously, because at the end of the study, there would be a 1-in-6 chance that one of their choices would be randomly chosen and implemented. Each participant verbally summarized the instructions to the investigator and any misunderstandings were corrected before the task began. Subjects then completed a task that involved making decisions about pictures of food (not reported here), and a second questionnaire that included the remaining health measures, including dieting and alcohol consumption. At the end of the session, subjects followed the procedure mentioned above to determine what (if any) additional payment they would receive based on their discounting task choices. The Cognition study examined individual differences in various cognitive and decision making abilities. Some results from this study are reported elsewhere (e.g., Chabris 2007). One hundred and thirty-six individuals participated in this study.Footnote 7 All subjects were recruited from the greater Boston area via advertisements appearing in a free community newspaper, postings in various public spaces, and lists of volunteers for studies in the Harvard University psychology department. They were offered $50, as well as a 1-in-6 chance of winning up to $85 more, for their participation. Approximately two-thirds were university students and one third were local community residents. All subjects reported no use of drugs or psychoactive medications, no history of psychiatric or neurological illness, fluency in English, and having completed high school. We exclude subjects who always chose either the delayed or the immediate reward (N=3), because for those subjects we cannot estimate a discount rate based on the data. As in the Weight Study, we do not include subjects whose discount rates were estimated to be in the top 5% (N=6) of the empirical distribution of the estimated discount rates. We exclude the top 5% because our data are extremely right-skewed (before the exclusion): the minimum discount rate is 2.87 ×10 − 4, the 5th percentile is 7.83 ×10 − 4, the 10th percentile is 0.001, the 50th percentile is 0.006, the 90th percentile is 0.036, the 95th percentile is 0.084, and the maximum is 0.281. As a consequence of this skewed distributional pattern, regressions are dominated by a few outliers in the right tail of the data. To explore the robustness of our results, we have redone all of our analysis using the percentile of the discount rate (instead of the raw discount rate) without excluding the top 5%. This nonlinear transformation of the discount rate data prevents the right tail observations from dominating the analysis and none of the qualitative results of this study change. Finally, we exclude subjects who were missing data in any of the variables we study (N=24). The three types of exclusions leave us with a final sample for the Cognition Study of 103 subjects. The task and procedure were the same as in the Weight study. A self-report questionnaire was used to measure age, sex, height and weight (from which BMI was calculated as in the Weight Study), and highest level of education completed (which was converted into years of education as in the Weight Study). Subjects completed a series of eight tests described in Appendix 1a. Subjects were tested individually in private rooms. After giving informed consent, each participant completed the self-report demographic questionnaire, followed by a series of computerized and paper-and-pencil tasks that took approximately 3.5 h. Included in this series were the delay-discounting task (which was administered as in the Weight Study) and the cognition measures mentioned above, as well as several other personality and decision-making tests not reported here. At the end of the session, subjects followed the procedure used in the Weight Study to determine what (if any) additional payment they would receive based on their discounting task choices. All payments were made by check following the procedure described for the Weight Study. The Web study examined individual differences in various cognitive abilities in relation to personal background characteristics (especially educational and occupational specialization) and behaviors. No laboratory visits were involved; subjects completed all components of this study via the Internet. Other results from these data are reported elsewhere (e.g., Chabris et al. 2007; Liebert et al. 2007). Four hundred and twenty-two individuals participated in the study.Footnote 8 Subjects were recruited through advertisements posted on community web sites (e.g., Craigslist) and direct emails to special membership lists (e.g., science graduate students). Because these data were collected as a part of a study primarily concerned with investigating differences in cognitive ability among individuals with different types of post-secondary training and occupations, recruitment materials specified that prospective subjects should hold a college degree (however, approximately 8% of the total sample reported having fewer than four years of post-secondary education). Advertisements offered a $10 gift certificate to Amazon.com and, similar to the Weight and Cognition studies, a 1-in-6 chance of winning an additional gift certificate worth up to $85 in exchange for completing a series of cognitive tasks and questionnaires online lasting approximately 45 min. All subjects reported being English speakers and U.S. citizens or residents. Of the 422 original subjects we do not include subjects who had three or more responses below 200 ms on the discounting task (N=5), because such rapid responses may indicate rote responding.Footnote 9 We exclude subjects who always chose either the delayed or immediate reward (N=8), because for those subjects we cannot estimate a discount rate based on their data. As in the Weight and Cognition Studies, we do not include subjects whose discount rates were estimated to be in the top 5% (N=19) of the empirical distribution of the estimated discount rates. We exclude the top 5% because our data are extremely right-skewed (before the exclusion): the minimum discount rate is 2.39 ×10 − 6, the 5th percentile is 4.83 ×10 − 4, the 10th percentile is 8.63 ×10 − 4, the 50th percentile is 0.006, the 90th percentile is 0.025, the 95th percentile is 0.041, and the maximum is 0.230. As a consequence of this skewed distributional pattern, regressions are dominated by a few outliers in the right tail of the data. To explore the robustness of our results, we have redone all of our analysis using the percentile of the discount rate (instead of the raw discount rate) without excluding the top 5%. This nonlinear transformation of the discount rate data prevents the right tail observations from dominating the analysis and none of the qualitative results of this study change. Finally, we exclude subjects who have any missing data in the variables that we study (N=64). The three types of exclusions leave us with a final sample for of 326 subjects for the Web Study. Subjects completed a series of 10 tests to measure individual differences in cognitive ability, nine of which were based on the “MRAB” (Minicog Rapid Assessment Battery) developed by Shephard and Kosslyn (2005) (see Appendix 1b for a complete listing of the cognitive tests). Performance on each test was measured as the percentage of trials answered correctly, except for Verbal Fluency, which was scored as the total number of different words generated in three one-minute trials. The Kirby et al.’s (1999) delay-discounting task was administered using Psyscope-FL, a tool for Macintosh OS X that compiles PsyScope experiments into Flash movies that run on the participant’s personal computer and transmit the response data back to a server. As in the Weight and Cognition studies, subjects were given a 1-in-6 chance of receiving their preferred reward on one randomly selected trial to encourage truthful revelation of preferences. Subjects self-reported a variety of personal heath-related behaviors, including smoking, physical exercise, healthy food choices, dental health (frequency of dentist visits, flossing frequency), and prescription drug compliance (see Appendix 2b for exact item wording). Subjects self-reported a variety of personal finance-related behaviors, including percentage of income saved, gambling, late credit card payments, and wealth accumulation relative to friends and siblings (see Appendix 2c for exact item wording). Individuals who responded to advertisements were sent an e-mail message containing a link to the Web-based battery of cognitive tests and questionnaires. After providing informed consent, subjects completed the 10 cognitive tests in order, followed by a series of questionnaires that included the delay-discounting task and a “lifestyle questionnaire” containing the health- and finance-related questions. All subjects received their $10 gift certificate to Amazon.com by email soon after completing the study, and approximately one-sixth of subjects were emailed an additional gift certificate in an amount determined by their choice on one (randomly selected) delay-discounting trial, after the specified delay. These rewards were normally emailed on the same or next business day that they were due to be paid.",228
38.0,1.0,Journal of Risk and Uncertainty,10 January 2009,https://link.springer.com/article/10.1007/s11166-008-9058-5,"Naive, resolute or sophisticated? A study of dynamic decision making",February 2009,John D. Hey,Gianna Lotito,,Male,Female,Unknown,Mix,,
38.0,1.0,Journal of Risk and Uncertainty,24 December 2008,https://link.springer.com/article/10.1007/s11166-008-9057-6,Risk aversion in the small and in the large: Calibration results for betweenness functionals,February 2009,Zvi Safra,Uzi Segal,,Male,Male,Unknown,Male,"We assume throughout that preferences over distributions are representable by a functional V which is risk averse with respect to mean-preserving spreads, monotonically increasing with respect to first order stochastic dominance, and continuous with respect to the topology of weak convergence. Denote the set of all such functionals by \(\mathcal V\). According to the context, utility functionals are defined over lotteries (of the form X = (x
1,p
1;...;x

n
 ,p

n
 )) or over cumulative distribution functions (denoted F,H). Degenerate cumulative distribution functions are denoted δ

x
. Betweenness functionals in \(\cal V\) (Chew 1983, 1989; Fishburn 1983; Dekel 1986) are characterized by linear indifference sets. That is, if F and H are in the indifference set \({\mathcal I}\), then for all α ∈ (0,1), so is αF + (1 − α) H. Formally: V satisfies betweenness if for all F and H satisfying V(F) = V(H) and for all α ∈ (0,1), V(F) = V(αF + (1 − α)H) = V(H). The fact that indifference sets of the betweenness functional are hyperplanes implies that for all F, the indifference set through F can also be viewed as an indifference set of an expected utility functional with vNM utility u(·;F). Following Machina (1982), this function is called the local utility of V at F. In Safra and Segal we strengthen Rabin’s (2000) results and proved the following facts (see Proofs of Theorems 1 and 2 in Safra and Segal (2008)).  Let \(V \in {\cal V}\) be expected utility. Let g > ℓ > 0 and G > b − a, and let  If for all x ∈ [a,b],   then  That is, a rejection of the small lottery \((-\ell, \frac 12; g, \frac 12)\) at all x ∈ [a,b] implies a rejection of the large lottery ( − L,p;G,1 − p) at a. For example, a rejection of \((-100,\frac12;105,\frac12)\) at all wealth levels between 100,000 and 140,000 implies a rejection at wealth level 100,000 of the lottery \((-5,\!035,\frac12;10,\!000,\!000,\frac12)\).  Let \(V \in {\cal V}\) be expected utility. Let 0 < ℓ < g < L and let b − a = L + g.  If for all x ∈ [a,b], \(V(x,1) > V(x-\ell,\frac12;x+g,\frac12)\), then for all p and G satisfying  it follows that  For example, a rejection of \((-100,\frac12;110,\frac12)\) at all wealth levels between 150,000 and 200,000 implies a rejection, at wealth level 200,000, of the lottery ( − 50,000,6.6·10 − 6; 100,000,000, 1 − 6.6·10 − 6).",5
38.0,1.0,Journal of Risk and Uncertainty,13 August 2008,https://link.springer.com/article/10.1007/s11166-008-9048-7,Betting on own knowledge: Experimental test of overconfidence,February 2009,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"As we already mentioned in the introduction, a popular method for measuring individual confidence is an elicitation of confidence intervals. In a typical setting, subjects are asked to reveal a lower and upper bound for the n-percent confidence interval of a correct answer to a general knowledge question, a future price in the experimental market, a ranking of their ability level etc. Subjects are classified as overconfident if a variable of interest falls into the stated interval in less than n-percent of cases. Experimental papers in psychology typically do not provide monetary incentives for reporting confidence intervals. Experimental papers in economics usually offer a small reward to subjects if a variable of interest falls into the revealed confidence interval with the stated frequency. For instance, Cesarini et al. (2006) ask subjects to provide a 90% confidence interval for numerical answers to ten general knowledge questions. Subjects are then asked to assess if the correct answers indeed fall into the stated intervals and they are awarded approximately US $6 for each correct assessment. Despite the popularity of the method, elicitation of confidence intervals is not incentive-compatible. If subjects are not informed about the exact mechanism how they earn money before they state their confidence intervals, there is no financial incentive for revealing subjective confidence intervals.Footnote 1 If subjects are informed about their payoff function at the beginning of an experiment, they can increase their monetary payoff if they deliberately misrepresent their confidence intervals. For instance, Cesarini et al. (2006) report that one subject anticipated subsequent development of the experiment and strategically misreported his stated confidence intervals—for nine questions this student reported extremely wide confidence intervals and for one question he made a point estimate (so that nine out of ten of correct answers would fall into the stated 90% confidence intervals with relative certainty). To avoid such problems associated with payoffs contingent on the realization of a variable of interest inside the stated confidence intervals, we propose a new method for measuring individual confidence. The proposed method consists of two parts. In the first part, a subject is asked to answer N > 1 general knowledge questions or to contribute his or her effort in N tasks, where success depends on individual ability. The subject is informed that the more questions he or she answers correctly (the more tasks he or she completes successfully), the higher are the chances of getting a fixed monetary payoff M. In the second part, the subject faces a choice between the following alternatives:
 One of N questions (tasks) is selected at random and the subject receives a fixed monetary payoff M if his or her answer to the selected question was correct (the selected task was completed successfully); The subject plays a risky lottery that yields a fixed monetary payoff M with probability n/N, where n is calculated as the number of correctly answered questions (the number of tasks completed successfully) in the first part; Either alternative 1 or alternative 2 is selected at random. Alternatives 1–3 yield identical distributions of monetary outcomes. Thus, a well-calibrated subject is indifferent between all three alternatives. However, an individual who is overconfident about own knowledge and/or ability would prefer alternative 1 and an individual who is underconfident would opt for alternative 2. Thus, we have a simple measure of individual confidence in own knowledge through a revealed choice. Notice that the proposed method is incentive-compatible because subjects cannot increase their payoff if they deliberately provide incorrect answers to some of the general knowledge questions or if they strategically contribute a lower effort in some of the tasks. In such cases subjects still face a choice among identical probability distributions at the second part of the experiment (although with less favorable odds). As long as individual preferences satisfy first-order stochastic dominance, subjects cannot gain from giving deliberately incorrect answers (or strategically lower effort). Thus, our method avoids the incentive problems associated with the elicitation of confidence intervals. However, one can argue that the proposed measure of individual confidence in own knowledge might be confounded with individual risk aversion and/or ambiguity aversion. At the second part of the experiment, individuals are confronted with a choice between an ambiguous lottery (alternative 1) and a risky lottery (alternative 2). Thus, risk and ambiguity attitudes may affect the observed choices. To investigate this possibility, we run a controlled laboratory experiment, where we measure individual confidence according to the proposed method as well as individual risk aversion and ambiguity aversion. We find that observed subjective confidence does not depend on risk/ambiguity aversion or seeking.",48
38.0,1.0,Journal of Risk and Uncertainty,19 December 2008,https://link.springer.com/article/10.1007/s11166-008-9059-4,Individual decision-making experiments with risk and intertemporal choice,February 2009,Lisa R. Anderson,Sarah L. Stafford,,Female,Female,Unknown,Female,"Subjects in this experiment are asked to make a choice between two payment options for a set of 25 scenarios. Each scenario asks subjects to choose between Option A, which will be paid 2 weeks from the date of the experiment, and Option B, which will be paid in 2 weeks + n days.Footnote 4 The structure of the two payment options varies across scenarios but the timing (n) does not. The 2-week delay between the date of the experimental session and the first payment date is included to control for the “immediacy effect” described by Keren and Roelofsma (1995).Footnote 5 The immediacy effect occurs if outcomes later in time are perceived as less certain and thus the imminent future receives a disproportionate weight in the evaluation process. It is important to control for this effect as Öncüler (2000) hypothesizes that it may be the source of the discrepancy in existing experimental results on intertemporal decision making with risk. Additionally Coller et al. (2003) note that there may be transactions costs involved with delayed payments that are not present with immediate ones. To ensure that transactions costs are as similar as possible across the two payment options, both future payment days occur at the same time and on the same day of the week. Also, because the subject pool is drawn from students at the College of William and Mary, payment dates do not occur on weekends or holidays and both payment dates occur within the same academic semester. To provide a baseline for each subject, the first five scenarios ask subjects to choose between certain payment options. The remaining scenarios ask subjects to choose between:
 ○ A certain Option A and a risky Option B; ○ A risky Option A and a certain Option B; and ○ A risky Option A and a risky Option B. Risk is modeled using an Ellsberg urn design.Footnote 6 The urn contains a known distribution of colored markers and each color corresponds to a different known payment. Subjects are paid based on the color of the marker they draw from the urn. For example, a 50:50 lottery between $18 and $22 would be represented by an Ellsburg urn containing 100 markers, 50 of which are red and 50 of which are blue. If the subject draws a red marker, she will receive $18 and if the subject draws a blue marker she will receive $22. For each risky option, subjects are told both the composition of the urn, that is the number of markers of each color in the urn, and the payment options associated with each color marker. A complete set of instructions and the scenario options used in one of the experimental treatments is presented in Appendix A. Option A always has a certain or expected value of $20. All of the Option B lotteries have expected values equivalent to one of the certain values offered in the first five scenarios. Therefore we can compare any decision that includes a risky option to a decision that is identical except that the risky option has been replaced by a certain option. This holds when both options are risky as well. For example, if there is a scenario with an Option A lottery with an expected value of $20 and an Option B lottery with an expected value of $22, there are also scenarios with (1) a certain Option A payment of $20 and a certain Option B payment of $22; (2) a certain Option A payment of $20 and an Option B lottery with an expected value of $22; and (3) an Option A lottery with an expected value of $20 and a certain Option B payment of $22. Given the time requirements for conducting the experiments, we chose to use three different temporal extensions: 14, 28, and 56 days. For each temporal extension, we conducted six experimental sessions. The session sizes ranged from eight to 13 subjects with a total of 183 subjects participating in these experiments. For the experiments with a 14 day extension, we used two different questionnaires which differ from each other only in the dollar values offered to subjects for Option B. Similarly we had two treatments for the experiments with a 28 day extension that are identical in all respects except for the Option B payment amounts. However, all of the 56 day experiments used the same questionnaire.Footnote 7
 Sessions began with the experimenter reading the instructions aloud. Subjects then completed an on-line questionnaire with 25 decision-making scenarios and five demographic questions.Footnote 8 In all sessions the 25 scenarios were listed in the same order on the computer screen. One concern with presenting the 25 scenarios to all subjects in the same order is that the precise order of the choices might bias decision making. To minimize order biases, subjects saw all of the scenarios at once, and they were told that they could make the 25 decisions in any order and change them as much as they wished (see instructions in Appendix A). Anecdotally, we observed many subjects skipping around the questionnaire rather than moving down the list of scenarios as presented. Furthermore, subjects made all 25 decisions without any feedback about the outcomes of the lotteries.Footnote 9
 Subjects were paid $5 on the day of their experimental session as a participation fee. In addition, at the end of the experimental session we randomly selected one of the 25 scenarios by drawing a numbered ball from a container containing 25 numbered balls. The lottery was only played for the one scenario selected at random, and subjects were paid based on their decision for that selected scenario.Footnote 10 Thus subjects had to return on one of the two payment days to receive the amount indicated. If the option selected was risky, the risk was resolved on the payment day. To minimize any credibility concerns on the part of subjects in the experiment, we stressed in the instructions that the payments students would receive were “personally guaranteed by Professors Anderson and Stafford of the William and Mary Economics Department,” at least one of whom was present for each session of the experiment. In addition, at the end of the session each student was given a “payment certificate” that was signed by both professors and indicated the amount to be collected (or the lottery that would be conducted) and the date and time for collection for the payment option selected. The initial experimental sessions took 30 to 45 min and each payment session took 5 min.",39
38.0,1.0,Journal of Risk and Uncertainty,13 November 2008,https://link.springer.com/article/10.1007/s11166-008-9056-7,"Adverse selection, moral hazard and propitious selection",February 2009,Philippe De Donder,Jean Hindriks,,Male,Male,Unknown,Male,"We model an economy where all individuals face the same potential monetary loss L. We study a three-stage game with two types of individuals who differ only in risk aversion, which is not observable by insurance providers. In the first stage, zero-profit insurance contracts are offered to the agents. They consist of a premium π and of a coverage rate δ, i.e. the fraction of the damage that will be reimbursed by the insurer in case the damage occurs. In the second stage, individuals choose which, if any, private contract to buy. In the last stage, they decide how much precaution to exert. As usual, we solve this game backwards, starting with the precaution choice. The individual preferences over lotteries are represented by the von-Neuman-MorgensteinFootnote 4 utility function u

θ
, with θ denoting the individual risk aversion level. Risk aversion is the only heterogeneity between individuals. We only consider two levels of risk aversion, θ ∈ {L,H}, where an individual exhibits a high risk aversion (type H) if her utility function u

H
 is obtained by an increasing concave transformation of u

L
. We assume that all individuals have access to the same technology p(e) which gives the loss probability p as a function of the (unobservable) amount of precaution e they exert. The function c(e) denotes the non-monetary costFootnote 5 as a function of precaution effort. The cost of precaution is the same for all individuals. We assume that p(e) is decreasing while c(e) is increasing and convex. The expected utility of an individual of type θ who exerts precaution e and buys the insurance contract (π,δ) is
 where w denotes the wealth endowment. In our model, the risk level p is endogenously determined by the optimal precaution choice of the agent. The optimal precaution choice e

θ
(δ,π) of type θ given the insurance contract (δ,π) is given by
 The optimal precaution decreases with the coverage rate, and so the risk level of any individual increases with coverage rate. This is the moral hazard effect. The optimal precaution with full coverage is zero for all. The cherry picking/propitious selection argument first assumes that (Regularity): For any insurance contract, the more risk averse individual exerts more precaution. This property does not always hold with expected utility functions. Jullien et al. (1999) give sufficient conditions for more risk averse individuals to exert more precaution.Footnote 6 This requires in particular partial insurance because complete insurance induces minimum prevention and maximum risk for all individuals regardless of risk aversion. Jullien et al. (2007) argue that the most appropriate way to model the cost of precaution is to use a monetary cost like U(R − c(e)), so that the marginal rate of substitution between precaution and wealth does not depend on the shape of the utility function. With this monetary formulation, they show that the regularity property does not necessarily hold. The reason is that precaution reduces wealth even in the event of a loss, so that more risk averse agents may choose to save on the cost of precaution to increase their wealth in the bad state. Obviously, for this effect to arise, the loss probability has to be sufficiently high. We now look at individual preferences over insurance contracts (π,δ). Plugging the optimal precaution choice (2) in the utility function (1), we obtain the indirect utility function of type θ for an insurance contract. A crucial question in terms of indirect preferences is whether they satisfy the single-crossing property, (i.e. whether marginal willingness to pay for insurance is monotone in risk aversion). There are two opposite effects at play. On the one hand, a higher risk aversion results in a higher willingness-to-pay for insurance given the same risk level. On the other hand, more risk averse agents may exert more precaution and end up being less risky, which lowers their willingness to pay for insurance. The cherry picking/propitious selection argument implicitly assumes that: (Single crossing) Given the optimal precaution choice, for any insurance contract, the marginal willingness to pay for insurance is higher for the more risk averse individual. In words, the preference effect dominates the risk effect on the willingness to pay for insurance, for any given insurance contract, so that the more risk averse individuals are more willing to pay for insurance even though they face lower risk by behaving more cautiously. Jullien et al. (2007) show that the single-crossing property always holds with the monetary formulation of the cost of precaution. De Meza and Webb (2001) show that the single-crossing condition between risk-neutral and risk averse individuals may not be satisfied with non-monetary costs of precaution. To summarize, Properties 1 and 2 together embody the cherry picking/propitious selection argument in our three-stage model: the more risk averse individuals will both exert more precaution (given any insurance contract) and have a higher willingness to pay for insurance (given their optimal precaution choice). We now solve the first stage of the game assuming that Properties 1 and 2 (as implied by the propitious selection argument) hold, and we look at the equilibrium insurance contracts these properties imply. Consider that low risk aversion individuals (denoted by type L) are in proportion λ and high risk aversion individuals (type H) are in proportion 1 − λ . We first show using Fig. 1 that there cannot exist a pooling equilibrium with partial coverage.
 Non existence of pooling equilibria The plain curves represent the fair price for each type, denoted by π(i), i = {L,H}. Regularity ensures that type L agents are more risky than type H agents, for any given coverage rate 0 < δ < 1, and thus that π(L) lies everywhere above π(H), except for δ ∈ {0,1}. The fair price when both types are lumped together and charged the same price is represented by the dashed curve whose formula is given by λπ(L) + (1 − λ)π(H). Due to moral hazard, the fair price is an increasing and convex function of coverage rate for both types. We have also depicted the single-crossing indifference curves of the two types (labelled v

L
 and v

H
) passing through a pooling contract with partial coverage.Footnote 7 It is clear from the graph that given this pooling contract, it is always possible for an insurer to propose a contract that can attract only the more risk averse (who are also less risky by the regularity property) and make a positive profit. This separation is possible given the single-crossing property of indifference curves. Likewise, Fig. 1 also makes clear that a pooling contract with full coverage can not be an equilibrium, since it would be possible to offer a contract with less coverage that attracts the more risk averse type (possibly together with the less risk averse) and makes a positive profit. Under Properties 1–2, there is no equilibrium pooling insurance contract. Figure 2 illustrates the existence of a separating equilibrium. In this equilibrium, the less risk averse individuals obtain their most-preferred contract (i.e. point A). Whenever the incentive compatibility constraint of type L is binding at equilibrium (which is the case in Fig. 2 since all feasible contracts preferred by type H to B
′—the shaded area—are also preferred by type L individuals to contract A), type H individuals are proposed a fair contract with more coverage than they would wish. The intuition for this equilibrium is that it is necessary to overprovide insurance to the more risk averse individuals in order to separate them from the less risk averse. Observe that type H always buys more insurance than type L at equilibrium, in accordance with the propitious selection argument.
 Separating equilibrium with overinsurance We are now in position to make our central point that the correlation between risk and insurance demand is a priori ambiguous at equilibrium. On one hand, more risk averse individuals are less risky than less risk averse individuals if they all face the same coverage rate. On the other hand, in the separating equilibrium more risk averse agents get more insurance, which reduces their relative level of precaution. Figure 3 shows that more risk averse individuals are in fact more risky, in any separating equilibrium, if the indifference curve of the less risk averse individuals through this contract is convex.
 Relative risks in separating equilibria The straight bold line going through point A shows all fair contracts with a risk level equal to that optimally chosen by type L at point A. The intersection (denoted by B
′) between type L’s indifference curve through A (denoted by v

L
) and π(H) gives us the minimum amount of coverage bought by type H at the separating equilibrium. To prove that type H is riskier at equilibrium than type L, we then have to prove that B
′ lies above the bold straight line through A. Observe first that the slope of the straight line through A is lower than the slope of π(L) at A, since the risk increases along the latter due to moral hazard. On the other hand, the slope of v

L
 is equal to the slope of π(L) at point A, by optimality of A for type L. We then obtain that a convex indifference curve v

L
 lies above the straight line through A to the right of A, so that its intersection with π(H) also lies above B
′. This means that type H is more risky than type L at any separating equilibrium, and that risks and demand of insurance are positively correlated. We then obtain the follow result. Under Properties 1–2, the equilibrium insurance contracts are separating, with the more risk averse individuals buying more coverage. More over, the more risk averse individuals display higher (lower) risk if the marginal willingness to pay for insurance of the less risk averse individual is increasing (decreasing) with coverage. We thus obtain that Properties 1 and 2 are not sufficient for the  negative correlation between insurance coverage and risk to emerge. Even though more risk averse agents tend to behave more cautiously, they also buy more insurance at equilibrium. This in turn induces the more risk averse individuals to behave less cautiously than the less risk averse individuals who purchase less insurance. To obtain that more risk averse individuals are less risky at equilibrium, we need the extra condition that the willingness to pay for insurance of the less risk averse individuals be decreasing with coverage. Observe that there are two opposite forces shaping the willingness to pay for insurance. For a fixed risk, the concavity of the utility function decreases the willingness to pay for insurance as coverage increases. With moral hazard, risk increases with coverage, which in turn means that willingness to pay for insurance increases with coverage. The net effect can then go either way. However, the reference individual is the less risk averse one, so that we can expect the moral hazard effect to dominate the concavity of the utility function effect. This is certainly true if the less risk averse individuals are almost risk neutral. In the Appendix, we solve the model using Yaari’s (1987) dual theory of preferences under risk and show that, although Properties 1 and 2 are naturally satisfied in this context, the marginal willingness to pay for insurance of the less risk averse individual is increasing with coverage, resulting in a positive correlation between risk and insurance purchase.",25
38.0,2.0,Journal of Risk and Uncertainty,05 February 2009,https://link.springer.com/article/10.1007/s11166-009-9065-1,A note on uncertainty and discounting in models of economic growth,April 2009,Kenneth J. Arrow,,,Male,Unknown,Unknown,Male,"I follow Dasgupta’s notation in general. Here, r

t
 is the net return on capital committed for one period, so that the gross return is 1 + r

t
 per unit of capital committed. At the beginning of time t, the capital is K

t
. The individual saves a fraction, s

t
, of that capital so that consumption in period t is, (1 − s

t
) K

t
 and s

t

K

t
 is available for investment. One unit of investment yields a random gross return, 1 + r

t
, so that,
 and,
 Note that, under Eq. 1, all investments are, in effect, made for one period; this is a “circulating capital” model, in a somewhat old-fashioned terminology. It differs from the assumption made by Gollier (2008, p.174) in which an investment yields return only after a fixed period of time. Assume that,
 I make here the usual assumption that felicity, U, is given by,
 and welfare, V

t
, at time t by,
 where 1 + δ is a discount factor. I comment on the assumption (4) in Section 6 below. Maximize V
0 with respect to the savings ratios, s

t
, for a given value of K
0 (In general, s

t
 can be a function of the history up to time t, i.e., the values of K
0 and of \(r_s \left( {0 \leqslant s <t} \right)\).). From the homotheticity of U and the homogeneity of degree 1 of the production relations (1–2), it is obvious that, if an optimum exists, s

t
 must be a constant, independent of t and of history. Then, Eqs. 1 and 2 can be written,
 From Eq. 1′,
 Then, from Eqs. 2′, 3, and 4,
 and therefore, from Eq. 5, 
V
0 is a (negative) constant times a geometric series
",17
38.0,2.0,Journal of Risk and Uncertainty,11 February 2009,https://link.springer.com/article/10.1007/s11166-009-9060-6,The ostrich effect: Selective attention to information,April 2009,Niklas Karlsson,George Loewenstein,Duane Seppi,Male,Male,Male,Male,"Economic models commonly assume that information affects utility indirectly as an input in decision making. Recent economic models also incorporate information and beliefs directly in utility via anticipation (Köszegi and Rabin 2007; Caplin and Leahy 2001; Loewenstein 1987), self-image or ego (Benabou and Tirole 2006; Bodner and Prelec 2001; Köszegi 1999), and recursive preferences that depend on beliefs about future utility (Epstein and Zin 1989; Kreps and Porteus 1978). Incorporating beliefs into the utility function has ramifications for time discounting, the effective level of risk-aversion, and preferences about the timing of the resolution of uncertainty.Footnote 2
 The insight that people derive utility from information has also enriched finance. Traditional finance theory assumes that investors only derive utility from their assets at the time they liquidate and consume them—e.g., upon retirement—but people clearly derive pleasure and pain directly from changes in their wealth prior to consuming the underlying cash flows. Barberis, Huang, and Santos (2001) show that a model in which investor utility depends directly on the value of their financial wealth can explain the equity premium puzzle as well as the low correlation between stock market returns and consumption growth (for earlier treatments, see Gneezy and Potter 1997; Benartzi and Thaler 1995). Research in psychology bolsters the work in economics by showing that people who hold optimistic beliefs about the future and positive views of themselves are happier (Scheier, Carver and Bridges 2001; Diener and Diener 1995) and healthier (Baumeister, Campbell, Krueger and Vohs 2003; Peterson and Bossio 2001), if not necessarily wiser (Alloy and Abramson 1979). There is also ample evidence from psychology that desires exert a powerful influence on beliefs, a phenomenon that psychologists call “motivated reasoning” (Kruglanski 1996; Babad 1995; Babad and Katz 1991; Kunda 1990). Economists, too, have been interested in motivated formation of beliefs, but have focused more on modeling the phenomenon than on studying it empirically.Footnote 3
 Empirical support for the selective exposure hypothesis can be found in diverse research conducted by psychologists. Ehrilch, Guttman, Schonbach and Mills (1957) found that new car owners pay more attention to advertisements for the model they purchased than for models they had considered but did not buy. Brock and Balloun (1967) observed that smokers attend more to pro-smoking messages and that non-smokers attended more to anti-smoking messages. Although some studies have equivocal findings (Cotton 1985; Festinger 1964; Freedman and Sears 1965), the most recent research provides quite strong support for the selective exposure hypothesis (e.g., Jonas, Schulz-Hardt, Frey and Thelen 2001; Frey and Stahlberg 1986). While not focusing specifically on selective exposure, research in behavioral finance, like that of psychologists, highlights the importance of attention for investor behavior. DellaVigna and Pollet (2005), for example, show that earnings announcements have a more gradual impact on stock prices when they occur on a Friday (when investors are likely to be inattentive) than when they occur on other days of the week. Barber and Odean (2008) predict and find that individual investors, as compared with institutional investors, tend to be net buyers of attention-grabbing stocks—e.g., those that receive special news coverage.",346
38.0,2.0,Journal of Risk and Uncertainty,11 February 2009,https://link.springer.com/article/10.1007/s11166-009-9061-5,Pre-commitment and flexibility in a time decision experiment,April 2009,Marco Casari,,,Male,Unknown,Unknown,Male,"We studied the time preferences of 120 subjects recruited from the undergraduate population of Jaume I University of Castellon, Spain.Footnote 6 Each subject faced a series of choices between a smaller-sooner payment (SS) and a larger-later (LL) payment with delays between 2 days and 22 months. Choices were divided into three parts: a first part to measure impatience, a second part to detect choice reversal, and a third part to assess preferences for commitment and flexibility. The procedures for parts one and two share similarities with the titration procedure used in Mazur (1987) and Kirby and Herrnstein (1995). In all parts special attention was given to equalize the implicit costs among alternative options. For instance, no option involved an immediate payment, so subjects had to return to the lab to cash rewards no matter what the choice was. In part one, each decision had four parameters: the amount of the two possible payments and their delays. Both payment amounts were held constant throughout the procedure at 100€ and 110€. The goal of part one was to elicit an approximate measure of impatience D* by fixing the delay of the smaller-sooner payment at 2 days, SS = (100€, 2 days), and varying the delay D of the larger-later payment, LL = (110€, D), up and down until the subject was approaching indifference between the two payments. The main reason for eliciting D* was not to accurately estimate impatience but to be able to individually calibrate choices in parts two and three. An example will clarify the procedure of part one. Consider a subject who selects LL = (110€, 10 days) over SS = (100€, 2 days), but chooses SS over LL = (110€, 17 days). In this example the point of the switch from LL to SS narrows down the subject’s indifference point between the two options to a delay in between 10 and 17 days. We take D* = 17 as our measure of impatience, i.e. the waiting time in days for LL in that decision where the subject switches from LL to SS.Footnote 7 To minimize trembling hand mistakes, following the switch, one or two additional decisions were prompted with longer delays D, and answers had to confirm the switch for part one to stop. Moreover, a list of all part-one decisions was presented and each one of them could be changed before moving to part two. In part two, subjects faced a series of decisions with a front-end delay (FED) added to both rewards SS and LL in order to detect possible choice reversals (Figs. 1 and 2). The smaller-sooner reward maintained a consistent delay of FED + 2 days and the larger-later reward maintained a delay of FED + D* days, which ensured throughout part two that the waiting time difference between SS and LL was constant. The remaining parameters of the decisions were unchanged. This design increased the chances to detect the presence of choice reversal patterns. To continue with the example, part two would open with a decision with a FED of 7 days between SS = (100€, 9 days) versus LL = (110€, 24 days). Assume that in this opening decision the subject chose SS. Then the following choice would be (100€, 16 days) versus (110€, 31 days). If the subject kept choosing SS, the FED would become progressively longer. If, instead, at this point a subject chose LL, such choice was the hint of a choice reversal. Following this switch from SS to LL, one or two additional decisions were prompted with longer FED delays, and the subject had to confirm the switch in order for part two to stop. In case the choice reversal was confirmed, we define FED*, which in the example is 14 days, as the shortest front-end delay that induced the subject to reverse her choice from SS to LL. The procedure stopped in any case when the front-end delay FED exceeded 395 days.Footnote 8 Finally, a list of all part two decisions was presented, and each one of them could be changed before moving to part three. Throughout the experiment, particular care was taken to avoid calendar effects, i.e. the inability or unwillingness to cash the reward on a particular day because of events such as birthdays, examinations, or traveling out of town.Footnote 9
 Willingness to wait in parts one and two of the experiment Soft commitment in decision 1 (Table 1) Part three included eleven decisions aimed at measuring preferences for commitment and flexibility (Table 1). These choices had two peculiarities. First, instead of only two alternative payments, each decision could involve either three or four. Second, each decision comprised two choices that took place on two separate days: one on the day of the experimental session and the other through email at a later date, sometimes months later. Part three gives interesting insights especially for those subjects who reversed their choices.
 In part three, a subject could either commit immediately to the larger-later reward {LL} or postpone the choice between a larger-later reward and a smaller-sooner reward, {LL, SS} (Table 1, decision 5). The commitment strictly eliminated the possibility of later choosing another option: in particular, once made, it irreversibly ruled out the possibility at a future date to opt for SS. On the contrary, postponing the choice allowed a subject to wait exactly FED* days and send an email stating a preference for either LL or SS.Footnote 10 Amount and delays of payments were structured in a way to resemble decisions already made in parts one and two. In particular, in the lab a subject faced SS = (100€, FED* + 2) versus LL = (110€, FED* + D*). Remember that the subject faced the same decision in part two, and those who reversed their choices chose LL. The subject could commit to LL and would need to send an email after FED* days. As an alternative, she could postpone the choice and, after having waited FED* days, she could send an email choosing between SS = (100€, 2) versus LL = (110€, D*). Remember that the subject faced this same decision in part one and chose SS. A commitment for LL at the time of the lab session reveals an awareness of the future temptation to choose SS. Part three included several variants of the choice above where a cost was added either to the restricted set {LL} or to the larger set {SS, LL} in terms of a lower payment amount or a longer delay. In one case, strict commitment was available for free (decision 5), and in the other two cases it was available for the cost in terms of a modified restricted set {LL′}. In decision 7, LL′ offered a lower amount than LL while in decision 10 LL′ exhibited a longer delay than LL. We also studied soft commitment using choices with four alternative rewards SS, SS′, LL, LL′. In the lab a subject selected one of the two sets, {SS, LL} or {SS′, LL′}, and in a later email she chose an option within the set already selected (Fig. 3). In three cases LL′ = LL and SS′ offered either a lower amount (decisions 1 and 2) or a longer delay (decision 4). In another case SS′ offered a substantially lower amount than SS and LL′ offered a slightly lower amount than LL (decision 3).
 Strict commitment in decision 7 (Table 1) Finally, there were four choices regarding flexibility. These decisions were formally similar to those pertaining to strict commitment (Fig. 3), but instead of paying a cost to restrict the choice set, a subject had to pay a cost to make it wider. More precisely, option LL involved a monetary cost (decisions 6 and 8) or a delay cost (decisions 9 and 11) with respect to option LL′. The payments related to choices over time were never immediate. Subjects had to come back at a future date at least 2 days after the session. Payments were made in cash, hence no trip to a bank was necessary.Footnote 11 For choices in part one, only one participant per session received a large, delayed payment. There were 20 participants in each session. A volunteer randomly drew a number out of a bag immediately at the end of the session to select one of the lucky persons. The selected person received a signed letter on university letterhead promising a later payment of 100€ or 110€. The exact date and amount of the actual payment reflected one of the subject’s choices in part one of the experiment. The choice paid out was selected through a second random draw.Footnote 12
 For choices in parts two and three, the computer randomly selected one other person per session for a large payment between 94€ and 110€ and randomly selected one choice in part two or part three. His or her name and the selected decision were not immediately revealed to the participants. Instead, along with all participants’ contact information, the name was placed in an envelope that was then sealed in front of them and signed by two subjects.Footnote 13 Only the subjects who emailed their follow-up choices were eligible to receive this second large payment. In particular, a choice for commitment did not save the cost of sending an email at the pre-specified date, because a lack of a confirmation message would have precluded participation in the draw for the payment. Subjects were seated at computer terminals and separated by partitions. Instructions were distributed and read aloud. First, instructions for a risk attitude task were read and the corresponding decisions taken with pen and paper.Footnote 14 Then, time choice instructions were read and the corresponding decisions were made via computers. Finally, a questionnaire was distributed. No communication among subjects was allowed. Including instruction and reading time, a session lasted between 2 and 2.5 h. Overall, the average payment per subject was 16.06€ ($19.11).",22
38.0,2.0,Journal of Risk and Uncertainty,19 February 2009,https://link.springer.com/article/10.1007/s11166-009-9063-3,Learning from mistakes: What do inconsistent choices over risk tell us?,April 2009,Sarah Jacobson,Ragan Petrie,,Female,Unknown,Unknown,Female,"The formal financial sector in Rwanda is limited and is consistently rated as one of the worst in the world. Because of its limitations, many Rwandans rely on informal channels for credit. We look at three common types of informal credit (savings groups, insurance groups and informal loans) and formal credit offered through a bank or credit union. Savings groups (tontines) are rotating credit associations that allow members to pool risk. Members deposit a fixed amount of money at a fixed interval (typically monthly), and once every interval, one member of the group receives all the money deposited by the members. Members may leave the group without penalty once a cycle in which all group members receive the pool of money is complete. Groups vary in size and in the interval length (e.g. monthly or every 2 months). These groups are common and popular in Rwanda. We expect risk-averse individuals to be more likely to join this group. Insurance groups (groupes d’entraide) come in two general forms. The first is a rotating work group for construction or agricultural work. Members exchange labor to help other members. The second offers financial assistance in the case of a bad shock (i.e. death, illness). It functions as insurance. More generally, the group may offer moral support. These groups sometimes also have a religious component to them as many are organized by churches. Members typically pay a monthly fee to belong.Footnote 5 Since the risk-pooling nature of these groups is not only monetary, we do not expect risk aversion over money to be as strongly correlated with membership. Informal loans are usually short-term, small, and are largely used for immediate consumption smoothing. They are widespread in Rwanda and almost never carry monetary interest. Most are store credit or cash loans from family and friends. While these loans are interest free, they do carry an expectation that the favor will be reciprocated. Default on these loans is risky since interactions with friends and family are expected to be long term. Not paying back this type of loan may imply not being able to borrow in the future. Further, taking no-interest loans may obligate a person to reciprocate to the lender in the future. We expect risk-averse individuals to be less likely to take out these loans. Formal credit is not widely accessible due to large financial barriers. To be eligible to apply for a formal loan, an individual must pay an application fee and maintain an account in the bank or credit union. Minimum deposits in banks are often very high, and membership in a credit union requires paying a small fee.Footnote 6 Formal banks and credit unions are relatively stable. However, prior to 2000, credit unions did have a reputation for making loans and not asking that they be paid back. Since 2000, regulation has made credit unions more accountable and stable.Footnote 7 Formal loans are primarily used for business and construction, rather than consumption smoothing. It is not clear how risk aversion will affect the probability of taking out a formal loan because the barriers to entry clearly select a certain segment of the population. This population might be more risk averse or more risk seeking.",91
38.0,2.0,Journal of Risk and Uncertainty,17 February 2009,https://link.springer.com/article/10.1007/s11166-009-9064-2,Conditional payments and self-protection,April 2009,Liqun Liu,Andrew J. Rettenmaier,Thomas R. Saving,Unknown,Male,Male,Male,"There exist two frameworks for self-protection: the state-independent framework and the state-dependent framework. The state-independent framework applies in situations where an uncertain loss is monetary in nature (such as the destruction of a house by a tornado), whereas the state-dependent framework applies in situations where an uncertain loss is non-monetary in nature (such as illness, imprisonment, death, or loss of a loved one). In the following, we first present the expected utility functions under both the state-independent and the state-dependent frameworks for the traditionally assumed upfront payment financing, and then present the expected utility functions under the two frameworks for an alternative, conditional payment financing. The state-independent (SI) framework, as in Ehrlich and Becker (1972), consists of initial wealth W
0, monetary loss L, a strictly increasing and concave utility function on wealth U(W), and a strictly increasing and concave probability function q(I), where q is the probability of realizing the favorable, no-loss outcome and I is the monetary value of self-protection investment.Footnote 5 With the upfront payment (UP) traditionally assumed in the literature of self-protection, the self-protection investment I is paid by the consumer regardless of the outcome realized. Therefore, the expected utility for the state-independent case, as a function of I, is
 The state-dependent (SD) framework, as in Jones-Lee (1974) and Cook and Graham (1977), consists of initial wealth W
0, two strictly increasing and concave utility functions on wealth respectively defined for two states of existence G(W) (for a “good” state) and B(W) (for a “bad” state), where G(W) > B(W) and G′(W) > B′(W),Footnote 6 and a strictly increasing and concave probability function q(I), where q is the probability of realizing the good state and I is the self-protection investment. Under UP financing, the expected utility function for the state-dependent case is
 UP financing has been exclusively assumed by the extant economic studies on self-protection. However, while UP is probably the most natural financing method for self-protection, it is by no means the only one. In reality, individuals can hire professionals to provide protection against a potential loss, with fee payment conditional on the loss being avoided. That is, self-protection can also be financed by the conditional payment such that the professional hired to reduce the probability of a loss gets paid if and only if the favorable outcome is realized. Because the professional gets paid only if the outcome is favorable under conditional payment (CP), the conditional fee in CP financing must exceed the uniform fee in UP financing. With the assumption that the safety provider can diversify the risk—which is essentially true when the safety provider has many cases or is affiliated with a large organization such as a law firm or a hospital—and that the market for safety provision is competitive, providers earn a zero expected profit. Under CP financing, therefore, the professional who does not receive anything when the unfavorable outcome is realized must get I/q(I) when the favorable outcome is realized—so the expected payoff is I—in order for her to make an effort that is worth I. With CP, the expected utility functions under the state-independent framework and the state-dependent framework are respectively
 and
",13
38.0,3.0,Journal of Risk and Uncertainty,10 March 2009,https://link.springer.com/article/10.1007/s11166-009-9062-4,Reconciling support theory and the book-making principle,June 2009,Enrico Diecidue,Dolchai La-ornual,,Male,Unknown,Unknown,Male,"Support theory is a descriptive representation of subjective probability originally proposed by Tversky and Koehler (1994) and later revised by Rottenstreich and Tversky (1997). The theory is a popular descriptive alternative to Bayesian probability (Idson et al. 2001). It is mathematically founded and psychologically appealing. The main assumption of support theory is the property of non-extensionality, which simply states that an individual’s probability judgment for a particular event is dependent on the provided description for that event (Fischhoff et al. 1978; Humphrey 1995; Starmer and Sugden 1993). In general, support theory postulates that the more explicitly an event is described with respect to its composition, the more support it receives and, thus, the higher the judged probability for that event. We now present the essential mathematical formulation of support theory (Tversky and Koehler 1994; Rottenstreich and Tversky 1997) that is relevant to our paper. Support theory is based on the distinction between events, which are subsets of the state space T, and hypotheses or descriptions of the events, which are elements of the set of hypotheses H. Support theory assumes that each description A ∈ H corresponds to a unique event A′ ⊆ T. A description A is elementary if A′ ∈ T. Descriptions A, B ∈ H are exclusive if A′ ∩ B′ = ∅. The assumption of non-extensionality implies that different descriptions A and B can map to the same event, i.e., A′ = B′. In this case, the two descriptions, A and B, are considered to be coextensional. Support theory proposes a ratio scale s, assigning to each description, a non-negative real number such that for any pair of exclusive descriptions A, B ∈ H,
 
P(A, B) is the judged probability that description A rather than description B is true and its equivalent counterpart in classical probability is P{A′|A′ ∪ B′}. The ratio scale s can be interpreted as the degree of support or the strength of evidence for a particular description that could be based on objective data, subjective impression, or personal reasons (Tversky and Koehler 1994). A description of an event according to support theory can be categorized as either an implicit hypothesis or an explicit disjunction. An implicit hypothesis such as A ∈ H is essentially a holistic description of an event. An explicit disjunction denoted by (A
1 ∨ A
2) ∈ H for example, is in contrast a description of an event as decomposition of two or more of its exclusive and exhaustive subsets. The following condition holds for A and (A
1 ∨ A
2) that are coextensional, i.e., A′ = (A
1 ∨ A
2)′.
 The first inequality in Expression 2, known as implicit subadditivity, asserts that the support for an implicit hypothesis A is less than that for its coextensional explicit disjunction (A
1 ∨ A
2). The second relation, which was originally an equality in Tversky and Koehler (1994), is the explicit subadditivity proposed by Rottenstreich and Tversky (1997). This inequality states that the support for an explicit disjunction (A
1 ∨ A
2) is still less than the sum of supports for the exclusive and exhaustive subsets s(A
1) + s(A
2).Footnote 1 Implicit subadditivity is due to the fact that people do not “unpack” an implicit hypothesis (Tversky and Koehler 1994). Unpacking is the process of breaking down an event into its exclusive components and adding up their supports. Explicit subadditivity is the result of people tending to “repack” an explicit disjunction (Rottenstreich and Tversky 1997). Repacking is the reverse process of combining exclusive components of an event into its entirety and considering the associated support. The consequence of the two subadditivities of supports in Expression 2 leads to subadditivities of judged probabilities as follows:
 where \(R\left( {A,B} \right) = \frac{{P\left( {A,B} \right)}}{{P\left( {B,A} \right)}} = \frac{{s\left( A \right)}}{{s\left( B \right)}}\) represents the probability ratio. While the focus of support theory is on numerical judgments of probability, there exist empirical evidence that probability judgments as assumed by support theory can affect people’s decisions and evaluations of uncertain prospects in insurance (Johnson et al. 1993), medical decisions (Redelmeier et al. 1995), and sports gambling (Ayton 1997). In a series of experiments, Fox and Tversky (1998) also find that different descriptions of an event may affect a person’s willingness to act and that people are willing to pay more for a prospect when its components are evaluated separately. These observations have decision analysis implications. That is, betting on prospects whose subjective probabilities for the payoffs are derived from support theory may lead to sure losses. It is to this implication that we now turn.",
38.0,3.0,Journal of Risk and Uncertainty,21 April 2009,https://link.springer.com/article/10.1007/s11166-009-9068-y,Valuing risks of death from terrorism and natural disasters,June 2009,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
38.0,3.0,Journal of Risk and Uncertainty,09 May 2009,https://link.springer.com/article/10.1007/s11166-009-9069-x,Valuing a homeland security policy: Countermeasures for the threats from shoulder mounted missiles,June 2009,V. Kerry Smith,Carol Mansfield,Laurel Clayton,Unknown,,Female,Mix,,
38.0,3.0,Journal of Risk and Uncertainty,24 April 2009,https://link.springer.com/article/10.1007/s11166-009-9067-z,Does television terrify tourists? Effects of US television news on demand for tourism in Israel,June 2009,David Fielding,Anja Shortland,,Male,Female,Unknown,Mix,,
39.0,1.0,Journal of Risk and Uncertainty,25 June 2009,https://link.springer.com/article/10.1007/s11166-009-9071-3,A theory of medical decision making under uncertainty,August 2009,Edi Karni,,,Male,Unknown,Unknown,Male,"Let Θ denote a finite set whose elements are health diagnoses.Footnote 3 For every θ ∈ Θ let \(A\left( \theta \right) \) denote a finite set of actions, that is, descriptions of the medical aspects of the procedures in all their relevant aspects.Footnote 4 For instance, when the diagnosis calls for surgery, an action includes specification of the surgical procedure itself, the facility in which the operation is to take place, the surgeon who is to perform the surgery, the hospitalization and medical follow-up. Let \(\Omega _{a}\left( \theta \right) \) denote the finite set of possible outcomes that might result when the diagnosis is θ and the action taken is \(a\in A\left( \theta \right) ,\) and let \(\Omega \left( \theta \right) =\cup _{a\in A\left( \theta \right) }\Omega _{a}\left( \theta \right) \). Denote by \(P\left( \theta \right) \) the set of all probability distributions on \(\Omega \left( \theta \right) \) and assume that it is endowed with the \(\mathbb{R}^{\mid \Omega \left( \theta \right) \mid }\) topology. Clearly, \( P\left( \theta \right) \) contains the set of \(\{p_{\theta }\left( \cdot \mid a\right) \mid a\in A\left( \theta \right) \}\) of probability distributions on \(\Omega \left( \theta \right) \) conditional on the available actions. For each \(\omega \in \Omega \left( \theta \right) ,\) let I

ω
 be a closed and bounded interval in ℝ. A bet, f, is an element of the product set \(F\left( \theta \right) :=\mathsf{\Pi }_{\omega \in \Omega \left( \theta \right) }I_{\omega },\ \)representing outcome-contingent monetary payoffs. For instance, one may bet on the outcome of a bypass surgery according to which he wins x dollars if he survives the operation and losses y dollars if he does not, to be paid by his estate. Assume that \(F\left( \theta \right) \) is endowed with the \(\mathbb{R}^{\mid \Omega \left( \theta \right) \mid }\) topology. (Note that a pair \(\left( p,f\right) \) defines a lottery that, for every ω ∈ Ω, assigns the probability \(p\left( \omega \right) \) to the monetary prize \(f\left( \omega \right) \)). For every θ, the patient is supposed to be able to conceive of having to choose among elements of \(\mathbb{C}\left( \theta \right) :=A\left( \theta \right) \times P\left( \theta \right) \times F\left( \theta \right) \) consisting of an action in \(A\left( \theta \right) ,\) a probability in \(P\left( \theta \right) ,\) and a bet in \(F\left( \theta \right) .\) Then \(\mathbb{C}\left( \theta \right) \)is the  conceivable choice set. Since a medical decision problem always begins with a diagnosis which is then fixed, to simplify the notation, henceforth I suppress the diagnosis θ. A preference relation ≽ on ℂ is a binary relation that has the following interpretation: \(\left( a,p,f\right) \succeq \left( a^{\prime },p^{\prime },f^{\prime }\right) \) means that if the patient were in a position that requires him to choose between \(\left( a,p,f\right) \ \) and \(\left( a^{\prime },p^{\prime },f^{\prime }\right),\) he would choose \( \left( a,p,f\right) \) or be indifferent between the two alternatives. The induced strict preference relation, ≻ , and indifference relation, ~, are defined as usual and have the usual interpretation. I assume throughout that ≽ is a weak order, that is, (A.1)  ≽ is complete and transitive.
 To describe the structure of the preference relation, it is convenient to dissect it and examine each of its components separately. For each action, a, define a conditional preference relation ⊆≽ 
a
 on P×F by \(\left( p,f\right) \succeq _{a}\left( p^{\prime },f^{\prime }\right) \) if \(\left( a,p,f\right) \succeq \left( a,p^{\prime },f^{\prime }\right) .\) By definition and (A.1), ≽ 
a
 is a weak order. For the conditional preferences ≽ 
a
, I adopt the structure of Karni and Safra (2000). Specifically, I assume that the conditional preference relations in the set { ≽ 
a
|a ∈ A} satisfy the following axioms: (A.2)  Continuity—For all 
\(\left( p,f\right) \in P\times F\)
 the sets 
\(\left\{\left( p^{\prime },f^{\prime }\right) \mid \left( p^{\prime },f^{\prime }\right) \succeq _{a}\left( p,f\right) \right\}\)
 and 
\(\left\{\left( p^{\prime },f^{\prime }\right) \mid \left( p,f\right) \succeq _{a}\left( p^{\prime },f^{\prime }\right) \right\}\)
 are closed in the product topology.
 The second axiom requires that every outcome matters. Formally, let e
ω be the ω −th unit vector in ℝ|Ω| (that is, e
ω ∈ P is the degenerate probability distribution that assigns the unit probability mass to ω) then, (A.3)  Coordinate Essentiality—For all ω ∈ Ω, there are f,f′ ∈ F such that 
\({}\left( e^{\omega },f\right) \succ _{a}\left( e^{\omega },f^{\prime }\right) . \)
 The next axiom requires that the evaluation of outcome-contingent payoffs be independent in the sense that preferences among alternatives of the form \( (e^{\omega },(r,f_{-\omega }){\kern-.25pt}),\) where \((r,f_{-\omega })\!:=\!\left( f{\kern-.25pt}\left( \omega _{1}{\kern-.25pt}\right){\kern-.25pt} ,...,f{\kern-.25pt}\left( \omega _{i-1}{\kern-.25pt}\right) ,r,f{\kern-.25pt}\left( \omega _{i+1}{\kern-.25pt}\right){\kern-.25pt} ,...,{\kern-.25pt}f{\kern-.25pt}\left({\kern-.25pt} \omega _{n}\right){\kern-2pt}\right){\kern-.25pt},\) depend solely on the payoff of the bet f if the outcome ω obtains. Formally, (A.4)  Certainty Principle — For all f,f′,f
″,f
″′ ∈ F, 
\((e^{\omega },(x,f_{-\omega }))\succeq _{a}(e^{\omega },(y,f_{-\omega }^{\prime }))\)
 if and only if 
\((e^{\omega },(x,f_{-\omega }^{\prime \prime }))\succeq _{a}(e^{\omega },(y,f_{-\omega }^{\prime \prime \prime })).\)
 Define the partial mixture operation on P×F as follows: for every given f ∈ F, \(\left( p,f\right) ,\left( p^{\prime },f\right) \ \)and \( \alpha \in \left[ 0,1\right] ,\)
\(\alpha \left( p,f\right) +\left( 1-\alpha \right) \left( p^{\prime },f\right) =\left( \alpha p+\left( 1-\alpha \right) p^{\prime },f\right) .\) This may be interpreted as a two-stage lottery in which, in the first stage, the alternatives \(\left( p,f\right) \) and \(\left( p^{\prime },f\right) \) obtain with probabilities α and \(\left( 1-\alpha \right) ,\) respectively. In the second stage, the payoff of f is determined by the lottery, p or p′, that was chosen in the first stage. With this interpretation in mind, assume that the decision maker prefers \(\left( p,f\right) \) over (p′,f′) and \( \left( q,f\right) \) over (q′,f′). Moreover, assume that if a decision maker faces a choice between the alternatives \(L=\left( \alpha p+(1-\alpha )q,f\right) \) and \(L^{\prime }=\left( \alpha p^{\prime }+(1-\alpha )q^{\prime },f^{\prime }\right) \) he reasons as follows: if the event whose probability is α is realized, he participates in the lottery \(\left( p,f\right) \) if he has chosen L and in the lottery \(\left( p^{\prime }\mathbf{,}f^{\prime }\right) \) if he has chosen L′. Conditional on the realization of this event, he is better off with L. By the same logic, he would also prefer L over L′ conditional on the realization of the event whose probability is 1 − α. Consequently, he prefers L over L′ unconditionally. Formally, (A.5)  Constrained Independence— For all 
\(\left( p,f\right) \)
, 
\(\left( q,f\right) \)
, 
\(\left( p^{\prime },f^{\prime }\right) \)
, 
\(\left( q^{\prime },f^{\prime }\right) \)
 in 
P×F and 
\(\alpha \in \lbrack 0,1)\)
 if 
\(\left( p,f\right) \sim _{a}\left( p^{\prime },f^{\prime }\right) \)
then 
\(\left( q,f\right) \succeq _{a}\left( q^{\prime },f^{\prime }\right) \)
 if and only if 
\(\left( \alpha p+(1-\alpha )q,f\right) \succeq _{a}\left( \alpha p^{\prime }+(1-\alpha )q^{\prime },f^{\prime }\right) .\)
 A real valued function V

a
 on P×F is said to represent ≽ 
a
 if, for all \(\left( p,f\right) \) and \(\left( p^{\prime },f^{\prime }\right) \) in P×F, \(\left( p,f\right) \succeq _{a}\left( p^{\prime },f^{\prime }\right) \) if and only if \(V_{a}\left( p,f\right) \geq V_{a}\left( p^{\prime },f^{\prime }\right) .\) If \(V_{a}\left( p,f\right) :=\sum_{\omega \in \Omega }p\left( \omega \right) U_{a}(f\left( \omega \right) ,\omega )\) for some functions, U

a
(·,ω):ℝ →ℝ,ω ∈ Ω, it is a linear representation of ≽ 
a
. Let \(\sum_{\omega \in \Omega }p\left( \omega \right) U_{a}(f\left( \omega \right) ,\omega )\) represent ≽ 
a
. The functions \(U_{a}\left( \cdot ,\omega \right) ,\omega \in \Omega \) are said to be unique up to uniform positive linear transformation if, for any other linear representation of ≽ 
a
, \(\sum_{\omega \in \Omega }p\left( \omega \right) \hat{U}(f\left( \omega \right) ,\omega ),\)
\( \hat{U}(\cdot ,\omega )=\beta U(\cdot ,\omega )+\gamma ,\beta >0,\) for all ω ∈ Ω. The next theorem restates, in the terminology of this paper, Theorem 2 of Karni and Safra (2000). 
 Let ≽ 
a

 be a binary relation on P ×F. Then the following conditions are equivalent:
 
 (a) ≽ 
a

 is a weak order satisfying (A.2)–(A.5).
 
 (b) There exist continuous, non-constant, functions 
V

a
:P×F→ℝ  and 
\(U_{a}\left( \cdot ,\omega \right) :\mathbb{R}\rightarrow \mathbb{R},\)
ω ∈ Ω, 
such that V

a

represents ≽ 
a

 and, for all 
\(\left( p,f\right) \in\)
\(P\times F\mathit{,}\)
 
Moreover, the functions 
\(U_{a}\left( \cdot ,\omega \right) ,\omega \in \Omega \left( \theta \right) ,\)
 are unique up to uniform positive linear transformation.
 The proof is given in Karni and Safra (2000).Footnote 5
 Medical treatments are costly in terms of time and discomfort. These are temporary, however, and unlikely to alter the patient’s risk attitudes. To capture this aspect of the patient’s preferences, the next axiom asserts that the risk attitudes are action independent. (A.6)  Action-independent risk attitudes—
For all 
a,a′ ∈ A, \(\succeq _{a}=\succeq _{a^{\prime }}.\)
 The next axiom asserts that preferences on mixtures of lotteries are action independent. Formally, (A.7)  Action-independent lottery mixtures—
For all
a,a′ ∈ A, f ∈ F and p,p′,p
″,p
″′ ∈ P, if
\(\left( a,p,f\right) \sim \left( a^{\prime },p^{\prime },f\right) \)
and
\(\,\left( a,p^{\prime \prime },f\right) \sim \left( a^{\prime },p^{\prime \prime \prime },f\right) \ \)
then
\(\left( a,\alpha p+\left( 1-\alpha \right) p^{\prime \prime },f\right) \sim \left( a^{\prime },\alpha p^{\prime }+\left( 1-\alpha \right) p^{\prime \prime \prime },f\right) \)
for all
\(\alpha \in \left( 0,1\right) .\)
 A certain richness of the choice space is necessary to link distinct action-contingent preferences. Specifically, there must be some staggered utility overlap among the actions. Formalizing this idea, it is useful to use the following additional terminology: two actions, a and a′, are said to be elementarily linked at f ∈ F if there are \(\bar{p} ,\underline{p},\bar{p}^{\prime },\underline{p}^{\prime }\in P\) satisfying \( \left( \bar{p},f\right) \succ _{a}\big( \underline{p},f\big) \) such that \( \left( a,\bar{p},f\right) \sim \left( a^{\prime },\bar{p}^{\prime },f\right) \) and \(\big( a,\underline{p},f\big) \sim \big( a^{\prime },\underline{p} ^{\prime },f\big) .\)
\(\big(\)Note that, by transitivity, \(\left( \bar{p}^{\prime },f\right) \succ _{a^{\prime }}\big( \underline{p}^{\prime },f\big) \big)\). The treatments a and a′ are linked if there is a sequence of actions a
1,...,a

n
 such that \(a=a_{1},a^{\prime }=a_{n}\) and the actions a

i
 and a

i + 1 are elementarily linked at f

i
 ∈ F, i = 1,...,n − 1. The set of actions is linked if all its elements are linked. The next theorem is the main result. 
 Let ≽ be a preference relation on ℂ  and denote by { ≽ 
a
|a ∈ A} the induced action-contingent preference relations on 
P×F. If A
 is linked, then the following conditions are equivalent:
 
 (a) ≽ is a weak order and the induced preference relations { ≽ 
a
|a ∈ A} satisfy (A.2) – (A.7).
 
 (b) There exist continuous nonconstant functions V:ℂ →ℝ, \(U\left( \cdot ,\omega \right) :I_{\omega }\rightarrow \mathbb{R},\)
ω ∈ Ω, 
λ:A→ℝ + + 
 and v:A→ℝ such that V
 represents ≽ and for all 
\(\left( a,p,f\right) \in \mathbb{C},\)
 
 Moreover, the functions 
\(U\left( \cdot ,\omega \right) ,\omega \in \Omega \)
 are unique up to a uniform positive linear transformation and, given
\(U\left( \cdot ,\omega \right) ,\omega \in \Omega ,\)
λ and
v are unique.
 The probabilities and the financial consequences of the different outcomes contingent on patient characteristics and available actions are determined by the “state of the art,” or technology. Formally, a technology is a function \(t:C\times A\left( \theta \right) \rightarrow P\left( \theta \right) \times F\left( \theta \right) \ \) that associates with each vector of personal characteristics and action a probability distribution on \(\Omega \left( \theta \right) \) and a bet in \( F\left( \theta \right) \) depicting the financial consequences associated with the different outcomes. These consequences depend on the patient’s health, disability, and life insurance coverage and occupation which, in turn, determine the potential loss of income. A medical decision entails a choice among alternatives in \(A\left( \theta \right) .\) Given the patient’s characteristics, c, and the technology, t, define a preference relation on \(A\left( \theta \right) \) by \(a\succcurlyeq _{c}a^{\prime }\) if and only if \(\left( a,t\left( a;c\right) \right) :=\left( a,p\left( a;c\right) ,f\left( a;c\right) \right) \succeq \left( a^{\prime },p\left( a^{\prime },c\right) ,f\left( a,c\right) \right) :=\left( a^{\prime },t\left( a^{\prime };c\right) \right) .\) Thus, given the patient’s characteristics and the existing technology, the application of Theorem 2 implies that, for all \(a,a^{\prime }\in A\left( \theta \right) ,\)
\(a\succcurlyeq _{c}a^{\prime }\) if and only if 
 Note that the choice of a affects the patient’s well-being in two distinct ways. First, as already mentioned, the alternative actions may involve different degrees of pain, suffering, and inconvenience. This aspect of the choice of action is captured by the functions λ and v. Second, the patient’s insurance may cover the cost of some actions fully and the cost of others only partially or not at all and, in addition, depending on his occupation, the various outcomes may have distinct financial implications. These financial aspects of the decision are captured by the dependence of \(f\left( \cdot ;a,c\right) \) on a. For instance, if the patient’s insurance fully covers the medical costs of the action a then \( f\left( \cdot ;a,c\right) =f\left( \cdot ;c\right) ,\) where \(f\left( \cdot ;c\right) \) depicts the contingent loss of income (uncovered by insurance). If the medical costs of some actions are coinsured (for instance, under coinsurance, only x percent of the cost of a is covered) then \(f\left( \cdot ;a,c\right) =f\left( \cdot ;c\right) -(1-x)g\left( a\right) ,\) where \( g\left( a\right) \) denotes the full financial cost of a.",25
39.0,1.0,Journal of Risk and Uncertainty,10 July 2009,https://link.springer.com/article/10.1007/s11166-009-9072-2,Insurance decisions for low-probability losses,August 2009,Susan K. Laury,Melayne Morgan McInnes,J. Todd Swarthout,Female,Unknown,Unknown,Female,,65
39.0,1.0,Journal of Risk and Uncertainty,17 April 2009,https://link.springer.com/article/10.1007/s11166-009-9066-0,Probability weighting and the ‘level’ and ‘spacing’ of outcomes: An experimental study over losses,August 2009,Nathalie Etchart-Vincent,,,Female,Unknown,Unknown,Female,"First, just recall that the PT model introduces two different weighting functions depending on the domain of consequences. They are denoted w+ and w− for the gain and loss domains respectively. In the simplified framework of a two-outcome non-zero lottery P = (x2, p; x1) with x2 < x1 < 0, the PT valuation of P is given by VPT(P) = w−(p)U(x2) + (1–w−(p))U(x1), where U is the utility function. Besides, the PT valuation of a mixed lottery P = (x2, p; x1, 1– p) with x2 >  0  > x1 is given by VPT(P) = w+(p)U(x2) + w−(1– p)U(x1). Now, let us present the basic principles of the two-stage semi-parametric procedure that was used to elicit the probability weighting functions at the individual level. It is basically the same as in Etchart-Vincent (2004). First, the utility function was elicited for each subject, using the now well-established non-parametric trade-off method (introduced by Wakker and Deneffe 1996 and applied to losses by Abdellaoui 2000; Etchart-Vincent 2004; Fennema and Van Assen 1999). As in Etchart-Vincent (2004), utility was individually investigated over small as well as large losses (until around 15,000 €). But in the 2004 study, only two distinct local parts of the utility function were obtained. Here, utility was elicited on a unique and wide interval I, enabling us to work with heterogeneous lotteries within this interval. In the second stage of the procedure, a certainty-equivalent method was introduced, using some points of the previously elicited utility function as well as its parametric fitting, to build the weighting function in each of three loss situations, called the ‘small loss situation’ (S), ‘large loss situation’ (L) and ‘small-large loss situation’ (S/L) respectively. Let us briefly recall the main features of the trade-off (TO) method. This method consists in eliciting a sequence of outcomes that are equally spaced in terms of utility. Using its more reliable ‘outward’ version (see Fennema and van Assen 1999), the general principle of the method is the following. Given fixed outcomes x0, r and R such that x0 < 0 < r < R, the subject is asked to make successive choices allowing (through a l-iterations bisection process) to determine the outcome x1 < x0 that makes her indifferent between the (mixed) lotteries (x0, p; r, 1–
p) and (x1, p; R, 1–
p). Then, x1 is used as an input, and a similar choice-based bisection process allows to determine the outcome x2 < x1 that makes the subject indifferent between (x1, p; r, 1–p) and (x2, p; R, 1–p). The procedure is implemented n times in order to obtain a sequence xn, …, xi, …, x0. Under PT, indifference between (xi, p; r, 1–p) and (xi+1, p; R, 1–p) implies that, for 0 ≤ i ≤ n−1, w−(p)U(xi) + w+(1–p)U(r) = w−(p)U(xi+1) + w+(1–p)U(R). In other words:
 from which the following equality: +
 Equation 2 implies that, for the subject under consideration, the xis are equally spaced in terms of utility. Using the conventional normalization U(x0) = 0 and U(xn) = –1, one gets U(xi) = – i/n, i = 0, …, n. Note that, by construction, the TO method neutralizes the role of probability in the elicitation process. It thus avoids those biases that are due to probability weighting and are known to distort traditional assessment methods (see Wakker and Deneffe 1996 for a critical review of these methods).Footnote 6
 Now, let us present the general principle of the certainty-equivalent method that was used in the second stage of the procedure to build the weighting function under a given payoff condition: the decision maker is asked to make successive choices allowing—at the end of a m-iterations bisection process—to determine the value CEj that makes her indifferent between two-outcome Lottery A = (xk, pj; xi, 1–pj), with xk and xi two elements of the former standard sequence and xk < xi < 0, and degenerate Lottery B = (CEj, 1). In generic Lottery A = (xk, pj; xi, 1–pj), k and i can be chosen so as to make xk and xi eligible as bounds of the payoff condition under consideration. By construction, xk < CEj < xi. Under PT, and with xk < xi < 0, the indifference between (xk, pj; xi, 1–pj) and CEj entails that w−(pj)U(xk) + (1–w−(pj))U(xi) = U(CEj). By construction of the TO method, U(xi) = –i/n and U(xk) = –k/n. So:
 The procedure thus makes it possible to determine algebraically the ‘subjective weight’ w−(pj) for any probability pj. By applying it for different values of pj, the whole weighting function can be obtained under the payoff condition given by [xk; xi]. In the present study, the weighting function had to be built under each of the S, L and S/L payoff conditions. In each case, the points (of the previously elicited standard sequence) xk and xi had to be properly chosen so as to be eligible as the bounds of the payoff condition under consideration. So, S was defined by k = 1 and i = 0, L by k = n and i = n–1, and S/L by k = n and i = 0 (see Fig. 1). Note that, even though k and i were the same for all the subjects, the values xk and xi were specific to each subject (since they were elements of her endogenously elicited utility function).
 The standard sequence and its three sub-intervals The interest of the above-described procedure is that it makes it possible to roughly disentangle the ‘level’ effect from the ‘spacing’ effect.Footnote 7 Indeed:
 w
−L
 and w
−S/L
 were both obtained using the last point of the standard sequence xn. But in the L situation, the alternative consequence was xn−1, instead of x0 in the S/L situation. The comparison between w
−L
 and w
−S/L
 thus makes it possible to investigate how probability weighting is to be affected by the ‘spacing’ of consequences. w
−L
 and w
−S
 were both constructed using homogeneous lotteries. Indeed, the distance (in terms of utility) between xn and xn−1 is (by construction) subjectively equivalent to the distance between x1 and x0. By neutralizing the ‘spacing’ effect, the comparison between the functions w
−L
 and w
−S
 thus makes it possible to investigate the sole impact of the absolute level of consequences. At this stage, an important point to make is that certainty equivalents CEjs are unlikely to be elements of the previously elicited standard sequence. So the U(CEj)s and w−(pj)s could not be obtained without fitting U parametrically (this is why our procedure should be called ‘semi-parametric’).Footnote 8 Moreover, U needs to be especially well fitted to allow reliable calculations using Eq. 3. This is why each individual utility function was fitted using three different specifications, and for each subject the best fitting specification was retained. The first two—standard—specifications are the one-parameter POWer function, with UPOW(x) = –(–x)α and α>0 (Tversky and Kahneman 1992) and the two-parameter EXpo-POWer function, with UEXPOW(x) = [1–exp(–β(–x)α)]/(exp(–β)–1), α>0 and β>0 (Abdellaoui et al. 2007; Saha 1993).Footnote 9
 The third specification is more unusual. Denoted GE (with reference to Goldstein and Einhorn, who introduced it in their 1987 paper), it is given by UGE(x) = \( - \frac{{\delta \left( { - {\text{x}}} \right)^{\gamma } }}{{\delta \left( { - {\text{x}}} \right)^{\gamma } + \left( {1 + {\text{x}}} \right)^{\gamma } }} \), with δ>0 and γ>0. Because it allows inverse-S shape, this specification has been extensively used in the literature to fit probability weighting functions. Since 25% of our individual utility functions exhibited an inverse-S shapeFootnote 10, the GE specification was used for the pragmatic purpose of best fitting.",28
39.0,1.0,Journal of Risk and Uncertainty,13 May 2009,https://link.springer.com/article/10.1007/s11166-009-9070-4,Precautionary behavior and willingness to pay for a mortality risk reduction: Searching for the expected relationship,August 2009,Mikael Svensson,,,Male,Unknown,Unknown,Male,"The data in this paper comes from a mail survey conducted in the spring of 2006 in the city of Örebro in Sweden. Örebro is situated approximately 200 km west of the capital Stockholm and has a population of 129,000 with an urban population of 97,000. Örebro is the regional capital in the county of Örebro, and also has a university population of around 10,000. The survey was sent to a random sample of 1,500 individuals based on the Swedish Governments’ Personal- and Address-Register (SPAR), which includes all individuals with an address and currently living in Sweden (both Swedish and non-Swedish citizens). The survey consisted of three parts. Part A contained questions on socio-demographics, risk behavior, attitudes to risk and traffic safety. Part B included the hypothetical scenario where the respondents were given a description of a private and/or public good that would reduce the number of fatal and serious accidentsFootnote 5 in Örebro and subsequently asked for their willingness to pay for the reduction. In this paper we only use the stated WTP for the private safety good, in order to facilitate comparisons with individual (private) risk behavior.Footnote 6 This part was framed using the Swedish “Vision Zero” as a starting point. The “Vision Zero” is a long-term road-safety objective in Sweden that states that roads and vehicles should be designed so as to prevent accidents from happening, but when they do happen to protect road users from fatalities and serious injuries. It is particularly suitable to perform this survey in the city of Örebro, since a well-known local program has been implemented along a major walking and biking route to demonstrate how this “vision” may be achieved in practice to protect these users. Part C of the survey asked the respondents about different attitudes towards private and public provision of goods (not used in this paper). Regarding mortality risks, the respondents were informed that annually four persons are killed and twelve persons are seriously injured from road traffic accidents in Örebro. The baseline risk is then the sum of this (16) divided by the population.Footnote 7 The good being offered to the respondents is a road safety improvement, which would reduce the average risk for both fatal and serious injuries by 50%. The WTP question was framed as: “How much would you at most be willing to pay each year for renting the safety product that cuts your own risk for fatal and serious traffic accidents in half?” Respondents stated an open-ended amount indicating their maximum WTP for the risk reduction.Footnote 8 Respondents that answered zero on the WTP question were asked a follow-up question to elicit if the response was a protest response or a “true” zero.Footnote 9
 The motivation to ask for their WTP for both a reduction in fatality risk and serious injury risk is that the “Vision Zero” targets both fatalities and severe injuries. In addition, a scenario where the proposed measure reduces both fatalities and severe injuries seems more plausible than a scenario targeting only fatalities.Footnote 10 Estimating VSL and the value of statistical severe injury (VSSI) can be estimated ex-post by using “death-rate-equivalents” (Persson and Cedervall 1991; Viscusi et al. 1991; Hultkrantz et al. 2006). Regarding the questions on risk behavior, which were asked before the CV-part and WTP-question of the survey, they were framed as: “How often do you do the following…//…use seat belts when you drive (or are a front seat passenger in) a car?” All in all six such questions were asked on different types of precautionary behavior (use of front/rear seat belt, bicycle helmet, bicycle light, reflector and adherence to speed limits). Respondents could choose among five answer categories, ranging from always performing the precautionary behavior to never performing it. It was also possible to tick a sixth box if they felt the question was irrelevant for them (e.g. they never go by car). After one reminder, the overall response rate of the survey was 59%. Considering that we only use surveys with the private good WTP question, not all data can be used here (1,250 observations were theoretical maximum). Excluding public good WTP surveys and returned surveys with missing observations on variables used in the paper, the dataset used in this paper consists of 552 observations. Table 1 below contains descriptive statistics of the sample. The mean age is 43 years and there are slightly more females in the sample (52%), and 46% have a university education of some sort. Only 60% of the respondents have full-time employment, but considering that individuals up to 74 years of age were in the survey, a significant share of senior citizens is included here. Looking at individuals aged 16 to 74 in Örebro, the employment ratio is 59%, hence the sample is in close accordance to the actual statistics for the city (Örebro 2007). A comment on the variable Beliefs is warranted as well. It measures the log of the perceived traffic fatality risk divided by the actual traffic fatality risk. If a respondent believes the risk is higher than the actual risk, this might imply that the respondent also perceives the risk reduction from wearing a seat belt as higher than the actual risk reduction, which may be important to control for.Footnote 11
 Regarding the different types of precautionary behavior, Table 2 below shows descriptive statistics. Almost 92% of respondents always use (front) seat belt. This figure is close to larger national surveys and observational studies on seat belt use in Sweden (Revkeldt and Labibes 2003). Individuals that do not always use seat belts tend to answer that they use them often or sometimes, very few never wear seat belts.
 Bicycle helmet use is the least frequent precautionary behavior. Almost half of the respondents use bicycle lights when riding in the dark, while reflector use is less popular. Most respondents state that they tend to obey speed limits often, but not always. The last row in Table 2 shows the percentage of respondents answering “not applicable”, e.g. because they never ride a bike, drive a car, etc. Table 3 shows some preliminary statistics on precautionary behavior tabulated with respect to different socio-demographic groups. Regarding age in all six precautionary behaviors, those that take precautions are older. There is also a clear pattern regarding gender, where females take more safety precautions in all six cases. This is consistent with results in Hakes and Viscusi (2007) and with gender differences in risk-taking (Byrnes et al. 1999). Regarding the income categories there are not any large differences, with the exception of the proportion that keep to speed limits; high income earners tend to report that they less frequently keep to speed limits. This could reflect higher opportunity costs among the high income earners (and perhaps the fine is less costly to them). Employed individuals tend to correlate with high income earners, and hence here we also see less of keeping to speed limits but more of using seat belts. The same is true for individuals with a university education.
 This section also provides descriptive statistics of the stated WTP and VSL for the mortality risk reduction based on the CV-part of the survey. Table 4 shows mean WTP for all respondents, as well as for the same socio-demographics as used in Table 3. Mean WTP for all respondents is 2,309 Swedish kronor (SEK), which is about $328. Table 4 also displays the VSL that follows, which is the stated WTP, divided by the risk reduction. The survey asked respondents about their WTP for a risk reduction targeting both fatal and severe injuries (value of a statistical severe injury, VSSI). To estimate a VSL from this one must use death rate equivalents (DRE) to convert WTP for serious injuries into fatalities (Viscusi et al. 1991; Hultkrantz et al. 2006). The DRE may be described as the ratio VSSI/VSL. The official Swedish value of DRE is 0.15 (Vägverket 2006). VSL may then be estimated according to:
 This gives a mean VSL of 77 million SEK ($11.0 million)Footnote 12. Table 4 shows how VSL varies by different socio-demographics as well. Mean WTP (and VSL) is higher for males compared to females. Individuals aged 31–50 display the highest VSL, while individuals aged 51 and older display the lowest VSL. VSL increases with income, and is higher than average for individuals with a university education as well as for employed individuals. In general the WTP answers show a skewed distribution, as is often the case. Figure 1 below shows a kernel density estimate graph to illustrate the distribution of answers. The median WTP (1,000 SEK) implies a VSL of 33.4 million SEK ($4.75 million), i.e. less than half compared to the mean.
 Kernel density estimate for stated WTP Comparing the descriptive statistics of WTP in Table 4 and precautionary behavior in Table 3, some differences are apparent. Females state a lower WTP, while they are more likely to be risk averse in their behavior. Risk aversion also increases with age, while WTP shows (raw comparison) an inverted U-shape with age. In the next section multivariate analysis will be performed to empirically test whether precautionary behavior increases with stated WTP.",16
39.0,1.0,Journal of Risk and Uncertainty,24 June 2009,https://link.springer.com/article/10.1007/s11166-009-9073-1,Time discounting: Declining impatience and interval effect,August 2009,Yusuke Kinari,Fumio Ohtake,Yoshiro Tsutsui,Male,Male,Male,Male,"Many studies, including those by Richards et al. (1999), Pender (1996), Kirby and Marakovic (1995), Myerson and Green (1995), Benzion et al. (1989), and Thaler (1981) typically asked subjects how much they will demand if, instead of receiving X dollars now, they receive it at time t (in the future). Varying t and X in their experiments, they drew the conclusion that per-period time discount rates diminish with delay, since the per-period time discount rate from now to t, R(0,t), is smaller than that from now to s, R(0,s), s < t, that is,
 In their deduction, however, they ignore the fact that at the same time, the interval changes from s to t. One can interpret inequality (1) as the per-period time discount rate decreases with an increase in the interval. In order to test declining impatience rigorously, we need to compare discount rates for various delays by controlling the intervals. Read (2001) conducts experiments, distinguishing explicitly the delay from the interval. By doing this, he elicited a pure effect of change in delay by fixing the intervals at 6 months, but found no evidence of declining impatience. Several studies that explicitly distinguish the delay and the interval have appeared. For example, Read and Roelofsma (2003) focus on the method that elicits time discounting from questions. They found that declining impatience is observed with the matching method that asks subjects of the amount at a specified date, which makes them indifferent to a specified option, but not with the choice method that asks subjects to choose the better one from specified two options. On the other hand, Read et al. (2005) focus on how the timing of two options is described. They found that declining impatience is observed when the timing is specified with calendar dates, but not when it is specified by the length of the delay. Thus, there are pros and cons on declining impatience even in experimental studies that explicitly distinguish the delay from the interval. Given these arguments, we examine whether declining impatience is really the case, setting the delay at an adequately short period. With regard to the results of Read (2001), we point out that his analysis has a problem, because even the shortest delay in his experiments is 6 months, which is too long to analyze the effect of a change in the delay. Frederick et al. (2002) reported that there is no evidence of declining impatience when the delay is over 1 year. In fact, using the matching method and a shorter delay than that of Read (2001), Read and Roelofsma (2003) found evidence of declining impatience. Ikeda et al. (2005) also found that a change in the delay longer than 1 month does not affect time discount rates. This research suggests that declining impatience is a phenomenon of short delays. In our experiment, we test much shorter delays, such as 1 day or 1 week, to find declining impatience within 8 weeks of delay, with the intervals being controlled. Another feature of our study is to employ a random order method in which options were presented to subjects randomly, irrespective of their past choice. Experimental conditions such as the length of the delay, the length of the interval, and the amount of the reward are chosen randomly for each question. In contrast, many studies used a sequential order method in which the options that are shown to subjects are arranged according to some experimental condition like discount rate. This method may suffer a bias originating from the sequential order. Read (2001) and Read and Roelofsma (2003) employ a logical order method according to which the future options available for subjects depend on the current options chosen by them. Although this method has an advantage that it is immune from a possible sequential order bias, it contains the risk that a series of subjects’ choices may be affected by the first presented options. In other words, a solution by logical order method may not achieve global maximum, but rather local maximum near the initial value.Footnote 3 The random order method has merits that make it immune from a possible sequential order bias, and so the global maximum is more warranted. A stressful burden is imposed upon subjects because the timing of acceptance and the amount of reward change randomly with every question, which may help to elicit subjects’ true preference. While Read (2001) found a negative answer to declining impatience, he found an affirmative answer to “subadditive time discounting” represented by the following inequality (2).
 where m stands for month(s), and β(a,b) indicates time discount rate for the period from time a to time b, that is, if receiving X dollars at time a is indifferent to receiving Y dollars at time b, β(a,b) is \( {{\left( {Y - X} \right)} \mathord{\left/{\vphantom {{\left( {Y - X} \right)} X}} \right.} X} \). Using the per-period time discount rate R(a,b), defining 6 months as a unit period, inequality 2 is rewritten as
 In short, “declining impatience”, falsely argued by studies so far that mixed the delay and the interval, is actually subadditivity and not true declining impatience: this is what Read (2001) found. In this paper, we define the interval effect such that the longer the interval, the lower the per-period time discount rate with the delay and magnitude effects being adjusted. That is,
 and we investigate whether the interval effect is confirmed with our subjects. It is easy to prove that inequality (3) is satisfied if inequality (4) is satisfied. Therefore, the interval effect is a sufficient condition for subadditive time discounting. In this sense, the interval effect is a more general concept than subadditive time discounting.",24
39.0,2.0,Journal of Risk and Uncertainty,25 August 2009,https://link.springer.com/article/10.1007/s11166-009-9076-y,Taste uncertainty and status quo effects in consumer choice,October 2009,Graham Loomes,Shepley Orr,Robert Sugden,Male,Unknown,Male,Male,"Reference-dependence in consumer choice is most commonly interpreted as an asymmetric attitude to gains and losses in each dimension of some commodity or attribute space, considered separately. We will call this the dimension-based approach. To see the intuition behind this approach, consider the experiment reported by Knetsch (1989), in which participants are divided equally between two treatments. In one treatment, each participant is endowed with a chocolate bar and is given the opportunity to exchange this for a coffee mug. In the other, participants are endowed with mugs, which they can exchange for chocolate bars. Contrary to the implications of conventional consumer theory, the proportion of participants choosing to exchange (summed over the two treatments) is significantly less than 50%. This is a status quo effect. One explanation of this finding is that it is the result of loss aversion with respect to consumption dimensions. In its most general form, the loss aversion hypotheses is that losses have more subjective weight than equal and opposite gains. If each participant perceives an exchange between a mug and a chocolate bar as a loss of one good and a gain of another, loss aversion will induce the status quo effect found by Knetsch. Tversky and Kahneman (1991)—henceforth, TK—develop this idea into a theory of reference-dependent consumer choice. To aid the exposition of this and other existing theories, we introduce some of the notation that we will need later when we present our own analysis. We consider the preferences of a given individual. These are defined in an n-dimensional space of consumption characteristics. Any non-negative vector of quantities of these characteristics is a bundle; typical bundles are denoted by x, y and z. In any given decision problem, one of these bundles is the reference point. A preference is a ranking of two bundles, assessed in relation to (or viewed from) some reference point; the reference point may be (but need not be) one of the two bundles in question. The proposition that x is weakly preferred to y, viewed from z is denoted x
\( \underline \succ \)
y | z; strict preference (\( \succ \)) and indifference (~) are denoted analogously. A preference structure is a set of such preference relations, one for each possible reference point. It is sometimes convenient to represent a preference structure by a real-valued function v(., .), such that v(x, z) ≥ v(y, z) if and only if x
\( \underline \succ \)
y | z. TK’s model defines a preference structure and imposes certain restrictions on it. TK describe this model as an ‘extension’ of prospect theory, Kahneman and Tversky’s (1979) theory of choice over money (or other one-dimensional) lotteries. The key features of the value function in prospect theory—reference-dependence, loss aversion, and diminishing sensitivity—are ‘applied to’ riskless consumer choice (TK, pp. 1039–40). However, the relationship between the two models is not made explicit. It seems that TK started from a dimension-based model in which
 where, for each characteristic i, w

i
(.) is a value function with the same properties as the corresponding function in prospect theory.Footnote 2 The special restrictions that TK impose on preference structures are implications of the basic model defined by (1), and are motivated by arguments which implicitly invoke the characteristic-based separability of that model.Footnote 3
 However, TK do not impose all the restrictions that are implied by the basic model. Were all those restrictions imposed, the resulting model would have certain unrealistic properties, which might be thought to disqualify it as a general-purpose consumer theory. First, the additively separable form of (1) does not allow characteristics to be complements; nor, in cases in which income effects are negligible, does it allow them to be substitutes. Second, preferences between bundles are independent of absolute levels of consumption; they depend only on increments and decrements of consumption, relative to the reference point. Third, because of diminishing sensitivity, indifference curves are concave to the origin in the region of characteristics space that is dominated by the reference point. (We will call this region the loss domain; the region that dominates the reference point is the gain domain.) In not imposing these restrictions, TK’s theory is a compromise between the basic model and Hicksian consumer theory. Kőszegi and Rabin (2006; henceforth KR) propose a theory of reference-dependent preferences which ‘build[s] on the essential intuitions’ of TK (p. 1134). KR are explicit in using an additively separable structure similar to that of TK’s basic model; as they explain, this structure ‘is at the crux of many implications of reference-dependent utility, including the endowment effect’ (p. 1138). However, they replace (1) by:
 where \( m_{i} {\left( {x_{i} } \right)} \) is ‘intrinsic consumption utility’ derived from characteristic i and μ(.) is a ‘universal gain-loss function’ with essentially the same properties as TK’s value function. This specification allows preferences to depend on absolute consumption, and does not entail the concavity of indifference curves in the loss domain (although this possibility is not ruled out). However, because the additive separability of the basic model is retained, KR’s model does not allow characteristics to be complements or (except as a result of income effects) substitutes. Because μ(.) is independent of characteristics, KR’s model has very strong implications about the relative strength of status quo effects in different circumstances. Here it is convenient to introduce another theoretical concept that will be important in our own analysis—the concept of exchange resistance. Since a status quo effect reveals an individual’s unwillingness to make exchanges, it is natural to measure the underlying attitude by identifying sequences of exchanges which, according to Hicksian consumer theory, the individual is indifferent about making, and then measuring any subjective resistance to those exchanges. Clearly, a Hicksian individual is indifferent about any sequence of exchanges which leads from one bundle back to itself. Thus, resistance to exchange can be measured by the net compensation required to induce an individual to complete such a cycle of exchanges. For simplicity, we consider a case in which n = 2. (This analysis can easily be generalised to cases in which there are three or more characteristics, and consumption of characteristics 3, ..., n is held constant.) Consider an individual for whom \( z = \left( {{z_1},{z_2}} \right) \) is the reference point. In conventional consumer theory, there is a uniquely-defined marginal rate of substitution of characteristic 2 for characteristic 1 at this point. In KR’s model, in contrast, loss aversion can induce a kink at the zero point of μ(.), with the implication that indifference curves are kinked at the reference point. Thus, there is a distinction between marginal WTP for characteristic 1 in terms of units of characteristic 2 (the absolute value of the rate at which decrements of characteristic 2 compensate for increments of characteristic 1), denoted r

WTP21
, and marginal WTA for characteristic 1 in terms of units of characteristic 2 (the absolute value of the rate at which increments of characteristic 2 compensate for decrements of characteristic 1), denoted r

WTA21
. Notice that \( {r_{21}}^{\text{WTP}} \equiv 1/{r_{12}}^{\text{WTA}} \) and \( {r_{21}}^{\text{WTA}} \equiv 1/{r_{12}}^{\text{WTP}} \). Starting from z, the individual is (at the margin) just willing to give up one unit of characteristic 2 for every \( {r_{12}}^{{\text{WTA}}} \) units of characteristic 1 that she gains in return. Having done so, and her reference point having adjusted to this change in her holdings, she is just willing to accept r

WTA21
 units of characteristic 2 as compensation for each unit of characteristic 1 she gives up. (Given suitable differentiability assumptions, the values of r

WTP21
 and r

WTA21
 are constant for sufficiently small changes in the reference point.) Thus, the net compensation required to induce the individual to trade one unit of characteristic 2 for an equivalent quantity of characteristic 1, and then to trade back again, is \( {r_{21}}^{\text{WTA}}.r{_{12}}^{\text{WTA}} - 1 \) units of characteristic 2. This value, which we denote by Q
21, is a measure of exchange resistance with respect to the two characteristics, defined at z. It is dimensionless and directionless (in the sense that Q
12 = Q
21). Positive values of exchange resistance indicate aversion to movements away from status quo positions (that is, the status quo effect); a zero value indicates neutrality towards such movements. In the case in which characteristic 1 is a non-money good and characteristic 2 is money, this measure has a familiar interpretation. In this case, \( {{\text{r}}_{21}}^{{\text{WTA}} } \) is the WTA valuation, in money units, of a marginal unit of the non-money good, while r
WTA12
 is the reciprocal of the corresponding WTP valuation. Thus, our measure of exchange resistance can be written as (WTA/WTP) – 1, or (WTA – WTP)/WTP: it is the excess of marginal WTA over marginal WTP, expressed as a proportion of marginal WTP. We now consider the extent of exchange resistance in KR’s model. Let μ′−(0) and μ′+(0) be the limits of μ′(.) as its argument tends to zero, respectively from below and from above, and define \( \gamma \equiv \left[ {1 + \mu {\prime_{-} }(0)} \right]/\left[ {1 + \mu {\prime_{+} }(0)} \right] \). Intuitively, γ measures the degree to which μ(.) is kinked at zero—in other words, the degree of ‘universal’ loss aversion. It can be shown that, at all reference points z, \( {Q_{21}} = {\gamma^2} - 1 \). Thus, the extent of exchange resistance between pairs of characteristics is constant, irrespective of the reference point and irrespective of what those characteristics are.Footnote 4
 It seems clear that, if characteristics were interpreted as objective goods, KR’s model would be too restrictive for general-purpose consumer theory. Certainly, it would be unable to explain differences in the strength of status quo effects between different decision environments. KR partially acknowledge these limitations. While suggesting that ‘in most applications it is appropriate to identify these dimensions [i.e. characteristics] with the physical consumption dimensions’, they refer to an alternative modelling strategy in which characteristics are identified with hedonic dimensions—that is, ‘dimensions of consumption that people experience as psychologically distinct’ (pp. 1156).Footnote 5
 A natural way to incorporate hedonic dimensions into consumer theory is to treat each objective good as a linear combination of hedonic characteristics. Then, if KR’s specification is assumed to apply to characteristics, we can derive reference-dependent preferences over goods as a reduced form of their model. This reduced-form model allows the degree of substitutability to be different for different pairs of goods, although it still cannot accommodate complementarity. (The minimum degree of substitutability occurs when two goods are composed of completely distinct sets of characteristics; in this case, if income effects are zero, the cross-price elasticity of substitution is zero.) If exchange resistance is defined in relation to goods rather than characteristics, the unrealistic result of the constancy of exchange resistance no longer holds. Instead, comparing across different pairs of goods, the extent of exchange resistance is inversely related to the degree of hedonic similarity between the goods in question. Intuitively, this seems realistic. (In terms of one of KR’s examples, an individual who can barely distinguish between two brands of premium orange juice is unlikely to reveal much resistance to exchanging one for the other [p. 1156, note 25].) In contrast to the model we will propose, KR’s model (whether formulated in terms of goods or characteristics) does not imply any systematic effect of information or experience on exchange resistance. This is a consequence of the fundamental hypothesis that status quo effects result from loss aversion with respect to consumption dimensions. As an individual gains information about two goods, she may change her beliefs about the mixtures of characteristics that they comprise. If she comes to believe that they are hedonically less distinct than she previously thought, she will become less exchange-resistant with respect to them; and conversely if she comes to believe them to be more distinct. But there seems no reason to assume that, in general, uninformed individuals tend to overestimate hedonic differences between goods. In understanding this implication of KR’s model, it is important to remember that exchange resistance is defined relative to given reference points. Another feature of KR’s model, conceptually distinct from the issues we address in this paper, is that reference points are endogenous. An individual’s reference point is ‘the probabilistic beliefs she held in the recent past about outcomes’; these beliefs are modelled as rational expectations (p. 1134–1135). Thus, one effect of repeated experience of a given trading environment may be to move reference points from pre-trade endowments of goods to expected post-trade holdings. If an individual’s expected post-trade position is her reference point, she will not exhibit loss aversion with respect to the trades that she expects to make. But this is not equivalent to the decay of exchange resistance. Suppose, for example, that some individual’s expected trade is to buy a daily newspaper on her way to work. If the outcome of this transaction is incorporated into her reference point, she will perceive her daily decision problem as a choice between the reference point (buying the paper) and the alternative of losing the paper and gaining the purchase price. In KR’s model, these dimensionally separate gains and losses are evaluated in terms of the gain-loss function μ(.). If characteristics are identified with goods, the degree of exchange aversion remains equal to γ2 – 1, whether a trade is anticipated or not; the effect of anticipation is merely to change the reference point at which exchange aversion is measured. One way of testing this implication is to elicit WTA and WTP for a trade that an individual expects to make (such as buying the newspaper). KR’s model implies that, in this case, WTP will exceed WTA to exactly the same degree that WTA exceeds WTP for unanticipated sales and purchases of the same good.Footnote 6 This result holds whatever the extent of the individual’s information or experience. As we have explained, KR build on the basic model that provides much of the motivation for TK’s theory; however, that latter theory is a compromise between the basic model and conventional consumer theory. An alternative strategy, followed by Munro and Sugden (2003; henceforth MS), is to use Hicksian consumer theory as a template and to make the minimum revisions necessary to accommodate observed patterns of reference-dependent choice. MS use the concept of a preference structure, as defined by TK, but impose only very general restrictions on the function v(., .) which represents it. Essentially, these conditions require that preferences viewed from any given reference point have conventional properties, that preferences do not change discontinuously as the reference point changes, and that reference-dependent preferences do not exhibit cycles. (For example, there is a cycle over x, y and z if \( x \succ y|y,\,y \succ z|z\,{\text{and}}\,z \succ x|x \).) The ‘acyclicity’ condition is a generalisation of the requirement that exchange resistance is non-negative. (Recall that exchange resistance measures the compensation that an individual requires for making a particular kind of cycle.) Because this model does not invoke additive separability, it inherits the flexibility of Hicksian consumer theory. For each pair of goods and for each reference point, any non-negative degree of exchange resistance is compatible with the model. However, the downside of this flexibility is that no predictions can be made about how exchange resistance varies across decision contexts. This silence reflects the fact that, unlike TK or KR, MS do not offer any psychological explanation for exchange resistance: they simply provide a theoretical framework in which it can be represented. In the remainder of this paper, we propose an explanation of status quo effects that is fundamentally different from that offered by TK and KR. It turns out that our approach provides psychological underpinning for a model similar to that of MS, while also generating firm implications about how exchange resistance varies across decision contexts. These implications are properties of preferences in commodity space—that is, in the domain of conventional consumer theory. These properties make no explicit reference to taste states. Thus, the applicability of our approach does not depend on the ‘observability’ of taste states. Like TK and KR, we use a concept of loss aversion, but we define gains and losses in terms of taste uncertainty rather than consumption dimensions. For example, with reference to Knetsch’s experiment, consider a participant who is endowed with the mug and whose preference between this and the chocolate bar is uncertain or imprecise. Extrinsic uncertainty exists if he can imagine some circumstances in which he would enjoy the chocolate more than the mug, but he can also envisage circumstances in which the opposite might be true. This may be supplemented by the intrinsic uncertainty entailed by vacillation between a state of mind in which he feels more desire for the chocolate and one in which he feels more desire for the mug. Due to either or both sources of uncertainty, he construes the act of trading the mug for the chocolate as one which gives him some chance of a utility gain and some chance of a utility loss. If he is loss averse with respect to utility, this will impart resistance to the exchange.",33
39.0,2.0,Journal of Risk and Uncertainty,12 August 2009,https://link.springer.com/article/10.1007/s11166-009-9075-z,Are risk preferences stable? Comparing an experimental measure with a validated survey-based measure,October 2009,Lisa R. Anderson,Jennifer M. Mellor,,Female,Female,Unknown,Female,"In the Holt and Laury (2002) design, subjects make 10 choices between Option A or Option B, where each option is a lottery that pays one of two amounts. In each decision, Option A is the “safe” choice and Option B is “risky,” since Option A has less variability in the payoffs than Option B. The 10 decisions differ in terms of the probability of winning the higher prize in each lottery. In Decision 1, the higher prize is paid if the throw of a 10-sided die is 1 and the lower prize is paid for any other throw of the die. For Decision 2, the higher prize is paid if the result of the die throw is 1 or 2 and the lower prize is paid if the die is 3 through 10. By Decision 9 there is a 90% chance of winning the higher prize, and Decision 10 is a choice between a certain prize in Option A and a certain prize in Option B. Table 1 shows the lottery choices for all 10 decisions in our version of the experiment, in which payoffs were three times those used in the Holt and Laury baseline treatment. In Decisions 1 through 4, the expected payoff for Option A is higher than the expected payoff for Option B; in Decisions 5 through 10, Option B has a higher expected payoff. Subjects generally begin by choosing Option A in Decision 1, when the chance of winning the higher Option B payoff is relatively small. The point at which subjects switch from the safe option to the risky option can be used to classify their risk aversion level. For example, subjects who choose Option A in the first four decision rows and switch to Option B in the fifth decision on are risk neutral. A risk averse subject will choose Option A more than four times and a risk-seeking subject will choose the safe option fewer than four times.
 Decisions in the experiment can also be used to define a range of values for each subject’s risk aversion parameter. These ranges are reported in Table 2 for a utility function of constant relative risk aversion (CRRA):
 where r is the coefficient of relative risk aversion and Y is the payoff in the lottery. For example, suppose an individual chooses the safe option in the first three decisions, then chooses the risky option for subsequent decisions. The lower bound of the risk aversion parameter is determined by solving for r such that the individual is indifferent between Options A and B at Decision 3:
 Likewise, the upper bound is r such that the individual is indifferent between Option A and Option B at Decision 4:
 The exact value of this subject’s risk aversion parameter lies somewhere in the range from −0.49 to −0.15. Values of r < 0 indicate risk-seeking preferences, r = 0 indicates risk neutrality, and values of r > 0 indicate risk aversion. The basic Holt and Laury (2002) design has been used in many subsequent studies on risk attitudes, including several that establish links between decisions in the experiment and real-world risky behaviors. For example, Elston et al. (2005) conducted the experiment at two small-business conventions and reported that full-time entrepreneurs were significantly more risk tolerant than part-time entrepreneurs and non-entrepreneurs. Lusk and Coble (2005) compared subject responses to survey questions on the willingness to consume genetically-modified foods to subject decisions in a lottery choice experiment and found that the experimental measure was significantly associated with risk-taking behavior. Anderson and Mellor (2008) reported that more risk averse subjects were less likely to smoke and more likely to wear seat belts. Our second means of measuring subject-specific risk preference is through survey questions involving hypothetical gambles. Variations of these questions appear in several large panel surveys of U.S. households: the Health and Retirement Study (HRS), the Panel Study of Income Dynamics (PSID), and the National Longitudinal Survey of Youth 1979 (NLSY79). Given their widespread availability, these questions have been used to define and control for risk preference in studies of a wide range of behaviors, from financial investment to fertility. Barsky et al. (1997) analyzed the hypothetical gamble questions as they originally appeared in the 1992 wave of the HRS. The first question in the series began: “Suppose you are the only income earner in the family, and you have a good job guaranteed to give you your current (family) income every year for life. You are given the opportunity to take a new and equally good job, with a 50–50 chance it will double your (family) income and a 50–50 chance it will cut your (family) income by a third. Would you take the new job?” Respondents who turned down the new job were asked a follow-up about taking the new job if the chances were 50–50 that income would double, and 50–50 that income would be reduced by one-fifth. Respondents who accepted the new job in the first scenario were asked a follow-up in which the circumstances became more risky; specifically the question referred to a 50–50 chance that income would double, and a 50–50 chance that income would fall by one-half. Based on replies to the two questions they were posed, respondents were classified into one of four categories ranging from least risk tolerant (rejecting the new job in both cases) to most risk tolerant (accepting the new job in both cases). The Barsky et al. (1997) study used a parameter measure of risk tolerance defined from these questions as an explanatory variable in a series of regression models of individual behaviors. While risk tolerance explained a relatively small portion of the variation in these behaviors, its estimated effects were statistically significant and of the expected sign in most cases. Respondents who were more risk tolerant according to the survey questions were more likely to report ever having smoked and smoking at the time of the survey; they also were more likely to drink, and reported drinking a higher number of drinks per day. More risk tolerant respondents were significantly more likely to be self-employed and to not own health or life insurance, and they held a larger share of their wealth in stocks. Several subsequent studies of wealth and investment decisions have used the hypothetical gamble responses to control for individual risk preference. Using the HRS, Lusardi (1998) found that a categorical measure of risk aversion was significantly associated with wealth accumulation, and Rosen and Wu (2004) reported that a risk-taking indicator was associated with risky asset ownership. Using the PSID, Charles and Hurst (2003) found a strong correlation between parental risk tolerance and child risk tolerance indicator variables, and showed that adult children with higher levels of risk tolerance were more likely to own a business and more likely to own stocks than those with the lowest levels of risk tolerance. Several recent studies on non-financial decisions also validate the use of hypothetical gamble questions to define risk preference. Kan (2003) used PSID data and found that risk aversion was negatively and significantly associated with job changes and residential moves. Also using PSID data, Brown and Taylor (2007) showed that risk aversion was negatively associated with educational attainment, and Schmidt (2008) reported that high levels of risk tolerance were associated with delayed marriage, earlier births at young ages, and delayed fertility at older ages among highly-educated women. Spivey (2007) used data from the NLSY79 to demonstrate that risk aversion was significantly associated with the timing of first marriage. The evidence regarding the predictive power of the survey-based measure is mixed in studies related to health and healthcare. Using a categorical measure of risk aversion as a control variable, Lahiri and Song (2000) reported a negative and significant effect on smoking initiation, and Dave and Saffer (2007) reported a negative and significant effect on alcohol consumption. In contrast, Sloan and Norton (1997) reported insignificant effects of risk aversion indicators on long-term care insurance demand, and Picone et al. (2004) found that risk tolerance had either statistically insignificant effects or significant and wrong-signed effects on the demand for preventive medical tests. As described above, both elicitation methods used in this study produce measures of risk preference that have been validated by evidence of their association with “real world” risky behaviors. Both tasks share some additional features. One is that they are relatively simple pairwise choices and can be administered with minimal instruction to subjects. For this reason, pairwise choice experiments may be less prone to misinterpretation and strategic behavior than certainty equivalent measures. Further, in an experimental comparison of risk elicitation methods, Hey et al. (2007) found that pairwise choice methods were associated with both less noise and less bias than certainty equivalent measures, including the BDM procedure. Another similarity is that both mechanisms have been studied in the context of temporal stability, and there is evidence that subjects tend to respond to each in a stable manner over time. Sahm (2007) used panel data from the HRS to show that risk tolerance defined from the gamble questions is relatively stable for individuals over time, and Harrison et al. (2005) and Andersen et al. (2008) showed that risk attitudes defined from the lottery choice task are stable over time. Finally, both tasks allow us to identify subjects who misunderstood the task or did not take it seriously, as we describe in the next section. There are, however, some obvious differences in the two tasks, such as the hypothetical nature of the gambles and the large difference in the scale of the payoffs. Holt and Laury (2002) found that increasing the size of real payoffs led subjects to behave in a more risk averse manner; however, there were no significant differences in subject decisions across a treatment with low real payoffs and other treatments with high hypothetical payoffs. Further, while differences in the scale of the payoffs may lead subjects to be more risk averse, there is little reason to think that payoff differences would cause the rank order of risk aversion estimates to change across the two tasks. Nonetheless, these differences may reduce our chances of finding consistency across the risk preference measures obtained from both tasks.",129
39.0,2.0,Journal of Risk and Uncertainty,27 August 2009,https://link.springer.com/article/10.1007/s11166-009-9077-x,Dirty money: Is there a wage premium for working in a pollution intensive industry?,October 2009,Matthew A. Cole,Robert J. R. Elliott,Joanne K. Lindley,Male,Male,Female,Mix,,
39.0,2.0,Journal of Risk and Uncertainty,07 August 2009,https://link.springer.com/article/10.1007/s11166-009-9074-0,On attitude polarization under Bayesian learning with non-additive beliefs,October 2009,Alexander Zimper,Alexander Ludwig,,Male,Male,Unknown,Male,"The effects of polarization have received ample attention in the politico-economic literature. A review of this literature is given in Lindqvist and Östling (2008). Examples are Alesina et al. (1999), Esteban and Ray (2001) and Fernández and Levy (2008) who study the effects of polarization on government spending, the provision of public goods as well as redistribution. Concepts for the measurement of polarization, which are relevant for much of the empirical work, are developed in Esteban and Ray (1994) and Duclos et al. (2004). Our work, however, relates to the literature on the determinants of polarization and we focus our following review of the literature accordingly. More precisely, we review learning models that give rise to some form of polarization or myside biases and thereby distinguish between learning models with additive beliefs and models with biased beliefs that are expressed by ambiguity attitudes. In our learning model agents revise their probability assessments about the parameters of some stochastic process, e.g., about the probability that a given coin turns up heads or tails, by Bayesian updating. Accordingly, agents have some prior beliefs and form posterior beliefs given the relative frequencies observed in the data. In contrast, according to the frequentist approach, agents learn probabilities by simply adopting relative frequencies observed in a given data sample. Within the frequentist approach, divergence of probability assessments of agents cannot occur if the data are drawn from a stationary stochastic process. Against this background, Kurz (1994a, b, 1996) assumes a non-stationary stochastic process and thereby establishes conditions under which agents may not agree about fundamentals in the long run even if they observe the same data sample. However, the application of a frequentist learning rule in a non-stationary environment is not fully consistent because the rationale for agents to apply a frequentist rule for inferring probabilities when the “underlying” probabilities cannot be learnt by this rule is not clear.Footnote 1
 While divergence of beliefs can thus not occur within the frequentist framework in a stationary environment, a similar observation holds true within the Bayesian framework when restricted to additive beliefs. Part of our analysis below is based on a specific model of Bayesian learning with additive beliefs according to which the agents’ uncertainty with respect to the parameter of a Binomial distribution is described by a Beta distribution. The fact that additive posteriors converge to the same limit belief in this model, however, can be regarded as a special case of more general results on the consistency of (additive) Bayesian estimates, in particular Doob’s consistency theorem (Doob 1949; for extensions see Breiman et al. 1964; Lijoi et al. 2004). Roughly speaking, Doob’s consistency theorem states that, for almost all true parameter values, the Bayesian estimate will converge to this value if an agent observes an i.i.d. process.Footnote 2 Moreover, for situations in which there are only finitely many possible observations in every period, Freedman (1963) establishes that the Bayesian estimate will converge to the true value whenever this value belongs to the support of the agent’s prior. In light of Doob’s consistency result and its extensions it is practically impossible to establish (at least for the case of finitely many observations in every period such as heads versus tails) attitude polarization, or even non-converging posteriors, within the framework of Bayesian learners with additive beliefs if all agents observe the same sample information drawn from an i.i.d. process. In order to nevertheless account for the empirical phenomenon of non-converging posteriors and/or attitude polarization, several authors have tried to circumvent these convergence results within the framework of Bayesian learning with additive beliefs. One approach is to restrict attention to the possibility of a short-run bias only, thereby deliberately ignoring long-run convergence (e.g., Brav and Heaton 2002; Dixit and Weibull 2007). Another line of research is to look into the possibility of weakening the i.i.d. assumption of the above framework. E.g., Lewellen and Shanken (2002) consider cases in which the mean of an exogenous dividend process may not be constant over time. Consequently, the agent can never fully learn the objective parameters of the underlying distribution because observed frequencies do not admit any conclusions about objective probabilities even in the long run. Along the same line, Weitzman (2007) considers a non-stationary exogenous stochastic process so that there is no “true” parameter that could be learnt by the agents. Furthermore, within the context of attitude polarization, Kandel and Pearson (1995) and, more recently, Acemoglu et al. (2007) consider two agents with different prior-distributions about imprecise signals from an i.i.d. process. Since these different priors imply different interpretation of new information, these authors avoid convergence of both agents’ posteriors according to Doob’s consistency theorem because these posteriors are effectively formed by observing two different stochastic processes. While the above approaches try—in one way or another—to reconcile the possibility of attitude polarization with Bayesian learning under the assumption of additive beliefs, our approach drops the assumption of additive beliefs altogether. As a consequence, Doob’s consistency theorem generally does not apply so that agents’ non-additive posteriors may diverge in the long-run despite the fact that they observe the same data drawn from an i.i.d. process. Moreover, our approach may even allow for diverging posteriors and attitude polarization in the case that agents start out with identical priors. This is impossible for models of Bayesian learning with additive beliefs because additivity implies a unique Bayesian update rule. Related to our approach, Marinacci (1999) studies a learning environment with non-additive beliefs whereby he considers a decision maker who observes an experiment where the outcomes in each trial are identically and independently distributed with respect to the decision-maker’s non-additive belief.Footnote 3 In this setup, Marinacci derives for (basically convex) capacities laws of large numbers as counterparts to the additive case thereby admitting for the possibility that ambiguity does not vanish in the long-run. While Marinacci’s approach may thus be regarded as a frequentist approach towards non-additive probabilities, our approach is a Bayesian one according to which an agent has a subjective prior belief over the whole event space while he uses sample information from an objective process in order to update his subjective belief. In contrast to our approach the learning behavior of different agents in Marinacci’s model must converge to the same limit if they have identical priors. As a consequence there cannot occur attitude polarization within Marinacci’s framework under the assumption of common priors. Epstein and Schneider (2007) also consider a model of learning under ambiguity which shares with our learning model the feature that ambiguity does not necessarily vanish in the long run. Their learning model is based on the recursive multiple priors approach (Epstein and Wang 1994; Epstein and Schneider 2003) that restricts conditional max min expected utility (MMEU) preferences of Gilboa and Schmeidler (1989) such that dynamic consistency is satisfied. While MMEU theory is closely related to CEU theory restricted to convex capacities (e.g., neo-additive capacities for which the degree of optimism is zero), the similarity between Epstein and Schneider’s approach and our learning model ends here. As one main difference, the restriction of Epstein and Schneider’s approach to dynamically consistent preferences excludes preferences that violate Savage’s sure-thing principle as elicited in Ellsberg paradoxes (Ellsberg 1961).Footnote 4 Since our learning model does not exclude dynamically inconsistent decision behavior, it can accommodate a broader notion of ambiguity attitudes than the Epstein-Schneider approach, including ambiguity attitudes that are not compatible with the sure-thing principle. Furthermore, Epstein and Schneider establish long-run ambiguity, i.e., the existence of multiple posteriors, under the assumption that the decision-maker permanently receives ambiguous signals, which they formalize via a multitude of different likelihood functions at each information stage in addition to the existence of multiple priors.Footnote 5 This introduction of multiple likelihoods is rather ad hoc and it would be interesting to see an axiomatic and/or psychological foundation of this approach which goes beyond the mere technical property that multiple likelihoods can sustain long-run ambiguity in the recursive multiple priors framework. On the contrary, our—comparably simple—axiomatically founded model of Bayesian learning with psychological attitudes offers a rather straightforward rationale for why long-run beliefs may be biased even in the case that the decision-maker receives signals that are not ambiguous.",24
39.0,3.0,Journal of Risk and Uncertainty,20 October 2009,https://link.springer.com/article/10.1007/s11166-009-9081-1,Noise and bias in eliciting preferences,December 2009,John D. Hey,Andrea Morone,Ulrich Schmidt,Male,Female,Male,Mix,,
39.0,3.0,Journal of Risk and Uncertainty,16 September 2009,https://link.springer.com/article/10.1007/s11166-009-9078-9,Preference reversals and probabilistic decisions,December 2009,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"The preference reversal phenomenon was first documented by Sarah Lichtenstein and Paul Slovic (1971). It was subsequently replicated in dozens of studies (see Seidl (2002) for a recent review). A typical illustration of the preference reversal phenomenon involves two risky lotteries of a similar expected value. The first lottery offers a relatively large sum of money with a small probability and zero otherwise. This lottery is usually called the $-bet. The second lottery yields a modest sum of money with a relatively high probability and zero otherwise. This lottery is called the P-bet. A standard preference reversal is observed when a decision maker chooses the P-bet over the $-bet in a direct binary choice and at the same time he or she states a higher certainty equivalent for the $-bet in the valuation task. A nonstandard preference reversal occurs when a decision maker chooses the $-bet over the P-bet while stating a higher certainty equivalent for the P-bet. The preference reversal phenomenon is an observation that standard preference reversals occur more frequently than nonstandard preference reversals. In a broader sense, the preference reversal phenomenon occurs when a decision maker reveals different preferences (in a predictable manner) in two elicitation procedures that are formally equivalent. The phenomenon is often interpreted as the failure of procedure invariance (e.g. Tversky et al. 1990). The preference reversal phenomenon can also occur when an individual chooses the P-bet over the $-bet in a direct binary choice even though the certainty equivalent of the $-bet is strictly greater than the highest possible outcome of the P-bet. Such preference reversals are called strong reversals (Fishburn 1988, p. 46). Several non-expected utility theories such as regret theory (e.g. Loomes and Sugden 1987) can explain the preference reversal phenomenon as a consequence of non-transitive preferences but they cannot rationalize strong reversals. Butler and Loomes (2007) reported a new form of the preference reversal phenomenon using probability equivalents instead of certainty equivalents. In this case, a standard preference reversal occurs when a decision maker chooses the P-bet over the $-bet while stating a higher probability equivalent for the $-bet. A nonstandard preference reversal is observed when the $-bet is chosen over the P-bet but the probability equivalent of the P-bet is higher. Butler and Loomes (2007) find that such nonstandard preference reversals occur more frequently than standard preference reversals when probability equivalents are used in the valuation task. This paper shows that a model of probabilistic choice and valuation can account for a higher incidence of standard preference reversals with certainty equivalents simultaneously with a higher incidence of nonstandard preference reversals with probability equivalents. The model can also rationalize the existence of strong reversals. To illustrate the main idea of the paper let us consider the following example. The P-bet yields $3.75 with a probability 0.8 (nothing otherwise) and the $-bet yields $10 with a probability 0.3 ($0 otherwise). Notice that these bets have the same expected value ($3). Suppose that an individual chooses with equal probabilities between these two bets in a direct binary choice. According to the model presented in this paper, the certainty equivalents of the $-bet and the P-bet are random variables with the following intuitive properties:
 the certainty equivalent of the $-bet is distributed between $0 and $10; the certainty equivalent of the P-bet is distributed between $0 and $3.75; a median certainty equivalent of the $-bet is equal to that of the P-bet ($3). Note that these properties imply that the distribution of the certainty equivalent of the $-bet (the P-bet) is positively (negatively) skewed as it is illustrated on Fig. 1. If we take two draws from such distributions, we can observe many instances when the realized certainty equivalent of the $-bet is greater than the realized certainty equivalent of the P-bet and only few instances of the reversed ranking. Thus, an individual ends up stating a systematically higher certainty equivalent for the $-bet (“overpricing error”).
 Probability density function of the certainty equivalent of the $-bet and the P-bet Specifically, Fig. 1 represents the probability density function of the P-bet f(x) = 6x(3−0.8x)/(3.4x
2−24x + 45)2, x∈[0,3.75] and the probability density function of the $-bet g(x) = 88.2x(10−x)/(5.8x
2−18x + 9)2, x∈[0,10]. It is easy to verify that this pair of density functions satisfies the three properties presented above. Given these two density functions, the probability that the realized certainty equivalent of the $-bet is greater (lower) than the realized certainty equivalent of the P-bet is 0.61 (0.39). The idea that preference reversals may be driven by random preferences or random errors has been around for quite a while. MacCrimmon and Smith (1986) were among the first who suggested that imprecise certainty and probability equivalents can account for the preference reversal phenomenon. Starting with a general premise “it appears that people are unable to determine precise equivalences for lotteries” (MacCrimmon and Smith 1986, p. 13) they moved on to demonstrate that “the imprecise equivalence theory explains traditional preference reversal…in terms of the spread of the certainty equivalence for the $-bet” (MacCrimmon and Smith 1986, p. 15). The ideas of MacCrimmon and Smith (1986) were recently reiterated in Butler and Loomes (2007). However, neither MacCrimmon and Smith (1986) nor Butler and Loomes (2007) developed a tractable mathematical model that formalizes the concept of imprecise certainty and probability equivalents. The present paper contributes to this literature by providing a formal definition of probabilistic certainty/probability equivalents within the framework of a general model of probabilistic decision making. Looking at the phenomenon from a different perspective, Schmidt and Hey (2004) found that preference reversals occur less frequently if people reveal consistent ordering of two alternatives in repeated pricing tasks. Hence, Schmidt and Hey (2004) suggest that the preference reversal phenomenon can be driven by random errors in the elicitation of certainty equivalents. The model presented in this paper is consistent with such interpretation (cf. Example 1 above). As an important methodological contribution, this paper proposes a general theory of probabilistic valuation that can be combined with various models of probabilistic choice (provided that they respect first-order stochastic dominance). This allows the development of a unified framework, where a decision maker does not only choose in a probabilistic manner but also is capable of assigning probabilistic certainty/probability equivalents to non-degenerate lotteries. Thus, even though much of this paper deals with a specific model of probabilistic choice, the main concept of probabilistic valuation is readily extendable to other models as well. The remainder of the paper is organized as follows. Section 2 presents a parsimonious model of probabilistic choice that was recently axiomatized by Blavatskyy (2008). This section also develops the concept of probabilistic certainty/probability equivalent of a risky lottery. Section 3 presents the main results of the paper. Specifically, it shows that the model of probabilistic choice proposed by Blavatskyy (2008), when complemented by a proposed theory of probabilistic valuation, can account for classical preference reversals (subsection 3.1), strong preference reversals (subsection 3.2) and preference reversals with probability equivalents (subsection 3.3). Section 4 concludes.",24
39.0,3.0,Journal of Risk and Uncertainty,15 October 2009,https://link.springer.com/article/10.1007/s11166-009-9080-2,Group cooperation under uncertainty,December 2009,Min Gong,Jonathan Baron,Howard Kunreuther,,Male,Male,Mix,,
39.0,3.0,Journal of Risk and Uncertainty,25 September 2009,https://link.springer.com/article/10.1007/s11166-009-9079-8,The influence of fear in decisions: Experimental evidence,December 2009,Olivier Chanel,Graciela Chichilnisky,,Male,Female,Unknown,Mix,,
40.0,1.0,Journal of Risk and Uncertainty,15 December 2009,https://link.springer.com/article/10.1007/s11166-009-9083-z,The heterogeneity of the value of statistical life: Introduction and overview,February 2010,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
40.0,1.0,Journal of Risk and Uncertainty,16 December 2009,https://link.springer.com/article/10.1007/s11166-009-9084-y,Policy relevant heterogeneity in the value of statistical life: New evidence from panel data quantile regressions,February 2010,Thomas J. Kniesner,W. Kip Viscusi,James P. Ziliak,Male,Unknown,Male,Male,"In a previous paper we established that individual heterogeneity in the form of a latent time invariant intercept was crucially important to estimating VSL in a linear panel data regression model, much more so than endogeneity of fatal injury risk (Kniesner et al. 2008). It was also the case that how one controlled for heterogeneity (fixed effects versus a mis-specified exogenous random effects model) was far less important to the estimates than whether one controls for differential intercepts at all. In continuing our exploration of the heterogeneity of regression parameters, we examine slope differences using the concept of quantile regression. It is only recently that estimators have appeared for quantile regression in a panel context that also permit latent person-specific heterogeneity (Koenker 2004; Lamarche 2006). Interquartile differences in estimated fatality risk marginal effects and the associated VSLs capture distributional issues such as asymmetry not evident in mean regression. Even a simple comparison of the mean versus median VSL is instructive for safety policy where the VSL is a benefit comparison point for evaluating life-saving programs with different cost levels. In particular, using the median program benefit as a cutoff value ensures that a majority of the affected population will benefit from the program, which need not be the case using the mean of an asymmetric benefit distribution as a cost-effectiveness cutoff. To fix notation let our regression model be
 where i indexes the worker, t indexes time, and the vector of covariates includes fatal injury risk (π) and the usual demographic characteristics of the worker (z). The α

i
’s are the time-invariant worker-specific effects, and u

it
 is the usual random error term. In (1) there are common marginal effects. In a quantile regression model one has β(τ

j
) where τ

j
 indexes the quantiles of the potential regression outcomes. For tractability, when T is finite the researcher usually begins by assuming that the worker-specific effect, α

i
, is a so-called pure location shift that applies (is common) to all the conditional quantiles of the regression outcomes. In our estimation we follow the innovative regression model developed in Lamarche (2006). He begins by noting that a shrinkage estimator wherein a tuning parameter (call it λ) controls the degree of inter-person intercept differences is a way to limit the variability of the multiplicity of marginal effect estimates in a quantile regression. The tuning parameter ranges across the spectrum of complete to no heterogeneity and can either be estimated or fixed ex ante. The idea is the optimal shrinkage of the differentials toward a common intercept. We minimize the trace of the covariance matrix when estimating the tuning parameter as well as provide estimates of several extreme cases, such as complete versus no intercept heterogeneity. Lamarche’s (2006) estimator finds the
 where \( {\omega_{{\tau_j}}} \) is the relative weight of the jth quantile and \( {\rho_{{\tau_j}}}(u) = u\left( {{\tau_j} - I\left( {u \leqslant 0} \right)} \right) \) is the quantile loss function. Here the tuning parameter \( \left( {\lambda = {{\sigma_u^2} \mathord{\left/{\vphantom {{\sigma_u^2} {\sigma_a^2}}} \right.} {\sigma_\alpha^2}}} \right) \) regulates the influence on the quantiles of the estimated worker effects. In the case where λ  =  0 the fixed effects estimator emerges while for the case where λ >  0 a penalized (shrinkage) estimator with fixed effects appears (Lamarche 2006). As noted, we explore two ways of valuing λ , fixing it ex ante, as the estimator \( \hat \beta \left( {\tau, \lambda } \right) \) is asymptotically unbiased for all λ > 0, and finding its trace minimizing estimated value, which is a form of feasible generalized least squares (GLS) analogous to penalized least squares for panel data (Lamarche 2006). Finally, the pattern of \( \hat \beta \left( {{\tau_j}} \right) \) describes how the resulting marginal impact of fatality risk varies across potential wage outcomes. We also use our estimates of \( {\hat \beta_1}\left( {{\tau_j}} \right) \) to construct estimates of the value of a statistical life and note how the associated VSLs vary across potential wage outcomes. Accounting for the fact that fatality risk is per 100,000 workers and that the typical work-year is about 2,000 h, the estimated value of a statistical life for a quantile of the potential wage distribution is
 To summarize, equations (1) and (3) describe our organizing framework: (1) is the hedonic market wage locus and (3) is the associated value of statistical life (VSL), which depends on the estimated parameters of (1) via the marginal effect of fatal injury risk (π) and is non-constant in the context of a quantile regression estimator (2). There are two kinds of heterogeneity that come into play. One is econometric heterogeneity whereby the wage equation intercepts vary with the person indicator (i). The other is economic heterogeneity whereby there is curvature of the hedonic locus (\( \hat \beta \) varies with τ) to reflect both latent worker and firm differences in risk tolerance and cost functions. The main body of our data comes from the 1993–2001 waves of the Panel Study of Income Dynamics (PSID), which provides individual-level data on wages, industry and occupation, and demographics. The PSID survey has followed a core set of households since 1968 plus newly formed households as members of the original core have split off into new families. The sample we use consists of male heads of household ages 18–65 who are in the random Survey Research Center (SRC) portion of the PSID, and thus excludes the oversample of the poor in the Survey of Economic Opportunity (SEO) and the Latino sub-sample. The male heads in our regressions (i) worked for hourly or salary pay at some point in the previous calendar year, (ii) were not permanently disabled or institutionalized, (iii) were not in agriculture or the armed forces, (iv) had a real hourly wage greater than $2 per hour and less than $100 per hour, and (v) had no missing data on wages, education, region, industry, and occupation. All real wages and VSL estimates are in terms of 2001 dollars based on the personal consumption expenditure deflator. Beginning in 1997 the PSID moved to every-other-year interviewing. For consistent spacing of survey response we use data from the 1993, 1995, 1997, 1999, and 2001 waves. The use of every-other-year responses will be one of many mechanisms to reduce the influence of measurement error in our estimated VSL. We do not require individuals to be present for the entire sample period; we have an unbalanced panel where we take missing values as random events.Footnote 10 Our sample filters yielded 2,036 men and 6,625 person-years. About 40% of the men were present for all five waves (9 years); another 25% were present for at least four waves. The demographic controls in the model include years of formal education, a quadratic in age, dummy variables for state of residence, dummy indicators for region of country, race, union status, marital status, and one-digit occupation. We used the fatality rate for the worker’s two-digit industry by one-digit occupation group. We distinguished 720 industry-occupation groups using a breakdown of 72 two-digit SIC code industries and the 10 one-digit occupational groups. After constructing codes for two-digit industry by one-digit occupation in the PSID we then matched each worker to the relevant industry-occupation fatality risk. Our worker fatality risk variable uses proprietary U.S. Bureau of Labor Statistics data from the Census of Fatal Occupational Injuries (CFOI) for 1992–2002.Footnote 11
 The CFOI provides the most comprehensive inventory to date of all work-related fatalities in a given year. The CFOI data come from reports by the Occupational Safety and Health Administration, workers’ compensation reports, death certificates, and medical examiner reports. To be classified as a work-related injury the decedent must have been employed at the time of the fatal event and engaged in legal work activity that required the worker be present at the site of the fatal incident. In each case the BLS verified the work status of the decedent with two or more of the above source documents or with a follow-up questionnaire in conjunction with a source document. Because our fatality risk variable is by industry and by occupation, it provided a much more pertinent measure of the risk associated with a particular job than a more broadly based index, such as the industry risk alone, which is the most widely used job risk variable. Taking into account the occupation as well as the industry as we did here substantially reduces the measurement error in the fatality risk variable. The importance of the industry-occupation structure of our risk variable is especially great within the context of a panel data analysis. By using a fatality risk variable that varies over time and is defined for 720 industry-occupation groups, we greatly expanded the observed variance in workers’ job risks across different periods. Our focal measure of the fatal injury risk rate used the number of fatalities in each industry-occupation cell. The denominator is the number of employees for that industry-occupation group in survey year t. Our measure of the fatality risk is time-varying because of changes in both the numerator and the denominator.Footnote 12
 We expect less reporting error in the industry information than in the occupation information, so our annual measure should have less measurement error than if the worker’s occupation were the basis for matching (Mellow and Sider 1983; Black and Kniesner 2003). To reduce the influence of large swings in fatality risk further, we dropped person-years where the percentage change in fatality risk exceeded a positive 300 percent or negative 75%. The sample mean fatality risk for the annual measure is 6.4/100,000.",101
40.0,1.0,Journal of Risk and Uncertainty,16 January 2010,https://link.springer.com/article/10.1007/s11166-009-9085-x,Measuring how risk tradeoffs adjust with income,February 2010,Mary F. Evans,V. Kerry Smith,,,Unknown,Unknown,Mix,,
40.0,1.0,Journal of Risk and Uncertainty,14 January 2010,https://link.springer.com/article/10.1007/s11166-009-9086-9,"Valuing fatal risks to children and adults: Effects of disease, latency, and risk aversion",February 2010,James K. Hammitt,Kevin Haninger,,Male,Male,Unknown,Male,"We administered a stated-preference survey to a random sample of the Knowledge Networks internet panel. Panel members were recruited through random digit dial and closely match the U.S. national population on demographic and socioeconomic factors. To improve estimates of WTP to reduce risk to children, we over-sampled households having at least one child younger than 18 years old living at home. Data were collected in several waves fielded between March and October 2007. The survey was sent to 2,741 panel members and yielded 2,018 completed interviews, a response rate of 74%. Respondents were asked to value reductions in the risk of a fatal disease that might affect a specified “target:” themselves, a child (aged 2–18 years) or another adult living in their household. In order to plausibly claim that the risk reduction could be isolated to the designated target, the risk was said to be presented by pesticides on a food that only that individual would eat. The risk could be reduced by purchasing an otherwise similar food produced using a hypothetical “Pesticide Safety System.” This hypothetical system was described as using pesticides that are safer to humans than conventional pesticides, but it was described as not “organic” and offering no benefits to the environment (the objective was to limit the benefit to reducing health risk to the consumer). The incremental cost of the safer food was expressed in dollars per year. Respondents having no children or other adults in their household valued two risks to themselves; all others valued one risk reduction to themselves and one each to a child and another adult in their household (if any). If a household included more than one child or other adult, the one targeted was randomly selected. Risks were described using a full-factorial design in which baseline risk of illness (with the conventional food type), risk reduction (with the alternative food type), affected organ (brain, bladder, liver, lymphocytes), disease type (cancer, non-cancer), and latency period (1, 10, 20 years) were randomly varied. To test whether any difference in WTP between cancer and non-cancer diseases depends on the amount of information about the disease that is provided, symptom descriptions were presented to half the respondents for all the diseases they valued; no symptom descriptions were provided to the other half. The symptom descriptions were paragraphs of approximately 150 to 200 words that included descriptions of pain, limitations in mobility, self-care, and other activities, and need for hospitalization. They were reviewed for accuracy by six physicians from a range of specialties (e.g., internal medicine, surgery, pediatrics). The baseline risk (3 or 4 per 10,000 per year) and risk reduction (1 or 2 per 10,000 per year) were illustrated using a visual aid in which areas of the computer screen proportional to these probabilities and the complementary probability of no illness were distinctively colored. This approach is adapted from the best-performing of the visual aids tested by Corso et al. (2001) to help communicate small risk changes to survey respondents. Before the valuation questions, respondents were presented with two practice questions with feedback. In the first, one food type was both safer and less expensive than the other. Respondents who chose the dominant alternative were told that the food they had selected was both safer and less expensive than the other and that this was the logical choice. Respondents who chose the dominated alternative were told that the food they had selected was both less safe and more expensive than the other and invited to choose again. In the second practice question, neither alternative was dominant. Respondents were told the food they had chosen was safer and more expensive, or less safe and less expensive, as appropriate and asked to confirm that was the choice they preferred. For each target (self, child, other adult), the respondent was asked to describe the target’s current health using two standard measures of the health-related quality of life, the EQ-5D and visual analog scales. The EQ-5D (EuroQol Group 1990) is a generic health-state classification and utility instrument used to estimate the health-related quality of life (HRQL) associated with an individual’s current health or hypothetical health state. It classifies health states using five dimensions (mobility, self-care, usual activities, pain/discomfort, anxiety/depression), each of which can take any of three levels (no, moderate, or severe problems). HRQL is an index scored as one for perfect health and zero for health states equivalent to dead (negative values are permitted). It can be estimated by applying a scoring rule to the EQ-5D health-state description; we use the scoring rule developed by Shaw et al. (2005) using preferences for health states elicited from a large representative sample of the U.S. population. The visual analog scale is a linear scale with numbers ranging from 0 to 100 that are associated with health states as bad as dead and perfect health, respectively. The respondent is asked to select a number on this scale that represents the current or hypothetical health state to be valued. In addition, the respondent was asked to estimate the expected age of death for each target, using an open-ended question. For each risk to be valued, the respondent was first presented with the description (name and affected organ, latency, symptom description if provided) then asked to evaluate the target’s health conditional on having this disease using both EQ-5D and visual analog scales. These questions encourage the respondent to think about the severity of the disease and allow us to estimate the perceived health decrement associated with the disease as the difference between the target individual’s current health and health conditional on having the disease. Next, the initial risk, risk reduction using the alternative food type, and additional annual cost of the alternative food type were specified and the respondent asked to choose which food type he or she would select. A typical valuation question is shown in Fig. 1. Values were elicited using a standard double-bounded binary-choice format (Hanemann et al. 1991). The initial bid (the incremental cost of the safer food type) varied between $10 and $5,000; the follow-up bid was twice the initial bid for respondents who indicated they would choose the safer food in the initial question and half the initial bid for other respondents. By asking the respondent to evaluate health conditional on having the disease immediately prior to the valuation question we attempted to focus his or her attention on the characteristics of the disease risk to be reduced.
 Typical valuation question. Text and values in brackets vary across questions After eliciting WTP to reduce risk to each target individually, we presented respondents with a question to elicit WTP to reduce risk to every member of the household simultaneously. For single-person households, we did not ask this question but use the response to the second of two questions valuing a reduction in individual risk in the analysis below (we use the second question to accommodate the possibility of order effects, because other respondents will come to the household question having already answered two or more questions about risk to individuals). The characteristics of the household risk (baseline risk, risk reduction, latency, disease type, affected organ) were randomly sampled from the same set as for the individual risks. The only difference between this question and the individual-specific questions is that the food in question was said to be one that all household members would eat, and hence all would face a smaller risk if the safer, more expensive food type were purchased. The initial bid varied between $20 and $5,000. For simplicity, we elicited HRQL conditional on developing the disease for the respondent only, not for each household member individually. The household-risk question may be viewed as more realistic than the individual-specific questions, as it is common for many household members to eat the same food. By asking this question, we can compare the results of asking about WTP to reduce risk to everyone in the household jointly with the sum of WTP to reduce risk to everyone in the household individually. The subsequent section of the survey asked a number of debriefing questions about the respondents’ confidence in her responses to the valuation questions, perception about whether the risks she and household members face are larger, smaller, or about equal to those presented in the survey, whether any of the household members had experience with any of the diseases that were presented, the respondent’s role in purchasing and preparing food for the household, and opinions about how effectively government and private organizations manage risks associated with pesticide use. Following the debriefing questions, respondents were presented with questions to elicit their WTP to reduce another risk: that of a fatal motor-vehicle crash. Respondents were told the risk could be reduced by installing a new safety device in the next vehicle they purchase. The device was said to be well-tested, safe, and reliable, and to protect the driver and all passengers. The baseline risk, risk reduction, and initial bid were randomly chosen from the same sets of values as for the pesticide risks and the cost was described as an increase in the annual loan payment for five years. The intent of this question is to compare WTP to reduce disease risks associated with pesticides on food with WTP to reduce fatal-injury risks associated with motor-vehicle crashes, an endpoint that has been often studied (e.g., Jones-Lee et al. 1985; Corso et al. 2001). We attempted to reduce any tendency to anchor responses to the motor-vehicle-risk question on the household-pesticide-risk question by using different values of the baseline risk, risk reduction, and initial bid for the two questions and by asking the debriefing questions between them. Following the health-risk-valuation questions, we asked questions to elicit the respondent’s aversion to financial risk. First, we asked one of two alternative sets of questions about hypothetical financial risks. One set was used in the Health and Retirement Study and analyzed by Barsky et al. (1997). In this set, the respondent is asked to assume that he or she is the only income earner in the household and has a good job guaranteed to pay the same annual income for life. The respondent is offered the opportunity to take a new and equally good job with equal chances that it will either double his or her income or cut it by a third. Depending on the respondent’s choice, a follow-up question is asked in which the new job is made better or worse (for those who reject or accept the new job in the first question) by changing the income loss to one-fifth or one-half, respectively. Responses to the two questions allow respondents to be sorted among four ordered categories by relative risk aversion. The alternative set of hypothetical questions was similar but modified to be more plausible. The respondent was told to assume that he or she had decided to take a new job (thus removing any status-quo bias) and that the choice was between a “salary job” (paying an annual salary that would increase modestly each year) and a “bonus job” (paying a small salary but with a large annual bonus in years when the firm prospered and no bonus in other years). The proportional effects on income were identical to those in the Barsky et al. questions and the respondents to these questions can similarly be sorted into four ordered categories by relative risk aversion. Note that the lifetime financial risk is smaller in the alternative questions because the bonus is received in some years but not in others. The survey was administered in 2007, before the recession and public attention given to large bonus payments in the financial industry. Finally, respondents were offered a real lottery to gauge their risk aversion. Each respondent was told he or she had earned 5,000 bonus points for completion of the survey (bonus points are routinely awarded to Knowledge Networks panel members for completing surveys and can be exchanged for merchandise or cash at a rate of 1,000 points to the dollar). They were offered the opportunity to trade these 5,000 points for any of three binary lotteries offering equal chances of more or fewer points as follows: lottery A – 9,000 or 3,000, lottery B – 11,000 or 2,000, lottery C – 15,000 or 0. Note that these lotteries are mean-increasing spreads of the degenerate lottery offering 5,000 points for sure and that respondents who are less risk averse are anticipated to select a riskier lottery. Respondents were told that if they selected a lottery the payoff would be resolved on the following screen. Note that the payoffs to the bonus-points lotteries are common across respondents whereas the payoffs to the hypothetical questions vary with respondent income. Hence the bonus-point lottery measures absolute risk aversion and the hypothetical lotteries measure relative risk aversion.",119
40.0,1.0,Journal of Risk and Uncertainty,10 November 2009,https://link.springer.com/article/10.1007/s11166-009-9082-0,"Responsibility, scale and the valuation of rail safety",February 2010,Judith Covey,Angela Robinson,Graham Loomes,Female,Female,Male,Mix,,
40.0,2.0,Journal of Risk and Uncertainty,17 March 2010,https://link.springer.com/article/10.1007/s11166-010-9089-6,Competence effects for choices involving gains and losses,April 2010,José Guilherme de Lara Resende,George Wu,,Male,Male,Unknown,Male,"A number of studies have refined the competence hypothesis. Fox and Tversky (1995) suggested that the feeling of competence or incompetence arises from a comparison of one’s knowledge about an event with either one’s knowledge about another event or another person’s knowledge of the same event. Thus, the feeling of knowledge reflects an inherently comparative process. One implication of their comparative ignorance hypothesis is that the classic Ellsberg preferences for the clear urn over the vague urn should be significantly diminished if participants evaluate the vague and clear urns in isolation. Consistent with this prediction, Tversky and Fox replicated the classic Ellsberg preferences when participants priced the vague and clear bets in a within-subject design but found no ambiguity aversion when the two bets were presented in a between-subject study. Using a similar setup, Chow and Sarin (2001) found significant between-subject and within-subject price differences. Nevertheless, ambiguity aversion was significantly weaker in the non-comparative condition than the comparative condition. In another study, Fox and Tversky documented how an interpersonal comparison might influence the feeling of competence. San Jose State University undergrads were much less willing to choose an uncertain financial prospect when they were told that the same prospect was being evaluated by more knowledgeable individuals, graduate students in economics at Stanford University and professional stock analysts (see also, Fox and Weber 2002). Other studies including See (2009), Taylor (1995), and Trautmann et al. (2008) provide additional psychological accounts of ambiguity aversion. A number of theoretical models have been proposed to accommodate ambiguity aversion (see Camerer and Weber 1992). A particularly useful formulation for modeling source preference was proposed by Gilboa (1987) and Schmeidler (1989). These models, termed Choquet expected utility, relax the Savage axioms to permit nonadditivity of subjective probabilities. Savage’s Sure Thing Principle then no longer applies to all acts, but only to the subset of acts that are “comonotonic,” i.e., that share the same weak ranking of states. Choquet expected utility permits nonadditive probability measures, replacing the standard subjective expected utility calculus with a scheme that involves Choquet integration (Choquet 1954).Footnote 1
 Tversky and Kahneman’s (1992) cumulative prospect theory (CPT) extended Choquet integration to permit sign-dependence. Under cumulative prospect theory, the value of a mixed prospect involving the possibility of gains as well as the possibility of losses is the sum of Choquet integrals of the gain and loss portions of the prospect. Cumulative prospect theory also extended the scope of Kahneman and Tversky’s (1979) original prospect theory from prospects with at most two non-zero outcomes to prospects with multiple outcomes, and from decision under risk (prospects with objective probabilities) to decision under uncertainty (prospects defined on events). The representation for decision under risk uses a rank-dependent expected utility form for both gains and losses (Quiggin 1982). In rank-dependent models, the weight attached to an outcome depends on the rank of the outcome relative to other outcomes. See Wakker (1990) for a detailed discussion of the relationship between rank-dependent utility and Choquet expected utility. Tversky and Kahneman (1992) also scaled prospect theory for decision under risk, assuming parametric forms for the value function and probability weighting function. Tversky and Fox (1995) extended the measurement of prospect theory to decision under uncertainty and proposed a two-stage model. Consider a lottery in which the decision maker wins x if event E occurs. The two-stage model posits that individuals first judge the probability of event E, then transform this subjective probability by the probability weighting function for risk (see also Fox and See 2003; Fox and Tversky 1998; Wu and Gonzalez 1999). The two-stage model proposed by Tversky and Fox, however, does not permit source preference, and thus cannot accommodate the Ellsberg Paradox. To deal with this shortcoming, Kilka and Weber (2001) extended the two-stage model, allowing the transformation of subjective probabilities to depend on the source of uncertainty. They elicited certainty equivalents for uncertain prospects involving two different sources of uncertainty, a more and a less familiar source. The certainty equivalents were consistent with Heath and Tversky’s competence hypothesis. Kilka and Weber estimated significantly different probability weighting functions for the two sources, with the weighting function more elevated when the source was more familiar. This paper investigates how choices for gain and loss prospects are affected by the level of knowledge one has about the decision being made. Numerous studies have shown differences in choices for gains and losses. Kahneman and Tversky (1979) documented a reflection effect for decision under risk. Most participants preferred $3,000 for sure to an 80% chance at $4,000, but preferred an 80% chance at losing $4,000 to losing $3,000 for sure. Consistent with this reflection effect, Tversky and Kahneman (1992) estimated nearly identical value and weighting functions for gains and losses, though subsequent studies have shown significant differences between gain and loss weighting functions (e.g., Abdellaoui 2000; Etchart-Vincent 2004). Several studies have extended the Ellsberg Paradox from gains to losses. These studies have produced mixed results, with some studies showing ambiguity seeking for losses, particularly for high probability losses (Cohen et al. 1985; Einhorn and Hogarth 1986; Hogarth and Einhorn 1990; Kahn and Sarin 1988). Most recently, Abdellaoui et al. (2005) elicited decision weights for gains and losses under uncertainty and found that the weighting function for losses was significantly more elevated than the weighting function for gains. We use cumulative prospect theory (CPT) as our theoretical framework, since it permits a differential treatment of gains and losses. Under CPT, preferences for uncertain prospects are represented by a value function
v : R → R defined for changes in wealth, and by a decision weighting function or capacity
W : 2S →  [0, 1] defined on the events under consideration, where S represents the (finite) state space and 2S is the collection of all events. Let A
c, as well as S − A, denote the complement of event A, i.e., A and A
c are disjoint and A ∪ A
c = S. Let \(\mathcal{A}\) and \(\mathcal{B}\) represent two distinct families of events that are each closed under union and complementation. Each family represents a distinct source of uncertainty. For example, the final point differential of a football game and the noon temperature in Chicago constitute two sources of uncertainty. Let (A, x) denote a prospect that provides x if event A happens and 0 otherwise. (A,x) is a positive prospect if the prospect involves a gain (i.e., x > 0) and a negative prospect if a loss is possible (i.e., x < 0). Under CPT,
  where i is equal to “+” if (A,x) is a positive prospect and “−” if (A,x) is a negative prospect. Thus, the decision weighting function W is sign-dependent. The value function v is defined on changes of wealth, in contrast to expected utility and subjective expected utility, which are most commonly defined on wealth levels directly. The function is normalized such that v(0) = 0. Empirical studies have found that v is most commonly concave for gains, convex for losses and is steeper for losses than for gains (i.e., v exhibits loss aversion) (Abdellaoui et al. 2007; Tversky and Kahneman 1992). The decision weighting function satisfies two properties: i) W
i( ∅ ) = 0 and W
i(S) = 1, where ∅ denotes the empty set, and S denotes the state space; and ii) W
i(A) ≤ W
i(B),∀ A ⊂ B. Therefore, W
i does not need to be additive, as is the case with subjective expected utility. Indeed, studies of decision making under uncertainty have shown that the decision weighting function is typically a non-additive function (e.g., Kilka and Weber 2001; Tversky and Fox 1995; Wu and Gonzalez 1999). We now present some properties of the decision weighting function that have been documented in previous empirical studies. A decision maker’s weighting function W
i displays bounded subadditivity (SA) for the source \(\mathcal{A}\) if the two conditions below are satisfied for all \(A, A' \in \mathcal{A}\):
 Lower subadditivity (LSA): Wi(A) ≥ Wi(A ∪ A′) − Wi(A′), when Wi(A ∪ A′) is bounded away from 1; Upper subadditivity (USA): 1 − Wi(S − A) ≥ Wi(A ∪ A′) − Wi(A′), when Wi(A′) is bounded away from 0. Intuitively, these conditions mean that the decision maker uses impossibility and certainty as reference points when choosing. Lower subadditivity (LSA) implies that the impact of an event A is higher when added to the null event (∅) than when it is added to another event A′, provided the two events, A ∪ A′ are bounded away from one. Upper subadditivity (USA) implies that the impact of an event A is higher when subtracted from the certain event S than when it is subtracted from another event A ∪ A′, provided that event A′ is bounded away from zero. Preference conditions for LSA and USA are provided by Tversky and Wakker (1995), and the basic properties have been documented empirically by Kilka and Weber (2001) and Tversky and Fox (1995) for gains and Abdellaoui et al. (2005) for losses.Footnote 2
 We compare decision weighting functions for different sources in order to analyze how knowledge affects choices. Let \(\mathcal{A}\) and \(\mathcal{B}\) be two distinct families of events that correspond to different sources of uncertainty. For our purposes, we can think of \(\mathcal{A}\) as a high knowledge source, that is, composed of events which the decision maker feels more knowledgeable about, and \(\mathcal{B}\) as a low knowledge source, that is, composed of events which the decision maker feels less knowledgeable about. We represent general events in \(\mathcal{A}\) by A,A′, etc. and general events in \(\mathcal{B}\) by B,B′, etc. We also assume that events A,A′ (and B,B′) are disjoint. Tversky and Wakker also proposed a second property of W
i(A), termed source sensitivity: A decision maker displays less sensitivity to source \(\mathcal{A}\) than to source \(\mathcal{B}\) in the domain i = + , − if the two conditions below are satisfied:
 If W
i(A) = W
i(B) and W
i(A ∪ A′) = W
i(B ∪ B′) then W
i(A′) ≥ W
i(B′), W
i(A ∪ A′) and W
i(B ∪ B′) are bounded away from 1. If W
i(S − A) = W
i(S − B) and W
i(S − A′) = W
i(S − B′) then W
i(S − A − A′) ≤ W
i(S − B − B′), W
i(S − A − A′) and W
i(S − B − B′) are bounded away from 0. Tversky and Fox (1995) studied source sensitivity by comparing decision weights inferred from choices involving risk (objective probabilities) and choices involving five domains of uncertainty. They found that individuals were more sensitive to risk than uncertainty. We now turn to the property that is the central focus of our paper, source preference (Tversky and Wakker 1995): In the domain of gains, a decision maker displays source preference for source \(\mathcal{A}\) over source \(\mathcal{B}\) if W
 + (A) = W
 + (B) implies W
 + (A
c) ≥ W
 + (B
c), for all \(A \in \mathcal{A}\) and \(B \in \mathcal{B}\). In the domain of losses, a decision maker displays source preference for source \(\mathcal{A}\) over source \(\mathcal{B}\) if W
 − (A) = W
 − (B) implies W
 − (A
c) ≤ W
 − (B
c) , for all \(A\in \mathcal{A}\) and \(B \in \mathcal{B}\). For both gains and losses, source preference can be written in preference terms, (A,x) ~(B,x) implies (A
c,x) ≽ (B
c,x) (see Tversky and Wakker 1995, pp. 1270–1271). Note that the inequality in Definition 3 reverses for losses, since (A
c,x) ≽ (B
c,x) holds for x < 0 if W
 − (A
c) ≤ W
 − (B
c). Therefore, source preference elevates the weighting function for gains and depresses the weighting function for losses. The Ellsberg two-urns experiment described in the Introduction indicates that people usually display a source preference for risk over ambiguity when gains are involved. If we denote by E the event of a red ball in the clear urn and by A the event of a red ball in the vague urn, the standard Ellsberg demonstration shows that (E,x) ≽ (A,x) and (E
c,x) ≽ (A
c,x). Next we state our hypothesis about the generalization of the competence effect from gains to losses. Decision makers prefer betting on high knowledge domains over low knowledge domains for gains. However, they prefer betting on low knowledge domains over high knowledge domains for losses. The gain part of the hypothesis is in accord with the findings of Heath and Tversky (1991) and Kilka and Weber (2001), with the loss portion previously untested. Our hypothesis extends Kahneman and Tversky’s original reflection effect (1979) to include source dependence. Kahneman and Tversky showed that preferences are generally risk averse for gains but risk seeking for losses. For uncertainty, Heath and Tversky’s competence hypothesis asserts that participants tend to bet on events that they feel more knowledgeable about when choices involve gains. We speculate, however, that there is an opposite tendency when choices involve losses: participants tend to bet on events that they feel less knowledgeable about. This is a fundamental behavioral difference between choices with gains and choices with losses that was not yet investigated. The basic motivation follows from the fourfold pattern of risk attitudes documented by Tversky and Kahneman (1992). Decision makers are typically risk averse for medium to high probability gains and risk seeking for low probability gains. This pattern reverses for losses. Tversky and Fox (1995) suggest that risk aversion and risk seeking might be explained in part by affective feelings such as fear and hope. Thus, the medium to high probabilities that produce fear for gains correspond to hope for losses, with low probabilities resulting in hope for gains and fear for losses. We hypothesize that a similar pattern holds for ambiguity. High knowledge domains are relatively more hopeful for gains but relatively more fearful for losses. A similar intuition is expressed in Basili et al. (2005). They extend prospect theory, dividing outcomes into familiar and unfamiliar ones, and propose that preferences reflect pessimism for gains and optimism for losses when outcomes are unfamiliar. The reason for this reflection from a preference for high knowledge for gains to low knowledge for losses is straightforward. For sources that differ in competence, it means that people tend to shy away from the certainty of having a loss. Therefore, when choosing between two uncertain events that differ only in the relative feeling of competence, decision makers choose the one they feel less competent about. We quantify our main hypothesis by using the two-stage process originally proposed by Tversky and Fox (1995) and extended by Kilka and Weber (2001). Tversky and Fox proposed that decision makers evaluate an uncertain prospect, (A,x), by first judging the probability of the event A and then transforming this judged probability using a risky weighting function:
  where W is the weighting function for uncertainty, P(A) is the judged probability for event A, and w

R
 is the risky weighting function, i.e., the weighting function applied to prospects with objective probabilities. Tversky and Fox (1995) and Fox and Tversky (1998) provided empirical support for this two-stage model, and Wakker (2004) offered a formal justification for this approach. However, this account is clearly incompatible with the Ellsberg Paradox. Thus, Kilka and Weber (2001) extended the two-stage model to account for source preference:
  where \(w_{\mathcal{S}}\) is the weighting function for source \(\mathcal{S}\) of uncertainty. Source preference and source sensitivity can be captured with the functional specification originally proposed by Goldstein and Einhorn (1987) for risk:
 This function was used by Birnbaum and McIntosh (1996), Gonzalez and Wu (1999), Kilka and Weber (2001), Lattimore et al. (1992), and Tversky and Fox (1995), among others. All of the participants in Gonzalez and Wu (1999) exhibited an inverse S-shaped probability weighting function, such that low probabilities were overweighted and high probabilities were underweighted. However, there was considerable heterogeneity across participants, and Gonzalez and Wu showed that this two-parameter function was better able to capture the heterogeneity than standard one-parameter weighting functions. The weighting function estimates for the median data in Gonzalez and Wu (1999) are plotted in Fig. 1.
 Probability weighting function estimates, from median participant data of Gonzalez and Wu (1999) Each parameter, δ and γ, in Eq. 1.1 captures a different psychological characteristic. Expected utility corresponds to w(p) = p, or γ = 1 and δ = 1. The parameter γ measures the curvature of the weighting function, capturing the way the decision maker discriminates between probabilities. As γ approaches 0, the function approaches a step function. The parameter δ measures the elevation of the weighting function, capturing how attractive gambling is to the decision maker, with higher δ corresponding to more risk-seeking for gains and more risk aversion for losses. Therefore, the curvature parameter γ is a measure of source sensitivity and the elevation parameter δ is a measure of source preference. If γ for source \(\mathcal{A}\) is higher than for source \(\mathcal{B}\), then source \(\mathcal{A}\) displays more source sensitivity than source \(\mathcal{B}\) (see Definition 2). For source preference, the relation is different for gains and losses. In accord with Definition 3, for gains, larger δ indicates more source preference; for losses, a smaller δ indicates more source preference. For a more thorough discussion about the psychological interpretation of these two parameters, see Gonzalez and Wu (1999). Thus, we extend Eq. 1.1 to accommodate source dependence, following Kilka and Weber (2001):
  where i = “+” or “−” (corresponding to gains and losses, respectively) and k = H or L (corresponding to high and low knowledge, respectively). Estimating the values of γ and δ for different sources thus allows us to test for source preference. In particular, we can test whether our hypotheses holds, by comparing \(\delta^+_H\) and \(\delta^+_L\), and \(\delta^-_H\) and \(\delta^-_L\), with our hypothesis requiring that \(\delta^+_H > \delta^+_L\) and \(\delta^-_H >\delta^-_L\). Finally, following Fox and Tversky (1998), who showed that judged probabilities satisfy support theory (Tversky and Koehler 1994), we also test whether judged probabilities satisfy binary complementarity, i.e., the probabilities of complementary events sum to one:Footnote 3
 Judged probabilities satisfy binary complementarity for a given source \(\mathcal{A}\) if P(A) + P(A
c) = 1 for all \( A\in \mathcal{A}\). Empirical support for binary complementarity is provided by Tversky and Koehler (1994) and Tversky and Fox (1995). Violations of binary complementarity, however, have been documented by Brenner and Rottenstreich (1999) and Macchi et al. (1999).",22
40.0,2.0,Journal of Risk and Uncertainty,09 March 2010,https://link.springer.com/article/10.1007/s11166-010-9088-7,Ambiguity and the value of information,April 2010,Arthur Snow,,,Male,Unknown,Unknown,Male,"The recursive utility model of risk preferences with smooth ambiguity preferences developed by Klibanoff et al. (2005) is adapted to investigate the value of information that reduces or resolves ambiguity, and the value of information that resolves risk.Footnote 6 The basic elements of the model are (1) a utility function U defined on state-contingent wealth that captures risk preferences, (2) second-order probabilities that capture the decision maker’s subjective beliefs about objective probabilities, represented here by a probability distribution function F, and (3) a nondecreasing transformation function φ defined on expected utility that captures ambiguity preferences. The decision maker chooses the level x of an activity that determines wealth W

i
 in each of n possible states indexed by i. Using π to denote a vector of subjective probabilities π

i
 for the n states, the decision maker’s criterion is given by
 where E

F
[·] denotes the expectation with respect to F(π). The decision maker is strictly risk averse if U is strictly concave. When φ is linear, the decision maker is ambiguity neutral, as then criterion (1) reduces to the Savage (1954) model of subjective expected utilityFootnote 7
 with the probability of state i equal to the expected value of π

i
 under F, E

F
[π

i
]. Klibanoff et al. (p. 1862) define the decision maker to be ambiguity averse if, for any given choice x, the value of criterion (1) decreases when ambiguous beliefs F(π) change in a way that induces a mean-preserving spread in the distribution of expected utility \( \sum\nolimits_i {{\pi_i}U\left( {{W_i}(x)} \right)} \), while the decision maker is ambiguity loving if the value of the criterion increases. They show that, by these definitions, ambiguity aversion (love) implies concavity (convexity) for φ, and that a concave transformation of φ results in greater ambiguity aversion. Since the resolution of ambiguity reveals objective probabilities, but these are explicitly not dealt with in the model developed by Klibanoff et al. (p. 1863), an adaptation of the model is required to incorporate objective risk in order to analyze the value of information that resolves ambiguity. Further, while ambiguity aversion and greater ambiguity aversion are characterized by Klibanoff et al., greater ambiguity is not. To address these issues, let p denote the vector of objective probabilities for the n states, and consider an environment of pure risk wherein the decision maker knows that these are the correct probabilities. In that event, the decision maker’s criterion is expected utility,
 Introducing ambiguity into this choice setting is logically inconsequential for an ambiguity-neutral decision maker, whose criterion is given by (2). Hence, (2) and (3) must be identical, which requires
 This condition can be interpreted as requiring that the decision maker’s ambiguous beliefs are objectively unbiased. To elaborate this point, consider the following thought experiments. First, suppose that an investigator deposits one-by-one an equal number of red and black balls into an urn while in full view of the decision maker. With respect to alternative gambles on the result of a random draw from the urn, the decision maker is in a situation of pure risk with criterion (3). In a second experiment, the decision maker is presented with another urn and told that the intention was for the second urn to have an equal number of red and black balls, but that the graduate assistant who loaded the urn may have miscounted red and black balls. It seems reasonable to expect that an ambiguity-neutral decision maker would be indifferent between a bet on the result of a random draw from the first urn and the same bet on the result of random draw from the second urn. Indeed, this presumption appears to underlie much of the discussion about ambiguity preferences beginning with Ellsberg (1961) and Fellner (1961). Moreover, if ambiguity preferences and ambiguous beliefs are completely separate in the same manner as risk preferences and risks, as they are in criterion (1), then the assumption that ambiguous beliefs are unbiased in the sense of condition (4) is compelling, since this condition must be satisfied with ambiguity-neutral preferences, and were it violated with ambiguity-averse or ambiguity-loving preferences, then ambiguous beliefs would depend on ambiguity preferences. Accepting condition (4) leads naturally to a definition of greater ambiguity in terms of mean-preserving spreads. Given ambiguous beliefs F(π) satisfying condition (4), any second-degree stochastic dominance (SSD) deterioration of F(π) that is mean-preserving in the sense that condition (4) is maintained increases ambiguity. Observe that SSD deteriorations of F(π) reduce the expected value of any univariate, increasing, concave function of π.Footnote 8 Hence, by this definition of greater ambiguity, an increase in ambiguity reduces the value of an ambiguity-averse decision maker’s utility as defined by criterion (1) for a given choice of x, since φ is concave for these decision makers. For ambiguity lovers, criterion (1) increases with greater ambiguity. The recursive utility model is well suited to analyzing the value of information that influences the degree of ambiguity for several reasons.Footnote 9 First, ambiguity aversion is defined in terms of mean expected utility preserving spreads, allowing ambiguity to be defined without reference to ambiguity preferences, and leading to natural definitions of greater ambiguity and greater ambiguity aversion. In contrast, these two concepts cannot be distinguished in the model of Choquet expected utility developed by Schmeidler (1989), wherein ambiguity and ambiguity preferences are conflated. Second, the separation of ambiguity preferences from ambiguous beliefs justifies condition (4) stipulating that ambiguous beliefs are objectively unbiased regardless of ambiguity preferences. In models that capture ambiguity preferences in a probability weighting function, such as the decision weighting model of Kahn and Sarin (1988), the weighting function must be conditioned on objective probability if resolution of ambiguity is to result in the expected utility criterion (3), and this links ambiguous beliefs to ambiguity preferences. Third, environments with a multiplicity of states are accommodated with state-contingent payoffs that need not have the same ranking under alternative choices for x. Preserving the ranking of payoffs would be needed for reasons of tractability in analyzing the value of information in models based on rank dependent expected utility developed by Quiggin (1982). Finally, since the decision criterion is differentiable, the envelope theorem can be exploited, which need not be the case if the max-min expected utility model of Gilboa and Schmeidler (1989) were used.",66
40.0,2.0,Journal of Risk and Uncertainty,03 March 2010,https://link.springer.com/article/10.1007/s11166-010-9090-0,Rationality on the rise: Why relative risk aversion increases with stake size,April 2010,Helga Fehr-Duda,Adrian Bruhin,Renate Schubert,Female,Male,Female,Mix,,
40.0,2.0,Journal of Risk and Uncertainty,23 February 2010,https://link.springer.com/article/10.1007/s11166-010-9087-8,Do people respond to low probability risks? Evidence from tornado risk and manufactured homes,April 2010,Daniel Sutter,Marc Poitras,,Male,Male,Unknown,Male,"The costs of natural disasters depend on the preparations taken by individuals such as location decisions, purchase of insurance, and investments in mitigation and evacuation. The impact of Hurricane Katrina in particular raised concerns about the adequacy of preparation for natural hazards (Meyer 2006). In fact, a considerable body of evidence suggests that people do not respond as predicted by expected utility theory to low probability, high consequence risks. For instance, many people fail to insure against floods despite the availability of subsidized insurance through the National Flood Insurance Program. As Kunreuther and Pauly (2004) note, any risk-averse individual should want to purchase insurance at such rates. Yet when Katrina struck, homeowners within blocks of the coast did not have flood insurance. People might fail to act on low probability risks for a number of reasons, including a minimum threshold for seeking out detailed risk information (Kunreuther and Pauly 2004), inability to comprehend very small probabilities (Kunreuther et al. 2001), overconfidence in one’s ability to control the situation and limit damage (Palm 1998; Meyer 2006), and excessive discounting of future hazard costs (Kunreuther and Kleffner 1992). Experiments on risk valuation conducted by McClelland et al. (1993) yield a bimodal distribution, with people tending either to overvalue low probability risks or to treat them as zero. In a national survey, Viscusi and Zeckhauser (2006) find that 90% of Americans believe they face natural disaster risk of average or below average. For any of the aforementioned reasons, it follows that people can treat the very small probability of loss as if it were zero, resulting in an inefficiently large amount of exposure. According to Camerer and Kunreuther (1989, p. 566), “[T]he evidence suggests that market outcomes often are suboptimal. ... [W]e ask what policy makers should do when people do not appear to behave according to economic rationality.” The prevalence of low probability event bias, however, is challenged by evidence that markets do respond to natural disaster risk. Shilling et al. (1985), MacDonald et al. (1987), and Spreyer and Ragas (1991) find that homes on Louisiana flood plains sell at a discount. Brookshire et al. (1985) detect a price discount for homes in California seismic zones, and Beron et al. (1997) conclude that the discount diminished after the 1989 Loma Prieta earthquake did less damage than expected. Hallstrom and Smith (2005) and Carbone et al. (2006) determine that Hurricane Andrew slowed the growth of home prices in areas of coastal Florida subject to storm surge flooding. Other studies discover premiums in the market for various types of hazard mitigation, specifically hurricane shutters (Simmons et al. 2002) and tornado shelters in mobile home parks (Simmons and Sutter 2007). Furthermore, while observed lack of preparedness might arise from low probability bias, there are alternative explanations. For example, insurance can reduce the incentive for self-protective behavior, especially if there are no high deductibles or premium discounts. And appropriate discounts may be difficult for insurers to offer due to moral hazard. Also, the incentive to prepare might be undermined by the prospect of public or private disaster relief. Even though surveys do not reveal that relief affects insurance purchase or preparations (Kunreuther 1978; Palm 1998), given the generosity of government and private charity, the Samaritan’s Dilemma is difficult to rule out as an explanation for lack of preparation. As a consequence, just how much under-preparedness is due specifically to low probability bias is difficult to gauge. We examine the impact of tornado risk on the demand for manufactured homes, which for several reasons provides a fruitful case study of hazard perception. First, tornadoes certainly qualify as a low probability risk. In our data, the U.S. state with the highest annual tornado probability is Arkansas at 0.000445, or a return period of over 2200 years. Second, as discussed in the introduction, over 40% of U.S. tornado fatalities occur in manufactured homes, so the risk has significant cost. Third, the risk is to life and limb and not property, and this helps to rule out disaster relief or insurance as explanations for inaction. Disaster relief or insurance is unlikely to provide adequate compensation for serious or fatal injuries, and so the risk is not shifted to third parties. Tornado risk therefore offers a relevant and relatively clean test of low probability risk perception. Fourth, our data set reflects actual market choices of millions of individuals over several decades, which complements evidence from surveys and experiments. Finally, the choice of a manufactured home is exercised primarily by relatively low income households. Evidence suggests that low income households are particularly vulnerable to natural hazards. For example, low income households are less likely to evacuate for hurricanes (Dash and Gladwin 2007), and low income neighborhoods recovered more slowly from Hurricane Andrew (Smith et al. 2006). Our analysis therefore provides evidence on risk perception and response by a relatively vulnerable population.",20
40.0,3.0,Journal of Risk and Uncertainty,14 May 2010,https://link.springer.com/article/10.1007/s11166-010-9094-9,How do people value extended warranties? Evidence from two field surveys,June 2010,Marieke Huysentruyt,Daniel Read,,Female,Male,Unknown,Mix,,
40.0,3.0,Journal of Risk and Uncertainty,07 May 2010,https://link.springer.com/article/10.1007/s11166-010-9093-x,Reverse common ratio effect,June 2010,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"Six pairs of common ratio decision problems were used in experiment 1. Pairs 1–3 were designed to test the common ratio effect when a sure monetary payoff is far below the expected value of the risky lottery in the “scaled-up” problem. Pairs 4–6 were designed to test the common ratio effect when the risky lottery yields a higher outcome with a probability less than 0.5 in the “scaled-up” problem. All six pairs of decision problems are presented in Table 1 below.
 In every pair, the “scaled-down” decision problem is obtained from the “scaled-up” problem by dividing the probabilities of all non-zero outcomes by three. Pairs 1–3 have the same risky lottery but a different sure payoff in the “scaled-up” problem. Pairs 4–6 follow a similar design. This is done to rule out the following “indifference” argument. In principle, an individual who chooses a sure payoff in the “scaled-up” decision problem but switches to choosing a riskier lottery in the “scaled-down” decision problem (or vice versa) does not necessarily violate expected utility theory. This individual may happen to be exactly indifferent between the sure payoff and a risky lottery in the “scaled-up” problem. Under expected utility theory, she is then also indifferent between both alternatives in the “scaled-down” decision problem. Since an individual who is exactly indifferent may choose in any way she likes, a revealed switching behavior can, in fact, be consistent with expected utility theory. This does not strike us as a very plausible explanation (many people happen to be exactly indifferent and, when indifferent, they randomize their choices with different probabilities in different decision problems). Even though this appears to be an unlikely scenario, we designed our experiment to control for this explanation because none of the existing experimental studies have done so. To rule out the “indifference” argument, we used the same risky lottery but several different sure payoffs in the “scaled-up” problem. Thus, if a revealed switching behavior is a mere reflection of indifference, it can be observed, at most, in one pair of decision problems. If it is observed in two or three pairs, it cannot be attributed to indifference because an individual cannot be simultaneously indifferent between a risky lottery and two or three distinct monetary amounts. Since not all subjects may be familiar or comfortable with probability calculus, the probability information was conveyed through a composition of red and black cards. Figure 1 shows a “scaled-down” problem from pair 1 as it was displayed in the experiment.
 A sample question as displayed in the experiment",32
40.0,3.0,Journal of Risk and Uncertainty,31 March 2010,https://link.springer.com/article/10.1007/s11166-010-9091-z,Do ambiguity avoidance and the comparative ignorance hypothesis depend on people’s affective reactions?,June 2010,Enrico Rubaltelli,Rino Rumiati,Paul Slovic,Male,Male,Male,Male,"To test our hypothesis we investigated the same bets that Fox and Tversky (1995) used in their Experiments 1 and 4. We decided to replicate these two studies because they use different types of bets. In their first experiment Fox and Tversky tested the materials originally used by Ellsberg. It should be a compelling result to show, using the materials most widely employed to test ambiguity avoidance, that people’s affective reactions have the same pattern as the willingness to pay measured in past studies. In addition, we decided to use the bets from Fox and Tversky’s Experiment 4 because these materials do not deal with known and unknown probabilities but with how competent individuals feel about different events. As a consequence, those bets should allow us to provide a stronger support for the role played by the affective reactions in causing comparative ignorance and, in turn, ambiguity avoidance. Participants were presented with a pair of bets composed by an ambiguous bet and a clear one (like in Fox and Tversky 1995). They were also presented with a second pair of bets composed by an unfamiliar bet and a familiar one (like in Fox and Tversky’s study 4). Participants were asked to rate the attractiveness of each bet (two bets for each pair in JE and only one of them in the two separate conditions).We expect to find that the clear (familiar) bet is rated significantly more attractive than the ambiguous (unfamiliar) one in JE but not in SE. In addition, we hypothesize to find that the feeling induced by the clear (familiar) bet should be clearer and more positive than the feeling induced by the ambiguous (unfamiliar) bet in JE but not in SE. Finally, the clear (familiar) bet should be judged easier to evaluate than the ambiguous (unfamiliar) one, but only in JE. One hundred fifty university students (50% females; mean age 22 years) at the University of Padova in Italy took part in the study. They were randomly assigned to one of the three conditions (50 students in each condition: JE, SE-clear bet, and SE-ambiguous bet). Participants were presented with either both or only one of the two bets from each pair. The materials were partially modified to make them more suited to the affective reactions measure employed here as it is different from both the willingness to bet measure tested by Fox and Tversky and the willingness to accept measure tested by Chow and Sarin. However, there were no major differences between our materials and those used in previous studies. The first pair of bets investigated the Ellsberg Paradox. In JE, the two bets were presented as follows: Imagine that there is a bag on the table (Bag A) filled with exactly 50 red poker chips and 50 black poker chips, and a second bag (Bag B) filled with 100 red and black poker chips, but you do not know their relative proportion. Imagine that someone offered you two tickets (priced €20 each). The first ticket allows you to draw a chip from Bag A, whereas the second ticket allows you to draw a chip from Bag B. In both cases the game will be played as follows: First, you are to guess a color (red or black); next, without looking, you are to draw a poker chip out of the bag. If the color that you draw is the same as the one you predicted, then you will win €100; otherwise you win nothing. 
BAG A:
 
BAG B:
 
50 red chips
 
? red chips
 
50 black chips
 
? black chips
 
100 total chips
 
100 total chips
 Participants in the two SE conditions saw basically the same description but with only one of the two bets. The second pair of bets investigated ambiguity inducing people to feel different levels of knowledge about the events on which they were supposed to bet. In their study Fox and Tversky asked people for their willingness to pay in order to play two different gambles. The first gamble offered people the chance to win $100 if the temperature in San Francisco a week later was at least 60 degrees Fahrenheit (familiar bet), whereas the second bet offered the same amount of money if the temperature in Istanbul a week later was at least 60 degrees Fahrenheit (unfamiliar bet). For the respondents, who were tested at the University of California at Berkeley, the first bet induced a feeling of higher knowledge and familiarity than the second one. Since they were not familiar with Istanbul they should have perceived the second bet as vaguer than the first one regardless of the fact that the two cities have similar climate. In the present study, we modified the bets in order to make them more suitable for Italian respondents. Therefore the familiar bet asked people to bet on the future temperature in Padova, the city where the participants attend the university, whereas the unfamiliar bet asked people to bet on the future temperature in Portland, Oregon, a place with similar climate but unfamiliar to most Italian people. In JE, the two bets were presented as follows: BET A: Imagine that you have been offered a ticket (which costs €20) to play a bet that will pay you €100 if the afternoon high temperature in Padova is at least 15 degrees Celsius one week from today. BET B: Imagine that you have been offered a ticket (which costs €20) to play a bet that will pay you €100 if the afternoon high temperature in Portland, Oregon, is at least 15 degrees Celsius one week from today. Participants in the two SE conditions saw basically the same description but with only one of the two bets respectively. Dependent variables were the same for both pairs of bets. In JE, participants were presented with each question and answered it rating both bets within each pair before advancing to the following question. In the first question, participants were asked to judge how attractive they found each alternative on a 9-point Likert scale ranging from −4 (“very unattractive”) to 4 (“very attractive”). The second question asked to rate how clear a feeling participants had toward each alternative. They rated the alternatives on a 7-point scale ranging from 0 (“not clear at all”) to 6 (“very clear”). After rating the clarity of the feeling participants were asked to rate the intensity of their good/bad feeling on a 9-point Likert scale ranging from −4 (“very bad”) to 4 (“very good”). Finally, the last question asked to rate how easy or difficult participants found to evaluate each alternative. Once again the ratings were provided on a 9-point Likert scale ranging from −4 (“very difficult”) to 4 (“very easy”). In the analyses we coded the three −4 to +4 Likert scales as dichotomous variables, therefore the participants who gave a negative rating were coded as 0 (e.g., “unattractive”) and those who gave a positive rating were coded as 1 (e.g., “attractive”). Participants who provided a rating of 0, meaning indifference toward the alternative, were discarded from the analyses. For the clarity of feeling, we broke the 0 to 6 Likert scale coding the participants who gave a rating between 0 and 2 as 0 (“not clear feeling”), while those who gave a rating of 3 or higher were coded as 1 (“clear feeling”). Therefore, we analyzed the data comparing the percentage of times that an alternative was evaluated using the upper half of a scale versus the percentage of times that participants evaluated the same alternative using the lower half of the scale.",24
40.0,3.0,Journal of Risk and Uncertainty,22 April 2010,https://link.springer.com/article/10.1007/s11166-010-9092-y,Emotional decision-makers and anomalous attitudes towards information,June 2010,Francesca Barigozzi,Rosella Levaggi,,Female,Female,Unknown,Female,"Following the methodology of Caplin and Leahy (2001, 2004), the DM’s utility is a function of physical outcomes and belief-based emotions, with anticipatory emotions responding to information. There are two periods, 1 and 2, and total utility is the sum of future utility from physical outcomes, and current anticipatory emotions, which depend on rationally formed beliefs about the exact same outcomes.Footnote 3
 The DM’s physical utility in period 2 is h(w

i
,a), where \(w_{i}=\left\{ w_{1},w_{2}\right\} \) is the physical outcome realized in period 1, with \( w_{i}\in \Re ,\) and a is an action. Importantly and as in the case of the motivating examples before, even if the physical outcome is realized in period 1, w

i
 has physical effects only in period 2. In the same way the action \(a\in \Re \) is taken in period 1 but it only affects utility in period 2.Footnote 4
 We assume that w
1 is the preferred outcome such that w
1 > w
2. Moreover, each outcome has the same probability p

i
, i = 1,2 of occurring, p
1 = p
2 = 1/2, and the DM’s priors are correct. By observing a signal the DM obtains information on the outcome realized. A signal is a random variable which can take two values, s
1 and s
2. A signal is characterized by a pair of conditional probabilities \(\left( q_{1},q_{2}\right) \), where \(q_{i}\in \left[ \tfrac{1}{2},1\right] ,\)
i = 1,2, is the probability of observing the realization s

i
 conditional on the outcome being w

i
: q

i
 =prob(s

i
|w

i
). For the sake of tractability we also assume that q
1 = q
2 = q, that is, the signal is symmetric. Since in the first period the DM derives utility from the anticipation of period-2 physical outcome, anticipatory utility depends on expected physical utility in period 2 conditional on the DM’s beliefs in period 1. In turn, the DM’s beliefs depend on the signal observed:
 The function u(·) is increasing in the expectation of physical utility \(E\left[ h\left( w,a\right) \right] \). The shape of u(·) determines the DM’s preferences for information. When u(·) is concave, the DM is averse to information and he dislikes bad news more than he likes good news. If u(·) is convex, on the contrary, the DM is ‘information-loving’. Finally, when u(·) is linear the DM is ‘information-neutral’. Utility in period 2 corresponds to physical utility \(h\left( w_{i},a\right) . \) To calculate the total DM utility from the perspective of period 1, we add to emotional utility 1.1 the expectation of period-2 physical utility: \(E\left[ \left. h\left( w,a\right) \right\vert s_{i}\right] \). As shown in the Appendix, by considering total utility we obtain results that are qualitatively equivalent to those we obtain by restricting ourselves to emotional utility. For this reason, in the rest of the paper we focus on anticipatory utility (Eq. 1.1).Footnote 5
 Let’s assume that physical utility is \(h(w_{i},a)=w_{i}-\left( w_{i}-a\right) ^{2}.\) Thus, given the outcome w

i
, w

i
 also corresponds to the maximum level of physical utility that can be achieved if the appropriate action (a = w

i
) is chosen. If the taken action a is not appropriate, the DM will be worse off: the loss function \(\left( w_{i}-a\right) ^{2}\) measures the damage resulting from inaccurate actions. The DM chooses the precision of the signal q. The decision to become completely informed is equivalent to the choice of q = 1, the fully informative signal, whereas the decision to remain ignorant is equivalent to the choice of \(q=\tfrac{1}{2},\) the uninformative signal. The decision to become partially informed corresponds to the choice of an intermediate value for q. The DM’s choice of the precision of the signals, given the prior p = 1/2, is rational. When choosing q, the DM anticipates both that he will update his beliefs upon observing the signal’s realization and that he will choose the action a according to such beliefs. Information is completely costless. To summarize, the DM maximizes his emotional utility (1.1). First he chooses the signal precision q and observes the realization of the signal, then he updates his priors according to Bayes’ rule, and finally he chooses the optimal action a given updated beliefs. All the actions take place in period 1. We assume that the function u(·) is concave. In fact, as will be clear in Section 3, no interesting trade-off exists between the physical benefits and emotional costs of information for DMs who are not information-averse. Let’s calculate posterior beliefs in our simple model. Given priors \(p=\frac{ 1}{2}\) and conditional probabilities q = prob(s

i
|w

i
), i = 1,2, the probabilities of the two signals and posterior beliefs are indicated in Table 1.
 We call z

i
 the posterior probability of the preferred outcome w
1, given that the signal s

i
 has been observed: z

i
 ≡prob (w
1|s

i
). Note that, in this simple setting, conditional probability q corresponds to the updated belief that the true state is w
1, given the signal s
1. In other words, in our model by choosing the precision of the signal q, the DM directly chooses the posterior belief associated with each signal.",8
41.0,1.0,Journal of Risk and Uncertainty,09 June 2010,https://link.springer.com/article/10.1007/s11166-010-9095-8,Excluded losses and the demand for insurance,August 2010,Donald J. Meyer,Jack Meyer,,Male,Male,Unknown,Male,"Exclusions in an insurance contract identify losses that are not reimbursed under the terms of the contract. For the examples mentioned in the introduction, it is the case that when an excluded loss occurs, a covered loss does not occur, and vice versa. For instance, if one dies from suicide, an excluded loss, one cannot simultaneously die from heart disease, a covered loss. Similarly if one drops a television and breaks the glass screen, the television does not simultaneously fail due to a manufacturing defect. It is this particular property of many excluded losses that is used here to define in a formal way what is meant by an excluded loss in this analysis. In the discussion, an excluded loss has two characteristics. First, an excluded loss is an unreimbursed loss that pertains to the object being insured, and second, excluded losses and covered losses do not occur together. It is this second characteristic, however, that defines an excluded loss in a formal way.Footnote 3 Excluded losses are simply losses that do not occur when an insured loss occurs and vice versa. That is, whenever an excluded loss occurs, an insured loss does not occur, and whenever an insured loss occurs, an excluded loss does not occur. This of course implies that excluded and insured losses are not independently distributed, but instead are negatively correlated. Unreimbursed losses, when they occur to other assets, are called background risk and are assumed to be independently distributed because that seems to be a reasonable simplifying assumption. Similarly, unreimbursed losses which impact the insured asset itself are referred to as excluded losses, and are assumed to be mutually exclusive from insured losses. This assumption is made because it seems to be reasonable and simplifying. To specify this definition of an excluded loss more formally and to introduce some notation, let x1 denote the magnitude of a loss that is partially or fully reimbursed under the terms of an insurance contract, and let x
2 denote the magnitude of an excluded loss under that same contract. Further assume that V represents the value of the object when no loss occurs, and for convenience assume that each of these loss random variables, when positive, is continuously distributed with support in the interval (0, V]. Assume that the probability that both types of loss are zero, (x1, x
2) = (0, 0), is δ. For x1 and x
2 to be covered and excluded losses respectively, the density function of the nonzero bi-variate random variable (x1, x
2), g(x1, x
2), must be able to be written as:
 for uni-variate density functions fi(x) and 0 < α < 1−δ. Probability mass δ is located at (0, 0). This specification of the joint probability function for (x1, x
2) requires that all probability mass be continuously distributed along the two axes in (x1, x
2) space, with probability mass of size δ at the origin. This implies that x1 is zero whenever x
2 is positive and vice versa. A way to interpret this condition on the joint probability function for (x1, x
2) is to assume that nature first determines whether a loss occurs or not, then determines whether the loss is to be covered or excluded, and finally determines the size of the loss if one occurs. δ is the probability that a loss does not occur, α is the probability that a loss occurs and is covered, and (1−α−δ) is the probability that a loss occurs and is excluded. The density function for the size of the covered or excluded loss is f1(x) or f2(x), respectively. There is no requirement that the density functions f1(x) and f2(x) be the same as one another. This allows one to examine the effect of changing the size or riskiness of excluded losses holding the density for covered losses, and hence the price of insurance, fixed. With this formal specification of the density of covered and excluded losses, incorporating excluded losses into the standard insurance demand model is easily carried out.Footnote 4
 The standard insurance demand model can be modified to include this bi-variate random loss variable (x1, x
2) rather than the usual uni-variate loss variable x. This model with excluded losses has as the outcome variable W which is given by:
 In this model, W is random final wealth, W0 is nonrandom initial wealth, V is the nonrandom value of an object subject to loss when no loss occurs. x1 is a random insurable loss with indemnification function I(x1). P is the selling price for this insurance, and x
2 is a random excluded loss. 0 ≤ I(x1) ≤ x1 is assumed. θ is the choice variable indicating the level of insurance selected and is assumed to be chosen to maximize expected utility, Eu(W). The indemnification function can be I(x1) = x1, representing coinsurance, or can take on the deductible or another form. For comparative static purposes, the price of insurance is assumed to satisfy P = (1 + λ)E[I(x1)], where λ is referred to as the load factor and is assumed to be nonnegative. No additional assumptions concerning I(x1) or P are required at this point. Expected utility is given by the expectation of u(W) with respect to the density for non zero losses g(x1, x
2) characterized in (1) plus utility when no loss occurs. This expression for Eu(W) is written below as the expectation over four disjoint sets that together represent all (x1, x
2) in [0, V] X [0, V].
 The properties of g(x1, x
2) assumed in (1) imply that the first two portions of this joint expectation reduce to expressions that involve the expectation over a single random variable, and that the third is equal to zero. That is, the expression for Eu(W) reduces to:
 The first term in this expression for Eu(W) is quite similar to the expression for expected utility in the standard model of insurance demand, while the second term is not included in a standard model of insurance demand and results from incorporating excluded losses into the analysis.Footnote 5
 When θ is chosen to maximize Eu(W) given in (2), the first order condition for expected utility maximization is:
 and the second order condition for this maximization is satisfied when u′′(W) < 0. This expression for expected utility and the resulting first order condition are very manageable expressions for comparative static purposes. The comparative static findings, however, are quite different from those in the standard insurance demand model. Before presenting the formal analysis of this insurance demand model in Section 3, this decision to insure is compared with the decision to self protect introduced by Ehrlich and Becker (1972). In addition, the model of insurance demand with the possibility of nonperforming insurance contracts presented by Doherty and Schlesinger (1990) and deductible insurance are discussed and shown to be important special cases of this model.",7
41.0,1.0,Journal of Risk and Uncertainty,16 June 2010,https://link.springer.com/article/10.1007/s11166-010-9098-5,Behavioral assumptions for a class of utility theories: A program of experiments,August 2010,R. Duncan Luce,,,Unknown,Unknown,Unknown,Unknown,,
41.0,1.0,Journal of Risk and Uncertainty,12 June 2010,https://link.springer.com/article/10.1007/s11166-010-9097-6,Separating curvature and elevation: A parametric probability weighting function,August 2010,Mohammed Abdellaoui,Olivier L’Haridon,Horst Zank,Male,Male,Male,Male,"Let X denote the set of outcomes. For simplicity of exposition, we assume a finite set of outcomes, such that X = {x
1,...,x

n
}. Our results hold for general sets of outcomes if we have at least four distinct outcomes. A prospect is a finite probability distribution over the set X. Prospects can be represented by \(P=(\tilde{p}_{1},x_{1};\ldots ; \tilde{p}_{n},x_{n})\) meaning that outcome x

j
 ∈ X is obtained with probability \(\tilde{p}_{j}\), for j = 1,...,n. Naturally, \(\tilde{p} _{j}\geq 0\) for j = 1,...,n and \(\sum_{i=1}^{n}\tilde{p}_{i}=1\). Let L denote the set of all prospects. A preference relation
\( \succcurlyeq \) is assumed over L, and its restriction to subsets of L (e.g., all degenerate prospects where one of the outcomes is received for sure) is also denoted by \(\succcurlyeq \). The symbol ≻ denotes strict preference while ∼ denotes indifference (\(\preccurlyeq \) and ≺ denote reversed weak and strict preferences, respectively). To further simplify the exposition, we assume that no two outcomes in X are indifferent, and further, that outcomes are ordered from best to worst, i.e., x
1 ≻ ⋯ ≻ x

n
. It will be convenient to use an alternative notation for prospects, following Abdellaoui (2002) and Zank (2008). In the cumulative probabilities notation P = (p
1,...,p
n − 1,1), where \( p_{j}=\sum_{i=1}^{j}\tilde{p}_{i}\) denotes the probability of obtaining outcome x

j
 or better, j = 1,...,n. Similarly, in the decumulative probabilities notation P = (1,1 − p
1,...,1 − p
n − 1) where entries denote the probability of obtaining outcome x

j
 or less, j = 1,...,n. Note that we have dropped outcomes from the (de)cumulative probability notation for prospects to further simplify the exposition. Recall, that under expected utility (EU) prospects are evaluated by
 with a utility function, U, which assigns to each outcome a real number and is monotone (that is, U agrees with the preference ordering over outcomes). Utility is cardinal, i.e., it is unique up to scale and location. A more general model is rank-dependent utility (RDU) where a prospect P = (p
1,...,p
n − 1,1) is evaluated by
 Utility is similar to EU, however, RDU involves a probability weighting function, w, which is uniquely determined. Formally, a weighting function, w, is a mapping from the probability interval [0,1] into [0,1] that is strictly increasing with w(0) = 0 and w(1) = 1. In this paper the axiomatically derived weighting functions are continuous on [0,1]. There is, however, empirical and theoretical interest in discontinuous weighting functions at 0 and at 1 (Kahneman and Tversky 1979; Birnbaum and Stegner 1981; Bell 1985; Cohen 1992; Chateauneuf et al. 2007; Webb and Zank 2008). In the next section, we also use linear but discontinuous weighting functions for illustrative purposes. A weighting function, w, is convex if for all probabilities p,q,r such that p + q + r ≤ 1 we have w(q + p) − w(q) ≤ w(q + r + p) − w(q + r). The weighting function is concave (linear) if for all probabilities p,q,r such that p + q + r ≤ 1 we have w(q + p) − w(q) ≥ w(q + r + p) − w(q + r) ( w(q + p) − w(q) = w(q + r + p) − w(q + r)). Note that in all cases monotonicity and the respective curvature imply that the weighting function is continuous on ]0,1[. If w is strictly convex then also continuity at 0 follows, and if w is strictly concave then also continuity at 1 is implied (see also Schmidt and Zank 2008). In this paper there is specific interest in weighting functions that are initially concave, say for probabilities in an interval [0,δ] for 0 < δ < 1, and convex for remaining probabilities, thus, on [δ,1]. We call these functions inverse-S shaped weighting functions, reflecting the shape of the corresponding mapping. Related to the curvature of weighting functions is the notion of probabilistic risk behavior (see Wakker 1994, 2001, 2009; Abdellaoui 2002; Zank 2008). A convex weighting function characterizes probabilistic risk aversion (or pessimism) and a concave weighting function characterizes probabilistic risk proneness (or optimism). A linear weighting function characterizes probabilistic risk neutrality. Observe that, as EU is characterized by probabilistic risk neutrality, optimism and pessimism are measuring probabilistic attitudes relative to EU as benchmark. This is similar to the way concave or convex utility measures corresponding risk attitudes towards (monetary) outcomes with the dual theory (Yaari 1987, so RDU with linear utility) as a benchmark. Figure 1 depicts examples of continuous weighting functions of the form w(p) = p
γ corresponding to the previous notions of (a) optimism ( 0 < γ < 1), (b) neutrality (γ = 1), and (c) pessimism (γ > 1 ), respectively.
 a Optimistic, b neutral, c pessimistic probability attitudes Observe, that in Fig. 1 the concave weighting function is never below the linear weighting function and that the convex one is never above it. This holds in general and is natural because optimism, respectively pessimism, is exhibited for all probabilities, thus, as a global property of probabilistic risk attitude. In the literature the previous EU and RDU formulae 1 and 2, respectively, are sometimes displayed using decision weights, hence, a weighted sum over utilities of outcomes of the form:
 with the decision weights, π

j
,j = 1,...,n, explained next. In the case of expected utility, the decision weights are the probabilities of obtaining the respective outcome, i.e., \(\pi _{j}=\tilde{p} _{j},j=1,\ldots ,n\). In the case of RDU, the decision weights are differences in transformed cumulative probabilities as follows: π
1 = w(p
1) and π

j
 = w(p

j
) − w(p
j − 1),j = 2,...,n. One can infer from Fig. 1, that optimism has the implication that the decision weight of the best outcome is larger than the probability of obtaining that outcome, a feature called overweighting (i.e., w(p) > p for all p ∈ ]0,1[). In the case of pessimism we have underweighting (i.e., w(p) < p for all p ∈ ]0,1[) of the probability of the worst outcome, which leads to a decision weight of the best outcome which is larger than the probability of obtaining that outcome. Observe that in general optimism and pessimism do not lead to similar implications for decision weights of intermediate outcomes. This follows from the fact that w(p) equals the decision weight of the best outcome, while for intermediate outcomes the decision weights are not necessarily equal to the transformed probabilities of those outcomes. Overweighting and underweighting of probabilities is intimately related to sensitivity towards small and large probabilities. Recall that probabilistic risk neutrality acts as the benchmark for measuring optimism or pessimism. One can therefore think of a linear weighting function as exhibiting objective sensitivity. A weighting function exhibits (subjectively) increased
sensitivity towards extreme probabilities if w(p)/p > 1 for \(p\in ]0,\varepsilon \lbrack \) and [1 − w(p)]/(1 − p) > 1 for p ∈ ]1 − ε
′,1[ for some ε,ε
′ > 0 arbitrary small. It exhibits reduced sensitivity if w(p)/p < 1 for \(p\in ]0,\varepsilon \lbrack \) and [1 − w(p)]/(1 − p) < 1 for p ∈ ]1 − ε
′,1[ for some ε,ε
′ > 0 arbitrary small. For example the weighting functions proposed by Goldstein and Einhorn (1987), Tversky and Kahneman (1992) and Prelec (1998) exhibit extreme sensitivity in the sense that w(p)/p and [1 − w(p)]/(1 − p) are unbounded for p approaching 0 and 1, respectively (see Zank 2010 for a discussion). The class of weighting functions of Bell (1985), Cohen (1992), and Webb and Zank (2008) too exhibit extreme sensitivity due to discontinuity of those weighting functions at 0 and at 1. It is, however, the combination of increased sensitivity and concavity for small probabilities of a weighting function followed by convexity for moderate and large probabilities which has been most successful in accommodating the empirical findings regarding probabilistic risk attitude (Tversky and Kahneman 1992; Prelec 1998; Abdellaoui 2000; Wakker 2001). We conclude this section by recalling prospect theory. Here, and elsewhere (e.g., Section 5), it will be convenient to use a notation for prospects that mixes cumulative and decumulative probabilities. Under prospect theory outcomes are interpreted as deviations from a reference point. Assuming that for some 1 < k < n the outcome x

k
 is the reference point, we call x
1,...,x
k − 1
gains and x
k + 1,...,x

n

losses. We can then write the prospect P as
  One can think of p
1,...,p

k
 as being cumulated probabilities for gains and of 1 − p

k
,...,1 − p
n − 1 as being cumulated probabilities for losses. Under prospect theory (PT) probability weighting and the distinction into gains and losses is relevant. There is a weighting function for probabilities of gains, w
 + , and a (possibly different) weighting for probabilities of losses, w
 − . A prospect P = (p
1,...,p

k
,1 − p

k
,...,1 − p
n − 1) is evaluated by
  where we use the convention that U(x

k
) = 0. The weighting functions under PT are uniquely determined and the utility function is a ratio scale. Similar to RDU, in prospect theory the decision weights are differences in transformed cumulative probabilities of gains (i.e., \(\pi _{1}=w^{+}(p_{1})\) and \(\pi _{j}=w^{+}(p_{j})-w^{+}(p_{j-1}),j=2,\ldots ,k-1\)), respectively, differences in transformed cumulative probabilities of losses (i.e., \(\pi _{j}=w^{-}(1-p_{j-1})-w^{-}(1-p_{j}),j=k+1,\ldots ,n-1\) and \(\pi _{n}=w^{-}(1-p_{n})\)). Note that PT reduces to RDU if we have duality between the weighting functions, i.e., if w
 + (p) = 1 − w
 − (1 − p) for all \( p\in \lbrack 0,1]\), or if we have only gains (only losses).",53
41.0,1.0,Journal of Risk and Uncertainty,08 June 2010,https://link.springer.com/article/10.1007/s11166-010-9096-7,At the nexus of risk and time preferences: An experimental investigation,August 2010,Keith H. Coble,Jayson L. Lusk,,Male,Male,Unknown,Male,"Kreps-Porteus preferences are capable of representing both risk and time preferences more generally than the previously discussed models. Kreps-Porteus preferences, and other similar variants that have been introduced, specify a recursive utility function that aggregates current (certain) consumption and future (uncertain) consumption. For example, utility in time t can be defined as a function of V, a time aggregator function, conditional upon, x

t
 (income or consumption in time t, the utility of current consumption, u, and μ

t
, which is the expected utility certainty equivalent function.
 As described by Epstein and Zin (1989), V, aggregates the utility of current consumption and future utility to determine current utility. In this specification, the curvature of the function V characterizes preferences for x across time. By contrast, the curvature of the function μ

t
 governs preferences for risk aversion. In commonly used parametric implementations of (1), such as the constant elasticity form, the curvature of the utility function over time is given by a single parameter ρ, and the curvature of the utility function over risky outcomes is given by a single parameter α. When the curvature of the two functions is equivalent, e.g., ρ = α, then the model is equivalent to the conventional additive DEU model. People prefer an early resolution of risk if the V function is less concave than the function μ

t
 (Gollier 2001; Epstein and Zin 1989). The typical approach taken to estimate the degree of curvature of these two functions is to first assume a particular functional form, such as the constant elasticity form, for μ

t
 and V, set-up a Bellman equation based on Eq. 1, and solve for optimal consumption and wealth (e.g., see Epstein and Zin 1991). The first order conditions of the Bellman equation give rise to Euler equations, which along with aggregate data on consumption and asset returns, form the basis of estimation. In this paper, we consider an alternative approach, in which people’s choices between two options are observed. Similar to the approach taken by Hey and Orme (1994), the model in Eq. 1 can be specified as a random utility model as in McFadden (1973) or Thurstone (1927), such that the inter-temporal utility of option j for individual i can be written as:
 Now, assume an individual is presented with a choice between option j and option k, which differ either in terms of the date with which monetary amounts are paid or the riskiness of the payout or both. The probability that option j is chosen is the probability that Z

itj
 > Z

itk
 or that \( {\varepsilon_{itj}} - {\varepsilon_{itk}} > V\left\lfloor {{u_k}\left( {{x_{tk}}} \right),{\mu_{tk}}\left( {{U_{t + 1,k}}} \right)} \right\rfloor - V\left\lfloor {{u_j}\left( {{x_{tj}}} \right),{\mu_{tj}}\left( {{U_{t + 1,j}}} \right)} \right\rfloor \). If the error terms are independently and identically distributed according to the distribution function F, then the probability that option j is preferred to option k is:
 In a sample with N individuals with options j and k taking on various values, the parameters of the utility function can be estimated by maximizing the following likelihood function:
 where y

itj
 = 1 if option j was chosen as most preferable and y

itj
 = 0 if option k was chosen as most preferable.",41
41.0,2.0,Journal of Risk and Uncertainty,28 July 2010,https://link.springer.com/article/10.1007/s11166-010-9102-0,The descriptive and predictive adequacy of theories of decision making under uncertainty/ambiguity,October 2010,John D. Hey,Gianna Lotito,Anna Maffioletti,Male,Female,Female,Mix,,
41.0,2.0,Journal of Risk and Uncertainty,10 July 2010,https://link.springer.com/article/10.1007/s11166-010-9099-4,A simplified axiomatic approach to ambiguity aversion,October 2010,William S. Neilson,,,Male,Unknown,Unknown,Male,"The model adopts the roulette and horse lottery framework of Anscombe and Aumann (1963). The bounded interval X is the payoff space, and Δ(X) is the set of all Borel-measurable probability distributions over X. Members of Δ(X) are also called roulette lotteries. Let S be the set of states of the world, with generic element s. Define Σ to be the set of all subsets of S, with generic element E, which is interpreted as an event. Savage (1954) defines an act as a mapping from S to X, while Anscombe and Aumann define a horse lottery as a mapping from S to Δ(X), that is, a mapping assigning a roulette lottery to every state. The resolution of an act is an outcome in the payoff space, while the resolution of a horse lottery is a roulette lottery, which is a probability distribution over payoffs. Let \(\mathcal{H}\) denote the set of all horse lotteries. The key to this paper is applying the Savage axioms to horse lotteries instead of acts. To do so, assume that the individual has a preference ordering \(\succsim\) defined over \(\mathcal{H}\). In what follows, \( f, f^{\prime}, h, h^{\prime } \in \mathcal{H}\) are horse lotteries, π, π
′, ρ, ρ
′  ∈  Δ(X) are roulette lotteries, and \( E, E^{\prime }, E_{i} \in \Sigma \) are events. Abusing notation when the context is clear, the roulette lottery π is also a degenerate horse lottery assigning the same probability distribution to every state in S, that is, h

s
 = π for all s ∈ S. These degenerate horse lotteries are called constant lotteries. The set E
c is the complement of E in S, that is, S ∖ E. A set E is null if h ~ f whenever h

s
 = f

s
 for all s ∈ E
c, and where ~ is the indifference relation. It is said that h = f
on E if h

s
 = f

s
 for all s ∈ E. It is said that h
\(\succsim f\)
given E if and only if h
′
\(\succsim f^{\prime }\) whenever \( h_{s}=h_{s}^{\prime }\) for s ∈ E, \(f_{s}=f_{s}^{\prime }\) for s ∈ E, and \(h_{s}^{\prime }=f_{s}^{\prime }\) for all s ∈ E
c. We use the following axioms over \(\succsim \). Axioms A1–A7 are the Savage axioms, and axiom A8 and A9 apply Grandmont’s (1972) versions of the von Neumann and Morgenstern continuity and independence axioms to constant horse lotteries, which are simply probability distributions.Footnote 9
 A1     (Ordering) \(\succsim \) is complete and transitive. A2     (Sure-thing principle) If f = f′ and h = h′ on E, and f = h and f′ = h′ on Ec, then f\( \succsim h\) if and only if f′\(\succsim h^{\prime }\). A3     (Eventwise monotonicity) If E is not null and if f = π and h = ρ on E, then f\(\succsim h\) given E if and only if π\( \succsim \rho. \) A4     (Weak comparative probability) Suppose that π\( \succsim \rho \), f = π on E, f = ρ on Ec, h = π on E′, and h = ρ on E′c, and suppose that π′\(\succsim \rho ^{\prime }\), f′ = π′ on E, f′ = ρ′ on Ec, h′ = π′ on E′, and h′ = ρ′ on E′c. Then f\(\succsim h\) if and only if f′\(\succsim h^{\prime }\). A5     (Nondegeneracy) π ≻ ρ for some π, ρ ∈ Δ(X). A6     (Small event continuity) If f ≻ h, for every π ∈ Δ(X) there is a finite partition of S such that for every E
i
 in the partition, if f′ = π on E
i
 and f′ = f on \( E_{i}^{c}\) then f′ ≻ h, and if h′ = π on E
i
 and h′ = h on \(E_{i}^{c}\) then f ≻ h′. A7     (Uniform monotonicity) For all E  ∈  Σ and for all π ∈ h(E), if f\(\succsim \pi \) given E, then f\(\succsim h\) given E. If π\(\succsim f\) given E, then h\(\succsim f\) given E. A8     (Continuity over risk) For every π0 ∈ Δ(X) the sets \(\{\pi \in \Delta (X)|\pi \succsim \pi _{0}\}\) and \(\{\pi \in \Delta (X)|\pi _{0} \succsim \pi \}\) are closed in Δ(X). A9     (Independence over risk) π\(\succsim \pi ^{\prime }\) if and only if aπ + (1 − a)ρ\(\succsim a\pi ^{\prime }+\) (1 − a)ρ for all ρ ∈ Δ(X) and all scalars a  ∈  (0, 1). Axioms A1–A7 are the standard Savage axioms modified so that they govern preferences over horse lotteries instead of preferences over acts. The main difference between these axioms and Savage’s, then, is that here probability distributions in Δ(X) replace outcomes in X. It is worth investigating whether the sure-thing principle (A2) makes the independence axiom (A9) redundant. The independence axiom implies that, for any roulette lotteries π, ρ, σ, τ  ∈  Δ(X) and \( a\in \lbrack 0, 1]\), if \(a\pi + (1 - a)\sigma \succsim a\rho + (1 - a)\sigma \) then \(a\pi + (1 - a)\tau \succsim a\rho + (1 - a)\tau \). These are probability mixtures, and therefore aπ  +  (1 − a)σ and the other three mixtures are all elements of Δ(X). One can obtain almost the same construction using a sure-thing principle framework, where E, E
c  ∈  Σ are events. Construct horse lotteries \(f, f^{\prime }, h, h^{\prime }\in \mathcal{H}\) according to the following table:
 The sure-thing principle states that f
\(\succsim h\) if and only if f
′
\(\succsim h^{\prime}\), that is, preferences only depend on states in which the two horse lotteries being considered have different outcomes. If the individual assigns subjective probability a to event E , the horse lottery f would generate a subjective probability distribution identical to the probability mixture aπ  +  (1 − a)σ. However, the probability in the horse lottery f is subjective, and so the mixture entailed in f is not a true probability mixture, and therefore not in Δ(X), so f and aπ  +  (1 − a)σ are really two different objects. Furthermore, there is no way to use a horse lottery to build a probability mixture from its component parts, and therefore the horse lottery framework of axiom A2 cannot duplicate the roulette lottery framework of axiom A9. Both are needed. Preferences satisfy A1–A9 if and only if there exists a unique, convex-ranged probability measure\(\mu :\Sigma \rightarrow \lbrack 0, 1]\), a bounded function u : X→ℝ, and a monotone increasing, bounded function w : u(X)→ℝ such that for all\(f, h \in \mathcal{H}\), h\(\succsim f\)if and only if Moreover, the function u is unique up to increasing affine transformations, and for a given specification of u the function w is unique up to increasing affine transformations over the domain u(X). Proof of the “if” direction is standard. For the “only if” direction, by the Savage axioms A1–A7, there exists a unique, convex-rangedFootnote 10 probability measure \(\mu : \Sigma \rightarrow \lbrack 0, 1]\), and a bounded function v : Δ(X)→ℝ such that the preference ordering \(\succsim \) is represented by the functional
 Furthermore, μ is unique and v is unique up to increasing affine transformations over its relevant domain. By axioms A8 and A9, \(\succsim \) restricted to constant horse lotteries can be represented by
 where u is bounded and unique up to increasing affine transformations. Axiom A3 implies that V(π) and v(π) must represent the same preferences over roulette lotteries, so V is bounded and there exists a monotone function w : ℝ → ℝ such that
 Because V is bounded so is w. Since v is unique up to increasing affine transformations over the relevant domain, so is w for a given specification of u, and the relevant domain is u(X). Substituting Eq. 6 into Eq. 7 and then Eq. 7 into Eq. 5 yields
  □ It is worth pointing out why the axioms do not get us all the way to subjective expected utility (and ambiguity neutrality). The key is that axioms A1–A7 only link the event space Σ to the roulette lottery space Δ(X), and not all the way to the payoff space X. The independence axiom A9 places structure on the link between the roulette lottery space and the payoff space, but not enough to provide that missing link. Consequently, risk attitudes and ambiguity attitudes remain separated.",64
41.0,2.0,Journal of Risk and Uncertainty,14 July 2010,https://link.springer.com/article/10.1007/s11166-010-9100-2,The hyperbolic factor: A measure of time inconsistency,October 2010,Kirsten I. M. Rohde,,,Female,Unknown,Unknown,Female,"Let \(\mathcal{X} = \mathbb{R}^m\) be a set of outcomes
Footnote 1 and \(\mathcal{T} = \mathbb{R}_+\) a set of time-points. A timed outcome (t, μ) yields outcome μ at time t and nothing (= 0) at all other points in time, where t = 0 corresponds to ‘today.’ We examine preferences \(\succcurlyeq\) over timed outcomes. The relations \(\preccurlyeq, \succ, \prec, \sim\) are as usual. Preferences over outcomes are derived from preferences over timed outcomes consumed today, i.e. \(\chi\succcurlyeq \mu\) if and only if \( (0,\chi)\succcurlyeq (0,\mu).\) We use μ as the greek analogue of ‘m’ to denote moderate outcomes and χ as a greek analogue (visually) of ‘x’ to denote extreme outcomes. Thus, χ will denote the larger gain or the larger, or more severe, loss. We assume that \(\succcurlyeq\) is a weak order, i.e \(\succcurlyeq\) is complete (\((s, \mu) \succcurlyeq (t, \chi)\) or \((t, \chi) \succcurlyeq (s, \mu)\) for all \(\mu, \chi \in \mathcal{X}\) and \(s, t \in \mathcal{T},\) possibly both) and transitive. Preferences are monotonic if \(\chi \succcurlyeq \mu\) implies \((t, \chi) \succcurlyeq (t, \mu)\) for every \(t \in \mathcal{T},\) and χ ≻ μ implies (t, χ) ≻ (t, μ) for every \(t \in \mathcal{T}.\) Preferences are impatient if for every s < t, χ ≻ 0 implies (s, χ) ≻ (t, χ) and χ ≺ 0 implies (s, χ) ≺ (t, χ). Preferences are continuous if for every (t, χ) the sets \(\left\{(s, \mu) \in \mathcal{T} \times \mathcal{X} \; | \; (s, \mu) \succcurlyeq (t, \chi) \right\}\) and \(\left\{(s, \mu) \in \mathcal{T} \times \mathcal{X} \; | \; (s, \mu) \preccurlyeq (t, \chi) \right\}\) are closed. Throughout this paper we make the following assumption. 
Preferences constitute a continuous, monotonic, and impatient weak order.
 Consider two equivalent timed outcomes (s, μ) ∼ (t, χ), with s < t and \(\mu \nsim \chi\). Then we have either χ ≻ μ ≻ 0 or χ ≺ μ ≺ 0 (μ is ‘moderate’ and χ is ‘extreme’). If the outcome μ is delayed by time τ, then stationarity implies that the outcome χ should also be delayed by τ in order to maintain indifference. Thus, under stationarity (s, μ) ∼ (t, χ) implies (s + τ, μ) ∼ (t + τ, χ). Stationarity reflects constant impatience. The preference relation \(\succcurlyeq\) exhibits decreasing impatience if for all s < t, \(\tau\in \mathcal{T},\)
(i)
χ ≻ μ ≻ 0 and (s, μ) ∼ (t, χ) imply \( (t + \tau, \chi)\succcurlyeq(s + \tau, \mu),\) and (ii)
χ ≺ μ ≺ 0 and (s, μ) ∼ (t, χ) imply \( (t + \tau, \chi)\preccurlyeq (s + \tau, \mu)\). Increasing impatience holds if the implied preferences are always the reverse. Thus, with decreasing impatience, when we consider two equivalent timed outcomes, then delaying both outcomes equally will result in less distinction between the time-points, and, thus, more preference for the timed outcome with the preferred outcome. In this sense, decreasing impatience reflects that a time difference becomes decreasingly important as it lies farther in the future. Assume another preference relation \(\succcurlyeq^*,\) which also is a continuous, monotonic, and impatient weak order. Preferences \(\succcurlyeq^*\) exhibit more decreasing impatience than
\(\succcurlyeq\) if for all s < t, \(\tau, \sigma \in \mathcal{T}\)
(i)
χ
* ≻ *
μ
* ≻ * 0, (s, μ) ∼ (t, χ), (s + σ, μ) ∼ (t + τ, χ), and (s, μ
*) ∼* (t, χ
*) imply \( (t + \tau, \chi^*)\succcurlyeq^*(s + \sigma, \mu^*),\) and (ii)
χ
* ≺ *
μ
* ≺ * 0, (s, μ) ∼ (t, χ), (s + σ, μ) ∼ (t + τ, χ), and (s, μ
*) ∼* (t, χ
*) imply \( (t + \tau, \chi^*)\preccurlyeq^*(s + \sigma, \mu^*)\) (Prelec 2004). Consider again two equivalent timed outcomes (s, μ) ∼ (t, χ) with s < t. Assume that (s + σ, μ) ∼ (t + τ, χ). Decreasing (increasing) impatience implies that τ − σ > 0 (τ − σ < 0). An obvious measure of decreasing impatience is, therefore, τ − σ. This measure τ − σ, however, will depend on s, t, σ, μ, and χ, and will be hard to compare across different outcomes and time-points. The main purpose of this paper is to propose a transformation of this measure that is better suited as a measure of decreasing impatience, and that can be compared across different outcomes and time-points. This proposed measure, the hyperbolic factor, is defined next. It is just as easily observable from preferences as τ − σ itself. Unlike τ − σ, however, it will be constant, i.e. independent of s, t, σ, μ and χ, for all hyperbolic discounting models currently used in the literature, as we will see in Section 2. Outcomes \(\mu, \chi \in \mathcal{X}\) and time-points \(s, t, \sigma, \tau \in \mathcal{T},\) with s < t, τ > 0, form an indifference pair if
 In these indifferences, waiting t instead of s offsets the same monetary advantage as waiting t + τ instead of s + σ. Such a use of pairs of indifferences (or preferences) to compare tradeoffs across attributes has been widely used in decision under uncertainty. Examples include Abdellaoui (2002, p. 726, Definition 6), Blavatskyy (2006, p. 320, TO method), Bleichrodt and Miyamoto (2003, p. 183, tradeoff consistency), Bouyssou and Pirlot (2003, p. 685), Chateauneuf (1999, p. 25, C.S.T.P.), Karni (2003), Schmidt and Zank (2001, p. 486, EL-tradeoff consistency), Skiadas (1997, p. 257, Axiom A10), and Wakker and Deneffe (1996, p. 1134, TO method). Applications to interpersonal comparisons are in Ebert (2004, p. 421, independence) and Pinto (1997, p. 73, PTO-3), and applications to inter-temporal choice, the topic of this paper, include Prelec (1998, p. 503, compound invariance). For every indifference pair as in Eq. 1 the hyperbolic factor is defined as
 Thus, in order to obtain the hyperbolic factor, we only need an indifference pair. One recipe to obtain an indifference pair is as given in the following theorem. 
An indifference pair can be constructed as follows.
 Take any\(\chi \nsim 0\) and any s, t, τ with s < t,  and τ > 0; Find μ such that (s, μ) ∼ (t, χ); Find σ such that (s + σ, μ) ∼ (t + τ, χ). In the second step μ exists, but may not be unique: any μ′ with μ′ ∼ μ will yield the same σ in the third step. In the third step σ exists and is unique. For general preferences, Steps II and III may not always be solvable. For instance, there may not exist μ and σ that satisfy Eq. 1. Our assumptions about preferences, however, imply that such a case can never arise, so that a μ and σ as described can always be found. In Attema et al. (2010) we did an experiment where we used a slightly different procedure to obtain indifference pairs. We fixed outcomes μ, χ, and time point s. Then we elicited a t such that (s, μ) ∼ (t, χ). Finally, we elicited a τ such that (t, μ) ∼ (t + τ, χ), thereby letting s + σ = t. A drawback of the method in that experiment is that in theory it can be the case that t or τ cannot be found. Nevertheless, in the experiment such a case did not occur. Now we can define the function H for every \(\chi \nsim 0,\)
s < t, and τ > 0, as
 where σ is such that together with a μ and the arguments of H, it yields an indifference pair as in Eq. 1.Footnote 2 The function H gives the hyperbolic factors. In general, H need not always be regular, i.e. H is infinite if tσ = sτ, and negative in spite of strongly decreasing impatience if tσ < sτ. Yet, as we will see later, for the most popular discounted utility models in the literature, regularity holds, i.e. for every indifference pair as in Eq. 1 we have t
σ > s
τ. Imposing regularity amounts to imposing an upper bound on the degree of decreasing impatience, as we will show next. Consider an indifference pair with corresponding s, t, σ, τ. We saw before that τ − σ can be viewed as a measure of decreasing impatience. From impatience we know that s − t < τ − σ < τ. For increasing impatience we have τ − σ < 0, and therefore also t
σ > s
τ. Thus, regularity does not restrict the degree of increasing impatience. Now say that moderate decreasing impatience holds if 0 < τ − σ < τ(t − s)/t and that strongly decreasing impatience holds if τ − σ ≥ τ(t − s)/t > 0. Then imposing regularity amounts to ruling out strongly decreasing impatience. Let us now summarize the main properties of the hyperbolic factor. It can easily be shown that non-negative hyperbolic factors correspond to decreasing impatience. We will see in Section 2 that hyperbolic discounting induces non-negative hyperbolic factors, and, thus, decreasing impatience. Let regularity hold. Preferences\(\succcurlyeq\) exhibit decreasing impatience if and only if H ≥ 0.  Preferences \(\succcurlyeq\) exhibit increasing impatience if and only if H ≤ 0. Note that a similar result would hold for any monotonic transformation of H. The hyperbolic factor indeed serves as a measure of decreasing impatience, as shown in the next theorem. Thus, it properly captures Prelec’s (2004) relative decreasing impatience. When we consider another preference relation \(\succcurlyeq^*,\) then it is assumed that \(\succcurlyeq^*\) is a continuous, monotonic and impatient weak order, and that the corresponding hyperbolic factors are given by H
*(s, t, χ, τ). Let regularity hold. Preferences \(\succcurlyeq^*\) exhibit more decreasing impatience than \(\succcurlyeq\) if and only if H*(s, t, χ*, τ) ≥ H(s, t, χ, τ) for all s, t, τ, χ, χ*. Thus, we have shown that the hyperbolic factor is an appropriate model-free measure of decreasing impatience that can easily be obtained from an indifference pair. To illustrate the measurement of the hyperbolic factor when discounted utility does not hold, consider the following example. Assume that preferences over timed outcomes are represented by V(t, x) = e
 − r(x) t
x, where r(x) is a positive function. We will go through the three steps of Theorem 2 to obtain the hyperbolic factor.
 Fix x, t, τ with t, τ > 0 and x ≻ 0. Determine y such that (0, y) ∼ (t, x). It follows that y = e
 − r(x) t
x. Determine σ such that (σ, y) ∼ (t + τ, x). It follows that
 Substituting y = e
 − r(x) t
x yields
 which yields
 Thus,
 The hyperbolic factor is then given by
 Note that if r(·) is a constant function, we have the common constant discounting. In that case the hyperbolic factor will be zero, as we will also show in the next section.",32
41.0,2.0,Journal of Risk and Uncertainty,28 July 2010,https://link.springer.com/article/10.1007/s11166-010-9101-1,The attraction of uncertainty: Interactions between skill and levels of uncertainty in market-entry games,October 2010,Natalia Karelaia,Robin M. Hogarth,,Female,,Unknown,Mix,,
41.0,3.0,Journal of Risk and Uncertainty,06 October 2010,https://link.springer.com/article/10.1007/s11166-010-9105-x,"Risk aversion and physical prowess: Prediction, choice and bias",December 2010,Sheryl Ball,Catherine C. Eckel,Maria Heracleous,Female,Female,Female,Female,"Economists tend to assume that individuals have a single domain-general risk parameter. If true, measuring risk attitudes is a simple matter of structuring a task or decision that measures the relevant parameter—in this case, the curvature of the utility function. This parameter then should apply to all risky domains, including physical and financial risk.Footnote 4 Psychologists tend to view risk-taking as an element of personality, which also implies a degree of preference stability across domains (Weber et al. 2002; Zuckerman 1994; Zuckerman and Kuhlman 2000). While it is apparent that physical prowess should be related to a person’s willingness to take physical risks, the corresponding relationship between physical prowess and financial risk is less obvious. Two separate theoretical literatures suggest a direct connection between physical prowess and risk attitudes: evolutionary biology and risk-portfolio (or background risk) analysis. Biologists view preferences as no less the result of natural selection than physical traits: Whether physical or psychological, traits that are helpful for survival are passed on and become more prevalent in subsequent populations. Robson (2001) reviews a growing literature on the biological basis for economic behavior. If risk-taking enhances survival, then the equipment to facilitate risk taking—physical prowess and a taste for risk—should co-evolve. A look back at our human ancestors suggests a risk taking species. Humans migrated from Africa to points all around the world quickly. These movements resulted in dramatic habitat changes. A decision to migrate necessitated a move from a place of known safety and resources to one of unknown levels, with attendant risks. Gender-related evolutionary selection pressures also may have shaped the relationship between physical prowess and a taste for risk. Males who are more successful at hunting and defending against predators, skills that depend both on physical prowess and risk taking behavior, are more genetically “fit.” Sexual selection theory suggests that females who prefer such males will produce more offspring than those who have the opposite tastes. This implies that women’s taste for successful males will be more common in the population. Dekel and Scotchmer (1999) show that males develop risk-taking preferences, entangling gender, physical prowess and risk preferences. Age may play a confounding role. Rubin and Paul (1979) model males’ risk preferences over their lifecycle and find that it is optimal for older males to be more risk averse than younger males, producing a relationship between age and risk preferences. This illustrates the importance of measuring risk preferences and physical prowess together with gender, age and other factors that might impact choices. A second mechanism that suggests a relationship between risk attitudes and physical prowess views individuals as holding a portfolio of risks, both physical and financial. Someone who faces high risks in one aspect of their lives may compensate by making more risk averse choices in others. Under certain conditions on preferences, higher “background risk” in other arenas makes individuals behave in a more risk-averse manner over available financial decisions (Pratt and Zeckhauser 1987; Eeckhoudt et al. 1996). Eeckhoudt and Kimball (1992) show that individuals with high labor income risk will hold fewer risky assets and buy more insurance. Jianakoplos and Bernasek (1998) analyze survey results that suggest that the investment portfolios of single women contain fewer risky assets than the portfolios of single men, perhaps reflecting greater employment risk for women. Married couples, whose pooled income risk is lower, hold more risky assets than singles of either gender. Sunden and Surette (1998) confirm that the gender/marriage interaction determines risk aversion. A person with high physical prowess lives, in a sense, in a safer physical world, so a choice that might be very risky for a weaker individual is not as risky for the stronger person. For a given pattern of choices in other arenas, physical prowess may make a person more able to tolerate high risk in other areas, such as the financial arena. Thus the tradeoff between physical and financial risks implied by this analysis is likely to be different for the physically strong, with stronger persons taking on greater financial risks for a given set of non-financial choices. The purpose of the current research is to explore the relationship between choices over monetary gambles and physical prowess, measuring and controlling for other factors that might affect this relationship. Physical prowess, then, might affect an individual’s decision when choosing among a series of gambles. As described below, gambles are numbered so that the least risky gamble has the lowest number designation, the second most risky the next lowest designation, and so on. An individual i’s gamble choice, Ci is
 Where Gi is i’s gender and Si is a vector including a number of measures of physical prowess, and Ti is a vector of measures of an individual’s attitudes about their own physical strength. For example, Si may include direct measures of physical strength such as how much weight an individual can lift or how fast they run, and measures of size. Related factors captured in Ti include whether an individual participates actively in competitive sports, and beliefs about his own prowess. In these experiments subjects are compensated for being able to correctly predict the gamble choices of others. As established in previous work, these predictions are empirically related to the forecaster’s own gamble choice as well as the gender of their counterpart. We also investigate whether the forecaster’s evaluation of the counterpart’s physical prowess affects their forecast. An individual i’s gamble prediction for j, Pij is
 Where Sij is a vector that captures i’s assessment of j’s physical prowess on a variety of observable dimensions. The experiment below is designed to allow us to estimate these models of gamble choices and predictions.",41
41.0,3.0,Journal of Risk and Uncertainty,23 September 2010,https://link.springer.com/article/10.1007/s11166-010-9104-y,How would you like your gain in life expectancy to be provided? An experimental approach,December 2010,Jytte Seested Nielsen,Susan Chilton,Hugh Metcalf,Female,Female,Male,Mix,,
41.0,3.0,Journal of Risk and Uncertainty,22 September 2010,https://link.springer.com/article/10.1007/s11166-010-9103-z,Eliciting risk preferences: When is simple better?,December 2010,Chetan Dave,Catherine C. Eckel,Christian Rojas,Unknown,Female,Male,Mix,,
41.0,3.0,Journal of Risk and Uncertainty,13 October 2010,https://link.springer.com/article/10.1007/s11166-010-9106-9,Demand for health risk reductions: A cross-national comparison between the U.S. and Canada,December 2010,Trudy Ann Cameron,J. R. DeShazo,Peter Stiffler,Female,Unknown,Male,Mix,,
42.0,1.0,Journal of Risk and Uncertainty,09 December 2010,https://link.springer.com/article/10.1007/s11166-010-9109-6,Digit ratios (2D:4D) as predictors of risky decision making for both sexes,February 2011,Ellen Garbarino,Robert Slonim,Justin Sydnor,Female,Male,Male,Mix,,
42.0,1.0,Journal of Risk and Uncertainty,23 December 2010,https://link.springer.com/article/10.1007/s11166-010-9112-y,Ambiguity aversion and the propensities for self-insurance and self-protection,February 2011,Arthur Snow,,,Male,Unknown,Unknown,Male,"Consider an individual endowed with initial wealth W who faces the risk of losing an amount L∈(0,W). The individual’s risk preferences are captured by the utility function U defined on final wealth. The objective probability of incurring the loss is denoted by p. In the absence of ambiguity the decision maker knows the value of p, but is uncertain about its value when ambiguity is present. Uncertainty about the objective probability is represented by the second-order probability distribution F(π), where π denotes a possible value of p.Footnote 6 For each value of π, the individual’s expected utility is evaluated by an increasing function φ that captures ambiguity preferences, and the individual’s decision criterion is the expected value of φ given beliefs F(π). Thus, the individual’s endowed utility is given by
 where E

F
[·] denotes the expectation with respect to F(π). The individual is risk averse under the assumption that U is concave, and is ambiguity averse under the assumption that φ is concave.Footnote 7 However, when the decision maker is ambiguity neutral, φ is linear and decision criterion (1) reduces to the Savage subjective expected utility criterionFootnote 8
 Alternatively, in the absence of ambiguity, the probability of loss is known to the decision maker and criterion (1) reduces to the von Neumann-Morgenstern expected utility criterion
 adopted in previous studies of the propensities for self-insurance and self-protection.Footnote 9
 The guiding principle underlying the results obtained in the present study of these propensities is that the behavior of an ambiguity-neutral decision maker is unaffected by the introduction of ambiguity into the choice setting. Accepting this principle as a logical consequence of ambiguity neutrality, decision criteria (2) and (3) must be identical, which requires that beliefs about the risk of loss be unbiased, so thatFootnote 10
 It follows that the introduction of ambiguity constitutes a mean-preserving spread in the individual’s beliefs about p and, in order to maintain the unbiasedness condition (4), an increase in ambiguity must be the result of a mean-preserving spread of the second-order probability distribution F(π).",75
42.0,1.0,Journal of Risk and Uncertainty,01 December 2010,https://link.springer.com/article/10.1007/s11166-010-9107-8,A Diamond-Stiglitz approach to the demand for self-protection,February 2011,Donald J. Meyer,Jack Meyer,,Male,Male,Unknown,Male,"The term self-protection, also referred to as prevention, was introduced into the economics literature by Ehrlich and Becker (1972), and was used to describe a costly action that reduces the likelihood of losses and increases the likelihood that no loss occurs. The cost of this action is that the size of all outcomes is reduced by a fixed amount, the cost of self-protection. This of course implies that an increase in self-protection makes the lowest outcomes even lower, and even though these low outcomes are less likely to occur, those decision makers with very high levels of risk aversion will not choose such increases in self-protection. As many have recognized, these properties of an act of self-protection imply that an increase in self-protection is not a Rothschild and Stiglitz (1970) (R-S) decrease in risk, nor is it a R-S increase in risk. As a result, the demand for self-protection does not increase with increased risk aversion as might be expected. Ehrlich and Becker indicate that “the incentive to self-protect, unlike the incentive to self-insure, is not so dependent on attitudes toward risk, and could be as strong for risk preferrers as for risk avoiders.” Many others have since verified this lack of a clear effect of increased risk aversion on the optimal level of self-protection. Before discussing the comparative static results developed since the work of Ehrlich and Becker, the standard model of the demand for self-protection is described. The notation of Eeckhoudt and Gollier (2005) is used to present the standard model of the decision to self-protect when there is a Bernoulli loss distribution. In their model they assume that a decision maker begins with certain wealth w and that this wealth is subject to loss L > 0. This loss occurs with probability p(e), where e represents the level of self protection chosen by the decision maker, and this magnitude is measured by its cost. The likelihood of the loss, p(e), is assumed to be decreasing in e, and to vary continuously. Let W denote final wealth. Under the Bernoulli assumption, W can take on one of two values, either (w−e) or (w−L−e), and thus the final outcome variable W is also Bernoulli distributed. Figure 1a gives the graph of the CDF for W at two different levels of self protection e1 = 0 and e2 > 0. Figure 1b represents the difference between the CDFs for W at e1 and at e0. That is, Figure 1b represents the change in the CDF for W that occurs as a result of an increase in self-protection.
 Wealth CDF change Expected utility to the decision maker with utility function u(W) is given by:
 V(e) is assumed to be concave in e, and the decision to self protect is analyzed using standard calculus based comparative static analysis techniques. Because the choice variable e impacts expected utility both by changing the probabilities of the two outcomes and changing each of their magnitudes, the first order condition for this maximization is far from simple. As indicated by Eeckhoudt and Gollier, the first order condition defining the expected utility maximizing value for e is:
 For such a simple model, this is a complicated first order expression involving many terms and is cumbersome when doing comparative statics. Often the signs of the various terms are opposite of one another and this makes demonstrating comparative static theorems difficult. Even representing how the solution to this equation would change if the utility function were more risk averse is not easy because of the presence of both utility and marginal utility terms in the first order expression. Dionne and Eeckhoudt (1985) are among the first to formally analyze this standard model of the decision to self-protect, and their main finding is to confirm and verify the claim of Ehrlich and Becker that increased risk aversion does not necessarily lead to an increase in the optimal level of self-protection. Dionne and Eeckhoudt provide several examples indicating that increased risk aversion can lead to more, or to less, self-protection for reasonable utility functions and risk preferences. Briys and Schlesinger (1990) extend these findings to models with state dependent utility and background risk. They also are the first to point out that when an increase in self-protection is mean preserving, this increase leads to a R-S increase in risk for the left portion of the support of the outcome variable, and a R-S decrease in risk for the right portion of this support. Briys and Schlesinger also note that increases in self-protection, when both mean and variance preserving, are downside increases in risk according to the definition of Menezes et al. (1980). This early research, which indicates that increased risk aversion does not necessarily imply a desire for increased self-protection, was followed by four papers that explore conditions under which increased self-protection is desired. Jullien et al. (1999) show that a more risk averse person chooses a higher level of self-protection when the probability of loss, p(e), is sufficiently small; that is, below a threshold level. This threshold level depends on the risk preferences of the decision makers under consideration. Chiu (2000) supports this finding, also demonstrating that more risk averse persons choose more self-protection when the probability of loss is small enough. In Chiu’s work, the threshold level for p(e) is related to the sizes of the measures of downside risk aversion and risk aversion of the decision maker. In 2005, both Eeckhoudt and Gollier (2005) and Chiu (2005) shift the focus from risk aversion to prudence. Chiu shows that when the increase in self-protection is mean preserving, persons who are more prudent relative to their own risk aversion level choose less self-protection. He goes on to indicate that the mean preserving assumption is a special and strong one, and one which is not typically met for many acts of self-protection. Gollier and Eeckhoudt also only consider mean preserving changes in self protection. Because our findings are patterned after those of Gollier and Eeckhoudt, further review and discussion of their results is deferred to the next section.",17
42.0,1.0,Journal of Risk and Uncertainty,22 December 2010,https://link.springer.com/article/10.1007/s11166-010-9110-0,Monetary incentives in the loss domain and behavior toward risk: An experimental comparison of three reward schemes including real losses,February 2011,Nathalie Etchart-Vincent,Olivier l’Haridon,,Female,Male,Unknown,Mix,,
42.0,2.0,Journal of Risk and Uncertainty,26 February 2011,https://link.springer.com/article/10.1007/s11166-011-9114-4,Dynamic decision making: what do people do?,April 2011,John D. Hey,Luca Panaccione,,Male,Male,Unknown,Male,"In this section, we briefly relate our approach and results to those available in the existing literature. As mentioned in the previous paragraph, we are interested in identifying different types of behaviour in dynamic problems under risk. In this sense, our approach is different from the idea of testing theories of choice by investigating which of the principles they are based on survive the experimental evidence. This approach is chosen in particular by Cubitt et al. (1998), who test which principles of dynamic choice are involved in common-ratio type violations of the Independence Axiom. Indeed, their strategy is based on the observation that, since the Independence Axiom can be shown to follow from specific principles of dynamic choice,Footnote 4 when it is violated, at least one of those principles must be violated as well. Following this approach, they set-up a between-subject experimental design and find evidence of failure of the time independence principle, and therefore those theories of choice based which are based on it.Footnote 5
,
Footnote 6 A similar approach is followed by Cubitt and Sugden (2001), who are however particularly interested in controlling for the role affective experiences have in dynamic choice under risk. They propose evidence that the time independence and the separability principles are jointly rejected by the data.Footnote 7 Finally, a related paper by Busemeyer et al. (2000) uses a within-subject design to investigate violations of a set of consistency principles in a dynamic choice problem. They find robust evidence of violation of dynamic and strategic consistency but not of consequential consistency.Footnote 8
 In contrast with the previous analyses, Hey and Paradiso (2006) focus on how preferences, and not behaviour, differ for decision trees that are strategically identical. By appropriately adapting some of the choice problems proposed by Cubitt et al. (1998), they use evaluations of different trees to test whether individuals use the strategy method or the backward induction method when tackling dynamic decision problems. The authors find evidence not only of dynamic inconsistency, as Cubitt et al. (1998) do, but more importantly they find evidence that subjects value more those choice problems where pre-commitment is available.Footnote 9 Expanding on these findings, Hey and Lotito (2009) propose an experiment where both behaviour and preferences are investigated. They use data on tree evaluations together with data on choices and find evidence that the strategy method, as opposed to that of backward induction, is followed by the majority of subjects. Our work improves substantially upon this strand of literature, since it uses an homogeneous and more informative type of data, namely choices which are expressed as a continuous variable. Moreover, it adds a new type of behaviour, that of myopia, which has been until now neglected in the literature. Issues of dynamic inconsistency have been intensively investigated also in a related branch of literature interested in studying the consequences of abandoning the hypothesis of exponential discounting because of its failure to match empirical evidence.Footnote 10 Indeed, in those theories that explain observed behavioural anomalies in dynamic choice by assuming a rate of time preference that declines over time, dynamic inconsistency naturally emerges.Footnote 11 Therefore, it is possible to find in that literature analysis of behaviour that refer to some of the categories we have described in the previous paragraph, in particular the sophisticated and the naïve behaviour.Footnote 12 Within this branch of literature, the empirical studies are mainly focussed either on the estimation of the discount function or on the identification of behavioural strategies that can reveal either sophistication or naïvety.Footnote 13 Therefore, the approach we follow is starkly different. Not only do we focus on the case of non-Expected Utility, but more importantly we identify a set of types of behaviour that are observationally different and we test which one fits the best the experimental data we have gathered.",21
42.0,2.0,Journal of Risk and Uncertainty,05 January 2011,https://link.springer.com/article/10.1007/s11166-010-9108-7,"Discount rates, social judgments, individuals’ risk preferences, and uncertainty",April 2011,Louis Kaplow,David Weisbach,,Male,Male,Unknown,Male,"We adopt a number of simplifications that are standard in the literature and largely orthogonal to our main points. Specifically, we suppose that there is only a single individual in each generation—perhaps a representative individual or one of many identical individuals, with a constant population size over time—a problematic restriction that we partially relax in Section 2.Footnote 1 The individual’s utility U is a function (only) of available consumption c in the pertinent time period. In this section, we focus on the problem of choosing among feasible, certain consumption paths over time, c(t), each corresponding to different policies, so as to maximize social welfare. (In Section 2, we examine the choice among uncertain consumption paths.) It is convenient to employ the continuous time representation, in which case the expression for social welfare SW is
 In this expression, δ is taken to be a pure social rate of time preference, one that some analysts argue should equal 0, reflecting a view that all generations should be weighted equally, or some low level that reflects the likelihood that humanity will become extinct. Although there is important debate about the choice of δ, we set the issue to the side. We instead focus on the term W(U(c)), where we often suppress the t, focusing on some arbitrarily chosen generation. In place of expression (1), it is common to find (in addition or instead) the following variant, which then becomes the focus of analysis:
 At this point, a further simplification (also tangential to our purposes) is sometimes introduced, namely that consumption c has a constant growth rate of g (also denoted \( \mathop{c}\limits^{ \cdot } /c \)). An additional common assumption, which we will explore further momentarily, is that the function relating a generation’s level of consumption to its contribution to social welfare has the form \( Z(c) = {{{{c^{{1 - \eta }}}}} \left/ {{1 - \eta }} \right.} \). Given all this, one can show that the social rate at which the stream of consumption should be discounted is the constant rate
 For concrete illustrations of the differing implications, one might roughly follow some of the Stern Review’s (Stern 2007) parameters, η = 1, g = 1.3, and δ = 0.1, yielding ρ = 1.4. By contrast, using Nordhaus’s (2008, p. 61) choices of η = 2 and δ = 1.5 and employing the growth path implicit in his simulation, the result is ρ = 5.5. When discounting over periods of 50 or 100 years (or more), the differences are staggering. Our focus is on how the value of η should be determined and interpreted. As is clear from the foregoing, η is a property of Z(c). But that function is a reduced form. The motivation for such a function, usually unelaborated, is that it is a composite of W(U) and U(c). Accordingly, we wish to know how η relates to the curvature of the underlying welfare and utility functions.Footnote 2 This decomposition is critical even to formulating what questions to ask since the properties of W are a matter of ethical debate whereas those of U are empirical; specifically, the curvature of U depends on preferences concerning decisions under uncertainty. We wish to know whether the choice of η in the literature is a normative question, an empirical question to be resolved by the literature on risk preferences, or some combination. And what combination? To begin, we introduce notation for the Arrow-Pratt relative risk aversion parameter and analogues thereto for each of our three functions of interest:
 Single and double primes denote first and second derivatives with respect to the function’s direct argument; in particular, W′ and W″ denote the corresponding derivatives of W with respect to U (and not with respect to c). The core of our analysis involves taking seriously the notion that the reduced form Z(c) is really the composite function W(U(c)). Specifically, we assume that the widespread use of the reduced-form Z(c) function is implicitly motivated by—and really has to be grounded in—a welfare economic framework under which social welfare depends in some justifiable fashion on individuals’ utilities in each generation that, in turn, in the models at hand, depend on consumption in each generation. To pursue this course, one can take the first and second derivatives of the composite function W(U(c)) with respect to c, which with some rearrangement yields
 Multiplying the second term by U(c)/U(c), letting ε

U
 denote the elasticity of utility with respect to consumption (equivalently, the ratio of the marginal utility of consumption, U′(c), to the average utility of consumption, U(c)/c), and making substitutions using the notation from expressions (4) and (5), this expression can be rewritten as
 Expression (8) indicates how to determine the relevant curvature of the Z(c) function when that function is taken to be a composite of the welfare function W and the utility function U (rather than some arbitrary reduced form that is chosen without regard to the underlying social welfare function and utility function). Of interest is the fact that the curvature of Z is not a simple sum of the curvatures of U and W. The latter is weighted by ε

U
. Note that this factor can be quite low in wealthy societies (as mentioned, it equals the ratio of the marginal to the average utility of consumption), so in that setting, the curvature of U is relatively more (perhaps much more) important (assuming that the curvature of W is not extreme, as with a Rawlsian maximin social welfare function).Footnote 3
 To make the foregoing more concrete, it is useful to consider (as is common in the literature) the case in which the R’s are constant.Footnote 4 Specifically, for U and W, suppose that
 Using these functional forms and continuing to suppose that Z(c) = W(U(c)), we have
 where γ = 1−(1−α)(1−β).Footnote 5
 These expressions for Z(c) are not what one might have expected from typical discussions, where, as mentioned, the standard reduced form is \( Z(c) = {{{{c^{{1 - \eta }}}}} \left/ {{1 - \eta }} \right.} \). There is the additional leading term in (11) and (12). Perhaps more surprising is the manner in which the curvature parameter for the reduced form (γ in our expression 12, η in the standard representation) relates to those for the underlying utility and welfare functions. Pursuing the latter, the familiar expression (3) for the overall social discount rate can be rewritten as follows:
 First, consider the functional relationship depicted in this expression. Taking the plausible case in which α > 1, we see that a higher curvature parameter β for the social welfare function implies a lower rather than a higher discount rate—even though g > 0, so that future generations are richer. Likewise, the sign of the effects for both α and β reverse when the other parameter crosses the value 1.0. Hence, familiar intuitions translating such curvature parameters for the U and W functions into overall curvature (γ in expression 12 or η in expression 3) fail.Footnote 6
 Second, reflecting on the decomposition more broadly, we find (as Section 1.4 elaborates in the climate change context) that some explications of the curvature parameter η are confusing and potentially misleading because of the failure to separate the two conceptually distinct sources of curvature. Determination of the curvature of individuals’ utility functions, α in our constant-relative-risk-aversion special case in expression (9), presents an empirical question that is usually addressed by examining individuals’ choices under uncertainty (Vickrey 1945). Granted, its magnitude is subject to serious dispute. The literature has generated a wide range of estimates, and there are concerns about whether the behavior on which such estimates are based is fully informed and rational. See, for example, Barsky et al. (1997), Campbell (1996), Chetty (2006), Choi and Menezes (1992), Kocherlakota (1996), and Zeckhauser and Viscusi (2008). Nevertheless, the existence of debate about empirical evidence on risk preferences does not justify analysts substituting their own values. By contrast, the curvature of the social welfare function, β in our expression (10), is a normative judgment subject to a quite different sort of analysis. See, for example, the conflicting views represented in Sen and Williams (1982). It is interesting to consider the case of a utilitarian social welfare function, which has received some endorsement, most famously from Harsanyi (1953, 1955), and seems to be accepted by many economists.Footnote 7 (See also our further discussion in Section 2.) In that case, β in expression (10) is zero, and the Z(c) function is coincident with the U(c) function. The difficulties with expression (13) become moot, and the reduced-form parameter η simply equals α. In this case, the question of the proper value for η is entirely an empirical one since, as explained, α is a property of individuals’ utility functions that is ordinarily taken to be revealed by behavior under uncertainty. On the other hand, for a nonutilitarian social welfare function, explicit justification would have to be offered for the parameter β. Relatedly, it is problematic, but common, to advocate the use of particular values for η without regard to specifications of the underlying U and W functions, the values of α and β in our special case. Consider, for example, the belief that η should be taken to equal 2. If it turned out that, empirically, individuals were nearly risk neutral, this would entail the view that a utilitarian social welfare function was insufficiently egalitarian. However, if evidence revealed instead that individuals were quite risk averse, with α > 2, then one would have to switch one’s ethical position to the view that utilitarianism is too egalitarian. Coherent normative principles for assessment cannot be contingent on empirical facts. Accordingly, when performing sensitivity analysis on an empirical parameter, it is expected that changing the parameter will change the results, but it is not appropriate for the social welfare function to be simultaneously changed as different values for the empirical parameter are considered. Put another way, optimal decisions depend on the particular circumstances, but the proper decision-making criterion should not. Third, we turn to an important and neglected question concerning the reduced-form approach: What social welfare function (if any) is implied by this methodology? Specifically, suppose one wishes to generate the standard reduced-form Z(c) function, \( Z(c) = {{{{c^{{1 - \eta }}}}} \left/ {{1 - \eta }} \right.} \), starting from the familiar constant-relative-risk-aversion utility function U(c) in expression (9), where again the curvature parameter η in the reduced-form Z(c) function is taken to equal 1−(1−α)(1−β). We are inquiring into the implied welfare function W(U). One might have thought it would be the function given by expression (10), but it is not. It is straightforward to show that the implied social welfare function is
 Expression (14) works in the mathematical sense just described, and it appears to be similar to expression (10). However, the result is normatively problematic.Footnote 8 Although it may not be immediately apparent, the stated form is not an individualistic social welfare function. To restate a basic point in welfare economics: Under an individualistic social welfare function, social welfare is taken to be a function only of individuals’ utilities and not, importantly, of any aspect of how those utilities were produced. Formally, this means that one can write W(U(x)), where x is a complete description of a state of the world. This functional form contrasts with W(U(x),x), which means that social welfare, in addition to depending on individuals’ utilities, can also depend directly on some trait of the state of the world. That is, some aspect of the world may affect social welfare even if it affects no one’s utility. Or, more relevant for our purposes, it can affect social welfare independently of (or different from) how it affects individuals’ utilities. Consider the manner in which expression (14) deviates from the more familiar individualistic social welfare function. In addition to the appearance of β, which is a parameter of the welfare function and therefore not problematic, there is also α, which of course is a parameter of individuals’ utility functions, which appears other than through the utility function. The direct implication is that different individuals (here, generations) would count more or less depending on the curvature of their utility functions (even if, say, they have the same utility level). This property is inconsistent with the standard normative framework. Indeed, it seems bizarre. It is as if two generations had different tastes, one preferring chocolate and the other vanilla, and both achieved the same utility; perhaps chocolate and vanilla are each produced at the same cost and each generation chooses more of the flavor it prefers. Nevertheless, the social welfare function depends directly on preferences for chocolate versus vanilla and thus weights one of the two generations more than the other on that account. The problem actually is worse, for it can be shown that this feature implies that the social welfare function (14) can favor choices that violate the Pareto principle. To demonstrate this point, suppose that there are two social states, each producing precisely the same utility levels for all individuals (in our setting, for individuals in all generations), but one involves a different value for α. (To give a possible motivation, a different climate path may affect how consumption is transformed into individual utility, but there might also be a different consumption path as well, one that produces an offsetting effect on utility such that the utility level in each generation is the same.) Under the social welfare function (14), these identical utility profiles, produced by two different regimes, will have different values for social welfare because the α’s in the leading term will differ. This difference implies that, starting with the regime yielding lower social welfare, we might imagine that there exists a small policy adjustment that would raise utility in each generation slightly. This modified regime would still have lower social welfare according to expression (14)—assuming that the change is sufficiently small—even though it yields higher utility for all generations. Hence, the social welfare function (14) sometimes instructs society to choose policies that reduce everyone’s well-being.Footnote 9
 The analysis thus far casts discussion of the social discount rate in economic policy analysis in a different light. This claim can be illustrated by work on climate change. We begin with Dasgupta’s (2008) extensive treatment in this Journal and follow with Arrow et al. (1996) and Stern (2007, 2008). As will be seen, the problems we identify are not particular to certain authors or papers but are widespread and have a common pattern. Dasgupta’s framework is fairly typical of much of the literature, which is not surprising since the article is an interpretative survey designed to have this feature. Converting his main expression for social welfare (Dasgupta 2008, expression 2) to the continuous time analogue, he has
 On its face, it is hard to know whether this welfare function is comparable to expression (1) or to expression (2) above. It is equivalent to both if the social welfare function is utilitarian. As mentioned, in that case W(U) = U, and Z(c) = W(U(c)) = U(c). However, Dasgupta is explicit in not limiting himself to the (formally) utilitarian case (e.g., allowing that “U [may be] not felicity, but an increasing, concave function of felicity” [p. 147]).Footnote 10 And, as will be discussed, he clearly envisions that the curvature is to be chosen by a social observer rather than determined empirically.Footnote 11 In short, the U function in Dasgupta is emphatically not a utility function—making his choice of notation unfortunate. Similarly, he regularly uses the term “felicity” (with no adjective “social”) and occasionally “well-being” (again with no preceding adjective) to refer to social welfare, not individuals’ well-being (although some usages earlier in the article do refer to individuals’ utility, rather than social welfare). Dasgupta’s U(c) must, in our notation, be a Z(c) function, meaning that the proper analogue to expression (15), Dasgupta’s social welfare function, is expression (2), not expression (1). In that case, the problematic nature of working directly with a reduced-form Z(c) function is fully applicable to Dasgupta’s analysis and to that of those whom he is following in this regard. Indeed, Dasgupta focuses on the functional form \( U(c) = {{{{c^{{1 - \eta }}}}} \left/ {{1 - \eta }} \right.} \) and derives an expression for the social discount rate for consumption corresponding to expression (3) above (Dasgupta 2008, expression 4a). To illustrate the difficulties, Dasgupta (2008, p. 147) begins a key part of his discussion by posing the question: “How should the social evaluator choose U.” Since this function is chosen by a social evaluator rather than reflecting individuals’ choices under uncertainty, as measured by an empiricist, he is taking the question of the proper value of η to be entirely a question of ethics. This approach would make sense if he was implicitly assuming that α is zero (i.e., individuals are risk neutral), so that the decomposed η simply equals β, but this stance would be contrary to the evidence. He elaborates that many would infer the parameter from choices people make, but he states that he is taking the view of those who adopt a philosophical approach. Moreover, he criticizes others, including Stern, for inconsistency because they chose “η on the basis of estimates obtained from consumer behaviour, but ignored consumer behavior entirely when it came to the choice of δ and sought the advice of moral philosophers instead. This is neither good economics nor good philosophy” (p. 159). However, under a utilitarian social welfare function, this is precisely the correct economics and philosophy since η is a parameter of individuals’ utility functions and δ is entirely an aspect of the social welfare function. On the other hand, Dasgupta’s analysis—under which individuals’ utility functions, as reflected in their choices under uncertainty, are wholly ignored in choosing η despite what is apparent in expression (8)—is difficult to rationalize. Another issue in Dasgupta’s exposition—again mirroring some other prominent work—concerns how he believes a social observer should choose η. At various points, he endorses the use of thought experiments in which one backs out parameter values, taking as primitives what one deems to be reasonable policy conclusions (such as on how high of an optimal savings rate is plausible). That is, one first determines correct policy conclusions in certain settings—based on what seem to be sensible, rather than absurd, outcomes—and then asks what ethical parameter is consistent with that conclusion. The problem is that this process reverses normative analysis. The method would be appropriate if one were attempting to determine what normative parameter is implicit in some actual society’s policy choices. For example, in the optimal income tax problem, one can back out a polity’s social welfare function from its policies (and empirical parameter estimates). Note that if, in Dasgupta’s example, a high savings rate is rejected, this may well reflect that society does not care that much about future generations. (He instead takes as a given that they value far distant generations equally with themselves and infers that they therefore must be highly inequality averse and hence will not make significant sacrifices for even astronomically large benefits for those in the far future.)Footnote 12 In any case, his analysis fails to reflect that, even if one adopts a social welfare function that is strictly concave in individuals’ utilities, expression (8) makes clear that the empirical question of the curvature of individuals’ utilities—their risk preferences—will be an important (and possibly the much larger) component of the proper value for aggregate curvature, the η in Dasgupta’s analysis. The well-known treatment of the social discount rate in Arrow et al. (1996) is much the same. Their social welfare function (expression 4A.1) isFootnote 13
 (This expression is immediately followed with a version of our expression 3, their 4A.2.) This formulation again raises the question: Is the social welfare function in (16) properly compared to expression (1) or expression (2) above? Closely related, is their function W(c) a utility function, a social welfare function, or a reduced-form composite? They tell us that their W is “welfare,” which might suggest social welfare, but immediately thereafter they refer to their analogue to η as “the elasticity of marginal well-being, or marginal utility” and then follow by presenting a “convenient form of W [as] one giving a constant elasticity of marginal utility” (p. 134). Just as one is about to conclude that their W is a utility function, meaning that the social welfare function in expression (16) is implicitly utilitarian, they state: “A higher value of [η] means greater emphasis on intergenerational equity. As [η]→∞, the well-being function in (4A.1) resembles more and more the Rawlsian max-min principle; in the limit, optimal growth is zero” (p. 135). Hence, in the end, their W(c) function has to be understood as the analogue to our reduced-form Z(c) function. Accordingly, the foregoing analysis is fully applicable to their treatment as well. To illustrate the potential for confusion, consider the following statement that appears under the heading “Diminishing marginal utility,” immediately after a discussion of utility that unmistakably refers to individuals’ actual utility functions as revealed by their behavior: “Just as the choice of the rate of pure time preference ([δ]) has important implications for intergenerational equity, as discussed above, so does the choice of the elasticity of marginal utility. The more weight the society gives to equity between generations, the higher the value of [η]” (p. 136, emphasis added). Begin with the Stern Review (Stern 2007). On one hand, the presentation seems to eschew the reduced-form approach. It states (p. 44) that one can think of “overall welfare, W, calculated across households (and generations) as a function of the welfare of these households.” The social welfare function for the simple case (Stern’s expression 3) is stated to be
 Although this is identical to expression (15), suggesting that U should be understood as a reduced-form welfare function, this function is introduced by informing the reader that it presents “a very special additive form of W.” Hence, the interpretation is that U is indeed a utility function and expression (17) should be understood as a utilitarian social welfare function. Furthermore, attention is focused (Stern’s expression 6) on the case in which
 leading to the interpretation that his η corresponds to our α, the coefficient of relative risk aversion, an empirical parameter of individuals’ utility functions reflecting their choices under uncertainty. Stern then presents his analogue to our expression (3) (his expression 8). However, this interpretation turns out to be incorrect, or at least misleading. Stern states (p. 44) that “[t]he joint specification of W and [U] constitutes a set of value judgments which will guide the assessment of consequences.” One might have expected him to have said that the specification of W constitutes a value judgment, whereas the specification of U does not, but rather involves an empirical assessment of individuals’ risk preferences. All ambiguity is eliminated in his discussion of his version of expression (17), which had been described explicitly as a utility function (recalling that he purports to be using a utilitarian social welfare function): “η which is the elasticity of the marginal utility of consumption…is essentially a value judgment” (p. 46). As we have discussed, however, it is hard to understand how the proper choice of an empirical parameter constitutes a value judgment. This subject is elaborated further in Stern (2008). He discusses how one “can interpret η as the parameter of relative risk aversion in the context of an expected utility model of individual behavior” (p. 17). After noting familiar behavioral anomalies, he claims that “there is very little to guide us.” From that, he somehow concludes that “we must address the ethics directly.” For example, he argues that “direct ethical discussion…suggests a broad range for η, although the consequences for simple transfers suggest that many would regard η in excess of 2 as unacceptably egalitarian.”Footnote 14 We find it difficult to understand how an actual, empirically grounded parameter of a utility function could be objected to on normative grounds. There appear to be two ways to interpret Stern’s discussions. One is that he adopts a social welfare function that is utilitarian (additive) in functional form but rejects a subjectivist view of utility, instead deeming individuals’ utility functions for evaluative purposes to be something to be chosen by the social observer, with the curvature parameter being based on ethical views concerning the distribution of consumption (without regard to how consumption actually influences individuals’ utilities).Footnote 15 The other is that he is following the fairly common approach of viewing U(c) as really a reduced-form social welfare function of sorts, tantamount to our Z(c). In any case, the distinct conceptual roles of the W and U functions, and the distinct types of arguments and evidence pertinent to each, are obscured.Footnote 16
",33
42.0,2.0,Journal of Risk and Uncertainty,23 February 2011,https://link.springer.com/article/10.1007/s11166-011-9113-5,Assessing small non-zero perceptions of chance: The case of H1N1 (swine) flu risks,April 2011,Wändi Bruine de Bruin,Andrew M. Parker,Jürgen Maurer,Unknown,Male,Male,Male,"The survey was conducted between May 26, 2009 and June 8, 2009. Respondents were members of RAND’s American Life Panel (ALP), who were recruited from the 2007 sample of the Michigan Survey of Consumers (MS), which was constructed using random digit dialing. MS respondents were asked to participate in a long-term research project with RAND. A snowball technique was used to grow the ALP, with participants referring friends or acquaintances. A detailed description of the panel appears at https://mmicdata.rand.org/alp/index.php/Main_Page. Among active ALP members (N = 2694), defined as those who have completed at least one survey since June of 2008, 76.8% (N = 2069) completed our survey. Of those, 44.4% had no post high school degree, 51.3% made less than the median of $60 k/year, 9.7% were nonwhite, and 58.5% were female. Respondents were between 18 and 91 years old (M = 50.13, SD = 14.85). The introduction to the flu risk perception questions explained the 0–100% range, explicitly encouraging the use of non-round numbers and numbers after the decimal point.Footnote 1 Subsequently, the initial flu risk perception questions asked (a) “What do you think are the chances that you will get H1N1 (swine) flu in the next month?” (b) “If you do catch the H1N1 (swine) flu, what do you think are the chances that you will die from it?” They answered by clicking on a 0%–100% linear scale that provided a tick mark for every integer, or typing a response in an accompanying text box, which accepted non-integer numeric responses. For each flu risk perception question, respondents initially judging probabilities in the 0–1% range were told, “We would like to get extra information about the last question.” They then received the follow-up question, which used the same wording as the initial question, and provided the response options 0%; more than 0% and less than or equal to 0.001% (1/100,000); more than 0.001% (1/100,000) and less than or equal to 0.01% (1/10,000); more than 0.01% (1/10,000) and less than or equal to 0.1% (1/1,000); more than 0.1% (1/1,000) and less than or equal to 1% (1/100); and 1%. These response options were displayed vertically in ascending order. Respondents self-reported their highest level of education, as well as their birth day and year, gender, race, and income category. The survey included three validation measures. First, respondents answered “how concerned are you that you might get sick from H1N1 (swine) flu?” by selecting “very concerned,” “somewhat concerned,” “not very concerned,” or “not at all concerned,” with the first three responses being coded as being concerned (=1) and the fourth response as not being concerned (=0).Footnote 2 Second, respondents checked whether they had taken “special precautions to protect yourself against H1N1 (swine) flu,” including increased frequency of hand washing, bought antiviral medication, bought any type of face mask, avoided public gatherings and contact with others, avoided travel, stockpiled food/water, stockpiled prescription drugs, other (please specify), and none, coded as any (=1) versus none (=0). Third, respondents answered “If a vaccine becomes available for the H1N1 (swine) flu this fall, what are the chances that you would get the vaccine?” on a visual linear scale ranging from 0% to 100%. Demographic information was reported when respondents first signed up for the panel. The present survey started by asking whether respondents were concerned that they might get sick with H1N1 (swine) flu. Subsequently, they reported the specific precautions they had taken to protect themselves against it. They also answered questions unrelated to our hypotheses (and not analyzed here). Probability questions appeared at the end, asking respondents to judge the probability of getting vaccinated against H1N1 (swine) flu, getting sick with H1N1 over the next month, and dying if H1N1 (swine) flue were contracted.",19
42.0,2.0,Journal of Risk and Uncertainty,27 January 2011,https://link.springer.com/article/10.1007/s11166-010-9111-z,"Information, risk perceptions, and smoking choices of youth",April 2011,Frank Sloan,Alyssa Platt,,Male,Female,Unknown,Mix,,
42.0,3.0,Journal of Risk and Uncertainty,28 April 2011,https://link.springer.com/article/10.1007/s11166-011-9118-0,Prospect theory for continuous distributions: A preference foundation,June 2011,Amit Kothiyal,Vitalie Spinu,Peter P. Wakker,Male,Male,Male,Male,"
X is an outcome set. It is often taken to be the set ℝ of reals, designating monetary outcomes. Everything that follows remains valid for more general outcome sets, such as commodity bundles or health states. We will assume for the utility function defined later that its range is a nonpoint interval, which will imply that the outcome set is a continuum and cannot be finite. One outcome, the reference outcome, denoted r, will play a special role in what follows. A common case is r = 0 for monetary outcomes, designating the status quo. A probability distribution
P on X maps the subsets of X to [0, 1], with P( ∅ ) = 0, P(X) = 1, and
 Sometimes a stronger property, countable additivity, is useful: If a countable sequence of events A

j
 are mutually disjoint then \(P(\cup _{j=1}^{\infty}A_{j}) = \sum_{j=1}^{\infty}P(A_{j}).\)
 
\(\mathcal{P}\) is a subset of the set of all probability distributions over X . Its elements are called prospects. Our analysis will be flexible regarding the domain (\(\mathcal{P}\)) of prospects. Our analysis holds with no modification if we restrict attention to countably additive prospects. It also holds if we add measure theoretic structure (Appendix A). 
\(\mathcal{P}^{s}\) denotes the set of simple prospects, i.e. the prospects that assign probability 1 to a finite set of outcomes. A simple prospect can be denoted (p
1:x
1, ..., p

n
:x

n
) with the obvious meaning. \(\succcurlyeq\) is a preference relation over \(\mathcal{P}\). ≻, \(\preccurlyeq\), ≺, and ∼ are defined as usual. Any degenerate prospect (1:α), assigning probability 1 to an outcome α, is identified with that outcome. Preferences over prospects thus generate preferences over outcomes. Outcomes strictly preferred to r are gains, and outcomes strictly less preferred than r are losses. Outcomes indifferent to r are neutral. For prospect P, outcome α is a certainty equivalent (CE) if α ∼ P. A function V
represents
\(\succcurlyeq\) if \(V:\mathcal{P} \rightarrow\mathbb{R}\) and
 A function w is a (probability) weighting function if it maps [0, 1] to itself, satisfies w(0) = 0 and w(1) = 1, and is nondecreasing. Under prospect theory, we use a different weighting function for gains than for losses. 
Prospect theory (PT) holds if there exist weighting functions w
+ and w
−, and a utility function U : X → ℝ with U(r) = 0, such that the evaluation
 is well defined and real-valued for all prospects \(P\in\mathcal{P}\), and represents preferences on \(\mathcal{P}\). Then PT(P) is the prospect theory value of P. Tversky and Kahneman (1992) only considered simple prospects, for which the integral in our Eq. 1 coincides with their definition of PT. Equation 1 agrees with Quiggin’s (1982) rank-dependent utility for gains separately, and also for losses separately. It may deviate when both gains and losses are present. We have maintained the classical symbol U and the classical term utility, instead of the symbol v and the term value function that Kahneman and Tversky used. For losses, our function U implicitly includes loss aversion; i.e., it is the loss aversion parameter times the value function of prospect theory. This is why we do not explicitly write the loss aversion parameter in this paper. Wakker (2010) called U the overall utility function. If PT holds and, more generally, if a representing function exists, then \(\succcurlyeq\) is a weak order. That is, then \(\succcurlyeq\) is complete (\(P \succcurlyeq Q\) or \(Q \succcurlyeq P\) for all \(P,Q \in\mathcal{P}\)) and transitive. For the extension of PT from simple to bounded prospects, we use the following two preference conditions.
 
Certainty equivalence, or CE equivalence, holds if for each prospect \(P \in\mathcal{P}\) there exists a CE. 
(Weak) stochastic dominance holds if, for all \(P,Q \in\mathcal{P}\), we have \(P \succcurlyeq Q\) whenever: \(P\{\beta\in X: \beta\preccurlyeq\alpha\} \leq Q\{\beta\in X: \beta\preccurlyeq\alpha\} \}\) for all α ∈ X. We next consider the extension to prospects with unbounded utility (of their outcomes) that, however, still have a finite and well-defined PT value. For this purpose we use a truncation-continuity condition, introduced by Wakker (1993). The condition imposes restrictions on the preference relation in combination with the preference domain \(\mathcal{P}\). It will rule out all unbounded prospects that have infinite or undefined PT values, and will do so entirely in terms of preferences. For prospect P and outcome μ, P
 ∧ μ, the above truncation of P
 at μ
, is equal to P on \(\{\alpha\in X: \alpha \preccurlyeq\mu\} \backslash\{\mu\}\), and assigns all remaining probability P{μ} + P{α ∈ X: α ≻ μ} to outcome μ. For outcome ν, P
 ∨ ν
, the below truncation of P
 at ν
, is equal to P on \(\{\alpha\in X: \alpha\succcurlyeq\nu\} \backslash\{\nu\}\), and assigns all remaining probability P{ν} + P{α ∈ X: α ≺ ν} to outcome ν. 
P is truncation-robust if for all outcomes α the following holds:
 All bounded prospects (defined formally in Appendix B), which includes all simple prospects, trivially are truncation-robust. Truncation-continuity holds if all prospects \(P\in\mathcal{P}\) are truncation-robust. We will assume the following richness on \(\mathcal{F}\): Truncation-closedness holds if for every \(P \in\mathcal{P}\) and α ∈ X, we have \(P^{\wedge\alpha} \in\mathcal{P}\) and \(P_{\vee\alpha} \in\mathcal{P}\). We will not relate the following theorem to one particular preference foundation of PT. We will state it in general, so that it can be used as an add-on to every preference foundation of PT for simple prospects that has a continuum of utility. This includes the common cases with real-valued outcomes or commodity bundles as outcomes and with continuous utility. Preference foundations for PT have so far mostly been studied for uncertainty. Chateauneuf and Wakker (1999) gave a preference foundation for risk with a continuum of utility, but only for simple prospects. Theorem 2 can be combined with their result to include continuous distributions. 
Assume that PT holds on 
\(\mathcal{P}^{s}\)
, with utility function U
 and weighting functions w
+
 and w
−
. Assume the following richness:
 
U(X) is an interval that contains 0 in its interior.
 
 We have: 
\(\mathcal{P}^{s} \subset\mathcal{P}\)
, certainty equivalence, and truncation-closedness. 
 
Then PT holds on all of 
\(\mathcal{P}\)
 with respect to the same U
, w
+
, and w
−
, with the PT value of all prospects in 
\(\mathcal{P}\)
 well defined and finite, if and only if the following conditions hold on 
\(\mathcal{P}\)
:
 
 Weak ordering; 
 
 Stochastic dominance; 
 
 Truncation-continuity. 
 Wakker (1993, Corollary 4.5) did not use the proof technique that we use in Appendix C (constructing the state space S as a high-dimensional product space) and, hence, provided his extension of rank-dependent utility to unbounded prospects only for countably additive prospects. We can now extend it to finitely additive prospects. Another difference between Wakker’s (1993) analysis and ours is that he used a step equivalence assumption instead of our simpler certainty equivalence assumption. If the utility range is a continuum, as in Wakker’s Corollary 4.5 and in our paper, then equivalence of the two conditions follows elementarily. A third difference is that Wakker (1993) assumed a condition called weak truncation-closedness, instead of our truncation-closedness, throughout his paper. Our results could be generalized to Wakker’s condition. That condition is, however, more complex, and we nowadays prefer accessibility to mathematical generality. 
Under truncation-closedness, Corollary 4.5 of Wakker (1993)Footnote 4
 also holds without the restriction of countable additivity. □",33
42.0,3.0,Journal of Risk and Uncertainty,03 May 2011,https://link.springer.com/article/10.1007/s11166-011-9117-1,Is imprecise knowledge better than conflicting expertise? Evidence from insurers’ decisions in the United States,June 2011,Laure Cabantous,Denis Hilton,Erwann Michel-Kerjan,Female,Male,Male,Mix,,
42.0,3.0,Journal of Risk and Uncertainty,05 April 2011,https://link.springer.com/article/10.1007/s11166-011-9115-3,"Dopamine receptor genes predict risk preferences, time preferences, and related economic choices",June 2011,Jeffrey P. Carpenter,Justin R. Garcia,J. Koji Lum,Male,Male,Unknown,Male,"Dopamine is a neurotransmitter that has been associated with the mesocorticolimbic reward circuitry (or pleasure system) in the brain. Dopamine, when released, provides feelings of joy that become associated with the triggering thoughts or acts. As such, dopamine provides reinforcement for certain behaviors, particularly those associated with the expectation of reward. Dopamine receptor alleles variably modulate the binding of the neurotransmitter and, therefore, regulate the intensity of the experienced sensation. Figure 1 illustrates the dopamine projection system in the human brain. Midbrain dopamine neurons, the main source, are located in the ventral tegmental area and innervate those areas which have been linked to the anticipation of, cognition of and appetite for rewards: the striatum, the prefrontal cortex and the nucleus accumbens (Schultz 1999). For our purposes, it is important to note that previous research predicts that certain aspects of personality such as sensation-seeking and novelty-seeking (i.e., being impulsive, or exploratory) may be influenced by the dopaminergic links in the brain (Cloninger et al. 1993) and that this system has also been linked to inconsistent time preferences (McClure et al. 2004). More generally, mesolimbic dopaminergic reward has been linked to the reinforcement of appetitive stimuli such as gambling and addictive drug use (Comings et al. 2001; Alcaro et al. 2007).
 Overview of the projection territories of midbrain dopamine neurons. Cell bodies of dopamine neurons are located mainly in the pars compacta of substantia nigra and the medially adjoining ventral tegmental area. Their axons project mainly to the striatum (caudate nucleus, putamen), ventral striatum including nucleus accumbens, and frontal cortex (dorsolateral, ventrolateral and orbital prefrontal cortex). Dopamine is released from axon terminals with impulses and influences neurons in these structures. Our experiments investigate the impulse activity at the level of dopamine cell bodies. Source: Schultz (1999) Specific genes code for the function of different dopamine receptors (i.e., D1–D5). We are particularly interested in one of these genes, DRD4, which produces receptors in the limbic system, the prefrontal cortex, and the striatum which has some role in executive function. Because these receptors exist in brain regions that are responsible for motivation, cognition, and emotion and the battle between the three, we posited that this particular gene would be the most promising candidate for phenotypic expression as behaviors that are the result of the tradeoffs between rewards, risk and impatience. The allelic variation of DRD4, samples of which we gather, is slightly complex because alleles differ in the number of times a segment of the gene repeats itself (generally between 2 and 11 times). The most common polymorphisms include either the 4-repeat allele (carried by approximately three-quarters of the population) or the 7-repeat allele. The presence of the “longer” repeating alleles (7 or more) is thought to be associated with reduced sensitivity to dopamine and the need for relatively more stimulation to provoke the same internal reward. Because possessing at least one allele of 7-repeats or longer (7R+) has been linked to novelty-seeking (Noble et al. 1998) and other risky behaviors like sexual intercourse at a younger age (Eisenberg et al. 2007b) or pathological gambling (Perez de Castro et al. 1997) we hypothesized that 7-repeats would behave less risk aversely in our baseline risk task as they have been shown to do elsewhere (Kuhnen and Chiao 2009; Dreber et al. 2009). However, we also sought to add to our knowledge by including new measures to see if this relative risk seeking spanned domains in which only gains were possible, losses were possible and the odds were more ambiguous. Additionally, given the established links between attention deficit hyperactivity disorder and DRD4 (Swanson et al. 2007) and the previous discounting work of Kobayashi and Schultz (2008), we hypothesized that 7-repeats might also be generally less patient and, to extend these results, perhaps even more likely to discount future outcomes quasi-hyperbolically.",47
42.0,3.0,Journal of Risk and Uncertainty,13 April 2011,https://link.springer.com/article/10.1007/s11166-011-9116-2,Self-protection against repeated low probability risks,June 2011,Aric P. Shafran,,,Male,Unknown,Unknown,Male,"Each subject began the session with $30.00 in earnings. In each round, subjects faced a risk of a loss \(\frac{L}{s}\) with probability sp
0. For a cost c, subjects could reduce the probability to sp
1. This gamble was repeated n times, with each outcome independent of all other outcomes. The parameters L, p
0, and p
1 were fixed at $3.00, 0.02, and 0.01, respectively. There were two between-subjects treatment variables. The first was c, the cost of protection. The cost of protection was either set so that expected payoffs were equivalent with or without protection ($0.03) or so that protection resulted in higher expected payoffs ($0.02). The second treatment variable was s. Subjects either faced a high probability of a small loss (s = 20) or a low probability of a large loss (s = 1), with expected payoffs held constant. This design yielded four possible scenarios which are summarized in Table 1. Each subject played only one of the four scenarios.
 Sessions were divided into four parts which differed in the value of n and in the feedback presented. All other parameters (L, s, p
0, p
1, and c) remained constant throughout the session. In the first part, denoted the one-shot gamble, subjects indicated whether or not they wanted protection when facing the gamble once. In the second part, denoted the repeated gamble without feedback, subjects indicated whether or not they wanted protection if the gamble was to be repeated 100 times. In the third part, denoted the repeated gamble with feedback, subjects made a sequence of 100 choices about whether or not to protect, receiving feedback after each round about whether they incurred a loss in the previous round. In the fourth part, which replicated the repeated gamble without feedback, subjects again indicated whether or not they wanted protection if the gamble was to be repeated 100 times.Footnote 3
 Subjects were students at California Polytechnic State University. The sessions lasted approximately thirty minutes, and average earnings were $14.78. This design allows tests of the effect of four treatment variables: the effect of increasing s, the effect of increasing n, the effect of increasing c, and the effect of providing feedback between rounds. To examine the effect of increasing s, n, and c in the descriptive paradigm, we state propositions based on expected utility theory with risk aversion. Given the already large body of literature showing deviations from expected utility maximization, we do not necessarily expect subjects to play as perfect expected utility maximizers. Rather, the theory of expected utility maximization acts as a starting point to be tested based on observed behavior in the experiments. Following each proposition, a discussion of how predictions would change under prospect theory is included. Lastly, five hypotheses for behavior in the treatments with feedback are presented, based on the theory and past empirical observations of decision making in the feedback paradigm. A risk averse expected utility maximizer maximizes the expected value of his or her utility function. The utility function is a function of total wealth Y which satisfies u′(Y) > 0 and u′′(Y) < 0. Assumption 1 reflects a preference for more wealth and Assumption 2 imposes risk aversion. Kahneman and Tversky (1979) highlight several systematic deviations from expected utility maximization and propose a new theory of decision making called prospect theory. For the experiments in this paper, there are three important distinctions between prospect theory and expected utility. First, gambles are expressed as gains or losses relative to a reference point rather than in terms of final outcomes. In this paper, all gambles are exclusively in the loss domain relative to their initial wealth at the beginning of the experiment. Second, the value function which translates dollar values to utility is convex for losses. Third, subjects weight the value function for each possible loss according to a subjective probability of that loss occurring rather than the objective probability. These subjective probabilities overweight unlikely events and underweight high probability events. Furthermore, the probability weighting function starts out concave and then becomes convex (see Tversky and Kahneman 1992; Prelec 1998). The parameter s allows for changes in the magnitude of the loss and the probability of loss, holding the expected loss constant. Risk averse expected utility maximizers are more likely to invest in protection against LPHC risks (small s) than HPLC risks (high s) with the same expected loss. This fact is stated formally in the following proposition: (Increasing s) A risk averse expected utility maximizer who dislikes protection for some s
0
also dislikes protection for all s > s
0. Let c
* denote the highest cost that an individual will pay for protection. c
* is defined so that the following equality holds:
 Totally differentiating yields the following expression for \(\frac{\partial c^{*}}{\partial s}\):
 The denominator is positive as a result of Assumption 1. Since, by the definition of protection,
  and, by Assumption 2,
  the numerator is negative. Thus, \(\frac{\partial c^{*}}{\partial s} < 0\). Let c denote the actual cost of protection. An individual who dislikes protection for an arbitrary s
0 has \(c^{*}(s_0)<c\). Since \(\frac{\partial c^{*}}{\partial s} < 0\), c
*(s) < c for all s > s
0, and thus the individual dislikes protection for all s > s
0.□ Under prospect theory, the value function is convex in the loss domain which leads to the reverse prediction with linear probability weights, that individuals are more likely to protect against the HPLC risk. However, under the range of probabilities in these experiments, the probability weighting function is concave which makes protection against the LPHC risks more desirable than under linear probability weighting. Combining the convexity of the value function with the concavity of the weighting function, prospect theory offers no clear prediction in either direction. It is still noteworthy that a preference for protection against the HPLC risk is consistent with prospect theory while inconsistent with expected utility theory under risk aversion. In a famous article, Samuelson (1963) proved that if an expected utility maximizing agent prefers a single gamble over another at all relevant income levels, then he or she must also prefer that gamble repeated n times over the other repeated gamble (for all n). A key assumption is that the single gamble is preferred at all relevant income levels. We restate that assumption below in the context of this paper: An agent prefers to self-protect when facing a one-shot risk with wealth Y = Y0 if and only if he also prefers to self-protect when facing a one-shot risk for all Y such that \(Y_0 \ge Y \ge Y_0-\frac{nL}{s}-nc\). We now show that individuals should make the same choice when facing a single gamble, a repeated gamble without feedback, and a repeated gamble with feedback. (Increasing n) An individual prefers protection when facing a gamble repeated n
 times (with or without feedback between rounds) if and only if the individual also prefers protection when facing the gamble once.
 Denote \(Y_{n-1} \in [Y_0-\frac{(n-1)L}{s}-(n-1)c,Y_0]\) as an individual’s income after n − 1 repetitions of the gamble, with n > 1. Conditional on outcome Y
n − 1, an individual’s expected utility from the final repetition is
  if he does not invest in protection, and
  if he does invest in protection. If Assumption 3 holds, the individual will make the same choice in the final round as for the one-shot gamble, for any possible Y
n − 1. Assume that an individual, in round i < n, plans to make the one-shot gamble choice for sure in all rounds (i + 1,...,n). In round i, conditional on outcome Y
i − 1, an individual’s expected utility for the remaining rounds is
  if he does not invest in protection, and
  if he does invest in protection, where I is an indicator equal to 1 if the individual prefers protection for the one-shot gamble and 0 otherwise, and
 p(k) is the probability of k losses in the last n − i rounds, given that the individual is making choice I. With the assumption that the individual plans to make the one-shot gamble choice for sure in all rounds (i + 1,...,n), p(k) is the same regardless of the choices the individual makes in round i. By Assumption 3, each element of the first sum is larger than each corresponding element of the second sum if the individual prefers not to protect for the one-shot gamble (for all possible Yi − 1) and each element of the second sum is larger than each corresponding element of the first sum if the individual prefers to protect for the one-shot gamble (for all possible Yi − 1). Thus, expected utility is maximized over the remaining rounds by making the same choice in round i as for the one-shot gamble. We have now shown that the individual makes the same choice in round n as for the one-shot gamble, and that, if the individual is making the same choice in all rounds (i + 1,...,n), he will make that choice in round i as well. Combining these two results, it must be the case that the individual makes the same choice in all n rounds as for the one-shot gamble. Because the previous analysis is valid for any possible outcome of the previous rounds (Y
i − 1), this result applies to a sequence of gambles with or without feedback between rounds.□ Under prospect theory, subjects make decisions about each gamble in terms of the loss or gain relative to their initial wealth. Each choice in a repeated gamble is framed identically to a single gamble, regardless of previous losses or gains. Thus, prospect theory also predicts that individuals make the same choice for single and repeated gambles. Individuals are less likely to protect when the cost of protection goes up. Although this result should be readily apparent, we provide the proposition and proof below in the interest of completeness. This proof relies only on the assumption of a preference for money and thus easily extends to prospect theory. (Increasing c) An individual who dislikes protection for some c
0
 also dislikes protection for all c > c
0. By Assumption 1,
  for all c > c
0. From that, it follows that, if
  then
  for all c > c
0.□ After making a single choice about a single gamble and a single choice about a gamble repeated 100 times, subjects made 100 repeated choices about a gamble with feedback between choices. We present five hypotheses to test regarding the effect of feedback across treatments. Proposition 2 predicts that subjects should make the same choice in each of the 100 repeated rounds. Nonetheless, prior study results indicate that, even in the absence of feedback, subjects may have preferences for a mixed option of some rounds with protection and some rounds without (Chen and Corter 2006). Other studies with feedback indicate that subjects may follow an adaptive process in which prior outcomes affect future choices. For example, subjects may act as Bayesian learners, placing some weight on the provided probabilities and forming subjective probabilities based on observed outcomes (Viscusi 1989). Alternatively, subjects may act as reinforcement learners, a less sophisticated learning process in which subjects make choices which have led to high payoffs in the past. Both kinds of adaptation lead to our first hypothesis: (Adaptation) Prior outcomes have a significant effect on subjects’ choices.
 The experimentation and adaptation noted above may reflect a Bayesian process where subjects update their subjective probabilities of incurring a loss based on prior outcomes. We now formalize this process and present a plan to test this hypothesis. Denote subjects’ perceived probability of a loss in round t without protection as \(p^t_0\) and with protection as \(p^t_1\). Denote the priors as \(p^1_0\) and \(p^1_1\). After observing the outcome of each round, subjects update their perceived probability of a loss for the chosen option. At any point in time,
 where i ∈ {0, 1} and \(l^j_i\) is 1 if the subject chose i and incurred a loss in round j, \(n^j_i\) is 1 if the subject chose i in round j, and n
0 is the weight placed on the prior probability. This implies that subjects update their perceived probability each round as follows:
 The perceived expected value from each choice in round t is:
 where \(\pi^t_i \in [0,1]\) is the actual payoff from choosing i in round t, normalized so that the worst possible payoff is 0 and the best payoff is 1. Note that \(EV^t_i=EV^{t-1}_i\) when choice i is not chosen in round t − 1 since subjects do not observe what would have happened had they chosen i. We assume that the probability of making choice i in round t depends on the perceived expected value associated with that choice:
 where λ is a parameter which captures the importance of perceived expected values in determining the choice (smaller values imply more random choices). The parameters λ, n
0, and \(EV^1_0\) will be estimated by maximum likelihood with \(EV^1_1\) normalized to 0 so that \(EV^1_0\) represents the increase in expected value from choosing not to protect. In running our estimations, Eq. 3 will be further modified to include two additional parameters to incorporate recency effects.
 The parameter δ ∈ [0, 1] allows for recency effects in which recent outcomes with a given strategy count more heavily than older outcomes with the same strategy. Values less than 1 imply that older outcomes are weighted less heavily than more recent outcomes in forming subjective probabilities. The parameter δ ∈ [0, 1] allows for recency effects in which strategies played more often in recent rounds are preferred to strategies which have not been played recently. When the parameter γ = 0, subjects only update the probabilities associated with choice i following rounds in which they actually choose i. When γ = 1, subjects update the probabilities associated with i as if they incurred a loss in rounds in which they do not actually choose i (\(\pi^{t-1}_i\) is set equal to the payoff when a loss occurs if choice i was not chosen in round t − 1). Values between 0 and 1 imply that subjects treat all foregone payoffs as losses but weight them less heavily than actual payoffs. The results of maximum likelihood estimations based on Eqs. 3 and 5 will be used to test the following hypothesis: (Bayesian updating) Subjects’ choices depend on the average payoff earned for each option over all prior rounds.
 Following our analysis of the adaptive process used by subjects over the course of the 100 rounds with feedback, three additional hypotheses regarding treatment effects in the rounds with feedback will be considered. First, some authors have found evidence that subjects making decisions with feedback depreciate past payoffs so that more recent payoffs are weighted more heavily than older payoffs in the decision making process (i.e. δ < 1). In the experiments in this paper, in the LPHC treatments, subjects are unlikely to have incurred a loss in the recent past, regardless of whether they have invested in protection. Thus, higher depreciation rates make protection less desirable since recent rounds with protection will have incurred the fixed cost of protection while typically providing no benefit. In the HPLC treatments, losses will occur at least every five rounds on average and almost every other round without protection. Thus, the benefits of protection will be more apparent even with depreciation of past payoffs. These “recency effects” lead to the prediction that feedback makes self-protection less desirable in the LPHC treatments. (Recency effects) Feedback leads to lower levels of self-protection in the LPHC treatments relative to the HPLC treatments.
 Second, the adaptive processes described above may allow subjects to learn their true preferences for self-protection. This leads to the prediction that when protection is a better deal in expected value terms, feedback will lead to more protection than when it is not. (Maximization) Feedback leads to higher levels of protection in the low cost treatments relative to the high cost treatments.
 Third, a commonly observed effect in decisions with feedback is the payoff variability effect. The payoff variability effect predicts that choice becomes random as the variability in possible payoffs increases. The payoff variability effect leads to the prediction that choice will be more random in the LPHC treatments than in the HPLC treatments since there is greater variability in payoffs in the LPHC treatments. (Payoff variability effect) In the LPHC treatments, subjects choose protection closer to 50% of the time than in the HPLC treatments.
",15
43.0,1.0,Journal of Risk and Uncertainty,18 June 2011,https://link.springer.com/article/10.1007/s11166-011-9121-5,A theorem for Bayesian group decisions,August 2011,Ralph L. Keeney,Robert Nau,,Male,Male,Unknown,Male,"The axiomatic theory of subjective expected utility introduced by Savage (1954), which integrated and extended earlier work by Ramsey (1926), de Finetti (1937) and von Neumann and Morgenstern (1947), provides a foundation for the modeling of rational decisions that has been widely and successfully applied in theory and practice. The subjective expected utility model is called the “Bayesian” model insofar as it implies that posterior probabilities obtained by conditioning one’s beliefs on the prospective occurrence of an event should be obtained from prior probabilities by an application of Bayes’ rule.Footnote 1 Anscombe and Aumann (1963) provided an alternative axiom system for the subjective expected utility model in which events with objective probabilities are assumed to be available for purposes of constructing randomized acts and calibrating the measurements of subjective probability, which simplifies the application of the model. The problem addressed by the Bayesian model is the following: 
Individual Decision Problem. An individual decision maker must choose among a set of alternatives that may lead to different consequences depending on the outcomes of events. Specifically, there is a finite
Footnote 2
set S of states of the world, that may occur and a finite set C of consequences that may be experienced. An alternative (“act”) is a function that maps events to consequences: if act f is chosen and state s ∈ S then occurs, the consequence will be f(s) ∈ C. The description of a consequence includes all aspects of the experience that might follow from the choice of an alternative that are valued by the decision maker. Under the assumptions of the Bayesian model, an individual decision maker should evaluate an alternative f on the basis of its expected utility,
 and should select the act with the highest expected utility, where p is a subjective probability distribution that represents her judgments about the likelihoods of events and u is a utility function representing her attitude toward risk and valuation of consequences. The latter function can be scaled without loss of generality so that
 where c° and c* are her least-preferred and most-preferred consequences, respectively. The problem of extending the Bayesian model for individual decisions to a model of group decisions has been studied by many authors. Some have searched for a group utility function and a group probability distribution that can be used analogously to (1) to calculate a group expected utility for each of the alternatives (Raiffa 1968; Hylland and Zeckhauser 1979; Seidenfeld et al. 1989; Mongin 1995; Gilboa et al. 2004). In addition, Keeney and Raiffa (1976) have worked on various approaches to develop a group utility function. Raiffa (1968), Aumann (1976), Clemen and Winkler (1999), and O’Hagan et al. (2006) have worked on various approaches to obtain group probabilities for the events that influence the consequences of the decision. A variety of results have been proved regarding the “impossibility” of extending the Bayesian model of individual decision making to a corresponding Bayesian model of group decision making. In the special case of a common utility function, the group decision making problem reduces to one of pooling probability judgments, and there is no pooling formula that yields group probabilities that simultaneously satisfy the “marginalization property” (namely that marginalization of pooled probabilities should yield the same result as pooling of marginal probabilities) and the “external Bayesianity” property (namely that pooled probabilities should be updated according to Bayes’ rule when individuals agree on the likelihood function). These issues are discussed by Genest and Zidek (1986). Hylland and Zeckhauser investigate the problem of separately aggregating the individuals’ probability assessments into a group probability assessment and aggregating their state-independent utility functions into a state-independent group utility function, then calculating the group expected utility for alternatives as in (1). In addition, they require that the procedure should be weakly Pareto optimal, meaning that when all members of a group have a common preference for one alternative over another, then any proposed Bayesian group model for their choice must reflect this preference and assign a higher expected group utility to the commonly preferred alternative. They also exclude aggregations that are dictatorial, meaning identical to the set of probabilities and the utility function of only a single individual in the group. They then prove that there is no group decision procedure consistent with these assumptions. Seidenfeld et al. (1989) state: “An outstanding challenge for ‘Bayesian’ decision theory is to extend its norms of rationality from individuals to groups. Specifically, can the beliefs and values of several Bayesian decision makers be amalgamated into a single Bayesian profile that respects their common preferences over options? … In other words, can their shared strict preferences over acts be reproduced with a Bayesian rationale (maximizing expected utility) from beliefs (probabilities) and desires (utilities) that signify a rational compromise between their rival positions?” They consider the implications of both weak and strong Pareto conditions, showing that the former leads to dictatorial solutions and the latter to non-existent solutions when both individual and group preferences satisfy all of the usual axioms. They go on to consider relaxing the completeness axiom for the preferences of the group, and they point out that incomplete group preferences can be determined from individual preferences via a Pareto condition, but not by considering probabilities and utilities separately. Rather, the group’s preferences are determined by the set of “probability-utility” pairs of the members. The latter approach does not attempt to perform any weighting of individual preferences in pursuit of a group decision: a lack of consensus among the members merely leads to indecisiveness on the part of the group. Mongin (1995) states that “The issue of Bayesian aggregation, as we shall refer to it, is obviously an important one in collective decision-making”. His concern is with a group of individuals who accept Bayesian principles and also wish that the group will act consistently with these principles. Specifically, he investigates aggregation of the individual group member’s probabilities over events, utilities over consequences, and expected utilities over alternatives to obtain representations of a group’s probabilities, utilities, and expected utilities that are consistent with a Pareto principle for the individual’s expected utilities. With Pareto indifference, Mongin concludes that the only consistent aggregation is that of a dictator (or inverse dictator) where the group aggregations are identical to (or the inverse of) those of one of the individuals in the group. With a strong Pareto condition, he proves that there are no consistent aggregations. Gilboa et al. (2004), when considering social decisions, argue that a Pareto assumption is not appropriate to justify choices when individuals in the group (society) have different sets of beliefs (i.e. probabilities) to describe the possible consequences. They invoke a Pareto assumption only when all of the group members have common probabilities, which was the situation that Harsanyi (1955) analyzed using the Pareto assumption. With the Pareto assumption limited to comparisons with common probabilities, Gilboa et al. prove that the group’s utility function should be a linear combination of the individual’s utility functions and that the group’s probabilities should be a linear combination of the individual’s probabilities. However, when individuals do not have identical probabilities, this result sometimes leads to evaluations of alternatives in which each individual prefers alternative A over alternative B and yet the group is advised to choose alternative B, in violation of Pareto optimality. Many of the difficulties mentioned above—dictatorships, non-existence, indecisiveness, lack of a basis for interpersonal tradeoffs—can be circumvented if the preferences of the group are not required to satisfy one of the other conditions imposed by the Bayesian model, namely that utilities for consequences should be state-independent. Harsanyi (1955) showed that Arrow’s famous impossibility theorem (Arrow 1951) for group decisions does not apply when a cardinal expected utility model rather than an ordinal utility model is used to represent individual preferences among a set of consequences. With cardinal utility functions, it is meaningful to compute sums or averages of the utilities of different individuals. Harsanyi showed that if the preferences of both the individuals and the group have a cardinal expected-utility representation, and if group members share an objective probability measure, then the group preferences satisfy a Pareto condition with respect to the preferences of the individuals if and only if the group utility function is a weighted sum of the individual utility functions. This result can be proved using a separating hyperplane argument (Border 1985). An analogous result holds for subjective expected utility preferences, as shown by Mongin (1998) and Chambers and Hayashi (2006). If both the individuals and the group have subjective expected utility preferences, then the preferences of the group satisfy a Pareto condition with respect to the preferences of the individuals if and only if the expected utility of the group is a weighted sum of the expected utilities of the individuals. This fact also follows from a separating hyperplane argument, as will be shown in Section 4. However, if the individuals are heterogeneous in both their probabilities for events and utilities for consequences, then a group utility function determined in this manner is not state-independent except in the trivial case of a dictatorship. So, the Pareto principle can be respected, but only by giving up the state-independent-utility assumption that is usually applied to individual preferences. This weakens the meaning of “Bayesian” to the extent that the term is equated with the representation of beliefs by subjective probabilities, because subjective probabilities are not uniquely determined by choices when utilities are state-dependent.Footnote 3
 From a decision-analysis viewpoint, the problem with the standard derivation of the additive expected utility model is not so much the lack of determination of group probabilities. (As noted earlier, the full range of properties of subjective probabilities cannot be preserved by any preference-aggregation method even in the special case where only probabilities are heterogeneous.) Rather, the problem is that the Pareto argument assumes the conclusion, namely that the group has preferences among the same set of alternatives as the individuals and the group’s preferences satisfy the same axioms as those of the individuals, apart from state-independence. It does not explicitly address the fact that the substance of the group’s problem is to make tradeoffs among the preferences of its members. The weights that it applies to the members’ utility functions are determined implicitly from the relation between a group utility function and individual utility functions that have been assumed into existence separately. In this paper we present an alternative and more constructive derivation of the additive model of group expected utility, in which the preferences of the group are not assumed a priori to satisfy the axioms of subjective expected utility with respect to the alternatives in the original problem, with or without state-independence. Instead, the group is assumed to have expected-utility preferences among gambles whose outcomes are assignments of expected utility to its individual members. Thus, interpersonal comparisons enter into the model at the level of fundamental measurements by the group. A utility independence condition, similar to one introduced by Fishburn (1965) for multiple objectives, is then used to link the group’s preferences for the risks taken on behalf of its members with the members’ own preferences for the risky alternatives, which turns out to yield a state-dependent expected utility representation of group preferences among the same alternatives.",17
43.0,1.0,Journal of Risk and Uncertainty,09 June 2011,https://link.springer.com/article/10.1007/s11166-011-9119-z,Dopamine and risk choices in different domains: Findings among serious tournament bridge players,August 2011,Anna Dreber,David G. Rand,Richard Zeckhauser,Female,Male,Male,Mix,,
43.0,1.0,Journal of Risk and Uncertainty,24 June 2011,https://link.springer.com/article/10.1007/s11166-011-9122-4,"Preference towards control in risk taking: Control, no control, or randomize?",August 2011,King King Li,,,Male,Unknown,Unknown,Male,"To test the hypothesis that individuals have preference towards control, I designed a set of experiments (a similar method has been used by Charness and Gneezy (2010)), in which subjects are endowed with 10,000 points (1,000 points = 0.5 euro) each, are asked how much they will allocate to a risky gamble, and are required to choose between two different methods of control when picking three numbers to bet on. The outcome of the gamble depends on which ball is drawn from an urn that contains 10 balls numbered from 1 to 10. Participants win 2.5 times of the amount bet if the ball drawn is one of the three numbers chosen. In experiment 1a, subjects are free to choose between picking their own numbers or letting the experimenter choose them; in experiment 1b, subjects need to pay 0.1 euro if they want to pick their own numbers; in experiment 1c, subjects need to pay 0.1 euro if they want to let the experimenter choose the numbers; in experiment 1d, investments under both methods are elicited using strategy method. A total of 295 subjects participated in experiment 1, experiment 2, and experiment 3. All subjects were university students from Jena, Germany; they were randomly recruited from a poll of approximately 2,500 subjects using an e-mail recruitment system. The number of subjects in each treatment ranged from 28 to 30. Each subject participated only in one of the sessions. These were conducted in German and took place in a laboratory, where subjects were randomly seated in partitioned cubicles. Subjects were informed that their lotteries would be implemented privately and they would receive their payment privately at the end of the experiment. Each session lasted about 40 min. Subjects received a show-up fee of 2.5 euro. Twenty-eight subjects participated in this experiment, in which each subject was free to choose between picking the numbers himself or letting the experimenter pick them. After the experiment subjects filled in a questionnaire on how much they would invest under the other, non-chosen method. Figure 1 (panel a) shows that 19 out of 28 subjects (67.9%) chose to pick their own numbers. If subjects were indifferent, we should expect to observe 50% choosing either method. The binomial test shows the proportion of subjects choosing to pick the numbers themselves differs significantly from the random prediction, p-value = 0.04 (one-tailed test). It is also found that conditional on preferring to choose their own numbers, subjects invested 4,157.9 points under this method and would invest only 3,342.1 points if the numbers were chosen by the experimenter, and the difference is significant with p-value equal to 0.04. On the other hand, there is no significant difference for those preferring to let the experimenter choose.
 Preference towards control, Panel
a, Panel
b, Panel
c
 In this experiment, I investigate whether subjects are willing to pay a small fee to have more control. Thirty subjects participated in this experiment, in which each subject chose between picking the numbers herself, costing 0.1 euro, or letting the experimenter pick the numbers (free). After the experiment, subjects filled in a questionnaire on how much they would invest under the other non-chosen method, and on probability belief, gender, and religious beliefs. Nine out of the 30 subjects chose to pay 0.1 euro to have more control. See panel a of Fig. 1. Thus, the result suggests that preference for control is quite strong, and individuals are willing to pay a small fee to gain more control. Of the nine subjects, only one believed that choosing his own numbers would lead to a higher chance of winning. In fact, the remaining 29 subjects held the belief that the probability of winning was the same under the two methods. This suggests that for most individuals, their preference for control is not due to illusion of control, but by source preference. Thirty subjects participated in this experiment, in which each subject chose between picking the numbers himself (free) or letting the experimenter pick them (costing 0.1 euro). After the experiment, subjects filled in a questionnaire on how much they would invest under the other, non-chosen method, and on probability belief, gender, and religious beliefs. Seven out of 30 subjects chose to pay 0.1 euro to use the method involving less control. See panel b of Fig. 1. These seven subjects believed the probabilities of winning under the two methods were identical. Thus, their choices cannot be explained by subjective probability belief and are more compatible with the source preference hypothesis. Of the 23 subjects preferring to pick the numbers themselves, only one held the belief that there was a higher chance of winning. In experiments 1a to 1c, the investment amounts under the less preferred method were elicited by a non-incentivized questionnaire. To test the robustness of the finding, I employed the incentivized strategy method to elicit subjects’ investment decision under both methods. Fifty-nine subjects participated in this experiment, in which each subject specified the investment amount when he chose the numbers versus when the experimenter chose them. Then a die would be randomly rolled to determine which method to implement. After the experiment, subjects filled in a questionnaire on their preference ordering of the two methods, and on probability belief, gender, and religious beliefs. If we compare the investment amount across the two methods, there is no significant difference. However, if the comparison is made conditional on preference, it is found that those who preferred to control on average invested 13.5% less when the experimenter picked the numbers, p-value = 0.04 (one-tailed). Two outliers were not included in the analysis, one invested 700% more and another 93.33% more when experimenter chose the numbers. Eighteen subjects (30.6%) indicated in the questionnaire that they preferred to pick the numbers themselves, while 4 preferred to let the experimenter pick them, and 37 subjects were indifferent. Only two subjects believed that choosing the numbers themselves had a higher chance of winning, while all others believed the winning probability under the two methods was the same.",9
43.0,1.0,Journal of Risk and Uncertainty,18 June 2011,https://link.springer.com/article/10.1007/s11166-011-9120-6,"Two-sided intergenerational moral hazard, long-term care insurance, and nursing home use",August 2011,Christophe Courbage,Peter Zweifel,,Male,Male,Unknown,Male,"The parent and the child (who could also be a spouse, a relative or a friend) are assumed to interact in the guise of non-cooperative game. As will be shown below, this does not preclude altruism on either side. The child, while not necessarily in the labor force anymore, is assumed to be still active enough to value his/her effort with an opportunity cost of time. The parent is characterized by a state-dependent VNM utility function defined over wealth and conditioned on being admitted to the nursing home (u) or not (υ), with u < υ for the same level of wealth. The probability \( \pi \) of being in the nursing home depends negatively (with \( \pi \prime (e) < 0 \) and \( \pi \prime \prime (e) > 0 \)) on child effort e. Entering the nursing home is therefore viewed as a chance event, whose probability depends only on child effort, possibly conditioned by the parent’s health (see Section 1.2 below). However, this event is the outcome of a decision on the part of the parent that should be modeled as well. To address this shortcoming, one could imagine a third party in the form of a physician who observes parental health status and decides in favor of nursing home admission when health status falls below a critical threshold (either because of exogenous illness or absence of child effort).Footnote 3 Since the parent is assumed to be retired, there is no labor income that could contribute to wealth. The parent is thought to be rich enough to leave a bequest but poor enough to rely on some means-tested public support M(W
0) when needing LTC in a home, such that \( M\prime \left( {{W_0}} \right) < 0 \). The cost of the nursing home is exogenously given by N. The individual can also purchase LTC insurance with a benefit I < N (full coverage is hardly ever written). The premium paid is assumed to depend linearly on I and a probability \( \bar{\pi } \) adjusted for a loading λ, with \( \bar{\pi } \) = \( {\pi^e} \)(1 + λ), where \( {\pi^e} \) denotes an average probability estimated by the insurer. Note that \( {\pi^e} \) and hence \( \bar{\pi } \) does not depend on effort e
, reflecting the assumption that the insurer cannot observe the effort of the child. Therefore, final wealth if in the nursing home is given by initial wealth W
0 minus the LTC insurance premium plus the LTC insurance indemnity. In all, one has the two wealth levels of the parent:
 Accordingly, the parent’s expected utility amounts to
 Focusing on an interior solution for simplicity, the first-order condition reads
 which can be rewritten to become
 For this condition to be satisfied, the term in brackets must be positive. A sufficient condition is \( u\prime \left( \cdot \right) < \upsilon \prime \left( \cdot \right) \). This condition is represented in panel (a) of Fig. 1, along with the natural assumption u < υ. This can be justified on two grounds. First, it reflects a form of parental altruism, causing the marginal utility of wealth to be higher when the associated consumption can be enjoyed with the child out of the nursing home [o] than when in the nursing home [i]. Second, the states [o] and [i] can be related to the states ‘healthy’ and ‘sick’. Evans and Viscusi (1991) as well as Finkelstein et al. (2009) present empirical evidence suggesting that the marginal utility of wealth is indeed higher when healthy than when sick.
 State-dependent VNM utility functions The next step is to derive the slope of the parent’s reaction curve, with child’s effort e constituting the exogenous shock. The comparative-static equation reads
 Noting that \( {\partial^2}EU/\partial {I^2} < 0 \) as the sufficient condition for a maximum, one has
 The crucial mixed derivative is given by
 Since \( u\prime \left( \cdot \right) < \upsilon \prime \left( \cdot \right). \), \( \frac{{{\partial^2}EU}}{{\partial I\partial e}} < 0 \) and hence \( \frac{{dI}}{{de}} < 0 \). Therefore, the parent is predicted to reduce LTC coverage if he or she can count on more informal effort on the part of the child, constituting the first element of two-sided intergenerational moral hazard. It has not been analyzed yet in a systematic way to the knowledge of the authors. In addition, the curvature of the parent’s reaction curve can be derived by noting that
 The parent’s reaction curve therefore runs as shown in Fig. 2. Accordingly, the parental moral hazard effect is predicted to taper off for high values of child effort in view of its low effectiveness.
 Reaction curves of parent and child Child preferences are also represented by a state-dependent VNM utility function defined over wealth and conditioned on the parent being admitted to the nursing home. The child derives utility from final wealth levels Z[i] and Z[o] respectively, with Z
0 denoting exogenous initial, pre-bequest wealth. She can expect an inheritance amounting to the share s of the parent’s final wealth. There is no discounting to present value for simplicity. The child bears cost of informal care valued at the opportunity value of time p (the wage rate if employedFootnote 4). If the child succeeds in keeping the parent out of the nursing home, the bequest is larger. Note that the cost of effort e accrues in both states of the world. Therefore, using Eq. 1, one obtains the two wealth levels of the child:
 Expected utility of the child is given by
 Focusing again on interior solutions for simplicity, one has for the first-order condition
 The second term of this condition is negative; therefore, since \( \pi \prime(e) < 0 \) it must be the case that \( \bar{u}\left( \cdot \right) < \bar{\upsilon }\left( \cdot \right) \) for \( {e^{*}} > 0 \) to obtain. Evidently, the child must be altruistic as well in the sense of valuing the ‘out of nursing home’ state of the parent [o] higher than the ‘in nursing home’ state [i]. We also assume \( \bar{u}\prime \left( \cdot \right) < \bar{\upsilon }\prime \left( \cdot \right) \), i.e. the child’s marginal utility of wealth is lower when the parent is in the nursing home. However, this difference has nothing to do with the health of the parent but rather mirrors the fact that possibilities for joint consumption are limited in this case. These conditions are represented in panel (b) of Fig. 1. In full analogy with Section 1.1, the sign of the mixed derivative determines the slope of the reaction curve,
 Here, we make the assumption that \( \bar{u}\prime \prime = \bar{\upsilon }\prime \prime \) (there is no reason why the child’s degree of risk aversion should differ depending on whether the parent is in the nursing home or not). Since \( \bar{u}\prime \left( \cdot \right) < \bar{\upsilon }\prime \left( \cdot \right) \) and assuming \( \pi (e) < \bar{\pi } \) (recall that \( \bar{\pi } \) contains a loading),
 This is the second component of two-sided intergenerational moral hazard, analyzed by Pauly (1990) and Zweifel and Strüwe (1996). With e becoming large, the difference \( \left( {\pi (e) - \bar{\pi }} \right) \) in the second term of the curly bracket increases but at a decreasing rate, while the first term goes to zero. This implies
 Therefore, the reaction curve of the child must run as shown in Fig. 2. As was to be expected, this moral hazard effect loses force too when effort on the part of the child becomes less effective.",22
43.0,2.0,Journal of Risk and Uncertainty,23 July 2011,https://link.springer.com/article/10.1007/s11166-011-9123-3,Risky investment decisions: How are individuals influenced by their groups?,October 2011,W. Kip Viscusi,Owen R. Phillips,Stephan Kroll,Unknown,Male,Male,Male,"Our analysis of decisions in both individual and group contexts intersects with several related literatures. It is instructive to briefly review these intersections, as doing so helps in formulating the experimental hypotheses and in highlighting the distinctive nature of our study. Because of the information exchanged in group contexts, choices made in a group environment may be different than those made by an individual not exposed to the choices of other people. Models of group polarization, such as Glaeser and Sunstein (2009), recognize herding effects or information cascades generated by observing others’ choices, as this information may alter the assessed probabilities and consequently affect decisions in the group environment.Footnote 2 The distinctive aspect of our work is that unlike all such previous studies, people in our study always have full information about all probabilities and payoffs.Footnote 3 We find that participants alter their choices even though there is no uncertainty regarding the probabilities or the lottery payoffs; all participants have full information. Unlike the findings in the existing literature, there is no rational basis for such an effect. What we suggest is that the complexity of making investment decisions that alter the lottery structure makes people unsure of their optimal investment amount. Observing decisions made by others can potentially be of assistance. If people observe risky decisions made by others in the group, will there be systematic effects on the riskiness of decisions? Will people become more risk-taking or less risk-taking after observing others’ behavior? Changes in the riskiness of choices in the direction of taking greater risk are known as a risky shift; risk shifts associated with less risk are known as conservative shifts or cautious shifts.Footnote 4 Most studies have found evidence of a risky shift.Footnote 5 Shupp and Williams (2008) observe in a study on lottery valuation decisions that the difference in risk preferences between groups and individuals depends at least partly on the stakes: the average group is more risk-averse than the average individual in high-risk situations, while this is reversed for low-risk situations. Our study differs from this literature in that for the experimental treatment that couples RTI with individual choice, there is no actual group interaction or group decision rule, only an exchange of information on choices. We compare the pure RTI results for individual decisions to the decisions individuals make when there is a group decision rule in order to analyze the nature of the risky shift across our different experimental treatments. In contrast to Shupp and Williams (2008), we do not allow for any form of communication even in treatments with group-decision rules. Our experimental design includes a component in which subjects make individual choices in a group context subject to a group decision rule that specifies either majority rule or unanimity. Whether the group decision rule affects investment decisions provides an additional perspective on whether there is an effect of influences other than those dictated by rational utility-maximizing choice. In particular, are there influential framing effects of the group decisions rule? For RTI group contexts in which the investment amount is guided by the median or minimum decision rule, one would not expect the group context alone to alter individual choices. When preferences are single-peaked, basing group decisions on the median value is equivalent to the results with simple majority rule. When unanimity is required, the lowest individual investment in the group guides the group decision. Thus, our case in which the lowest investment amount determines the outcome is analogous to the unanimous choice case though our experiment does not require agreement by the entire group. The person with the lowest investment amount in the minimum value treatment has no incentive to raise that amount, and the subjects with the higher investment amounts have no incentive to lower their amounts in this group setting.Footnote 6
 When the group RTI environment is coupled with a group decision rule, we find framing effects and group norms at work even when group members interact in a relatively sterile environment that allows them to only register their investment “votes” on a computer screen. Median investor and minimum investor rules not only define the payoff structure of the experiment but also have a framing effect in highlighting the pivotal values in the group context. Framing effects are well established in the psychology literature, but the major task always has been to predict ex ante how the frame will affect the decisions.Footnote 7 The median and minimum value rules have well-defined, predictable focal points that are described in the experiments as being the instrumental value that will govern group decisions. By comparing the amounts spent as individuals alone and as individuals in a group, it is possible to quantify the effect of RTI and the framing and social norm effects of the group contexts on investments.Footnote 8
 The group investment decision is made by the subjects revealing how much they are willing to invest, rather than for example simply asking members for a vote on whether to invest a specific amount or not. These features of the experiment create a rich data set on how people and groups make decisions in a context that has risk, and for which net gains are concave. The findings provide evidence of a strong influence of RTI both when choices are governed by a group decision rule as well as when individuals make their own decision after being exposed to RTI from other members of the group.",44
43.0,2.0,Journal of Risk and Uncertainty,02 August 2011,https://link.springer.com/article/10.1007/s11166-011-9125-1,Asymmetric discounting of gains and losses: A query theory account,October 2011,Kirstin C. Appelt,David J. Hardisty,Elke U. Weber,Female,Male,Female,Mix,,
43.0,2.0,Journal of Risk and Uncertainty,02 August 2011,https://link.springer.com/article/10.1007/s11166-011-9124-2,Guaranteed renewability uniquely prevents adverse selection in individual health insurance,October 2011,Mark V. Pauly,Kai Menzel,Richard A. Hirth,Male,Male,Male,Male,"We adopt the set of assumptions used by Rothschild and Stiglitz in their classic treatment of adverse selection (1976). There are two levels of risk: high risks with loss probabilities p

H
 for a loss of $X, and low risks with a probability of p

L
. Individual consumers know their own risk level. Insurers newly enrolling a consumer know the proportion of high and low risks in a population of potential insureds at any point in time and/or at any age, but they cannot distinguish between high risks and low risks. Competitive insurers offer a variety of policies for sale; policies differ in terms of the number of covered dollars C they will pay if a loss occurs (X ≥ C > 0). All insurers know the total amount of coverage an individual has obtained. We invoke the assumptions of the simple three-period model we used in our treatment of guaranteed renewability. A population begins in period 1 with all potential consumers as low risks facing the uniform loss probability p

L
. The proportion of buyers who actually suffer a loss of $X in period 1 become high risks in periods 2 and 3, while the proportion of remaining low risks who suffer a loss in period 2 become high risks only for period 3. A time path of premiums Π and policies C is a competitive equilibrium if no insurer can enter with a new policy and attract a set of buyers who allow it to have a positive expected profit. We have previously shown that there is an optimal GR contract for a given amount of coverage purchased C
*. The quantity C
* is the expected utility maximizing amount of coverage purchased by a low risk facing a premium for that policy that reflected only the loss probability p

L
. If the only change from period to period is the risk level, the optimal quantity will remain at C
*as long as income effects from changes in the magnitude of current-period premiums are absent.Footnote 3 For simplicity, we assume the administrative cost loading is zero, so the low risk premium ∏L equals \( p_L^* C \) and the amount of insurance C equals X, the cost of treatment. As noted, we also assume that a high risk facing a fair premium rate p

H
 would also demand C = X = C
*. We assumed in Pauly et al. (1995) that all insurers knew each person’s value of p at every point in time. In this modification, we continue to assume that the insurer that sold insurance to a consumer in one period knows perfectly that person’s risk in subsequent periods. After all, risk reclassification is an issue only if the initial insurer can know any change in risk level and therefore base premiums on the new level. However, other insurers that might attract the consumer away from the initial insurer and contract now are assumed to be ignorant of changes in each person’s risk level and not constrained in what they might offer. Outside insurers know the proportion who have become high risk in each future period, but not which persons experienced such changes.",9
43.0,2.0,Journal of Risk and Uncertainty,12 August 2011,https://link.springer.com/article/10.1007/s11166-011-9126-0,Does sorry work? The impact of apology laws on medical malpractice,October 2011,Benjamin Ho,Elaine Liu,,Male,Female,Unknown,Mix,,
43.0,3.0,Journal of Risk and Uncertainty,05 November 2011,https://link.springer.com/article/10.1007/s11166-011-9129-x,Viewing the future through a warped lens: Why uncertainty generates hyperbolic discounting,December 2011,Thomas Epper,Helga Fehr-Duda,Adrian Bruhin,Male,Female,Male,Mix,,
43.0,3.0,Journal of Risk and Uncertainty,20 October 2011,https://link.springer.com/article/10.1007/s11166-011-9127-z,Risk attitudes in a social context,December 2011,Ingrid M. T. Rohde,Kirsten I. M. Rohde,,Female,Female,Unknown,Female,,70
43.0,3.0,Journal of Risk and Uncertainty,27 October 2011,https://link.springer.com/article/10.1007/s11166-011-9128-y,Controlling for initial endowment and experience in binary choice tasks,December 2011,Enrique Fatás,Francisca Jiménez,Antonio J. Morales,Male,Female,Male,Mix,,
43.0,3.0,Journal of Risk and Uncertainty,10 November 2011,https://link.springer.com/article/10.1007/s11166-011-9130-4,Does nurture matter: Theory and experimental investigation on the effect of working environment on risk and time preferences,December 2011,Quang Nguyen,,,,Unknown,Unknown,Mix,,
44.0,1.0,Journal of Risk and Uncertainty,07 December 2011,https://link.springer.com/article/10.1007/s11166-011-9134-0,Ambiguity aversion and familiarity bias: Evidence from behavioral and gene association studies,February 2012,Soo Hong Chew,Richard P. Ebstein,Songfa Zhong,,Male,Unknown,Mix,,
44.0,1.0,Journal of Risk and Uncertainty,28 December 2011,https://link.springer.com/article/10.1007/s11166-011-9132-2,Reference-dependent valuations of risk: Why willingness-to-accept exceeds willingness-to-pay,February 2012,W. Kip Viscusi,Joel Huber,,Unknown,Male,Unknown,Male,"The reference dependence model developed here addresses reference dependence effects for costs and health risk probabilities. Both costs and health risks are negatively valued attributes. We assume that the reference points are well defined, as is the case in the choice experiment structure described in subsequent sections. The general formulation of the model utilizes a gain-loss utility format for reference dependence, as in Sugden (2003), Munro and Sugden (2003), and Kőszegi and Rabin (2006), which we generalize to account for reference dependence for probabilities rather than goods. The basic model assumes additive separability of the utility of cost and expected utility of health risks with a gain-loss utility for each component. We then incorporate the possibility that reference effects may be interactive. The cost utility component of the model is analogous to standard utility functions for money except that the model is in terms of cost, which is negatively valued. Let c0 be the reference initial cost level and c1 be the new cost level, where all cost terms are positive. The utility function v(c) for costs has the properties v < 0, v′ < 0, and v″ ≤ 0. The health risk considered in this article is a temporary morbidity risk similar to those that Evans and Viscusi (1991) have shown can be treated empirically as monetary equivalents using an additively separable utility function. Thus, the health outcome in our study is not a permanent disability or fatal outcome that affects the marginal utility of money. Let p0 be the initial reference probability of the adverse health effect, p1 be the new morbidity risk probability, r be the morbidity risk cost, and h(r) be the utility function for health. Although h(r) equals a fixed monetary loss of r that never varies in our example, in general one might expect h < 0, h′ < 0, and h″ ≤ 0 to characterize the dependence of h(r) on the severity r. Both cost and health have reference-dependent gain-loss components that, for purpose of the empirical analysis, we treat as multiplicative constants for which we estimate their average values.Footnote 3 The reference-dependent factor for cost will be denoted by μ, where cost increases have a factor μ+ and cost decreases have a factor μ−, where μ+>μ−, as one would expect losses from one’s initial cost reference point to loom larger than gains.Footnote 4 The reference-dependent factor for the health risk is λ, where λ+ is the factor for risk increases and λ− is the factor for risk decreases, and λ+>λ− because of the relative aversion to increases in the probability of health losses. The expected utility u from moving from a situation with c0 cost and probability of adverse health effect p0 to a cost level c1 and risk probability p1 is Thus, the monetary cost portion of the expected utility function follows the familiar structure in which the valuation is the utility value of the new cost level plus the gain-loss utility value. The reference-dependent component for the health risk probability is the distinctive component, but it is formulated analogously. However, unlike other models of reference dependence, the reference dependence effect for risks is with respect to the gain-loss value of probabilities, not utilities. The tradeoff between cost and risk can be determined by implicit differentiation of Eq. 1, yielding The role of reference dependence is captured by the reference dependence factor \( {\text{f}}(\mu, \lambda ) = (1 + \lambda )/(1 + \mu ) \). Given this structure, the value of f(μ, λ), and consequently the value of tradeoff rates, is greater for shifts from the reference point involving cost decreases, for which μ is given by μ−, and for risk increases, for which λ is given by λ+. Each of these changes will boost f(μ, λ). Although our focus is on marginal risk-cost tradeoff rates rather than exchanges of goods, it is useful to characterize the four gain-loss combinations in conventional terminology pertaining to such directions of influence. Figure 1 summarizes the four possibilities.Footnote 5 Quadrant 1 is the WTP factor f(μ+, λ−) in which a cost increase is incurred to purchase a risk probability reduction. This value is expected to be smaller than the WTA value in Quadrant 3 since the factor f(μ−, λ+) for WTA embodies a diminished valuation of cost in the denominator and a higher valuation of risk in the numerator than does WTP. Because a WTA-WTP disparity can arise from either of these influences, observation of WTA-WTP discrepancy does not pinpoint the source of the difference. The situation in Quadrant 2 in which both cost and risk are increasing is labeled tradeoff among losses (TL), where TL has a factor of f(μ+, λ+) that should yield a higher tradeoff rate than WTP. Both Quadrant 2 and WTP in Quadrant 1 include μ+ in the denominator of the reference dependence factor, but the TL value in Quadrant 2 also includes an increase in the risk probability, which raises the numerator of f because λ+>λ−. We designate the situation in which both risk and cost are decreasing in Quadrant 4 as the tradeoff among gains (TG). The TG case in Quadrant 4 should exhibit a lower tradeoff rate than for the WTA value in Quadrant 3 because of the decrease in risk level, producing a lower value in the numerator of the reference dependence factor f. Thus, of the four cases shown in Fig. 1, WTA should have the highest value, and WTP should have the lowest value. Taxonomy of possible cost-risk reference point effects Formulating comparisons in terms of ratios leads to additional relationships. Thus, and While the empirical analysis treats λ and μ in terms of average values that affect the gain-loss utility components for this particular risk context, one would expect these values to be different for other choice situations. In the simplest case, each of the parameters depends only on one particular component of the tradeoff. Thus, the cost parameter μ is a function of the level of costs, or μ(c0, c1), and the reference risk parameter λ is a function of the risks and their severity, or λ(p0, p1, r). This separability facilitates both the modeling and estimation of reference dependence effects. However, while the formulation above has the advantage of simplicity, there is no theoretical basis for ruling out the possibility that the reference dependence contextual effects interact in some fashion. Thus, the valuations of shifts in the risk level could depend on the changes that are occurring in the financial gain or loss domain. The most general formulation of the reference dependence factors would be μ(c0, c1, p0, p1, r) and λ(c0, c1, p0, p1, r). A more structured variant of this interdependence that is examined below is that in which the reference dependence effect for risk hinges on whether one is in the financial gain or loss domain. In particular, suppose that reference dependence for risks is only observed when the individual is in the cost decrease domain so that financial loss aversion is the dominant factor when costs are increasing. Such a formulation leads to a generalization of Eq. 1. Let δ = 1 if costs are in the decrease domain μ−, and δ = 0 if costs are in the cost increasing domain μ+. Then Eq. 1 can be rewritten as While a term such as δ may not enter in all applications, it will be pertinent when there is a clear asymmetry in which risk reference dependence is only manifested when costs are decreasing, and available resources are consequently increasing. Thus, when costs are increasing, the focus on the cost increase may be so great that the salient financial concerns swamp other reference point influences. We find that Eq. 5 is a more appropriate characterization of reference point effects in our study than is Eq. 1, which is patterned on more standard frameworks.",36
44.0,1.0,Journal of Risk and Uncertainty,30 November 2011,https://link.springer.com/article/10.1007/s11166-011-9135-z,Social comparison and risky choices,February 2012,Jona Linde,Joep Sonnemans,,Female,Male,Unknown,Mix,,
44.0,1.0,Journal of Risk and Uncertainty,02 December 2011,https://link.springer.com/article/10.1007/s11166-011-9131-3,"Deterrence, expected cost, uncertainty and voting: Experimental evidence",February 2012,Gregory DeAngelo,Gary Charness,,Male,Male,Unknown,Male,"The topic of enforcement costs and benefits has been studied extensively in the literature on law and economics. However, there are only a handful of controlled experimental studies. Baker et al. (2004) use an experimental setting to test for the effect of uncertainty in an environment where the choice involves an act that could yield financial losses or gains. They find that uncertainty over a deterrence mechanism increases the level of deterrence. However, the experimental framework examines the effect of uncertainty in a risky environment, which is not framed as an environment with an illegal option.Footnote 6
 Using auctions and the possibility of collusion among participants, Block and Garety (1995) test whether attitudes towards certainty and severity of punishment differ between students and prisoners. They find that the prisoners are more concerned with the likelihood of punishment, while this is reversed for the students. In a related paper, Anderson and Stafford (2003) examine the effect of punishment on free-riding in regulatory compliance and find (a) that compliance is increasing in expected cost of punishment and (b) that punishment severity has a larger effect on compliance than punishment probability. Note that our results differ in that we find that larger probability enforcement regimes (with equivalent expected costs) increase compliance. There are also studies using data from natural experiments. Bar-Ilan and Sacerdote (2004) measure the change in red lights run when the fine for running the red light changes, Ihlanfeldt (2003) measures the increase in crime due to the construction of commuter rails in Atlanta, and McCormick and Tollison (1984) measure the reduction in the number of fouls committed by NCAA division 1 men’s basketball players when the number of referees on the court increases from two to three. Also, Di Tella and Schargrodsky (2004) examine the effect of targeted increases in police enforcement (in the aftermath of terrorist attacks) on the level of crime (notably car thefts) and find significant reductions in crime due to the exogenous increase in law enforcement presence. However, none of these studies is ideally-suited for testing deterrence theory. The studies by Bar-Ilan and Sacerdote (2004) and Ihlanfeldt (2003) suffer from the fact that the individual committing the crime is uncertain about their probability and amount of the fine that will be charged.Footnote 7 Although the basketball players in McCormick and Tollison (1984) are certainly aware of the increased probability of misbehavior being detected, the estimation suffers from unintentional misclassification, as the main dependent variable reported in the box scores—fouls—sometimes included both the number of fouls and the number of rebounds. When corrected, the estimation was only significant at the 10% level.Footnote 8 Finally, Di Tella and Schargrodsky (2004) provide a somewhat limited analysis in terms of the crimes committed, focusing only on car thefts over a short time period. Since one of the foci of this article is the effect of uncertainty on deterrence, we wish to address how uncertainty is characterized in our environment. The economics literature distinguishes between risk and uncertainty. The first work to make this distinction was Knight (1921), stating: “Uncertainty must be taken in a sense radically distinct from the familiar notion of Risk, from which it has never been properly separated.... It will appear that a measurable uncertainty, or ‘risk’ proper, as we shall use the term, is so far different from an unmeasurable one that it is not in effect an uncertainty at all.”Footnote 9 Ellsberg (1961) provides persuasive examples that people having preferences distinguish between risk (known probabilities) and uncertainty (unknown probabilities). However, Heath and Tversky (1991) point out that ambiguity also makes people shy away from taking either side of a bet, as not knowing important information about the environment is psychologically uncomfortable and effectively reduces confidence. Different researchers appear to have very strong convictions with respect to the issue of what constitutes uncertainty. One approach is normative and views ambiguity as a situation in which the decision-maker cannot assign probabilities to events. According to this approach, being ambiguity averse is perfectly rational and is a natural response to lack of information.Footnote 10 In much of the research designed to examine the Ellsberg paradox, it is assumed that individuals can reduce compound objective lotteries. An alternative approach is more descriptive and is based on the observation that there exists a very strong empirical association between ambiguity aversion and violation of reduction of compound lotteries. For example, Halevy (2007) discusses uncertainty—from the perspective of the decision maker—that arises from an individual’s inability to comprehend compound lotteries or to calculate probabilities of final outcomes in compound lotteries according to the laws of probability. Since this is difficult or impossible to justify normatively, and can be safely viewed as a mistake or bias, the holders of this view tend to agree that ambiguity aversion is a form of bounded rationality.Footnote 11 From this perspective, the use of compound lotteries in decision-making can in effect induce uncertainty. This is the perspective that we take in this paper, and we do in fact find consistent evidence that speeding rates are lower when a compound lottery is present.Footnote 12
 In our experiment, we have environments with and without voting. In the case without voting, there is a compound lottery involving the probability that a particular deterrence regime will be chosen and the probability and consequences of detection. Since in principle all of the probabilities are known, some people would view this as a form of risk, rather than uncertainty; alternatively, if people are unable to reduce compound lotteries, then our experimental results in these environments can be informative as to how subjects respond to ambiguity or uncertainty. In our environment with voting for a preferred regime (with equivalent expected costs of committing a crime), the voters are informed of the regime that is implemented so that there is no compound lottery and ambiguity is averted. This environment provides a situation with risk, whereby individuals are aware of the regime that is implemented (and corresponding probability of being caught) before deciding whether or not to speed. This allows us to test environments containing risk versus uncertainty. Thus, one can view our environments as either comparing behavior under risk with behavior under uncertainty or comparing behavior under different degrees of uncertainty. In any case, it seems fair to say that our paper examines the effect of uncertainty on an individual’s willingness to commit proscribed activities.",38
44.0,2.0,Journal of Risk and Uncertainty,21 February 2012,https://link.springer.com/article/10.1007/s11166-012-9139-3,The disgust-promotes-disposal effect,April 2012,Seunghee Han,Jennifer S. Lerner,Richard Zeckhauser,Unknown,Female,Male,Mix,,
44.0,2.0,Journal of Risk and Uncertainty,10 March 2012,https://link.springer.com/article/10.1007/s11166-012-9140-x,Aggregating imprecise or conflicting beliefs: An experimental investigation using modern ambiguity theories,April 2012,Aurélien Baillon,Laure Cabantous,Peter P. Wakker,Male,Female,Male,Mix,,
44.0,2.0,Journal of Risk and Uncertainty,03 February 2012,https://link.springer.com/article/10.1007/s11166-011-9138-9,The Schwarzian derivative as a ranking of downside risk aversion,April 2012,Donald C. Keenan,Arthur Snow,,Male,Male,Unknown,Male,"The review of risk aversion here is mostly classical, but sets up the discussion of downside risk aversion presented in the next section. Following the mathematics literature, one says that u decreases (increases) ratios if, for \( {\rho_2}\left( {{y_0},{y_1},{y_2}} \right) \equiv {{{\left( {{y_2} - {y_1}} \right)}} \left/ {{\left( {{y_1} - {y_0}} \right)}} \right.} \) and any y
0 < y
1 < y
2, one has Our first result relates these properties of u to the sign of R

u
(y) ≡ −u′′(y)/u′(y).Footnote 1
 Given u′ > 0 everywhere, u decreases (increases) ratios if and only if R

u
(y) > (<)0 throughout some open, dense set of y. By the mean value theorem, there exists \( \overline y \in ({y_0},{y_1}) \) and \( \widetilde{y} \in ({y_1},{y_2}) \) such that \( u\left( {{y_1}} \right) - u\left( {{y_0}} \right) = u'\left( {\overline y } \right)\left( {{y_1} - {y_0}} \right) \) and \( u\left( {{y_2}} \right) - u\left( {{y_1}} \right) = u'\left( {\widetilde{y}} \right)\left( {{y_2} - {y_1}} \right) \). Then and u decreases ratios if and only if \( {{{u'(\widetilde{y})}} \left/ {{u'(\bar{y})}} \right.} < 1 \). By a perturbation argument we may assume that \( \overline y \) and \( \widetilde{y} \) belong to the open, dense set. With \( {R_u}(y) > 0 \) on the set and u′(y) > 0 throughout, we have \( u'(\overline y ) > u'(\widetilde{y}) > 0 \), and so \( {{{u'(\widetilde{y})}} \left/ {{u'(\overline y )}} \right.} < 1 \). By the same line of reasoning, u increases ratios if R

u
(y) < 0 for all y in the open, dense set. From this observation, we obtain the necessity of R

u
(y) > 0 for all y in the open, dense set in order for u to decrease ratios, since if \( {R_u}\left( {\widehat{y}} \right) < 0 \) for some such \( \widehat{y} \), then we could take [y
0, y
2] as belonging to an open interval about \( \widehat{y} \) on which, by continuity, \( {R_u}(\widehat{y}) < 0 \) for all y in that interval, implying that u increases ratios in that case, in violation of the supposition that it decreases them throughout. ■ Lemma 1 establishes the concavity of u when R

u
(y) is uniformly positive and u is positive monotonic (as we shall henceforth assume for all utility and transformation functions u, v, and φ). By decreasing ratios of distance along the real line, u(y) itself increases at a decreasing rate, indicating risk aversion. R

u
(y) is known to economists, of course, as the Arrow-Pratt measure of risk aversion for the von Neumann-Morgenstern utility function u. In a variety of contexts, economists are interested in comparing functions u and v = φ(u), where one then obtains the striking results: Part (a) is just an application of the chain rule, with part (b) then following from part (a) when u = φ
1, φ = φ
2, and v = φ
2 ∘ φ
1. Finally, part (c) follows from part (b) when one selects φ
1 = φ and φ
2 = φ
−1, so that φ
2 ∘ φ
1 = ι, the identity mapping, with R

ι
 = 0.Footnote 2 ■ Part (a) is termed a 1-cocycle property in the mathematics literature, and shows how composition of functions is transformed under the action of R into the addition operation, an indication of the invariant nature of R. Indeed, since it obeys a 1-cocycle property, one knows that R, unlike its components, u′(y) and u′′(y), must then remain unaffected by some maximal family of transformations of the real line, and in this case that family turns out to be the affine transformations, t(u) = A + Bu for constants A and B (thus including positive cardinal transformations). Since affine transformations t(u) preserve ratios of distance, they preserve the Arrow-Pratt measure of risk aversion, that is, R

t(u)(y) ≡ R

u
(y). All this suggests that, since a positive sign for R is invariant under composition, R should serve as a natural measure by which to rank utility functions by the degree of risk aversion. Diamond and Stiglitz (1974) introduced the compensated approach to defining v more risk averse than
u by adopting the natural requirement that v dislikes any compensated increase in risk for u, defined to be a parametric change of r in the distribution F(y, r) for income y such thatFootnote 3
 Then, parameterizing preferences by ρ, Diamond and Stiglitz (1974) show that \( \int_a^b {{u_{\rho }}(y,{\rho_0})d{F_r}(y,r)} \leqslant 0 \) if and only if \( d{R_{u({\rho_0})}}(y)/d\rho \geqslant 0 \) for all y, which is to say compensated increases in risk for u(ρ
0) are disliked by those with uniformly higher R values, when those changes in risk preferences are small. Taking the limit of (3) as v approaches the given \( u\left( {{\rho_0}} \right) = u \), it is easily seen that \( dR_{{u{\left( {\rho 0} \right)}}} /d\rho = {\left( {dR_{{\varphi {\left( {\rho 0} \right)}}} /d\rho } \right)}{\left[ {u^{\prime } {\left( {\rho 0} \right)}} \right]} \), where φ(ρ)(u) = u(ρ) and \( \varphi \left( {{\rho_0}} \right)(u) = u \) is the identity map, and thus the condition on the change in preferences can be expressed equivalently as \( d{R_{\varphi \left( {{\rho_0}} \right)}}/d\rho \geqslant 0 \). From part (b) of Proposition 1, it is then easy to see that if the v more risk averse than u ranking is defined in terms of R

φ
 > 0 uniformly over y, given v = φ(u), then that ranking is indeed transitive, as one would want of such a relation. An additional property, more peculiar to the special nature of the more risk averse comparison, but which is liable also to be desired, is that when the ranking v less risk averse than u is defined in terms of R

φ
 < 0, this ranking should coincide with there being a function ψ (the inverse function φ
−1) from v to u such that \( {R_\psi } = {R_{{\varphi^{ - 1}}}} > 0 \), so that v less risk averse than u would be equivalent to u more risk averse than v in all possible senses. We refer to this property as invertibility of the ranking. The fact that invertibility holds for the Arrow-Pratt measure is an immediate consequence of part (c). Keenan and Snow (2009) show that, for large changes in risk preferences, it is the condition φ′′ < 0 that characterizes an increase in risk aversion in the Diamond-Stiglitz sense that v then dislikes all compensated increases in risk for u. Since the sign of φ′′ must equal the sign of R

φ
, given positive monotonic utilities, there is then complete compatibility between the condition \( d{R_{\varphi ({\rho_0})}}/d\rho > 0 \), established by Diamond and Stiglitz in the case of small preference changes, and the required condition, φ′′ < 0, in the case of large preference changes. While an obvious point in this second-order case, one should compare this equivalence to what happens in the higher-order case of downside risk aversion considered next.Footnote 4
",25
44.0,2.0,Journal of Risk and Uncertainty,23 December 2011,https://link.springer.com/article/10.1007/s11166-011-9133-1,Belief elicitation in the presence of naïve respondents: An experimental study,April 2012,Li Hao,Daniel Houser,,,Male,Unknown,Mix,,
44.0,3.0,Journal of Risk and Uncertainty,12 May 2012,https://link.springer.com/article/10.1007/s11166-012-9144-6,Avoiding the curves: Direct elicitation of time preferences,June 2012,Susan K. Laury,Melayne Morgan McInnes,J. Todd Swarthout,Female,Unknown,Unknown,Female,"We begin by assuming exponential discounting and an additively-separable intertemporal utility function. In the typical discount rate model, an individual decides between Option A which yields extra income M

t
 at time t and Option B which yields M

t + τ
 at time t + τ by choosing the option with the larger present value. We can express these present values as
 and
 where ω is the time invariant amount of background consumption, δ is the discount rate, and U(·) is the per-period expected utility function. By equating the present value expressions in Eqs. 1 and 2, we create the following indifference condition
 and can now solve for the discount rate δ. If we further assume the utility function is linear in ω (as most discount rate studies do) this equation reduces to
 Equation 4 has been the basis for many previous time preference studies. If, however, preferences are not linear, then Eqs. 3 and 4 clearly are not the same. Any analysis incorrectly assuming linearity and applying Eq. 4 will yield upwardly-biased discount rate estimates. To better illustrate this bias, consider a person who is indifferent between $1 now and $2 in one year. This indifference condition implies an annual discount rate of 100% over monetary amounts. However, concave preferences imply a diminishing marginal utility of money, and so the utility of $2 in one year is actually less than twice the utility of $1 now. This, in turn, results in an annual discount rate over utility values of less than 100%. One solution to this upward bias of discount rate estimates is to obtain an estimate of the curvature of the utility function and then apply it to Eq. 3. An alternative solution that is the focus of this paper is to apply the binary lottery payoff procedure first introduced by Roth and Malouf (1979) to the elicitation of discount rates. Continuing with Eq. 3, we now hold the amount of extra income constant \(\left( M_t = M_{t+\tau} = M \right)\) and instead vary the probability that the payment is made. Let p

t
 be the probability of receiving extra income M at time t and p

t + τ
 be the probability of receiving extra income at time t + τ. An individual is indifferent between these two options if
 Without loss of generality, we let \(U\left( \omega \right) = 0\) and \(U\left( \omega + M \right) = 1\), and Eq. 5 reduces to
 This expression now provides us an approach to estimate discount rates without having to parameterize and estimate the utility function. In Section 2.1 we develop a simple elicitation procedure for individual discount rates in a manner similar to the widely-used risk aversion elicitation procedure of Holt and Laury (2002). Of course, this approach maintains the expected utility assumption that requires linearity in the objective probabilities.Footnote 2 Alternative assumptions about preferences, such as cumulative prospect theory, allow for probability weighting. We address this possibility in Section 1.4 and empirically in Section 3.3. We also consider consumption smoothing, background consumption, and non-exponential discounting. When evaluating intertemporal consumption choices, it seems unduly restrictive to assume away consumption smoothing. Individuals may prefer to spread the consumption of M over several periods. We consider the dual-self model of impulse control proposed by Fudenberg and Levine (2006) and introduced into the context of discounting by Andersen et al. (2008). This model assumes individuals succumb to temptation for immediate gratification when offered short-term gains but are able to plan and smooth consumption when offered gains over a longer term. To apply the dual-self model, we assume that t is sufficiently far in the future that the short-run impulsive “self” is not involved in the decision.Footnote 3 Further, we assume that extra income is smoothed evenly over η time periods. Given these assumptions, we rewrite Eq. 5 as
 We assign \(U\left( \omega \right) = 0\) and Eq. 7 simplifies to
 We next divide both sides of this expression by \(\sum\limits_{k=0}^{\eta - 1} \left(\frac{1}{1+\delta}\right)^k U\left(\omega + \frac{M}{\eta} \right)\), and see the expression simplifies to Eq. 6. Thus, consumption smoothing—as long as consumption is even—does not alter our probability discounting approach for eliciting time preference. In our model, we assume background consumption ω to be constant over the time frame we analyze. This assumption seems reasonable given our experimental design: all payments were made within twelve weeks and within a single semester.Footnote 4 Given constant background consumption, we see from Eq. 6 that our procedure is invariant to the level of background consumption. In contrast, elicitation techniques that require estimating and controlling for the curvature of the utility function (e.g. Andersen et al. 2008; Andreoni and Sprenger 2012) are sensitive to the level of background consumption. We report a sensitivity analysis of such estimation to changing levels of background consumption in Section 3.2. We assume exponential discounting, but hyperbolic discounting models can also be considered. For example, Keller and Strazzera (2002) apply the hyperbolic model axiomatized by Harvey (1986) to obtain the hyperbolic equivalent of Eq. 4 in which \(\left( \frac{1}{1+\tau} \right)^{\delta^\prime}\) replaces \(\left( \frac{1}{1+\delta} \right) ^ \tau\). Unless utility is linear in wealth, this approach yields upwardly biased estimates of hyperbolic discount rates. Our approach combined with the assumption of hyperbolic discounting yields unbiased estimates of δ
′. We now consider how our approach is affected if we relax the assumption of linearity in the probabilities by allowing probability weighting. Suppose we assume that cumulative prospect theory (hereafter CPT) (Tversky and Kahneman 1992) characterizes preferences rather than expected utility theory. Following CPT, we assume that w(p) is the probability weighting function and V
 + (m) is the value function for gains, with V
 + (0) = 0 and V
 + (M) increasing in M. CPT does not provide guidance about how to evaluate prospects that offer both immediate and future payouts, so we maintain our assumption of exponential discounting for future values and temporal separability. Given these assumptions, Eq. 5 becomes
 More intuitively, because \(V_+ \left( 0 \right) = 0\), this expression becomes
 which shows that an individual is indifferent when the probability-weighted value of winning M at time t is equal to the discounted probability-weighted value of winning M at time t + τ. Finally, dividing both sides by \(V_+ \left( M \right) > 0\) gives
 This expression is our CPT analogue to Eq. 6 where the probabilities have been replaced with the weighted probabilities. The advantage of using this expression to estimate discount rate is that we can avoid estimating the value function V
 + (m). We will, however, need to estimate w(p), and that will require additional parametric assumptions. For example, we could assume Prelec’s (1998) probability weighting function \(w\left( p \right) = e^{-\left(-\ln p\right)^\gamma}\) and would then need to jointly estimate γ and δ. It is also worth noting that the importance of neglecting probability weighting will depend on the ratio w(p

t
)/w(p

t + τ
), and this will vary over the range of probabilities used in the elicitation. In our case, we have set p

t
 = 0.5 and 0.5 < p

t + τ
 < 0.67. If w(p) is well approximated by a ray through the origin over this range—that is, if there exists α such that w(p) = αp over the required range for p—then ignoring probability weighting will not appreciably bias our estimates.",41
44.0,3.0,Journal of Risk and Uncertainty,15 April 2012,https://link.springer.com/article/10.1007/s11166-012-9143-7,Updating beliefs with imperfect signals: Experimental evidence,June 2012,François Poinas,Julie Rosaz,Béatrice Roussillon,Male,Female,Female,Mix,,
44.0,3.0,Journal of Risk and Uncertainty,06 May 2012,https://link.springer.com/article/10.1007/s11166-012-9141-9,"Decreasing absolute risk aversion, prudence and increased downside risk aversion",June 2012,Liqun Liu,Jack Meyer,,Unknown,Male,Unknown,Male,"Menezes et al. (1980) define an increase in downside risk using combinations of Rothschild and Stiglitz (1970) mean preserving spreads and contractions. The definition they present requires that these increases and decreases in risk occur in such a way that risk is unambiguously transferred from the right to the left in the support of the random variable. The total amount of risk is left unchanged. MGT then show that this intuitive definition can be characterized by conditions on the cumulative distribution functions, F(x) and G(x), representing the two random alternatives. These conditions on the CDFs are given below and are used here as the MGT definition of an increase in downside risk. CDF G(x) has more downside risk than F(x) if and only if:
 
\( \int {_{\text{a}}^{\text{b}}\left( {{\text{G}}\left( {\text{x}} \right) - {\text{F}}\left( {\text{x}} \right)} \right){\text{dx}} = 0} \)
 
\( \int {_{\text{a}}^{\text{y}}\int {_{\text{a}}^{\text{x}}\left( {{\text{G}}\left( {\text{s}} \right) - {\text{F}}\left( {\text{s}} \right)} \right){\text{ds}} \cdot {\text{dx}} \geqslant 0} } \) for all y in [a, b] with equality holding at y = b and > 0 holding for some y in (a, b). In this definition the supports of the random variables are assumed to be contained in a finite interval denoted [a, b] and that assumption is maintained here. The first condition, part a), requires the two random alternatives to have the same mean value. The second condition defines a downside risk increase in that it requires the risk increases to unambiguously occur to the left of the risk decreases. The condition that equality holds at y = b requires the risk changes in total to add to zero; that is, the risk increases and decreases are equal in size. The profession has accepted this definition and characterization of an increase in downside risk. Following this characterization of an increase in downside risk, MGT demonstrate the following theorem. This theorem provides one link between downside risk increases and aversion to those increases by decision makers with utility functions displaying a positive third derivative. F(x) is preferred or indifferent to G(x) for all decision makers with u(x) satisfying u′′′(x) > 0 if and only if G(x) has more downside risk than F(x). Keenan and Snow (2009) demonstrate a general theorem for expected utility preserving downside risk increases, which for the special case of u′(x) = 1, reduces to Theorem 2 given below. This theorem also provides a link between downside risk increases and the third derivative of utility. All increases in downside risk result in lower expected utility for u(x) if and only if u′′′(x) ≥ 0. Keenan and Snow assume a weak rather than strong inequality on the third derivative of utility, and also assume that u(x) is positively monotonic, a condition which MGT do not assume. This assumption, that u′(x) > 0, is also made here so that division by zero is not an issue when examining the measure of absolute risk aversion. These two theorems clearly establish a connection between downside risk aversion and the sign of the third derivative of utility. In addition, Eeckhoudt and Schlesinger (2006) find that positive third derivative can also be characterized by preference for combining “good” with “bad”. As a result the sign of the third derivative of utility has been an important feature of the extensions of the work of MGT. Keenan and Snow (K-S) (2009) note that u′′(x) ≤ 0 characterizes risk aversion and that Pratt (1964) has shown that ∅′′(u) ≤ 0 when v(x) = ∅(u(x)) characterizes increased risk aversion. Using this, and the fact that u′′′(x) ≥ 0 characterizes downside risk aversion, quite naturally K-S suggest the following definition of increasing downside risk aversion. A decision maker with utility function v(x) is more downside risk averse than a decision maker with utility function u(x) if and only if v(x) = ∅(u(x)) where the transformation function ∅(u) satisfies ∅′′′(u) ≥ 0. Keenan and Snow go on to support this definition with extensive analysis. They also expend considerable effort in attempting to obtain a measure of the intensity of downside risk aversion that is consistent with this definition of increasing downside risk aversion. They finally settle on a measure denoted s(x), but find a uniformly larger s(x) is only a sufficient, not a necessary and sufficient, condition for a utility function to be more downside risk averse by Definition 2. K-S conclude that “there is no measure that fully characterizes greater downside risk aversion.” (2009, p.1097). Upon examining Definition 2 further, it becomes apparent that this definition does not lead to a transitive or asymmetric partial order over utility functions. That is, if w(x) is more downside risk averse than v(x) and v(x) is more downside risk averse than u(x) then it need not be the case that w(x) is more downside risk averse than u(x). Furthermore, there exist distinct utility functions such that each is more downside risk averse than the other. To show that transitivity does not hold for Definition 2, one can verify that when v(x) = ∅(u(x)) and w(x) = ψ(v(x)) then (ψ◦∅)′′′ = ψ′′′∅′3 + 3ψ′′∅′∅′′ + ψ′∅′′′. Thus, ψ′′′(v) ≥ 0 and ∅′′′(u) ≥ 0 cannot guarantee (ψ◦∅)′′′ ≥ 0 in general. For example, let ∅(u) = u1/2, ψ(v) = v3, and therefore (ψ◦∅)(u) = u3/2. Then (ψ◦∅)′′′(u) < 0 even though ∅′′′(u) > 0 and ψ′′′(v) > 0. Additionally, Definition 2 makes it possible for two distinct decision makers to each be more downside risk averse than the other. That is, the partial order over risk preferences is not asymmetric. To see this let u(x) = ecx and v(x) = edx. This implies that ∅(u) takes the form ∅(u) = ud/c. It can be readily checked that ∅′′′(u) > 0 for both d/c = 3 and d/c = 1/3. Therefore the two distinct utility functions e3x and ex are each strictly more downside risk averse than the other under Definition 2.Footnote 4 This lack of transitivity and asymmetry are serious flaws for orders. Furthermore, because the partial order is not transitive, there cannot be a numerical intensity measure for downside risk aversion that is associated with Definition 2 for all changes in risk aversion. In order to add to the discussion of increased downside risk aversion, Definition 2 must be replaced by one that is transitive. The next section uses more decreasing absolute risk averse to define increased downside risk aversion. First, to justify this definition, a connection is established between aversion to downside risk and decreasing absolute risk aversion.",47
44.0,3.0,Journal of Risk and Uncertainty,19 April 2012,https://link.springer.com/article/10.1007/s11166-012-9142-8,Allais for all: Revisiting the paradox in a large representative sample,June 2012,Steffen Huck,Wieland Müller,,Male,Male,Unknown,Male,"We administer the original “Allais questions,” which consist of two pairwise lottery choices. Consider the following two choice problems. First, a subject is asked to choose between lotteries A and A
 ∗  where
 Second, a subject is asked to choose between lotteries B and B
 ∗  where
 Of the four possible answers AB, A
 ∗ 
B
 ∗ , AB
 ∗ , and A
 ∗ 
B only the first two are consistent with expected utility theory (henceforth, EUT) whereas the last two are not.Footnote 4 Many laboratory experiments have shown that violations of EUT are frequent and that a larger share of subjects violating EUT chooses AB
 ∗  instead of A
 ∗ 
B.Footnote 5
 We have six simple treatments using a between-subjects design. To introduce these treatments, consider the following lotteries over three outcomes of monetary payoffs with probabilities as above, i.e., A = (0,1,0), A
 ∗  = (.01,.89,.10), B = (.89,.11,0), B
 ∗  = (.90,0,.10). Our three treatments were then as follows:
 Treatment HighHyp: Original Allais questions with high hypothetical payoffs of € 0, € 1 million,and € 5 million. Treatment LowHyp: Allais questions with low hypothetical payoffs of € 0, € 5, and € 25. Treatment LowReal: Allais questions with low real payoffs of € 0, € 5, and € 25. Note that the amounts of money we use in these treatments are the same as in Conlisk (1989) with the sole difference that he used dollars instead of euros. For all three treatments we had two sub treatments reversing the order of decisions. As we do not find any order effects in the data we pool the data throughout. We collected data from a representative sample of the Dutch population. The experiments were conducted by CentERdata—an institute for applied economic and survey research for the social sciences—that is affiliated with Tilburg University in the Netherlands. CentERdata carries out its survey research mainly by using its own panel called CentERpanel. This panel is Internet based and consists of some 2000 households in the Netherlands which form a representative sample of the Dutch population.Footnote 6 One of the advantages of the CentERpanel is that the researcher has access to background information for each panel member such as demographic and financial data. Every weekend, the panel members complete a questionnaire on the Internet from their home. After logging on to our experiment, panel members were randomly assigned to one of the six different treatments introduced above. After being informed about the nature of the experiment, subjects decided whether or not to participate—as common with many modules of the panel. For participating subjects, the next screen introduced an example of a pair of lotteries (which were referred to as “Options”). Subjects were told that their task would be to express preference for one of the two lotteries and, additionally, how the preferred lottery would be executed.Footnote 7 When subjects indicated that they were ready to start the experiment, they were, in two consecutive screens, presented with their two Allais questions. Only after answering both Allais questions, the two preferred lotteries were played out (by the computer) and subjects were informed about the outcome of their two preferred lotteries. In the treatments with real monetary payments, subjects were paid according to the outcomes in both of their preferred lotteries.Footnote 8
 In total 1676 members of the CentERpanel logged on to our experiment. Of the subjects logging on, 1426 (85.1%) subjects decided to participate in our experiment while 250 (14.9%) subjects decided not to participate. Table 1 shows descriptive statistics of our sample. The column labeled “Participation” in Table 1 shows descriptive statistics of participating subjects in each of the three main treatments as well as statistics of subjects who chose not to participate in the experiment. The data in Table 1 is grouped according to gender, age, education, occupation and income. (The column labeled “Violation” shows statistics for participating subjects violating or not violating EUT, respectively, which we will analyze further below. It also contains tests on the role of socioeconomic characteristics for EUT violation which will also be discussed later.)
 Concentrating on descriptive statistics for participating subjects in Table 1, we note that by and large most variables are relatively identically distributed across treatments. However, in some of the age and income brackets as well as in the category savings account, there is some more variation. A comparison of the descriptive statistics in the columns describing participating subjects with those of non-participating shows that there are no big differences except for the age categories. Basically, older people appear to be a little more reluctant to participate. Since this causes concern about sample selection problems, we ran for all regressions reported below Heckman (1976) selection models using the variable “Ratio” as one of the exclusion variables. The variable “Ratio” measures the proportion of questionnaires completed by panel members in the three months proceeding our experiment. This variable can be assumed to affect the participation decision but not the decisions taken in the experiment. For none of the regressions we found evidence for a selection bias.Footnote 9
",54
45.0,1.0,Journal of Risk and Uncertainty,01 July 2012,https://link.springer.com/article/10.1007/s11166-012-9145-5,Does the WTA/WTP ratio diminish as the severity of a health complaint is reduced? Testing for smoothness of the underlying utility of wealth function,August 2012,Susan Chilton,Michael Jones-Lee,Hugh Metcalf,Female,Male,Male,Mix,,
45.0,1.0,Journal of Risk and Uncertainty,15 July 2012,https://link.springer.com/article/10.1007/s11166-012-9146-4,Failing to learn from experience about catastrophes: The case of hurricane preparedness,August 2012,Robert J. Meyer,,,Male,Unknown,Unknown,Male,"Decisions about how much to invest in protection against extreme events are not easy to make. Unlike decisions involving frequently-encountered risks, the probabilities associated with hazards such as earthquakes and hurricanes are often ambiguous, and the conditional effectiveness of investments in protection are rarely known with any certainty. Hence, while a new resident moving to coastal Florida may see wisdom in retro-fitting their home with window protection, there may no objective guidance to help in assessing which window technology would be most cost-effective to install, or indeed whether the investment should be made at all. Yet, the fact that protective decisions may be difficult to make does not necessarily imply that they will be made poorly by residents, particularly in the long run. One of the major findings of both theoretical and empirical work in decision making in complex tasks is that knowledge of the normative basis of a decision—such as knowing the actuarial odds of a hazard—is by no means a necessary condition for behavior to appear optimal, at least in the aggregate (e.g., Fudenberg and Levine 2000). Specifically, as long as individuals are placed in environments where optimal actions are more reliably rewarded than sub-optimal actions, decisions made using simple trial-and-error rules will often converge to optimality, even among naïve decision makers who have few insights into normative behavior (e.g., Kalai and Lehrer 1993; Meyer and Hutchinson 2001). This result, however, comes with a critical catch: if the decision environment is not ideal for learning—such as if feedback is rare and noisy—one can no longer be assured that trial-and-error policies will discover optimal equilibria (e.g., Camerer et al. 2001). Of particular relevance to the current investigation is recent work by Weber et al. (2004) and Shafran (2011) who provide examples of how reinforcement rules can distort asymptotic choice frequencies when applied to repeated decisions involving rare events (see also March 1996). In an effort that has parallels to the present, Shafran (2011) posed experimental participants with a series of repeated gambles in which they could pay a fixed fee to reduce the probability of a loss by a known amount. One of the key findings was that even though participants were fully informed about the odds of each gamble, they nevertheless acted as if they updated their beliefs about whether they would win or lose based on the most recently-observed outcomes. As a consequence, participants were more likely to buy protection when faced with higher-probability, lower-consequence, gambles—where losses were observed more frequently—compared to lower-probability, higher-consequence, ones. The tendency to make decisions by looking at the success or failure of recent outcomes provides a potential mechanism for explaining why investments in protection against rare events may be difficult to sustain in real-world settings. By their nature, investments in protection are counter-reinforcing; because hazards are rarely encountered, risk-taking will rarely be punished by nature while prudence rarely rewarded. A resident who cancels his/her flood insurance for a year in an effort to save money, for example, is unlikely to be punished by experiencing a flood that year (or the next), while the community that invests large amounts in the construction of levees may fail to see tangible benefits for years (e.g., Browne and Hoyt 2000; Kunreuther et al. 1985). This lack of reinforcement may, in turn, be exacerbated by the censoring effect of the investments themselves; the more successful a protective investment is in precluding harm, the more difficult it becomes to directly observe its benefits. In such cases, reinforcement relies on the ability of the decision maker to imagine the counter-factual of what would have happened had the investment not been made. If individuals lack effective counter-factual reasoning skills, investments in protection may be, paradoxically, self-extinguishing; the more effective an investment in preventing harm, the more difficult it becomes for decision makers to recall the original need for the investments (e.g., Meyer 2006). To illustrate these points, consider what a simple reinforcement learning process would predict about how investments in protection would evolve over time in a task that we call the “storm shutter game.” It works like this: a decision maker lives in a community that is prone to periodic impacts from major windstorms. At the start of each storm season, the decision maker has the opportunity to install shutters at a cost c that he or she knows will eliminate the risk of losses from potential storms. Specifically, if a storm occurs and shutters are in place the decision maker suffers no damage, but if it occurs and no shutters are in place he or she will experience a loss (with certainty) l>c. In each year there is a constant probability p < 1of the community being impacted by a storm, and p is sufficiently large that annual investments in mitigation are long-term optimal; i.e., in each year E(l-c|p) > 0. To explore whether individuals would learn this optimal long-term policy by trial-and-error alone, consider the case where the probability that the decision maker will invest in mitigation at a given time t, P

t
, evolves as where α is inertia parameter that controls the rate of learning and x

t-1
 is a (0,1)-bounded measure of the degree of positive reinforcement received in year t-1 about the benefit of shutters. Expression (1) should be recognized as a variant of the family of linear operator models that have been widely used to characterize associative learning in psychology as well as repeated play in games (e.g., Bush and Mosteller 1953; Erev and Barron 2005; Weber et al. 2004). To extend these formulations to the kind of learning that might take place in natural hazard contexts, we assume that reinforcement (x

t-1
) can be received through one of two routes: by direct experience with real or precluded losses if a storm occurred in year t-1 or by fictitious experience if a storm did not occur. Fictitious experience reflects the strength of the decision maker’s beliefs about the losses that could have occurred had a storm occurred the previous year without mitigation in place. A simple assumption about this feedback is that in each year x

t-1
 takes the form: where s

t-1
 = 1 if a storm was experienced in year t-1 and 0 otherwise, and d

t-1
 is a (0,1)-bounded measure of the imagined (or counter-factual) loss in a year when there was no storm. It should be transparent that if counterfactual beliefs are constant (i.e., d

t-1

=d for all t) and less than one—that is, imagined losses are less effective in encouraging investment than real ones—a learning process described by (1) and (2) will yield a temporally-oscillating pattern of investments where the likelihood of shutters increases to αP

t-1

+(1-α) in years after a storm but decreases to αP

t-1

+(1-α)d in years after the absence of a storm, with a long-term mean of pd+(1-p). These dynamics are illustrated in Fig. 1, which plots the learning path yielded by Eq. (1) for the case of a simulated 50-year time horizon in which p = .2, the fictitious reinforcement d equals the baseline odds of a storm (0.2), and shutter probabilities are moderately inertial (α = 0.6). The figure displays a pattern of forgetfulness similar to the anecdotal examples provided at the outset, where a storm experience in year t acts to boost mitigation in year t + 1, but this diligence decays as the years since the last storm increase. Simulated evolution of shutter probabilities given a simple reinforcement learning process. “1” on the ordinate axis denotes a storm incident in a given year It is important to emphasize, of course, that this lack of learning is not a necessary property of expressions (1) and (2): they could allow a resident to learn a policy of persistent shutter installation simply if one is willing to make more favorable assumptions about the strength of fictitious reinforcement in periods without hazards. For example, Fig. 2 plots the learning path that Eq. (1) yields for the same simulated 50-year horizon in the case when counter-factual beliefs about damage (d

t
) evolve as a cumulative function of past experienced losses, where \( {d_t} = \left( {{d_{{t - 1}}} + {s_{{t - l}}}} \right)/t \)
. Here, trial-and-error learning continues to evolve as an oscillating, “step forward, step back” process, but is now marked by gradual convergence toward a policy of persistent investment in mitigation. Modification of the learning path in Fig. 1 when fictitious reinforcement is monotonically increasing in cumulative storm experience Which of these characterizations is likely to offer the better portrait of reality? The answer is far from clear. On one hand, because natural environments afford decision makers with a far richer set of cues that could potentially aid learning than is assumed in these examples, one might predict that the second, more optimistic, view will be the better reflection of reality. In natural settings people are capable of having long memories, and have frequent opportunities to indirectly learn by observing the experiences of others. One does not have to experience an earthquake in one’s own community, for example, to understand their dangers, and news reports of destructive quakes and hurricanes in other areas can serve as compelling reminders of the value of strong building codes and diligent mitigation practices. On the other hand, one could just as easily argue the opposite, that in natural settings such favorable factors would be offset by psychological influences that act to inhibit learning to an even greater degree than modeled above. First, while individuals may have long memories, the long real-time intervals that typically arise between catastrophic events implies that ongoing-feedback will be dominated by observations of how they are not needed, and/or prompt beliefs that the base rate risk of the hazard has abated (the gambler’s fallacy; see, e,g., Kahneman and Tversky 1973). Likewise, while opportunities for indirect learning may well abound, it is not clear that remote observation is an effective replacement for direct experience. For example, in experimental work on “near miss” effects, Dillion and Tinsley (2008) found that close encounters with negative events that did not impose damage (but could have) served to deflate rather than reinforce propensities to invest in protection by encouraging misplaced optimism that future threats will also be near-misses. Likewise, similar findings showing that events affecting others do little to influence beliefs about personal risk have been offered by Raghubir and Menon (1998), Weinstein (1980), and others. Finally, even if forgetting effects are small in natural settings, investments in protection may be asymptotically limited by the inability of experience to cure myopic reasoning (or hyperbolic discounting) biases—the tendency to focus more on up-front costs than longer-run benefits when making inter-temporal tradeoffs (see, e.g., Loewenstein and Prelec 1992; Thaler 1980). Because, by definition, all protective decisions involve a decision to incur up-front costs to achieve a delayed benefit, even decision makers who accurately anticipate the long-run material consequences of failing to invest in protection may still under-invest in the short run due to a tendency to excessively focus on the immediate disutility of cost outlays (e.g., Kunreuther et al. 2012). In the next section we describe the findings of a program of empirical work designed to shed some light on this empirical uncertainty. The goal of the work is to examine the ability of individuals to learn optimal strategies for protective investment in a laboratory setting where optimal behavior can be objectively defined, experimental control can be exerted over the magnitude and incidence rates of hazards, and available cues in the learning environment can be precisely measured. In the work we examine both the degree to which empirical learning patterns display short-term feedback effects consistent with trial-and-error learning, and the degree to which these decisions show evidence of counter-factual reasoning, where decisions about how much to invest in protection are supported by beliefs about losses that could have occurred. We report the findings of two studies: one focusing on recurrent decisions to make short-term investments in protection against immediate threats (Study 1), and one focusing on longer-term mitigation against temporally distant threats (Study 2).",34
45.0,1.0,Journal of Risk and Uncertainty,23 August 2012,https://link.springer.com/article/10.1007/s11166-012-9148-2,Single-year and multi-year insurance policies in a competitive market,August 2012,Paul R. Kleindorfer,Howard Kunreuther,Chieh Ou-Yang,Male,Male,Unknown,Male,"We consider an insurance market operating over two periods to cover a set of households exposed to a common catastrophic risk such as earthquakes or hurricanes. The insurance market is assumed to be competitive with free entry and exit, but subject to solvency regulation. Risk bearing capital is obtained from premium income and reinsurance. The price of reinsurance in period 1 is known, but the price in period 2 is uncertain, and is specified by a binary random variable with a specified increase or decrease relative to the price in period 1 which depends on whether the catastrophic risk occurs in period 1. Two types of products, single-period policies and multi-period policies, compete for consumer demand for insurance. Homeowners can purchase either no insurance, single-year policies in one or both periods, or a multi-year policy purchased at the beginning of the first period and covering both periods. The competitive insurance price is determined so that it covers expected losses, marketing and operating expenses, plus the cost of risk capital necessary to provide protection against insolvency, where the level of the required capital is determined exogenously by the insurance regulator. We assume that households are identical in terms of their exposure to the hazard, but with some correlation in the losses, as would be the case if a natural disaster damaged a number of insured homes. Denote the set of potential homeowners in the market by A. Homeowners are assumed to be risk averse with a ∈ A being a scalar index of risk aversion and with two–period separable risk preferences given by
 where U(x, a) is concave increasing in x, and x
1 and x
2 are monetary outcomes in periods 1 and 2. Note that for simplicity we neglect discounting of utility in period 2. While A represents the set of potential homeowners, the actual distribution of homeowner risk aversion will be specified by the counting function defined for any a ∈ A by
 To illustrate, suppose the number of homeowners is 100, of which 40 have a = 0.1 and 60 have a = 0.2. Then, G(a) = 0 for a <0.1, G(a) = 40 for 0.1 ≤ a <0.2, G(a) = 100 for a ≥0.2. We make the following assumptions concerning the hazard and the policies offered in the insurance market to homeowners.
 Only full insurance is offered and each household a ∈ A faces the same annual/period risk \( \widetilde{X}(a) \) of loss, distributed according to the generic cumulative distribution function (cdf) F
0(x, μ, σ, ρ), with mean \( E\left\{ {\widetilde{X}(a)} \right\} = \mu \) and variance \( VAR\left\{ {\widetilde{X}(a)} \right\} = {\sigma^2} \), where ρ ≥ 0 is a scalar index which is intended to measure the extent to which the loss distribution for a book of business (BoB) made up of properties from the set A is correlated. The loss distribution is assumed to be identical for both periods, and statistically independent between the periods. The impact of the index ρ will be specified below; it may be thought of as an index of the cost of reinsurance cover for a BoB made up of properties from the set A. We use ρ to model the impact of correlated losses on standard reinsurance pricing models with constant or increasing loading factors. The reinsurance will be an Excess of Loss (XoL) contract with fixed upper and lower limits designated as L
1(n) and L
2(n) respectively. Insurers offering single-year (SY) policies may cancel any policy at the end of the first period in response to an increased cost of risk capital that leads them to want to reduce their BoB. Homeowners are aware of this possibility and assume that there is a probability q ∈ (0, 1) of cancellation, with an ensuing transaction cost of τ ≥ 0 to search for new coverage alternatives for period 2 if their SY policy is canceled at the end of period 1. Homeowners face the choice of purchasing either two single-year policies or a multi-year policy to cover the risks they face over the two-period horizon of interest. Of course, they may also elect to buy no insurance (NI) in one or both periods. At the beginning of period 1, homeowners must either choose an MY policy covering both periods, or they must plan on some other sequence of SY policies or NI decisions. In doing so, we assume that homeowners have complete information on the prices they will face.
 Homeowners know the prices for all policies in all states of the world at the beginning of period 1. For the MY policy, the price per period price PM is constant over both periods. For the SY policy, price in the first period is denoted PS1, and the state-contingent price in the second period is denoted \( P_{{S2}}^w \), where w ∈ {d, u} reflects the state of the world in terms of reinsurance/capital costs with probability of state d = φ ∈ [0, 1] and probability of state u = 1−φ. The consequences of these alternative states for the insurer are described below, but their general import is that the cost of reinsurance in period 2 will go down in state d and go up in state u relative to period 1. At the beginning of period 1, there are three alternatives available to a homeowner:
 Purchase an SY policy for period 1, at price P

S1
, possibly facing cancellation of this policy at the end of period 1, with probability q and with resulting transaction costs τ. Purchase an MY policy at price per period of P

M
 covering both periods 1 and 2. Purchase No Insurance (NI). At the beginning of period 2, after the state of the world w ∈ {d, u} is known, a homeowner faces the following choices:
 If the homeowner chose either an SY policy or NI in period 1, then the homeowner can either choose NI or purchase an SY policy for period 2 at price \( P_{{S2}}^w \) where the price can either decrease to \( P_{{S2}}^d \) or increase to \( P_{{S2}}^u \) depending on reinsurance capital costs. The homeowner who chose an MY policy in period 1 can continue to be covered under this MY policy or can cancel it with a cancellation fee ψ ≥ 0. The cancellation fee is set by the insurer so that it breaks even at the end of period 2. If there were no cancellation costs associated with an MY policy, then all homeowners will want to switch to an SY policy if the following two conditions hold:
 the price of an SY policy decreased in period 2 so it was less than PM; and the price differential between the MY and SY policy is greater than the transaction cost τ of purchasing a new policy. Homeowner’s choice problem in period 2 In this case, the MY insurer would offer coverage only in the state of the world w = u. As we show in Appendix A, P

M
 is less than the MY insurer’s average cost at w = u, so that the MY insurer would suffer a loss in the process. It will have priced its policy under the assumption that it would recover its costs from revenues generated in both periods, but could not recoup these costs (some of which will be sunk in period 1) because its policyholders abandoned ship at the end of period 1. Hence, for MY insurance to be viable, the cancellation fee ψ imposed by the MY insurer has to satisfy: \( \psi \geqslant {P_M} - P_{{s2}}^d - \tau \).Footnote 1 In this case, all homeowners would maintain their MY policy in period 2. For our benchmark analysis in this paper, we assume, per A3, that homeowners are perfectly informed about prices and the probability distribution on the states of the world w ∈ {d, u}, that is, they know φ. We assume that their beliefs about the cancellation probability q are fixed and independent of the actual BoB changes by insurers.Footnote 2 Figure 1 shows the relevant choices for a homeowner in period 2. Given our assumption that homeowners will not cancel MY policies in period 2, the state-dependent insurance decision I
2(a, w) of homeowner a ∈ A, w ∈ {d, u} at the beginning of period 2 will be the following:
 
I
2(a, w) = MY if I
1(a) = MY, with resulting period 2 utility of U(−P

M
, a) 
\( {I_2}\left( {a,w} \right) = SY_2^w\;if\;{I_1}(a) \ne MY\;and\;U\left( { - P_{{S2}}^w,a} \right) \geqslant U\left( {CE\left( {NI,a} \right),a} \right) \) with resulting period 2 utility of \( U\left( { - P_{{S2}}^w,a} \right) \)
 
\( {{I}_{2}}\left( {a,w} \right) = NI\:if\:{{I}_{1}}(a) \ne MY\:and\:U\left( { - P_{{S2}}^{w},a} \right) < U\left( {CE\left( {NI,a} \right),a} \right) \) with resulting period 2 utility of U(CE(NI, a), a) where the certainty equivalents (CEs) of the various policies offered, MY and \( SY_2^w,w \in \left\{ {d,u} \right\} \), are CE(MY) = −P

M
 and \( CE\left( {SY_2^w} \right) = - P_{{S2}}^w \). CE(NI, a) is characterized by \( U\left( {CE\left( {NI,a} \right),a} \right) = E\left\{ {U\left( { - \widetilde{X}(a),a} \right)} \right\} \).Footnote 3
 The above three conditions can be interpreted in the following manner. A homeowner will have an MY policy in period 2 only if he purchased an MY policy in period 1 (D2-i). A homeowner will purchase an SY policy in period 2 if she did not purchase an MY policy in period 1 and is sufficiently risk averse so that the utility of having full insurance is greater than the expected utility of having no insurance in period 2 (D2-ii). She will be in state NI if the expected utility of having no insurance in period 2 exceeds the utility of SY (D2-iii). Demand in period 2 [D
2(z, w)] for the policy options \( {Z_2} = \left\{ {MY,SY_2^w,NI} \right\} \) follows directly from the above. Let ∆2(z, a, w) = 1 if I
2(a, w) = z and ∆2(z, a, w) = 0 otherwise, where \( z \in \left\{ {MY,SY_2^w,NI} \right\} \). Then,
 Equation (2) just allocates homeowners in period 2 to an MY, SY or NI policy as a function of their degree of risk aversion and whether the reinsurance/capital costs are in state d or u. Turning to period 1, recall that an SY insurer can cancel a policy at the end of period 1, and that homeowners believe this will occur with probability q and result in transactions costs τ. Then the certainty equivalent CE(SY
1, a) of a first-period SY policy is characterized by
 Equation (3) indicates that higher values of q reduce the attractiveness of an SY policy because the homeowner is more likely to have her policy canceled and will have to incur a search cost τ to find an insurer who will be willing to sell her a policy in period 2. The optimal period 1 decision (note that we ignore discounting) is then determined through dynamic programming as follows. First, define the expected utilities V
1(z) associated with choosing each of the options z ∈ Z
1 = {MY, SY
1, NI} in period 1 and the optimal state-dependent choice following this in period 2. Then
 where, in view of the possibility of cancellation of the SY policy at the end of period 1, the period 1 expected utility of choosing an SY policy is given by (3). Equation (4) states that a homeowner who purchases MY insurance in period 1 will continue to be insured by the same policy in period 2. Equation (5) states that a homeowner who purchases SY insurance in period 1 incurs the immediate cost of the premium PS1 and, with probability q, may incur an additional transactions cost τ if the policy is canceled at the end of period 1, as reflected in the CE of SY1 given in (3). This same homeowner has the option of buying a second SY policy in period 2 or NI, and will choose the best of these two options in each state of the world w ∈ {d, u}. Equation (6) states that a homeowner choosing NI in period 1 can choose to purchase an SY policy in period 2 or remain uninsured (NI) and will choose the best of these options in each state of the world w ∈ {d, u}. The optimal first-period choice for homeowner a ∈ A is then given by
 The demand in period 1[D
1(z)] for the policy options Z
1 = {MY, SY
1, NI} follow directly from the above. Let ∆1(z, a) = 1 if I
1(a) = z and ∆1(z, a) = 0 otherwise, where z ∈ {MY, SY1, NI}. Then,
 The above characterization of demand for MY, SY policies and NI is general. Final demands for these policies in both periods depend on the distribution of homeowner risk preferences as reflected in their degree of risk aversion characterized by G(a). It also of course depends on the loss distribution \( \widetilde{X} \). In Section 2 we consider the special case where per period losses \( \widetilde{X} \)are normally distributed and the risk preferences of homeowners are of the CARA form, U(x, a) = −e
−ax. We assume a competitive insurance market which consists of two types of firms, those offering SY policies and those offering MY policies. Firms offering SY policies can adjust the size of their BoB at the end of period 1 in response to changes in the cost of reinsurance (i.e., in response to the realized state of the world w ∈ {d, u}). In a competitive equilibrium, the size of the insurer’s BoB is determined by the minimum of the average cost curve for the insurer. SY insurers will therefore cancel some policies at the end of period 1 in the state of the world in which reinsurance rates increase from period 1 to period 2 and will expand their BoB when reinsurance rates decrease. MY insurers do not have this option and must provide coverage in both periods to all homeowners to whom they issued policies in period 1. Reinsurance costs are assumed to depend on the non-negative scalar index ρ > 0, which may be thought of as an index of the fat-tailed nature of the distribution of a BoB of size n from the set A (see A1). A more specific model for reinsurance pricing is discussed in Appendix C. The following assumption summarizes the relationship of the regulated solvency risk level and reinsurance costs for insurers.
 Insurers are required to satisfy a regulatory solvency constraint.Footnote 4 It requires their combined premium revenue plus reinsurance for each period to be sufficiently large to pay all claims with a probability of at least 1−γ
*. This regulatory solvency constraint is similar to a safety first model that insurers often utilize to determine the optimal BoB and pricing decisions. It was first proposed by Roy (1952), examined in the context of the theory of the firm and profit maximization by Day et al. (1971) and applied to insurance by Stone (1973a) and Stone (1973b). Following the series of natural disasters that occurred at the end of the 1980s and in the 1990s, many insurers focused on the solvency constraint to determine the maximum amount of catastrophe coverage they were willing to provide. More specifically they were concerned that their aggregate exposure to a particular risk not exceed a certain level. Today, rating agencies, such as A.M. Best, focus on insurers’ exposure to catastrophic losses as one element in determining credit ratings, another reason for insurers to focus on the solvency constraint (Kunreuther & Michel-Kerjan 2011). For simplicity, we assume that insurers with a BoB of size n meet their solvency constraint by purchasing XoL reinsurance with limits L
1(n), L
2(n), with L
1(n) = nμ (the expected loss cost for a BoB of size n) and L
2(n) set to meet the solvency constraint. Consider an insurer with BoB = {a
1, a
2, …, a
n} and the associated total loss distribution \( \widetilde{X}{\left( n \right)} = {\sum\nolimits_{i = 1}^n {\widetilde{X}} }{\left( {a_{i} } \right)} \), with cdf \( F\left( {L;\widetilde{X}(n)} \right) \). Then, L
2(n) is characterized byFootnote 5
 Figure 2 illustrates the above assumption on reinsurance attachment points and the solvency constraint. Reinsurance contracts are on a per period basis, corresponding to the solvency constraints that are required to be fulfilled in each period.
 Illustrating reinsurance attachment points under Assumption A4 The costs to insurers of providing coverage encompass administrative and selling expenses, loss costs and reinsurance costs, and depend on the size of the BoB (n). Formally, the expected total costs per period for SY and MY policies are given as:
 where the vector of parameter values is given by ζ = (μ, σ, ρ, γ
*, φ, q, τ) and where the elements of total expected cost are: C
0(n) represents administrative costs; C

m
(n) represents marketing costs; μn are total expected losses; and C

s
(n; r, μ, σ, ρ) represents reinsurance cost. Note that, with the exception of marketing costs, all of the elements of total expected cost are identical for SY and MY insurers. Concerning marketing costs, it is assumed that ν ∈ (0, 1] so that these per period marketing costs are likely to be less for an MY insurer since its policyholders in the second period are locked in as a result of first period choices. Thus, if SY and MY insurers were to choose the same BoB in both periods, and if v = 1, then both insurers would have identical total expected cost. However, SY insurers can adjust their BoB from period to period in response to changes in reinsurance costs, while the MY insurer is constrained to offer the same BoB in both periods. Thus, SY and MY insurers will in general face different total expected costs because of potential marketing cost advantages for the MY insurer and potential flexibility advantages of the SY insurer in responding to changing reinsurance costs. Reinsurance costs are net of expected payoffs from reinsurance contracts, which are all of the XoL variety. In other words, the reinsurance cost reflects the additional premium above the expected loss paid by the insurer to the reinsurer for protection against a portion of the loss between the attachment points of the XoL contract. We assume that the average underwriting costs [C
0 (n)/n] are convex and decreasing as n increases to reflect the spreading of fixed costs over more policies. The average marketing costs [C

m
(n)/n] are convex and increasing in n, representing the higher marginal cost of marketing as the insurance territory increases. Losses to the insurer have a mean μ and standard deviation σ. Average reinsurance costs [C

s
(n; r, μ, σ, ρ)/n] are convex in n and are dependent on whether the reinsurer is in state d or u so that r = r(w), w ∈ {d, u}. These costs are also assumed to be increasing in ρ (the fat-tail index) reflecting the need for the reinsurer to hold higher reserves due to an increased probability of experiencing large losses. Appendix A specifies the derivation of the average costs and the equilibrium conditions in a competitive market. Competitive equilibrium in both the SY and MY markets occurs where insurers of each type select a BoB that minimizes their average cost, C

SY
(n; r, ζ)/n, C

MY
(n; r, ζ)/n, with price given by the minimum of the respective average cost curve. The assumptions concerning the elements of average costs discussed in Appendix A assure the existence of the equilibrium for both SY and MY markets. These assumptions also imply a number of intuitively appealing results for the comparative statics of equilibrium prices and BoBs for both MY and SY insurers. For example, since reinsurance costs in period 2 increase (u) or decrease (d) relative to period 1, depending on the state of the world, w ∈ {d, u}, equilibrium prices in the SY market satisfy: \( P_{{S2}}^d < {P_{{S1}}} < P_{{S2}}^u \) and the corresponding optimal BoBs satisfy: \( n_{{S2}}^d > {n_{{S1}}} > n_{{S2}}^u \). Also, the average costs of the MY insurer satisfy: \( AC_{{M2}}^d < {P_M} < AC_{{M2}}^u \) so that the MY insurer has positive profits in state w = d and losses in state w = u.Footnote 6
 The model proposed here suggests a number of trade-offs on both the demand and the supply side in evaluating the survival and efficiency of MY vs. SY policies in competitive equilibrium. On the demand side, MY policies offer a constant price over both periods and therefore are attractive to risk-averse homeowners in avoiding intertemporal price volatility. MY policies also protect homeowners from the transactions costs of policy cancellation (represented by the parameters q, τ above) associated with changes in the equilibrium BoB for SY insurers between periods 1 and 2 that result from changes in the cost of capital and reinsurance. On the supply side, there may be marketing and policy service cost savings associated with MY policies. However, MY policies expose the insurer to increased risk of reinsurance cost volatility, since the MY insurer cannot adjust his BoB in response to the realized state of the world w ∈ {d, u} in contrast to the SY insurer. The ultimate outcome in terms of equilibrium prices and demands for MY vs. SY policies is an empirical matter depending on which of these trade-offs dominates and on the risk preferences of homeowners. The results in the next sections illustrate this for the case of Gaussian loss distributions and CARA preferences.",22
45.0,1.0,Journal of Risk and Uncertainty,05 July 2012,https://link.springer.com/article/10.1007/s11166-012-9147-3,Do administrators have the same priorities for risk reductions as the general public?,August 2012,Fredrik Carlsson,Dinky Daruvala,Henrik Jaldell,Male,Unknown,Male,Male,"In order to learn more about the priorities of the general public and public administrators we constructed two surveys. The first concerned the trade-off between small and large accidents and the trade-off between subjective and objective risks. The second survey was a choice experiment concerning the trade-off between the number of dead and the number of serious injuries at different ages for two accident types. We sent out two surveys to the general public, but because of the limited sample size for administrators a version containing both surveys was sent to them. Thus, the administrator group received a somewhat longer survey than the general public. In the first survey, respondents were asked to make choices between different projects. Both the general public and administrators were asked to assume that they were decision makers and required to choose which policy they would recommend, i.e. not the policy they preferred as private citizens. In doing so, we believe we may interpret the responses as a reflection of the priorities the respondents wish the decision maker to have. In the first question, respondents were asked to choose between two projects, both of which resulted in the same total number of saved lives. One of the projects reduced the number of many small accidents, while the other led to the avoidance of one big accident. The scenario and question are presented in Fig. 1. Scenario and question for small versus large accidents In the second part of the survey, respondents were asked to choose between projects with different effects on the actual number of lives saved and the general public’s perception of the number of lives saved. Again, respondents were asked to make a choice between two projects. Both projects affected fatality risks for a population of 100,000 people. In one case, the general public’s perception of the risk is accurate. In the other case, they overestimate the risk. The scenario and questions are given in Fig. 2. below. In the first question, Project A reduces the actual risk from 5 deaths to 1 per 100,000 inhabitants. Project B is less effective and leads to a reduction in actual risk from 5 deaths to 3 per 100,000. However, in Project B the general public perceives the risk reduction to be much greater than it actually is: from 10 to 3 fatalities. In the second question, we keep the figures in Project A unchanged, but now the actual risk reduction in Project B is larger, from 5 to 2 deaths per 100,000, further, the perceived risk reduction is also larger, from 10 to 2 deaths per 100,000. Scenario and question for actual versus perceived risks The second survey conducted was a choice experiment, where respondents were asked to choose between different public investment projects. Again, all respondents were asked to answer what they would choose if they were the decision maker. They made six pair-wise choices. Each project was described by four attributes: cause of accident (fire or traffic), number of avoided fatalities, number of avoided serious injuries, and their age groups. The attributes and the possible levels are presented in Table 1. The choice sets were created using a cyclical design, or a so-called fold-over (Carlsson and Martinsson 2003). First an orthogonal main effects design was generated, consisting of 24 attribute level combinations. Each of these combinations is one alternative in one of the 24 choice sets. The levels of the attributes in the second alternative in a choice set are obtained by adding two levels to each attribute level of the first alternative, and when the highest level is reached, it starts over from the lowest level. The 24 sets were then randomly blocked into four survey versions. An example of a choice situation to the general public is given in Fig. 3. Question for choice between different attributes We observe respondents making a number of choices between different programs. Each program implies a reduction in the number of fatalities and serious injuries avoided for a certain age group and cause of accident. We apply a binary probit model and estimate the influence on their choices of the various attributes; see Carlsson, Daruvala and Jaldell (2010a) and Johansson-Stenman and Martinsson (2008) for details. From this model we estimate the social marginal rate of substitution (SMRS) between different groups and accident types.",17
45.0,2.0,Journal of Risk and Uncertainty,19 September 2012,https://link.springer.com/article/10.1007/s11166-012-9150-8,A genuine foundation for prospect theory,October 2012,Ulrich Schmidt,Horst Zank,,Male,Male,Unknown,Male,"We analyze decision problems under uncertainty and consider a finite set S of states of nature.Footnote 5 That is, S = {s
1,...,s

n
} for a natural number n ≥ 3, and \(\mathcal{A}=2^{S}\) is the algebra of subsets of S (events). An act
f assigns to each state a real valued outcome. The set of acts \(\mathcal{F}\) can be identified with the Cartesian product space ℝn, and hence, we write f = (f
1,...,f

n
), where f

i
 is short for f(s

i
). An act f is rank-ordered if its outcomes are ordered from best to worst: f
1 ≥ ⋯ ≥ f

n
. For each act f there exists a permutation ρ of {1,...,n} such that
 i.e., such that the outcomes are rank-ordered with respect to
ρ. For each permutation ρ of {1,...,n} the comoncone ℝconsists of those acts which are rank-ordered with respect to ρ. Acts from the same comoncone are comonotonic. We use the notation f

E

g for an act that agrees with the act f on event E (i.e., f(s) = f

E

g(s) for all s ∈ E) and agrees with the act g on the complement of E, E
c. Also, we use h

i

f instead of \( h_{\{s_{i}\}}f\) for a state s

i
 ∈ S. Sometimes we identify constant acts with the corresponding outcome. We may thus write f

E

x for an act agreeing with f on E and giving outcome x for states s ∈ E
c. We consider a preference relation \(\succcurlyeq \) on the set of acts. The symbols ≻ and ~ denote strict preference and indifference. The preference relation \(\succcurlyeq \) is a weak order if it is complete (\(f\succcurlyeq g\) or \(g\succcurlyeq f\) for all acts f,g ) and transitive. A functional \(V:\mathcal{F}\rightarrow \mathbb{R}\)
 represents the preference relation \(\succcurlyeq \) if for all \(f,g\in \mathcal{F}\) we have \(f\succcurlyeq g\Leftrightarrow V(f)\geq V(g)\). An example of a representing functional is Choquet expected utility (CEU) introduced by Schmeidler (1989) and Gilboa (1987). It extends the classical subjective expected utility of Savage (1954) by introducing a non-additive measure for events: a capacity
v satisfies v(S) = 1,v( ∅ ) = 0, and v(A) ≥ v(B) if \(A\supseteq B\) and \(A,B\in \mathcal{A}\). A capacity v is strictly monotonic if v(A) > v(B) for \(A\supsetneqq B\) and \(A,B\in \mathcal{A}\). 
Choquet expected utility holds if the preference relation can be represented by the functional
 where ρ is as in Eq. 1. The strictly increasing and continuous utility, U, is cardinal (i.e., it can be replaced by a positive linear transformation of U) and the capacity, v, is unique. In terms of behavioral conditions, CEU can be derived by restricting Savage (1954)’s sure-thing principle to acts which are pair-wise comonotonic, and further by requiring a consistent ordering of utility differences across states (Köbberling and Wakker 2003). Prospect theory generalizes CEU by introducing a reference point r, which may have an impact on utility and on the capacity. In all axiomatic work we are aware of, the existence and location of this reference point is given from the outset. Formally, previous models considered a preference relation \( \succcurlyeq _{r}\) on acts with outcomes being deviations from r, i.e. for the act f the outcome f

i
 is interpreted as a gain (loss) if it is better (worse) than r. 
Prospect Theory (PT) holds if the representing functional for \( \succcurlyeq _{r}\) has the form
 where ρ is as in Eq. 1. Utility is fixed at the reference point, i.e., U(r) = 0. The two (possibly different) capacities v
 +  and v
 −  are uniquely determined and the utility is a ratio scale (i.e., unique up to multiplication by a positive constant). If v
 + (A) = 1 − v
 − (A
c) for all events \(A\in \mathcal{A}\), then PT reduces to CEU. In that case the decision weights of gains are the same as those of losses. If v
 + (·) ≠ 1 − v
 − (S − ·), then we have sign dependence. In the next section we impose conditions for a general preference over acts and derive reference dependence and PT from these conditions.",18
45.0,2.0,Journal of Risk and Uncertainty,21 September 2012,https://link.springer.com/article/10.1007/s11166-012-9152-6,Losers and losers: Some demographics of medical malpractice tort reforms,October 2012,Andrew I. Friedson,Thomas J. Kniesner,,Male,Male,Unknown,Male,"To understand the fundamental economics of the decision to settle and why there may be age and other interpersonal differences in the effects of malpractice insurance damage caps, consider two actors, A and B. Here both have been negligently injured and now have the right to sue. The right to sue is a risky asset S that takes on two values. One can go to court and win with probability p, in which case S takes on the value S
* > 0, or may lose with probability 1-p, in which case S takes on the value zero. For simplicity, assume that \( p = 1 - p = 0.5 \), although the implications of the theoretical exercise that follows does not depend on the assumption of a 50–50 chance of winning the case. A and B have different risk preferences here; A is risk neutral and B is risk averse. More formally, the actors have respective utility functions U

A
(S) and U

B
(S) such that \( U_A^{\prime }(A),\;U_B^{\prime }(B) > 0 \) and \( U_A^{{\prime \prime }}(A) = 0,\;U_B^{{\prime \prime }}(B) < 0 \). We also assume for the purpose of initial discussion that utility is not health state dependent and (more innocuously) that \( {U_A}(0) = {U_B}(0) = 0 \) as well as that the utility functions do not cross. This gives the two utility functions shown in Fig. 1. The effect of risk aversion on settlement value Let \( E\left[ {{S^{ * }}} \right] = {S^{ * }}^{ * } \). Each actor receives utility from the asset. Person A receives \( {U_A}\left( {{S^{ * }}^{ * }} \right) = E\left[ {{U_A}(S)} \right] \), which can be seen in Fig. 1 by tracing up from S
** to U

A
(S) and over to the vertical axis. Person B receives expected utility \( E\left[ {{U_B}(S)} \right] \), which can be seen in Fig. 1 by tracing up from S
** to the ray connecting the origin to U

B
(S
*) and over to the vertical axis. Both actors are indifferent between going to court and a settlement that gives them their expected utility of the risky asset, and will settle for that amount or any greater amount. Person B is willing to accept a settlement of less than S
** due to risk aversion.Footnote 3 There will then be age differences in settlement willingness to the extent that risk aversion varies by age (Halek and Eisenhauer 2001; Andersen et al. 2008). Now consider a cap on the amount that can be recovered in damages in a court award. This will change the maximum amount of the risky asset. The new asset S′ can now either take on the value zero or S
** with equal probability. Let \( E\left[ {S\prime } \right] = {S^{*}}^{{**}} \). We can find each actor’s utility from the new asset in a similar fashion as before. Person A receives \( {U_A}\left( {{S^{ * }}{{^{ * }}^{ * }}} \right) = E\left[ {{U_A}\left( {S\prime } \right)} \right] \), which can be seen in Fig. 1 by tracing up from S
*** to U

A
(S) and over to the vertical axis; B receives expected utility \( E\left[ {{U_B}\left( {S\prime } \right)} \right] \), which can be seen in Fig. 1 by tracing up from S
** to the ray connecting the origin to U

B
(S
*) and over to the vertical axis. If we take the difference between the utility from the original asset S, and the capped asset S′ we get L

A
 for actor A, and L

B
 for actor B. It is immediately noticeable that L

A

> L

B
, or that the less risk averse actor has a larger reduction in utility from the implementation of a cap on damages. The implication is that risk aversion differences by age or predicted settlement size can lead to age and other differences in the welfare loss from damage caps. There are a few more remarks that should be made about our theoretical exercise above. The first is that the behavioral implications do not depend on one of the actors being risk neutral. If actor A is also risk averse, the result that the less risk averse party suffers a larger utility loss is maintained as long as the other assumptions are still met. It is also important to note that actors’ changes in minimum acceptable settlements do not follow as clean a rule as their changes in utility. Although Fig. 1 may make it look as if there is a clear association between changes in minimum acceptable settlement and the relative risk aversion of the actors, that is an artifice of A being risk neutral and utility being independent of health state. Any systematic interpersonal differences in how damage caps affect welfare are in general indeterminate a priori (Shavell 1978) and how caps affect the size (asset value) of the settlement needs to be discovered empirically, which is what we turn to next.",7
45.0,2.0,Journal of Risk and Uncertainty,07 September 2012,https://link.springer.com/article/10.1007/s11166-012-9149-1,Comparing risk preferences over financial and environmental lotteries,October 2012,Mary Riddel,,,,Unknown,Unknown,Mix,,
45.0,2.0,Journal of Risk and Uncertainty,26 September 2012,https://link.springer.com/article/10.1007/s11166-012-9151-7,Experts in experiments,October 2012,Hans-Martin von Gaudecker,Arthur van Soest,Erik Wengström,Unknown,Male,Male,Male,"This section provides a detailed description of our experimental design, the sampling populations and our econometric model. The starting point of the experiments is the multiple price list format, a well-established methodology for preference elicitation. We first describe this format and how we implement it. We then point out the aspects of the experiment that are specific to the Internet and laboratory settings, respectively. In particular, we highlight the features of our design aimed at disentangling the effects of experimenter-induced selection and implementation method. One of these features is the introduction of two environmental treatments in the laboratory. The first treatment replicates traditional experiments, and the other one mimics the Internet setting as much as possible. We term them “Lab-Lab” and “Lab-Internet” to avoid confusion with the CentERpanel experiment (also denoted as “Internet experiment”). The full experimental instructions, samples of choice problems, help screens, final questions, and the debriefing part are available in the Online Appendix. The Internet experiment is also used in von Gaudecker et al. (2011). Section 1.4 sketches the behavioural model and its empirical implementation. Our experiments use a modified versionFootnote 5 of the multiple price list format; see, e.g., Binswanger (1980) and Holt and Laury (2002). Subjects get series of lotteries with identical payoffs but varying probabilities such as those in the screenshot in Fig. 1, taken from von Gaudecker et al. (2011). In each of the four cases, the subject chooses between Options ‘A’ and ‘B’. The lotteries are designed such that according to all economic theories that we consider (see below), subjects who do not make mistakes switch at some point from the safer Option ‘A’ to the riskier Option ‘B’, or choose ‘B’ throughout (since the last row is always a choice between certain payoffs, higher for ‘B’ than for ‘A’). Probabilities of the high payoff on the first screen increase from 0.25 to 1 in steps of 0.25. To obtain more precise information on switching points, subjects who are consistent in the sense that they do not switch back and forth between ‘A’ and ‘B’ and choose the higher certain payoff in the final question are routed to a second screen, containing lotteries with the same payoffs but a refined probability grid—involving 10%-steps located approximately between the respondent’s highest choice of ‘A’ and lowest choice of ‘B’ on the previous screen, similar to the iterative multiple price list format described in Andersen et al. (2006). For example, if the subject switches between 0.25 and 0.50, the second screen has probabilities 0.2, 0.3, 0.4 and 0.5.
 Screenshot of payoff configuration 5, first screen Each subject faced the seven payoff configurations described in Table 1;Footnote 6 for each configuration they make four or eight decisions, depending on whether they get the second screen. While ‘A’ always guarantees a positive payoff, some of the ‘B’ lotteries involved a possible loss. Actual payments were always made three months after the experiment. At the top of each screen, we indicated whether the outcome of the lottery was revealed immediately or in three months’ time (see Fig. 1). This is exploited to analyze preferences for early or late resolution of uncertainty in von Gaudecker et al. (2011) but is not used in the current paper.
 Subjects were randomly split into three groups: groups with hypothetical and real lotteries with the amounts shown in Table 1, and a third group with real payoffs but amounts divided by three. We refer to these groups as hypothetical, high, and low incentive treatments. Subjects in the high and low incentive groups received an upfront payment of 15 or 5 Euros, respectively, if they completed the experiment. These amounts were chosen to compensate for the maximum negative payoffs in the lotteries. No payment was made to the hypothetical group of the CentERpanel experiment, but to recruit the laboratory subjects in the hypothetical group, they were given a participation fee of 5 Euros. In the incentives treatments, everyone received the participation fee, but only one in ten subjects additionally got paid for one of the chosen lotteries (as in, e.g., Dohmen et al. 2011). The lottery to be paid out was selected at random to ensure incentive compatibility. We randomised the order in which the seven payoff configurations were presented. The subjects in the Internet experiment are respondents in the CentERpanel,Footnote 7 aged 16 and older. The CentERpanel is managed by CentERdata, a survey research institute affiliated with Tilburg University. The panel contains more than 2,000 households and covers the complete Dutch population, excluding the institutionalised. Questionnaires and experiments are fielded over the Internet. To avoid selection bias, households without Internet access are provided with a set-top box for their TV (and a TV if necessary). Panel members get questions every weekend. They are reimbursed for their costs of participation (fees of dial-up Internet connections etc.) on a regular basis. We conducted our experiments in November and December of 2005 and payments were made with the regular transfers three months after the experiments. The data for the Internet experiment were also used in von Gaudecker et al. (2011); see that paper and the Online Appendix for more details, screenshots, etc.. The welcome screen contained a brief introduction to the experiment followed by a non-participation option (see Fig. 6 in the Online Appendix for the introductory screens of all treatments). For the treatments with real incentives, subjects were told the amount of the participation fee and that they had the chance to win substantially more or lose (part of) this money again. It was made clear that no payment would be made upon non-participation or dropping out. In the hypothetical treatment, subjects were informed that the questionnaire consisted of choices under uncertainty in a hypothetical setting. In all treatments, subjects then had to indicate whether they wanted to participate or not. Participants first went through two pages of online instructions before facing the seven price list configurations. Instructions and specially designed help screens could be accessed throughout the experiment, to improve comparability with similar laboratory experiments, compensating for the absence of an experimenter. In total, 2,299 panel members received an invitation to participate in the experiment. About 12.7% declined the invitation, leaving 2,008 respondents who started the experiment. 80 subjects dropped out before completing the questionnaire. Moreover, a number of participants sped through the experiment and finished extremely rapidly. Using the minimum completion time observed in the laboratory, 5:20 min, as the cut off, we classify 138 respondents as speeders and remove these from the final sample (see also Section 3). Our final sample thus consists of 1,790 subjects who made 91,808 choices. The first three columns of Table 2 list descriptive statistics for the participants who completed the experiment (“final sample”), those who opted for non-participation, and those who dropped out in the course of the experiment or sped through it. As expected, the three groups differ in many respects. The variables in Table 2 can be broadly classified into six groups: Incentive treatment; education; sex and age; employment status and residential area; financial literacy and experience with financial decision making; income. Not everybody answered all of the questions, implying smaller sample sizes if we include all variables. Item non-response is strongest for the questions on assets and financial literacy and experience (which were asked in other weekends).
 Respondents randomised into the hypothetical treatment are underrepresented in the final Internet sample; they more often decide not to participate and more often drop out or speed through. Those who got the high incentive treatment are particularly unlikely to drop out or speed through the Internet experiment, as one would expect. Non-participation is negatively related to education level and positively associated with age. On the other hand, dropping out or speeding through is more likely for the younger age groups. Women are less likely to complete the Internet experiment than men; they more often decide not to participate and also more often drop out or speed through. Respondents with high household income participate more often. Participation also rises with several indexes of experience with financial decision making, such as being the person mainly responsible for household finances or having stocks. We also included two variables referring to employer provided savings plans. This is a specific type of long term savings plan that is heavily subsidised by the government through tax deductions; see Alessie et al. (2006), making net returns much higher than on any other safe asset. While it is easy to sign up for these plans and the employer does most of the paperwork, the default is not to participate. This may explain why employees with little financial knowledge or interest often do not sign up; see, for example, the work of Madrian and Shea (2001) on non-take up of 401(k) plans. Conditional on being offered such a plan, participating in it can be seen as an index of financial literacy. The fact that non-participation and dropping out or speeding through are negatively associated with such a plan therefore suggests that completing the experiment is positively associated with financial literacy. In order to compare the answers in the Internet experiment to those from a “standard” laboratory experiment, we performed the same experiment in the economics laboratory at Tilburg University. In total, 178 students participated in 16 sessions, divided equally between September 2005 and May 2006. The same treatments were carried out as in the Internet survey. The only difference was the payment of a show-up fee in the hypothetical treatment mentioned in Section 1.1. The payment procedure for the incentives treatments was the same as in the CentERpanel experiment: The participation fee was transferred to participants’ bank accounts three months after the experiment; one in ten subjects received the sum of the participation fee and the (possibly negative) payment from one randomly drawn lottery. To distinguish effects due to different sampling populations from effects due to replacing the controlled laboratory setting by the Internet environment, we also replicated this latter change in the lab. The first environmental treatment, labelled the “Lab-Lab” treatment, replicates the traditional setup used in laboratory experiments. In particular, an experimenter was present in the room to help the subjects and answer questions. In contrast to the CentERpanel experiment, no links to the instructions or help screens were shown in the core part of the experiment. Otherwise, the screens resembled the one in Fig. 1. Participants also had to wait until everyone else in the session had finished before they could leave the room. In the second environmental treatment—termed the “Lab-Internet” treatment—the experimenter was not present. Instead subjects had access to the same help screens (including the introductory screens) as in the CentERpanel experiment. Moreover, subjects could leave directly after completing the experiment—they did not have to wait for everyone else. The last column of Table 2 contains the available demographic characteristics of the laboratory subjects. Compared to the CentERpanel experiment, there is less information and less variation in the basic demographic characteristics. Specifically, in terms of age and education, the laboratory subjects cover only a small fraction of the population represented in the first three columns. In order to organise the data and to interpret the magnitude of selection effects, it is important to specify a behavioural model. Here we use a simplified version of the model of von Gaudecker et al. (2011), assuming no preference for early or late resolution of uncertainty. Starting point is a standard expected utility formulation with an exponential utility function:
  where z ∈ ℝ denotes a lottery outcome and γ ∈ ℝ is the Arrow-Pratt coefficient of absolute risk aversion. The choice of the exponential utility is motivated in von Gaudecker et al. (2011), who find that exponential utility provides a better fit of the data than power utility. Equation 1 is augmented with a loss aversion parameter λ ∈ ℝ+ , following prospect theory (Kahneman and Tversky 1979) and in line with the widely recognised stylised fact that “losses loom larger than gains” (see, e.g., Starmer 2000 for a review):
 The degree of loss aversion is measured by the ratio of the left and the right derivatives of the utility function at zero, as suggested by Köbberling and Wakker (2005). von Gaudecker et al. (2011) demonstrate that this model is able to capture the main features of the data in the CentERpanel experiment and that it is superior to several alternative functional form assumptions on the grounds of model fit.Footnote 8
 Based on Eq. 2, we formulate a structural econometric model which can be estimated by maximum likelihood. The model allows for individual heterogeneity in preference parameters and in the tendency to make errors. Individual differences are captured by observed characteristics (“observed heterogeneity”) or not (“unobserved heterogeneity”). Our decision problems are all choices between binary lotteries π
k , characterised by a low outcome ( k
low ), a high outcome ( k
high ) and the probability of the high outcome ( p
high ). Each pair of lotteries shares a common probability of the high outcome. Each individual \( i \in \{1, \hdots, N\} \) faces \( j \in \{1, \hdots, J_i\} \) dichotomous choices between two binary lotteries \( \pi^A_{j} = \left( A^{\rm low}_{j}, A^{\rm high}_{j}, p^{\rm high}_{j} \right) \) and \( \pi^B_{j} = \left( B^{\rm low}_{j}, B^{\rm high}_{j}, p^{\rm high}_{j} \right) \). Let Y

ij
 = 1 if the individual opts for \( \pi^B_{j} \) and Y

ij
 = 0 otherwise. Define the difference in certainty equivalents of the two lotteries in decision task j as:
 where \( \text{CE}(\pi^k_{j}, \gamma_i, \lambda_i) \), k = A, B is the certainty equivalent of lottery \( \pi^k_{j} \) given the utility function defined by Eq. 2 with the individual-specific parameters γ

i
 and λ

i
. Ignoring cases of indifference (which have probability zero in our setup), a perfectly rational decision maker would choose \( \pi^B_{j} \) if and only if \( \Delta \text{CE}_{ij}>0 \). As a first step to allow for stochastic decision making, we add so-called Fechner errors (see, for example, Loomes 2005) and model the individual’s choice as:
 where τ
ε

ij
 represents the Fechner error and \( \mathbb{I}\{\cdot\} \) denotes the indicator function. We assume that the ε

ij
 are independent of each other and of the random coefficients driving the utility function, and follow a standard logistic distribution. Fechner errors imply that as \( \Delta\text{CE} \) becomes small, the likelihood of choosing the “wrong” lottery increases. The probability of such a mistake increases with the parameter τ ( > 0). In addition to Fechner errors τ
ε

ij
 , we also allow for the possibility that subjects choose at random in any given task, following Harless and Camerer (1994). The propensity to do so is governed by the (individual specific) “trembling hand” parameter ω

i
 ∈ [0, 1] . Under these assumptions, the probability of the observed choice Y

ij
 of individual i in choice situation j , given all the individual specific parameters, is given by:
 where Λ(t) = (1 + e
 − t) − 1 is the logistic distribution function. For the sake of a parsimonious and easily interpretable model, we restrict τ to be the same for all individuals, but allow that subjects vary in their probability to make random choices.Footnote 9
 We use a random coefficients model for the individual-specific parameters γ

i
, λ

i
, and ω

i
 . Define
 where η

i
 denotes one of the three individual specific parameters, \( X^{\eta}_{i} \) are 1 × K
η vectors of regressors, β
η are K
η × 1 parameter vectors, and \( \xi_i^\eta \) are the unobserved heterogeneity components of the parameters. The first element of each \( X^{\eta}_{i} \) contains 1. The functions g

η
(·) are used to impose the theoretical restrictions on the individual specific parameters. This is just the identity function for γ; an exponential function ensures a positive value of λ; and the logistic distribution function guarantees that ω is always between 0 and 1. We write g(X

i

β + ξ

i
) for the vector of these three functions. We assume that the vector \( \xi_i = (\xi_i^\gamma, \xi_i^\lambda, \xi_i^\omega)^\prime \) follows a joint normal distribution independent of the regressors. von Gaudecker et al. (2011) find that the difference between the low and high incentive / hypothetical treatments is better captured by a multiplicative specification than by adding a low incentive dummy to X . For the low incentive treatment, we therefore multiply all slope coefficients as well as the standard deviations of the unobserved heterogeneity terms by the same parameter \( \beta^\eta_\text{low incentive} \). Defining ξ
 ∗  = (Σ′) − 1
ξ, where Σ′Σ is the covariance matrix of ξ, we can express the likelihood contribution of subject i as:
 where l

ij
 is the probability given in Eq. 4 and ϕ( ·) denotes the joint standard normal probability density function with appropriate dimension (three in this case). The log likelihood is given by the sum of the logarithms of l

i
 over all respondents in the sample and can be maximised by standard maximum simulated likelihood methods. The variance-covariance matrix of the parameter estimates is based on the outer product of gradients of the logarithm of Eq. 6. Standard errors for transformed parameters are calculated using the delta method.",27
45.0,3.0,Journal of Risk and Uncertainty,24 November 2012,https://link.springer.com/article/10.1007/s11166-012-9154-4,Testing the ‘standard’ model of stochastic choice under risk,December 2012,David Butler,Andrea Isoni,Graham Loomes,Male,Female,Male,Mix,,
45.0,3.0,Journal of Risk and Uncertainty,28 November 2012,https://link.springer.com/article/10.1007/s11166-012-9155-3,How much ambiguity aversion?,December 2012,Ken Binmore,Lisa Stewart,Alex Voorhoeve,Male,Female,Male,Mix,,
45.0,3.0,Journal of Risk and Uncertainty,04 December 2012,https://link.springer.com/article/10.1007/s11166-012-9153-5,When is ambiguity–attitude constant?,December 2012,Jürgen Eichberger,Simon Grant,David Kelsey,Male,Male,Male,Male,"Let S denote a state space, which we take to be finite. The set of consequences X, is assumed to be a convex subset of \( \mathbb {R} ^{n}\). An act is a function \(a:S\rightarrow X\). Let \(A(S)\) denote the space of all acts. The decision maker has a preference relation \(\succcurlyeq \) defined over \(A( S) \). We shall assume that \(\succcurlyeq \) satisfies the CEU axioms (Schmeidler 1989; Sarin and Wakker 1992). The CEU model of ambiguity represents beliefs as capacities, which are defined as follows. A capacity on S is a function \(\nu :2^{S}\rightarrow \mathbb {R}\) such that \(A\subseteq B\Rightarrow \nu ( A) \leqslant \nu ( B) \) and \(\nu ( \varnothing ) =0\),\(\ \nu \left ( S\right ) =1\), where \(2^{S}\) denotes the set of all subsets of S. The dual capacity
\(\bar {\nu }\) is defined by \(\bar {\nu }( A) =1-\nu ( A^{c}) \). The capacity and its dual are two alternative ways of representing the same information. A third and sometimes convenient way to encode the information contained in a capacity is by way of its Möbius inverse. Let \(\nu \) be a capacity on \(S.\) The Möbius inverse of \(\nu \) is a function \(\beta :2^{S}\rightarrow \mathbb {R} \) defined by \(\beta _{E}=\sum _{D\subseteq E}( -1) ^{\left \vert E\right \vert -\left \vert D\right \vert }\nu ( D) .\)
 The Möbius inverse has the property that \(\nu ( A) =\sum _{B\subseteq A}\beta _{B}\) and \(\sum _{B\subseteq S}\beta _{B}=1\). In the sequel we shall define some examples of capacities in terms of their Möbius inverses. In addition, some proofs in Appendix A proceed by demonstrating the requisite properties of the associated Möbius inverses. In the CEU model, preferences over \(A( S) \) are represented by the Choquet expected utility of an act a. The Choquet expected utility of an act a with respect to the capacity \(\nu \) is defined as
  where \(u:X\rightarrow R\) denotes the von Neumann–Morgenstern utility function. The class of convex capacities is of particular interest for us. A capacity, \(\mu \), is convex if \(\nu ( A\cup B) \geqslant \nu ( A) +\nu ( B) -\nu ( A\cap B) \). As is well-known, for any convex capacity there exists at least one probability distribution that dominates it. The set of such dominating probability distributions is referred to as the core of the capacity. More formally, we have: Let \(\nu \) be a capacity on S. The core, \(\mathcal {C }( \nu ) \), is defined by
 A special subclass of convex capacities are the belief functions. A capacity \(\nu \) on S is a belief function if for all \( A_{1},...,A_{m}\subseteq S;\)
  for all \(m,2\leqslant m\leqslant \infty .\)
 Convexity is the special case where this property is only required to hold for \(m=2\). One can show that a capacity is abelief function if and only if its Möbius inverse is non-negative, that is, for all \(B\subseteq S,\beta _{B}\geqslant 0\) (Dempster 1967; Shafer 1976). This section introduces the class of JP-capacities which will prove important in our analysis. Jaffray and Philippe (1997) study capacities which may be written as a convex combination of a convex capacity \(\mu \) and its dual. We shall restrict attention to JP-capacities since there is a natural way to distinguish between the perception of ambiguity and the attitude towards this ambiguity for such capacities. Note that here we only study deviations from expected utility due to ambiguity. In other words we assume that decision makers use expected utility when probabilities are known. JP-capacities are formally defined as follows. A capacity \(\nu \) on S is a JP-capacity if there exists a convex capacity \(\mu \) and \(\alpha \in \left [ 0,1\right ] \), such that \(\nu =\alpha \mu +( 1-\alpha ) \bar {\mu }\). A special class of JP-capacities are the Hurwicz capacities. The Hurwicz capacity with parameter \(\alpha \) is a JP-capacity \(\nu ^{H}\) with the convex capacity \(\mu ^{H}( A) =0\), for all \( A\varsubsetneqq S\), \(\mu ^{H}( S) =1\); i.e.
 We take the degree of ambiguity associated with the JP-capacity to correspond to standard measures of ambiguity for convex capacities. Let \(\mu \) be a convex capacity on S. Define the degree of ambiguity of event A associated with the capacity \(\mu \) by: \(\chi \left ( \mu ,A\right ) =\bar {\mu }( A) -\mu ( A) \), and the maximal degree of ambiguity associated with \(\mu \) by
 This definition is based on one in Dow and Werlang (1992). It provides an upper bound on the amount of ambiguity which the decision maker perceives. The degree of ambiguity measures the deviation from event-wise additivity. For a probability it is equal to zero. Convex capacities have degrees of ambiguity between 0 and 1, with higher values corresponding to more ambiguity. For a JP-capacity \(\nu =\alpha \mu +( 1-\alpha ) \bar {\mu }\), we apply this definition to the convex part \(\mu \). As the following proposition shows, the CEU of a JP-capacity is a convex combination of the minimum and the maximum expected utility over the set of probabilities in the core of \(\mu \). (Jaffray and Philippe 1997) The CEU of an act a with respect to a JP-capacity
\(\nu =\alpha \mu +\left ( 1-\alpha \right ) \bar {\mu }\) on S is:
 If beliefs may be represented by JP-capacities, preferences lie in the intersection of the CEU and multiple priors models. Proposition 1.1 suggests an interpretation of the parameter \(\alpha \) as a degree of (relative) pessimism, since it gives a weight to the worst expected utility an individual could expect from the act a. If \(\alpha =1\), then we obtain a special case of the MEU model axiomatized by Gilboa and Schmeidler (1989). On the other hand, the weight \((1-\alpha )\) given to the best expected utility which an individual can obtain with act a provides a natural measure for her optimism. For \(\alpha =0\) we have a pure optimist, while in general for \(\alpha \in (0,1)\), the individual’s preferences have both optimistic and pessimistic features. Ambiguity may be measured by the core of the convex capacity \(\mu \). A larger core corresponds to a situation, which is perceived to be more ambiguous. Hence, JP capacities allow a distinction between ambiguity and ambiguity attitude. The neo-additive capacity defined below is another special class of JP-capacities, which will prove useful in our analysis. Let \(\alpha ,\delta \) be real numbers such that \(0<\delta <1,0<\alpha <1.\) A neo-additive-capacity \(\nu \) on S is defined by \(\nu ( A) =\delta ( 1-\alpha ) +( 1-\delta ) \pi ( A) \), for \(\emptyset \subsetneqq A\subsetneqq S\), where \(\pi \) is an additive probability distribution on \(S.\)
 Formally, a neo-additive capacity is formed by taking a \(\left ( \delta ,1-\delta \right ) -\) convex combination of a Hurwicz capacity with parameter \(\alpha \) and a probability distribution \(\pi \). This capacity can be interpreted as describing a situation where the decision maker’s ‘beliefs’ are represented by the probability distribution \(\pi \). However she may have some doubts about these beliefs. This ambiguity about the true probability distribution is reflected by the parameter \(\delta \). The highest possible level of ambiguity corresponds to \(\delta =1\), while \(\delta =0\) corresponds to no ambiguity. The reaction to these doubts is in part pessimistic and in part optimistic. As is the case for JP-capacities in general, the ambiguity attitude associated with the neo-additive capacity may be measured by the parameter \(\alpha \). The Choquet expected utility of an act a with respect to the neo-additive-capacity \(\nu \) is given by
 That is, the Choquet integral for a neo-additive capacity is a weighted average of the highest pay-off, the lowest pay-off and the average pay-off with respect to \(\pi \). The following rule for updating a capacity has been axiomatized in Eichberger et al. (2007) and Horie 2007). Let \(\nu \) be a capacity on S and let \(E\subseteq S\). The Generalized Bayesian Update (henceforth GBU) of \(\nu \;\)conditional on E is given by:
 It is straightforward to check that the GBU rule coincides with Bayesian updating when beliefs are additive. In Eichberger et al. (2010) we show that GBU applied to updating neo-additive capacities leaves the ambiguity attitude parameter \(\alpha \) unchanged. In the present paper we investigate when a similar result applies to the larger class of JP-capacities.",5
45.0,3.0,Journal of Risk and Uncertainty,13 November 2012,https://link.springer.com/article/10.1007/s11166-012-9156-2,School environment and risk preferences: Experimental evidence,December 2012,Catherine C. Eckel,Philip J. Grossman,Rick K. Wilson,Female,Male,Male,Mix,,
46.0,1.0,Journal of Risk and Uncertainty,25 January 2013,https://link.springer.com/article/10.1007/s11166-012-9157-1,Ambiguity attitudes and social interactions: An experimental investigation,February 2013,Gary Charness,Edi Karni,Dan Levin,Male,Male,Male,Male,"The main experiment consists of two stages. In both stages the subjects were asked to choose between betting on an unambiguous event and ambiguous events of their choice in a three-color version of the Ellsberg (1961) experiment similar to the one described in the introduction. The experimental instructions are in Appendix A. In the first stage, the subjects, acting on their own, were asked to consider envelopes containing red, green and blue slips of paper. They were informed of the total number of slips in each envelope and the corresponding number of red slips. For each of the envelopes the subjects were asked to choose between betting $\(10\) on red, on green, or on blue. For example, if the total number of slips was \(36\), then the number of red slips in the different envelopes varied between \(9\) and \(14\) (in each case the number of red slips is known to the subject).Footnote 7 For each envelope they were asked to indicate the color they would like to place the bet on, red, green, or blue. At this stage the subjects were not informed of the existence of a second stage. After the first stage, one of the six envelopes was chosen at random and a ball was drawn from that urn. The subject was paid $\(10\) if she won the bet and nothing otherwise. At the completion of the second stage, again, one of the six envelopes was chosen at random and a ball was drawn from that urn. The subjects were paid if they won the bet. Subjects who chose to bet on red when the number of red slips was \(9\), \(10\) or \(11\) and continue to bet on red all the way to \(14\) were classified as ambiguity averse. Subjects who always chose another color when the number of red slips was smaller than \(12\), and chose to bet on red when the number of red slips is \(13\) and \(14,\) were classified as ambiguity neutral. Subjects who chose to bet on another color when there are from \(9\) to, at least, \(13\) red slips were classified as ambiguity seeking.Footnote 8 Finally, subjects that display any other pattern of choice were regarded as having ambiguity attitudes that do not fit any of these descriptions and are classified as ambiguity incoherent. Note that if the number of red slips is \(12\), then betting on any color from that urn is consistent with ambiguity neutral attitudes. One important feature of our design is that we are able to identify inconsistent (or ambiguity incoherent) attitudes. Simply because a subject chooses red when the number of red slips of paper in the envelope is 10 doesn’t necessarily imply that the subject is ambiguity averse. For example, the same subject may choose green when the number of red slips in the envelope is 11; these choices are inconsistent with respect to ambiguity preferences. Once the subjects filled out the questionnaires, they entered the second stage of the experiment.Footnote 9 In this stage, the subjects were matched in pairs. In each pair there was almost always at least one ambiguity neutral subject. The subjects were allowed to discuss their choices before placing their personal bets on the colors of their choice. In one version of the second stage, following their discussion, the subjects were asked to indicate again their betting preferences by filling out the same questionnaire as in the first stage. In this version, there was no incentive for the subject to try to reach an agreement about the way they should choose. In another version of the second stage, the subjects were motivated to make an effort to persuade their counterparts to choose the same bet. This was done by increasing the payoff to subjects whose choices, following the discussion, agreed. The extra payoff was chosen in a way that makes it worthwhile to switch only if the subject is actually persuaded that his original choice was wrong. In other words, the increased payoff in itself was not sufficient to induce a subject to choose differently from her preferences unless she is convinced that her original preferences were in some sense mistaken.
 There are three essential aspects to the second version of the design. First, there is a conflict of interests between subjects exhibiting distinct ambiguity attitudes. Second, the extra payoff creates an incentive for the subjects to reach an agreement. Third, when the subjects in a pair exhibit distinct ambiguity attitudes, to reach an agreement one of the subjects in the pair has to persuade the other that the latter’s preferences are misguided, and that it is in her best interest to change her choice. We intended this part of the experiment to reveal whether there is a tendency for one type of ambiguity attitude to be more “persuasive” in the sense of attracting the other attitudes toward it rather than being attracted by them. It is also intended to discover to what extent incentives play a role in making subjects change their attitudes. Our hypothesis is that significantly more switches will occur from ambiguity averse, ambiguity seeking and ambiguity incoherent attitudes to ambiguity neutral attitudes than vice versa, and that these tendencies are more pronounced when the incentives are built into the payoffs. A tricky element of the design involves the calculation of the extra payoff (premium) to be paid to subjects who, after discussing their choices with their pair-mates, come to an agreement on the best way to place their bets. The issue here is to choose the extra payoff so as to motivate subjects with distinct ambiguity attitudes to engage in a discussion trying to persuade each other to accept their respective positions, but not enough that any of them is ready to accept the other’s position for the extra payoff without being persuaded of the correctness of the position. The following example will show how the extra payoff can be set in the case of envelopes containing 36 slips. Consider an ambiguity averse subject who chooses one of the “other colors” when the number of reds is 1–9 and switches to red when the number of reds is 10 and up. Her choice indicated that 
 where \(x\) is the money payoff and \(\frac {11}{36}\) and \(\frac {10}{36}\) are the objective probabilities of red in line with 11 and 10 red slips, respectively, and \(\frac {k(36-10)}{36},\) is the probability of the “other color”. Hence,
 Thus
 What does it mean for this subject to be persuaded by the ambiguity neutral subject? It requires that she stays with the “other color” when the number of reds is 10 and 11 and switch to red at 12. However, she should not be induced to do so just because of the increased payoff. This puts an upper limit on what the extra payoff can be. This upper bound is calculated below: Let \(y>x\) be the payoff if there is an agreement and suppose that the subjects are risk averse. Normalize the utility function so that \(u(0)=0,\) and let \(k=5/13,\) then \(y\) must satisfy the following:
 Under expected utility this implies
 Hence, choosing \(y\) such that
 is not going to make the ambiguity averse type change her switch point. Consider next an ambiguity neutral subject. She prefers betting on another color when the number of reds slips is 11 or below, she may switch at 12 red slips, and she will definitely choose to bet on red when the number of red slips is 13 and above. What does it mean for this subject to be persuaded by the ambiguity averse subject? It requires that she chooses betting on red when the number of reds is 11 or below. However, she should not be induced to do so just because of the increased payoff. In other words, if the ambiguity averse person is unwilling to accept the choices of the ambiguity neutral person, it should not be the case that the ambiguity neutral subject rather would accept the choices of the ambiguity averse person in order to collect the “agreement premium” rather than avoid an agreement. This puts an upper limit on what the extra payoff can be. This upper bound is calculated below: For an ambiguity neutral subject, \(y\) must satisfy
 which under expected utility (the utility function is normalized so that \( u(0)=0\)) implies
 It is reasonable to assume that, for the small stakes involved, the utility function is approximately linear.Footnote 10 Hence,
 Thus, \(y<1.09x.\)
 Our experiments were conducted in a spacious classroom at the University of California, Santa Barbara (UCSB). The 404 participants were recruited using the ORSEE software (Greiner 2004) from the general student population that had expressed interest in participating in such experiments. No participant could be in more than one session. Participants were seated at some distance from each other, so that people did not observe one another’s choices. There were 272 participants in our first experiment. We first passed around an opaque bag that contained index cards with a number for each participant. We then presented the subjects with instructions (which were also read aloud) and decision sheets with six rows. These rows displayed the number of red slips of paper (ranging from 9–14) present in an envelope in which there were 36 slips of paper; the remaining slips of paper were either green or blue and no information was given about the numbers of green or blue slips of paper. There was an envelope corresponding to each row in the decision sheet, so that one envelope contained nine red slips, one contained 10 red slips, etc. In the first stage of the experiment, the participants were asked to choose among betting on red, blue, or green in each row. They were informed that a die would be rolled to select which row would be used to determine the actual payoff. After the participants selected rows, the decision sheets were collected. The die was rolled, and a slip of paper was drawn from the corresponding envelope. If the color drawn matched the color chosen, the chooser received $10. In the second stage of the experiment, there was a sufficient number of ambiguity neutral subjects so that almost every person who was not ambiguity neutral in the first stage was paired with an ambiguity neutral subject. This pairing was done quickly (less than 30 sec) by the experimenter from behind a lectern, while making sure that there was no conversation. The decision sheets were then passed back to the participants, with the ID number of the person with whom one was paired also written on the sheet. Subjects then formed pairs (if the number of participants was odd, one subject was asked to choose individually; we didn’t count this decision in our data). Pairs were told that they could (quietly) discuss the scenario with their partner before making their choices; it was not required that responses be the same for the two members in the pair. After the consultation, each subject chose his bets. Pairs were seated apart, so that they did not observe the choices of others or hear other discussions. Once again, a die was rolled to select the row and a colored slip of paper was drawn from the corresponding envelope to determine the winning color. There were two variants of this experiment, which differed across sessions. In the first, we paid a premium of $1.25 to each subject in the pair if the correct color was guessed and both subjects made the same choices in all six rows. In the second variant, we did not pay any premium. Note that the premium is below what it would take to make an ambiguity averse subject act against her preferences and accept the choice of an ambiguity neutral subject, and enough to incentivize a (weakly) risk-averse, ambiguity neutral subject to accept the choice of an ambiguity averse subject. Hence, except when the ambiguity neutral subject is risk inclined, if an ambiguity averse subject and an ambiguity neutral subject are to reach an agreement in order to collect the premium, the former subject is in an advantageous position. In other words, if the ambiguity averse person is unwilling to change his choice, the ambiguity neutral person would rather accept the choice of the ambiguity averse person rather than stick to his original choice and forego the premium. Our first finding is that the large majority of subjects display ambiguity neutral attitudes.Footnote 11 Of the 272 subjects that participated in the experiment, 164 subjects (60.3%) displayed ambiguity neutral choices.Footnote 12 Only 22 subjects (8.1%) displayed ambiguity averse choice and 32 subjects, (11.8%) displayed ambiguity seeking attitudes. The rest, 54 subjects (19.9%), displayed choices that were incoherent (see Appendix B Table 1).Footnote 13
 The next question is how ambiguity averse are the ambiguity averse subjects? As with risk aversion the degree of ambiguity aversion could, in principle, be measured by the premium they are willing to pay to avoid the ambiguous bets. In the preceding subsection we demonstrated how we may estimate this premium. The maximal premium for an ambiguity averse subject who (hypothetically) chooses to bet on one of the “other colors” when the number of reds is 1–9 and switches to red when the number of reds is 10 and up was shown to be 14.4% of the payoff of the bet. The same premium for an ambiguity averse subject who (hypothetically) chooses one of the “other colors” when the number of reds is 1–8 and switches to red when the number of reds is 9 and up is 15.4% of the payoff of the bet.Footnote 14Clearly, if an ambiguity averse subject chooses one of the “other colors” when the number of reds is 1 to 10 and switches to red when the number of reds is 11 and up, the premium is smaller than 14.4%. Among the 22 subjects in our study who displayed ambiguity aversion, 16 subjects switched at a number smaller or equal to 9, two subjects switched at 10, and five subjects switched at 11. Thus, for the majority of subjects displaying ambiguity averse attitudes, the degree of ambiguity aversion exceeds the 15% premium mark.Footnote 15
 Our session-level data and summary for pair behavior is shown below in Appendix B Table 2. Persuadinganother person to change his mind, especially when the issue is regarded as a matter of taste, takes effort. Participants in the study should be more inclined to make the necessary effort if motivated by an incentive scheme. If these presumptions are correct and important, we expect to see more agreements when the subjects are motivated by an agreement premium than without it. Our findings lend support to this hypothesis. Incentives to reach an agreement dramatically increase the agreements rate, from 30.5% to 84.0% (Z \(=\) 6.28, p \(=\) 0.000).Footnote 16 Based on these findings we conclude that incentives, in this context, are very significant. In so far as the “direction” that the persuasion took, the finding shows the following tendencies: First, there is a slight increase in ambiguity averse choice behavior (from 8.1% in individual choices to 10.4% following consultations).Footnote 17 Second, there is a considerable increase in ambiguity neutral attitudes (from 60.3% in individual choices to 75.2% following consultations).Footnote 18 Third, there is a substantial decline in ambiguity seeking attitudes (from 11.8% in individual choices to 6.7% following consultations).Footnote 19 Fourth, there is a decline in ambiguity incoherent attitudes (from 19.9% in individual choices to 15.7% following consultations).Footnote 20 These results suggest that the ambiguity averse and ambiguity neutral subjects had a “persuasive edge” over the ambiguity seeking and ambiguity incoherent subjects. Moreover, the ambiguity neutral subjects turned out to be more persuasive than ambiguity averse subjects. In addition, these tendencies are more pronounced in the presence of incentives. The analysis of individual choices suggests the following general tendencies: (a) In pairs involving ambiguity averse and ambiguity neutral types, there is no clear direction of influence. Roughly as many subjects displayed ambiguity aversion before the interaction and ambiguity neutrality after the interaction (7 with incentives and 2 without incentives) as the number of subjects who displayed ambiguity neutral attitudes before the interaction (8 with incentives and 2 without incentives) and ambiguity aversion following the interaction. This is true despite the “bargaining” advantage that the agreement premia gives the ambiguity averse individuals.Footnote 21 (b) In pairs involving ambiguity seeking and ambiguity neutral types, the direction of influence is clear. Again, the ambiguity neutral subjects had a persuasive edge over the ambiguity seeking subjects. Roughly speaking, twice as many subjects displayed ambiguity seeking attitudes before the interaction and ambiguity neutral choice patterns after the interaction (11 with incentives and 11 without incentives) as the number of subjects who displayed ambiguity neutral attitudes before the interaction (4 with incentives and 5 without incentives) and ambiguity seeking attitudes following the interaction. This is true with and without incentives.Footnote 22 (c) In pairs involving ambiguity incoherent and ambiguity neutral types, the direction of influence is clear. The ambiguity neutral subjects had a persuasive edge over the ambiguity incoherent subjects. Roughly speaking, one and a half times as many subjects displayed ambiguity incoherent attitudes before the interaction and ambiguity neutral choice patterns after the interaction (18 with incentives and 10 without incentives) as the number of subjects who displayed ambiguity neutral attitudes before the interaction (12 with incentives and 6 without incentives) and ambiguity incoherent attitudes following the interaction. This is true with and without incentives.Footnote 23 (d) No clear pattern can be ascribed to other pairs as the number of changes is too small to indicate a tendency. It is conceivable that subjects are more “open to persuasion” following a failure than following a success. If this is indeed so, then in pairs consisting of a “winner” (that is, a subject that won in the first round) and a “loser” (that is, a subject that lost in the first round) the latter subject is more open to persuasion, which may tilt the results towards the choice of the winners independently of their ambiguity attitude. To neutralize this effect, we also looked at “homogenous” pairs (that is, pairs of subjects that had the same experience in the first round, consisting of either two winners or two losers). Our data include 16 such pairs. Here the results are more striking. Among these, only one ambiguity neutral subject switched to the ambiguity averse choice, while all the other 15 subjects who displayed ambiguity non-neutral attitudes in the first round switched their choice to that of ambiguity neutrality. These include three ambiguity averse subjects, seven ambiguity seeking subjects and five subjects that displayed ambiguity incoherent choice behavior in the first round.Footnote 24 These results are suggestive. Teasing out more fully the effect of the normative appeal as opposed to the effect of past performance is beyond the scope of the current study. 
These findings suggest that ambiguity neutral subjects possess a persuasive edge over ambiguity seeking and ambiguity incoherent subjects, and to a lesser degree over ambiguity averse subjects. They also indicate that, while the presence of incentives leads to more changes following the interactions, incentives do not influence the direction of the changes.
",75
46.0,1.0,Journal of Risk and Uncertainty,29 December 2012,https://link.springer.com/article/10.1007/s11166-012-9158-0,Life expectancy as a constructed belief: Evidence of a live-to or die-by framing effect,February 2013,John W. Payne,Namika Sagara,Eric J. Johnson,Male,Unknown,Male,Male,"The concept that consumers’ forecasts of their life expectancies (their subjective probabilities) influence many economic decisions is widely accepted. Consequently, there is growing effort to collect these subjective probability judgments (see Hurd 2009 for a recent review of studies on life expectancies). The most influential of the life expectation surveys has been the Health and Retirement Study (HRS) conducted since the early 1990s. The HRS interviews people age 50 or older and includes a question using a 101-point scale (zero to 100) asking a person’s belief that he or she “will live to be 75 or more.” More recently, the HRS has also asked the chance of living to age 85 or beyond. Another influential survey, the Survey of Consumer Finances (SCF) sponsored by the Federal Reserve, asks, “About how long do you think you will live?” A survey of teenagers asked its respondents (mean age 15.8) the probability of “dying by” age 20 (Fischhoff et al. 2000). Subjective probability responses about expected longevity are also collected internationally (see, for example, van Solinge and Henkens 2009). While a live-to frame and a probability response are often used in surveys, other framings of life expectancy questions and other forms of response (e.g., an age rather than a probability) are also used. The standard theoretical view has been that subjective judgments of life expectancies are unbiased, reflecting personally held information about one’s likelihood of living to various ages, expressed with some random error or noise. The assumption that personal longevity judgments incorporate information people have about their own health, as well as more public actuarial information they have about how survival probabilities vary by age, gender, and race, appears to be true to some extent. For instance, self-rated health appears to have predictive power beyond what is contained in life tables (Hurd 2009). While subjective survival probabilities tend to generally reflect actual survival, some researchers have found evidence of systematic mispredictions (Elder 2007). For example, subjective survival rates are lower than actual probabilities for ages below 80 but are higher than actual for ages above 80. Research also suggests that men are relatively more optimistic than women about their likelihoods of living to older ages, i.e., beyond age 85. In addition, some researchers have argued that individual estimates of life expectancies may reflect certain motivational biases. Ludwig and Zimper (2007), for instance, suggest that subjective life expectations become more optimistic as people attain age 80 because of a desire to avoid feelings of despair about death. On the other hand, Fischhoff et al. (2000) present evidence that teenagers are pessimistic with regards to life expectations and suggest this may be due to a perceived lack of control over their world. It has also been proposed that life expectancies reflect a person’s general degree of optimism (Puri and Robinson 2007). To summarize, prior research suggests that subjective probabilities for events such as survival, where individuals have considerable personal information, have “considerable predictive power” (Hurd 2009). Nonetheless, Hurd argues that research needs to continue on the properties of the subjective probabilities of events such as longevity, including how those subjective probabilities are formed. We agree completely. This paper suggests that belief judgments, even for events such as longevity where people have substantial valid knowledge, are “constructed” responses based upon information accessible at the moment of judgment. Consequently these responses will also reflect normatively irrelevant task variations such as question framing that are likely to make certain types of information more or less accessible.",48
46.0,1.0,Journal of Risk and Uncertainty,19 January 2013,https://link.springer.com/article/10.1007/s11166-012-9159-z,Deterring domestic violence: Do criminal sanctions reduce repeat offenses?,February 2013,Frank A. Sloan,Alyssa C. Platt,Claire E. Blevins,Male,Female,Female,Mix,,
46.0,1.0,Journal of Risk and Uncertainty,27 January 2013,https://link.springer.com/article/10.1007/s11166-012-9160-6,One-sided commitment in dynamic insurance contracts: Evidence from private health insurance in Germany,February 2013,Annette Hofmann,Mark Browne,,Female,Male,Unknown,Mix,,
46.0,2.0,Journal of Risk and Uncertainty,28 March 2013,https://link.springer.com/article/10.1007/s11166-013-9164-x,Assessing multiple prior models of behaviour under ambiguity,April 2013,Anna Conte,John D. Hey,,Female,Male,Unknown,Mix,,
46.0,2.0,Journal of Risk and Uncertainty,26 March 2013,https://link.springer.com/article/10.1007/s11166-013-9163-y,Estimating discount factors for public and private goods and testing competing discounting hypotheses,April 2013,Andrew Meyer,,,Male,Unknown,Unknown,Male,"Samuelson (1937) first developed the discounted utility (DU) model in an attempt to provide a general model of intertemporal choice. Commonly referred to as the exponential discounting model, the DU model simplified all discounting into a single parameter, the discount rate. A consumer’s preferences over consumption bundles, \((c_{o},c_{1},...,c_{T})\) are represented by an intertemporal utility function, \(U(c_{o},c_{1},...,c_{T}).\) Furthermore, the DU model assumes that the intertemporal utility function is described by
 where the discount factor for year t  is \(\psi _{t}=\left [ \frac {1}{ 1+\rho }\right ] ^{t}\) and \(\rho \) is the discount rate. Samuelson’s DU model was accepted almost immediately because of its analytic simplicity and elegance. Interestingly, Samuelson did not endorse the DU model as a normative model of intertemporal choice or as a valid descriptive model. The DU model was never empirically verified but still became the standard model for intertemporal utility (Frederick et al. 2002). In the past several decades, research has uncovered many situations in which the DU model does not fit behavior.Footnote 1 One major departure from the DU model is that inferred discount rates often decline over time in experimental settings. This phenomenon is commonly termed hyperbolic discounting. This discounting gets its name because a hyperbolic functional form fits the data better than the traditional exponential functional form. Several functional forms have been suggested for hyperbolic discounting. The most popular of these takes the form of
 (Loewenstein and Prelec 1992). As \(\alpha \) goes to 0, this hyperbolic discounting function becomes the exponential discounting function. To facilitate estimation, researchers typically simplify Eq. 2 to have only one parameter. Constraining \(\alpha \) to be equal to one produces the model suggested by Harvey (1986). Harvey’s single-parameter hyperbolic structure is given by
 Alternatively, constraining the ratio of \(\beta /\alpha \) to be equal to one results in the single-parameter model suggested by Herrnstein (1981) and Mazur (1987) (HM);
 In recent years, an alternative model of discounting that has received much attention is the quasi-hyperbolic (\(\beta ,\delta )\) discounting model. This model, developed by Laibson (1997), is also motivated by the observation of declining discount rates. The functional form was first introduced by Phelps and Pollak (1968) in the context of intergenerational altruism. The form of the quasi-hyperbolic discounting function is very simple and its contrast with the standard exponential discounting model is readily apparent. The functional form is given by
 Thus, the only difference between discount factors in the quasi-hyperbolic formulation and the exponential formulation is that all future time periods are discounted by the additional \(\beta \) factor in the quasi-hyperbolic model. Especially large importance is placed on immediate utility as compared to deferred utility. The (\(\beta ,\delta \)) discounting model is much easier to analyze than the true hyperbolic model, yet it retains many of the qualitative aspects of the more complicated model. As shown in Fig. 1,Footnote 2 both hyperbolic and the quasi-hyperbolic discounting functions weight the near future less heavily than exponential discounting. However, for time periods far in the future, exponential discounters place less weight on the deferred utility than hyperbolic or quasi-hyperbolic discounters. Figure 2 shows the corresponding marginal discount rates for all four discounting functions. The point plotted for time period t is the marginal discount rate between time period \(t-1\) and time period t.
 Comparison of discount factors: exponential \((\delta ^{t}) \) with \(\delta =.9\), Harvey hyperbolic \(((1+t)^{-\mu })\) with \(\mu =.4\), Quasi-hyperbolic \((1,\)
\(\beta \delta ^{t})\) with \(\beta =.75\), \(\delta =.92\), and HM hyperbolic (\( (1+\omega t)^{-1})\) with \(\omega =.15\)
 Comparison of marginal discount rates: exponential \((\delta ^{t})\) with \(\delta =.9\), Harvey hyperbolic \(((1+t)^{-\protect \mu })\) with \(\mu =.4\), Quasi-hyperbolic (1, \(\beta \delta ^{t})\) with \(\beta =.75\), \(\delta =.92\), and HM hyperbolic \(((1+\omega t)^{-1})\) with \(\omega =.15\)
 Exponential discounters will always display time consistency because their marginal discount rate is constant over all time periods. Quasi-hyperbolic discounters have a large marginal discount rate between time period 0 (now) and time period 1 and a constant marginal discount rate thereafter. Thus, quasi-hyperbolic discounters are dynamically consistent for any choice that does not involve the present. Regardless, most interesting economic choices involve the present. Finally, hyperbolic discounters always have declining discount rates. Therefore, a hyperbolic discounter is subject to dynamic inconsistency for any time period. However, hyperbolic marginal discount rates change less for time periods farther in the future. That is, they will be less likely to be dynamically inconsistent for tradeoffs that occur far in the future than for tradeoffs that occur near to the present. Hyperbolic discounting makes individuals appear to be impatient for immediate tradeoffs, but sufficiently patient for tradeoffs occurring far enough in the future. A simple example highlights the time inconsistency inherent in hyperbolic and quasi-hyperbolic discounting. Assuming parameter values that are in the range of those found in the literature, I analyze the choice between $100 now and $120 a year from now and compare this with the choice between $100 five years from now and $120 six years from now. The interval length between the options for each choice is one year so a dynamically consistent discounter should choose either the more proximate reward in both scenarios or the more distant reward in both scenarios. Table 1 presents the discounted values of $100 now and $120 one year from now for all four discounting models. Table 2 shows the discounted values of $100 five years from now and $120 six years from now. The exponential discounter remains consistent in their choice to take the deferred payoff. However, the hyperbolic and quasi-hyperbolic discounters choose the early reward for the immediate tradeoff and choose the more distant payoff for the future tradeoff.
 It is desirable to be dynamically consistent from a normative standpoint. With free access to capital markets, individuals should equate the marginal rate of substitution between two time periods to one plus the interest rate. A third party planner could improve a hyperbolic or quasi-hyperbolic discounter’s intertemporal utility by rearranging consumption between time periods. In contrast, the welfare of an exponential discounter that is trading off consumption between time periods at one plus the interest rate cannot be improved upon by a third party planner.Footnote 3
 I concentrate on several of the more recent contributions and note that a more extensive literature review on discounting is provided by Frederick et al. (2002). While three recent working papers use utility-theoretic models incorporating goods other than money, the majority of previous studies examine monetary trade-offs over time. I point out that much of the evidence supporting hyperbolic discounting can be recast in terms of confounding factors. The most common method for gathering data on discounting is to elicit experimental responses to hypothetical or real monetary rewards. Two approaches are most widely utilized. Respondents are either asked to choose between two different sized rewards realized at different times in the future or to state the payoff today that would make them indifferent to a larger payoff in the future (or the payoff in the future that would make them indifferent to a smaller payoff today). Harrison et al. (2002) represents the former approach and Coller and Williams (1999) falls into the latter category. Harrison et al. find an overall individual discount rate in Denmark of 28.1% using money data and they observe significant heterogeneity in the data. One notable exception to the experimental emphasis is the revealed-preference study by Warner and Pleeter (2001). They examine the decisions of military personnel when faced with a downsizing. Personnel choices of whether to take a lump sum payment or an annuity reveal information about their intertemporal preferences. Several recent experimental papers have relaxed the assumption of linear utility. If individuals truly possess concave utility functions, the erroneous assumption of linear utility would lead to an overestimation of discount rates. Andersen et al. (2008) elicit risk preferences and discount rates of Danish individuals using experiments. They find an average discount rate of approximately 10% when allowing for a concave utility function as compared to approximately 25% when assuming linear utility. They are unable to test for quasi-hyperbolic discounting since they employ a front-end delay experimental design. Andreoni and Sprenger (2010a), Andreoni and Sprenger (2010b) design a set of experiments to generate simultaneous estimates of discounting and utility curvature parameters. They find average annual discount rates that range between 20 and 35% across specifications and studies, which are considerably lower than the rates from many previous experimental studies. Andreoni and Sprenger (2010a) do find evidence of concave utility but do not find evidence of hyperbolic discounting. Various studies have examined discounting for health outcomes. This branch of the discounting literature appears to begin with Horowitz and Carson (1990). In these studies, respondents state how many lives saved in the future is equivalent to saving a certain number of lives today, or respondents choose between varying durations of illness experienced at different times in the future. van der Pol and Cairns (2001) use this second method and provide the first example of discrete choice experiments to address discounting for health outcomes.Footnote 4
 Two recent related papers use an empirical model that is similar to the model I propose. Bosworth et al. (2006) jointly estimate individual-specific discount rates and the demand for preventative public health policies. They utilize a conjoint survey design in which respondents make choices between policies that reduce the number of illnesses and deaths in their community. At the same time, they have individuals choose between a hypothetical lottery that provides a series of payments over several years and a lottery that provides a lump sum payment. This method is based on the identification strategy developed by Cameron and Gerdes (2003). The authors of both papers argue that the two distinct data sources allow improved joint estimation of the utility parameters and discount rates and that it is often not possible to identify discounting parameters out of a public goods choice. I show that discounting parameters for public goods are identified in a stated-preference framework if the policy options are designed correctly. Bosworth et al.’s (2006) empirical model uses a utility-theoretic structure for preferences and assumes i.i.d. extreme value errors for the policy choices. Bosworth et al. (2006) do not allow for discount rates to take forms other than the standard exponential and single parameter hyperbolic models. I extend the model to test for quasi-hyperbolic preferences. Viscusi et al. (2008) design a study to infer discount rates for a publicly provided good. They utilize a stated preference survey concerning improvements in local water quality to identify individual rates of time preference. Using a random utility model, they find that the data fit better with the quasi-hyperbolic discounting model than with the exponential discounting model. They employ a two-stage empirical approach to generate parameter estimates but do not conduct hypothesis tests on the quasi-hyperbolic discounting parameters. I build upon the survey design from Viscusi et al. The random utility theoretic framework herein produces explicit standard errors for all discounting parameters so I am able to formally test quasi-hyperbolic discounting. Although evidence in the literature suggests that individuals have hyperbolic discounting preferences, I propose that some of this evidence can be explained by confounding factors. As emphasized in the review article by Frederick et al. (2002), it is important to differentiate between pure rates of time preference and other reasons that cause individuals to care less about future outcomes. Pure time preference refers to “the preference for immediate utility over delayed utility” (Frederick et al. 2002). Confounding factors that cause individuals to care less about the future but should be considered separately from pure time preference include uncertainty about a future outcome, perceived future transaction costs, and the phenomenon of subadditive discounting. In this section, I show how experimental designs that do not address these three confounding factors could make an exponential discounter appear as though they are a hyperbolic discounter. Imagine an experimental setting in which an individual is choosing between a smaller immediate reward and a larger delayed reward. Uncertainty in the receipt of the future reward can be problematic for estimating discount rates in this scenario. Suppose that this individual is truly an exponential discounter but perceives only a 70% chance that the researcher will actually deliver the delayed reward at any time in the future and a 100% chance that the immediate reward will be delivered. Then, the results from the experiment would look exactly like the individual is a quasi-hyperbolic discounter with a \(\beta \) value of 0.7. Or, suppose that this individual is truly an exponential discounter with a constant discount factor of \(\delta <1\) but believes with probability \( p_{0}=1\) that they will receive an immediate reward, with probability \( p_{1}<1\) that they will receive a delayed reward at \(t=1\), and with probability p

t
, such that \(p_{t+1}<p_{t}\) and \( p_{t+1}-p_{t}>p_{t+2}-p_{t+1}\), that they will receive a delayed reward at time t. That is, the perceived probability of receiving a future reward declines at a decreasing rate. Then, observed discount factors including the confounding effect of uncertainty are given by \(\{1,p_{1}\delta ,p_{2}\delta ^{2},p_{3}\delta ^{3},p_{4}\delta ^{4},...\}.\) Marginal observed discount rates are given by \(\{1/p_{1}\delta -1,p_{1}/\delta p_{2}-1,p_{2}/\delta p_{3}-1,p_{3}/\delta p_{4}-1,...\}.\) These resulting observed discount rates are consistent with a hyperbolic functional form. To further illustrate with a numerical example, assume \(\delta =.9,\)
\( p_{1}=.8,p_{2}=.7,p_{3}=.65,p_{4}=.61.\) This gives marginal discount rates of \(\{38.9\%,26.9\%,19.7\%,18.4\%\}.\) However, when abstracting from the effects of uncertainty, true marginal rates of time preference are given by \(\{1/\delta -1,1/\delta -1,1/\delta -1,...\}\). Thus, it is important to minimize the effects of future uncertainty in a discounting study. Next, suppose that within an experimental setting an individual perceives a transaction cost of c

t
 in order to collect a payment at time t in the future. Also suppose that this individual is an exponential discounter with a discount factor of \(\delta ^{t}\). Then, in order to be indifferent between an immediate payment of \(\$x_{0}\) and a delayed payment of \(\$x_{t},\) it must be that \(x_{0}=\delta ^{t}(x_{t}+c_{t})\). If \(c_{t+1}=c_{t}\) for all \(t>0\), observed marginal discount rates look like quasi-hyperbolic discount rates. If \(c_{t+1}>c_{t}\) for all \(t>0\) observed marginal discount rates can look like hyperbolic discount rates. To make ideas more concrete, consider the following example. Consider this individual indicating their indifference point between an immediate reward of $100 and a larger delayed reward. Let the perceived future transaction costs \(c=\{c_{0},c_{1},c_{2},c_{3},c_{4}\}=\{0,10,20,30,40\}.\) Assume \( \delta =.9.\) Denote the marginal discount rate between time periods t and \(t+1\) as \(r_{t,t+1}.\) Let superscripts of true and obs denote the true (exponential) and observed values. Then \(r_{t,t+1}^{true}=11.1\%\) for all t. Denote the delayed reward at time period t as x

t
. Next, ignoring the transaction cost, c
0, it holds that \(100=.9\ast x_{1}.\) Solving, \(x_{1}=111.11\) would make this individual indifferent in absence of transaction costs. Taking into account the effect of the transaction cost, \(100=\delta _{1}^{obs}(111.11+10).\) Solving, \(\delta _{1}^{obs}=.8257\). Then, \(r_{0,1}^{obs}=1/\delta ^{obs}-1=21\%.\) Again ignoring the transaction cost, c
1, \(100=.81\ast x_{2}\). Solving, \(x_{2}=123.46\) would make this individual indifferent in absence of transaction costs. Taking into account the effect of the transaction cost, \(100=\delta _{2}^{obs^{2}}(123.46+20).\) Solving, \(\delta _{2}^{obs^{2}}=.6971\). This implies \(r_{1,2}^{obs}=\delta _{2}^{obs^{2}}/\delta _{1}^{obs}-1=18.45\%.\) Continuing with this pattern, I find \(r_{2,3}^{obs}=16.53\%\) and \( r_{3,4}^{obs}=15.10\%\). I observe declining marginal discount rates even though the true marginal discount rates are constant. The larger the transaction cost relative to the size of the reward, the more pronounced this effect will be. One other explanation for the observation of declining discount rates is the idea of subadditive discounting. That is, “discounting over a delay is greater when the delay is divided into subintervals than when it is left undivided” (Read 2001). Many laboratory experiments look over days or months and confound the length of the delay with the length of the interval between choices. For example, a researcher will compare the discount rate inferred from a choice involving zero to six month delays to that from a choice involving zero to twelve month delays. When annualized, the discount rate will look larger from the choice involving zero to six month delays. Therefore, the discount rate looks like it declines over time. However, discount rates are declining because the length of the interval is increasing. Many researchers anchor all choices to a particular time and do not design choices to keep interval length independent from the length of delay. Typically, a shorter interval length necessarily means a shorter delay until the delayed outcome. Read (2001) uses experiments to verify the presence of subadditive discounting but finds no evidence of hyperbolic discounting. Several studies have attempted to determine whether exponential or hyperbolic discounting is preferred. In this section, I summarize the studies that have indirectly tested for hyperbolic discounting. Also, I analyze how each study addresses uncertainty in a delayed reward, perceived future transaction costs, and subadditive discounting. Kirby and Marakovic (1995) fit hyperbolic and exponential discount functions for each subject. They utilize nonlinear regression techniques on the continuous time equations for exponential and hyperbolic discounting. They find that, while both do a good job explaining subjects’ responses, the hyperbolic model fits better in terms of R
2 for almost all of the subjects. Uncertainty in the payment of the delayed reward is present since delayed rewards were not to be delivered until the evening on the day that it came due. Transaction costs are especially relevant because the rewards are small ($14.75–$28.50 for delayed rewards). This study confounds length of delay until the delayed reward is received with the length of the interval between options since all choices are anchored to the present. Slonim et al. (2007) conduct an experimental study in which they examine whether or not possession of the delayed reward affects subjects’ discounting patterns. They find that discount rates decline over time in all cases. Possession of rewards supports quasi-hyperbolic discounting and no possession supports hyperbolic discounting. They do not find any evidence of exponential discounting. This study attempts to control for transaction costs in the best way possible by using possession of the reward as a control variable. Also, this study uses a common interval length of two months for all choices so interval length is not confounded with the length of delay until the receipt of the future reward. Uncertainty in future rewards is nullified in the cases where individuals choose between two future rewards if the perceived probability of receipt of the reward is constant over time. However, uncertainty in future rewards is still an issue if the probability of receipt of the reward declines with longer time delays. Also, for the choices anchored to the present, uncertainty in future rewards remains a confounding factor. Cairns and van der Pol (2000) compare three hyperbolic models with the exponential model. For each individual and discounting model, they first estimate optimal parameter values using non-linear least squares. Second, they regress these parameter values on the period in years for which the benefit is delayed, claiming that delay should be insignificant for a correctly specified discounting model. Delay is insignificant only in the Loewenstein and Prelec model (2 parameter hyperbolic). They also note that the first stage regressions have the highest R
2 for the hyperbolic models. Since all choices are anchored to one year in the future, uncertainty in rewards is controlled for if the perceived probability of receiving the reward is constant over all time periods but not if the perceived probability of receipt declines with time. Transaction costs are minimized in the case of social financial benefits since the receipt of the reward does not require any work on part of the survey respondent. For private financial benefits, transaction costs likely get larger as the delayed reward moves farther into the future. If transaction costs are constant over all future time periods, they will have no influence in this study since all choices are anchored to one year from the present. However, because of this common anchor, the length of delay and length of interval are confounded. Subadditive discounting may explain any evidence for hyperbolic discounting. Keller and Strazzera (2002) examine the predictive accuracy of the exponential and hyperbolic models in a simulated data set. Using Thaler’s (1981) 1981 experimental data to calculate implicit monthly discount rates, the authors generate a simulated data set of predicted matching values, m

t
, that would make a respondent indifferent to an immediate reward, m
0. Comparing these predicted values with the actual matching values from Thaler’s data set, they find that the hyperbolic model does a better job than the exponential model. Thus, indirect tests suggest that hyperbolic discounting is preferred to exponential discounting. All choices are anchored to the present. This leaves open the possibility of confounding effects from uncertainty in future rewards, future transaction costs, and subadditive discounting. As previously mentioned, Andreoni and Sprenger (2010a) do not find any evidence of hyperbolic discounting. They are careful to equate transaction costs between more proximate and delayed payments so that any transaction cost effect would be negated. They also take considerable action to reduce uncertainty in the receipt of a delayed reward. Interestingly, when Andreoni and Sprenger (2010b) reintroduce uncertainty in the reward payoff, they observe that experiment participants exhibit less utility concavity for certain consumption compared to uncertain consumption and disproportionately prefer certain consumption. The result is that present consumption would be disproportionately preferred to delayed consumption and individuals could appear to be quasi-hyperbolic discounters due to the differences in their evaluation of certain and uncertain consumption. I build on these previous discounting studies by closely considering potential confounding factors. I select data sets that minimize uncertainty in (hypothetical and real) delayed rewards, decision-maker transaction costs, and subadditive discounting. Through jointly addressing these experimental concerns and developing a new empirical model that directly estimates the discounting parameters, I attempt to isolate pure rates of time preference for various models and test to find the statistically preferred model.",31
46.0,2.0,Journal of Risk and Uncertainty,21 March 2013,https://link.springer.com/article/10.1007/s11166-013-9162-z,The arguments of utility: Preference reversals in expected utility of income models,April 2013,Luke Lindsay,,,Male,Unknown,Unknown,Male,"A framework based on the one developed by Savage (1954) and adapted by Sugden (2003) is used to describe expected utility models, decision problems, and anomalies. There is a set, S, of mutually exclusive states of the world. States represent different resolutions of uncertainty. The number of states is finite. The probability of state s obtaining is \(p(s)\) where \(p:S\rightarrow [0,1]\). For all \(s\in S\), \(p(s)>0\). An act, a is a function mapping from S to consequences
C, \(\mathbf {a}:S\mapsto C\). The set of all possible acts is denoted by A. A special case is a constant act. It gives the same consequence in all states of the world, that is for all \(s,s^{\prime }\in S\), \(a(s)=a(s^{\prime })\), and represents a sure amount. Acts serve two purposes in the framework. First, acts are the objects of choice. Second, acts are used to define the initial wealth of a decision maker and so determine what part of the outcome is counted as income. In this paper consequences are restricted to levels of wealth. Let \(C_{w}=\{w\in \mathbb {R}:w\geq 0\}\) be a set of non negative real numbers representing wealth levels. A utility of final wealth function is \(u_{w}:C_{w}\rightarrow \mathbb {R}\) where for all \(w,w^{\prime }\in C_{w}\), if \(w>w^{\prime }\) then \(u_{w}(w)>u_{w}(w^{\prime })\). The set of all such functions is denoted \(U_{w}\). The utility functions in \(U_{w}\) can be used to construct corresponding expected utility models.
 Act a is preferred to act b if and only if \(EU_{w}(\textbf {a})>EU_{w}(\textbf {b})\). Preference is a binary relation between a and b. To model utility of income, final wealth w is disaggregated into initial wealth r and income y. Let \(D_{y}=\{y\in \mathbb {R}:r\geq 0\wedge (r+y)\in C_{w}\}\). A utility of income function is \(u_{y}:D_{y}\rightarrow \mathbb {R}\) where for all \(y,y^{\prime }\in D_{y}\), if \(y>y^{\prime }\) then \(u_{y}(y)>u_{y}(y^{\prime })\). The set of all such functions is denoted \(U_{y}\). To model expected utility of initial wealth and income, let a set of ordered initial wealth-income pairs \(D_{ry}=\{(r,y)\in \mathbb {R}^{2}:r\geq 0\wedge (r+y)\in C_{w}\}\). A utility of initial wealth and income function is \(u_{ry}:D_{ry}\rightarrow \mathbb {R}\) where for all \((r,y),\,(r^{\prime },y^{\prime })\in D_{ry}\), if \(r=r^{\prime }\) and \(y>y^{\prime }\) then \(u_{ry}(r,y)>u_{ry}(r^{\prime },y^{\prime })\). The set of all such functions is denoted \(U_{ry}\). An act a gives final wealth in each state of the world. Income, y is the difference between initial and final wealth. Let initial wealth be defined by the act r. Income in state s is \(a(s)-r(s)\). When r is a constant act, expected utility of income for utility function \(u_{y}\) is defined as follows.
 Expected utility of initial wealth and income for utility function \(u_{ry}\) is defined in a similar way.
 For both \(EU_{y}\) and \(EU_{ry}\) models, when initial wealth is given by act r, act a is preferred to act b if and only if \(EU(\mathbf {r},\mathbf {a})>EU(\mathbf {r},\mathbf {b})\). When a set of options is compared, the same act r defining initial wealth is used for all the options. Preference is a triadic relation between a, b, and r. The framework allows initial wealth to be uncertain. When r is not a constant act, there are a number of ways expected utility of income and expected utility of initial wealth and income can be modeled. A simple approach is a state-by-state comparison of r and a with \(EU_{y}(\mathbf {r},\mathbf {a})\) and \(EU_{ry}(\mathbf {r},\mathbf {a})\) defined by the equations above.Footnote 4
 The framework and the differences between the three expected utility models can be illustrated using game of roulette. An American roulette wheel has 38 numbered pockets where the ball can land. When a player bets on a single number, they receive 36 times the stake if the ball lands on the number and zero otherwise. Suppose the player has a certain initial wealth of 1$00 when entering the casino. This can be represented by the constant act \(\mathbf {w_{100}}\). The set of states of the world contains one state for each pocket. The set of acts that are chosen between are the admissible bets. The act of betting one dollar on number 8, denoted \(\mathbf {b_{8}}\), gives the consequence of $100 − $1 + $36 = $135 if the ball lands on number 8 and $100 − $1 = $99 otherwise. The expected utility of placing the bet in each of the three models is given by the following equations.
 Notice that although the utility functions \(u_{y}\) and \(u_{ry}\) take income as an argument, \(EU_{y}\) and \(EU_{ry}\) take acts as arguments and acts always map from S to \(C_{w}\). The example can be extended to illustrate using an uncertain act to define initial wealth. Suppose one dollar has been bet on 8 but the roulette wheel has not been spun yet and the player considers placing an additional dollar bet on 8, denoted by the act \(\mathbf {b^{\prime }_{8}}\). The act \(\mathbf {b_{8}}\) could be taken as the initial wealth, in which case the expected utility of wealth and income using the state-by-state approach would be as follows.
 The extended example can also illustrate the difficulties inherent in deciding which consequences are initial wealth. An alternative act (either constant or uncertain) could be taken as the initial wealth which could give rise to different decisions. For instance, if act \(\mathbf {w_{100}}\) defined initial wealth, the expected utility from betting an extra dollar on number 8 would be \(EU_{ry}(\mathbf {w_{100}}, \mathbf {b^{\prime }_{8}})\) rather than \(EU_{ry}(\mathbf {b_{8}}, \mathbf {b^{\prime }_{8}})\). Preference reversals resulting from changing the initial wealth can be defined as follows. A framing induced preference reversal occurs if there exist \(\mathbf {r},\hat {\mathbf {r}},\mathbf {a},\mathbf {b}\in \mathbf {A}\) such that
 but
 This definition is used in the following section and in the theorem reported in Section 3. Two additional types of preference reversal, reference-dependent-valuations and, are also defined and discussed in the next section.",5
46.0,2.0,Journal of Risk and Uncertainty,17 March 2013,https://link.springer.com/article/10.1007/s11166-013-9161-0,Heterogeneity in life-duration preferences: Are risky recreationists really more risk loving?,April 2013,Mary Riddel,Sonja Kolstoe,,,Female,Unknown,Mix,,
46.0,3.0,Journal of Risk and Uncertainty,25 May 2013,https://link.springer.com/article/10.1007/s11166-013-9167-7,An expected utility maximizer walks into a bar...,June 2013,Daniel R. Burghart,Paul W. Glimcher,Stephanie C. Lazzaro,Male,Male,Female,Mix,,
46.0,3.0,Journal of Risk and Uncertainty,21 May 2013,https://link.springer.com/article/10.1007/s11166-013-9165-9,The St. Petersburg Paradox at 300,June 2013,Christian Seidl,,,Male,Unknown,Unknown,Male,"In the French ancien régime the aristocrats disposed of large fortunes, but had lost their political influence, in particular after the inauguration of Louis XIV. Hence, many of them spent their time with games of hazard. The more sagacious of them, primarily a certain Chevalier de Méré, brought the famous mathematicians Fermat and Pascal to give them guidance for optimum gambling. Their suggestions centered around the expected value of the winnings (cf. Samuelson (1977, pp. 37–38) and Bernoulli (1738, p. 175). For an English translation see Bernoulli (1954); for a German translation see Bernoulli (1896)). Yet the distinguished mathematician Nicolas BernoulliFootnote 1 discovered a rather odd game of hazard, which he told to the mathematician Pierre Rémond de Montmort in a letter dated 9th September 1713, as the fifth of several problems: A promises to give a coin to B, if with an ordinary die he achieves 6 points on the first throw, two coins if he achieves 6 points for the first time on the second throw, 4 coins if he achieves 6 points for the first time on the third throw, 8 coins if he achieves 6 points for the first time on the fourth throw, etc. What is the expected value of this game for B? In his response of 15th November 1713, Montmort expressed his opinion that these examples have easy solutions along the lines of geometric progressions, but entered Bernoulli’s problems into the second edition of his Essay d’analyse sur les jeux de hazard (de Montmort 1713, p. 402). It was only in his letter of 20th February 1714, that Nicolas Bernoulli demonstrated the momentousness of his discovery, viz. that this game has an expected value of infinity, to Montmort, who expressed skepticism, but had to admit that he was unable to solve this problem. Gabriel Cramer, a professor of mathematics at the University of Geneva, read Montmort’s book and pondered N. Bernoulli’s fifth problem. In a letter of 21st May 1728 from London, he addressed Bernoulli proposing first a simplification of the gamble by replacing throwing a die by flipping a coin (whereby he reformulated the problem as it has become known since). In addition to that he proposed two solutions of the paradox, viz. by bounding the gambler’s perception of winnings from above, or by replacing the winnings by their square root. In a letter of 27th October 1728, Nicolas Bernoulli communicated the fifth problem in Cramer’s simplified version to his cousin Daniel Bernoulli who was at that time a professor of mathematics at the University of St. Petersburg. Subsequently, Daniel had become interested in this paradox and had sent Nicolas a first draft of his later publication. In his response of 5th April 1732, Nicolas thanked Daniel for a copy of his essay and remarked: “I have read it with pleasure, and I have found your theory most ingenious, but permit me to say to you that it does not solve the knot of the problem in question.” Together with this response, Nicolas sent Daniel a copy of Cramer’s letter, which Daniel appended in the original French version to his Latin manuscript for its final publication (Bernoulli 1738, pp. 190–2).Footnote 2
",18
46.0,3.0,Journal of Risk and Uncertainty,26 May 2013,https://link.springer.com/article/10.1007/s11166-013-9168-6,Solomonic separation: Risk decisions as productivity indicators,June 2013,Nolan Miller,Alexander F. Wagner,Richard J. Zeckhauser,Male,Male,Male,Male,"One day a wealthy man came to Solomon for advice. He observed: “I have two sons, X and Y. They are both fine boys, and help me administer my business. I do not spoil them, but they both receive an adequate income. Alas, the great sadness of my life is that they do not get along, and I must keep them apart so they do not quarrel. When I die, and fortunately my health is still good, one must get my business. The other will receive my worldly possessions, but alas the division will be unequal. The business is worth far more, and the burden to run it is not great. I cannot rely on either to provide an income interest to the other. My sons are equally capable, and I love them equally. Today, knowing what the future portends, they both spend what they receive. But I know that some people receive more pleasure from consumption expenditures than do others. I would like to leave my business to the son who receives the greater pleasure. However, when I ask them, they both say their pleasure is immense. How shall I decide?” Solomon responded. “The day after the second new moon, bring your sons to me, and we shall resolve this problem. I have but one constraint. You must let me resolve this problem, and you must remain silent as I do so.” The man agreed. The appointed day arrived, and the wealthy man and his two sons appeared before the King. Solomon then spoke to the sons. “Alas, the two of you do not get along. When your father passes from this Earth, his wish is that one of you receive his business, and the other his worldly possessions. You will then have no need for further contact with each other. “But wonderful things do not come without sacrifice. You see before you a large jar with a scorpion and some leaves. One of you will place his hand in this jar for a period of time to risk his sting. The scorpion may not see your hand for a while. But even when seen, it will not look like his natural prey; it may be ignored. But should the scorpion sting, it will be intensely painful, and perhaps worse. I have a papyrus scroll for each of you. You will each go to a corner of the room and write down how many minutes you are willing to leave your hand in the jar to be the one who inherits the business.” Solomon then explained how he would conduct this as a second-price auction, and the virtues of that method. The father was sad, because he did not want either son to risk the scorpion’s sting, but he got false succor from the second-price auction, thinking that it would lead to less time at risk. But most important, as promised, he remained silent. The sons returned to King Solomon and their father. X had written 2 minutes on his scroll. Y had written 30 minutes. Solomon then decreed: “The business shall go to Y upon your father’s death, because he is the son I have determined would reap greater benefits from the excess income that would offer. Moreover, Y need not place his arm within the scorpion’s bottle. That would be a deadweight loss, conceivably in the literal sense of that term. I was confident that neither of you would decipher this game. Just as I had no intention of dividing the baby in an earlier decision, I had no intention of forcing either of you to take a dreaded risk.” He then said: “Unlike judges in the democracies of future centuries, I do not have time to write down and justify my opinion. But I will explain to the court scribes the principles underlying my decision, so they may be recorded and available to future generations.” The father did not understand what had happened, but Solomon was Solomon. Thus, he knew the decision was wise. The father lived to a ripe old age. When he died, Y took his business, X the worldly possessions. As mentioned above, the scroll of the scribes has only recently come to light. It is reproduced here, together with contemporary comments from modern scholars. King Solomon observed: “My job was to find a way to identify which of two sons would derive greater utility from a substantially increased income. I have spent many years receiving my many subjects, from rich, moderate, and poor circumstances. I have struggled to perceive their levels of satisfaction. I have concluded that life in moderate or poor circumstances is much the same for all. But having riches separates men. Some are possessed of exquisite taste, and turn their riches to great consumptive pleasures, both for themselves and with their celebrations for the community. Others, alas, turn riches into little of value. They purchase ostentatiously to impress, and impress no one, not even themselves. I label these groups connoisseurs and boors. A connoisseur benefits greatly from securing riches, and this possibility is, therefore, worth making great sacrifices for. Hardly so for the boor. My test was a simple one. Son Y showed himself to be a connoisseur by his willingness to take a substantial risk to win the business; son X gave away his boorish nature when he answered a mere two minutes. I would like to claim originality for my method, but any fairy tale king who sent suitors into battle against dragons before they could claim his daughter’s hand understood the underlying principle: Any hopeful dragon slayer faced a 20% chance of death, with only an 80% chance of blissful marriage to the princess. (History is written by the victors, which is why traditional accounts suggest better odds). The fairy tale king—anticipating von Neumann and Morgenstern—recognized the implicit requirement:
  Only the deeply devoted would have such a utility for marriage to the princess.” We will prove the wisdom of Solomon’s idea below. As we show, a principal is indeed able to use risk taking as a gauge of preference intensity to great advantage. Not only can she separate connoisseurs from boors, but for a broad class of utility functions, such as constant relative risk aversion, she can approach the first-best. We also show that the typical lottery for a connoisseur involves the risk that he receives a very bad outcome (indeed, the worst possible outcome) with a very low probability. There is suggestive evidence that in devising this screening mechanism Solomon may have been influenced as well by family history. His father, King David, won his first wife, Michal (not Solomon’s mom), in an equivalent test gamble. As you remember, Goliath repeatedly challenged the Israelites for forty days. David, then but a humble shepherd, responded when King Saul promised a reward to he who defeats Goliath. “And it will be that the king will enrich the man who kills him with great riches and will give him his daughter and make his father’s house free in Israel.” (Samuel 1, 17:25). (Saul, some believe, was not looking for devotion to his daughter. He recognized David as a future power threat and perhaps was hoping he would be killed by Goliath). Solomon continued: “I have now sought to generalize this method to help future adjudicators. My method, like the procedure of the fairy tale kings, employs lotteries, but death-by-dragon (or by scorpion) seems a rather extreme penalty. My methods employ only risk taking with money. Some day, I am confident, a highly respected profession will develop that studies money and decisions, and employs experiments. I sought to anticipate their methods. Thus, I conducted a survey among a sample of my subjects of moderate means. Among our citizens, how much pleasure would you get from a consumption of 50,000 shekels per year? Please rate yourself on a percentile basis relative to your peers. Say you were given a lottery offering a 50-50 chance of 20,000 or 100,000 shekels per year. What certain amount would make you just as well off as this lottery? As I expected, there was a strong positive correlation between the answers to the two questions. That is, if we graph absolute utility as a function of income, the steeper curves were also straighter. To check for robustness, I then varied these amounts, but found that the pattern persisted. In these experiments with many subjects I discovered that risk aversion and reported pleasure from increased consumption were negatively correlated. I expect researchers in the far future to retrogress, and to express skepticism about the use of surveys or any attempt to gauge interpersonal comparisons of utility. But I have the extreme research advantage of having ruled for 36 years, to have met regularly with my subjects, and to be blessed with what they call wisdom. This gives me the power to detect cheap talk, and to make it expensive. Generalization can be of one’s method, or of its areas of application. I have found other areas where citizens can be induced to reveal their true assessments by subjecting them to some risk. Thus, in dispensing plots for farming for citizens turning 21, beyond assuring adequate incomes for all and well paid employment for young people, I seek to create prosperity for the kingdom. Thus, I wish to put substantially more land in the hands of high productivity workers. The more land combined with any worker, the less per hectare he will produce, but high productivity workers both get more output from the land and trim such diminution. The distinguishing feature of high productivity workers is their ability to manage young workers effectively. Thus, beyond the initial scale, their output increases linearly with land provided. I discovered that if I offer my subjects a choice, two hectares for sure, or a lottery offering an 80% chance of five hectares and a 20% chance of 1 hectare, productivity differentials make the lottery the best choice for the high productivity workers, the certain two hectares for those of low productivity. I hope that this generality—the ability to address two quite different problems—will help my methods to gain use in the future.” Solomon would rule for an additional four years. His two questions have been employed in contemporary surveys by the annotators. We shall return to our survey results near the end of the paper. As indicated by Solomon, his method also helps in resource allocations, where productivity of the agent, not personal benefits, are at stake, as is the case in corporate capital budgeting. Before discussing issues in practical application, we present the theory supporting the separation method identified by Solomon, what we consider to be the primary contribution of the paper. Private information plays a major role in many economic settings. Screening mechanisms are widely used to address the problems that arise when information is asymmetrically held.Footnote 1 This paper relates to several strands of literature, including work in capital budgeting, random mechanism design, optimal taxation, and fair division. While each of these literatures provides us with interesting insights, none directly addresses the class of problems that Solomon needed to solve, i.e., gauging the benefit that an agent, who is the source of welfare for a principal, will generate from an amount of resources. We discuss the differences in turn. The allocation of central resources to decentralized units is the canonical business example that motivates this analysis. For example, a corporation center must provide resources—e.g., capital, marketing capability, R&D support, executive time—to its operating divisions. When studying such situations, it is often assumed that divisions and the center have divergent interests (see, for example, Harris and Townsend (1981); Harris et al. (1982); Antle and Eppen (1985); and Stein (1997)). By contrast, in our paper agent welfare feeds positively into the principal’s welfare function—an assumption that often captures the relationship between the center and the division receiving resources. Of course, the principal would still like to correctly gauge the productivity of each division, and then fund each division to the level where a dollar of resources equals a dollar of profit. A division, however, being undercharged for such resources, would like far more resources. In the literature that does assume that the center benefits directly from the productivity of the agent, the approach has been to establish truthful revelation through auditing, or a compensation scheme, or both (see, for example, Harris and Raviv (1996); and Bernardo et al. (2001, 2004)). Our mechanism, by contrast, is based solely on the capital allocations themselves. In both Solomon’s examples and our analysis below, random mechanisms elicit private information. Other work has also used random mechanisms.Footnote 2 For example, risk-averse agents often take on undesirable risk to signal to others certain desirable qualities. Thus, risk-averse heads of start-up firms tend to retain substantial undiversified stakes to assure the market of their positive views of their firms’ prospects (Leland and Pyle 1977). By contrast, in the Moselle et al. (2005) model, good types communicate their quality by choosing less risk. In a different realm, buyer risk aversion can make it in a haggling seller’s interest to employ a possibly-final offer strategy, an offer which, if rejected, may be the final offer made (Miller et al. 2006). Probabilistic insurance policies can be theoretically appealing (though consumers are reluctant to accept them, an observation that Wakker et al. (1997) explain by reference to the weighting function of prospect theory). Three innovations differentiate Solomonic Separation from these methods: (1) agent productivity plays a central role, (2) risk tolerance on the resource to be allocated and productivity are correlated, and (3), the principal’s payoff is strictly proportional to that of the agent. Therefore, the principal would like to avoid variability in budgets. The optimal income tax literature has also considered the potential advantages of randomization, in the form of random taxes. High-productivity agents are endogenously richer than low-productivity agents, and thus have lower marginal utility. The problem for the government is one of redistributing from high-productivity agents to low-productivity agents, i.e., from richer to poorer, subject to the constraint that individuals choose how much to work. Randomization of outcomes for low-productivity agents can theoretically alleviate incentive constraints for high-productivity agents sufficiently that the additional scope for redistribution outweighs the immediate welfare losses from the randomization (Weiss 1976; Stiglitz 1982; Brito et al. 1995). Hellwig (2007) shows, however, that randomization of taxes is only welfare-enhancing with a particular type of increasing risk aversion; it is undesirable with weakly decreasing risk aversion. In Solomon’s problem, unlike in the optimal income tax problem, individuals differ in their utility functions, not in their income earning abilities. In fair division problems, the social planner uses agents’ relative preferences between different types of goods to divide a set of goods efficiently, while retaining envy-freeness. (See Steinhaus (1948) for the original problem formulation, Brams and Taylor (1999) for a popular book on the subject, Pratt and Zeckhauser (1990) for a case study of a practical application, and Brams (2005) and Pratt (2005) for reviews of existing division rules and developments of more sophisticated division methods). Solomon’s approach instead gauges the relative preference between different amounts of the same good, namely money (though it could be anything else), to judge how strongly the agent likes the good.Footnote 3
 In what follows, we aim to formalize Solomonic Separation based on risk taking. Section 3 presents the model. It considers a (female) principal who wishes to distribute funds to a (male) agent. There are two agent types who differ in their level of marginal benefits (either marginal productivity or marginal utility) that they derive from funds and in their risk tolerance. The central assumption of the model is that the agents who receive higher marginal benefits are also more risk tolerant. Given this condition, the analysis shows how the principal can screen agents by offering a choice between a nonrandom budget and a risky budget. Moreover, it proves that an optimizing principal need employ only two prizes with associated probabilities in the risky budget. The benefits of using this screening method can be substantial. Indeed, when the ratio of the more risk-averse type’s marginal utility to that of the other type is unbounded above for a very low payoff (e.g., as with CRRA), the first-best outcome can be approached. Section 4 discusses the results, and investigates whether marginal benefits and risk tolerance will be positively correlated, as the model requires. It provides an intuitive argument showing this is likely to be true in a production setting. For the consumption setting, it presents supportive evidence from survey results. These results, admittedly only suggestive, find a strong positive correlation between self-reports of utility gains from windfall funds and risk tolerance. This section contains additional results. For example, it identifies conditions under which our screening methods may be feasible but not beneficial. Section 5 concludes, and offers examples of applications.",3
46.0,3.0,Journal of Risk and Uncertainty,16 May 2013,https://link.springer.com/article/10.1007/s11166-013-9166-8,Bias and brains: Risk aversion and cognitive ability across real and hypothetical settings,June 2013,Matthew P. Taylor,,,Male,Unknown,Unknown,Male,"The experiment was designed to test two potential explanations for hypothetical bias. First, is the relationship between risk aversion and cognitive ability different when individuals face hypothetical choices versus real choices? Second, do individuals acquire information about choices differently across real and hypothetical settings? The present paper focuses on the former. Taylor (2012) discusses the latter and shows that information acquisition is not significantly different across settings. However, because the exploration of information acquisition resulted in several non-trivial changes in how the choices were presented to subjects, those modifications are explicitly described below. The experiment was completely computer-based and each subject’s interaction with experiment administrators was minimal and limited to signing in the subjects, reading a brief set of general instructions, verifying the subject’s payment, and paying the subject.Footnote 6 Each subject made choices in both the real and hypothetical settings and the setting of the first task was randomly assigned. It was not revealed to subjects at the beginning of the experiment that they would make choices in both settings, only that there would be two sections. A computer-based tutorial explained the gambles to the subjects using a vignette in which they were asked to imagine that there were two opaque jars filled with green balls and red balls and that the probability and payoff associated with drawing each color from each jar would be masked, but that they would be able to unmask this information by moving the computer mouse over the cell with the information. They were then instructed that they would be asked to choose the jar from which they would prefer to draw a ball.Footnote 7
 After the tutorial, subjects were then informed that there would be two sections (tasks), each involving some choices. Subjects randomly selected to complete the real setting first were informed that they would make ten choices, one of which would be selected to determine their payoff. These subjects were reminded that, although only one choice would be selected, each choice was equally likely to be selected so they should make each decision carefully. Those designated to complete the hypothetical choices in the first task were informed that they would make ten choices, one of which would be selected, and they would then be shown how much their payoff would have been had the choice been real. These subjects were then asked to consider these choices as if they would count toward their actual payoff. Once subjects completed the first task, they were notified that the results from the task would be revealed to them after they had completed the choices in their second task. Subjects then saw the instructions for whichever setting they had not yet completed. After the subjects completed the second task, the results from both tasks were revealed to them in the order in which the subject completed the tasks. Once subjects completed the choice tasks and their payoffs were revealed, they were asked to complete a test that measured both numeracy and cognitive abilities, which is described below. The final section of the experiment was a debriefing questionnaire that inquired whether subjects had been distracted during the experiment, whether they were liquidity constrained, the income level of their household, their education level and educational aspirations, the extent of their math and probability training, their academic major, and their gender. The choices presented to subjects in this experiment were derived from the Multiple Price List (HL MPL) format pioneered by Holt and Laury (2002), and is often utilized to elicit risk preferences.Footnote 8 The HL MPL format presents a subject with ten pairs of gambles, or lotteries, and asks her to choose the more-preferred gamble in each pair. Columns (1) through (4) of Table 1 illustrate the conventionally ordered HL MPL format. The payoffs of the gambles are structured so that Gamble B is more risky than Gamble A because the differences in its payoffs are larger. In the example shown in the table, Gamble A has possible payoffs of $40 and $32 and Gamble B has higher-variance payoffs of $77 and $2. The probabilities are adjusted progressively down the list of decisions so that the difference in the expected value between Gamble A and Gamble B, calculated in columns (5) through (7) but not shown to the subject, is monotonically decreasing. A risk neutral subject choosing solely on the basis of expected value would select the safe gamble in the first four decisions and the risky gamble in the last six decisions.
 Depending on the decision number at which a subject switches from the safe gamble to the risky gamble, bounds for a preference parameter that measures risk attitudes can be calculated directly from these choices. Column 8 in Table 1 contains the bounds for the risk aversion parameter if we assume that utility is described by the commonly used constant relative risk aversion (CRRA) utility function, \(u(w) = \frac {w^{(1-r)}}{(1-r)}\). More safe choices implies a greater level of individual risk aversion.Footnote 9
 As mentioned above, a second aim of this experiment was to test whether individuals acquire information differently when presented with choices with real payoffs versus those with hypothetical payoffs. Although those results are not discussed here, other than to say that they do not differ significantly across settings on average, a brief description of how this affected the presentation of the choices to subjects is provided below because I depart from previous studies that have used the conventional HL MPL design in five ways. These changes are discussed in more detail in Taylor (2012). 
 
Masked probability and payoff information. First, the probabilities and payoffs (the attributes) of each gamble were masked and subjects unmasked the attributes by scrolling over the cell with that information in order to view it. I utilize a software program called Mouselab that has been previously used by economists to explore other types of information acquisition and decision making (Costa-Gomes et al. 2001; Gabaix et al. 2006; Johnson et al. 2008). In a way, Mouselab is a lower-tech, more expedient alternative to eye-tracking technology and provides analogous measures of attention. Footnote 10 The resulting data allows the researcher to “observe” the process a subject used to acquire information about a choice. An example of how each pair of gambles was presented to subjects in the experiment is shown in Fig. 1.Footnote 11
 Choice example 
Randomized order of the choices. Second, the order of the pairs of gambles is randomized and subjects are presented with only one pair of gambles at a time. Facing the conventionally-ordered HL MPL as shown in Table 1, once an individual switches from the safe gamble to the risky gamble, it would be inconsistent (according to expected utility theory) to switch back to the safe gamble in subsequent decisions. In fact, several studies simply show subjects all ten choices at the same time and ask them to identify the gamble-pair at which they would switch. Footnote 12 That structure is not imposed here. 
Random perturbations in probabilities. Third, the probabilities are randomly perturbed, dynamically, by up to two percentage points on each gamble. For instance, consider HL Decision 7 in the HL MPL shown in Table 1. Rather than seeing a 70 percent chance of the high payoff in both the safe and risky gambles, a subject could be presented with a 72 percent chance of the high payoff in Gamble A and a 68 percent chance of the high payoff in Gamble B. 
Random scaling of the payoffs. Fourth, the payoffs for each pair of gambles were randomly scaled by a multiple of 19, 20, or 21 times the “low” payoffs used in Holt and Laury (2002). In that study, the low payoffs for the safe gamble were $2 and $1.60 and the low payoffs for the risky gamble were $3.85 and $0.10. In our experiment, if the payoffs for a gamble-pair were scaled by 19, then a subject saw safe payoffs of $38 and $32 and risky payoffs of $73 and $1.90 for that particular pair of gambles. If a gamble-pair was scaled by 21, then a subject saw safe payoffs of $42 and $34 and risky payoffs of $81 and $2.10. The “20 times” scale is shown in Table 1. The scale of the payoffs is varied because if they were the same, subjects might have begun to ignore this information after making just a few choices. 
Randomization of the choice format across subjects. Finally, to address concerns about possible left-to-right and top-to-bottom bias in subjects’ attention to different areas in a choice display, the presentation of the gambles is randomized along three different dimensions. I randomized (i) whether the probabilities or the payoffs are presented in the top row; (ii) whether the safe gamble or the risky gamble was on the left; and (iii) whether the high payoff or low payoff was on the left within each gamble. This randomization is by subject, and not across choice scenarios for any one subject.Footnote 13
 Once subjects completed the choices in both settings and learned their payoffs, they were asked to complete an ability test that contained both a numeracy test and a cognitive ability test. Numeracy tests are designed to measure an individual’s ability to understand and manipulate numeric and probabilistic information. The computational complexity of the tasks used in this experiment, along with the impact that numeracy could have on information acquisition behavior, made it prudent to elicit such a measure of numeracy. An eight-item test developed by Weller et al. (2013) that includes two of the three items in the cognitive reflective test (CRT) introduced by Frederick (2005) was adapted to measure numeracy and cognitive ability. The third item from Frederick’s CRT is included in the test as well, so the resulting test was nine items and provides two separate measures of ability: a numeracy score that measures a subject’s numeracy, and a CRT score that measures a subject’s cognitive ability.Footnote 14
 The three-item CRT was designed to evaluate an individual’s “System 2” cognitive processes—the ability to solve problems that require “effort, motivation, concentration, and the execution of learned rules” (Frederick (2005), pp. 26). All three questions in the CRT are designed to induce an immediate incorrect response, which requires effort and concentration to suppress, and a higher score on the CRT indicates a greater level of System 2 cognition. In contrast, “System 1” processes are spontaneous and do not require significant effort.Footnote 15 Kahneman (2011) stresses the differences in the role that these two systems play in our decision making and highlights that, although some problems can easily be handled with System 1 (“Fast”) processes, more difficult problems require the use of System 2 (“Slow”) processes. As will be shown below, it appears that individuals with this ability to “think slow” appear to behave quite differently when asked to make hypothetical risky choices relative to when they make real risky choices.",29
47.0,1.0,Journal of Risk and Uncertainty,10 July 2013,https://link.springer.com/article/10.1007/s11166-013-9169-5,Eliminating the U.S. drug lag: Implications for drug safety,August 2013,Mary K. Olson,,,,Unknown,Unknown,Mix,,
47.0,1.0,Journal of Risk and Uncertainty,24 July 2013,https://link.springer.com/article/10.1007/s11166-013-9170-z,The “bomb” risk elicitation task,August 2013,Paolo Crosetto,Antonio Filippin,,Male,Male,Unknown,Male,"In the basic version of the task, subjects face a 10 × 10 square in which each cell represents a box. They are told that 99 boxes are empty, while one contains a time bomb programmed to explode at the end of the task, i.e., after choices have been made. Subjects are asked to choose a number k
∗ ∈ 0, 100 that corresponds to the number of boxes they want to collect, starting from the upper left corner of the square. The position of the time bomb (b ∈ [1, 100]) is determined after the choice is made by drawing a number from 1 to 100 from an urn. If \(k^*_i\geq b\), it means that subject i collected the bomb, which by exploding wipes out the subject’s earnings. In contrast, if \(k^*_i < b\), subject i leaves the minefield without the bomb and receives γ euro cents for every box collected.Footnote 2
 Subjects’ decisions can be formalized as the choice of their favorite among the lotteries
 summarizing the trade-off between the amount of money that can be earned and the likelihood of obtaining it. Note that the task amounts to choosing the preferred among 101 lotteries, fully described both in terms of probabilities and outcomes by a single parameter k ∈ [0, 100], while γ > 0 is a scale factor. The expected value of these lotteries is equal to γ(k − 0. 01k
2), a bow-shaped function with a maximum at k = 50 and trivially equal to zero for k = 0 and k = 100. Normalizing u(0) = 0, an expected utility maximizer should choose:
 Assuming the classic (CRRA) power utility function u(x) = x
r:
 which implies that a risk neutral subject should choose k
∗ = 50. The implied levels of r for every possible choice k can be found in Appendix A: our task allows estimation of 100 intervals for r ∈ [0, 68. 275]. This basic version of the BRET is simple, and it can be run with paper and pencil. However, it requires a certain level of abstraction that, in principle, could hamper its effective comprehension. In order to facilitate subjects’ understanding we elaborated an equivalent version represented as a visual task in (almost) continuous time. The dynamic visual version in continuous time of the BRET is represented on the PC screen as a square formed by 10 × 10 cells, each one representing a box. Below the square is a “Start” and a “Stop” button. From the moment the subject presses “Start” one cell is automatically deleted from the screen at each second, representing a box that is collected. The deletion process follows a predetermined (arabic) sequence – i.e., the first element of the first row is deleted first, the second element of the first row second, and so on. A screenshot of the visual version after 45 seconds (i.e., after 45 boxes have been collected) as shown to the subjects is reported in Fig. 1. The subject is informed about the number of boxes collected at any point in time. Each time a box is collected, the subject’s provisional account is credited with γ additional euro cents. The subject can, at any time, stop the drawing process by hitting the “Stop” button.
 The BRET interface after 45 seconds - dynamic version Subjects are not informed about the content of the boxes (“empty” or “bomb”) since the position b ∈ [1, 100] of the time bomb, and therefore whether it has been collected or not, is randomly determined only at the end of the experiment. The metaphor of the time bomb has the crucial merit of avoiding the truncation of the data that would otherwise happen in case of a real-time notification. Subjects are explicitly warned that the earnings they see during the experiment are provisional since they would be equal to zero in case the time bomb was in one of the boxes collected. From a theoretical point of view, the decision can be represented as a sequence of binary choices, governed, as in the static case, by the parameter k. After k boxes have been collected, the choice subjects face is:
 The solution is equivalent to the static version, provided the subject is characterized by well-behaved preferences.Footnote 3 In fact, in both cases the subject has no way of determining the bomb’s position during the task and faces the same opportunity set. The dynamic visual version of the BRET has several advantages. First, it presents the set of lotteries in a sequential manner, inducing subjects to focus on only two lotteries at any moment in time. Such a framework makes evident that the lotteries can be ranked from the safest but less rewarding to the riskiest and more rewarding. Second, the visual representation and information provided allow a clear description of the probabilities involved. These features make the visual version intuitive and understandable even for subjects with low mathematical skills, ensuring that the likelihood of the subject’s decision being driven by confusion or an imperfect comprehension of the final outcomes and their probabilities is reduced to a minimum. Moreover, the dynamic version of the task is particularly appropriate to measure risk attitudes in decisions entailing a time dimension, such as trading, Dutch auctions, etc. The BRET is well positioned from several points of view along which risk elicitation tasks can be evaluated. The task allows precise estimation of the coefficient of risk aversion both in the risk aversion and the risk loving domains. Moreover, the task can provide estimates of the coefficient of risk aversion that are not biased by the degree of loss aversion. This is possible because subjects cannot be assumed to form any reference point, as their choice set does not include any option that offers a positive amount of money with probability 1; at most, subjects can assure themselves a payoff of 0 by choosing either k = 0 or k = 100. Additionally, the task requires subjects to make a single decision, and it is therefore robust to possible violations of the Reduction Axiom that would instead affect the results in case of multiple choices, one of which paid at random (Bernasconi and Loomes (1992), Halevy (2007) and Harrison and Swarthout (2012)). Last but not least, the task is simple. From this point of view the two versions are not identical, because the dynamic BRET is less demanding on a cognitive level as well as better suited to facilitate subjects’ comprehension. This reason, together with the fact that the dynamic version is characterized by a richer set of parameters that can be manipulated, makes the visual version in continuous time our preferred choice. We use it both to test the robustness of the task (Section 3 below) and to compare the BRET with other risk elicitation mechanisms (Section 4), although increasing the amount of money at stake in the latter case.",192
47.0,1.0,Journal of Risk and Uncertainty,03 July 2013,https://link.springer.com/article/10.1007/s11166-013-9171-y,Does insurance fraud in automobile theft insurance fluctuate with the business cycle?,August 2013,Georges Dionne,Kili C. Wang,,Male,Unknown,Unknown,Male,"Insurance fraud incentive is induced by the nature of the insurance contract.Footnote 13 Two important contract types are replacement cost endorsement and no-deductible endorsement. We posit that the degree of incentive to defraud is also affected by the business cycle. In this section, four hypotheses are proposed for the empirical tests. Before we explore the impact of the business cycle, the claim timing pattern of fraud induced by each contract is identified.Footnote 14
 There are many factors explaining insurance fraud.Footnote 15 On the demand side, maximizing their expected utility, each insured has a critical value of the probability of a fraud being successful. The lower this critical value, the greater the insured’s incentive to commit fraud. One can show that this critical value decreases when the difference between the vehicle’s replacement cost and the vehicle’s market value increases near the expiration of the insurance contract. Accordingly, the insured would consider planning fraud near the contract’s expiration with increasing probability. On the supply side, the probability of the insurer’s conducting an audit also decreases near the contract’s expiration because the market value of the vehicle decreases over time.Footnote 16 Hence, as Dionne and Gagné’s (2002) theoretical model indicates, insurance fraud probability is higher near the expiration of the replacement cost endorsement. We also infer the probability of insurance fraud under such an equilibrium model. On the demand side, the individual’s expected utility model is similar to that of Dionne and Gagné (2002). On the supply side, we modify their insurer’s audit probability to become flat over time.Footnote 17 The theoretical model under replacement cost endorsement is presented in Appendix B. The main result is consistent with Dionne and Gagné (2002) in that the equilibrium fraud probability is higher near the end of the policy year. When we study the empirical link between fraud and endorsement contract, this contract is compared with the reference contract with both depreciation and deductible. We treat the contract with replacement cost endorsement as a high-coverage contract, and the reference contract as a low-coverage contract. A contract with replacement cost endorsement reimburses the total value of the car evaluated at the beginning of the contract period less the proportional deductible while the reference contract reimburses the loss of depreciated value of the vehicle less the proportional deductible. The former contract exhibits the claim timing pattern described above, whereas the latter does not. Hence, our first hypothesis: If the claims are induced by fraud, individuals who choose a replacement cost endorsement contract have a higher probability of filing a claim, and this probability is even higher near the end of the contract period. In addition, we examine incentives to defraud induced by the no-deductible endorsement contract. Dionne and Gagné (2001) propose a theoretical model and empirically verify that the design of the deductible would increase the incentive to build up a claim. However, this conclusion cannot be applied directly to our research design because we investigate fraud related to total theft rather than build-up. We assume that people would have a greater incentive to invent fraudulent claims when there is no deductible designed in the contract than with the reference contract. The incentive of fraud is higher under a no-deductible endorsement because the insurer reimburses the depreciated value of the car without deductible. We derive this result in Appendix C. We also discuss the claim timing pattern for the contract with depreciation in Appendix C. Whereas the vehicle depreciation from the insurer’s indemnity is much more stringent than that in the market,Footnote 18 and when the incentive to cheat is large enough, the insured would have a stronger incentive to organize fraud at the beginning of the contract period under a contract with depreciation and the non-deductible endorsement. Hence, under a flat audit mechanism, the equilibrium fraud probability is higher at the beginning of the policy year. To identify fraud induced by the no-deductible endorsement contract in our empirical test, we compare this contract with the reference contract, which comprises both a deductible and depreciation. The contract with depreciation and no-deductible endorsement is thus a high-coverage contract, and the reference contract is a low-coverage contract. As shown in Appendix C, the relative claim timing pattern is focused on the beginning months of the policy year. Our second hypothesis is therefore: If the claims are induced by fraud, individuals who choose the non-deductible endorsement contract have a higher probability of filing claims. This probability is even higher at the beginning of the contract period. Although the main research concern is insurance fraud, the former parts of the above two hypotheses could evidently also result from adverse selection. Under adverse selection, high-risk individuals tend to purchase the two types of high-coverage contracts and are more likely to make a claim. However, the claim would be equally distributed among the 12 months. In contrast, only insurance fraud would create a particular pattern in the timing of the claim during the months of the policy year. This characteristic enables us to clearly rule out adverse selection. Hence, when the first and second hypotheses are empirically sustained, they would provide evidence of insurance fraud rather than adverse selection. It is also important to distinguish fraud from ex ante moral hazard.Footnote 19 Insurance fraud results from an individual’s decision to invent a fraudulent claim or not. Ex ante moral hazard arises from the decision to pay more or less attention to self-protection. Under the replacement cost endorsement, ex ante moral hazard could be stronger near the end of the policy year.Footnote 20 Conversely, under the no-deductible endorsement contract, ex ante moral hazard could be stronger at the beginning of the policy year.Footnote 21 Ex ante moral hazard thus has the same claim timing pattern as insurance fraud. Dionne and Gagné (2002) show that fraud can be induced only when the benefit from fraud is sufficiently large. They maintain that the benefit from fraud based on partial theft is minor. The incentive to defraud through total theft is much stronger than that related to partial theft. This difference in incentives provides an opportunity to distinguish fraud from ex ante moral hazard. Self-protection has an equal effect in terms of reducing the probability of both total theft and partial theft, but fraud solely leads to a stronger incentive to file a total theft claim. Accordingly, fraud could emerge mainly based on the probability of a total theft claim instead of a partial theft claim. Hence, our third hypothesis distinguishes insurance fraud from ex ante moral hazard. If the claim timing patterns in the first and second hypotheses emerge only in relation to the total theft claim, insurance fraud exists rather than ex ante moral hazard. If the above patterns also emerge relative to the partial theft claim, then ex ante moral hazard may exist in the market, and we cannot conclusively determine whether insurance fraud exists. Investigating the impact of the business cycle on insurance fraud is the second objective of this research. Whether fraud will fluctuate consistently or inversely with the business cycle is unclear. Regarding risk aversion, it has been accepted that most people are risk-averse and exhibit decreasing absolute risk aversion. In an economic recession, people’s wealth decreases and they become more risk-averse. This will make them more hesitant to adopt risky actions, including a fraud lottery. Hence, the probability of fraud could decrease during a recession. Alternatively, because the individual’s wealth decreases during a recession, the increment of utility from the benefit of fraud increases concomitantly, because of a direct wealth effect. Furthermore, if recession reduces individuals to the poverty level, they may feel they have much less to lose if they get caught committing fraud. This may increase people’s likelihood of defrauding during a recession. Fraud is highly related to an individual’s morality. Morality may also vary with wealth level. Husted (1999) argues that societal corruption is highly related to GDP per capita. He provides empirical evidence that indirectly shows that individuals’ morality level is positively related to their wealth. Dionne et al. (2009) establish in a theoretical model that moral cost affects individuals’ decision to defraud. Therefore, from the standpoint of morality, recession reduces the average wealth level. A lower wealth level could weaken morality and reduce the moral cost of fraud, which raises the probability of fraud. Finally, insurers may reduce their operational risk management during recessions because they have fewer resources although they should increase them according to the previous predictions from the insured. Because our fourth hypothesis encompasses many conflicting effects that we cannot isolate, we do not make a prediction on the sign of the business cycle effect. Insurance fraud could be positively or negatively affected by the business cycle.",17
47.0,1.0,Journal of Risk and Uncertainty,01 August 2013,https://link.springer.com/article/10.1007/s11166-013-9172-x,Are people overoptimistic about the effects of heavy drinking?,August 2013,Frank A. Sloan,Lindsey M. Eldred,Yanzhi Xu,Male,,Unknown,Mix,,
47.0,2.0,Journal of Risk and Uncertainty,26 September 2013,https://link.springer.com/article/10.1007/s11166-013-9175-7,Risk and choice: A research saga,October 2013,Christian Gollier,James K. Hammitt,Nicolas Treich,Male,Male,Male,Male,"In the 1970s and early 1980s, economists were busily trying to make general equilibrium models more realistic by including risk in their models. One of the many relevant questions was that of the impact of uncertain output prices on firms’ production decisions. Sandmo (1971) conjectured that a marginal increase in uncertainty will decrease output, but concluded that this conjecture cannot be proved categorically. Similarly, while intuition suggests that an increase in risk should reduce the demand for risky assets, this is not true in general. To see that, consider a portfolio choice problem with a safe asset with a zero return and a risky asset. The risky asset has a return of +100%, 0% and −100% respectively with probability 3/5, 1/5 and 1/5. Consider an increase in risk with a new distribution of return (+150%, 3/10; +50%, 7/20; −16.67%, 3/20; −100%, 1/5). A straightforward analysis shows that a risk-averse investor with concave utility function u(c) = min(c, 3 + 0.3(c − 3)) and an initial wealth w = 2 will double her investment in the risky asset (from 1 to 2) due to this large increase in risk on the return of this asset. Thus, in the framework of expected utility with two assets, one safe and one risky, a Rothschild-Stiglitz increase in risk in the return of the risky asset may increase the demand for it by some risk-averse agents. Similarly, a risk-averse policyholder can reduce her demand for insurance when the insurable risk increases, and some risk-averse entrepreneurs will increase their output after an increase in uncertainty on the output price. Let us explore the evolution of this early literature. Rothschild and Stiglitz (1970) define an increase in risk as follows: Lottery Y is riskier than lottery X with the same mean if and only if all risk-averse agents prefer X to Y: EX = EY, and for all u increasing and concave, Because any concave function can be expressed as a convex combination of min functions v

θ
(z) = min(z,θ), inequality (1) holds for all u concave if it holds for all min functions. This observation yields the well-known integral condition for increases in risk. In a companion paper (Rothschild and Stiglitz 1971), the same authors explore another problem. Suppose that agents can decide how many lottery tickets to purchase. When making the lottery riskier from X to Y, should all risk-averse agents reduce the number of tickets they purchase? In other words, can we guarantee that when Y is riskier than X, the following inequality holds for all u increasing and concave, As said before, the answer is no. Because Rothschild and Stiglitz (1971) found this result counter-intuitive, they looked for restrictions on the set of acceptable utility functions that eliminate the paradox. This line of research has also been explored by Fishburn and Porter (1976), Cheng et al. (1987) and Hadar and Seo (1990). But the outcome may be judged disappointing, because its main conclusion is a sufficient condition that is complex and quite stringent.Footnote 5
 Louis and many co-authors were attracted by this puzzle as early as the mid 1970s. Eeckhoudt and Hansen (1980) demonstrated that a mean-preserving price “squeeze” through an increase in the minimum price and a reduction of the maximum price always raises the optimal production, with some interesting applications to the European agricultural policy for example. During the next 15 years, this initial contribution generated a tsunami of incremental results, most often producing very specific restrictions on the increase in risk (IR), and more generally on the second-order stochastic dominance order, to yield the desired result: strong IR, relatively strong IR, relatively weak IR, portfolio dominance, monotone likelihood ratio order, monotone probability ratio order, and many others. With the exception of the last two restrictions, these results did not attract much attention from the economics profession. The development of this literature culminated with the publication of the necessary and sufficient condition on the change in distribution from X to Y that implies that property (2) holds for all concave utility functions (Gollier 1995). This property can be rewritten as follows: In words, if it is optimal to purchase one unit of X, it is optimal to purchase less than one unit of Y. It is tempting to rewrite this condition as follows: For all u increasing and concave, Because any concave function is a convex combination of min functions, it is easy to simplify this condition in a similar fashion to that for the integral condition for IR. That was done by Rothschild and Stiglitz (1971), who claimed that they obtained the necessary and sufficient condition. But this was a mistake. Indeed, condition (4) is more restrictive than (3), hence so is the Rothschild-Stiglitz condition coming from (4). Indeed, this condition needs to hold only for utility functions u that yield EXu ′ (w + X) = 0. It is intriguing that the seminal paper that launched this literature was fundamentally wrong and that it took 25 years to detect the mistake and characterize the right solution. Georges Dionne and Louis Eeckhoudt had a paper accepted in the International Economic Review in 1992 whose main result was using the erroneous Rothschild-Stiglitz (1971) condition. Christian Gollier, who was visiting Dionne at the time, found a counterexample to the main result in the Dionne-Eeckhoudt paper. The team spent an entire month trying to find the error in that paper, before realizing that the problem was in Rothschild and Stiglitz (1971)! This led to a new version of their paper that was eventually published in the same journal by Dionne et al. (1993).Footnote 6
 Currently, the line of research described above is still used in a few strands of the economics literature. Perhaps most significantly, it is used for topics at the frontier between decision theory and behavioral economics. Indeed, these results are instrumental to understanding the effect on risk taking of pessimism and heterogeneity in beliefs (Abel 2002; Barro 2006), anticipatory feelings (Brunnermeier et al. 2007), and ambiguity aversion (Gollier 2011).",20
47.0,2.0,Journal of Risk and Uncertainty,17 September 2013,https://link.springer.com/article/10.1007/s11166-013-9177-5,Arrow’s theorem of the deductible: Moral hazard and stop-loss in health insurance,October 2013,Jacques H. Drèze,Erik Schokkaert,,Male,Male,Unknown,Male,"In its simplest form, a medical insurance problem concerns an individual facing uncertainty about her future health condition. There are S states of health indexed s = 1, … , S with probabilities p

s
. Individuals have conditional preferences between vectors \((M_{s},C_{s})\in \Re _{+}^{2}\), where \(M_{s}\geqslant 0\) and \(C_{s}\geqslant 0\) stand respectively for medical expenditures and for disposable wealth (or expenditures on consumption exclusive of medical expenditures) in state s.Footnote 4 In general these preferences could be represented by state-dependent utility functions \(U_{s}^{{}}(M_{s},C_{s})\). To simplify the analysis we assume, in line with much of the related literature, that preferences are separable between medical expenditure and consumption and that preferences over disposable wealth are state-independent, i.e. \(U_{s}^{{}}(M_{s},C_{s})=f_{s}(M_{s})+g(C_{s})\).Footnote 5 The function f

s
(M

s
) captures both the effect of medical expenditures on health and the effect of health on utility.Footnote 6 We assume f

s
and g to be continuously differentiable and strictly concave, i.e. \(f_{s}^{\prime }>0,~f_{s}^{\prime \prime }<0,~g^{\prime }>0,~g^{\prime \prime }<0\). We also assume that resources are state-independent, i.e. W

s
 = W

t
 = W for all s, t = 1, … , S. Under these assumptions, preferences over S-vectors of medical expenditures and disposable wealth are represented by the expected utility
 The individual may buy medical insurance α

s

M

s
, 0 ≤ α

s
 ≤ 1 at a premium
 where λ is a state-independent loading factor and α

s
 is a state-specific insurance rate (with 1 − α

s
 as the coinsurance rate). The assumption that the insurance rate α

s
 can be state-specific seems to suggest that the state s is observable. This is in general not a realistic assumption. However, we will show that the observability of the states is not needed to implement the optimal policy in a first best-setting. Let us now consider optimal health insurance in a first-best setting without moral hazard. This means that the individual decisions about medical expenditures in state s take into account their impact on the premium π. The optimal policy is then found by solving the problem
 subject to Eq. 2. The first-order conditions are
 Simplifying these first-order conditions immediately yields
 and
 Equation 6 shows that medical expenditures are set optimally, with marginal benefits equal to marginal costs in each state s. Equation 7 is more interesting. Since \((1+\lambda )\overline {g}^{\prime }\) is independent of s, \(g_{s}^{\prime }\) (and therefore (1 − α

s
)M

s
) will be the same for all states s with α

s
 > 0. Define the deductible D := (1 − α

s
)M

s
 and write \(g_{D}^{\prime }\) for the marginal utility of wealth at C = W − π − D. We can then rewrite Eq. 7 as
 This is precisely Arrow’s theorem of the deductible. The marginal utility of wealth must be the same in all states for which α

s
 > 0; if medical expenditures are smaller than D, expenses are fully borne by the insured. Note that, if the loading factor λ = 0, we get full insurance (\(g_{s}^{\prime }=\overline {g}^{\prime }\) for all s). Note also that this deductible policy can easily be implemented, even if the state s is not observable. It is readily verified that, under DARA preferences,Footnote 7 in the optimum D is increasing in W and λ but decreasing in risk aversion, as measured for instance by the Arrow-Pratt coefficient of relative risk aversion.",18
47.0,2.0,Journal of Risk and Uncertainty,15 September 2013,https://link.springer.com/article/10.1007/s11166-013-9174-8,Risk aversion and religion,October 2013,Charles N. Noussair,Stefan T. Trautmann,Nathanael Vellekoop,Male,Male,Male,Male,"We use data from the LISS panel, managed by CentERdata, an organization affiliated with Tilburg University. The LISS panel consists of approximately 9,000 individuals, who complete a questionnaire over the internet each month. Respondents are reimbursed for the costs of completing the questionnaires four times a year. Additionally, incentivized economic experiments are conducted routinely on the LISS panel. A payment infrastructure is available to pay participants according to their decisions in experimental tasks. In terms of observable background characteristics, the LISS panel is a representative sample of the Dutch population. A large number of background variables are available, including data from a prior survey on religious beliefs and participation, and measures of risk attitudes from a study by Noussair et al. (2013). The sample consists of 2,304 individuals of whom 906 were in a real payoff condition in which the risk preference elicitation involved monetary incentives. Risk attitudes were measured by letting each participant choose, in five trials, between a lottery that paid €65 or €5 with equal probability and thus had an expected value of €35, and a sure payoff that differed by trial. The sure payoff varied from €20 to €40 in steps of €5. Each of the five choices was presented on a separate screen, and the order of the sequence of sure payoffs was counterbalanced among subjects. That is, for one half of participants, the first decision consisted of a choice between the lottery and a sure payment of €20, the second decision was between the lottery and €25, etc. For the other half of subjects, the first decision consisted of a choice between the lottery and a sure payment of €40, the second decision was between the lottery and €35 for sure, etc. The side of the screen (left/right) on which the lottery and the sure payoff appeared was also counterbalanced, with one half of the subjects having the lottery always displayed on the left of their screen, and the other half having it always shown on the right. Subjects did not learn of the actual outcome of any of the lotteries during the experimental session. Each lottery was presented in terms of a die roll, with the die representing a computerized equal probability draw (see Appendix A for an example of a screen shot illustrating the format). 906 subjects made these choices for potentially real stakes. For each subject in the Real stakes condition, one decision problem she faced was randomly selected to potentially count as her earnings. The prize was paid to a given individual with a probability of 1/10. This allowed for significant payoffs to some individuals (Benjamin et al. 2010).Footnote 1 The probabilities that an individual would be paid, and that any given decision would count conditional on her being paid, was known at the time she made her decisions. Another 718 subjects made the same decision, but with hypothetical payoffs. Additionally, another 680 subjects made the same choices, but with hypothetical payoffs scaled up by a factor of 150. There are no differences in observed average risk aversion levels between hypothetical and real payoffs of the same nominal stake size (z = .124, p = .90, Mann-Whitney-U test). Our measure of individual risk aversion is the number of instances in which a subject chose the sure payoff. Thus, our risk aversion measure ranges from a lowest possible value of 0 to a highest possible value of 5. A risk neutral agent would make either one or two safe choices, out of the five choices, and making more than two safe choices indicates risk aversion. More safe choices indicate greater risk aversion. The survey on religion that participants had completed earlier contains data on religious activities and beliefs of the survey participants at the date of the survey, as well as responses reporting their parents’ activities when the participant was 15 years old. Table 1 provides summary statistics of responses to each question for each religious group. The religiosity variables we employ are the following. We define dummy variables for frequency of church attendance. The categories are church/service attendance of more than once a week, once a week, and once a month. We also use the same categories of attendance frequency at age 15. We define denomination dummies for adherence to the Catholic and Protestant faiths. The variable “degree of belief” is measured in two ways. The first is with the response to a question in which the respondent was asked to indicate one of six degrees of belief in God. These ranged from 1: “I do not believe in God” to 6: “I believe without any doubt in God.” The second measure of the strength of religious belief is a count of the number of affirmative answers on a set of seven questions asking the subjects whether they believe in specific Christian theological concepts. These are (i) life after death, (ii) existence of heaven, (iii) the Bible as the word of God, (iv) existence of hell, (v) the devil, (vi) that Adam and Eve existed, and (vii) that it makes sense to pray. Finally, we include dummy variables for the frequency of prayer outside of religious services. Table 1 also shows the average values for two sets of independent variables that we use in our analysis. Controls A consist of the purely exogenous variables of gender, age, treatment, and counterbalancing in the presentation. Controls B consist of a set of socioeconomic background variables. These consist of marital status, number of children, income, homeownership and health status, educational and occupational status, and whether one has a Dutch or a foreign passport. The table also provides averages of the responses to the religiosity questions and of the control variables, for Catholics and Protestants separately. A number of interesting patterns are evident from the table. Overall, 42.4% of respondents are affiliated with either the Catholic or a Protestant church. This compares to 66.3% of respondents’ parents at the time they were 15 years old, illustrating the decline in church membership over the last several decades in The Netherlands (Dekker et al. 1997). Almost all of the respondents who are currently affiliated—more than 94%—report that their parents were church members when they were 15 years old. On average, Protestants attend church services more often, pray more, and indicate stronger religious beliefs than Catholics. The demographics are similar between the two groups. The religiously affiliated are somewhat more likely to be female and older than average. Church members are more likely to be married and less likely to be divorced than the overall population.",106
47.0,2.0,Journal of Risk and Uncertainty,18 September 2013,https://link.springer.com/article/10.1007/s11166-013-9173-9,Normalized measures of concavity and Ross’s strongly more risk averse order,October 2013,Liqun Liu,Jack Meyer,,Unknown,Male,Unknown,Male,"This section discusses in some detail a way to normalize -u″(x) as a measure of the concavity of u(x). Like the A-P risk aversion measure Au(x), the normalization employed involves dividing -u″(x) by marginal utility. The procedure used here differs from that of A-P, however, in that the divisor is chosen to be a constant, and is the value of marginal utility at a specific point in [a, b]. Any point in [a, b] can be selected. The analysis proceeds to show that such normalized concavity measures can be used in many of the same ways as the A-P risk aversion measure. In addition, one particular divisor yields a normalized measure of concavity that fully characterizes Ross’s strongly more risk averse order. This analysis is reserved for Section 2. To keep the discussion as simple as possible, the assumption is made that the supports of all random alternatives lie in some bounded interval [a, b], that the decision maker is risk averse over this interval, and that marginal utility is bounded and greater than zero. This implies that u′(a) denotes the largest possible value of marginal utility for the decision maker, and u′(b) the lowest possible value.Footnote 3
 Using the magnitude of the second derivative of u(x) to measure its concavity is hardly a new idea.Footnote 4 As Arrow points out, however, using only -u″(x) to measure concavity in an expected utility context is problematical in that a von-Neumann-Morgenstern u(x) is unique up to a positive linear transformation, and thus u″(x) (and u′(x)) can always be scaled by an arbitrary positive constant. This scaling ambiguity is eliminated by Arrow and Pratt when they divide -u″(x) by u′(x) to form Au(x). The normalization procedure used here similarly divides by marginal utility, but now the denominator is chosen to be marginal utility evaluated at a particular point x0 in [a, b]. Thus, the divisor, u′(x0), is a fixed value. To establish notation, let Cu(x; x0) be defined by \( {\mathrm{C}}_{\mathrm{u}}\left(\mathrm{x};{\mathrm{x}}_0\right)=\frac{-\mathrm{u}""\left(\mathrm{x}\right)}{\mathrm{u}\hbox{'}\left({\mathrm{x}}_0\right)} \). Cu(x; x0) is referred to as the normalized measure of the concavity of u(x) with normalization factor u′(x0). While such normalized measures of concavity have been discussed, their uses have not been presented or supported to the extent that they are here. The first property of Cu(x; x0) noted here involves using these normalized measures to represent the risk preferences for an expected utility maximizing decision maker. As is the case for Au(x), Cu(x; x0) completely and uniquely represents risk preferences on [a, b] when utility is monotonic. That is, if Cu(x; x0) is a normalized measure of the concavity of u(x), then this same measure is obtained from all utility functions that are a positive linear transformation of u(x). Furthermore, integrating Cu(x; x0) twice recovers the utility function up to a positive linear transformation. Thus, if one knows Cu(x; x0), Au(x) can be determined and vice versa. This property of Cu(x; x0) allows all familiar concepts and terms concerning risk preferences to be stated as restrictions on Cu(x; x0). Prudence and temperance, for instance, are identified by the signs of the first two derivatives of Cu(x; x0). The second property of Cu(x; x0) involves an “in the small” result that is quite similar to that relating Au(x) and the usual risk premium π(x). Arrow and Pratt, in their argument relating Au(x) to the risk premium π(x), use analysis that can be easily adapted to provide an interpretation for Cu(x; x0). A-P consider two different changes to a fixed starting point x. One is the introduction of a small risk at x which is carried out by adding \( \tilde{z} \) to x with \( \mathrm{E}\left(\tilde{z}\right)=0 \). The utility change, also called the utility premium,Footnote 5 from this newly introduced risk is given by \( \left[\mathrm{Eu}\left(\mathrm{x}+\tilde{\mathrm{z}}\right)-\mathrm{u}\left(\mathrm{x}\right)\right] \). This change is negative for risk averse decision makers. A-P show that this utility premium is approximately equal to u ′ ′ (x)·σ
z
2/2 for small \( \tilde{\mathrm{z}} \). The second change to fixed starting point x discussed by A-P is a constructed change. This change subtracts π(x), the risk premium, from x, and by construction, alters utility by an amount that is equal to the loss in utility from the introduction of risk. Subtracting π(x) from x gives a utility change equal to [u(x - π(x)) - u(x)] which is approximated by u′(x)(−π(x)). Equating the sizes of the two utility changes gives the very familiar equation: This equation indicates that π(x) is proportional to the product of the size of the risk, σz
2, and the risk aversion measure Au(x). In their analysis, Arrow and Pratt assume that the starting point is a nonrandom value x, and that the introduction of risk and the subtraction of the risk premium each occur at this nonrandom starting point. Because their starting point is certainty, this assumption makes sense for analysis of the question that is being addressed. It is the case however that the structure of the A-P argument allows the introduction of risk and the subtraction of a risk premium to occur at different points; that is, the locations where these two changes occur can be decoupled. As in the A-P analysis, consider an introduction of risk that occurs at x. As before, the change in utility from this risk introduction is approximately u ′ ′ (x)·σ
z
2/2 for the small risk \( \tilde{\mathrm{z}} \). Now assume that a risk premium π is subtracted not from x, but from x0, a fixed value in [a, b]. Again choose the size of π so that the utility decrease caused by subtracting π from x0 is equal to the utility decrease from the introduction of risk at x. Subtracting this risk premium leads to a utility change that is approximately equal to u′(x0)(−π) . When these two utility changes are equated and the notation is expanded to reflect the fact that the risk premium depends on where the risk increase occurs and also where the risk premium is subtracted, π(x; x0) is given by: Thus, Cu(x; x0) has the following interpretation. For a small increase in risk at x in interval [a, b] and with a risk premium subtracted at x0, the risk premium is proportional to the product of the size of the normalized concavity measure and the size of the risk. This interpretation is very similar to that connecting risk premium π(x) to Au(x). Both π(x) and π(x; xo) measure the decision maker’s reaction to a small risk, converting the subjective measure whose unit is utils to an equivalent amount of the outcome variable given up at a point.Footnote 6 The difference between π(x) and π(x; x0) is where the subtraction of the risk premium takes place. The relationship between the risk aversion and concavity measures is given by: \( \left[{\mathrm{A}}_{\mathrm{u}}\left(\mathrm{x}\right)\right]\left[\frac{\mathrm{u}\prime \left(\mathrm{x}\right)}{\mathrm{u}\prime \left({\mathrm{x}}_0\right)}\right]={\mathrm{C}}_{\mathrm{u}}\left(\mathrm{x};{\mathrm{x}}_0\right) \). This implies that the two risk premiums are also related by the same scale factor \( \frac{\mathrm{u}\prime \left(\mathrm{x}\right)}{\mathrm{u}\prime \left({\mathrm{x}}_0\right)} \). This scale factor behaves much like an exchange rate, and is the rate of exchange between additional units of the outcome variable at x and at x0. The A-P risk premium π(x) represents the valuation of the risk increase in the local currency since x is where the risk is introduced, while π(x; x0) represents the valuation of this same risk increase, but in a distant or foreign currency, the value of a unit of outcome at x0. Each of the two different risk premiums convert the utility loss or “pain” caused by the introduction of risk into an equivalent change in outcome, but the outcome changes occur at different points. The final properties of Cu(x; x0) discussed here and in the next two sections involve “in the large” results, and are the primary purpose for discussing normalized measures of concavity. First, for two utility functions u(x) and v(x), Cu(x; x0) ≥ Cv(x; x0) for all x in [a, b] and x1 > x0 implies that Cu(x; x1) ≥ Cv(x; x1) for all x in [a, b]. That is, a larger normalized concavity measure with normalization factor u′(x0) implies a larger normalized concavity measure for all normalization factors u′(x1) where x1 > x0. The proof of this result involves steps similar to those taken when proving Theorem 1 and is omitted here. Second, C(x; a) and C(x; b) have additional “in the large” properties. Specifically, Cu(x; a) ≥ Cv(x; a) for all x in [a, b] implies that Au(x) ≥ Av(x) for all x in [a, b], which in turn implies that Cu(x; b) ≥ Cv(x; b) for all x in [a, b]. Having a larger normalized concavity measure is sufficient for more A-P risk averse for one normalization, while larger for a different normalization provides a necessary condition for A-P more risk averse. Thus, normalized measures of concavity allow one to both strengthen or weaken the A-P more risk averse order. The proof of this result is again similar to the steps taken in proving Theorem 1 in the next section, and is omitted here. To conclude this section, a new result in the basic model of portfolio choice is presented. Consider the standard portfolio model where the expected utility maximizing decision maker chooses α, the share of starting wealth W0 to invest in the risky asset. Final wealth \( \tilde{\mathrm{W}} \) is given by \( \tilde{\mathrm{W}}={\mathrm{W}}_0\left(\alpha \cdot \tilde{\mathrm{r}}+\left(1-\alpha \right)\rho \right) \), where \( \tilde{\mathrm{r}} \) and ρ are the gross return on the risky and the riskless assets, respectively. Denote the optimal choices for decision makers with utility functions u(W) and v(W) as αu and αv, respectively. It is well known that if Au(x) ≥ Av(x) for all x, then αu ≤ αv. This finding of Arrow and Pratt is augmented here by demonstrating a similar result, but one where the assumption of more A-P risk averse is replaced with an assumption on normalized concavity measures. When u(W) is more concave than v(W) in the sense that Cu(W; ρW0) ≥ Cv(W; ρW0) for all W, then αu ≤ αv as well. That is, u(W) more concave in this sense implies choosing to invest less in the risky asset. For this result, it is important that the normalization factor be u′(ρW0), marginal utility at the wealth that results from investing only in the risk-free asset.Footnote 7
 The demonstration of this result is straightforward. The first order condition for finding the optimal α is \( \mathrm{E}\left[\mathrm{u}\prime \left(\tilde{\mathrm{W}}\right){\mathrm{W}}_0\left(\tilde{\mathrm{r}}-\rho \right)\right]=0 \). To show that αu ≤ αv, it is sufficient to demonstrate that \( \mathrm{E}\left[\mathrm{u}\prime \left(\tilde{\mathrm{W}}\right){\mathrm{W}}_0\left(\tilde{\mathrm{r}}-\rho \right)\right]\le 0 \) when evaluated at αv. Now it is the case that, Cu(W; ρW0) ≥ Cv(W; ρW0) for all W implies that \( \frac{\mathrm{u}\prime \left(\mathrm{W}\right)}{\mathrm{u}\prime \left(\rho {W}_o\right)}\ge \frac{\mathrm{v}\prime \left(\mathrm{W}\right)}{\mathrm{v}\prime \left(\rho {W}_o\right)} \) when W ≤ ρW0 (or equivalently r ≤ ρ) and \( \frac{\mathrm{u}\prime \left(\mathrm{W}\right)}{\mathrm{u}\prime \left(\rho {W}_o\right)}\le \frac{\mathrm{v}\prime \left(\mathrm{W}\right)}{\mathrm{v}\prime \left(\rho {W}_o\right)} \) when W ≥ ρW0 (or r ≥ ρ) . Therefore, evaluated at αv, \( \frac{1}{\mathrm{u}\prime \left(\rho {W}_o\right)}\mathrm{E}\left[\mathrm{u}\prime \left(\tilde{\mathrm{W}}\right){\mathrm{W}}_0\left(\tilde{\mathrm{r}}-\rho \right)\right]\le \frac{1}{\mathrm{v}\prime \left(\rho {W}_o\right)}\mathrm{E}\left[\mathrm{v}\prime \left(\tilde{\mathrm{W}}\right){\mathrm{W}}_0\left(\tilde{\mathrm{r}}-\rho \right)\right]=0 \), which implies αu ≤ αv. This result indicates that normalized concavity measures can play a role in comparative static analysis similar to the role played by the A-P more risk averse order. It is worth noting that Au(W) ≥ Av(W) for all W is neither necessary nor sufficient for Cu(W; ρW0) ≥ Cv(W; ρW0) for all W. Thus, this finding enlarges the set of decision makers for whom it is known that αu ≤ αv.",7
47.0,2.0,Journal of Risk and Uncertainty,20 September 2013,https://link.springer.com/article/10.1007/s11166-013-9176-6,Economic consequences of Nth-degree risk increases and Nth-degree risk attitudes,October 2013,Elyès Jouini,Clotilde Napp,Diego Nocetti,Unknown,Female,Male,Mix,,
47.0,3.0,Journal of Risk and Uncertainty,10 December 2013,https://link.springer.com/article/10.1007/s11166-013-9181-9,Sign-dependence in intertemporal choice,December 2013,Mohammed Abdellaoui,Han Bleichrodt,Olivier l’Haridon,Male,,Male,Mix,,
47.0,3.0,Journal of Risk and Uncertainty,29 November 2013,https://link.springer.com/article/10.1007/s11166-013-9179-3,Discriminating among probability weighting functions using adaptive design optimization,December 2013,Daniel R. Cavagnaro,Mark A. Pitt,Jay I. Myung,Male,Male,Male,Male,"An ADO framework for discriminating among models of risky choice was presented by Cavagnaro et al. (2013) and Chaloner and Verdinelli (1995). In this framework, an experiment proceeds across a sequence of stages, or mini-experiments, in which the design at each stage (e.g., a set of one or more choice stimuli) is optimized based on the data observed in preceding stages. Optimizing the design means identifying and using the design that is expected to provide the most useful information possible about the models under investigation. The optimization problem to be solved at each stage is formalized as a Bayesian decision problem in which the current state of knowledge is summarized in prior distributions, which are incorporated into an objective function to be maximized. New information gained from observing the result of a mini-experiment is immediately incorporated into the objective function via Bayesian updating of the prior distributions, thus improving the optimization in the next mini-experiment. Formally, the objective function to be maximized at each stage can be formulated as
 where s ( = 1,2,...) is the stage of experimentation, m ( = 1,2,..., K) is one of K models under consideration, d is an experimental design to be optimized, and y is the choice outcome of a mini-experiment with design d. In the above equation, \(p_{s}(y|m,d) = \int _{\theta } p(y|\theta _{m},d)p_{s}(\theta _{m})d\theta _{m}\) is the marginal likelihood of the outcome y given model m and design d, which is the average likelihood weighted by the parameter prior p

s
(θ

m
). Here, p(y | θ

m
, d) is the likelihood function that specifies the probability of the outcome y given the parameter value θ

m
under model k. For instance, for a choice experiment between two gambles, the likelihood function would be a binomial likelihood. The expression \(p_{s}(y|d)=\sum _{m=1}^{K} p_{s}(m)p_{s}(y|m,d)\) is the “grand” marginal likelihood, obtained by averaging the marginal likelihood across K models weighted by the model prior p

s
(m). Equation 1 is called the “expected utility” of the design d because it measures, in an information theoretic sense, the expected reduction in uncertainty about the true model that would be provided by observing the outcome of a mini-experiment conducted with design d (Cavagnaro et al. 2010). On stage s of an ADO experiment, the design \(d_{s}^{*}\) to be implemented in the next mini-experiment is chosen by maximizing U(d). Upon the observation of a specific experimental outcome z

s
 in that mini-experiment, the prior distributions to be used to find an optimal design for the next stage are updated via Bayes’ rule and Bayes factor calculation (e.g., Gelman et al. 2004) according to the following equations
 In the equation \(BF_{(k,m)}\left (z_{s}|d_{s}^{*}\right )\) is the Bayes factor that is defined as the ratio of the marginal likelihood of model k to that of model m given the outcome z

s
 and optimal design \(d_{s}^{*}\) (Kass and Raftery 1995). To recap, the ADO process involves, in each stage of experimentation, finding the optimal design \(d_{s}^{*}\) by maximizing the utility function U(d), conducting a mini-experiment with the optimized design, observing an outcome z

s
, and updating the model and parameter priors to the corresponding posteriors through Bayes’ rule, as illustrated in Fig. 3. This process continues until one model emerges as a clear winner under some appropriate stopping criterion, such as p

s
(m) > 0.99.
 Schematic illustration of the sequential steps of ADO, adapted from Fig. 2 of Cavagnaro et al. (2013) Before closing this section, we discuss two noteworthy features of ADO. Firstly, an advantage of ADO is that model fitting and model selection are incorporated into the procedure for selecting optimal designs. Model fitting is done through Bayesian updating of the parameter estimates, and model selection can be done through comparing the marginal likelihoods of the models. More precisely, the posterior probability of model m after s stages, in which choices y
1, … , y

s
were observed, is defined as the ratio of the marginal likelihood of y
1, … , y

s
given m to the sum of the marginal likelihoods of y
1, … , y

s
given each model under consideration, where the marginal is taken over the prior parameter distribution. The ratio of the posterior probabilities of two models yields the Bayes factor (Kass and Raftery 1995). It is worth noting that the Bayes factor, as a model selection measure, will properly account for model complexity or flexibility so as to avoid over-fitting, unlike measures that assess only goodness of fit such as r
2(e.g., Myung 2000, p. 199). Secondly, given that the priors are updated independently for each participant, each participant in a risky choice experiment could respond to different choice-stimuli that are best suited to the participant’s particular preferences. Thus, ADO’s efficiency partially derives from adapting to an individual’s unique behavior. Furthermore, the Bayesian foundation of ADO gives it flexibility to accommodate various forms of stochastic error, which is essential for adequately describing real choice data (e.g., Hey 2005). For example, if a stochastic error function is assumed such that p(y | m, d, 𝜖) is the probability of the outcome y in a mini-experiment with design d given that the true model is m with stochastic error parameter 𝜖, then the likelihood function p(y | m, d) in Eq. 1 is obtained by marginalizing p(y | m, d, 𝜖) with respect to the prior on 𝜖.",44
47.0,3.0,Journal of Risk and Uncertainty,19 November 2013,https://link.springer.com/article/10.1007/s11166-013-9180-x,A further exploration of the uncertainty effect,December 2013,Yitong Wang,Tianjun Feng,L. Robin Keller,Unknown,Unknown,Unknown,Unknown,,
47.0,3.0,Journal of Risk and Uncertainty,12 November 2013,https://link.springer.com/article/10.1007/s11166-013-9178-4,"Admissible utility functions for health, longevity, and wealth: integrating monetary and life-year measures",December 2013,James K. Hammitt,,,Male,Unknown,Unknown,Male,"The effects of health, longevity, and wealth (or consumption) on individual utility have been represented by a variety of alternative utility functions (Rey and Rochet 2004). Grossman (1972) assumed a general function, where h

τ
 is the health state, φ

τ
 the rate of service flow, and c

τ
 is other consumption in period τ. Subsequent authors have imposed more structure, often adapting Yaari’s (1965) model (in which lifetime utility is the expected present value of period utilities that depend on consumption) to include health state, where δ is a discount factor,Footnote 1
s

τ
 is the probability of surviving to age τ, and T is a maximum possible age (e.g., Cutler and Richardson 1997; Meltzer 1997). The period-utility function in equation (1.2) is often assumed to be multiplicative in health and consumption, where Q(·) and V(·) are monotone increasing (e.g., Garber and Phelps 1997; Murphy and Topel 2006
Footnote 2). Bleichrodt and Quiggin (1999) provide an axiomatic basis for the representation in equations (1.2–1.3) under both expected-utility and rank-dependent expected-utility theories. Bommier and Villeneuve (2012) propose a generalization of the Yaari (1965) model (1.2) that provides more flexibility in representing risk posture with regard to longevity, but do not consider the effect of health.Footnote 3
 Alternative representations for a single period include the monetary-loss-equivalent model, in which the effect of impaired health is equivalent to the effect of reduced consumption, where h* represents full health and m(h* – h) is a monetary value (Evans and Viscusi 1991), and an additively separable model, in which Q(·) and V(·) are monotone increasing in their arguments (Eeckhoudt et al. 1998). The literature on valuing mortality risk typically uses a health-state-dependent specification, where the health state is restricted to two values, alive and dead (e.g., Drèze 1962; Jones-Lee 1974; Weinstein et al. 1980). Equation (1.7) can be generalized to include multiple alternative health states while living (Evans and Viscusi 1991; Sloan et al. 1998). The conventional single-period model for VSL assumes the individual seeks to maximize expected state-dependent utility of wealth, where p is the probability of dying in the current period and U

a
 and U

d
 represent utility conditional on surviving and dying, respectively. Differentiating equation (1.8) with respect to w and p yields It is conventionally and reasonably assumed that where primes denote derivatives; i.e., survival is preferred to death, the marginal utility of wealth is non-negative and larger given survival than death, and weak risk aversion with respect to wealth in both states. Under these assumptions, VSL is positive and increases with wealth and current-period mortality risk. The effect of a change in health or longevity conditional on survival is theoretically ambiguous (Hammitt 2002). An increase in health or longevity may be presumed to increase U

a
(w), increasing the numerator of equation (1.9). However, it may also increase U'

a
(w), increasing the denominator. Hence the effect on VSL cannot be signed without further assumptions. VSLY is derived from VSL by dividing VSL by longevity conditional on survival, t, or by the present value of a series of t years (Moore and Viscusi 1988; Hirth et al. 2000). Implicitly, the function U

a
(w) in equation (1.9) is replaced by a function U

a
(w,t), which is assumed to satisfy assumptions (1.10a-c) for all positive values of t and to be increasing in t for relevant values of w.Footnote 4 This implies VSLY is positive, increasing in wealth and current-period mortality risk. As for VSL, the effect of an increase in longevity on VSLY cannot be signed without further assumptions. Substituting U

a
(w,t) for U

a
(w) in equation (1.9) and dividing by t yields Differentiating with respect to t yields where N and D are defined in equation (1.11). The first term in brackets is negative, the second is positive, and the third depends on the sign of the cross-partial derivative of U

a
(w,t) with respect to its two arguments. It is clear from equation (1.12) that VSLY cannot be independent of t except under restrictive conditions. While the expression in brackets might equal zero for some values of t, for it to equal zero for all t in a finite range would require that changes in the first term (which is inversely proportional to t) would have to be exactly offset by appropriate changes in the first and second-order derivatives in the following terms. The QALY model for constant health state can be expressed as or It weights longevity t by q(h), the ‘health-related quality of life’ (HRQL) associated with health state h, scaled so that a value of 1 corresponds to full health and 0 to a state of health as bad as dead (health states worse than dead are permitted, for which HRQL < 0). The function V(·) accounts for risk posture with respect to longevity (and hence time preference). If V is not the identity function (implying risk neutrality toward longevity), the HRQL associated with health states other than full health and as bad as dead depend on whether QALY is represented using model (1.13) or (1.14). Under model (1.13), the HRQL q

t
(h) is the response to a time-tradeoff question in which the respondent states that a fraction q

t
(h) of lifespan t lived in full health is as desirable as lifespan t lived in health state h. Under model (1.14), the HRQL q

s
(h) is the response to a standard-gamble question in which the respondent states that a lottery with probability q

s
(h) of lifespan t lived in full health and complementary probability of immediate death is as desirable as lifespan t lived in health state h. In their pioneering paper, Pliskin et al. (1980, equations (4a, b)) propose a special case of model (1.13) and derive conditions on preferences under which it represents utility. These include mutual utility independence of h and t and constant proportional tradeoff of t for h. These conditions imply that V exhibits constant proportional risk posture (including risk neutrality as a special case).Footnote 5
 Bleichrodt et al. (1997) show that the risk-neutral case can be characterized by the conditions that the individual is risk-neutral with regard to longevity conditional on health for all health states, and indifferent to health state when longevity is zero (the so-called ‘zero condition’). Miyamoto and Eraker (1985) propose model (1.14) and Miyamoto et al. (1998) show that it represents preferences that satisfy the zero condition and ‘standard-gamble invariance’ (a weaker form of the condition that t is utility independent of h). In conventional cost-effectiveness analysis, QALYs are discounted at a constant positive rate, which implies constant absolute risk aversion with respect to longevity (including risk neutrality as a special case).Footnote 6
 The DALY (Murray 1994) is similar to the QALY in weighting longevity by an index of health. Unlike QALY, DALY is measured as a loss from an ideal health profile (one with full health and a specified survival function), but an inverse measure can be obtained by subtraction from this ideal profile, which has the property that more inverse DALYs are preferred to fewer. Like QALYs, DALYs are often discounted for time preference. In addition, life years are often weighted by a function of age τ (measured in years), τ
e
−βτ with β = 0.04. This function gives less weight to years lived when young or old than to years lived at intermediate ages. HYE is defined as the number of years of life in full health that are as desirable as a specified time lived in a health state, or to a specified time path through multiple health states (Mehrez and Gafni 1989). Unlike the QALY and DALY models, it does not assume a fixed weight for each health state or that the preference ordering over health states is independent of duration. Because it imposes less structure on preferences than the QALY and DALY models, it requires that the HYE for each time path over health states be elicited independently. It is rarely used in practice.",40
48.0,1.0,Journal of Risk and Uncertainty,01 March 2014,https://link.springer.com/article/10.1007/s11166-014-9185-0,An experimental test of prospect theory for predicting choice under ambiguity,February 2014,Amit Kothiyal,Vitalie Spinu,Peter P. Wakker,Male,Male,Male,Male,"Until the 1990s, quantitative descriptive studies focused on risk, in the absence of quantitative models for ambiguity.Footnote 4 These descriptive studies were often carried out in the probability triangle (probability distributions over three fixed outcomes). Since 1990, quantitative ambiguity models have become available (Gilboa 1987; Gilboa and Schmeidler 1989; Schmeidler 1989), and many extensions have been proposed after (reviewed by Etner et al. 2012). These models were all normatively motivated (Gilboa et al. 2012, p. 18 3rd para), assuming expected utility for risk, backward induction in an Anscombe and Aumann (1963) model, and (with a-maxmin excepted) universal ambiguity aversion. These assumptions, even if accepted normatively, fail descriptively.Footnote 5
 Some quantitative descriptive studies fitted data for simple situations with no more than two events.Footnote 6 Until recently, comparative studies of ambiguity models were qualitative, testing particular predictions. Ahn et al. (2013) compared kinked models (mostly multiple priors and rank-dependent models) with smooth models and found that the former performed better. Hayashi and Wada (2010) falsified multiple priors models by showing that not only the priors with maximal and minimal expected utility play a role in decisions, but also intermediate priors. HLM achieved a recent breakthrough: They extended the comparative studies of different nonexpected utility models in the probability triangle from risk to ambiguity. One key step was the introduction of a valuable device to generate ambiguity: a bingo blower. Another step was that they brought together different ambiguity models, making them quantitatively testable for empirically tractable one-stage prospects. HLM did include two models in their competition that they called prospect theory, but these were not Tversky and Kahneman’s (1992) model, or any other common model.Footnote 7 Hence, HLM did not test Tversky and Kahneman’s (1992) PT for ambiguity. Although HLM succeeded in implementing CEU in full generality, CEU has too many parameters to perform well. We will use Abdellaoui et al.’s (2011) source method to reduce the number of parameters of CEU. We will extend it to PT, the reference-dependent generalization of CEU. We next discuss multiple priors models. Sets of priors are often used as a tool of communication, such as when describing stimuli in experiments, but they rarely arise exogenously from naturally generated ambiguity. Thus, they do not arise exogenously with the bingo blower. They are, accordingly, hard to implement in incentivized experiments with guaranteed absence of deception. The focus of this paper is, hence, on endogenous sets of priors. These sets comprise even more parameters than PT or CEU (Eron and Schmeidler 2012). They have an even greater need for a specification of tractable submodels before they can become implementable. An important contribution of HLM was the introduction of such a tractable submodel. To the best of our knowledge, HLM were the first to reveal endogenous sets of priors from preferences when there are more than two events. We will use their submodel in our analysis (Subsection 3.3). Because our stimuli do not have exogenously given separate stages, we will, following HLM, not consider two-stage models of ambiguity such as those in Ergin and Gul (2009), Nau (2006) or Neilson (2010). Taking the stages endogenous, as in Klibanoff et al.’s (2005) smooth model, would lead to even more free parameters: the collection of all second-order probability distributions over the (first-order) probability measures over S is of higher dimensionality than, for instance, the collection of all sets of priors. Developing tractable subcases of the smooth model is a topic for future research.",41
48.0,1.0,Journal of Risk and Uncertainty,04 March 2014,https://link.springer.com/article/10.1007/s11166-014-9184-1,When Allais meets Ulysses: Dynamic axioms and the common ratio effect,February 2014,A. Nebout,D. Dubois,,Unknown,Unknown,Unknown,Unknown,,
48.0,1.0,Journal of Risk and Uncertainty,28 January 2014,https://link.springer.com/article/10.1007/s11166-014-9182-3,"On risk aversion, classical demand theory, and KM preferences",February 2014,Leonard J. Mirman,Marc Santugini,,Male,Male,Unknown,Male,"In this section, we study the effect of risk aversion on the optimal choice of the consumption profile \((x,\tilde {y}) \in \mathbb {R}_{+}^{2}\) with utility function \(U(x,\tilde {y})\), U
1, U
2 > 0, U
11, U
22 < 0. In the stochastic environment, x is the sure good, while \(\tilde {y}\) is the risky good due to the presence of randomness in the budget constraint. Using the KM utility representation, the consumer’s maximization problem under uncertainty is
 where \(\mathbb {E}_{\tilde {y}(x)}\) is the expectation operator over \(\tilde {y}(x)\), and v

KM
 is a strictly increasing and concave function, \(v_{KM}^{\prime } > 0,v_{KM}^{\prime \prime } \leq 0\). Note that the risky good depends on x through the budget constraint, i.e., y(x) = (I − P

x

x) / P

y
, where I is income, and P

x
and P

y
 are the prices of goods x and y, respectively. The effect of risk aversion is studied in three different cases: random income, random price for the sure good, and random price for the risky good. When income is random, Eq. 1 is rewritten as
 where \(\mathbb {E}_{\tilde {I}}\) is the expectation operator for \(\tilde {I}\). Proposition 1 states that the effect of risk aversion depends on the income effect when only income is random. The change in consumption due to a change in risk aversion does not result from a change in income as in the usual income effect. Instead Proposition 1 deals with the distribution of utilities associated with random income and the effect of that distribution of utilities on the choice of the consumption bundle as the consumer becomes more risk-averse. In particular, when the sure good is normal, a more risk-averse individual always consumes more of the risky good. 
Given Eq. 2, a more risk-averse individual
 
decreases the amount of a normal good x, 
increases the amount of an inferior good x, and
 
does not change the amount of good x if there is no income effect. See the Appendix. □ This counter-intuitive result is explained by the fact that the individual faces a utility gamble with each possible choice of the good x. The riskiness of the utility gamble is implicit in the optimal trade-off between the sure good and the risky good and is crucial to the choice of the individual, overshadowing the relevance of the riskiness of the good \(\tilde {y}\). In fact, a more risk-averse individual chooses a level of consumption that reduces the riskiness of the utility levels associated with random income. To see this, we proceed in two steps. We first establish a relationship between the income effect and the types of utility gambles an individual faces. We then explain how optimal behavior is changed when risk-aversion increases. To that end, it is convenient to adopt a simple distribution for income, i.e., \(\tilde {I} \sim \left ( \pi \circ \underline {I}, (1-\pi ) \circ \overline {I} \right )\), π ∈ [0, 1]. The income effect is key in explaining how changes in x affect the riskiness of the utility gambles. To see this, let \(x_{\underline {I}}\) and \(x_{\overline {I}}\) be the optimal consumption for the sure good when π = 1 and π = 0, respectively. For nondegenerate distributions of income, \(x \in \left [\min \left \{x_{\underline {I}},x_{\overline {I}}\right \},\max \left \{x_{\underline {I}},x_{\overline {I}}\right \}\right ]\) is the range of possible choices. We consider two cases. Suppose that the sure good is normal, i.e., \(x_{\underline {I}} < x_{\overline {I}}\), and let \(MU(x,I) \equiv U_{1}\left (x,\frac {I-P_{x}x}{P_{y}}\right ) - U_{2}\left ( x,\frac {I-P_{x}x}{P_{y}}\right ) \frac {P_{x}}{P_{y}}\) be the marginal utility of consumption under income \(I \in \{\underline {I},\overline {I}\}\). Then, for any choice of x, the marginal utility under low income at the corresponding point of the lower budget constraint is smaller than the marginal utility under high income at the corresponding point of the upper budget constraint. Moreover, when the marginal utility under low income is tangent to the corresponding budget constraint, (i.e., \(x = x_{\underline {I}}\)), then the marginal utility under high income is strictly positive. Hence, for \(x \in [x_{\underline {I}},x_{\overline {I}}]\), the difference between utility levels \(U\left ( x,\frac {\overline {I}-P_{x}x}{P_{y}} \right ) - U\left ( x,\frac {\underline {I}-P_{x}x}{P_{y}} \right )\) is positive and strictly increasing in \(x \in [x_{\underline {I}},x_{\overline {I}}]\). In other words, a decrease in x brings the two utility levels closer together. In terms of gambles, this means that a decrease in x results in a less risky utility gamble. The relationship between x and the riskiness of the utility gamble is shown in Fig. 1 when the sure good is normal, i.e., \(x_{\underline {I}} < x_{\overline {I}}\). The straight lines represent the budget constraints under low income and high income, while the convex lines are indifference curves. Note that the bundles \(\left (x_{\underline {I}},y_{\underline {I}}(x_{\underline {I}})\right )\) and \(\left (x_{\overline {I}},y_{\overline {I}}(x_{\overline {I}})\right )\) are the optimal bundles under certain low income and certain high income, respectively.Footnote 6 When income is random, choosing x implies choosing the utility gamble
 for \(x \in [x_{\underline {I}},x_{\overline {I}}]\). From Fig. 1, the choice \(x_{\overline {I}}\) has a utility gamble corresponding to the solid circles, while the choice \(x_{\underline {I}}\) has a utility gamble corresponding to the empty circles. Hence, the gamble \(g(x_{\underline {I}})\) is less risky than the gamble \(g(x_{\overline {I}})\). In general, as shown in Fig. 1 this implies that, for \(x,x^{\prime } \in [x_{\underline {I}},x_{\overline {I}}], x < x^{\prime }\),
 Utility gambles with normal good x
 Suppose next that the sure good is inferior, i.e., \(x_{\underline {I}} > x_{\overline {I}}\), so that the marginal utility under high income is smaller than the marginal utility under low income at the corresponding point on the budget constraint. For \(x \in [x_{\underline {I}},x_{\overline {I}}]\), the difference between utility levels \(U\left ( x,\frac {\underline {I}-P_{x}x}{P_{y}} \right ) - U\left ( x,\frac {\overline {I}-P_{x}x}{P_{y}} \right )\) is positive and strictly decreasing in \(x \in [x_{\underline {I}},x_{\overline {I}}]\). In other words, an increase in x brings the two utility levels closer together. In terms of gambles, this means that an increase in x results in a less risky utility gamble, as depicted in Fig. 2, where the utility gamble associated with x
∗ is less risky than the utility gamble corresponding to x. In general, this implies that, for \(x,x^{\prime } \in [x_{\overline {I}},x_{\underline {I}}], x < x^{\prime }\),
 Utility gambles with inferior good x
 Having shown that the income effect determines the direction of a reduction in the riskiness of a gamble, we next turn to the optimal behavior. Without loss of generality, we define two different KM utility representations, \(W^{1}_{KM}(x,\tilde {y}(x)) = U(x,\tilde {y}(x))\) and \(W^{2}_{KM}(x,\tilde {y}(x)) = \varphi (U\left (x,\tilde {y}(x)) \right )\), φ′ > 0, φ′′ < 0, so that \(W^{2}_{KM}\) is strictly more risk-averse than \(W^{1}_{KM}\). Recall that \(MU(x,I) \equiv U_{1}\left (x,\frac {I-P_{x}x}{P_{y}}\right ) - U_{2}\left ( x,\frac {I-P_{x}x}{P_{y}}\right ) \frac {P_{x}}{P_{y}}\) is the marginal utility of consumption for \(I \in \{\underline {I},\overline {I}\}\). Then, the first-order conditions corresponding to preferences \(W^{1}_{KM}\) and \(W^{2}_{KM}\) are 
 and
 respectively. Here,
 is a weighting function that depends on the risk aversion of the individual, \(\rho (x,\underline {I},\varphi ^{\prime }) =1 - \rho (x,\overline {I},\varphi ^{\prime }) \in [0,1]\). Note that risk-aversion measured by the function φ enters the first-order condition only through the weighting function ρ. Remark 1 states the effect of risk aversion on the weighting function. When income is random, the more risk-averse individual adds more weight to the low value of income, i.e., \(\rho (x,\underline {I},\varphi ^{\prime }) > 1/2\). Given Remark 1, the effect of risk aversion is determined by the income effect, which orders the marginal utilities. When the sure good is normal, \(MU(x,\underline {I}) < MU(x,\overline {I})\), while an inferior sure good yields \(MU(x,\underline {I}) > MU(x,\overline {I})\). Combining Remarks 1 and 2 implies that a more risk-averse individual puts more weight on the lower marginal utility, which corresponds to the low income when the sure good is normal and the high income when the sure good is inferior.Footnote 7 Hence, a more risk-averse agent decreases the amount of the sure good if and only if it is normal. It is worth noting that before imposing the more risk-averse transformation φ, expected utility maximization yields a trade-off between the sure good and the risky good. However, the introduction of φ changes that trade-off by giving the more risk-averse individual an incentive to choose a less risky utility gamble. In the random income case, this is done by reducing the amount of the good x. From this vantage point, it appears that the cardinality of the utility function determines the consumer’s choice. However, it is clear from Fig. 1, that, for any normal good x, it is ordinal preferences that dictate a decrease in the amount of the good x, which results in a less risky utility gamble. Figure 3 further illustrates the effect of risk aversion on optimal behavior when preferences are Cobb-Douglas, i.e., U(x, y) = x
α
y
1 − α, α ∈ (0, 1).Footnote 8 Here, the sure good x is normal. The solid lines represent the utility functions, while the dotted decreasing lines represent the marginal utility functions. The points x
1and x
2are the optimal bundles corresponding to preferences \(W^{1}_{KM}\) and \(W^{2}_{KM}\), respectively. From Fig. 3, an increase in risk-aversion adds more weight to the marginal utility under low income, which decreases the amount of the sure good, i.e., x
1 > x
2, so as to reduce the riskiness of the utility gamble, i.e., \(\underline {u}^{1} < \underline {u}^{2} < \overline {u}^{2} < \overline {u}^{1}\).
 Optimal utility gamble with normal good x
 Having shown that the effect of risk aversion depends on the income effect when income is random, we next study the cases of random prices. Here, the relative strength of the income and substitution effects determine the effect of risk aversion on the optimal choice of x. When the price of the sure good is random, Eq. 1 is rewritten as
 where \(\mathbb {E}_{\tilde {P}_{x}}\) is the expectation operator for \(\tilde {P}_{x}\). Proposition 2 states that the effect of risk aversion is determined by the interplay of the income and substitution effects. 
Given Eq. 9, a more risk-averse individual
 
decreases the amount of a normal good
x, and
 
increases the amount of an inferior good
x
if and only if the income effect is stronger than the substitution effect. See the Appendix. □ To explain the results in Proposition 2, it is convenient to adopt a simple distribution for the price of the sure good, i.e., \(\tilde {P}_{x} \sim \left ( \pi \circ \underline {P}_{x}, (1-\pi ) \circ \overline {P}_{x} \right )\), π ∈ 0, 1. Without loss of generality, we define two different KM utility representations, \(W^{1}_{KM}(x,\tilde {y}(x)) = U(x,\tilde {y}(x))\) and \(W^{2}_{KM}(x,\tilde {y}(x)) = \varphi (U\left (x,\tilde {y}(x)) \right )\), φ′ > 0, φ′′ < 0, so that \(W^{2}_{KM}\) is strictly more risk-averse than \(W^{1}_{KM}\). Letting \(MU(x,P_{x}) \equiv U_{1}\left (x,\frac {I-P_{x}x}{P_{y}}\right ) - U_{2}\left ( x,\frac {I-P_{x}x}{P_{y}}\right ) \frac {P_{x}}{P_{y}}\) be the marginal utility of consumption for \(P_{x} \in \left \{\underline {P}_{x},\overline {P}_{x}\right \}\), the first-order conditions corresponding to preferences \(W^{1}_{KM}\) and \(W^{2}_{KM}\) are
 and
 respectively. Here,
 is a weighting function that depends on the risk aversion of the individual, \(\rho (x,\underline {P}_{x},\varphi ^{\prime }) =1 - \rho (x,\overline {P}_{x},\varphi ^{\prime }) \in [0,1]\). Note that risk-aversion measured by the function φ enters the first-order condition only through the weighting function ρ, as in the case of random income. Remark 3 states the effect of risk-aversion on the weighting function when P

x
 is random. When the price of the sure good is random, the more risk-averse individual adds less weight to the low value of P

x
, i.e., \(\rho (x,\underline {P}_{x},\varphi ^{\prime }) < 1/2\). The effect of risk aversion is determined by the income and substitution effects, which orders the marginal utilities. Consider the sign of the derivative of MU(x, P

x
) with respect to P

x
is useful in ordering the marginal utilities, i.e.,
 Here, \(I{}E_{P_{x}}\) and \(SE_{P_{x}}\), both negative, are proportional to and of the same sign as the income effect and the substitution effect, respectively. When the sure good is normal, both the income and substitution effects are negative, so that \(MU(x,\underline {P}_{x}) > MU(x,\overline {P}_{x})\). When the sure good is inferior, i.e., \(I{}E_{P_{x}} > 0\), the relative strengths of the income and substitution effects determine the ordering of the marginal utilities. For instance, if the (positive) income effect is stronger than the (negative) substitution effect, then, from Eq. 13, \(MU(x,\underline {P}_{x}) < MU(x,\overline {P}_{x})\). Remarks 3 and 4 explain the result in Proposition 2. In particular, when the sure good is normal, a more risk-averse individual puts less weight on the marginal utility corresponding to the high price of x. Proposition 2 is illustrated in Fig. 4 for the case of a normal good x. Due to the randomness of P

x
, the slope of the budget constraint makes the utility gamble less risky as consumption decreases. Specifically, when P

x
 is random, the pure substitution effect induces a less risky utility gamble in the direction of less quantity of the sure good, from x to x
∗, which increases the amount of the risky good y.
 Utility gambles with normal good x and random \(P_{x}\)
 When the price of the risky good is random, Eq. 1 is rewritten as
 where \(\mathbb {E}_{\tilde {P}_{y}}\) is the expectation operator for \(\tilde {P}_{y}\). Proposition 3 states that the effect of risk aversion is again determined by the interplay of the income and substitution effects. 
Given Eq. 14, a more risk-averse individual
 
decreases the amount of a normal good
x
if and only if the income effect is stronger than the substitution effect, and
 
increases the amount of an inferior good
x. See the Appendix. □ Proposition 3 is illustrated in Fig. 5, which shows that, when the price of the risky good is random, the substitution effect induces a less risky utility gamble by increasing the amount of the sure good from x to x
∗.
 Utility gambles with normal good x and random \(P_{y}\)
 Propositions 1, 2, and 3 establish the connection between risk aversion and classical demand theory implicit in KM, and explain, in that context, the limits of the intuition of the Ross (1981) critique on risk aversion. In particular, increasing the amount of the risky good when risk aversion increases, is natural and not counterintuitive, as thought by Ross (1981) and his followers. We now illustrate our results by considering specific classes of preferences: Cobb-Douglas, Leontief, and quasi-linear utility functions. Suppose that preferences are Cobb-Douglas, i.e., U(x, y) = x
α
y
β, α, β > 0, so that x is a normal good. Note that the results in Propositions 1, 2, and 3 continue to hold, even when the utility function is concave or convex, as long as there is an interior solution to the constrained optimization problem. For random income, more risk aversion has the effect of decreasing the amount of x. This result is even stronger when the price of x is random, since the income and the substitution effects go in the same direction. However, when the price of y is random, the income and the substitution effects not only go in opposite directions, but cancel each other out with Cobb-Douglas preferences. This is exactly the consumption-saving problem discussed in KM, in which the rate of return (the price of y) is random. Suppose next that preferences are Leontief, i.e., U(x, y) = u(min{x, y}), u′ > 0, u′′ < 0, so that x is a normal good. Then, there is no substitution effect and the income effect determines the direction of the change along with an increase in risk aversion. In particular, regardless of the source of risk, an increase in risk aversion always decreases the amount of the sure good in favor of the risky good. To see this, consider the income distribution \(\tilde {I} \sim ( 1-\pi \circ \underline {I}, \pi \circ \overline {I})\), \(\underline {I} < \overline {I}\). Moreover, assume that P

x
 = P

y
 = 1. Since the optimal solution lies in \(\underline {I}/2 \leq x \leq \overline {I}/2\), the individual’s maximization problem isFootnote 9
 The first-order condition is \(\pi u^{\prime }(x) - (1-\pi ) u^{\prime }(\underline {I}-x) = 0\) so that
 Consider next a more risk-averse individual, i.e., the more risk-averse individual’s maximization problem is
 
φ′ > 0, φ′′ < 0. The first-order condition is πφ′(u(x
∗))u′(x
∗) − (1 − π)φ′\((u(\underline {I}-x^{*})) u^{\prime }(\underline {I}-x^{*}) = 0\) so that
 Since φ is strictly concave and \(\underline {I}/2 \leq x^{\ast } \leq \overline {I}/2\), it follows that
 Hence, from Eqs. 16, 18, and 19,
 which implies that u′(x
∗) > u′(x) or x
∗ < x as shown in Fig. 6.
 Leontief preferences The more risk-averse individual consumes even more of the risky good due to the presence of only income effects.Footnote 10 This decrease in the amount of the sure good x yields a less risky utility gamble as depicted in Fig. 6. Note that, with Leontief preferences, the amount of x always decreases because there is only a pure income effect, i.e., without substitution. That is why Leontief preferences yield results opposite to the Arrow-Pratt’s portfolio problem, in which there is no income effect and only substitution effect. Suppose finally that preferences are quasi-linear, where x is a normal good. These preferences shed light on the Arrow-Pratt result, i.e., an increase in risk aversion increases the amount of income invested in the safe asset (i.e., allocated to the sure good in our context). We now demonstrate that the Arrow-Pratt result holds due to the absence of the income effect, and that the source of uncertainty lies in the rate of return of the risky asset. Specifically, we consider two cases and show stark difference in results between the two. First, consider the case in which there is no income effect for the sure good x, i.e., \(U(x,y) = u_{1}(x) + y, u_{1}^{\prime }>0,u_{1}^{\prime \prime } < 0\). When income is random, since there is no income effect, risk aversion has no effect on the amount of x. When the price of x is random, increased risk aversion causes the amount of x to decrease solely due to the substitution effect. However, for a random price of the risky good, the substitution effect dominates (since there is no income effect), which implies that the amount of x increases along with an increase in risk aversion. This result generalizes the result in the Arrow-Pratt portfolio problem. In fact, it is only in this case that increasing risk aversion increases the amount of the sure good without reference to income and substitution effects. However, the result is not robust to a slight modification in the utility function. To see this, consider the quasi-linear utility function, i.e., \(U(x,y) = x + u_{2}(y), u_{2}^{\prime }>0,u_{2}^{\prime \prime } < 0\). In this case, the sure good x is normal, so that if either income or the price of x is random, risk aversion decreases the amount of the sure good x. On the other hand, for random price of the risky good, the income and substitution effects pull in opposite directions. If the income effect is dominant, then an increase in risk aversion leads to a decrease in the amount of the sure good x. This last result illustrates that the Arrow-Pratt result is solely due to the absence of an income effect on x. Finally, note that another version of the Arrow-Pratt theorem is that, if income increases, then an individual with decreasing risk aversion reduces the amount of the sure good. This result is not general and is due only to the fact that, in the portfolio problem, there is no income effect for the sure good.",3
48.0,1.0,Journal of Risk and Uncertainty,22 January 2014,https://link.springer.com/article/10.1007/s11166-014-9183-2,Prospect theory and the “forgotten” fourfold pattern of risk preferences,February 2014,Marc Scholten,Daniel Read,,Male,Male,Unknown,Male,"While Kahneman and Tversky (1979) did not propose a parametric specification of the value function, they did use the following power value function for illustrative purposes (Kahneman and Tversky 1982) and to derive quantitative predictions from cumulative prospect theory (Tversky and Kahneman 1992): where 0 < α, β < 1 is diminishing absolute sensitivity, and λ > 1 is constant loss aversion. This value function has constant elasticity α for gains and β for losses, and therefore does not predict the M4 pattern. One prominent value function that does exhibit decreasing elasticity is the normalized exponential value function examined by Köbberling and Wakker (2005): where α, β > 0 is diminishing absolute sensitivity. The elasticity of this value function ranges from 1 (as x approaches 0) to 0 (as x approaches infinity). It is bounded from above by 1/α and from below by -λ/β. An alternative value function that also exhibits decreasing elasticity is the normalized logarithmic value function, previously developed in the domain of intertemporal choice (Rachlin 1992; Scholten and Read 2010), and recently applied to risky choice as well (Kirby 2011; Kontek 2011). It is given as follows: where α, β > 0 is diminishing absolute sensitivity. Just as with the normalized exponential value function, elasticity decreases from 1 (as x approaches 0) to 0 (as x approaches infinity).Footnote 2 The normalized logarithmic value function, however, is unbounded. The normalized value functions combine several desirable properties. First, they exhibit the three properties from prospect theory: Reference dependence, diminishing absolute sensitivity, and loss aversion. Second, they cover the entire range from constant sensitivity (as α, and β go to 0, so that v(x) = x for x ≥ 0 and v(x) = λx for x < 0), through diminishing absolute sensitivity (0 < α, β < 1), to insensitivity (as α and β go to infinity, so that v(x) = 0). Third, they exhibit decreasing elasticity, when the power value function exhibits constant elasticity. The power value function also differs from the normalized value functions in that insensitivity means v(x) = 1 (as α and β go to 0). To make it fully compatible with the other value functions, it may be normalized as well: where α, β > 0 is diminishing absolute sensitivity. Like the other value functions, it ranges between a linear function (as α and β go to 0) and a zero function (as α and β go to infinity). We will evaluate prospect theory with value functions 1, 2, and 3 on data containing the M4 pattern.",40
48.0,2.0,Journal of Risk and Uncertainty,27 April 2014,https://link.springer.com/article/10.1007/s11166-014-9188-x,Behavioral insurance: Theory and experiments,April 2014,Andreas Richter,Jörg Schiller,Harris Schlesinger,Male,Male,Male,Male,"EUT has long been the cornerstone for normative models of insurance demand under risk, and also provides a common base of comparison for various positive models. Indeed, pioneers in the early days of insurance economics, such as Karl Borch (1968), often referred to EUT under risk aversion as “the Bernoulli Principle,” in homage to the famous treatise by Daniel Bernoulli (1738). Results from EUT models are still used to explain many real-world transactions, such as the design of insurance contracts, which may entail deductibles and/or proportional risk sharing.Footnote 3 Indeed, various non-EUT models often support the results derived under EUT; but at other times they do not. A first attempt at collecting such comparisons was Machina (1995). Of course, EUT and its founding axioms have been heavily challenged. But another important stream of literature stems from doubts regarding the positive explanatory power of EUT. Actual contractual features, survey answers and experimental results are often inconsistent with expected utility. Many extensions of the standard EUT model give additional degrees of freedom and may have a higher predictive power for some individuals. However, the entangled relationship between perceptions and preferences remains. As controlled experiments became more common in economics, doubts about the general explanatory power of EUT and about rational-behavior models were reinforced (Starmer 2000). The appealing feature of laboratory experiments is that they can control the decision-making environment. This artificial environment is both boon and bane. Although we can control the complexity of the decision-making environment, and can thus focus more on preferences, we might not tend to trust information gleaned about preferences when such information is derived from artificial (or unrealistic) situations. Analyzing the transferability from laboratory experiments to the field is, fortunately, a real concern of many experimentalists (Harrison and List 2004). However, experimental evidence and “behavioral economics” need not be narrowly defined on positive theories differing from EUT. Instead, such research might best be viewed as an examination of models describing the actual behavior of individuals, including both alternatives to and modifications of standard EUT. Many basic observations are still in need of further explanation. For example, do people tend to behave in a manner that is more risk averse or less risk averse than theory predicts? Various authors have come to very different conclusions (Rabin 2000; Andersen et al. 2008; Sydnor 2010; Cohen and Einav 2007; Laury et al. 2009). And the literature is filled with questions based on this very notion, such as the famous equity-premium puzzle of Mehra and Prescott (1985). Moreover, many competing theories are hard to test for directly. Can we ever really know how individuals set reference points, if we want to test other aspects of prospect theory? Or can we empirically disentangle ambiguity beliefs from ambiguity preferences? Will theory help us design experiments or will it work in reverse? The importance of this interaction between theory and experiments is the main focus of Samuelson (2005), who espouses the complementary nature of these two approaches. In their own separate ways, both approaches need to abstract from reality. Apart from general theories of decision making under risk, behavioral insurance has some idiosyncrasies that make it a topic of special interest within behavioral economics and behavioral finance. Two important factors contribute to this interest in insurance. First, insurance markets constitute a specific institutional environment, which necessitates studies in their own right. Insurance contracts have very particular features. The second important factor is the psychological and economic evidence that indicates that individuals might react differently towards decisions within the domain of insurance compared to decisions made in other domains. Insurance markets differ in many ways from the markets for other types of goods. Insurance contracts are aleatory contracts: one purchases, in essence, a promise from the insurer to pay for contingent events. Risk aversion in monetary stakes as well as a belief that the insurer will pay as promised is what drives the demand for insurance. In addition, insurance contracts require “uberrimae fidei” (“utmost good faith”), which is a higher standard of disclosure from both parties than most other contracts. The role of information is more important in insurance markets than in most other markets. This is why many, if not most, studies of adverse selection and moral hazard are focused on insurance markets (e.g. Rothschild and Stiglitz 1976; Chiappori and Salanié 2000; Finkelstein and McGarry 2006; Einav et al. 2010). These two features of insurance contracts also (unfortunately) beget various types of insurance fraud (e.g. Crocker and Morgan 1998; Picard 1996). Even when claims are not fraudulent, insurable damages are oftentimes less than fully transparent. For instance, liability claims often include “non-economic damages,” which can be quite subjective. Indeed, the legal system is often used to limit—or at least to better delineate—such types of damages. For example, tort regulation has very definitive effects on insurance markets; see, among others, Viscusi and Born (2005). Insurance contracts are highly complex and mostly incomplete. In this situation, the fact that individuals are insured may lead to spillover effects in related health care or repair markets (Nell et al. 2009). An important role of behavioral insurance is to explain some of the discrepancies found in studies of insurance markets through new behavioral theories or modifications of classical theory. Some of the papers included in this special issue have exactly this purpose. The other important reason why behavioral insurance, at least to some extent, needs to be studied as an independent field is the difference in decision behavior regarding insurance choices vs. other risky choices. Psychological evidence suggests that individuals perceive risks differently in various contexts and also adapt different risk management solutions (Slovic 1987). For instance, it has been shown that subjects in laboratory experiments act in a more risk averse manner when confronted with an insurance decision than when the decision is framed in some alternative context (e.g. Schoemaker and Kunreuther 1979; Hershey and Schoemaker 1980; Kusev et al. 2009). While there also is some evidence to the contrary (Wakker et al. 1997), the findings seem to be especially robust in settings which closely resemble real-world insurance markets (see Hershey and Schoemaker 1980 for a general result and Buckley et al. 2012 for an application to health insurance). Einav et al. (2012) further confirm this hypothesis using field data on people’s choices of different insurance policies and their investment decisions in 401(k) plans. Barseghyan et al. (2011) go even further to show that risk attitudes might differ between different insurance policies. The fact that results for insurance contexts differ significantly from those for other contexts highlights the importance of establishing insurance as a field of behavioral research. Since it is common to use insurance data for testing behavioral models (e.g. Sydnor 2010; Barseghyan et al. (2013)), it is imperative for behavioral insurance to establish to what degree findings made within an insurance context can be extrapolated to other types of decisions. Similarly, behavioral insurance has the task of confirming whether or not behavioral observations from other fields also apply within a specific insurance context. Insurance decisions are perhaps the most basic type of choice under uncertainty. They are arguably one of the regularly encountered real-life choices which come closest to those posed by theoretical models. Thus, insurance markets can often be fruitful in gleaning insight into the validity of such theoretical models, particularly as first evidence. But many questions remain. Can we realistically model an aversion to large real-world financial losses in a laboratory setting? Will standard ceteris paribus assumptions hold for decisions made in the field? Or even if such decisions are made in isolation from other contemporaneous decisions, will the mere existence of other risks create a type of background noise that affects choices? And while we can control probabilities in any laboratory experiment in a de jure sense, can we really control them de facto? Indeed there are many possible associated epistemic risks over which the experimenter really has no control: Are the coins really fair? Are my answers truly held confidential? Are my choices meant to reflect my intelligence, as opposed to my tastes?",26
48.0,2.0,Journal of Risk and Uncertainty,02 May 2014,https://link.springer.com/article/10.1007/s11166-014-9189-9,Insurance demand and social comparison: An experimental analysis,April 2014,Andreas Friedl,Katharina Lima de Miranda,Ulrich Schmidt,Male,Female,Male,Mix,,
48.0,2.0,Journal of Risk and Uncertainty,03 May 2014,https://link.springer.com/article/10.1007/s11166-014-9186-z,Take-up for genetic tests and ambiguity,April 2014,Michael Hoy,Richard Peter,Andreas Richter,Male,Male,Male,Male,"Genetic tests have the potential to provide information about one’s health and mortality prospects. Such information can in turn be used by individuals to improve their life planning decisions, including whether to have children, better choosing an optimal savings plan, or deciding on what occupational training to acquire. If insurers are not allowed access to the information, then one can also improve (opportunistically) one’s insurance purchase decisions. Moreover, genomic science has advanced to the stage of being able to identify some so-called “disease genes” or, more appropriately, “disease alleles” which improve our understanding of how specific sequences of genes interact with each other and with environmental factors to affect the onset and influence the treatment of diseases (e.g., see Filipova and Hoy (2014) for a discussion on how such information provides private value with respect to decision-making about surveillance and prevention for disease). Despite these apparent benefits from genetic information, the take-up rates for existing genetic tests seem surprisingly low.Footnote 1
 There are some potential negative implications of genetic tests. If insurers are allowed access to such information, as for example are life insurance providers in the United States and Canada, then bad test results can shrink one’s market opportunities or lead to uninsurability.Footnote 2 Concern over this possibility, often referred to as genetic discrimination, has led many countries to restrict insurers’ access to such information.Footnote 3 Take-up rates for genetic tests are surprisingly low even in countries which restrict the use of genetic information by insurers and employers. All of the above discussion has been cast, implicitly, within the context of expected utility theory (EUT). If test results do not shrink one’s market opportunities (or more generally, one’s choice set), then the expected utility generated by a (costless) genetic test must be at least as great as not taking the test. Moreover, if one’s optimal decision is changed in light of at least one of the possible test results, then the test has positive expected value. The empirical evidence quoted above is, therefore, surprising as it relates to information that is provided costlessly and anonymously. As noted by Gollier (2001), in non-expected utility models agents might dislike information. In this paper, we consider the possibility that the behavioral model of ambiguity aversion, as formulated by Klibanoff et al. (2005)—KMM hereafter—can offer an explanation for the observed low take-up rates of genetic tests. This approach is suitable due to the fact that taking a genetic test can be interpreted as a situation where an individual faces second-order probabilities. According to the beliefs of the individual, which may reflect the prevalence of the disease in the population or be based on family medical history, a genetic test with a given test technology leads to a positive test result for a genetic mutation with a certain probability and to a negative test result with the complementary probability. Now each test result itself leads to a specific change in the beliefs of the individual, as individuals receiving “bad” news necessarily revise their priors towards being high risk, whereas people receiving “good” news from a test revise their priors in the other direction. In this sense, a genetic test corresponds ex-ante to a lottery between a deterioration and an improvement in beliefs. By not taking the test, however, the individual avoids this uncertainty and is free to stick to her individual priors over the disease risk. Let us give an example. A mutation of the genes BRCA1 and BRCA2 is known to be associated with a higher risk for breast or ovarian cancer (see Thompson et al. 2002). If women do not take a test for whether they have such a mutation, they recognize that they might or might not have it. Hence, the risk for breast or ovarian cancer may be viewed as a compound lottery in the first place and the woman is likely to think of her risk exposure in terms of priors over the risk type she may be, especially given the vast amount of information that has been disseminated regarding the role of the BRCA1 and BRCA2 mutations. Someone who chooses not to take a genetic test presumably bases her priors over these probabilities on the relevant subpopulation based on family medical history. Individuals can receive counsel on such likelihoods from medical professionals.Footnote 4 If, however, she takes a test, she could either obtain a positive or negative result. The perceived probability of testing positive depends on two factors, individual priors and test technology (or accuracy). If a woman now finds out that she carries a mutation of the respective genes, she will, of course adjust her priors to reflect the new information. In the extreme case where test technology is such that there are no false positives, she will even believe that she is a high-risk type with certainty. Naturally, in the case of a negative test result she will think of her breast and ovarian cancer risk more in terms of her being a low risk. Again, she believes being a low risk with certainty if the test has a rate of false negatives of zero. Before taking the test, however, there is still the uncertainty of test results to be resolved. Lastly, the value of genetic information also depends on the opportunities at hand given a specific test result. Since the 1970s strong medical progress has been made regarding therapies available for breast cancer especially when detected early (see Goldhirsch et al. 2007). Hence, individuals would probably adjust their surveillance behavior once they test positive. It becomes apparent that the interaction of decision-making value created by new information and the attitude towards changes in priors together determine the attractiveness of genetic information. Following the definition given by Camerer and Weber (1992), “ambiguity is uncertainty about probability created by missing information that is relevant and could be known.” If individuals consider taking a genetic test, they realize that information regarding disease alleles is missing, but can be obtained, possibly imperfectly, from the test. If they fear ambiguity, the uncertainty of test results might be detrimental in terms of welfare. While a person choosing not to submit to a genetic test also recognizes the potential to be a high- or low-risk type, by choosing not to find out, there is no ex-ante prospect that she will experience the resolution of this uncertainty. In other words, a person submitting to a genetic test “lives through” the ambiguity while a person who remains ignorant does not. Ellsberg (1961) was the first to demonstrate that people might fear uncertainty about probabilities, a phenomenon which has been coined ambiguity aversion. The prevalence of ambiguity aversion has been documented in laboratory experiments (see Einhorn and Hogarth 1986; Chow and Sarin 2001), in market set-ups with educated individuals (see Sarin and Weber 1993), and in surveys of business owners and managers (see Viscusi and Chesson 1999; Chesson and Viscusi 2003). This paper demonstrates that ambiguity aversion over genetic test results may provide a simple and straightforward explanation of empirically observed low take-up rates for genetic tests. Prior models of information transmission in doctor-patient relations choose different avenues through which to incorporate negative attitudes towards “bad news”. In the context of HIV testing, Caplin and Eliaz (2003) introduce an anxiety cost function that captures both the anxiety an agent experiences when diagnosed with HIV for sure and how this relates to the accuracy of tests. They employ these preferences to design a mechanism that encourages testing and slows down the transmission of disease. Kőszegi (2003) models patients’ fears as arising from expectations about future health conditions by formulating a utility function of beliefs about physical outcomes. This can explain why agents avoid visiting doctors or obtaining readily available information. Also Caplin and Leahy (2004) incorporate patients’ anxiety into a model of information revelation by policy makers. In their models, individuals are heterogeneous regarding the source of anxiety: For some individuals anxiety results from uncertainty about future conditions, whereas for others the extent of certainty of specific future health states constitutes anxiety. The problem of information revelation between two parties where the information has decision-making value, but potentially adverse emotional consequences for one party and the other reacts strategically to those fears, is studied in a general set-up by Kőszegi (2006). Oster et al. (2013) study the decision to undergo a genetic test for Huntington’s disease and find low take-up rates of 10 %. They demonstrate that the behavior of tested and untested individuals is consistent with a model of optimal expectations. Schweizer and Szech (2012) study attitudes towards genetic testing in a framework of anticipatory utility and derive implications for test design. The models so far incorporate fear of information in rather ad hoc manners by modeling preferences in various ways that reflect this anxiety. We demonstrate that the information structure implied by a genetic test in combination with ambiguity-averse preferences alone are sufficient to generate low take-up rates and to diminish the decision-making value of genetic information. We suggest that health information derived from genetic tests is different from other diagnostic or predictive information obtained through traditional medical testing such as determining one’s blood sugar level. Even though both types of tests could imply, for example, a revised and higher perceived predisposition towards future onset of type 2 diabetes, a genetic test is viewed as having broader and more profound implications for future lives of individuals. The extent to which this is an appropriate distinction in how these different types of information are perceived by individuals may indeed vary across genetic and other medical tests and we would not claim that all other types of medical testing are immune to the effects of ambiguity aversion. However, much attention has been given to the special character of genetic tests—a phenomenon which has been termed genetic essentialism.Footnote 5
 Given the likely future of genetic research and technological developments, it is important to understand the surprising reticence of people to opt for potentially useful genetic tests. With the prospect of the so-called $1000 genome close to reality (see Davies 2010), whole genome sequencing may soon become the norm for developed countries. Even now, according to the web site of the Centers for Disease Control and Prevention, there are over 3000 diseases for which genetic tests have been developed and about 2000 are in use in clinical settings.Footnote 6 The information that can be gleaned from an individual’s whole genome has the potential to revolutionize the practice of medicine with population wide genome sequencing forming the basis of so-called P4 medicine (i.e., medicine that is Predictive, Preventive, Personalized and Participatory).Footnote 7 However, we must understand how individuals assess the value of this information if it is indeed to provide substantial benefits to society.",48
48.0,2.0,Journal of Risk and Uncertainty,16 May 2014,https://link.springer.com/article/10.1007/s11166-014-9190-3,Too risk averse to purchase insurance?,April 2014,Antoine Bommier,François Le Grand,,Male,Male,Unknown,Male,"The literature on annuities was initiated by Yaari’s (1965) seminal contribution. Agents who do not care for bequest but value consumption should invest all their wealth in annuities.Footnote 6 Full annuitization is no longer optimal when bequest motives are introduced. However, Davidoff et al. (2005), as well as Lockwood (2012a), prove that the optimal behavior consists in annuitizing the discounted value of all future consumptions. The low level of observed annuitization was then identified as a puzzle, for which different explanations were suggested. A first explanation is related to unfair pricing of annuities, as reported by Mitchell et al. (1999), or Finkelstein and Poterba (2002, 2004). Lockwood (2012a) demonstrates that this aspect, together with bequest motives of a reasonable magnitude, may be sufficient to explain the low level of annuitization. Another possible explanation is that inadequate insurance products such as health or long term care insurance may encourage people to save a large amount of liquid assets. As a result of adverse selection issues, annuities are not very liquid and difficult to sell back. The optimal strategy while facing uninsurable risks may then involve investing wealth in buffer assets, such as bonds or stocks rather than in annuities. Sinclair and Smetters (2004), Yogo (2009), and Pang and Warshawsky (2010), among others, emphasize this explanation. Pashchenko (2013) shows that the illiquidity of housing wealth together with public annuities, minimum annuity purchase requirement, and bequest motives can quantitatively contribute to explain the low demand for annuities. A related channel is the fact that annuities diminish individuals’ investment opportunity sets by preventing savings in high return and high risk assets. Milevsky and Young (2007) and Horneff et al. (2010) argue that the annuity puzzle stems from the lack of annuities backed by high-risk and high-return assets. Finally, behavioral economics provides a whole range of explanations. For example, Brown et al. (2008) emphasize that framing effects could be at the origin of the low demand for annuities.Footnote 7 Brown (2007) reviews other behavioral hypotheses, such as regret aversion, financial illiteracy and the illusion of control or loss aversion. Hu and Scott (2007) also point out the role of loss aversion. Interestingly enough, papers discussing these behavioral aspects also underline the role of annuity riskiness. In particular Brown et al. (2008, p. 305) explain that “annuities appear riskier than the bond”, since purchasing annuities generates a substantial loss in the event of early death. Similarly, Brown (2007) explains that agents seem to be willing to purchase insurance that pays off well in the case of bad events, while annuities pay in the case of good events (i.e., survival). Agnew et al. (2008) confirm through lab experiments the importance of annuity riskiness perception.Footnote 8 The role of framing is also highlighted by Benartzi et al. (2011, p. 156), who state that “while economists tend naturally to think about annuitization as a risk-reducing strategy like the purchase of insurance, many consumers may not share this point of view”. It seems that agents are extremely sensitive to the riskiness of annuities, and that risk aversion may therefore play a significant role. The role of risk aversion has not hitherto been formalized. The reason is that most papers use Yaari’s approach, based on an assumption of additive separability of preferences, which imposes risk aversion to be equal to the inverse of the intertemporal elasticity of substitution. As underlined in several papers (e.g., Epstein and Zin 1989), the additive framework is ill-suited for the analysis of the role of risk aversion, since it cannot disentangle aspects of preferences over certain outcomes from the ones related to risky gambles. A few papers on annuities focus on Epstein and Zin’s (1989) approach to disentangling risk aversion from the elasticity of substitution.Footnote 9 However, as shown in BCL, Epstein-Zin utility functions are not well ordered in terms of risk aversion. This generates surprising results when studying the relation between risk aversion and savings choices. For example, in a simple two-period model, simple dominance arguments developed in BCL indicate that precautionary savings rise with risk aversion.Footnote 10 The same conclusion is drawn when considering well ordered specifications based on expected utility or on rank dependent expected utility (see Drèze and Modigliani 1972; Yaari 1987; Bleichrodt and Eeckhoudt 2005 among others). On the contrary, Kimball and Weil (2009) prove that this relation is ambiguous for Epstein-Zin preferences. A simple and robust way of studying risk aversion involves remaining within the expected utility framework and increasing the concavity of the lifetime–and not instantaneous–utility function, as initially suggested by Kihlstrom and Mirman (1974). This approach has notably been followed by van der Ploeg (1993), Eden (2008), and Van den Heuvel (2008). In the case of choice with lifetime uncertainty, this approach was first used in Bommier (2006) and leads to novel predictions on a number of topics, including the relation between time discounting and risk aversion, the impact of mortality change and the value of life. In particular, as highlighted in BCL, these preferences are well ordered in terms of risk aversion and deliver meaningful results when studying intertemporal choice problems. They were shown to generate realistic lifecycle consumption profiles (Bommier 2013). In the present paper we consider such an approach in a framework accounting for bequests and inter-vivos transfers.",25
48.0,2.0,Journal of Risk and Uncertainty,02 May 2014,https://link.springer.com/article/10.1007/s11166-014-9187-y,An experimental investigation of risk sharing and adverse selection,April 2014,Franziska Tausch,Jan Potters,Arno Riedl,Female,Male,Male,Mix,,
48.0,3.0,Journal of Risk and Uncertainty,03 July 2014,https://link.springer.com/article/10.1007/s11166-014-9192-1,Willingness to accept equals willingness to pay for labor market estimates of the value of a statistical life,June 2014,Thomas J. Kniesner,W. Kip Viscusi,James P. Ziliak,Male,Unknown,Male,Male,"The starting point of our analysis is to examine the implications of incorporating reference point effects into a standard model of compensating differentials. Let the worker’s baseline job be characterized by a fatal injury risk p0 and a wage w0. Theoretical models of compensating differentials often assume that the baseline job is a zero risk or low risk job. The worker has an indirect utility function u(w), where u′ > 0 and u″ ≤ 0. Levels of wealth and other income are subsumed in the functional form of u(w). If the worker is killed on the job, the utility level is zero. The expected utility v(p0, w0) of the base case job situation is given by Let the worker compare the baseline job to a position with a fatal injury rate p1 and wage w1. For the alternative position to be desirable compared to the initial job the worker’s reservation wage for the new job must satisfy where v(p
*0
, w0) equals some constant value. If there are no reference point effects involving the wage or the fatal injury risk, whether the job change involves an increase in risk for more pay or a decrease in risk for less pay does not enter the expected utility calculation. By implicit differentiation of v(p1, w1), one can calculate the reservation wage-risk tradeoff rate for the new job, or where this expression is defined as the VSL. In the case of a new job involving a higher wage rate for greater risk, ∂w1/∂p1 is a willingness-to-accept measure. In the standard situation with no reference point effects, so that The starting point for reference point effects is Kahneman and Tversky’s (1979) prospect theory model in which financial losses loom larger than comparable gains. In the labor market context, these losses will be in terms of a decrease in wages (see below for probability reference points). For concreteness we assume that all wage and risk reference points are with respect to the baseline reference position. The comparison job has a wage w1 < w0 and an associated initial risk p1 ≤ p0. Here the worker is incurring a lower wage to buy greater safety. To model the reference dependence effect with respect to wages, we adopt a gain-loss model similar to the type of structure used in other reference dependent contexts in the literature.Footnote 3 In particular, for a drop in wages from w0 to w1 there is an additional utility decrease of λ[u(w0) – u(w1)] associated with the difference. The expected utility of the comparison job is given by As before, v(p1, w1, λ) must equal v(p
*0
, w0) to make the jobs equally attractive in the compensating differentials model. The wage-risk tradeoff rate for the comparison job is The reference wage effect serves to lower the value of WTP as incurring a wage cut for greater safety is less attractive. Because the wage reference effect influences the value of WTP but not WTA, the ratio The ratio of WTA to WTP′ exceeds 1 because WTP has been reduced by the reference wage effect, while WTA is unaffected. Suppose that the reference point of consequence is not with respect to wages but rather with respect to the unattractiveness of a job with greater objective risk value p1 > p0. To model the risk reference point case, we assume a reference utility loss of μ(p1 – p0) associated with increases in the worker’s objective risk level. The expected utility of the new job is consequently Assessment of the wage-risk tradeoff yields the result that In calculating the ratio of willingness to accept to willingness to pay, it is WTP that is the pertinent value rather than WTP′ because the wage rate is not declining here. As a result, for comparison jobs involving more pay for greater risk, Any different job that is as equally attractive as the worker’s baseline position will trigger at most only one reference point effect even if both potentially may be influential. For p1 < p0 and w1 < w0, there will be a wage reference effect making WTP′ < WTP. For p1 > p0 and w1 > w0, there is a probability reference effect leading to WTA′ > WTA. Both reference point effects will not result from the same job change. Irrespective of whether it is the wage reference point or fatal injury risk reference point or both reference points that come into play, willingness to accept will exceed willingness to pay. The implication of such reference points for worker utility functions is that there will be kinks in an otherwise smooth constant expected utility locus. Figure 1 illustrates a constant expected utility locus uu. For a worker whose wage-risk combination is at point A, the slope of uu at A would represent both the WTA and WTP for standard models, where WTA would equal WTP for small changes in risk. Consider the situation of an increase in risk from point A for a worker with a risk probability reference point effect. The constant expected utility locus with such reference point effects is given by AB because the worker requires a higher wage to accept an increase in risk than that along uu. The AB curve is steeper than the constant expected utility locus uu to the right of point A, implying a higher WTA. Similarly, if the worker has a reference point effect with respect to decreases in wages, that worker will be willing to accept less of a wage reduction to achieve a reduction in risk than along uu, as the worker’s constant expected utility locus for risk reductions is AE. Thus, with both reference dependent effects, the local slope of uu at point A corresponds to neither the WTA nor the WTP, and WTA ≠ WTP, as in the standard model. Job preferences with reference dependent effects Our empirical focus is on job changers. Although people do not switch jobs in the standard hedonic model, information acquisition over time may generate the incentive to switch jobs. The first type of learning is on-the-job learning about the worker’s current position, as in Viscusi (1979). If the worker’s experiences on the job are favorable, the position becomes increasingly attractive relative to other positions, and there is no incentive for the worker to quit. If the worker has an unfavorable experience, such as observing risky job conditions, the posterior probability of a fatal accident is p
*0
 > p0.Footnote 4 Thus, the worker with an adverse experience may believe that the job is more dangerous than given by the average objective risk value. Alternatively, the information acquisition may be with respect to other market opportunities. In particular, as shown by Altonji and Paxson (1988), with the presence of search costs workers are not necessarily selecting from jobs along a market opportunities locus as in the hedonic model but instead are confronting a dispersion of wage offers. Whether the information acquisition is with respect to the worker’s current job or market alternatives, there may be a potential rationale for changing jobs. Figure 1 illustrates a situation in which a worker at point A is considering a job that poses greater risk but pays a higher wage. The job at point C would be an attractive alternative in the absence of reference dependent effects, but with risk reference effects it would not be desirable. However, jobs on or to the northwest of AB, such as the job at point D, would be attractive. Because job changes involving a risk increase may lie on or above AB, the effect of worker sorting is that the tradeoff rates for workers who incur an increase in risk provide an upper bound on WTA values. Villanueva (2007) makes a similar observation for job changes when there are no reference point effects where the upper bound arises from the censoring of tradeoff rates, not reference dependence effects. For job changes that involve a decrease in risk, the tradeoff rates for job changers provide a lower bound on WTP values for analogous reasons. Our empirical exploration of job changers below will distinguish the tradeoff rates for both increases and decreases in risk and test for possible differences in tradeoff values.",35
48.0,3.0,Journal of Risk and Uncertainty,12 July 2014,https://link.springer.com/article/10.1007/s11166-014-9194-z,Estimating subjective probabilities,June 2014,Steffen Andersen,John Fountain,E. Elisabet Rutström,Male,Male,Unknown,Male,"For simplicity we assume throughout that the events in question only have two outcomes.Footnote 9 A scoring rule asks the subject to make some report θ, and then defines how an elicitor pays a subject depending on their report and the outcome of the event. This framework for eliciting subjective probabilities can be formally viewed from the perspective of a trading game between two agents: you give me a report, and I agree to pay you $X if one outcome occurs and $Y if the other outcome occurs. The scoring rule defines the terms of the exchange quantitatively, explaining how the elicitor converts the report from the subject into a lottery. We use the terminology “report” because we want to view this formally as a mechanism, and do not want to presume that the report is in fact the subjective probability π of the subject. In general, it is not. The QSR was apparently first used by McKelvey and Page (1990), and later by Offerman, Sonnemans and Schram (1996), McDaniel and Rutström (2001), Nyarko and Schotter (2002), Schotter and Sopher (2003), Costa-Gomes and Weizsäcker (2008) and Rutström and Wilcox (2009).Footnote 10 In each case the subject is implicitly or explicitly assumed to be risk-neutral.Footnote 11 Scoring rules that are linear in the absolute deviation of the estimate have been used by Dufwenberg and Gneezy (2000) and Haruvy, Lahav and Noussair (2007). Croson (2000) and Hurley and Shogren (2005) used scoring rules that are linear in the absolute deviation as well as providing a bonus for an exactly correct prediction. Scoring rules that provide a positive reward for an “exact” prediction and zero otherwise have been used by Charness and Dufwenberg (2006). In many cases the inferential objective has been to test hypotheses drawn from “psychological game theory,” which rest entirely on making operational the beliefs of players in strategic games. For this purpose, and also in other applications, the elicitation of beliefs is combined with some other experimental task. Other applications include testing the hypothesis that the belief elicitation task will encourage players in a game to think more strategically (Croson (2000), Costa-Gomes and Weizsäcker (2008), Rutström and Wilcox (2009)). Of course, combining tasks in this way violates the “no stakes condition” required for the QSR to elicit beliefs reliably unless one assumes that the subject is risk neutral (Kadane and Winkler (1988), Karni and Safra (1995) and Karni (1999)). Only one of these studies employs a “spectator” treatment in which players are asked to provide beliefs but do not take part in the constituent game determining the event outcome: study #2 of Offerman, Sonnemans and Schram (1996). The popular QSR is defined in terms of two positive parameters, α and β that determine a fixed reward the subject gets and a penalty for error. Assume that the possible outcomes are A or B, where B is the complement of A, that θ is the reported probability for A, and that Θ is the true binary-valued outcome for A. Hence Θ = 1 if A occurs, and Θ = 0 if it does not occur (and thus B occurs instead). The subject is paid S(θ|A occurs) = α − β(Θ − θ)2 = α − β(1 − θ)2 if event A occurs and S(θ|B occurs) = α − β(Θ − θ)2 = α − β(0 − θ)2 if B occurs. In effect, the score or payment penalizes the subject by the squared deviation of the report from the true binary-valued outcome, Θ, which is 1 and 0 respectively for A and B occurring. An omniscient seer would obviously set θ = Θ. The fixed reward is a convenience to ensure that subjects are willing to play this trading game, and the penalty function simply accentuates the penalty from not being an omniscient seer. In our experiments α = β = $100, so subjects could earn up to $100 or as little as $0. If they reported 1 they earned $100 if event A occurred or $0 if event B occurred; if they reported ¾ they earned $93.75 or $43.75; and if they reported ½ they earned $75 no matter what event occurred. It is intuitively obvious, and also well known in the literature (e.g., Winkler and Murphy (1970) and Kadane and Winkler (1988)), that risk attitudes will affect the incentive to report one’s subjective probability “truthfully” in the QSR.Footnote 12 A sufficiently risk averse agent is clearly going to be drawn to a report of ½, and varying degrees of risk aversion will cause varying distortions in reports from subjective probabilities. If we knew the form of the (well-behaved) utility function of the subjects, and their degree of risk aversion, we could infer back from any report what subjective probability they must have had. Indeed, this is exactly what we do below, recognizing that we only ever have estimates of their true degree of risk aversion. The LSR is also defined in terms of two positive parameters, γ and λ, that serve as fixed rewards and penalties, respectively. The subject is paid a fixed reward less some multiple of the absolute difference between their report and what actually happened, which is also what an omniscient seer would have reported. Thus the payment is S(θ|A occurs) = γ − λ(1 − θ) if event A occurs and S(θ|B occurs) = γ − λ(θ − 0) if B occurs. We again set γ = λ = $100, generating payoffs of $100 or $0 for a report of 1; $75 and $25 for a report of ¾; and $50 no matter what the outcome for a report of ½. The LSR is not a favorite of decision theorists, since a risk neutral subject would jump to corner-solution reports of 1 or 0 whenever their true beliefs were either side of ½. But when the subject is (even modestly) risk averse, an interior solution is obtained, and we face the same issues of inference as with the QSR. The LSR is a favorite of experimental economists because of the simplicity of explaining the rule: the score for a report and an event is linear in the reported probability report, so there is no need for elaborate tables showing cryptic payoff scores for discrete reports.Footnote 13
 In order to avoid portfolio effects from the combined choices rewards in these scoring rule task are sometimes very, very small. For example, Nyarko and Schotter (2002) and Rutström and Wilcox (2009) gave each subject an endowment of 10 cents, from which their penalties are to be deducted. In the latter study this does not present a problem since the focus is not on analyzing the elicited beliefs but on analyzing the game play as a function of exposing subjects to a belief elicitation process. Nevertheless, one has to worry about the incentive properties of the elicitation method once the interest is on analyzing the beliefs themselves. The need to calibrate or control for risk aversion is often not made explicit, or is claimed to be of marginal concern. Schotter and Sopher (2003; p. 504) recognize the role of risk aversion, but appear to argue that it is not a factor behaviorally: It can easily be demonstrated that this reward function provides an incentive for subjects to reveal their true beliefs about the actions of their opponents. Telling the truth is optimal; however, this is true only if the subjects are risk neutral. Risk aversion can lead subjects to make a “secure” prediction and place a .50 probability of each strategy. We see no evidence of this type of behavior. Of course, evidence of subjects selecting the probability report of ½ only shows that the subject has extreme risk aversion. The absence of that extreme evidence says nothing about the role that risk aversion might play in general. Only one QSR study attempts to explicitly calibrate the beliefs for non-linear utility functions and/or probability weighting: Offerman, Sonnemans, van de Kuilen and Wakker (2009; §6). We introduce an alternative experimental method where the decision model can be identified econometrically from one set of tasks for the same subjects, or even on a different pool of subjects drawn from the same population, and then statistically integrated with the belief elicitation task, while transparently allowing error terms to propagate. The estimation of the decision model over risk and subjective probabilities is joint and simultaneous, even if one can think of the lottery choices as recursively identifying the decision model.",67
48.0,3.0,Journal of Risk and Uncertainty,18 July 2014,https://link.springer.com/article/10.1007/s11166-014-9193-0,"Joint measurement of risk aversion, prudence, and temperance",June 2014,Sebastian Ebert,Daniel Wiesen,,Male,Male,Unknown,Male,"Our experimental methodology is based on the model-independent concept of risk apportionment (Eeckhoudt and Schlesinger 2006). Therefore, risk aversion, prudence, and temperance are defined as preferences over lottery pairs. We first define (second-order) risk aversion. Let x be the individual’s wealth, and k, r > 0 are fixed monetary amounts. With B
2 = [x − r, x − k], for example, we denote the 50/50 gamble B
2 that has equally likely payoffs x − r and x − k. An individual is called risk-averse if she prefers B
2 to A
2 = [x − r − k, x] for arbitrary parameter values x, r, and k. The lotteries are displayed in Fig. 1. Thus, a risk-averse individual prefers to disaggregate unavoidable losses −r and −k across states of nature. This preference is equivalent under EU to u
′′ < 0 as shown in Section A of the web appendix.Footnote 3 Under EU, the preference is also equivalent to a preference for decreases in risk in the sense of Rothschild and Stiglitz (1970). Risk aversion lottery pair (A
2, B
2). This figure illustrates lotteries of the type used to measure risk aversion in the experiment in an abstract way. x is the subject’s wealth, and −r and −k denote sure reductions therein. To imagine a prudence lottery pair denoted by (A
3, B
3), simply replace the −r with a zero-mean risk \(\tilde {\epsilon }_1.\) To imagine a temperance lottery pair denoted by (A
4, B
4), additionally replace −k with a second independent zero-mean risk \(\tilde {\epsilon }_2\)
 In order to define prudence (third-order or downside risk aversion), the sure reduction in wealth −r in the definition for risk aversion (see again Fig. 1) is replaced with a zero-mean risk \(\tilde {\epsilon }_1.\) That is, an individual is called prudent if she prefers \(B_3=[x-k,x+\tilde {\epsilon }_1]\) over \(A_3=[x, x-k+\tilde {\epsilon }_1]\) for all wealth levels x, sure wealth reductions −k, and zero-mean risks \(\tilde {\epsilon }_1.\) That is, a prudent individual prefers to disaggregate an unavoidable risk and a loss across different states of nature. Equivalently, an unavoidable risk is preferred when wealth is higher. Eeckhoudt and Schlesinger (2006) show that this preference is equivalent to u
′′′ > 0 in EU or to a preference for decreases in downside risk as defined by Menezes et al. (1980). Finally, temperance (fourth-order or outer risk aversion) is defined as a preference of \(B_4=[x+\tilde {\epsilon }_1,x+\tilde {\epsilon }_2]\) over \(A_4=[x,x+\tilde {\epsilon }_1+\tilde {\epsilon }_2]\) where \(\tilde {\epsilon }_1\) and \(\tilde {\epsilon }_2\) are two independent zero-mean risks. Under EU, this preference is equivalent to u
(4) < 0, and it is also equivalent to a preference for decreases in outer risk as defined by Menezes and Wang (2005). Our experiment aims to elicit compensations for nth-order risk aversion for n = 2, 3, 4, i.e., a (second-order) risk compensation m
RA, a downside risk (imprudence) compensation m
PR, and an outer risk (intemperance) compensation m
TE. For example, in the case of risk aversion, for every individual we elicit m
RA, i.e., the smallest amount that makes her choose A
2+m
RA = [x+m
RA, x − r − k+m
RA] over B
2 = [x − r, x − k].Footnote 4 For a (second-order) risk-loving individual m
RA will be negative. This methodology yields model-independent intensity measures of risk attitudes of different orders, rather than only tests for preference direction.",86
48.0,3.0,Journal of Risk and Uncertainty,19 July 2014,https://link.springer.com/article/10.1007/s11166-014-9191-2,Buying and selling price for risky lotteries and expected utility theory with gambling wealth,June 2014,Michal Lewandowski,,,Male,Unknown,Unknown,Male,"I start with basic assumptions and definitions. 
Preferences obey expected utility axioms. Bernoulli utility function
\(U: \mathbb {R} \rightarrow \mathbb {R}\)
is twice continuously differentiable, strictly increasing and strictly concave.
 A lottery x is a real- and finite-valued random variable with finite support. The space of all lotteries will be denoted \(\mathcal {X}\). I define the maximal loss of lottery x as: min(x) =  min supp(x). The typical lottery will be denoted as x ≡ (x
1, p
1;...;x

n
, p

n
), where \(x_{i}\in \mathbb {R}, \ i\in \{1, 2, ..., n\}\) are outcomes and p

i
 ∈ [0, 1], i ∈ {1, 2, ..., n} the corresponding probabilities. Outcomes should be interpreted here as monetary values. Although most results that follow are true for more general lotteries, the finite support assumption is sufficient for the purposes of this paper. Now I define buying and selling price for a lottery given wealth level along the lines of Raiffa (1968). To avoid repetitions, I will henceforth skip statements of the form: “Given utility function U satisfying Assumption 1, any lottery x and wealth W...”. I define selling price and buying price for a lottery x at wealth W as functions of wealth W denoted, respectively, S(W, x) and B(W, x). Provided that they exist, values of these functions will be determined by the following equations:
 If utility function is defined over the whole real line as is the case for constant absolute risk aversion, buying and selling price as functions of wealth exists for any wealth level by Assumption 1. If the domain of the utility function is restricted to a part of the real line as is the case of the constant relative risk aversion utility function analyzed here, I will specify later on in the paper on which domain buying and selling price are defined as functions of wealth. In economic terms, given an individual with initial wealth W whose preferences are represented by utility function U(⋅), S(W, x) is the minimal amount of money which he demands for giving up lottery x. Similarly, B(W, x) is the maximal amount of money which he is willing to pay in order to play lottery x. Additionally I define a concept of buying/selling price reversal. Given two lotteries x and y and some wealth level W, define buying/selling price reversal as:
 This kind of preference pattern may be interpreted as follows. For a given initial wealth, an individual’s certainty equivalent for lottery y is higher than for lottery x, and yet he is willing to pay more to play lottery x than to play lottery y. In other words, an individual exhibiting buying/selling price reversal may prefer to buy x than y if he does not have the right to play any lottery initially. When, on the other hand, he does play the lottery initially, he would prefer to sell x than y. Before introducing the main point of this paper I need a couple of theoretical results which describe properties of buying and selling price for a lottery for different risk attitudes. The most basic property of buying and selling price which is true for any concave strictly increasing utility function is the following: 
For any non-degenerate lottery
x
and any wealth W such that buying and selling price exist, S(W, x) and B(W, x) lie in the interval (min(x), E(x)). For a degenerate lottery
x, S(W, x) = B(W, x) = x.
 In the Appendix. □ Below I state propositions which characterize CARA, DARA and CRRA utility functions in terms of buying and selling price. Proofs of these propositions may be found for example in Lewandowski (2011) along with an extensive discussion on multiplicative and nominal gambles and risk aversion notions for the two kinds of gambles. I start with the results on CARA and DARA utility functions: 
The following two statements are equivalent:
 
 i. Bernoulli utility function exhibits CARA
 
ii. Buying and selling price are independent from wealth and equal i.e.
 
where α is absolute risk aversion coefficient and C

α

takes real values and depends only on α. 
The following two statements are equivalent:
 
 i. Bernoulli utility function exhibits DARA
 
ii. buying and selling price are increasing in W
 
for a non-degenerate lottery
x. The above propositions show that in the expected utility model a gap between buying and selling price can only arise due to wealth effects. Selling price is higher than buying price for a lottery for which I would be willing to pay a positive amount only if absolute risk aversion decreases in wealth. In the subsequent analysis I will be especially interested in the CRRA class, which is a subset of DARA: 
The following two statements are equivalent:
 
 i. Bernoulli utility function exhibits CRRA
 
ii. buying and selling price for any lottery are homogeneous of degree one i.e.
",9
49.0,1.0,Journal of Risk and Uncertainty,12 September 2014,https://link.springer.com/article/10.1007/s11166-014-9198-8,The explanatory and predictive power of non two-stage-probability theories of decision making under ambiguity,August 2014,John D. Hey,Noemi Pace,,Male,Female,Unknown,Mix,,
49.0,1.0,Journal of Risk and Uncertainty,20 August 2014,https://link.springer.com/article/10.1007/s11166-014-9197-9,Sampling experience reverses preferences for ambiguity,August 2014,Eyal Ert,Stefan T. Trautmann,,Male,Male,Unknown,Male,"The effects of sampling experience on beliefs and on attitudes under uncertainty can be modeled by extending the expected utility approach to account for deviations from Bayesian updating and from ambiguity neutrality (Baillon et al. 2013; Viscusi 1997; Viscusi and Magat 1992). We employ the belief-based weighting account of Fox and Tversky (1998; Wakker 2004) to model the two components. Let u be a utility function over outcomes, w a probability weighting function, and B(E) the subjective probability that event E occurs. The value of an ambiguous prospect x

E

y that pays outcome x if event E occurs, and outcome y < x otherwise is given by w(B(E))u(x) + (1 − w(B(E)))u(y). Consequently, sampling experience can potentially affect either the belief B (E), possibly in a non-Bayesian way as in Viscusi (1997) and Viscusi and Magat (1992), or the ambiguity attitudes in form of changes in w, as in Baillon et al. (2013), or both.Footnote 2
 Based on the previous literature, we identify two mechanisms that can affect preferences in ambiguous environments with sampling opportunities. First, familiarity and competence effects may affect attitudes when ambiguous alternatives are encountered repeatedly. Repeated exposure to ambiguous alternatives may lead to feelings of higher competence/familiarity with this alternative, and therefore to more ambiguity seeking (Curley et al. 1986; Heath and Tversky 1991). Because of the motivational nature of the competence effect, it should affect the weighting function w, and therefore change ambiguity attitudes in the same direction for all levels of the underlying probabilities. Second, research on the effect of experience on risk taking suggests that people evaluate samples of uncertain options as if they underweight rare outcomes (Erev et al. 2008; Hertwig and Erev 2009; Hertwig et al. 2004; Jessup et al. 2008; Ungemach et al. 2009). To illustrate, consider a prospect that yields a payoff of $1 with probability 0.1, and $0 otherwise. The rare outcome of the desired $1 gain would show only in the minority of the samples. Thus, sampling might decrease the attractiveness of this prospect as it would emphasize that the desired outcome is unlikely. In contrast, when sampling a prospect that yields $1 with probability 0.9, and $0 otherwise, the rare outcome is the undesired outcome of receiving nothing. The $1 gain will be present in the majority of the samples. Thus, sampling this prospect can increase its attractiveness. Applying this pattern to our setting, we predict that the attractiveness of the sampled ambiguous prospect, relative to a known-probability risky prospect with equal winning probability, increases when the probability of the good outcome is high, and decreases when the probability of the good outcome is low. This effect should work directly through the belief component B
(E), and would vary with the underlying probabilities (low vs. high). The experienced-based account of ambiguity attitude thus predicts the opposite pattern of preference than what is commonly observed in no-sample decisions, where a high winning probability for the risky prospect leads to strong ambiguity avoidance, and a low winning probability leads to more choosing of ambiguity. With sampling, a risky prospect will be compared to an uncertain prospect of equal underlying probability, and the perception and weighting of this ambiguous probability is potentially affected by the sampling experience. We test our predictions in two studies. In the first study we consider sampling effects on choices, and in the second study we consider sampling effects on beliefs.",29
49.0,1.0,Journal of Risk and Uncertainty,16 September 2014,https://link.springer.com/article/10.1007/s11166-014-9196-x,Probability perceptions and preventive health care,August 2014,Katherine Grace Carman,Peter Kooreman,,Female,Male,Unknown,Mix,,
49.0,1.0,Journal of Risk and Uncertainty,22 August 2014,https://link.springer.com/article/10.1007/s11166-014-9195-y,Do we follow others when we should outside the lab? Evidence from the AP top 25,August 2014,Daniel F. Stone,Basit Zafar,,Male,Male,Unknown,Male,"The AP poll is conducted once per week during the season and teams usually play one game per week and sometimes have the week off. Teams play 11-14 games over the course of the season and there are approximately 120 teams. The first poll is conducted before any game is played, and the final poll after the season ends. The poll is voted on by 60-65 leading journalists (the number varies from year to year), most of whom work for newspapers throughout the U.S. A small percentage work for television stations and national news organizations. Each poll member votes by submitting a ranking of the top 25 teams, and the aggregate ranking is determined by assigning teams 25 points for each first place vote, 24 for second, etc., and summing points by team (a Borda ranking). The vast majority of games are played on Saturdays, throughout the day and evening, and the rankings are submitted to the AP the following day by 11:00 AM EST. The AP compiles and releases the aggregate rankings that evening (Sunday). Consequently, for each poll after the first poll, voters can update their ranks based on both the game results and peer ranks from the previous week (we use the terms “peer ranks” and “aggregate ranks” interchangeably).Footnote 7 We define a week to start just before voters submit their ranks, and thus each week’s ranks are based only on information obtained in previous weeks; the timing of the key events that occur in each week is presented in Fig. 1.
 Timing of events in a week The individual voter rankings are not confidential, but not widely available.Footnote 8 We have data on the individual and aggregate ranks, and game scores, from each week of the 2006-08 seasons, and aggregate rank and score data for each season dating back to 1989.Footnote 9 The 2006 individual ranks were obtained directly from the AP and the 2007-08 ranks were obtained from the AP’s website. The unit of observation for the analysis is a (ranked) team-voter-week-season. The sample is restricted to the first half of each season, for reasons discussed below, and to ranked teams that play games in the current week. While the voters do not have explicit incentives relating to the rankings, discussions with voters indicate they put substantial effort into determining their rankings. This may be due to a sense of journalistic duty, or because voters are judged by the quality of their rankings, so there are reputational and other implicit incentives. There are well known examples of voters gaining extreme notoriety due to the accuracy, or lack thereof, of their rankings.Footnote 10 Incentives are therefore likely at least as strong as those used in standard experiments. Moreover, Weizsäcker (2010) finds that while stronger incentives in experiments improve success rates, error rates remain relatively large regardless, suggesting that if voters were to make substantial mistakes it is very unlikely these would completely be an artifact of low incentives.",6
49.0,2.0,Journal of Risk and Uncertainty,14 November 2014,https://link.springer.com/article/10.1007/s11166-014-9200-5,Why Chinese discount future financial and environmental gains but not losses more than Americans,October 2014,Min Gong,David H. Krantz,Elke U. Weber,,Male,Female,Mix,,
49.0,2.0,Journal of Risk and Uncertainty,13 November 2014,https://link.springer.com/article/10.1007/s11166-014-9199-7,The neural correlates of contractual risk and penalty framing,October 2014,W. Gavin Ekins,Andrew M. Brooks,Gregory S. Berns,Unknown,Male,Male,Male,"We recruited 30 healthy, right-handed participants from Emory University (M: 15, F: 15, age: 18–41). Two of these participants had to be removed from the data because of excessive head movement during image acquisition. All procedures were approved by the Emory University IRB, and each participant provided written consent and was asked to complete three surveys: Risk Preference Worksheet, EPQ-R, and BIS/BAS.Footnote 1 All instructions were presented on a computer terminal with periodic quizzes to ensure participant’s comprehension of the experiment. In the experiment, participants were contracted to answer trivia questions correctly. Trivia questions from the Who Wants to Be a Millionaire board game were displayed as a multiple choice quiz with 4 possible answers. The terms of the contract required the participants to answer 10 out of 10 trivia questions correctly. Before participants were asked to perform the contract, i.e. answer 10 questions correctly, they were given the opportunity to purchase additional questions. Each additional question allowed the participant to answer a question incorrectly without failing to complete the terms of the contract. We manipulated the framing of the contract as well as the cost of failing to complete the contract. The 2x3 factorial design captured the effects of the contract framing and the cost of failure as well as any interaction effects (Table 1). In the first factor, we framed the contract as either a penalty or a bonus. In the second factor, we varied the cost of failure: high, medium, and low. The benefit of completing the contract was consistently $30 for all contracts, but the cost of failure dictated the amount a participant received if he failed to complete the contract: $45, $30 and $15. If the cost of failure was $15 and the participant failed to complete the contract, then he received $15 from the experimenter. Likewise, if the cost of failure was $45 and the participant failed to complete the contract, then he had to pay the experimenter $15 from his endowment. In the bonus treatments, participants received either -$15, $0, or $15 upfrontFootnote 2 depending on the cost of failure treatment and an additional $45, $30, or $15, respectively, if they were able to complete the contract. Thus, there was no cost to the participant for failing in the bonus treatments. In this case, if the participant failed to complete the contract, they incurred the opportunity cost of losing the bonus. In the penalty treatments, participants received $30 upfront and no additional payment if they were able to complete the contract. If the participant failed, he had to return $15, $30, or $45 to the experimenter based on the cost of failure treatment. Regardless of the frame, participants received $30 if the contract was completed and -$15, $0, or $15 if they failed, depending on the cost of failure treatment. As such, after the endowment is adjusted for the upfront payment, all bonus treatments displayed a gain for completing the contract ($15, $30, $45) while all penalty treatments displayed a loss for failing to complete the contract (-$15, -$30, -$45). The experiment was separated into three sections. In the first section of the experiment, participants answered trivia questions to earn an endowment and to gauge their ability to answer the trivia questions correctly. In the second section, participants were shown different incentive contracts, based on the treatments, after which they were given the chance to purchase additional trivia questions. In the third section of the experiment, participants were randomly assigned a contract to perform. Participants had to answer the appropriate number of questions correctly. In the first section, participants earned $30 by correctly answering trivia questions from the Who Wants to Be a Millionaire board game. Each question answered correctly earned the participant $2. Difficulty of trivia questions were titrated such that the participants were correct approximately 50 % of the time (Table 2). There were 10 levels of question difficulty. A moving-average adjustment algorithm was employed to assign the appropriate level of difficulty to each participant. The number of correct answers over the total number of questions answered was displayed at the top of the screen to provide the participants with a rough estimate of their probability of answering a trivia question correctly. The initial phase ended when the participant had earned $30, i.e. the participant correctly answered 15 trivia questions correctly. In the second section of the experiment, the participants were shown 60 contracts, 10 repetitions of the 6 treatments. After each contract was presented, participants were given the opportunity to purchase additional trivia questions at a cost of $0.50 each. During this section of the experiment, fMRI images were acquired while participants viewed the contracts and decided the amount of additional questions to purchase. Each contract consisted of four screens: fixation, passive contract, decision, and review; in order of display (Figs. 1 and 2). The participants were required to view the passive contract phase for a minimum of 4 sec after which they used a button to move forward. During the decision phase, participants were able to purchase additional trivia questions. The number of additional questions purchased along with the total cost was displayed in the lower half of the screen while the contract terms were visible in the upper half. This phase was not timed and participants chose to move forward by pressing a button. During the review screen, participants reviewed their decision for 2 sec. During the fixation screen, participants viewed a blank screen for 2–6 sec, uniformly distributed. After the fixation screen, the next trial began with a new contract. Experiment software screenshots and trial timing Additional questions purchased by treatment After the completion of the second section of the experiment, the participants rolled dice to determine which of the contracts they performed. Participants performed the contract with the number of additional questions chosen in the second section of the experiment. When the participant no longer had a sufficient number of questions to complete the contract or the participant had answered the 10 questions correctly, the participant was paid according to the terms of the contract minus the cost of the additional questions purchased. To calculate the theoretically optimal amount of additional trivia questions to purchase, the total number of questions in the first section of the experiment for each participant was used as a proxy for the perceived probability of answering a question correctly. We calculated the probability of completing the contract with the binomial model (1), where a was the total additional trivia questions purchased, q was the required number of questions to answer, b was the required endowment questions (first at 15 for this experiment), and t was the total number of trivia questions answered in the initial portion of the experiment. Note that b/t is an estimate for the perceived probability of incorrectly answering a trivia question. We calculated the optimal number of additional trivia questions to purchase by using a risk neutral, expected utility model (2), where CF is the outcome if the contract is incomplete. The optimal number of additional questions (a*) was calculated using iterative methods (Table 3). We used blood-oxygen level dependent functional magnetic resonance imaging (BOLD-fMRI) to measure neural activity of participants while performing our task. BOLD-fMRI measures changes in local magnetic fields due to the presence of oxygen in the blood. When neurons fire in the brain, oxygen-rich blood flows to the active region to replenish the neurons. The change in oxygenated blood affects the local magnetic field in a manner such that the intensity of the MRI signal changes in proportion to the local concentration of oxygenated-hemoglobin. Moreover, this signal change follows a well-defined pattern called the canonical hemodynamic response function (HRF), which we denote as h(t). By mapping signals from the BOLD response onto the HRF, we can determine the approximate time and level of activity within a voxel, a unit of 3-dimensional space similar to a pixel. The properties of the BOLD signal are such that multiple sources of neural activity can be linearly combined (Boynton et al. 1996). In other words, the BOLD signal can be viewed as a sum of multiple HRFs stemming from activations at a particular time and location, which we denote as a(t). The level of BOLD response related to a stimulus at time period t is approximated by a convolution of the HRF of all stimuli (3). Functional imaging was performed with a Siemens 3-Tesla Trio whole-body scanner. We obtained two types of images. The first image, known as a T1-weighted structural image, was a high-resolution image of the participants’ brain structures. These imagesFootnote 3 were acquired for each participant prior to the second portion of the experiment. The second image, known as a T2*-weighted echo-planar image, registered the signal change due to the BOLD response. These imagesFootnote 4 were acquired while the participants were viewing the contracts and purchasing additional questions. The BOLD images were subjected to standard preprocessing commonly used in neuroimaging. First, we corrected for the participant’s head motion within the scanner. Second, we temporally aligned the images, known as slice timing correction, which accounts for the different times from which each location in the brain is measured. Third, we spatially registered the images using landmarks in the brain so that we could compare images between participants. This process normalized the images to the Montreal Neurological Institute (MNI) template brain. Finally, the images were smoothed to improve signal to noise ratio of individual voxels, which allows for slight misalignments between participants.Footnote 5
 Although we cannot observe individual neurons firing, we can search for clusters of neurons that were active during a particular stimulus. In other words, we sought a collection of spatially contiguous voxels with similar BOLD responses stemming from a particular event. In order to isolate the BOLD response, we controlled for events unrelated to the event of interest. Let the BOLD signal from the HRF of event i be denoted as S

i
(t) .In addition, let c

i
(t) be additional controls for head movement and time trends. Given the linearity of the BOLD signal, we can estimate the level of activation from an event by regressing the HRF of all events with controls on the BOLD signal for each voxel (v). Our BOLD signal regression model was structured as the sum of both HRF, controls, and a constant (4). The error term of (4) followed an AR(1) process. In our experiment, we have 4 event types for which we tracked the hemodynamic response in our model (5). In each period, the participant viewed the contract passively (PC), decided the amount of additional questions to purchase (BD & PD), and reviewed their decision (R). We also split the passive contract phase and the active decision phase events into separate events depending on the framing of the contract, bonus or penalty. Thus, we were able to compare the neural activity between a bonus and penalty contract during the passive and active phases of the experiment. Initially we separated the passive phase into both penalty and bonus frames, but the lack of significant difference between the passive bonus and the passive penalty phase and the increase in error led us to combine the passive phase into one event. In addition, we developed a model, (6), to account for the cost of failure (CF) treatments. In this model, the decision phase of the experiment is split by framing and by cost of failure. This model included the framing of bonus and penalty and the three levels of the cost of failure: 15, 30, and 45. where Penny et al. (2007) provided the standard method within neuroscience for estimating the magnitude and significance of our models’ coefficients. In this method, each participant’s regression coefficients were estimated independently. The coefficients from each participant were then averaged and a t-test was used to estimate significance. In this case, participant specific error is absorbed into the constant of each participant’s regression model. We used SPMFootnote 6 to estimate the coefficients from (6). Since each regression model does not consider the BOLD signal of the surrounding voxels, significance of any one voxel could be a function of smoothing or random search. Standard methods for correcting multiple comparisons, such as Bonferroni, would increase significance to unobtainable thresholds due to the larger number of voxels in the brain. Given the properties of the hemodynamic response to neural activity, larger clusters were less likely to be a function of smoothing or random search. By setting an appropriate cluster threshold, we can avoid the false detection of elevated neural activity by eliminating sufficiently small clusters. We used Monte Carlo simulationsFootnote 7 with the smoothness parameters from the neuroimages to estimate a clusters size threshold such that the false detection rate (FDR) was less than 0.05 for a given BOLD signal height threshold.",3
49.0,2.0,Journal of Risk and Uncertainty,16 November 2014,https://link.springer.com/article/10.1007/s11166-014-9202-3,To bet or not to bet? Decision-making under risk in non-human primates,October 2014,M. Pelé,M. H. Broihanne,V. Dufour,Unknown,Unknown,Unknown,Unknown,,
49.0,2.0,Journal of Risk and Uncertainty,08 November 2014,https://link.springer.com/article/10.1007/s11166-014-9201-4,The role of forgone opportunities in decision making under risk,October 2014,Ivan Barreda-Tarrazona,Ainhoa Jaramillo-Gutierrez,Gerardo Sabater-Grande,Male,Female,Male,Mix,,
49.0,3.0,Journal of Risk and Uncertainty,23 December 2014,https://link.springer.com/article/10.1007/s11166-014-9205-0,Testing for independence while allowing for probabilistic choice,December 2014,Graham Loomes,Ganna Pogrebna,,Male,Female,Unknown,Mix,,
49.0,3.0,Journal of Risk and Uncertainty,05 December 2014,https://link.springer.com/article/10.1007/s11166-014-9203-2,A general rationale for a governmental role in the relief of large risks,December 2014,Steven Shavell,,,Male,Unknown,Unknown,Male,"The government plays a well-recognized role in providing relief against many large risks. Notably, the government traditionally furnishes disaster assistance and subsidizes markets for flood and earthquake insurance.Footnote 1
 That the government is often observed to ameliorate substantial risks naturally raises the question about the justification for that policy. I consider here a general rationale for the government to relieve significant risks that applies even when, as I will assume, private contracting to share risks is perfect (unimpeded by transaction costs, asymmetric information, externalities, or other sources of market failure). The rationale, in essence, is that the optimal private sharing of very large risks will not result in complete coverage against them. Therefore, when the risks eventuate, the marginal utility to individuals of relief from the government will be high in a relative sense and may exceed the marginal value to individuals of public goods. Consequently, social welfare may be raised if the government reduces public goods expenditures and directs these freed resources toward individuals who have suffered losses. Although the foregoing argument is straightforward, it does not seem to have been clearly articulated before. I develop it below employing a simple model involving a population of identical individuals, a single consumption good, a risk of loss of the consumption good—where this accident risk may be correlated across individuals—frictionless joint contracting among all individuals to share risk, and a government that produces a public good. Two times are considered in the model: time 1, before risk resolves itself, and time 2, after possible accident losses have occurred. At time 1, individuals pay a tax to finance the public good and they contract in a privately–optimal way to share risk. Their best risk-sharing contracts do not lead to full coverage against loss. Indeed, in the extreme case of a single, economy-wide risk (a flood that affects everyone), contracting would not result in any coverage; the privately–optimal risk-sharing contract would be the null contract. The main results shown are these. First, there exists a welfare-enhancing policy under which the government grants relief to accident victims whenever the number of accidents in the population is sufficiently high.Footnote 2 Welfare is enhanced for the reason I noted above: when the number of accidents is high, the wealth of accident victims after optimal risk-sharing is relatively low and their marginal utility of income is relatively high, implying that they will be made better off if resources are shifted from public goods to them by the making of relief payments. Second, a policy of government relief exists that achieves the first-best outcome in terms of both risk-sharing and the provision of public goods. Third, a policy of government subsidy of risk-sharing contracts (analogous to the subsidy of insurance contracts) may provide social benefits similar to those of government relief.Footnote 3 And fourth, a policy under which taxes are lowered appropriately as the number of accidents increases allows the first-best outcome to be achieved without government relief or the subsidy of risk-sharing contracts. The general justification for government aid in the event of large risks that is examined in the model seems consistent with actual policy in at least an approximate sense. On one hand, the important risks mentioned above that the government helps to alleviate have the character that they may affect many individuals simultaneously; this is frequently true of disasters. On the other hand, private insurance coverage against these positively correlated risks tends to be circumscribed,Footnote 4 making it plausible that a shift of resources from the provision of public goods to those who have suffered losses would often be socially desirable. The rationale for government help in the face of large, correlated risks presented here is different from those discussed in prior writing to my knowledge. First, some authors who have stressed that insurance coverage against disasters is limited suggest that this problem per se justifies government relief.Footnote 5 But that conclusion does not follow when the limited coverage is the result of privately-optimal risk sharing—government relief is warranted only if the government provides public goods. If the government has no need to provide public goods, the government will not have command over resources to advantageously draw upon to furnish relief when privately-optimal risk sharing leaves individuals wanting.Footnote 6
 A second reason for government aid is premised on the assumption that the government is uniquely able to distribute risk over the entire population through the medium of the income tax system. Under this assumption, when the government assumes a risk, like that of a natural disaster, the impact on the individual taxpayer in a large population is small, making the government a desirable bearer of risk.Footnote 7 The foregoing argument is different from the one I consider because I assume that all individuals in the population are able to jointly contract to share risk. Hence, in the model examined below, the government does not enjoy any advantage over the private sector in the spreading of private risks across the population. A third justification for government relief is that individuals may systematically underestimate risk, leading them to underinsure.Footnote 8 A fourth justification is adverse selection, for it can reduce or eliminate private coverage even though coverage would be socially desirable.Footnote 9 These two reasons for government aid are obviously different from that advanced here. Finally, let me comment on writing that is skeptical of the basis for a governmental role in relieving large risks. The most commonly made argument against a governmental role is an expression of the general view that governmental intervention in reasonably well-functioning markets—here insurance markets—tends to be undesirable.Footnote 10 This classic laissez-faire belief, though, is not necessarily valid when applied to insurance markets. As I have emphasized is the case in the model developed here, even if private markets for risk-sharing function efficiently, the government should sometimes act to relieve risk using funds that otherwise would finance public goods. More particular arguments are also made against government relief against risk. One of note is that although problems of moral hazard (for example, an insured person inefficiently building a house in an area vulnerable to floods) may lead to limited coverage against risk, moral hazard is not a justification for government relief. Moral hazard is in fact generally exacerbated by provision of government relief.Footnote 11 Such problems are serious ones that should clearly be taken into account in the design of policies for relief, but the model here abstracts from them.",8
49.0,3.0,Journal of Risk and Uncertainty,30 December 2014,https://link.springer.com/article/10.1007/s11166-014-9204-1,Evidence that the accuracy of self-reported lead emissions data improved: A puzzle and discussion,December 2014,Sammy Zahran,Terrence Iverson,Anthony Underwood,Male,Male,Male,Male,"Environmental regulatory agencies increasingly rely on self-reported emissions data to oversee polluters. The U.S. Environmental Protection Agency’s Toxics Release Inventory (TRI) program, for instance, uses firm-reported data to track emissions and transfers for roughly 650 toxic chemicals. Reliance on firm disclosure raises obvious concerns since firms may have significant incentive to misreport (Livernois and McKenna 1999; Cohen 1999; Pfaff and Sanchirico 2004; Arora and Cason 1995; Brehm and Hamilton 1996). Meanwhile, assessing the accuracy of self-reported emissions data is inherently difficult. This paper adds to a small number of studies that attempt to gauge the accuracy of self-reported emissions data by checking for statistical anomalies in large datasets (Dumas and Devine 2000; de Marchi and Hamilton 2006). As the physicist Frank Benford first observed, for a wide range of datasets—among them city size, river drainage rates, and stock returns—the relative frequency of first significant digits is consistent with a distribution that is uniform on a logarithmic scale (Benford 1938; Fewster 2009). This implies, for example, that a first-digit 1 is three times more likely than a first-digit 4 and six times more likely than a first-digit 8. Since there is no reason to expect fabricated data to obey this surprising distribution, Benford fit offers a promising strategy to test for evidence of systematic data irregularities (Varian 1972). Also, in analysis of monitoring data from over 1,600 U.S. air monitors in 2005, we find that the first digit distribution of daily mean atmospheric lead concentrations follows Benford’s empirical law.Footnote 1 The behavior of objectively ascertained emissions data from air monitoring stations provides additional motivation for use of Benford fit to check for irregularities in self-reported emissions data for lead. Our analysis provides evidence that the accuracy of self-reported lead emissions improved markedly following the 2001 implementation of the Final Rule, a major regulatory change governing the U.S. Environmental Protection Agency’s oversight of lead emissions. Analysis of concurrent data on penalty size and auditing intensity with respect to reporting irregularities shows that the expected penalty associated with any particular act of misreporting did not increase (and likely decreased) in the period following the rule change (see Appendix Table 5). Meanwhile, the abatement cost burden for lead emitters increased substantially. Our findings suggest a puzzle for a simple Beckerian model (Becker 1962; Downing and Watson 1974) of firm compliance. Why would reporting accuracy improve when the expected net benefit from reporting accurately had—apparently—decreased? The extensive literature on monitoring and enforcementFootnote 2 suggests other channels through which the rule change may have influenced behavior. With decreasing expected net benefits for reporting emissions accurately, the simplest explanation for improvement in accuracy of self-reported emissions is that firms were simply bluffed into compliance. Firms could have viewed the rule change as signaling the EPA’s intent to increase auditing and enforcement intensity for lead.Footnote 3 Hammit and Reuter (1989) discuss survey evidence showing that small hazardous waste generators substantially over-estimate the probability of being subject to a random government audit. Thus, even though auditing intensity did not in fact increase in the post-Final Rule period, firms may have acted on the false expectation that it would. A further channel is market pressure. Hamilton (1995) estimates that publicly-traded firms who report above-threshold toxic emissions for the first time under the TRI disclosure requirements experience an average negative abnormal return of −0.3%. Given growing public awareness about the dangers of lead exposure around 2000 and after, firms could have anticipated a heightened negative market response if caught cheating on a self-reported lead release filing. This would imply that the de facto penalty associated with getting caught was higher. Finally, heightened public awareness about lead exposure risks could have induced stronger action for social norms operating within the firm (Pargal and Wheeler 1996; Arora and Cason 1996; Cohen 1999). Legal studies show that firms exercise a social license—to meet the expectations of society and avoid actions deemed unacceptable—that may lead to actions that go beyond compliance (Gunningham et al. 2004, 2005; Kagan et al. 2003). Aside from these possible explanations, we find evidence for a further channel that seems to be operating in our example. A key feature of the Final Rule was a requirement for firms to report emissions more precisely. Of course, a more precise reporting requirement does not imply that reported emissions will necessarily be more accurate—after all, if firms have an incentive to misreport emissions, it is just as easy to lie in round numbers as it is to one decimal place of precision. Nevertheless, the rule change prompted organizational changes that may have increased the internal pressure on firms to report accurately. In particular, firms responded to the Final Rule by investing in better monitoring equipment and professional monitoring staff.Footnote 4 Presumably these changes were made to reduce the likelihood of an EPA audit. But, as we argue in Section 4, they would have also made it harder for firms to get away with false reporting as a viable long-run strategy. Consistent with this, we show that reporting accuracy is higher among firms with industry standard monitoring equipment and among firms with professional specialists in charge of signing off on reported emissions. Our emphasis on the organizational change channel contributes to research extending optimal penalty theory to extra-governmental constraints on firm violations of environmental law (Lynch et al. 2004; Stretesky and Gabriel 2005). Our empirical design extends de Marchi and Hamilton’s (2006) groundbreaking work on the accuracy of self-reported emissions in four distinct ways. First, rather than cross-sectional analysis of reporting accuracy across pollutant categories, we exploit a regulatory experiment to test for changes in reporting accuracy in time, using a difference-in-differences regression strategy comparing self-reported lead emissions (subject to the Final Rule) and emissions of pollutants not subject to the Final Rule. Second, we validate Benford fit as an analytic tool by comparing self-reported emissions across emissions categories over which firms have varying levels of reporting discretion. Third, to accommodate concern about data censoring due to threshold reporting, we analyze self-reported data with respect to second- and third-digit Benford fit. Fourth, we investigate reasons for the improved accuracy in self-reported lead emissions, describing a simple theoretical model of firm behavior and providing empirical evidence consistent with theoretical expectations. The paper is organized as follows. Section 2 discusses the idea behind using Benford fit to test for the accuracy of self-reported emissions data. Section 3 presents the empirical results. Section 4 presents a simple theoretical model of firm reporting and uses this to explore a few hypotheses for the observed “puzzle.” Section 5 concludes.",5
50.0,1.0,Journal of Risk and Uncertainty,20 February 2015,https://link.springer.com/article/10.1007/s11166-015-9207-6,On preference imprecision,February 2015,Robin P. Cubitt,Daniel Navarro-Martinez,Chris Starmer,,Male,,Mix,,
50.0,1.0,Journal of Risk and Uncertainty,21 February 2015,https://link.springer.com/article/10.1007/s11166-015-9208-5,Subjective Bayesian beliefs,February 2015,Constantinos Antoniou,Glenn W. Harrison,Daniel Read,Unknown,Male,Male,Male,"We employed two choice tasks. The first was aimed at identifying risk attitudes in settings with known probabilities, and the second at eliciting subjective beliefs for events that differed in terms of the priors and sample stimuli presented to subjects. Both tasks are needed to fully evaluate the extent to which individuals correctly apply Bayes’ Rule. We presented subjects with 20 binary choices between lotteries, patterned on those used by Hey and Orme (1994). Each lottery consisted of 1, 2 or 3 monetary prizes, with four possible monetary values of £0, £5, £10 or £15. Figure 1 displays a typical lottery pair. We created 60 distinct lottery pairs, and then divided them into 3 groups of 20. Each subject made choices for one of these groups. At the end of the task one of the 20 choices of each subject was selected at random and played out for money. Typical lottery choice task These data allow us to identify the risk attitudes of subjects, using the econometric methods described in Harrison and Rutström (2008). The estimation approach can include RDU specifications as well as traditional SEU. In the second task subjects chose between bets concerning events with unknown probabilities. They were provided with enough information to infer these probabilities using Bayes’ Rule. From the choices made, in combination with some assumptions about risk attitudes, we could estimate the subjective probabilities that best described their choices. Table 1 shows the “betting sheet” with which respondents made their choices. The task involved a white box and a blue box, each containing 10-sided dice. The white box contained N 10-sided dice that each had 6 white and 4 blue sides, and the blue box contained similar dice that each had 6 blue and 4 white sides. The number of dice in each box varied across choice tasks (N = 3, 5, 9 or 17), but the two boxes always contained the same number of dice. At the beginning of each round we rolled a 6-sided die, with 3 blue and 3 white sides, and then selected either the white or blue box depending on the outcome of this roll. We then rolled the N dice in the selected box and announced the outcome. Hence the prior probability of the box being white or blue was 0.5, and the subject was given some sample information from the selected box with which to make more informed inferences about the color of the selected box. For example, suppose the two boxes each contained N = 3 dice, and the 6-sided die showed a white face. We would then roll the 3 dice in the white box and announce the number of white and blue sides that came up. The color of the selected box was not announced. After announcing the outcome from rolling the dice, subjects were asked to place their bets. This was framed as placing a £3 bet in each of 19 different betting houses that offer different odds on which box was selected. The £3 stake from one bookie was not transferable to another. The general form of this betting procedure for eliciting choices that depend on subjective probabilities is well known (e.g., Savage 1971). For each bookie, subjects placed their £3 stake on either the blue or the white box for that bookie. One would expect that subjects would be inclined to bet on the white box when the odds were generous enough for that box, and to switch to betting on the blue box for less generous odds, where what is “generous” depends on the subjects’ beliefs. For example, the odds offered by Bookie 4 imply a probability of 1/5 for the white box, and a probability of 4/5 for the blue. A risk-neutral subject would then bet on White if and only if they believed the probability that the selected box was white was 1/5 or greater. The switch point therefore corresponds to an interval of betting house probabilities that the white box was chosen. Again, if the same risk-neutral subject chose White for Bookie 4 and Blue for Bookie 5, this means his subjective probability for the White box would lie in the interval [1/5, 1/4]. In this way that subjective probability is “trapped” in the classic revealed preference manner. The recovery of subjective probabilities and beliefs requires formal theoretical and parametric assumptions, which we detail below. The essential logic is that this decision sheet is a “multiple price list,” just like the decision sheet used to infer discount rates by Harrison et al. (2002), the decision sheet used to infer risk attitudes by Holt and Laury (2002), and the decision sheet used to infer valuations for goods by Andersen et al. (2007). The general “multiple price list” betting interface employed here was first used by Fiore et al. (2009). Each subject participated for 30 rounds of this betting task, 4 rounds with N = 3 dice, 14 rounds with N = 5 dice, 6 rounds with N = 9 dice, and 6 rounds with N = 17 dice. This distribution of sample sizes was chosen to ensure that we observe roughly the same number of “extreme” samples.Footnote 1
 To control for possible order effects, in half of the 12 sessions we counterbalanced the order of the risk and betting tasks. In the betting task we varied the presentation of sample sizes in ascending order (i.e., first 4 rounds of 3, then 14 rounds of 5, etc.) and in descending order (6 rounds of 17, 6 rounds of 9, etc.). Therefore we have an overall 2 × 2 design, with 4 treatments in total. Our instructions illustrated the factors that affect betting in field betting markets, such as betting on a horse race with different bookies, and drew parallels between such naturally occurring events and our task. Subjects first read these instructions quietly; we then read them aloud and allowed time for questions. We had 3 practice rounds with boxes containing 4 dice and hypothetical bets, so that subjects could become accustomed to the process. At the end of the 30 rounds we randomly chose one bet for each subject, and played that bet out for real consequences.Footnote 2
 The complete instructions for both these tasks are reproduced in Appendix 1, which can be found in the accompanying online document. We recruited 111 subjects from the University of Durham. Subjects were recruited using a computerized interface, after being solicited in general terms to register for paid experiments. All subjects received a £5 show up fee. Apart from the tasks described above, each subject completed a survey of demographic characteristics shown in Appendix 1. Payments for the experiment totaled £2,692, for an average payment of £24.26 per subject. There were 12 experimental sessions, each having approximately 10 subjects. To ensure credibility, in each session a randomly selected subject acted as a monitor for the belief task, rolling the dice, counting the number of blue and white sides, and announcing the outcome. The monitor was paid a flat fee of £10 for the belief task, and participated with everyone else in the risk task.",15
50.0,1.0,Journal of Risk and Uncertainty,17 February 2015,https://link.springer.com/article/10.1007/s11166-015-9206-7,Gender differences in reward sensitivity and information processing during decision-making,February 2015,Kaileigh A. Byrne,Darrell A. Worthy,,Unknown,Male,Unknown,Male,"The conventional wisdom that men never stop to ask for directions while women prefer receiving directions over consulting a map reflects a gender difference in the types of information males and females prefer to utilize when making decisions. People make decisions every day—from choosing brands of foods to purchase at the grocery store, to career choices, to financial investments, decisions are made in a multitude of domains and often can have significant consequences. Consequently, it is important to understand how people make decisions, the information they attend to or ignore, and how individual differences account for variation in decision-making. Much of the prior research on gender differences in decision-making has focused on risk-taking behavior and attention to gains compared to losses. However, much work remains to be done to develop a full understanding of how men and women differ during decision-making. Specifically, it is not clear if there are gender biases in the use of external information and reward sensitivity and whether such differences are advantageous or detrimental when making decisions. One consistent finding is that women are less risk-seeking than males (e.g., Croson and Gneezy 2009; Jianakoplos and Bernasek 1998; Powell and Ansic 1997; Wong and Carducci 1991). Risk-taking is a critical aspect of investing, and indeed, women demonstrate significantly greater risk aversive behavior in investing mutual funds compared to men (Dwyer, Gilkeson, and List 2002) and their pensions after retirement (Bajtelsmit and Bernasek 1996). While increased risk-taking in the stock market can result in larger gains, it is still a gamble and can equally result in large losses. Barber and Odean (2001) analyzed a sample of over 35,000 household investments and found that men traded 45% more often than women, but the increased trading also resulted in a larger net loss. It appears that in an effort to maximize the future value of their investments, males take more risks but also pay more for the consequences of this behavior, often leading to counterproductive results. Reduced risk-taking may be related to other work that suggests that women are more sensitive to losses than men. This is one purported reason for why women have performed more poorly on the Iowa Gambling Task than men (e.g., Bolla et al. 2004; Reavis and Overman 2001; Weller et al. 2010). Females exhibit more loss aversive behavior in the task by avoiding the advantageous decks more after they have yielded a loss compared to males (van den Bos et al. 2013). Other gambling tasks with a gain-loss structure have also shown that females exhibit more risk aversive behavior (e.g., Levin et al. 1988; Brinig 1995; Eckel and Grossman 2008). Even on a riskless task in which participants were asked to rate their willingness to sell their car or purchase a new car, females gave more loss aversive responses (Gaechter et al. 2007). In addition to risk-taking, females also exhibit reduced risk adjustment. It is important to note that risk adjustment is not necessarily reflective of risk aversive behavior, but rather indicates a strategy to potentially improve a high-risk situation (Wehrung et al. 1989). Gender differences in risk adjustment have been examined in the context of gambling situations. For example, compared to males, females are less likely to increase bets as the probability of a win increases on the Cambridge Gambling Task (Deakin et al. 2004; van den Bos et al. 2012). In this task participants try to earn as many points as possible by choosing which of two boxes an object is hidden in and then betting on their decision. Each object is given a ratio reflecting the likelihood that the object is hidden there. As the likelihood that a given box contained the object increased, males bet higher amounts, but females tended to change their betting amount less even as the likelihood of a win increased. Although no gender differences in risk-taking were observed, the tendency to engage in less risk adjustment indicated that females exhibited slightly more cautious betting behavior than males in the task. Gender differences during decision-making have also been attributed to differences in information processing (van den Bos et al. 2013). The way in which a decision-making situation is contextualized influences the information processing styles individuals utilize (Slovic and Lichtenstein 1983). For example, in a bidding context, dollar values might be the most salient feature that individuals use to make decisions. However, in an investment scenario, opportunities that are both immediately cost effective and also maximize future returns may be most salient. Thus, the situations that involve single compared to multiple salient features entail different information processing styles (Schoemaker 1998). Males attend more to global information by focusing on a single aspect of an overall task while women attend more to detailed information by combining multiple aspects of a task (Williams and Meck 1991; Andreano and Cahill 2009; Cahill 2006). For example, females attend to both the frequency of wins and losses of each deck as well as the long-term pay-off structure on the Iowa Gambling Task (van den Bos et al. 2013). In contrast, males make choices on the Iowa Gambling Task (IGT) based more on the long-term pay-off of each deck alone, which may explain why females underperform, relative to males on the IGT. Females tend to select the disadvantageous deck (Deck B) that leads to larger immediate reward and infrequent but very large losses more than males, while males are better able to globally process both the frequencies of wins with the magnitude of their values, thus attending to the long-term pay-offs of each deck (van den Bos et al. 2013). Furthermore, the selectivity model of information processing proposes that males process information selectively, relying more on the overall objective individual cues and heuristics, while females are more likely to comprehend and integrate all available details, including both subjective and objective information (Meyers-Levy 1989). Because the IGT only utilizes objective cues, the frequency and magnitude of wins and losses, it is unclear how males and females would perform on a decision-making task if external information were also provided. For example, proficiency at focusing on the long-term rewards on the IGT relies on objective cues alone, but whether males also process information globally when subjective external information is involved remains ambiguous. The present study seeks to resolve this issue by directly examining how gender affects attention to immediate versus long-term rewards and external information. To do this, we utilize a dynamic, frequency-driven decision-making task with a choice-history dependent reward structure in which the reward offered on each trial depends on the individual’s selection of choices on prior trials (Byrne and Worthy 2013; Gureckis and Love 2009; Worthy et al. 2011; Worthy et al. 2012). This type of reward structure mirrors real-world decision-making situations in which the long-term consequences are contingent on choices made previously, such as choosing to invest a sum of money rather than spend it immediately. Figure 1 shows the reward structure used in the Increasing-Optimal task in Experiment 1. The Increasing option always provides a smaller immediate reward on each trial, compared to the alternative option (the Decreasing option). However, rewards provided by both options are a function of how often the Increasing option has been chosen over the previous ten trials, as indicated on the x-axis. For example, the reward options for the 11th trial depend on choices made on the first 10 trials of the task. Similarly, the reward options for the 31st trial depend on the choices made on trials 21–30. Rewards increase as the Increasing option is selected more often, but decrease as the Decreasing option is selected more often. In this task the optimal action is to select the Increasing option on every trial, even though it provides smaller immediate rewards. Consequently, the task is frequency-driven as performance is determined by the number of times the Increasing option is selected. Reward structure for the Increasing-optimal task in Experiment 1 We manipulate the information that is presented on each trial by showing half of participants the foregone reward for the non-chosen option (Fig. 2). The foregone reward information should bias participants toward the sub-optimal alternative by making it salient that the Decreasing option always provides a larger immediate reward than the Increasing option. This manipulation allows us to examine whether females generally prefer options that provide larger immediate reward more than males, or whether females are more likely to use all available information to guide decisions. If females have a general preference for immediate reward then they should select the Increasing option less, and perform more poorly throughout the task regardless of whether foregone reward information is presented or withheld. However, if females are less able to process information selectively, compared to males, then they may be biased toward the sub-optimal decreasing option only when presented with foregone reward information. This would lead to equivalent performance for males and females when foregone reward information is absent, but poorer performance for females when foregone reward information is present. Sample screenshot from the foregone present (right) and foregone absent (left) condition of the Increasing-optimal decision-making task In Experiment 2 we further examine how gender affects dynamic decision-making by using a Decreasing-Optimal task in which the reward structure is altered so that the Decreasing option is now the optimal choice (Fig. 3). In this task the gain in future reward from selecting the Increasing option is too small to match the larger immediate rewards consistently provided by the Decreasing option. Selecting the Increasing option is ultimately futile because the maximum reward that can be obtained from the Increasing option is smaller than the minimum reward that can be obtained from selecting the Decreasing option. If males outperform females on the Increasing-Optimal task in Experiment 1, then one possibility is that males have a general bias toward improving reward on future trials, while females are biased toward immediate reward. This bias would lead to better performance in Experiment 1, but poorer performance in the Decreasing-Optimal task in Experiment 2. Alternatively, males could be biased toward selectively filtering out the foregone reward information which would lead to poorer performance, relative to females, in Experiment 2 when foregone reward information is present, but equivalent performance when foregone reward information is absent. Reward structure for the Decreasing-optimal decision-making task in Experiment 2",31
50.0,1.0,Journal of Risk and Uncertainty,21 February 2015,https://link.springer.com/article/10.1007/s11166-015-9209-4,Peer effects in risk taking: Envy or conformity?,February 2015,Amrei M. Lahno,Marta Serra-Garcia,,Female,Female,Unknown,Female,"Decision making under risk is mainly studied at the individual level. Yet, an increasing body of research documents peer effects in risk taking. Peers have a large impact on stock market participation (e.g., Shiller 1984; Hong et al. 2004), investment decisions (Bursztyn et al. 2014) and insurance choices (Cai et al. forthcoming), among others.Footnote 1 Further, recent laboratory studies have documented the existence of peer effects in lottery choice tasks (e.g., Cooper and Rege 2011; Viscusi et al. 2011). A main open question is what are the sources of peer effects? In the terminology proposed by Manski (2000), a main source of peer effects is preference interactions, whereby individual preferences depend on the actions of others. Such preferences were argued as central to risk taking early on by Shiller (1984). As Manski (2000) writes, preference interactions may arise from “everyday ideas” such as envy or conformity. In other words, in environments with complete information, peer effects may be generated because individuals care about others’ outcomes (envy) or because they care about others’ choices (conformity). In this paper we ask which factor leads to peer effects: concern about others’ allocations, others’ choices, or both? Our study is the first to distinguish between these two sources of peer effects, using two main treatments: a treatment in which peers are randomly allocated a lottery and a treatment in which peers choose between lotteries. Understanding the sources of peer effects is important for several reasons. First, much attention in the literature on peer effects has been given to envy, a central concept in models of distributional social preferences, e.g., Fehr and Schmidt (1999). These types of preferences have been used to explain peer effects in risk taking, including asset pricing (e.g., Galí 1994; Gebhardt 2004, 2011). However, less attention has been given to preferences where the choices of peers, conditional on payoffs, have a direct impact on an individual’s behavior. Among others, a main reason why choices may matter is provided by studies in social psychology, which show that individuals are often driven by a norm to conform to others’ behavior (e.g., Cialdini and Trost 1998; Cialdini and Goldstein 2004). In models of conformity, others’ choices provide a social anchor to which individuals conform (Festinger 1954). If peer effects are strongly driven by peer choices, as suggested by our data, this suggests that models of social interaction effects in risk taking should not only focus on envy, but also allow for peer choices per se to matter. The source of peer effects in risk taking is also of central relevance for policy implications. Consider the case of a social planner who aims at increasing the spread of insurance take-up in a society. If peer effects are mainly driven by others’ allocations, a potentially effective policy would be to endow some individuals with insurance (e.g., giving it for free). In contrast, if active choices of peers are relevant, a more effective policy would be to publicly inform people about the choice to purchase insurance by some individuals. Other examples include pension plan choices, where individuals can be viewed as making active choices or following the default plan (through automatic enrollment). Our paper provides a clean laboratory comparison of the two sources of peer effects and as such can be considered a first step towards understanding how policies aimed at shaping risk taking behavior in society would work. To be able to cleanly identify peer effects, we use a controlled lab experiment in which individuals make risky choices, first individually and then in groups of two. One player is assigned to be the first mover (peer) and the other the second mover (decision maker). Risky choices are made between two simple lotteries, with at most two outcomes and the same probabilities, and there is complete information. We examine behavior in two main treatments. In the first treatment the peer chooses among lotteries (Choice treatment). In the second treatment, the peer is randomly allocated a lottery (Random treatment). We compare these treatments to a control treatment in which the peer makes a random draw, by clicking on a computer-simulated die, whose outcome (odd or even) is completely unrelated to the lotteries or payoffs. Since there are only two equally-likely outcomes, we refer to this as the Coin treatment. Our experimental design allows us to investigate the question of how decision making changes in the presence of peers. Depending on the type of preference interaction, peer effects may lead to imitation or deviation (Clark and Oswald 1998). To identify the direction of peer effects, as well as to avoid feedback effects, we elicit the decision maker’s choices conditional on the peer’s choice, allocation or unrelated act. This allows us to observe four different potential strategies. The decision maker may condition his choice on the peer’s choice, allocation, or act, by either imitating or deviating. On the other hand, the decision maker may choose not to condition. In this case he either makes the same choice as he made individually or changes it, both being irrespective of the peer’s choice, allocation, or act. We say peer effects occur if the decision maker chooses not to stay with his individual choice. Our experiment yields three main results. First, we find that peer effects differ significantly when the peer is allocated a lottery compared to when she chooses a lottery, though payoffs remain constant across treatments. Decision makers choose not to stay with their individual choices in 18% of the cases in Random and in 33% of the cases in Choice. Hence, choices of the peer matter, above and beyond their direct impact on payoffs, and almost double peer effects. Second, we observe that the direction of peer effects in both Random and Choice is towards imitation. The likelihood of imitation increases, more precisely doubles, in Choice compared to Random. This indicates that, conditional on being affected by the peer’s presence, individuals seem to exhibit preferences over others’ payoffs and, in particular, these are such that individuals prefer to imitate their peers. In terms of pure relative payoff concerns, which are present in Random, this indicates that individuals are loss averse with respect to the peer. The dislike of falling behind is higher than the desire to be ahead, leading to imitation. Additionally, imitation is significantly more frequent both in Choice and Random compared to a control treatment, while revision rates (which are not conditional on the peer) do not differ across any of these treatments. Third, we find that imitation is more frequent when the peer has chosen the “safer” (in terms of variance) lottery, compared to when she chooses the “riskier” lottery. This tendency to imitate the peer especially when she chooses a safer option does not differ across treatments Random and Choice. It is an unexpected and interesting result. It suggests that peer effects may depend on the relative risk positions and thus that decisions such as insurance take-up may spread faster or more strongly through peers than decisions to purchase stocks. Overall, in our environment, peer effects cannot only be explained by concerns about others’ payoffs relative to own. This implies that a parsimonious explanation of preference interactions in risk taking needs to allow peer choices to matter. We examine two alternative explanations for why peer choices matter. First, one may consider a more flexible specification of preferences over others’ payoffs, which change depending on whether the peer makes choices or not. Studies of fairness considerations in risk taking (e.g., Cappelen et al. 2013) suggest that the strength of relative payoff concerns might depend on whether the peer actually makes a choice and, if so, whether she chooses to take on more or less risk than the decision maker. Their results suggest that in the Choice treatment we should observe a stronger increase in imitation when the safer lottery is chosen. A second alternative explanation for the increase in peer effects is, broadly speaking, that individuals are influenced by a norm to conform to others. More specifically, according to Festinger’s (1954) theory of social comparison, individuals care about making correct choices and, in the absence of objective measures of correctness, consider others’ choices as an anchor for correctness. Hence, if individuals exhibit such a preference to conform, peer choices should matter. They should increase the likelihood of imitation in Choice and this increase should not depend systematically on the type of lottery, risky or safe, or its expected payoff. Since there was complete information, peer effects in our experiment cannot be explained by a model of rational social learning (e.g., Bikhchandani et al. 1998).Footnote 2 In the presence of complete information, under standard assumptions of rationality and self-interest, decision makers do not learn from others. However, decision makers who exhibit preferences for conformity may learn about the correctness of their choice. Our results reveal that the increase in imitation from Random to Choice does not depend on the lottery choice of the peer. This result is at odds with an increase in envy that is dependent on how much risk the peer chooses to take. However, it is broadly in line with an explanation that choices matter due to a norm to conform to others. As an additional test, we structurally estimate a model of relative payoff concerns and a model based on social comparison theory, where individuals derive a constant utility from conforming to the peer’s choice or allocation. We allow both relative payoff concerns and the utility from conforming to vary depending on whether the peer makes choices or not. We find that, under a model of relative payoff concerns, preference parameters do not change significantly when moving from Random to Choice. In contrast, the utility from conforming increases significantly in Choice. The model based on social comparison theory fits our data significantly better, which provides further suggestive evidence that a reason why choices of peers matter may be due to a norm to conform to others. Recent laboratory experiments have documented peer effects when peers are allocated lotteries (see Trautmann and Vieider 2011, for an overview). Several studies (Bault et al. 2008; Rohde and Rohde 2011; and Linde and Sonnemans 2012) report that lotteries allocated to peers affect, in varying degrees, individual risky choices and emotions. Other studies focus on peer effects when peers make choices, and show that observing either the desired risky choices of others (Viscusi et al. 2011) or their past choices (Cooper and Rege 2011) significantly affects risk taking. A main contribution of our study is to compare peer effects when peers are allocated lotteries, relative to when they make active choices between lotteries. We show that, over and above relative payoff concerns, the choices of peers play a significant role in the decision maker’s behavior. As mentioned above, this suggests that, when modeling preference interactions in risk taking, it may be misguided to focus only on relative payoff concerns. Our paper also complements studies testing the channels of peer effects in other environments. Gächter et al. (2013) and Goeree and Yariv (2007) examine whether peer effects are driven by distributional social preferences or social norms (or a norm to act like others), in a gift-exchange game and a social learning environment, respectively. While Gächter et al. (2013) find that peer effects can be explained by distributional social preferences, Goeree and Yariv (2007) find that conforming behavior cannot be explained by distributional social preferences, but is consistent with a preference for conformity.Footnote 3 We find that, in a risky and non-strategic environment, peer effects are explained by both relative payoff concerns and a preference to conform to others. The remainder of the paper is organized as follows. In Section 2 we describe the experimental design and procedures in detail and derive testable hypotheses. Our main results are presented and discussed in Section 3. Section 4 concludes.",74
50.0,2.0,Journal of Risk and Uncertainty,17 April 2015,https://link.springer.com/article/10.1007/s11166-015-9210-y,The impact of statistical learning on violations of the sure-thing principle,April 2015,Nicky Nicholls,Aylit Tina Romm,Alexander Zimper,Male,Unknown,Male,Male,"This section describes in detail the experiment that we conducted to test whether violations of the STP decline through learning by thought, resp. statistical learning, or not. As our point of departure consider the following thought experiment of Marinacci (2002):  “Consider a decision maker (DM) who has to make a decision based on the drawings of an urn of known composition. The confidence he has in his decisions will depend on the quality of the information on the balls’ proportion, the more he knows, the more he will feel confident. Suppose the DM can sample with replacement from this urn before making a decision. Regardless of how poor is his a priori information about the balls’ proportions, it is natural to expect that eventually, as the number of observations increases, he will become closer and closer to learn the true balls’ proportion and become more and more confident in his decisions.” (p. 143) In accordance with Marinacci’s thought experiment, we generated uncertainty by means of drawings (with replacement) from an ambiguous urn containing balls of red, yellow, and blue color. Whereas the test group was given the opportunity to gradually learn the true proportions of different colored balls in the urn, the control group did not receive any statistical information over the course of the experiment. The test for violations of the STP follows similar approaches in the literature (e.g. Wu and Gonzalez 1999; Chapter 10.4.3 in Wakker 2010) whereby a caveat applies (see Appendix D). Consider the three events R(ed ball is drawn), Y(ellow ball is drawn), and B(lue ball is drawn). We test for violations of the STP through a choice between prospects A and B, on the one hand, and a choice between prospects A
′ and B
′ on the other hand. These pairs of prospects have the following payoff structure:
 where z>y>x are monetary amounts. In the experiment any such pair of prospects will correspond to a pair of observed choices whereby we interpret these choices as revealed (strict) preferences; that is, we interpret, e.g. the choice pair A,A
′ as revealed preferences A
≻
B and A
′
≻
B
′. In Appendix A, we formally show that the choice pairs 
 are consistent with the STP, whereas the choice pairs 
 violate the STP. Further, suppose that STP violating decision makers can be described as prospect theory decision makers (cf. Wakker 2010; Abdellaoui et al. 2011). Prospect theory in our sense encompasses Gilboa’s (1987) Choquet EU axiomatization as well as the axiomatizations of cumulative prospect theory in Tversky and Kahneman (1992) and in Wakker and Tversky (1993) and it has the descriptive advantage that it can simultaneously accommodate violations of the STP as well as of the IA.Footnote 4 We demonstrate in Appendix B that the choice pair A,B
′ is associated with optimistic whereas B,A
′ is associated with pessimistic ambiguity attitudes of prospect theory decision makers. Undergraduate commerce students at the University of the Witwatersrand (Wits) were recruited to participate in the experiment. Students were approached during economics lectures and were given an information sheet with brief background about the study and requesting their voluntary participation. Students agreeing to participate had to be over 18 years of age, and had to be students at Wits. Since the study design included a test and control group, the experiment was designed to start with a fairly homogenous sample in terms of education, then to randomly assign participants to one of the two groups. To avoid introducing possible biases in responses, the presence of the test and control groups was not discussed with participants, nor was the exact nature of the experiment. Participants were simply told that this was a study on decision making. In total, 63 students were recruited, allowing for a minimum of 30 students in each group (31 in the test group and 32 in the control group). The control group had 56% male and 44% female participants, while the test group had 48% male and 52% female participants. In line with Wits’ ethical policy on experiments, participants were assured of anonymity in the experiment whereby questionnaire responses were recorded with numbers instead of names. We used an actual urn that contained 20 red, 20 yellow, and 60 blue balls. Although the participants did not know the true proportions of the colors in the urn, they were informed that there were 100 balls in total. All respondents answered 30 questions about their preferred choice between two different prospects. By varying the monetary payoffs associated with these prospects (measured in South African Rand whereby R10 ≃ US$ 1),Footnote 5 these 30 questions were organized as 15 subsequent choice pairs (A,B;A
′,B
′) exhibiting the payoff structure described in Section 1.1. This design thus allowed for the observation of up to 15 subsequent violations of the STP by any subject through revealed preferences (2). Following the draw of a ball after each question, respondents in the test group received feedback on the color of the ball drawn from the urn. Respondents from the control group did not receive such feedback. In contrast to the control group, the test group could thus observe statistical information in the form of 30 actual drawings (with replacement) from the urn. Technically speaking, the respondents from the test group were thus exposed to 30 successive iid multivariate Bernoulli trials such that the true proportions of differently colored balls were (supposedly) driving the data generation process. The respondents sat in front of the computer where they first read participant instructions and then went through the 30 questions of the questionnaire (see the Supplementary Appendix). The first author was present at all times to answer questions and to assist with the random ball selection following each question for the test group. We used randomization of the question pair order to avoid any bias from possible order effects due to, e.g. different magnitudes or ratios from the prospects’ possible payoffs. More specifically, the computer programme selected a random starting question pair, and randomized each subsequent question pair. In this way, each respondent would see a unique order of questions, with all respondents seeing all 15 pairs of questions, but in a random order. To avoid bias from the order of presentation within question pairs the labelling of prospects within question pairs was varied. Once an option had been selected, the test group respondents were allowed to draw a ball from the urn and the computer programme would show the payout based on the color of ball drawn and the prospect selected. The control group received no feedback. To give incentives for the truthful revelation of preferences, subjects in both groups were told that one of the prospect choices would be selected at random to be paid out in real money (cf., e.g. Starmer and Sugden 1991). The possible payout could be as low as R0 or as high as R150, since these were the minimum and maximum payouts across the range of questions. At the end of the experiment, i.e. after answering all 30 questions and receiving (the test group) versus not receiving (the control group) statistical information, the respondents from both groups were asked about their estimates for the balls’ proportions in the urn.",5
50.0,2.0,Journal of Risk and Uncertainty,10 June 2015,https://link.springer.com/article/10.1007/s11166-015-9214-7,Erratum to: The impact of statistical learning on violations of the sure-thing principle,April 2015,Nicky Nicholls,Aylit Tina Romm,Alexander Zimper,Male,Unknown,Male,Male,,
50.0,2.0,Journal of Risk and Uncertainty,19 April 2015,https://link.springer.com/article/10.1007/s11166-015-9211-x,Choice reversal without temptation: A dynamic experiment on time preferences,April 2015,Marco Casari,Davide Dragone,,Male,Male,Unknown,Male,"The Discounted Utility model (Samuelson 1937) is the most popular model for studying intertemporal choices. Under exponential discounting and additive separability, the model predicts the future to be discounted at a constant rate, which precludes the possibility of time-inconsistent behavior (Strotz 1955). To test this prediction, in most experimental studies subjects are required to solve intertemporal trade-offs by choosing between goods or monetary amounts available at different future dates. The typical design requires subjects to go to the lab only once, and the whole elicitation process is completed in that occasion (Kirby and Herrnstein 1995). Only a minority of experiments are dynamic in the sense that they track individual choices over multiple dates (for a similar point, see also Sayman and Öncüler 2009). Studies with a dynamic design have focused on real-effort tasks, as well as on monetary choices, and provide direct evidence on how subjects implement their plan of action as time goes on. For example, Ainslie and Haendel (1983) focus on substance abuse patients, Read and van Leeuwen (1998) study the choice between healthy and unhealthy snacks, Read et al. (1999) ask subjects to choose between highbrow and lowbrow rental movies, Ariely and Wertenbroch (2002) consider studying activities, Dellavigna and Malmendier (2006) and Charness and Gneezy (2009) focus on choices concerning attendance at the gym and physical exercise. In the recent literature, a dynamic experimental design with monetary incentives has been used in Casari (2009), Sayman and Öncüler (2009), Ginè et al. (2012), and Read et al. (2012). A robust finding in the literature on intertemporal behavior is that people are often subject to present-biased preferences and to choice reversals. To account for this evidence, many scholars argue in favor of a quasi-hyperbolic model of time preferences as a better descriptor of intertemporal choices, and suggest that individuals are tempted by, and some of them fall for, instantaneous gratification (e.g., Laibson 1997). Other scholars have proposed alternative explanations. A well-known theoretical explanation is based on the role of uncertainty, which can be articulated in many ways. Yaari (1965) studies the impact of uncertain lifetime for intertemporal choices; Sozou (1998) rationalizes discounting in terms of risk that a delayed reward may not be received; Azfar (1999) studies the role of uncertain discount rates; Fernández-Villaverde and Mukherji (2002) consider random shocks on consumption preferences; Dasgupta and Maskin (2005) study an uncertain environment where payoffs may be realized early; and Halevy (2008) considers the role of uncertain lifetime and default probability. A way of dealing with both uncertainty and present-biased preferences is presented in O’Donoghue and Rabin (1999), who consider the case where the cost to complete a task is stochastic and agents may have incentives to procrastinate. Interestingly, despite the abundance of theoretical models pointing to the role of uncertainty, experimental results of intertemporal choices are often interpreted under the assumption of a certainty scenario. This assumption facilitates the identification of a point estimate for discount factors, but it neglects that uncertainty is often an intrinsic feature of intertemporal decisions, even when the experimenter does not explicitly introduce it. This may be one of the sources of the large variability in the estimates of discount factors reported in the literature (Frederick et al. 2002). There exist experiments on intertemporal choices which employ money and experiments involving non-monetary rewards, such as noise (Millar and Navarick 1984), rice (Pender 1996), drinking water (Brown et al. 2009), and chocolate (Reuben et al. 2010). Both monetary and non-monetary domains are relevant for economic decisions and both may help in studying time preferences. The most common practice among experimental economists is to use money as a reward, which provides direct information concerning financial decisions and monetary trade-offs. It is possible, however, that the experimental results over money differ from those over consumption goods if preferences are domain-specific and/or if there are measurement errors when using monetary rewards. Hence additional evidence with non-monetary rewards can shed light on economic decisions related to consumption goods, effort exertion and completion of chores. For instance, if time preferences turned out to be domain specific, the results obtained over money rewards would not extend to very common activities in the workplace and in everyday life such as filing a tax return, tidying up the office desk, completing an assignment, going to the gym, watching TV, or playing with the children. In addition, when the stakes are non-monetary, drivers other than discounting may emerge. For example, Loewenstein (1987) argued that there may be anxiety while waiting for a bad (non monetary) outcome, and savoring while waiting for a positive (non monetary) outcome. When this is the case, one would predict behavior which goes in the opposite direction with respect to the one predicted by the Discounted Utility model. The idea behind domain-specific anticipatory feelings is appealing and intuitive, but the existing evidence over intertemporal choices is still weak (Casari and Dragone 2011a) and deserves further experimental validation. Furthermore, a methodological issue which may lead to measurement errors arises when the reward used in the experiment is storable and tradable outside the lab (Coller and Williams 1999; Cubitt and Read 2007). Storability is an issue because it does not allow us to exactly control for the timing in which the reward is consumed, which is the primitive an experimenter would like to measure. On the contrary, when the reward is to be consumed in the lab and cannot be stored for later, one can be sure about the timing of consumption and obtain more reliable information on time preferences. Tradability of money outside the lab may also bias the experimental results, because it allows for intertemporal arbitrage due to the existence of an external credit market.Footnote 3 By employing a consumption good that cannot be stored, nor consumed outside the lab, our experiment directly addresses these issues in order to improve the reliability of the data.",5
50.0,2.0,Journal of Risk and Uncertainty,24 April 2015,https://link.springer.com/article/10.1007/s11166-015-9212-9,Behavioral bias and the demand for bicycle and flood insurance,April 2015,Mark J. Browne,Christian Knoller,Andreas Richter,Male,Male,Male,Male,"In the current section, we review the literature on HPLC and LPHC risks and insurance demand. The section contains three subsections. In the first, we review prior studies based on laboratory experiments on HPLC and LPHC risks. In the second, we review prior evidence on the underestimation of low probability events and the purchase of insurance. Much of the research in this area focuses on flood insurance demand. Finally, we consider the role of policyholder characteristics and sales channel in the demand for insurance for HPLC and LPHC risks. Consider a simple model. An individual with an initial wealth \( w \) faces a risk R
1(p
1, L
1): a loss L
1 occurs with probability p
1. The individual is risk averse (u′ > 0, u″ < 0) The expected utility (EU) of the individual is given by the following equation: Now assume that the individual faces a different risk R
2(p
2, L
2) with the same expected value instead. However, L
2 > L
1 and p
2 < p
1. Assuming equal loading factors, due to risk aversion the individual will value insurance for R
2 higher than for R
1. Since this model is based on one individual’s utility function and individuals have different utility functions, the model does not allow us to predict differences in insurance demand across individuals. In contrast to this standard prediction derived from expected utility analysis, there is experimental evidence that many individuals prefer insurance for HPLC risks over insurance for LPHC risks.Footnote 2 In an early study, Slovic et al. (1977) modeled the probability of a loss via the drawing of balls from an urn. Holding the expected value of losses constant, the researchers varied the probability and size of possible losses. They find evidence that the propensity to buy insurance increases with the loss probability. Schoemaker and Kunreuther (1979) report results consistent with these findings and discuss the adequacy of expected utility theory and prospect theory for explaining these results. McClelland et al. (1993) find an extreme bimodality in the willingness to pay for coverage on low probability risks. Individuals either had a very high willingness to pay or a willingness to pay of almost zero. This bimodality diminished when the loss probability increased. The participants in the experiments of Ganderton et al. (2000) also preferred to buy insurance for losses that occurred with a higher probability. Moreover, they find evidence that individuals are more sensitive to changes in probabilities than to changes in the size of the loss. In contrast to all other experiments, Laury et al. (2009) observe that participants bought more insurance for low probability events than for higher probability events when the expected loss was held constant. Shafran (2011) finds that participants repeatedly making choices were more willing to take self-protection measures on high probability risks. Overall, the results of the experiments reported in the literature provide evidence that individuals prefer insurance for HPLC risks. In the marketplace, there are a number of insurance products which provide coverage on HPLC risks. Examples include cellular phone insurance and extended warranties (for an interesting study thereon, see Huysentruyt and Read 2010). However, relatively few empirical studies using real-world insurance data have addressed this issue. Sydnor (2010) analyzes data on consumers’ choices of deductibles for home insurance. He finds evidence for an extremely high level of risk aversion over modest stakes in the market. Analyzing the demand for flood insurance in Florida, Michel-Kerjan and Kousky (2010) also find that more than 80% of policyholders choose the lowest possible deductible. Moreover, people with a higher limit of coverage are more likely to choose the lowest deductible. These results are in line with findings from Johnson et al. (1993) and Kunreuther and Pauly (2005), but contradict expectations based on an expected utility model. According to Schwarcz (2009), the low demand for natural hazards insurance can to a large extent be explained by policyholders’ lack of information and limited cognitive abilities. The use of heuristics seems to bias individuals’ probability judgment. Kunreuther and Michel-Kerjan (2009) suggest that besides mental accounting and planning myopia, the underestimation of risk exposure is one of the main drivers of the low demand for natural hazards insurance. In particular, individuals tend to ignore events with a probability that is below a certain threshold level. Market data on individuals’ demand for insurance for LPHC risks focuses largely on flood insurance. The National Flood Insurance Program (NFIP) in the United States has been the basis for several studies. Browne and Hoyt (2000) find that flood insurance demand increases in a state following the occurrence of a flood in the same state. Kriesel and Landry (2004) similarly find that risk exposure drives the demand for insurance: households further away from the shoreline have a lower propensity to buy flood insurance and the demand for flood insurance decreases with an increase in the hurricane return period. Landry and Jahan-Parvar (2011) find that insurance coverage is higher in higher risk zones. Using county level and policyholder level data on all NFIP flood insurance policies issued in Florida from 2000 to 2005, the results from Michel-Kerjan and Kousky (2010) indicate that the FEMA mapped flood zone is positively correlated with the demand for flood insurance. Summing up, these studies find clear evidence that in the NFIP, loss experience and risk exposure are important drivers of flood insurance demand. However, the NFIP is highly regulated, and thus it is questionable whether these results can be transferred one to one to countries where flood insurance markets are less regulated. Besides those focused on the NFIP, several survey studies have analyzed the perception of flood risk. In Switzerland very detailed risk maps for flood risk have been developed based on the risk assessment of experts. Siegrist and Gutscher (2006) find a positive correlation between participants’ risk perception and the experts’ risk assessment. However, they observe no differences in concrete prevention behavior between people living in different flood risk areas. They find evidence that some people overestimate their flood risk while others highly underestimate their risk exposure. Own flood experiences were positively correlated with risk perception. Botzen et al. (2009) analyzed the role of geographical and socio-economic characteristics for risk perception in the Netherlands based on a survey study of homeowners. They find that overall participants assessed the risk of flood as rather low. But the actual risk exposure is positively correlated with the perceived risk. Individuals living closer to a river or in low-lying areas had a higher risk perception. However, individuals that were not protected by a dike tended to underestimate their risk. Botzen and van den Bergh (2012) analyzed the demand for flood insurance for homeowners in the Dutch river delta. They find that a significant proportion of homeowners neglected the flood risk. In addition, the willingness to pay for flood insurance was less than proportionally related to increased flood probabilities that were presented to respondents in the questionnaire. Viscusi and Zeckhauser (2006) asked individuals in the U.S. to rate their risk of being killed by a hurricane, earthquake, flood, and tornado. Overall, 93.5% of the participants believed they faced an average or below-average risk. This number was slightly lower for individuals that had experienced a natural hazard event or that lived in a more risk prone area. Hence, both risk exposure and loss experience had an effect on risk perception. However, this effect was lower than would be expected based on rational calculation. The authors conclude that “risk belief results…are quite sensible in direction, but insufficient in magnitude”. Summing up, there is some evidence that individuals tend to correctly estimate their flood risk exposure relative to others, but absolutely underestimate their risk exposure. However, risk perception regarding natural hazards is still not completely understood. In particular, there is lack of data regarding real insurance decisions outside the NFIP. As discussed, the literature on the demand for insurance for HPLC vs. LPHC is mostly based on laboratory experiments. Thus, there is hardly any literature about how behavioral biases in insurance demand differ with policyholder characteristics. However, one could assume that behavioral biases differ by sales channel. A reasonable expectation is that policyholders will react to information provided by insurance agents. According to Cummins and Doherty (2006), the intermediary serves as a “market maker” that helps individuals to identify their insurance needs and matches policyholders with appropriate products. Informing policyholders about their loss probabilities and loading factors should help them to make better informed decisions (see Kunreuther and Pauly 2004). In a survey conducted in Germany, Zhou-Richter et al. (2010) analyzed the perception of long-term care (LTC) risk and the propensity to purchase LTC insurance. Respondents mostly underestimated the probability that their parents would require LTC and had a rather low willingness to pay for insurance covering this risk. After they were provided information about their actual risk exposure, the percentage of participants willing to purchase LTC insurance substantially increased. There is evidence that the abstract disclosure of probabilities will not strongly affect policyholders’ decision behavior (see, for instance, Agarwal et al. 2009 and Schwarcz 2009). However, Kunreuther et al. (2001) show that individuals have less of a problem evaluating small probabilities if they are given rich contextual information (for instance, a comparative scale or scenarios) that provides them with the opportunity to consider the event in a familiar context that evokes their own feelings of risk. Insurance agents might be able to provide this informational context and thus be able to increase the willingness of policyholders to purchase flood insurance.Footnote 3
",44
50.0,2.0,Journal of Risk and Uncertainty,02 June 2015,https://link.springer.com/article/10.1007/s11166-015-9213-8,Parametric preference functionals under risk in the gain domain: A Bayesian analysis,April 2015,Kelvin Balcombe,Iain Fraser,,Male,Male,Unknown,Male,"There is a long history of research questioning the validity of Expected Utility Theory (EUT), with many economists wishing to apply non-EUT theories to problems relating to decisions under risk. Within the literature some degree of consensus appears to have emerged that probability weighting models such as Prospect Theory (PT) or Rank Dependent Utility (RDU) offer the best alternative to EUT (Wakker 2010; Fehr-Duda and Epper 2012). Yet, while the scope for applications of PT and RDU is increasing (see Barberis 2013, and Shleifer 2012), the growth of empirical applications is arguably less than one might expect given their theoretical prominence. One potential reason is that the range of parametric variants of these theories can itself be baffling, and perhaps may inhibit their adoption. Therefore, this paper reconsiders the appropriate selection of parametric specifications of choice under risk within the gain domain. While there is plenty of evidence that most economic agents do not seem to unerringly use probabilities as summative linear weights to utilities of outcomes, what they actually do remains the subject of debate. Leading critics of EUT include Kahneman and Tversky (1979) and more recently Rabin (2000) and Rabin and Thaler (2001), who consider the weight of evidence against EUT sufficient to label it an ‘ex-hypothesis’. In contrast, Birnbaum (2006) argues a case against probability weighting of the PT form, and more recently, the unfavourable implications for EUT from the concavity-calibration argument of Rabin (2000) have been challenged by Cox and Sadiraj (2006) and Cox et al. (2013) on the grounds that calibration arguments lead to equally problematic implications for nonlinear probability weighting. Furthermore, models in which outcomes are weighted by functions of probabilities have also been challenged at the process level (Fiedler and Glöckner 2012). The understanding that emerges from the literature is further muddied by the fact that individuals may use different (and multiple) strategies. For example, Bruhin et al. (2010) report results that indicate that at least 20% of respondents in their experiments can be classified as EUT types, while Harless and Camerer (1994), and Hey and Orme (1994) have presented analysis of a range of theories and models suggesting that no one theory clearly outperformed all others. There is also the important question about how best to nest what are ostensibly deterministic theories within a stochastic setting. As Hey and Orme (1994) observed, while the issue of “noise” has often been treated as an ancillary one, it deserves greater attention such as the research presented by Wilcox (2011). In practice, for applied researchers examining decision making under risk the implications of the above for conducting research come down to a choice of appropriate functional forms. Thus, the choice of functional forms to be employed to operationalise the theory is key and even if researchers narrow the range of candidate models to within the PT or RDU class,Footnote 1 they face an enormous set of potential models. Furthermore, the literature is still unable to give definitive advice in this regard mainly because there are so many potential combinations of functional forms that are used to model the different aspects such as value (utility), probability transformations and those linking the deterministic models to stochastic outcomes. Each of these model aspects interacts with others to determine overall model performance and there is a need to understand how different model aspects perform in combination. The data employed here is from Stott (2006), which to date provides one of the most comprehensive studies of the performance of a range of functionals characterising PT in the gain domain.The results reported in Stott are frequently cited for the choice of functionals employed in PT/RDU research (e.g., Bruhin et al. 2010). Unlike Stott (2006) we employ a Bayesian approach to the analysis of this data. The analysis in Stott (2006) and Booij et al. (2010) are typical in that they have been conducted from a classical perspective using maximum likelihood as the estimation method. However, serious issues emerge in deciding on an optimal combination of functional forms when there are so many combinations of competing specifications. Also, as noted by Booij et al. (2010) the wrong choice of a functional form can result in contamination and bias of other estimates of parameters. In this paper, we exploit the advantages of Bayesian Model Averaging (BMA) which provides an internally consistent and coherent approach to this type of modeling problem. Like Stott (2006), we examine a large number of functional forms at the individual as well as the aggregate level. Furtermore, we examine and compare specifications based on the Contextual Utility approach developed in Wilcox (2011), a generalisation of the Priority Heuristic (PH) developed in Brandstätter et al. (2006), and the Transfer of Attention Exchange (TAX) weighting function of Birnbaum and Chavez (1997). The TAX and PH models are examined as they are viewed as alternatives to PT/RDU and in both cases positive experimental evidence has been presented. The difficulty in deciding on an optimal specification for this type of problem stems from the very large model space. Classical pairwise comparison of nested models can be made using a range of standard tests (e.g., Likelihood Ratio, F, Wald) providing appropriate adjustment is made for cases where parameters lie on the edge of the parameter space or alternatives are restricted to a subset of possible values (e.g. Andrews 1998). Classical non-nested models can also be tested using the methods developed by Vuong (1989) and others.Footnote 2 However, when the number of potential models is very large, pairwise testing implies an extremely large number of tests,Footnote 3 whereby the transitivity of these tests is not assured in finite samples (Findley 1990). This means that an unambiguous ranking of models is difficult. Information criteria (IC) offer an alternative way to evaluate models. However, while IC are additive over individuals (when models are estimated at the individual level), the formal basis for using them as model weights is through their asymptotic approximation of logged marginal likelihoods. The use of IC in Bayesian Analysis of Classical Estimates has been motivated by the desire to avoid informative priors (e.g., Sala-i-Martin et al. 2004). Yet as shown in Fernandez et al. (2001) the choice of alternative g-priors leads to asymptotically different IC, which rather weakens the claim that using IC means that one is less dependent on priors. In contrast, full BMA yields an internally consistent and coherent approach to this problem, if priors can be provided. In the context of PT, we believe that there is substantive theoretical and previous empirical evidence that give a basis for setting these priors, and we examine this in detail. We are also concerned here with explicitly recognising that model selection may depend on whether one is seeking an overarching model that explains aggregate behaviour, or whether one is seeking models that allow heterogeneity in behaviour. As discussed in Andersen et al. (2008), arguments for and against models have sometimes been implicitly or explicitly based on the idea of a ‘representative agent’ where it is assumed that there exists a common model of behaviour across all individuals both in the preference functionals and forms and the parameters that characterise those functionals (e.g., Brandstätter et al. 2006). However, what has been insufficiently recognised is that choosing one specification that best represents all individuals is a different task from choosing multiple specifications that represent different groups of individuals. Different people may do different things when it comes to making decisions. For example, it is possible that some may employ a heuristic like the PH, and others adhere to PT. In this paper, we recognise that optimal model specification may differ depending on whether the researcher seeks a model that performs best when applied to all individuals, or whether one is interested in explaining individuals’ behaviour. Importantly, there may be models that do extremely well in explaining the behaviour of a subgroup of individuals, but do very badly if applied to all individuals. When parametric models are being estimated, there are three levels of heterogeneity that are commonly applied. Level 0 is where individuals share functional forms and have the same parameters values (i.e., the representative agents). Level 1 is where individuals share functional forms but with potentially heterogenous parameter values. And Level 2 is where individuals need not share functional forms or parameter values. Heterogeneity in parameters can be introduced, in a limited sense, by allowing the parameters to be conditioned on covariates, but more general models include those that are either a latent class model (or finite mixture of distributions) or a random parameter (or Hierarchical Bayes) approach (e.g., Nilsson et al. 2011). Heterogeneity in models can be introduced using the weighted likelihood approach outlined in Harrison and Rutström (2009) and related approaches in Bruhin et al. (2010) and Conte et al. (2011). In contrast, a number of papers, (e.g., Hey and Orme 1994; Birnbaum and Chavez 1997; and Stott 2006) have estimated multiple models at the individual level. This approach is flexible in terms of model estimation, but also requires large amounts of information to be collected at the individual level. Studies that have pursued this approach typically offer a very large number of choices (e.g., 100 or more) to each person. While an individual specific approach is flexible, it is clearly less than optimal if there is an overarching framework that is able to allow heterogeneity on one hand, but allows the pooling of information across individuals to estimate parameters that are common to all. In this paper, we consider the model performance at all three levels which involves estimating models at the representative agent level (Level 0) and the individual level (Level 2). Inference about Level 1 specifications can be examined by using the Level 2 models, and calculating the log marginal likelihoods with the common model restrictions imposed. That is, there is no additional estimation required for Level 1 models, once all Level 2 models have been estimated. The paper proceeds by describing general framework and specific models in Section 2. Section 3 discusses our approach to model comparison and model estimation. Our results are presented and discussed in Section 4 and Section 5 concludes.",15
50.0,3.0,Journal of Risk and Uncertainty,15 July 2015,https://link.springer.com/article/10.1007/s11166-015-9216-5,Probabilistic sophistication and reverse Bayesianism,June 2015,Edi Karni,Marie-Louise Vierø,,Male,Unknown,Unknown,Male,"The theory of “reverse Bayesianism” is intended to depict the response of Bayesian decision makers to expansions of their universe in the wake of discoveries of new consequences and/or acts, and improved understanding of the links between acts and consequences. In particular, we are interested in those aspects of the structure of the preferences that persist as the universe expands, since those aspects allow one to infer from existing preferences something about the preferences in the expanded environment. This feature of our model is worth emphasizing against the backdrop of consumer theory under certainty. This issue was discussed in Lancaster (1966) who made the following statement:Footnote 1
 “Perhaps the most important aspects of consumer behavior relevant to an economy as complex as that of the United States are those of consumer reactions to new commodities and to quality variations. Traditional theory has nothing to say on these. In the case of new commodities, the theory is particularly helpless. We have to expand from a commodity space of dimension n to one of dimension n+1, replacing the old utility function by a completely new one, and even a complete map of the consumer’s preferences among the n goods provides absolutely no information about the new preference map. A theory which can make no use of such information is a remarkably empty one.” (Lancaster (1966) p. 133, the italics are ours). In our earlier work on reverse Bayesianism (see Karni and Vierø 2013) the aspects of the preference structure that remain intact when the decision maker’s state space expands are his risk attitudes and beliefs regarding the relative likelihoods of the events in the original state space. Thus, in the context of decision making under uncertainty, this theory does provide some information about the updated preferences. This paper extends our earlier work on reverse Bayesianism, replacing the subjective expected utility model with probabilistic sophistication. Probabilistically sophisticated choice is characterized by a unique subjective probability measure on a state space by which acts (that is, mappings from the set of states to the set of lotteries over consequences) are transformed to lotteries over consequences, a utility function on the set of these lotteries, and choice behavior that maximizes the utility over the lotteries corresponding to feasible sets of acts. In two seminal papers, Machina and Schmeidler (1992, 1995) axiomatize probabilistically sophisticated choice in the analytical frameworks of Savage (1954) and Anscombe and Aumann (1963). The main contribution of these works is to break the link between the subjective expected utility model and the existence of choice-based subjective probabilities. This is an important extension of Bayesian theory. Karni and Vierø (2013) introduced a model describing the evolution of the beliefs of subjective expected utility maximizing decision makers as they discover new acts, consequences, and information pertaining to links between acts and consequences. We introduced the notions of conceivable and feasible state spaces and showed how the former state space is constructed from what the decision maker perceives to be the feasible acts and consequences while the latter is derived from his revealed preferences. In addition, we showed how these state spaces expand in response to the discoveries of new consequences and/or acts. In this paper, we extend our earlier work showing that the reverse Bayesianism model is not predicated on subjective expected utility maximizing behavior. This inquiry is motivated, in part, by the large body of experimental work documenting systematic departures from the expected utility model. Accordingly, we relax the strictures of expected utility theory, assuming instead that decision makers are probabilistically sophisticated, and show that the results of Karni and Vierø (2013) hold in this, more general, framework. In particular, the model pursued here admits choice behavior that displays reversals of conditional preferences as the universe expands, a phenomenon that is inconsistent with reverse Bayesianism founded on the subjective expected utility model. As we show below (see Section 3.4), the conditional preference reversals are the result of the nonseparability of preferences across states. In particular, our model admits nonseparability with respect to outcomes on null events that are rendered nonnull as a result of the discovery of links between acts and consequences that the decision maker thought impossible. Because our concrete example relies on the analytical framework exposed in the next section and the results in Section 3.3, we relegate it to a later section. Invoking the analytical framework of Anscombe and Aumann (1963), we demonstrate that our main results in Karni and Vierø (2013), namely, (modified) representations of preferences and corresponding rules for updating beliefs over expanding state spaces that constitute “reverse Bayesianism,” hold when preferences are probabilistically sophisticated. We accomplish this by replacing, at each level of awareness, the axioms of expected utility theory with the axioms of Machina and Schmeidler (1995), and introduce new axioms to connect the preferences across distinct levels of awareness. Unlike the axioms of Machina and Schmeidler which are well known, the “connecting” axioms are new and merit brief description. The first axiom, invariant risk preferences, introduced in Karni and Vierø (2013), asserts that growing awareness does not alter the decision maker’s risk attitudes. The formal expression of the other “connecting” axioms varies with the context. In essence, however, they all assert that the ranking of risky vs. uncertain prospects, conditional on a feasible state space, remains intact when the state space expands. This is in contrast to the corresponding axioms in Karni and Vierø (2013) which require that the entire conditional preference relation remains unchanged. The results of this work depict the evolution of a decision maker’s beliefs in the wake of discoveries of new consequences and acts. In a nutshell, the results assert that, as his state space expands, a probabilistically sophisticated decision maker updates his beliefs in a way that preserves the likelihood ratios of events in the original state space. In addition, we show how a probabilistically sophisticated decision maker updates his beliefs when he arrives at new understanding of the links between acts and consequences. Specifically, when he discovers that links that he believed possible are in fact impossible, he updates his beliefs according to Bayes’ rule. When he discovers that links between feasible acts and consequences that he believed impossible are, in fact, possible, he updates the zero probability events using a formula that is analogous to Bayes’ rule and is best described as a reverse Bayesian updating rule. Since the analytical framework and the related literature were discussed in Karni and Vierø (2013), in what follows, we review briefly those aspects of the model necessary to make the exposition self-contained, underscoring instead the adjustment necessary for the transition from expected utility to probabilistically sophisticated choice. This is done in the next section. In Section 3 we expose the representation theorems and analyze the evolution of beliefs in the wake of discovery of new consequences, acts and links between them. Concluding remarks appear in Section 4. The proofs are collected in Section 5.",21
50.0,3.0,Journal of Risk and Uncertainty,29 July 2015,https://link.springer.com/article/10.1007/s11166-015-9220-9,A neuroimaging study of preference for strategic uncertainty,June 2015,Robin Chark,Soo Hong Chew,,,,Unknown,Mix,,
50.0,3.0,Journal of Risk and Uncertainty,16 July 2015,https://link.springer.com/article/10.1007/s11166-015-9215-6,Risk taking and risk sharing: Does responsibility matter?,June 2015,Elena Cettolin,Franziska Tausch,,Female,Female,Unknown,Female,"The fundamental premise for the support of safety nets, such as social security systems and private insurance, is that individuals are willing to share risk with others, thereby accepting the resulting redistribution of income. The decision to share risk may be backed by both insurance and redistribution motives. The first has a selfish nature, as it allows risk averse individuals to reduce their risk exposure. The second is driven by a preference for equality, as whenever risk is shared those who are lucky support the more unlucky individuals in society. The more risk is shared the more income inequalities are reduced ex-post. For a long time, the idea of tailoring insurance rates to risk types has been debated in public.Footnote 1 For example, proposals to charge higher health insurance premiums to smokers and obese people have been advanced, with the motivation that a high proportion of health care costs can be directly attributed to patients’ bad habits (see Cawley and Ruhm 2011 and Thomson Reuters 2011). In light of this evidence, we hypothesize that in the absence of responsibility attributions for risk exposure, redistribution motives are stronger and the willingness to share risk higher, as compared to when individuals can influence the risk they face. We test this conjecture using a controlled laboratory experiment, focusing on endogenous and exogenous differences in risk exposure. Our set-up allows studying how the support for risk sharing depends on individuals’ risk preferences, their own risk exposure, and their sharing partner’s risk exposure. Empirical research on risk sharing has identified a number of factors that affect individuals’ propensity to share risk, e.g. group size (Chaudhuri et al. 2010), group selection and commitment (Barr and Genicot 2008), risk preferences and social networks (Attanasio et al. 2012), one’s own and others’ exogenous risk profiles (Tausch et al. 2014) and reciprocity in repeated interactions (Charness and Genicot 2009). However, to the best of our knowledge, we are the first to investigate how risk sharing depends on whether individuals perceive themselves and others to be responsible for the extent to which they are exposed to risk. Our results help in understanding whether perceived choice responsibility is a crucial variable influencing the support of modern safety nets. The experiment consists of two treatments. In the Exogenous Risks (EXO) treatment subjects cannot influence the extent to which they are exposed to risk, while in the Endogenous Risks (ENDO) treatment subjects can choose their risk exposure. In the first part of the ENDO (EXO) treatment subjects choose (are assigned) one of two risky lotteries. Both lottery options have the same expected value but differ in their variance. In the second part of both treatments, subjects are paired and one subject in each pair is randomly selected to choose a risk sharing level. Importantly, the risk sharing decision is made ex-ante, that is before the lotteries’ outcomes are determined. The risk sharing level indicates the percentage amount that will be subtracted from the eventual outcomes and then equally redistributed in the pair at the end of the experiment. We implement the strategy method, which means that participants are asked to choose a risk sharing level both for the case that their partner faces the same risk exposure as themselves, and for the case that risk exposures differ. In the last part of the experiment we use a series of incentivized lottery choices to elicit participants’ risk preferences. Our main result is that when risk exposure is a choice (ENDO) average risk sharing among low risk taking individuals is higher when their partner is exposed to low risk as compared to high risk, while no such difference exists when risks are exogenously assigned (EXO). When further differentiating individuals based on their general risk preferences, we find that the result holds for risk averse individuals that choose a low risk exposure, and also for risk seeking individuals that choose a high risk exposure. Our results are important for understanding how responsibility attributions affect risk sharing in settings where risk exposure is perceived as a choice as opposed to settings where risk exposure is perceived as uncontrollable. Since in our experiment high and low risk exposure leads to the same outcome in expectation, our results can be considered a lower bound for the role of responsibility attributions. Their role may be more significant when the high risk has a lower expected value than the safer option. The research in this paper is related to some experimental studies that investigate the support for ex-post income redistribution in contexts where individuals’ outcomes are the product of risky decisions. In Cappelen et al. (2013) participants make choices between a risky lottery and a safe alternative and after observing the eventual outcomes, they are asked how much they want to redistribute to another randomly matched participant. The authors find that individuals who avoid risk do not redistribute much in favor of unlucky risk takers, while the willingness to reduce inequalities is higher between lucky and unlucky risk takers. Thral and Rademacher (2009) implement the solidarity game of Selten and Ockenfels (1998) and compare it to a treatment where individuals choose between a safe payment and a risky lottery. The authors show that individuals that choose the safe payment are less willing to reduce inequalities when matched with individuals that choose the lottery and become needy, as compared to individuals that become needy by pure chance. To summarize, it seems that risk taking is negatively perceived by individuals that avoid risk, and thus reduces their willingness to equalize earnings ex-post. In the cited literature, redistribution decisions are made at a point when risk is resolved and individuals’ outcomes are thus known. Naturally, however, decisions to endorse a given redistributive system or policy have consequences that affect future time periods, for which individuals’ outcomes are yet uncertain. Our experiment allows testing whether responsibility for risky choices matters when individuals do not know how risk will eventually materialize. Importantly, this implies that individuals face uncertainty about whether risk sharing will be profitable for them or not. Furthermore, unlike in previous experiments, individuals cannot entirely eliminate their risk exposure but—like in reality—only influence the degree of risk exposure with their choices. Our paper is also related to recent studies showing that income inequalities are more acceptable when they can be traced back to factors within people’s control. Surveys, as well as experiments, reveal that support for redistribution is higher among people that think that wealth results from unjust motives, like luck or immoral behavior, as opposed to hard work, effort and skills (Alesina and Glaeser 2004; Alesina and La Ferrara 2005; Fong 2011; Durante and Putterman 2009; Krawczyk 2010). It is not a trivial question whether responsibility attributions play a key role also in a risk sharing context, as it differs substantially from a context in which individuals decide about how to redistribute income. Lastly, we contribute to a recent line of research that analyzes insurance choices from a behavioral perspective (for an overview see Richter et al. 2014). Friedl et al. (2014), for example, investigate how insurance demand is affected by social comparisons and in particular how it depends on whether risks are correlated or not. Social comparisons are also the main focus of Rohde and Rohde (2011) and Linde and Sonnemans (2012) who study how decision making under risk is affected by observing the payoffs or the risk exposure of others. In contrast to the cited studies, the choices in our set-up affect both the decision maker’s and the partner’s income. The reminder of the paper is organized as follows. Section 2 describes the experimental design. Section 3 summarizes theoretical predictions and hypotheses. Results are presented in Section 4. In Section 5 we discuss the results and conclude.",26
50.0,3.0,Journal of Risk and Uncertainty,24 July 2015,https://link.springer.com/article/10.1007/s11166-015-9218-3,The effect of ambiguity on risk management choices: An experimental study,June 2015,Vickie Bajtelsmit,Jennifer C. Coats,Paul Thistle,Female,Female,Male,Mix,,
51.0,1.0,Journal of Risk and Uncertainty,27 August 2015,https://link.springer.com/article/10.1007/s11166-015-9221-8,Valuing gains in life expectancy: Clarifying some ambiguities,August 2015,Michael Jones-Lee,Susan Chilton,Jytte Seested Nielsen,Male,Female,Female,Mix,,
51.0,1.0,Journal of Risk and Uncertainty,04 September 2015,https://link.springer.com/article/10.1007/s11166-015-9222-7,Saving lives with stem cell transplants,August 2015,Damien Sheehan-Connor,Theodore C.  Bergstrom,Rodney J. Garratt,Male,Male,Male,Male,"For patients suffering from leukemia and other blood diseases, a stem cell transplant is often the best available, and life-saving, treatment. Stem cell transplants are likely to succeed only if the donor and recipient have sufficiently similar immunity types.Footnote 1 Because of the great diversity of human immunity types, finding a match can be difficult. The probability that two randomly selected persons of European extraction are of matching type is less than one in 10,000, while about 20 percent of these individuals are of types shared by less than one person in a million.Footnote 2
 Current medical technology allows stem cells to be made available by two alternative means. Stem cells may be collected from adult volunteers at the time when they are needed, or they may be obtained from umbilical cord blood collected by hospitals from newborns. These two methods are supported by two different types of “inventory.” For adult donors, a registry is maintained of potential donors. Volunteers are asked to submit saliva samples, which are DNA-tested to determine their immunity type. Volunteers agree that if, at some time in the future, they are found to be the best available match for some patient, they will consent to make a donation. In this case, all that the registry needs to store is the volunteer’s contact information and immunity type. The collection of cord blood requires storage of actual physical material. With the consent of parents, hospitals collect umbilical cord blood from newborns. This material is kept under refrigeration in anticipation that at some future time it may be the best available match for some patient in need. In the United States, the largest adult donor registry and the largest cord blood bank are both maintained by the National Marrow Donor Program (NMDP), a non-profit organization funded partially by the U.S. Department of Health and Human Services. The NMDP donor registry includes more than 11.2 million registrants and its cord blood bank has approximately 190,000 units (Health Resources and Services Administration 2014b). Many other countries also have large donor registries. Currently there are more than 23 million registrants and 580,000 cord blood units maintained by registries around the world (Bone Marrow Donors Worldwide 2014). Both the donor registry and the cord blood bank can be viewed as public goods whose benefits take the form of small increments in survival probability for most members of the population. No one knows in advance whether he or she will contract leukemia, and almost no one knows how common or rare their immunity type is. The larger the two inventories, the greater the chance that someone will have an available match if it is needed, and hence the larger is the survival probability of each population member. This is exactly the kind of environment for which the estimated value of statistical life (VSL) is an appropriate tool for measuring the money value of benefits in a benefit-cost analysis. Previous studies have explored the benefits and costs of maintaining adult registries and cord blood banks. In this paper, we introduce some innovations that permit a much more thorough analysis of the economics of stem cell donations. As far as we know, this paper is the first benefit cost analysis that considers the adult registry and the cord blood bank as a simultaneous allocation problem, taking account of substitutability between these resources. It is also the first to account for international exchanges of stem cell material and it is the first benefit-cost study to use the most recently available and highly detailed estimates of the frequencies of immunity type data, which were published by Gragert et al. (2013).Footnote 3
 The benefits of registries and banks are realized when a patient in need of a transplant finds a match for her immunity type. The probability that such a match can be found depends upon the distribution of immunity types, which varies by race and ethnicity. Because the type distributions are extremely diffuse, they cannot be determined by simply sampling individuals from the population, but methods have been developed for estimating the prevalence of even very rare types (Mori et al. 1997; Kollman et al. 2007). Evaluating the marginal benefits of expanding registries and cord blood banks requires determining how much the probability of finding a match will increase with further additions of registrants and cord blood units. In addition, matching is not all-or-nothing. There are degrees of match and transplants of imperfectly matched patients have become increasingly common. Registry and bank expansion will lead to higher quality matches as well as a higher quantity of them. Existing studies of the economic benefits of stem cell transplantation (Kollman et al. 2004; Howard et al. 2008; Bergstrom et al. 2009, 2012) consider United States registries and patients in isolation, ignoring exports and imports of stem cell material. Given the emergence of international stem cell sharing, this is arguably no longer appropriate. An organization called Bone Marrow Donors Worldwide (BMDW) maintains a clearinghouse of registry data that facilitates access to the pools of potential donors and cord blood units in many nations. When a matching donor cannot be found in a patient’s home country, it is now common practice to search for a donor internationally. In 2009, approximately 45% of all adult stem cell transplants and 33% of all cord blood transplants involved donors and recipients living in separate countries. In the United States, about 44% of all stem cell transplants come from foreign donor registries (including those maintained by the NMDP) and about 31% of all stem cell donations from U.S. residents go to foreign patients (World Marrow Donor Association 2009). Here we analyze benefits and costs from two perspectives that take into account international exchanges through the world registry. We compare the costs of adding new registrants to the world registry with the expected benefits to U.S. citizens and with expected benefits to the world population. Earlier studies indicate that the marginal social benefits of additions to either the adult registry or to the cord blood bank exceed marginal costs (Bergstrom et al. 2009; Howard et al. 2008). Since these studies were published, the medical profession has concluded that matching at a more finely partitioned level is advantageous, and a new data source that presents estimates of the distribution according to this more detailed partition has become available (Gragert et al. 2013). Our benefit cost analysis is based on this new data source. Several technical innovations were required to extend our analysis to incorporate the more detailed immunity type data and to calculate the probability of various partial matches. At this level of detail, the number of possible types to be considered is in the hundreds of millions and the number of acceptable partial matches is also enlarged.Footnote 4
 A recent paper by Gragert et al. (2014) also uses the more detailed immunity type data and employs methods conceptually similar to ours to estimate probabilities of finding a matching donor from the US adult registry and/or the US cord blood bank. In addition to estimating match probabilities, our paper presents economic benefit cost analyses of the adult stem cell registry and of the cord blood registry and also takes into account the availability of the world registry. To do so, we need to estimate not only match probabilities, but also the marginal effects of small changes in the size of either registry. In addition to calculating marginal effects, we simultaneously estimate optimal sizes for the adult registry and the cord blood bank. Since the adult registry and the cord blood bank are, in part, substitutes, it is important to consider the possibility that even if marginal benefits exceed marginal costs of increasing the size of either resource, while holding the other constant, it might be advantageous to increase the size of one while decreasing that of the other. Our method of direct calculation of match probabilities contingent on the size and racial composition of the adult registry and the cord blood bank allows us to estimate simultaneously the optimal size and composition of the adult registry and of the cord blood bank. Our estimates, which are displayed in Tables 13 and 14, indicate that marginal benefits exceed marginal costs of expansion both for adult registries and for cord blood banks. However, the ratio of benefits to costs is higher for the adult registries than for the cord banks. Our calculations of optimal registry sizes, shown in Tables 15 and 16, indicate that optimal adult registries would be at least twice as large as current registries, while the optimal cord blood registries, with the exception of that for blacks, would not be much larger than their current size.
 Section 2 summarizes aspects of medical technology and genetics that are central to understanding the purpose of donor registries and cord blood banks. Section 3 presents estimates of the size and racial composition of the adult donor registries and of the cord blood banks for the U.S. and for the world as a whole. In this section, we discuss the way in which physicians prioritize imperfect matches, we calculate the probability that a person of specified race will find a source of stem cells at each possible level of match quality, and we assess our method by comparing our calculated results to those obtained in previous work. In Section 4 we estimate the expected number of lives saved in the U.S. and in the entire world by increases in the size of the adult registry and of the cord blood bank. To do so, we estimate the annual number of patients seeking transplants. Using this information and the results on matching probabilities from Section 3, we estimate the expected number of additional stem cell transplants at each level of match quality that result from an increment in the size of the adult registry and of the cord blood bank. We then use medical estimates of survival rates of patients who obtain transplants at each level of match quality to estimate the numbers of lives saved. In Section 5.1 we show how to use existing estimates of the value of a statistical life to evaluate the increases in survival probability that result from adding registrants and cord blood units and we also supply cost estimates of adding to the adult registry and to the cord blood bank. Section 6 compares the costs with the benefits to U.S. citizens and to the world population of expanding the adult registries and the cord blood banks. In this section, we also conduct a sensitivity analysis of the results to alternative assumptions concerning the value of statistical life. Section 7 presents estimates of optimal registries of each type and compares these estimates to the existing registries. Finally, Section 8 discusses policy implications and concludes.",4
51.0,1.0,Journal of Risk and Uncertainty,14 August 2015,https://link.springer.com/article/10.1007/s11166-015-9219-2,The value of a statistical life for transportation regulations: A test of the benefits transfer methodology,August 2015,W. Kip Viscusi,Elissa Philip Gentry,,Unknown,Female,Unknown,Female,"To explore the willingness to accept different types of risk, we use a model that differentiates between types of risk. The standard semi-logarithmic hedonic wage model takes the following form: for worker i in industry j and occupation k, nonfatal risk level for the worker’s industry j, and job fatality rate for the worker’s industry j and occupation k. This formulation focuses on a general fatality rate including all types of death. In the estimates below, we will often break fatality rates into two mutually exclusive groups, which we will denote by Transport Risk and Non-Transport Risk. The division of fatality rates will be either by the accident event (i.e., transportation events versus all other events) or source (i.e., vehicle sources versus all other sources). The estimating equation then becomes We test for the benefits transfer assumption by testing whether γ
3 = γ
4. The vector X includes worker-specific characteristics and job-specific characteristics to control for compensation unrelated to risk borne. It also includes a series of dummy variables for each state, thus controlling for state differences in workers’ compensation programs and local labor market conditions. The nonfatal injury rate is also included in order to control for compensation for risks that are sufficiently severe to result in at least one lost day away from work but not a fatality. Using Eq. 1, we provide an example of the procedure to calculate VSL. We construct the VSL, or \( \frac{\partial wage}{\partial risk} \), for the fatality rate by based on a 2,000 hour work year and risks levels per 100,000 workers.",29
51.0,1.0,Journal of Risk and Uncertainty,31 July 2015,https://link.springer.com/article/10.1007/s11166-015-9217-4,Preferences for life-expectancy gains: Sooner or later?,August 2015,James K. Hammitt,Tuba Tunçel,,Male,Female,Unknown,Mix,,
51.0,2.0,Journal of Risk and Uncertainty,05 November 2015,https://link.springer.com/article/10.1007/s11166-015-9224-5,Managing social risks – tradeoffs between risks and inequalities,October 2015,Ingrid M. T. Rohde,Kirsten I. M. Rohde,,Female,Female,Unknown,Female,"Societies are increasingly concerned with public risks such as health epidemics, acts of terrorism, and financial crises (Quiggin 2007). One reason for this increased preoccupation is that these risks are likely to affect many members of society at the same time. Public risks entail not only individual risks for each member of society, but also risks concerning the inequalities between members of society. It is therefore important to evaluate the allocation of such risks not only from an individual, but also from a broader societal perspective. Two essential dimensions of public risks are the inequality concerning the distribution of risks over various groups and members of society and the level of risk faced by individuals and by the society as a whole. For policymakers who design policies to cope with public risks, it is crucial to understand what role these two dimensions play in people’s evaluations of such risks. This paper studies people’s preferences concerning different types of allocations of risks over groups of people and disentangles these two essential dimensions. We do so in a laboratory experiment with real incentives. To rule out selfish and strategic motives, we study choices between allocations of risk that cannot affect the payoffs or risks faced by the decision maker himself. Thus, we consider decision makers who can be viewed as social planners, or impartial spectators (Smith 1759). As Konow (2009) states, studying preferences of impartial spectators is particularly relevant for social choice theory, welfare analysis, and public policy, as such preferences are characterized by unbiasedness. The perspective of an impartial spectator is also relevant in other situations where people make decisions on behalf of others. One can think of parents deciding on behalf of their kids, medical doctors deciding on behalf of their patients, or managers deciding on behalf of their employees. This study is complementary to the studies on interactions between social preferences and risks where the decision maker is affected by his decisions. Examples of such studies are the ones analyzing peer effects on risk taking and risk sharing (Rohde and Rohde2011; Linde and Sonnemans2012; Lahno and Serra-Garcia2015; Cettolin and Tausch2015) and the ones on risky dictator games (e.g. Brock et al.2013; Krawczyk and Le Lec2010) and strategic games like in Bolton et al. (2005). The decision situations we consider can be viewed as a special class of risky dictator games with 11 players where the decision maker can choose between two distributions of risks that do not differ in his own payoff. Several other experimental studies with real incentives took a similar approach in riskfree settings in order to focus on pure distributional preferences and to exclude selfish and strategic concerns (Charness and Rabin2002; Engelmann and Strobel2004; Cappelen et al.2013). When evaluating allocations of risk across society, two notions of inequality are critical (Diamond1967; Harsanyi1955): ex-ante inequality, which concerns the procedure that generates the allocations, and ex-post inequality, which concerns the eventual allocation of outcomes (Fleurbaey2010; Sarin1985; Trautmann2009). The literature on public risk (Fishburn and Sarin1991; Keeney1980a, (Keeney 1980b); Keeney and Winkler1985; Sarin1985; Gajdos et al.2010) indicates that, next to inequalities and individual risks, the possible aggregate payoffs of the group or society also play a role in decisions to allocate risks over society. Therefore, we consider two levels of risk: individual risk and collective risk. Individual risk refers to the dispersion of the possible outcomes for one individual, irrespective of the outcomes of others. Collective risk refers to the dispersion of the total sum of outcomes. We will consider allocations of risk which differ only in terms of these inequalities and risks. The relevance and implications of our four inequality and risk concepts can be illustrated with the following example. Consider a group of individuals and the following two possible allocations of risk. In allocation A each individual receives an independent lottery yielding €20 with 50% probability and nothing otherwise. In allocation B 50% of the individuals receive €20 and the others nothing, where the 50% fortunate ones are predetermined by a certain characteristic of the individuals, such as age. Which allocation would be considered to be socially superior? Should we give everyone an equal opportunity or should we favor particular groups in society? Allocation A, which is in line with the notion of equality of opportunity, could be regarded as superior as all individuals face equal chances. From the perspective of solidarity, though, one might argue that it is favorable to support the weakest in society such as children and the elderly, which will lead to predetermined allocations, as is the case in allocation B. This conflict between equality of opportunity and solidarity is present in many real life situations such as the allocation of donor organs, student placement procedures, the allocation of power plants, and the design of rescue plans in case of calamities. Now suppose that allocation B is replaced by allocation C, where with a 50% probability all individuals receive €20 and otherwise all individuals receive nothing. This allocation C guarantees that everyone receives the same outcome but does not take away the uncertainty whether this outcome will be high or not. Whether or not a person finds allocations A and C different from each other depends partly on the order in which he processes risks and inequalities. A person who first processes risk and then inequality may argue that in both allocation procedures all individuals get the same lottery and thereby have the same chances, i.e. equal opportunities, so that there is no inequality. This person will find both procedures equivalent. A person who first processes inequalities and then risk may argue that in allocation C there will always be equality after resolution of the uncertainty, whereas in allocation A it is very likely that there will be inequality after resolution of uncertainty. This person will not find the two procedures equivalent. Thus, the order in which the two types of dispersion, inequality and risk, are processed affects preferences and, thereby, also sheds light on people’s concerns for procedural fairness. The aim of this paper is twofold. The first aim is to analyze people’s preferences over allocations of risks over groups of people. The second aim is to disentangle these preferences into concerns for risk and inequality. During the experiment, subjects are placed in the role of a social planner and choose between different types of allocations of risks over 10 other participants. The allocations differ only in terms of dispersion, i.e. they differ only in terms of inequality and risk. If subjects care only about the expected payoffs of individuals and the expected aggregate payoff in the group, they will be indifferent between these allocations. Yet, if they also care about risk and inequality, they will not be indifferent. Though many studies have analyzed people’s concerns for risk, and many have analyzed people’s concerns for inequality, only very few have studied how concerns for risk and inequality are integrated (Bolton et al.2005; Kroll and Davidovitz2003; Keller and Sarin1988; Bian and Keller1999; Loomes1982). This paper is the first to experimentally, and by using real incentives, analyze people’s preferences over public risks which affect other people. Our experiment is inspired by a previous study (Rohde and Rohde 2011) which found that subjects had a strong preference for an allocation rule which gave every individual the same independent risk, while subjects were reluctant to implement allocation rules which favored a randomly selected, predetermined number of individuals. The experiment in the current paper was designed to provide insights into the reasons for this preference. The results of our experiment are striking. We observe a clear pattern in individuals’ preferences over different allocation procedures. Subjects are not indifferent between these procedures and take risks and inequalities into account. In particular, our subjects cannot be viewed as social planners who maximize utilitarian social welfare functions of selfish individual expected utilities. Moreover, we find that our subjects are averse towards ex ante inequality and individual risk, while they seek ex post inequality and collective risk. Though surprising at first sight, we will show that these results do not contradict the existing literature.",15
51.0,2.0,Journal of Risk and Uncertainty,05 October 2015,https://link.springer.com/article/10.1007/s11166-015-9223-6,Responsibility effects in decision making under risk,October 2015,Julius Pahlke,Sebastian Strasser,Ferdinand M. Vieider,Male,Male,Male,Male,"Economic situations in which an agent is called on to take decisions affecting somebody else’s financial payoffs as well as her own constitute a common class of phenomena. For instance, they represent situations in which a decision maker’s choices affect not only her own outcomes, but those of her family as well. Another common instance of such decision problems consists of financial agency contracts in which the incentive structure of the agent coincides with the one of the principal. An example may be executives who are compensated through company shares, or a stock broker whose payoffs are determined by the outcomes of the investments she undertakes. Nevertheless, such different situations have generally been modeled with a single objective function treating all decisions like individual decisions. There is an extensive literature on individual decision making under risk and uncertainty (Abdellaoui et al. 2011; Dohmen et al. 2011; Maafi 2011; Viscusi and Huber 2012), as well as a substantial literature on risk attitude in agency problems and how to influence it through performance-contingent pay (Wiseman and Gomez-Mejia 1998). There is, however, much less evidence on decisions under responsibility. To the extent that decisions under responsibility may differ from decisions commonly found in the individual decision making literature, findings from the latter will only constitute an imperfect predictor of attitudes under responsibility. In this paper, we therefore systematically explore the difference in risk attitudes between situations of decision-making for oneself and situations of responsibility, i.e., situations in which the decision maker decides for others as well as herself. Several recent papers touch upon the issue of responsibility in risky decisions. Bolton and Ockenfels (2010) report results of decisions between payoff pairs in a dyad under payoff equality, but do not find significant differences between individual decisions and decisions under responsibility. Reynolds et al. (2009) found risk aversion to increase relative to an individual benchmark when subjects were deciding only for somebody else, with no consequences for their own payoffs. Chakravarty et al. (2011) found risk aversion to decrease under conditions of responsibility in a similar setup. Humphrey and Renner (2011) found no effect of responsibility on risky choices. In a somewhat related study from the game-theoretic literature, Charness and Jackson (2009) found that in a stag hunt game the efficient equilibrium obtained less frequently under responsibility for someone else than in an individual baseline, giving an indication of increased risk aversion under responsibility. The literature discussed above reaches widely different conclusions, ranging from increased risk aversion to increased risk seeking or null results. In two experiments, we explore risky decisions for situations in which an anonymous other (the recipient) obtains the same payoff as the decision maker, and compare such decisions to purely individual decisions. We are the first to explore such decisions systematically for risky choices in the gain domain, the loss domain, and the mixed domain, as well as for different probability levels. This allows us to adopt prospect theory (Kahneman and Tversky 1979)—the prevalent descriptive theory of choice under risk and uncertainty today (Starmer 1999, 2000; Wakker 2010)—as a descriptive theory of choice and to systematically explore potential differences along the relevant dimensions in the light of that theory. We find that in the gain domain, being responsible for others as well as oneself does indeed increase risk aversion for moderate probabilities, thus showing that Bolton and Ockenfels’ (2010) intuition of responsibility inducing a “cautious shift” was correct. In addition, we show that for loss prospects, subjects are unaffected by responsibility and even become slightly more risk seeking when responsible for others. Loss aversion, on the other hand, being already strong in individual decisions, does not seem to increase when subjects are responsible for others.Footnote 1
 In a second experiment aimed at exploring social norms on risk taking in the gain domain in more detail, we replicate the finding that risk aversion increases under responsibility for large probabilities. When choices regard small probability prospects, however, we find increased risk seeking under conditions of responsibility. Overall, our results can be organized by an accentuation of the fourfold pattern of risk attitudes typically found in individual decision making when subjects are responsible, with subjects becoming more risk averse for moderate to large probability gains and small probability losses, as well as more risk seeking for moderate to large probability losses and small probability gains when responsible for others. The paper proceeds as follows. Section 2 describes the first experiment, with Section 2.1 describing the methodology and Section 2.2 presenting the results; Section 2.3 discusses the result of experiment I and derives hypotheses for experiment II. Section 3 introduces experiment II, with Section 3.1 describing the methodology and Section 3.2 presenting the results. Section 3.3 discusses the results of experiment II as well as the overall results. Section 4 concludes this paper.",80
51.0,2.0,Journal of Risk and Uncertainty,13 November 2015,https://link.springer.com/article/10.1007/s11166-015-9226-3,Heterogeneity in preferences towards complexity,October 2015,Peter G. Moffatt,Stefania Sitzia,Daniel John Zizzo,Male,Female,Male,Mix,,
51.0,2.0,Journal of Risk and Uncertainty,21 November 2015,https://link.springer.com/article/10.1007/s11166-015-9225-4,Demand for fixed-price multi-year contracts: Experimental evidence from insurance decisions,October 2015,Howard Kunreuther,Erwann Michel-Kerjan,,Male,Male,Unknown,Male,"Researchers interested in risk and uncertainty have been aware of the limited interest by consumers in purchasing insurance and investing in protection against extreme events. While the number and cost of natural disasters have been increasing steadily, studies reveal that many homeowners do not have adequate coverage against floods, hurricanes and earthquakes (Kunreuther et al. 1978; Michel-Kerjan 2010; Kousky and Cooke 2012; Botzen and van den Bergh 2012; Friedl et al. 2014). To illustrate this point in the context of hurricane losses in the United States, the U.S. Department of Housing and Urban Development revealed that 41% of homes damaged by Hurricanes Katrina, Rita and Wilma in 2005 were uninsured or underinsured. Of the 60,196 owner-occupied homes with severe wind damage from these hurricanes, 23,000 did not have insurance against wind loss (Government Accountability Office 2007). Kriesel and Landry (2004) and Dixon et al. (2006) found that only about half of the homes in high-risk areas had flood insurance. Along the entire New York coast affected by Hurricane Sandy in 2012, the City of New York found that 80% of residents in the inundated area did not have flood insurance, despite that coverage being subsidized for many (New York City 2013). We discuss research in the economics and behavioral literature that provides explanations for this behavior in the body of the paper. One contractual arrangement that has not been given much attention is the marketing of property insurance as an annual policy in most countries around the world.Footnote 1 Following natural disasters, such as major hurricanes, floods and earthquakes, premiums often increase significantly and policies may be canceled by insurers. After the 2004 and 2005 hurricane seasons many insurers did not renew coverage for a number of homeowners in coastal regions of the United States (Klein 2007). While most of these residents were able to find coverage with other insurers, their new contracts typically were more expensive and had a higher deductible and a lower maximum compensation limit than their previous policies (Vitelo 2007). The average homeowner’s premium in the state of Florida increased from $723 in 2002 to $1465 in 2007. In coastal areas, some insurers were permitted by regulators to triple or even quadruple their premiums for some homeowners after 2005 (Kunreuther and Michel-Kerjan 2011).Footnote 2 Those who could not obtain coverage from the private market had to purchase coverage from state insurance pools, which play the role of insurers of last resort (Grace and Klein 2009). Residents in hazard-prone areas often question why insurance companies, viewed as experts in risk assessment, increase their rates so dramatically following a disaster. Those in harm’s way believe pre-disaster premiums reflected the likelihood of claims arising from extreme events, so premiums should not change after an event occurs. Many residents are also concerned as to whether coverage will be available against these risks from their insurer in the future, and for good reason. Following Hurricane Andrew, which caused severe damage to property in Miami-Dade County, Florida in 1992, insurers threatened to discontinue homeowners’ policies in Florida. To prevent this from happening, the state legislature enacted a bill that prohibited individual insurance firms from canceling more than 10% of their homeowners’ policies in any Florida county in any one year, and more than 5% of their policies statewide between 1993 and 1996 (Lecomte and Gahagan 1998). To address the above concerns by property owners, we introduce a fixed-price multi-year insurance (MYI) contract and examine its demand relative to standard annual contracts in a field experiment. Multi-year insurance offers several advantages over single-year policies. For the insured, who are the focus of this paper, MYI would offer stable premiums over time rather than having them significantly increase after a disaster, thus providing homeowners with more certainty regarding their budgetary planning process.Footnote 3 Multi-year contracts would also have important societal benefits by limiting the number of residents in hazard prone areas who cancel their insurance after only a short period of time, behavior that has been documented empirically (Michel-Kerjan et al. 2012). When disaster victims suffer losses, they often request federal relief because they are unprotected. The political debate around the $50 billion post-Hurricane Sandy relief in 2012 demonstrates the importance of developing strategies for increasing personal responsibility by those in harm’s way. MYI has the potential of doing this. There is precedence for multi-year contracts in the life insurance arena. Term-life policies are typically offered with premiums “locked in” for 5 to 10 years; buyers can choose whether they want to pay extra for such guarantees over annual contracts knowing that they may drop coverage at any time. Policyholders are then certain what their life insurance premiums will be over the next 5 or 10 years, regardless of what happens to their health or the overall mortality rate of their insurer’s portfolio. Hendel and Lizzeri (2003) examine 150 term-life insurance contracts, some of which have fixed premiums for 5, 10 or 20 years while others are 1-year renewable policies. They show that on average, the extra prepayment of premiums to protect consumers against being reclassified into a higher risk category for a fixed period of time is more costly over the total period of coverage than a series of 1-year term policies that can be renewed but where premiums may fluctuate from year to year. Still, people buy these multi-year fixed-price life insurance policies, indicating that they view the stability of premiums as an important attribute and are willing to pay extra for a guaranteed price. To date there has been no empirical analysis on the demand for multi-year property coverage except for Papon (2008), who undertook a lab experiment with 64 undergraduate students from the University of LeMans in France. Subjects were divided into two groups: one group was given the option of purchasing a single period insurance policy and the other presented with a policy that covered them for four periods in a row. Papon found that the demand for insurance was significantly higher for those who could purchase a multi-period contract relative to those who were offered a single period insurance policy, attributing this effect to the longer period of commitment. This paper builds on this research by developing a multi-period field experiment with single and multi-period insurance contracts that cannot be cancelled. Participants are presented with the following options with large amounts of real money at stake: no insurance, 1-period insurance, and 2-period insurance. Our goal is to determine the extent to which individuals have an interest in purchasing multi-period coverage against losses from a disaster that occur with a relatively low probability and to understand the factors that influence their insurance decisions. While commitment is an important aspect, other elements also play a key role in explaining the significant demand for the fixed-price multi-period contracts, even when they cost more in total than single-period policies priced at an actuarially fair rate. We are also interested in whether risk attitudes and socio-economic characteristics such as age, gender and income impact on the insurance purchase decision for either type of contract. We find that demand for a fixed-price 2-period insurance policy is five times greater than a 1-period contract where the premium can increase in period 2 should a disaster occur in period 1. We also find that individuals are ready to pay more for a 2-period fixed priced contract than for a potentially variable 2-period contract. We call this a premium for price stability. Our findings also reveal that the overall demand for insurance (i.e., the combined demand for 1-period and 2-period contracts) increases when both types of contracts are offered rather than when just 1-period policies are available. Finally, those who are more risk averse are more likely to purchase 2-period rather than 1-period policies. Although we focus on the demand for disaster insurance our findings should have important implications for the design of other multi-year contracts outside of the insurance domain. The paper is organized as follows. Section 1 provides a simplified model for determining the premiums that an insurer would charge if it were offering 1- and/or 2-period contracts in a competitive market with free entry and exit. Section 2 describes the web-based experiment where participants can decide to be uninsured or purchase 1- or 2-period insurance when they face significant losses from a hurricane that occurs with a well-specified probability. The amounts of money at stake were in the hundreds of dollars and the winners were selected randomly on the basis of games from the Pennsylvania Lottery to assure full transparency. Section 3 introduces testable hypotheses that are examined in Section 4 using data from 445 adult individuals who participated in the experiment. Section 5 discusses the implications of these findings for marketing insurance and for designing public policy.",31
51.0,3.0,Journal of Risk and Uncertainty,22 December 2015,https://link.springer.com/article/10.1007/s11166-015-9228-1,Loving the long shot: Risk taking with skewed lotteries,December 2015,Philip J. Grossman,Catherine C. Eckel,,Male,Female,Unknown,Mix,,
51.0,3.0,Journal of Risk and Uncertainty,16 December 2015,https://link.springer.com/article/10.1007/s11166-015-9227-2,Estimating ambiguity preferences and perceptions in multiple prior models: Evidence from the field,December 2015,Stephen G. Dimmock,Roy Kouwenberg,Kim Peijnenburg,Male,Male,,Mix,,
51.0,3.0,Journal of Risk and Uncertainty,17 December 2015,https://link.springer.com/article/10.1007/s11166-015-9229-0,No aspiration to win? An experimental test of the aspiration level model,December 2015,Enrico Diecidue,Moshe Levy,Jeroen van de Ven,Male,Male,Male,Male,"Aspiration levels have been formalized by, amongst others, Diecidue and van de Ven (2008) and Levy and Levy (2009).Footnote 3 These models make two important assumptions. First, decision makers (DMs) are concerned with aspiration levels and, in particular, with the overall probability of meeting an aspiration level. Second, DMs are also sensitive to the level and likelihood of all other outcomes. In these models, DM preferences are expressed as a combination of expected utility and the aspiration level. We denote by P(x
+) (resp. P(x
−)) the overall probability of reaching an outcome strictly above (resp. below) the aspiration level. The valuation of a lottery L with outcomes x

j
 ( j = 1, 2, . . ., n) and probabilities p

j
 is It is straightforward to show that this expression is equivalent to V
AL(L) = ∑
j

p

j

v(x

j
) if we define: v(x

j
) = u(x

j
) + μ
+ for x > x
0; v(x

j
) = v(x

j
) − μ
− for x < x
0; and v(x

j
) = u(x

j
) where u is a continuous value (or utility) function and x
0 is the aspiration level. The result is a value function v that is discontinuous around the aspiration level. Figure 1 illustrates such a value function v (the solid line), when the aspiration level is set at zero. As the graph shows, the value function v jumps at the aspiration level. Example of a value function for the aspiration level (AL) model To derive predictions from the aspiration level model, we assume a value function as in Fig. 1. The value function jumps at the aspiration level. In principle, the theory does not impose any restrictions on the concavity/convexity of the smooth parts of the function. However, a value function as in Fig. 1 is consistent with two major findings from laboratory studies—namely, people exhibit risk-averse behavior in the domain of gains but risk-seeking behavior in the domain of losses. Prospect Theory (PT) (Tversky and Kahneman 1992) accommodates the risk-seeking behavior with a convex value function in the loss domain.Footnote 4 However, the aspiration level model offers a different explanation for the risk-seeking behavior. Faced with the choice between a lottery and a sure negative outcome, a DM may prefer the lottery simply because it offers the only chance of reaching the aspiration level. We can illustrate this in Fig. 1 for a 2-outcome monetary lottery that yields either 0 or −y. The expected utility of this lottery (a point on the dashed line) is always at or above the value function, implying that the lottery is preferred to receiving its expected value for sure. We now derive two predictions from the model that form the basis of our experimental design. Although we derive these predictions using lotteries with only two outcomes, it is straightforward to generalize the predictions to many outcomes. The first prediction is derived from shifting the lotteries by adding some amount of money to all the outcomes. This design is inspired by Payne et al. (1980, 1981) and Lopes and Oden (1999).Footnote 5 Suppose that, as a consequence of the shift, an outcome crosses the aspiration level. In that case, the model predicts (i) a jump in the value function and the valuation of the lottery (the certainty equivalent), (ii) a change in risk attitude. If, for instance, we start with the lottery that gives 0 or –y with equal probabilities, the value function plotted in Fig. 1 predicts risk-seeking behavior. If we then subtract an amount of \( z>0 \) from both outcomes, all outcomes are below the aspiration level. The value of the highest outcome drops (in Fig. 1 from v
1 to v
0) and the person becomes risk-averse. Suppose that people have an aspiration level. Shifting all the outcomes of a lottery by a constant amount results in a jump in the certainty equivalent and a change in risk attitudes if the shift causes an outcome to cross the aspiration level. For the second prediction, consider a mixed gamble with one outcome above and one outcome below the aspiration level. If we assume that the aspiration model is the correct representation of preferences, then by examining a large number of gambles that differ only in the probability of getting the high outcome, and eliciting the certainty equivalent (CE) of each gamble, one can trace the value function in the entire range between the low outcome and the high outcome (where the value of the low and high outcomes can be set arbitrarily).Footnote 6 If an aspiration level exists, this analysis will reveal it as a discontinuity, or “jump”, in the value function. Suppose that people have an aspiration level and consider a lottery with two given outcomes. We can trace the value function by varying the probability of the high outcome. The aspiration level is revealed by a discontinuity in the value function at the aspiration level.",15
51.0,3.0,Journal of Risk and Uncertainty,18 December 2015,https://link.springer.com/article/10.1007/s11166-015-9230-7,Representativeness and managing catastrophe risk,December 2015,Jacqueline Volkman-Wise,,,Female,Unknown,Unknown,Female,"There is a large empirical and theoretical literature on catastrophe insurance that focuses on optimal insurance contracts to manage such unique risks (see for example, Doherty 1997; Jaffee and Russell 1997; Louberge and Schlesinger 2001; Schlesinger 1999). There is varying evidence of how recent disasters affect the future demand for insurance. Catastrophic events have been shown theoretically to lead to increases in insurance demand when there are informational cascades; that is, increased public information about a disaster and word-of-mouth effects can lead to demand overreaction (Seog 2008). Browne and Hoyt (2000) find empirically that higher prices for flood insurance lead to lower demand, which could be consistent with low demand prior to a disaster; but this evidence does not support the purchase of overpriced contracts post-disaster. The authors also find that flood insurance purchases are highly correlated with the level of flood losses in the previous year, which is supportive of the representative heuristic. Ganderton et al. (2003) experimentally observe the demand for insurance against low probability, high loss situations and also find that demand is sensitive to price. Furthermore, they find that individuals do not behave as expected when they are repeatedly exposed to a risk as they exhibit greater sensitivity to the probability of the loss relative to the loss size. Recent experimental work shows that individuals purchase more insurance for low versus high probability events, for a given loss size and loading (Laury et al. 2009), though that study did not control for subjects’ prior loss experience. It has been documented that individuals often miscalculate risks, yet once they are exposed to a risk, they increase their estimates of it recurring. Viscusi and Evans (2006) estimate posterior risk beliefs after subjects receive additional risk information and find evidence consistent with the notion that individuals overweight posterior probabilities. Similarly, Viscusi and Zeckhauser (2006) find that people assess their own risks for natural disasters as below average; those having had a previous experience with a disaster estimate the risk as higher, but they still estimate it as less than what would be expected from rational calculations. Also Zhou-Richter et al. (2010) find that the demand for long term care (LTC) insurance is low due to low risk perceptions, and the more individuals become aware of the risk, the more they buy LTC insurance. Bacon et al. (1989) also find that many people are generally unaware of some risks, such as those for long-term illnesses, which would lead them to underestimate their probability of occurrence. Furthermore, individuals do respond to natural hazards by reducing the demand for housing in hazard prone areas (Sutter and Poitras 2010; Smith et al. 2006) A growing literature examines how differences in preferences might affect insurance decisions. Most previous work which considers how psychology impacts insurance decisions focuses on regret and/or disappointment. Braun and Muermann (2004) show how regret impacts the demand for insurance, Muermann et al. (2006) examine how regret impacts portfolio choice for defined contribution pension plans, and Huang et al. (2008) analyze how regret can impact an equilibrium insurance setting. Similarly, Gollier and Muermann (2010) consider a decision-making model where individuals have beliefs which include ex-ante optimism and ex-post disappointment to explain the preference for low deductibles. Shapira and Venezia (2008) show experimentally that individuals are subject to an anchoring effect and anchor their preferences on the prices associated with full insurance policies. As a result, they undervalue partial insurance policies (policies with deductibles) causing them to prefer full insurance and low deductibles. The closest work to ours is by Viscusi (1989) and Kunreuther and Pauly (2004). Viscusi (1989) develops prospective reference theory which models preferences where subjects place an error on true probabilities given to them. Such preferences lead to an overestimation of low probabilities and underestimation of high probabilities. Viscusi (1989) shows that repetition of exposure to the risk will lower the degree of distortion. This is somewhat similar to the representative heuristic, in that posterior probabilities do influence choice. Yet, prospective reference theory leads to the overestimation of small probability risks which only explains the over-demand for insurance post-disaster. Furthermore if individuals overestimate the probability of a disaster, evidence does not seem to support that repeated exposure to disasters causes individuals to reduce their estimation as prospective reference theory would suggest. In related work by Kunreuther and Pauly (2004), the consumer faces transaction costs to determine insurance premiums, coverage, and the underlying loss probabilities. If such transaction costs are too high, individuals will not buy insurance. The authors postulate that, since information about the pricing of disaster insurance is not transparent and since disaster-type events are rare, search costs to determine their probability are high. Consequently, transaction costs can explain the low demand for insurance for low-probability risks. Yet this model does not explain the overestimation of probabilities post-disaster. In sum, prior work can explain only the low demand for insurance pre-disaster or the over-demand post-disaster whereas we find the representative heuristic can explain the dynamic pattern of both under- and over-insurance observed in the market. In what follows and distinct from the literature described above, we develop a behavioral decision model which formally captures individual error in estimating and/or updating risk probabilities for disaster insurance through the representative heuristic. By allowing miscalculations in Bayes’ Rule through the overweighting of posterior information, we can evaluate the effect of such errors on the demand for catastrophe insurance both before and after a disaster. We show that if individuals are subject to the representative heuristic, they will underinsure prior to a disaster, even if there is subsidization from the government. They will also over-insure post-disaster, even if insurance contracts are unfairly priced.",20
52.0,1.0,Journal of Risk and Uncertainty,19 March 2016,https://link.springer.com/article/10.1007/s11166-016-9234-y,Measuring Loss Aversion under Ambiguity: A Method to Make Prospect Theory Completely Observable,February 2016,Mohammed Abdellaoui,Han Bleichrodt,Dennie van Dolder,Male,,Male,Mix,,
52.0,1.0,Journal of Risk and Uncertainty,29 February 2016,https://link.springer.com/article/10.1007/s11166-016-9231-1,How do risk attitudes affect measured confidence?,February 2016,Zahra Murad,Martin Sefton,Chris Starmer,Female,Male,,Mix,,
52.0,1.0,Journal of Risk and Uncertainty,03 March 2016,https://link.springer.com/article/10.1007/s11166-016-9232-0,Ellsberg paradox: Ambiguity and complexity aversions compared,February 2016,Jaromír Kovářík,Dan Levin,Tao Wang,Male,Male,,Mix,,
52.0,1.0,Journal of Risk and Uncertainty,10 March 2016,https://link.springer.com/article/10.1007/s11166-016-9233-z,Preference reversals: Time and again,February 2016,Carlos Alós-Ferrer,Ðura-Georg Granić,Alexander K. Wagner,Male,Unknown,Male,Male,"The concept of preference is of fundamental importance for decision theory and economic analysis. Yet, preferences are not a primitive but a derived object which structures choices as long as they exhibit some basic consistency, e.g. in the form of the weak axiom of revealed preference. If choices are consistent, a number of elementary predictions can be derived, which form the basis for decision theory, microeconomics, consumer research, and judgment and decision making. One such prediction is that choices should agree with valuations: if a decision maker chooses one option over another, he should value the former more than the latter. This common-sense prediction is at odds with observed decisions under risk. The preference reversal phenomenon, first documented in psychology by Slovic and Lichtenstein (1968) and Lindman (1971), describes a situation in which participants are asked to state monetary valuations for a series of lotteries (usually through minimum selling prices), and separately choose from pairs of those lotteries. The pairs consist of a P-bet, which has a high probability of paying a moderate amount of money, and a $-bet, which has a low probability of paying a high amount of money. A preference reversal occurs if either the P-bet is chosen from a pair in which the $-bet is priced higher or the $-bet is chosen from a pair in which the P-bet is priced higher. The preference reversal phenomenon is characterized by a high rate of reversals of the first type (between 40 and 80% in most experiments), which are called predicted reversals. Reversals of the second type, termed unpredicted, are less frequent (between 5 and 30%). The asymmetry between both types of reversals is especially problematic, for, if reversals were due to e.g. participants’ errors, one should expect similar numbers of both types. In other words, while one could explain away unpredicted reversals as noisy observations, predicted reversals remain a serious challenge to basic economic analysis. It is no surprise that preference reversals have received a great deal of attention in the last half century. After the first replication in economics by Grether and Plott (1979), a large number of experimental and theoretical studies has shown that the phenomenon is extremely stable. It has been replicated using hypothetical and real payments, different payment schemes, and different elicitation methods for lottery prices (for a survey, see e.g. Seidl 2002). Preference reversals of this particular form have been documented beyond lottery choice, e.g. in the field of health utility measurements (Stalmeier et al. 1997; Bleichrodt and Pinto Prades 2009; Oliver 2013). Furthermore, other forms of inconsistencies between different preference elicitation methods have been established in the literature, including reversals between pricing and rating (Schkade and Johnson 1989) as well as discrepancies between certainty and probability equivalents (Hershey and Schoemaker 1985; Johnson and Schkade 1989; Delquié 1993). In addition to their conceptual importance for decision theory, these phenomena are of great relevance for applied economics, since they cast doubts on the validity of e.g. consumer valuations, and, accordingly, on demand estimations and policy decisions based on those valuations. The present research provides new evidence on the determinants of preference reversals by investigating decision times in two separate experiments. This allows us to go beyond existing accounts of behavioral data and investigate the decision processes underlying reversals. Using a well-established, stylized fact on decision times (that decisions take longer when the decision maker is closer to indifference), we argue that the combination of two standard elements of existing accounts of preference reversals is enough to explain the pattern of reversals while delivering testable predictions on decision times. The two determinants are the imprecision of preferences in the evaluation phase (Schmidt and Hey 2004; Butler and Loomes 2007) and the overpricing of $-bets. The latter can be due to anchoring of evaluations on the largest monetary outcomes of a lottery (Tversky et al. 1990), which is itself a consequence of the cardinal/monetary framing of the evaluation phase. An alternative explanation for the overpricing of $-bets is reference dependence (Sugden 2003; Schmidt et al. 2008; Lindsay 2013).Footnote 1
 The main, clear-cut prediction that we obtain for decision times is that choices associated with reversals of either type are slower than the corresponding non-reversal choices. Our first experiment reproduced the standard preference reversal pattern and confirmed this prediction. We employed two different payment methods, the BDM procedure (Becker et al. 1964) and an ordinal payment scheme (Goldstein and Einhorn 1987; Tversky et al. 1990; Cubitt et al. 2004), showing that our findings are robust to changes in the evaluation task. Our first experiment hence provided novel evidence in favor of theories based on preference imprecision and the overpricing of $-bets. In our second experiment, we showed that these two determinants of preference reversals can be disentangled in the laboratory by “shutting down” the overpricing phenomenon. This was accomplished by moving away from cardinal elicitation tasks. Instead, we employed two different ranking methods (plus a control BDM replication), one with a price framing, and one where we carefully removed all references to prices. This experiment helps establish that the imprecision of elicited monetary valuations is an important key for the existence of reversals, while the overpricing phenomenon causes their asymmetry, i.e. the predominance of predicted preference reversals. When the latter is eliminated, predicted reversals become rare, but choices associated with reversals remain significantly slower than corresponding non-reversal choices. This confirms our interpretation because this prediction can be shown to arise exclusively from preference imprecision in the choice phase. Received evidence on preference reversals could potentially be explained by a number of alternative, “as if” models. The analysis of decision times, however, allows us to put different hypotheses on the determinants of preference reversals to a more stringent test than if we relied on behavioral data only, and further enables us (through predictions arising from one determinant only) to disentangle the causes of reversals.Footnote 2
 Our research also delivers additional theoretical and methodological insights. On the basis of our assumptions, we also obtain an additional, surprising prediction, namely that decisions where the riskier $-bet is chosen without giving rise to a reversal should be slower than those non-reversals where the P-bet is chosen. This nontrivial prediction arises as a consequence of the conjunction of imprecise preferences and the overpricing phenomenon, and hence was predicted for (and observed in) the first experiment but not for the second. A further, striking observation was that choices in the treatment with unframed ranking-based evaluations were much faster than those in other treatments, in spite of the fact that choice phases were identical across treatments. This fact has a simple process-based explanation (which we relegate to the discussion of that experiment). Last, our design specifically allowed comparing reversal rates when prices are elicited before the choice phase to reversal rates occurring when prices are elicited after the choice phase. This comparison, which has not been previously made in the literature, was motivated by evidence from psychology (see Section 2.3 below) indicating that choices might sharpen and even modify previously imprecise preferences. In agreement with this observation, we show that ordering effects, although small, are present in the measurement of reversals. The remainder of the paper is organized as follows. In Section 2 we derive our experimental hypotheses from a simple formal model. Sections 3 and 4 describe the first and second experiments and their results, respectively. Section 5 concludes.",36
52.0,2.0,Journal of Risk and Uncertainty,15 June 2016,https://link.springer.com/article/10.1007/s11166-016-9237-8,Group decision rules and group rationality under risk,April 2016,Aurélien Baillon,Han Bleichrodt,Peter P. Wakker,Male,,Male,Mix,,
52.0,2.0,Journal of Risk and Uncertainty,07 June 2016,https://link.springer.com/article/10.1007/s11166-016-9236-9,Strategic self-ignorance,April 2016,Linda Thunström,Jonas Nordström,Klaas van’t Veld,Female,Male,Male,Mix,,
52.0,2.0,Journal of Risk and Uncertainty,26 April 2016,https://link.springer.com/article/10.1007/s11166-016-9235-x,Dread and latency impacts on a VSL for cancer risk reductions,April 2016,Rebecca L. McDonald,Susan M. Chilton,Hugh R. T. Metcalf,Female,Female,Male,Mix,,
52.0,2.0,Journal of Risk and Uncertainty,30 May 2016,https://link.springer.com/article/10.1007/s11166-016-9238-7,Reducing risks in wartime through capital-labor substitution: Evidence from World War II,April 2016,Chris Rohlfs,Ryan Sullivan,Thomas J. Kniesner,,,Male,Mix,,
52.0,3.0,Journal of Risk and Uncertainty,10 September 2016,https://link.springer.com/article/10.1007/s11166-016-9241-z,Is social choice gender-neutral? Reference dependence and sexual selection in decisions toward risk and inequality,June 2016,Steven R. Beckman,Gregory DeAngelo,Ning Wang,Male,Male,,Mix,,
52.0,3.0,Journal of Risk and Uncertainty,10 September 2016,https://link.springer.com/article/10.1007/s11166-016-9240-0,A measurement of decreasing impatience for health and money,June 2016,Han Bleichrodt,Yu Gao,Kirsten I. M. Rohde,,,Female,Mix,,
52.0,3.0,Journal of Risk and Uncertainty,13 September 2016,https://link.springer.com/article/10.1007/s11166-016-9242-y,On the functional form of temporal discounting: An optimized adaptive test,June 2016,Daniel R. Cavagnaro,Gabriel J. Aranovich,Jay I. Myung,Male,Male,Male,Male,"The commonplace and readily understood notion that delaying gratification is unpleasant has become one of the most thoroughly explored constructs in the behavioral sciences. In the scientific literature, the phenomenon is referred to as delay, or temporal, discounting, and is more precisely defined as the amount by which the subjective value of a reward decreases as a function of delay to delivery. Temporal discounting has come to be viewed as indispensable to the analysis of a wide range of important decision making behaviors, from saving behavior to environmental policy (Laibson 1997; Frederick et al. 2002; Dasgupta 2008). The study of temporal discounting has generated promising applications in psychiatry and neuroscience, wherein distinct patterns of discounting behavior have been linked to various types of mental illness, including addiction, gambling, ADHD, and other disorders associated with impaired impulse-control (Koffarnus et al. 2013; Sharp et al. 2012; Reynolds 2006; Story et al. 2014). Studies examining the neural correlates of temporal discounting have led to signifiant advances in our understanding of how decision making is realized in the brain (McClure et al. 2004; McClure et al. 2007; Kable and Glimcher 2007; Rangel et al. 2008). In essence, temporal discounting has been adopted as a behavioral biomarker of impulsivity (Peters and Büchel 2011), to the extent that targeting temporal discounting directly with cognitive training has become a promising intervention for self-control impairment (Bickel et al. 2011). In scientific studies, temporal discounting behavior is described quantitatively with a discounting curve, which ostensibly characterizes the extent of decline in value as a function of delay. Precise estimates of the shape of the discounting curve are obtained by fitting a parametric function to choice data from experiments that require participants to choose between reward options that vary in magnitude and delay to delivery. The resulting estimates are subsequently used for tests of group differences in temporal discounting behavior (Reynolds 2006), for correlation with neural and physiologic data (McClure et al. 2007; Kable and Glimcher 2007; Moore and Cusens 2010; Rangel et al. 2008), for treatment targeting (Bickel et al. 2011), and for making behavioral predictions (Dallery and Raiff 2007; MacKillop and Kahler 2009; Story et al. 2014). The precise shape of the fitted curve depends on which functional form is assumed, and has important predictive implications for unobserved behavior. Therefore, choosing the right function is critical to the success of the wide-ranging applications of the temporal discounting paradigm. Despite decades of investigation, the proper parametric form of the discounting curve remains an active area of research. Bodies of literature have emerged across disciplines, proposing different theoretical constructs to account for various patterns of observed choice behavior (Frederick et al. 2002; Van den Bos and McClure 2013; Doyle 2013). Candidate models vary in complexity and include the Exponential, Hyperbolic, Generalized Hyperbolic (Green and Myerson 2004), Beta-Delta (Laibson 1997), Constant Sensitivity (Ebert and Prelec 2007), Double Exponential (McClure et al. 2007), and several others. Among them, the Hyperbolic has become the dominant model in the literature, due largely to its success in fitting individual data in experiments and the simplicity of its one-parameter form (Van den Bos and McClure 2013). In published reviews of the temporal discounting literature, various explanations for the elusiveness of consensus have been advanced, including context-dependence and inter-individual variability of temporal discounting preferences, as well as the possibility that tasks designed to elicit time discounting preferences disregard the complexity of temporal discounting decisions, which likely involve several component processes in addition to time preference (Frederick et al. 2002; Berns et al. 2007; Scholten and Read 2010; Peters and Büchel 2011; Van den Bos and McClure 2013). Another possible reason why a consensus has yet to develop is that discriminating among functional forms at the individual level presents formidable methodological challenges. Due to practical limitations on the number of questions that can be asked in a single experiment, the questions that are asked must be finely tuned to the task of discriminating among the models and estimating their parameters as precisely as possible. Moreover, due to individual differences, the right questions to ask to best distinguish among models will differ across participants. Ideally, the set of questions in an assessment would be tuned to each individual, rather than a one-size-fits-all approach, yet parameter estimation and model comparison studies in the discounting literature have often employed one-size-fits-all designs, either using well-studied instruments such as the Kirby Monetary Choice Questionnaire (Kirby et al. 1999), or ad hoc designs developed through a mixture of literature search and personal judgment (Madden and Bickel 2010). Additional challenges lie in eliciting temporal discounting preferences and in the statistical analysis of experiment data for fitting and comparing models. By far the most common method involves presenting subjects with a series of binary choices between smaller-sooner and larger-later monetary rewards, which are systematically varied by amount or delay in order to aid in the identification of indifference points, to which statistical techniques such as nonlinear regression are applied for curve fitting. More recently, maximum likelihood estimation (MLE) has become a popular alternative to analyses based on indifference points (McClure et al. 2004; McClure et al. 2007; Pine et al. 2009; Pine et al. 2010). However, most previous model comparison studies (Myerson and Green 1995; Rachlin 2006; Takahashi et al. 2008; McKerchar et al. 2009; Pine et al. 2009; Pine et al. 2010; Peters et al. 2012) have relied on the coefficient of determination (R
2) to determine the goodness-of-fit of the models under consideration, even though this approach can be misleading due to the problem of over-fitting (Pitt and Myung 2002), although several recent studies (Takahashi et al. 2008; Pine et al. 2009; Peters et al. 2012; Abdellaoui et al. 2013) have used the Akaike Information Criterion, or AIC (Akaike 1976), for model comparison, which does account for model complexity.Footnote 1
 The present study demonstrates and implements a Bayesian inference method for discriminating among models of temporal discounting using Adaptive Design Optimization, or ADO (Cavagnaro et al. 2010). ADO integrates likelihood-based data-modeling with adaptive experimental designs to maximize the efficiency and informativeness of an experiment. In an ADO experiment, stimuli are tailored to each participant by updating model and parameter estimates in real time as data are collected, and using the latest estimates to select stimuli that maximize the expected information gain about the models under consideration. The approach has proved to be effective for discriminating among models of memory retention (Cavagnaro et al. 2011) risky choice (Cavagnaro et al. 2013a) and among functional forms of the probability weighting function in Cumulative Prospect Theory (Cavagnaro et al. 2013b). The present work extends the approach to temporal discounting. This work builds on a growing body of research in adaptive designs in experimental economics (e.g, Toubia et al. 2013; Ray et al. 2012; Wang et al. 2010), and the relationship between model evaluation and experimental design (Broomell and Bhatia 2014). Like ADO, the method called Dynamic Experiments for Estimating Parameters, or DEEP (Toubia et al. 2013), also updates model estimates in real-time and optimizes stimuli based on expected information gain. The main difference between the two methods is that DEEP is specifically formulated for estimating the parameter(s) of an assumed model of preference, whereas ADO was originally formulated for selecting among a group of candidate models (Cavagnaro et al. 2010). However, in the Bayesian framework, model selection and parameter estimation are essentially the same problem, and the special case of ADO in which the goal is to estimate the parameters of a single assumed model was made explicit by Myung et al. (2013). In the present study, we present an ADO method for eliciting temporal discounting preferences and modeling individual-level discounting behavior. The goal of the experiment was to determine which model, among a group of the most prominent in the literature, best accounts for discounting behavior. The data from the ADO-based experiment conclusively discriminate among the six functional forms listed above, at the individual level, but we find that there is substantial heterogeneity in terms of both the qualitative shape of the discounting curve and its functional form. In particular, about 25% of subjects showed increasing impatience (i.e., concavity in the discounting curve), a distinct pattern of behavior that cannot be accommodated by most commonly utilized models. The Hyperbolic model was very rarely the preferred model, even using model selection criteria that reward its parsimony. Exponential and Beta-Delta discounting also performed poorly. Overall, we found that the Double Exponential and Constant Sensitivity models provide the best explanation for the largest number of subjects. Implications of these findings are discussed in Section 5. We also tested the ADO method in simulation experiments, pitting it against several benchmark elicitation methods to see how quickly and precisely each method can recover a predetermined generating model from among a group of the most prominent models in the literature. We find ADO to be vastly more effective than each benchmark, including the most widely used instrument in applications, the Kirby Monetary Choice Questionnaire (Kirby et al. 1999). Results also suggest that the method is also robust to misspecification of the stochastic error component of the models. Details of the simulation experiments are reported in the online appendix. The rest of the paper is organized as follows. Section 2 describes the discounting models that we consider in our analysis. Section 3 gives some background to the ADO methodology and the novelties in the current implementation. Section 4 reports the experiment procedure and results, and Section 5 concludes with additional discussion of the results and their implication for both temporal discounting and the ADO methodology.",33
52.0,3.0,Journal of Risk and Uncertainty,28 July 2016,https://link.springer.com/article/10.1007/s11166-016-9239-6,Did the Great Recession keep bad drivers off the road?,June 2016,Vikram Maheshri,Clifford Winston,,Male,Male,Unknown,Male,"Previous analyses of automobile safety, such as Crandall et al. (1986) and Edlin and Karaca-Mandic (2006), have taken an aggregate approach to estimate the relationship between accident fatalities and VMT by including state-level controls for motorists’ socioeconomic characteristics (e.g., average age and income), riskiness (e.g., alcohol consumption), vehicle characteristics (e.g., average vehicle age), and the driving environment (e.g., the share of rural highways). Taking advantage of our novel data set, our disaggregated approach focuses on individual drivers to estimate the effect of changes in the macroeconomic environment on automobile fatalities, which could be transmitted through three channels: individual drivers might respond to the economic changes by altering their behavior; the composition of drivers or vehicles in use might respond to the economic changes; the driving environment itself might be affected in ways that influence automobile safety (e.g., the public sector might increase spending on road maintenance as fiscal stimulus). Our empirical analysis is based on data provided to us by State Farm (hereafter referred to as the “State Farm data”).Footnote 5 State Farm obtained a large, monthly sample of drivers in the state of Ohio containing exact odometer readings transmitted wirelessly (a non-zero figure was always reported) from August 2009, in the midst of the Great Recession, to September 2013, which was well into the economic recovery.Footnote 6 The number of distinct household observations in the sample steadily increased from 1907 in August 2009 to 9955 in May 2011 and then stabilized with very little attrition thereafter.Footnote 7 The sample also contains information about each driver’s county of residence, which is where their travel originates and tends to be concentrated, safety record based on accident claims during the sample period, socioeconomic characteristics, and vehicle characteristics. For each of the 88 counties in Ohio, we measured the fluctuations in economic activity and the effects of the recession by its unemployment rate.Footnote 8 We use the unemployment rate because it is easy to interpret and because other standard measures of economic activity, such as gross output, are not well measured at the county-month level. Using the size of the labor force residing in each county instead of the unemployment rate did not lead to any changes in our findings.Footnote 9
 The sample is well-suited for our purposes because drivers’ average daily VMT and Ohio’s county unemployment rates exhibit considerable longitudinal and cross-sectional variation. Figure 2 shows that drivers’ average daily VMT over the period we examine ranges from a few miles to more than 100 miles and Fig. 3 shows that county unemployment rates range from less than 5% to more than 15%. Finally, we show in Fig. 4 that for our sample average daily VMT and the unemployment rate are negatively correlated (the measured correlation is -0.40), which provides a starting point for explaining why automobile fatalities are procyclical. Distribution of daily VMT in Ohio, 2009–2013 Distribution of unemployment rates by county in Ohio, 2009–2013 Ohio average individual daily VMT and unemployment rate We summarize the county, household, and vehicle characteristics in the sample that we use for our empirical analysis in Table 1. Although we do not observe any time-varying characteristics of individual drivers such as their employment status, we do observe monthly odometer readings from drivers’ vehicles that allow us to compute time-varying measures of their average daily VMT. The drivers included in our sample are generally State Farm policyholders who are also generally the heads of their respective households. The data set included information on one vehicle per household, which did not appear to be affected by seasonal or employment-related patterns that would lead to vehicle substitution among household members because less than 2% of the vehicles in the sample were idled in a given month. The sample does suffer from potential biases because individual drivers self-select to subscribe to telematics services that allow their driving and accident information to be monitored in return for a discount from State Farm. Differences between the drivers in our sample and drivers who do not wish their driving to be monitored suggest that the Ohio drivers in our sample are safer compared with a random sample of Ohio drivers. This is confirmed to a certain extent in Table 1 because our sample, as compared with a random sample, tends to contain fewer younger drivers, with the average age of the household head nearly 60. The table also suggests our sample is likely to have safer drivers, as compared with a random sample, because it has a somewhat higher share of new cars and of trucks and SUVs. To assess the potential bias on our findings, we obtained county-month level data from State Farm containing household and vehicle characteristics of all drivers in the (Ohio) population, and we used that data to construct sampling weights on each observed characteristic. But because we expect that unweighted regressions using our sample of disproportionately safe drivers, as we have hypothesized, should yield conservative estimates of the effect of the Great Recession on automobile safety, we initially report the results from those regressions. As a sensitivity check, we then re-estimate and report our main findings weighting by the age of drivers in the population in each county, which corrects for the most important potential source of sample bias. Of course, drivers who select not to be in our sample may have unobserved characteristics that we cannot measure that contribute to their overall riskiness. Nonetheless, a weighted sample that somehow accurately represented those unobserved characteristics in the population would likely still be composed of a population of drivers who are less safe than the drivers in our unweighted sample, which again suggests that the unweighted sample yields conservative estimates of the effect of the Great Recession on automobile safety. Another consideration regarding our sample—and generally any disaggregated sample of drivers’ behavior—is that although it consists of a large number of observations (291,834 driver-months, covering 15,228 drivers, and 17,766 vehicles, none of which was strictly used for commercial purposes), only a very small share of drivers ever experiences a fatal automobile accident. Thus our sample would have to be considerably larger than 300,000 driver-months to: (1) assess whether the change in fatalities during a business cycle can be explained by more than a change in VMT alone; and (2) identify the specific causal mechanism at work by jointly estimating how individual drivers’ employment status affects their VMT, and how any resulting change in their VMT affects their likelihood of being involved in a fatal automobile accident. Accordingly, our empirical strategy proceeds as follows: We identify the causal effect of changes in the local economic environment over our sample period, as measured by the local unemployment rate, on the driving behavior of individual drivers, as measured by the variation in their monthly VMT. We first carry out this estimation at the aggregate (county) level, which appears to show that the primary channel through which increasing unemployment reduces fatalities is by reducing VMT. Estimating identical model specifications using disaggregated measures of average monthly VMT for individual drivers, however, reveals that rising local unemployment has no apparent effect on individual drivers’ VMT. In order to ascribe a causal interpretation to these estimates and address concerns about endogeneity bias, we replicate the disaggregate estimations using an instrumental variables approach that relies on plausibly exogenous spatial variation in economic conditions. The results reinforce our previous finding that the variation in local unemployment has no apparent effect on individual drivers’ VMT. We enrich our analysis by estimating heterogeneous effects of local economic conditions on VMT by individual driver and vehicle characteristics. We find that plausibly riskier drivers disproportionately reduce their driving in response to adverse economic conditions. Although we cannot separate the contribution of changes in drivers’ behavior and in their composition, both responses suggest that an important reason that highway safety improves during a recession is that a larger share of VMT is accounted for by safer drivers during periods of greater unemployment. Finally, we identify the effect of the local unemployment rate on the local automobile fatality rate (as measured by fatalities per VMT), and we find that rising unemployment within a county has a statistically and economically significant effect in reducing that county’s fatality rate. Our analysis controls for a variety of factors related to the driving environment in order to explore the extent to which this effect is mediated solely through safer driving by some individuals (including switching to safer vehicles) or by changes in the representation of a greater share of less risky drivers on the road. Although we cannot control for all of the unobserved factors that characterize the driving environment, our results strongly suggest that the notable improvement in safety during the Great Recession has occurred largely because risky drivers’ share of total VMT has decreased.",28
53.0,1.0,Journal of Risk and Uncertainty,14 November 2016,https://link.springer.com/article/10.1007/s11166-016-9243-x,Heterogeneous risk and time preferences,August 2016,Alina Ferecatu,Ayse Önçüler,,Female,Unknown,Unknown,Female,"When making decisions in an intertemporal setting, individuals are known to heavily discount future outcomes and to have a strong preference for immediate gains (Loewenstein and Prelec 1992; Frederick et al. 2002). Understanding individuals’ intertemporal decisions and designing tools to improve their decision making process has been a major research concern, with individual welfare and public policy implications. Are people myopic about health decisions such as following better diets, quitting smoking, or exercising? Do public policy makers respond appropriately to the threat of global warming, a decision involving a tradeoff between short term expenses and long run rewards? Chesson and Viscusi (2003) analyze the joint influence of time and uncertainty and conclude that people may have difficulties choosing the optimal precautionary measures to prevent climate change, a long term hazard, due to both ambiguity in the probability of global warming phenomena and also due to the ambiguity in the timing of global warming consequences. This suggests that a joint model of risk and time preferences is necessary to assess decision makers’ tradeoffs between outcomes at different points in time. The current study proposes a methodology that jointly elicits and estimates risk and time preferences at the individual level. We elicit risk attitudes and intertemporal choices following Holt and Laury’s (2002) risk aversion experimental procedure and Coller and Williams’s (1999) time discounting methodology involving price lists. We embed a constant relative risk aversion specification in an exponential discounting model, following Andersen et al. (2008). Our main contribution is methodological. We add a layer of flexibility to the estimation method through a hierarchical Bayes methodology, which allows us to recover individual level estimates and assess the heterogeneity in intertemporal choices and risk attitudes. Hierarchical Bayes modeling is a validated alternative to maximum likelihood estimation (Rossi et al. 2005; Toubia et al. 2013). The flexibility of the method in dealing with highly non-linear model specifications represents an important benefit. Such individual level estimates can be used in simulation studies to assess the effectiveness of changes in public policy decisions. To our knowledge, this is the first study that uses hierarchical Bayes modeling to jointly estimate risk and time preferences and to analyze the heterogeneity in the data. Our results suggest that individuals are generally risk-averse, with a mean constant relative risk aversion coefficient of 0.515; their discount rates are in line with previous findings on joint elicitation (mean discount rate of 12%). The methodology is general and can be adapted to different functional forms. To test the robustness of our results, we estimate time preferences by using an exponential model, a hyperbolic model proposed by Mazur (1987), another hyperbolic specification by Prelec (2004), and a mixture model that allows a part of the population to behave as exponential discounters, and the remaining part as hyperbolic discounters. Our study provides evidence that controlling for the curvature of the risk aversion leads to lower estimated discount rates in experiments with hypothetical stakes. We conduct a clustering study and find three types of decision makers based on their risk and time preferences: a high patience type (risk-averse, low discounting), a moderate patience type (with average risk and time preferences), and a low patience cluster (risk-seeking, high discounting). This result suggests that regulators could design policy interventions specifically aimed at people with various degrees of patience. The increased reliability of our estimates also allows us to revisit the issue of the correlation between risk and time preferences. We find evidence that risk-seeking people are more impatient and require higher interest rates to postpone consumption. The remainder of the paper is organized as follows. In the next section, we present a brief literature review on joint elicitation and estimation of risk and time preferences. The experimental procedure for the joint elicitation of risk and time preferences is introduced in Section 3. In Section 4, we introduce our empirical model and present the estimation methodology involving a hierarchical Bayes approach. In Section 5, we report the results from our Bayesian estimation and further investigate the heterogeneity of risk and time preferences via individual differences and clusters of decision makers. The paper concludes with a discussion and the implications of our findings.",28
53.0,1.0,Journal of Risk and Uncertainty,12 December 2016,https://link.springer.com/article/10.1007/s11166-016-9245-8,Time preferences and risk aversion: Tests on domain differences,August 2016,Christos A. Ioannou,Jana Sadeh,,Male,Female,Unknown,Mix,,
53.0,1.0,Journal of Risk and Uncertainty,14 December 2016,https://link.springer.com/article/10.1007/s11166-016-9244-9,Experimental evidence on valuation with multiple priors,August 2016,Jianying Qiu,Utz Weitzel,,Unknown,Male,Unknown,Male,"In many real world situations there is too little information to form a unique prior that individuals feel confident enough to use as a sole base for decision making. In such situations of ambiguity, people may have not one but a set of priors, or ‘multiple priors’, which they consider in their decision making process. Some of the most popular models for decision making under ambiguity, which are used to explain the valuation of ambiguous assets, explicitly consider multiple priors: the maxmin model of Gilboa and Schmeidler (1989), the α maxmin model of Ghirardato et al. (2004), the variational preferences model of Maccheroni et al. (2006), and the smooth models of ambiguity by Klibanoff et al. (2005). In the quest to find out which of these models best explains real decision making, a substantial body of the pertinent literature provides empirical tests (see, e.g., Ahn et al. 2014, Hey et al. 2010, Cubitt et al. 2014 and the references therein). The results are mixed, however. We argue that one reason for this is that the predictions of the models were tested, but their underlying mechanisms were not. The latter would involve the elicitation and characterization of multiple priors, which has not been done yet. Hence, in order to understand how people use multiple priors in decisions under ambiguity, this paper attempts to measure beliefs with multiple priors and then uses these multiple priors to test the above mentioned models of decision making under ambiguity. Characterizing beliefs under ambiguity when it is possible to have multiple priors is tricky as it calls for higher orders of beliefs. Consider, for example, an ambiguous Ellsberg urn (Ellsberg 1961) with ten balls that are either white or black. A first-order belief refers to the overall (expected) probability of a drawn ball being white or black.Footnote 1 If we want to study beliefs involving multiple priors, we need to elicit an individual’s second-order beliefs, that is her confidence weights for all potential priors.Footnote 2 Such a procedure can be complicated and counter-intuitive. While it is difficult enough to properly elicit one prior, it appears to be impossible to elicit more than one prior from the same individual. Even if such a procedure could be implemented, it would be difficult to find an incentive compatible mechanism for it. Note that, to properly incentivize individuals to report their multiple priors and their respective confidence weights, we need to make sure that at least one of the multiple priors actually occurs; otherwise individuals cannot benefit from reporting sincere beliefs (or lose when reporting fake ones). In almost all real world scenarios, however, only a potential state in a prior is realized but never a prior itself. As a first objective, this paper attempts to elicit the min and the max of multiple priors. To measure beliefs with multiple priors we construct an ambiguous scenario for which we first elicit experimental participants’ ‘single prior’. For an incentive compatible elicitation of multiple priors, we exploit the uncertainty about other participants’ single priors to indirectly elicit a subject’s own perception of uncertainty in the ambiguous scenario. Explicitly, we ask each participant, without any additional information, to state her confidence weights over all other experimental participants’ single priors. As the real distribution of other subjects’ single priors can be easily obtained from the experimental data, the elicitation of confidence weights for these priors can be properly incentivized. We argue that the confidence statements of a participant about others’ priors indirectly reflect her own perception of uncertainty in the ambiguous scenario. Note that, when having to guess the priors of other experimental participants in the absence of any additional information, the best participants can do is to use their own set of multiple priors. This claim is essentially the “impersonally informative” assumption of Prelec (2004). Given the participants’ own set of multiple priors, in particular the min and the max of priors, and being uncertain about other participants’ decision models of ambiguity, a participant — whatever her own decision model is — knows that there is some possibility that her own min and max of multiple priors will be reported by others. We define the min (max) of multiple priors of a participant as the most pessimistic (optimistic) prior that receives a positive confidence weight. Having obtained the min and the max of priors, we go one step further and explore the possibility that the confidence statements of a participant about others’ priors might indirectly reflect her own perception of uncertainty in the ambiguous scenario. Hence, a participant’s confidence statements can be interpreted as her ‘confidence weight distribution’ of (multiple) priors. Although this interpretation requires a leap of faith, it is not far-fetched. Note that it is cognitively very demanding to think of other participants’ single priors. For this, each participant would have to imagine and compute multiple alternative processes and decision rules according to which a set of multiple priors is aggregated into a single prior. In comparison, it is much more intuitive and straightforward for participants to simply state their own confidence weight distribution. This idea has been explored in some studies implicitly and indirectly. Ilut and Schneider (2014) point out that the distribution of survey forecasters is often used as a measure of ambiguity since disagreement of experts plausibly reflects uncertainty about what the right model of the future is. Here we use participants’ own uncertainty about others’ opinions as a measure of ambiguity. We make a first attempt to systematically apply this idea to elicit a confidence weight distribution of multiple priors. To examine the validity of this claim, we analyze the role of the confidence weight distributions of multiple priors in the participants’ perception of the constructed ambiguous scenario, e.g. in the evaluation of ambiguous assets. Eliciting an individual’s beliefs about others’ behavior is nothing new. It is frequently done in experiments with strategic games, (see Crawford et al. 2013 for a recent review). However, our purpose of belief elicitation is entirely different: in experiments with strategic games beliefs are used to check the consistency of strategies and beliefs or to investigate the depth of an individual’s strategic deliberation of her opponents, whereas here the elicited beliefs are used to gain a deeper understanding of the individual’s perception of an ambiguous scenario, not of her opponents. As a second objective, this paper tests four popular ambiguity models of multiple priors: the maxmin model, the α maxmin model, the variational preferences model, and the smooth ambiguity model. In contrast to previous studies, the elicited min and max of multiple priors, together with the confidence weight distribution of multiple priors, allows us to test these models more directly. Our results suggest that, when comparing the maxmin model and the α maxmin model, subjects consider both the max and the min of multiple priors in the evaluation of the ambiguous asset. Although they seem to place more weight on the minimum, we find more support for the α maxmin model than the maxmin model. The estimated weight on the minimium, α, is about 2/3. A likelihood ratio test suggests that the improvement in the explanatory power is statistically significant. We tested two particular versions of the variational preferences model. The explanatory power of our specifications of the variational preferences model stand between the maxmin and the α maxmin model. We find that participants’ confidence statements about others’ priors have significant explanatory power for their own valuation of the ambiguous asset. This result provides strong support for the interpretation of confidence statements as a participant’s confidence weight distribution of own priors. Encouraged by this result, we performed a test of the smooth model of ambiguity and find that it performs best among the four multiple priors models. Furthermore, we find that the estimated coefficients of the priors are generally increasing in confidence weights, a pattern that is consistent with the prediction of the smooth model of ambiguity. The paper complements a growing literature that experimentally tests various models of ambiguity by developing and analyzing competing predictions that discriminate between the different approaches (see, e.g., Hey et al. 2010; Cubitt et al. 2014). Many of these studies explicitly use the complete set of probability measures over states as the set of multiple priors, and, hence, the support of the probability measures as the minimum (min) and maximum (max) of multiple priors. The problem with this approach is, however, that the set of priors does not need to be equal to the complete set of probability measures (Baillon et al. 2011). Furthermore, previous studies did not account for a confidence weight distribution of priors, although the smooth model of ambiguity explicitly calls for it. The rest of this paper is organized as follows. Section 2 summarizes the four models of multiple priors and presents the experimental design. Section 3 reports and discusses the experimental results and Section 4 concludes.",10
53.0,2.0,Journal of Risk and Uncertainty,04 February 2017,https://link.springer.com/article/10.1007/s11166-016-9249-4,"Process fairness, outcome fairness, and dynamic consistency: Experimental evidence for risk and ambiguity",December 2016,Stefan T. Trautmann,Gijs van de Kuilen,,Male,Male,Unknown,Male,"Preferences that are affected by considerations of fairness have been widely used to explain contractual agreements in organizations and firms. The literature distinguishes between preferences for outcome fairness, where the agent is concerned about the actual distribution of payoffs, and preferences for process fairness, where the agent is concerned about the random process by which outcomes are created, but not what these outcomes actually are (e.g., Trautmann and Vieider 2012). That is, outcome fairness is consequentialist while process fairness is non-consequentialist: an agent with process fairness preferences takes into consideration the counterfactual outcomes at branches of the decision tree that never materialize. A potential problem of non-consequentialist fairness preferences therefore concerns the dynamic inconsistency of these preferences. A decision maker may evaluate some outcome differently ex-ante, before a procedurally fair process has been implemented, versus ex-post, after its implementation has led to some distribution of outcomes. This happens when the consideration of counterfactual outcomes is easy ex-ante but difficult ex-post. Machina’s (1989) parental example illustrates the problem. A mother of two children, Abigail and Benjamin, has exactly one candy. Both children agree that the candy will be allocated by coin flip (fair process). After Abigail wins the candy, Benjamin complains that it is unfair that only Abigail receives a treat (unfair outcomes). He is unable to appropriately consider the counterfactual situation of himself winning the candy. Similar problems arise in all situations where scarce resources are allocated by a fair process, but lead to unfair outcomes: agents obtaining the unfavorable outcome may ex-post question the fairness or appropriateness of the process, despite their initial acceptance of it. Issues relating to the dynamic consistency of process fairness, and more generally the role of ex-ante versus ex-post evaluations of fairness, have been discussed in a few theoretical contributions. Andreozzi et al. (2013) present a model of a decision maker who is concerned about the correlation of her own and another person’s payoffs in the context of risky random allocations. Fudenberg and Levine (2012) show that process fairness preferences are incompatible with the independence axiom underlying expected utility representations, and Saito (2013) subsequently introduces an axiomatic non-EU model of process and outcome fairness. Chassang and Zehnder (2016) study incomplete contracts with an arbitrator who is concerned about ex-ante and ex-post fairness. Most closely related to the current study, Trautmann and Wakker (2010) present the dynamic inconsistency problem inherent to process fairness. While there is clear empirical evidence that process fairness plays an important role in many societal settings (e.g. Brock et al. 2013; Krawczyk 2010; Krawczyk and Le Lec 2010; Linde and Sonnemans 2015; Schildberg-Hörisch 2010 and references thereinFootnote 1), there is little evidence yet on the dynamic consistency of process fairness preferences (see Trautmann and Wakker 2010, section 4). Apart from our current results, the only paper directly assessing the dynamic consistency of process fairness preference is Andreoni et al. (2016). These authors investigate dynamic consistency of fairness preferences of an allocating third party (like the mother in Machina’s example), and find evidence for dynamic inconsistency of fairness preferences. In Section 4 we discuss these findings in more detail in the context of our own results, which are based on the preferences of the agents who are themselves exposed to a random allocation of their outcomes (like Benjamin and Abigail in Machina’s example). We believe that establishing the prevalence of violations of dynamic inconsistency under process fairness preferences is of major importance for applications. If dynamic consistency cannot be empirically supported, preferences may typically revert to outcome fairness after procedures have been executed, with various implications for contracts and the feasibility of organizational outcomes. In many settings, process fairness to some extent depends on whether other agents subscribe to it as well. Consider Machina’s example. If the mother is responsive to Benjamin’s ex-post complaint, Abigail may feel that the procedure was not fair after all. Such a dependence on the preferences of the other agents does not apply to outcome fairness. In practical applications, it will typically be the case that agents will not know for sure whether others subscribe to process fairness, or to outcome fairness, or possibly are dynamically inconsistent. Consider for example a fair promotion process. An unsuccessful candidate may afterwards try to delay or even prevent the promotion of the selected candidate by legal means, despite his initial agreement with the procedures for assessment. But then the process was not balanced after all. For a candidate who consistently subscribes to process fairness, the outcomes are either the other candidate getting promotion, or his own selection for promotion being contested. Because of this additional problem for process fairness (which is absent for outcome fairness), we study whether people consistently hold the process fairness view in situations where there is some dependence on other people behaving in accord with process fairness as well. That is, we implement a demanding test of process fairness. The fairness of an allocation process can be established unambiguously in an experimental setting. However, in practical situations, the process may be ambiguous. To probe the robustness of process fairness when the process is ambiguous, we study dynamic consistency of fairness preferences both if the random distribution is known to the decision makers (“risky process”) and if it is unknown (“ambiguous process”). Because decision makers often hold pessimistic attitudes toward ambiguous random processes (Ellsberg 1961; Trautmann and van de Kuilen 2015; Viscusi and Magat 1992), ambiguity may lead to a biased perception of the fairness of the allocation process. That is, the decision makers may put too much weight on the poor allocation outcomes for themselves and too little weight on the good ones, leading to a breakdown of process fairness. Ambiguity may also make it easier to justify a switch towards the outcome focus after observing a bad outcome ex-post, leading to more dynamic inconsistency. We study dynamic consistency of social preferences in a simple 2-player random allocation game (described in detail in Section 2.1). In the presence of uncertainty about the selected allocation, players are asked whether they accept or reject a highly disadvantageous and a highly advantageous allocation, respectively. Allocations are only implemented if both players accept it. Otherwise an outcome-fair allocation with a low payoff for both players is implemented.Footnote 2 In a 2 × 2 between-subjects design, we vary the nature of the random process that determines whether the advantageous or the disadvantageous allocation is proposed (risky vs. ambiguous), and the point in time when players are asked to make their decision, relative to the moment that the uncertainty concerning the proposed allocation is resolved (ex-ante vs. ex-post). We observe that a substantial share of the participants accept the disadvantageous allocation ex-post, i.e., after the resolution of uncertainty (30% in the risky environment and 24% in the ambiguous environment). When decisions are made from the ex-ante rather than the ex-post perspective, the percentage of accepted disadvantageous allocations is somewhat higher (37% in the risky environment and 46% in the ambiguous environment). We also observe direct evidence for dynamic inconsistency in the ex-ante condition: when offered the chance to reconsider their ex-ante decisions, about 20% of the participants change their decision and do so significantly more often in the direction predicted by outcome fairness or selfishness. Thus, even in an environment that is arguably unfavorable to process fairness, a significant share of decision makers behave according to process fairness in both ex-ante and ex-post decisions, rejecting consequentialism. However, there is some evidence for dynamic inconsistency, with outcomes being more salient after the resolution of uncertainty. Institutional arrangements based on procedural fairness thus need to be on guard against a potential “unraveling” of process fairness as outcomes are determined and communicated to those exposed to the allocation process.",15
53.0,2.0,Journal of Risk and Uncertainty,14 January 2017,https://link.springer.com/article/10.1007/s11166-016-9248-5,What can multiple price lists really tell us about risk preferences?,December 2016,Andreas C. Drichoutis,Jayson L. Lusk,,Male,Male,Unknown,Male,"The abundance of uncertainty in life has prompted a great many investigations into humans’ response to risk. The interest in understanding risk preferences has created a latent demand for effective, easy-to-use risk preference elicitation devices. Following a long line of previous research by Becker et al. (1964), Binswanger (1980) and Binswanger (1981), and many others, in 2002 Holt and Laury (H&L) introduced a risk preference elicitation method that has subsequently become a mainstay. In a testament to the general interest in risk preference elicitation and to the appeal of the specific approach introduced by H&L, their work has been cited more than 3,700 times to date according to Google Scholar and is the second most highly cited paper published by the American Economic Review since 2002 according to ISI’s Web of Knowledge. The approach used by H&L has subsequently come to be referred to as a type of multiple price list (MPL) (Andersen et al. 2006; Harrison and Rutström 2008), an approach thought to have been first used by Miller et al. (1969).Footnote 1 The key advantage of the MPL is its ease of use. Respondents make a series of consecutive choices between two outcomes, where the expected value of one outcome increases at a higher rate than the other. The point at which an individual switches from choosing one outcome over the other is often used as a measure of risk aversion. Despite the fact that MPLs are easy to use and relatively easy for participants to understand, the approach has some weaknesses. Harrison et al. (2005) pointed out that inferences from MPLs can be influenced by order effects (see also Holt and Laury 2005), and Andersen et al. (2006) discussed the potential for choices in MPLs to be influenced by the ranges of values used. Here, we point to a more fundamental problem with MPLs that seems to have been overlooked by practitioners. In particular, the H&L approach is subject to Wakker and Deneffe’s (1996) critique that many risk preference elicitation methods confound estimates of the curvature of the utility function (i.e., the traditional notion of risk preference) with an estimate of the extent to which an individual weights probabilities non-linearly. These are two conceptually different constructs that have different implications for individuals’ behavior under risk, and without controlling for one, biased estimates of the other are obtained. This observation about MPLs is well known to experts in the field of risk preference elicitation, and yet in our experience, it is not well known to newcomers or those outside the field. The purpose of this paper is to further elucidate some of these issues and more widely disseminate this knowledge among the (apparently large) audience of individuals interested in risk preference elicitation. Moreover, while we agree that the use of a single “choice list” or MPL may not perform well in fully capturing the multidimensional aspects of risk preferences, it must be acknowledged that their popularity results from ease of use. Accordingly, in this paper, we show that different types of MPLs are better able to capture some risk dimensions than others and that by using two (or more) easy to use MPLs, a researcher might achieve a more balanced picture of risk preferences, and thus might attain improved predictive validity.Footnote 2
 In what follows, we show that H&L’s original MPL is, perhaps ironically, not particularly well suited to measuring the traditional notion of risk preferences — the curvature of the utility function. Rather, it is likely to provide a better approximation of the curvature of the probability weighting function. We then introduce an alternative MPL that has exactly the opposite property. By combining the information gained from both types of MPLs, we show that greater prediction performance can be attained.",31
53.0,2.0,Journal of Risk and Uncertainty,01 February 2017,https://link.springer.com/article/10.1007/s11166-016-9247-6,How to reveal people’s preferences: Comparing time consistency and predictive power of multiple price list risk elicitation methods,December 2016,Tamás Csermely,Alexander Rabas,,Male,Male,Unknown,Male,"Risk is a fundamental concept that affects human behavior and decisions in many real-life situations. Whether a person wants to invest in the stock market, tries to select the best health insurance or just wants to cross the street, he/she will face risky decisions every day. Therefore, risk attitudes are of high importance for decisions in many economics-related contexts. A multitude of studies elicit risk preferences in order to control for risk attitudes, as it is clear that they might play a relevant role in explaining results — e.g. De Véricourt et al. (2013) in the newsvendor setting, Murnighan et al. (1988) in bargaining, Beck (1994) in redistribution or Tanaka et al. (2010) in linking experimental data to household income, to name just a few. Moreover, several papers try to shed light on the causes of risk-seeking and risk-averse behavior in the general population with laboratory (Harrison and Rutström 2008), internet (Von Gaudecker et al. 2011) and field experiments (Andersson et al. 2016; Harrison et al. 2007). Since the seminal papers by Holt and Laury (2002, 2005), approximately 20 methods have been published which provide alternatives to elicit risk preferences. They differ from each other in terms of the varied parameters, representation and framing. Many of these risk elicitation methods have the same theoretical foundation and therefore claim to measure the same parameter — a subject’s “true” risk preference. However, there are significant differences in results depending on the method used, as an increasing amount of evidence suggests. It follows that if someone’s revealed preference is dependent on the measurement method used, scientific results and real-world conclusions might be biased and misleading. As far as existing comparison studies are concerned, they usually compare two methods with each other and often use different stakes, parameters, framing, representation, etc., which makes their results hardly comparable. Our paper complements existing experimental literature by making the following contribution: Taking the method by Holt and Laury (2002) as a basis, we conduct a comprehensive comparison of the multiple price list (MPL) versions of risk elicitation methods by classifying all methods into nine categories. To the best of our knowledge, no investigation — including various measures of between- and within-method consistency — has ever been conducted in the literature that incorporates such a high number of methods. To isolate the effect of different methods, we consistently use the MPL representation and calibrate the risk intervals to be the same for each method assuming expected utility theory (EUT) and constant relative risk aversion (CRRA), while also keeping the risk-neutral expected payoff of each method constant and employing a within-subject design. Moreover, our design allows us to investigate whether differences across methods can be reconciled by assuming different functional forms documented in the literature such as constant absolute risk aversion (CARA), decreasing relative risk aversion (DRRA), decreasing absolute risk aversion (DARA), increasing relative risk aversion (IRRA) and increasing absolute risk aversion (IARA). Additionally, we extend our analysis to incorporate EUT with probability weighting and also to incorporate prospect theory (PT) and cumulative prospect theory (CPT). We investigate the within-method consistency of each method by comparing the differences in subjects’ initial and repeated decisions within the same MPL method. Moreover, we assess methods’ self-perceived complexity and shed light on differences and similarities between methods. In the end, we provide suggestions for which specific MPL representation to use by comparing our results to decisions in two benchmark games that resemble real-life settings: investments in capital markets and auctions. Therefore, we analyze the methods along two dimensions, robustness and predictive power, and determine which properties of particular methods drive risk attitude and its consistency. We find that a particular modification of the method by Holt and Laury (2002) derived by Drichoutis and Lusk (2012, 2016) has the highest predictive power in investment settings both according to the OLS regression and Spearman rank correlation. In addition, specific methods devised by Bruner (2009) also perform relatively well in these analyses. However, the method by Drichoutis and Lusk (2012, 2016) clearly outperforms the other methods in terms of within-method consistency and is perceived as relatively simple — in the end, our study provides the recommendation for researchers to implement this method when measuring risk attitudes using an MPL framework. Moreover, our results remain qualitatively the same if we relax our assumption on the risk aversion function, or if we take probability weighting or alternative theories such as prospect theory or cumulative prospect theory into account. Incentivized risk preference elicitation methods aim to quantify subjects’ risk perceptions based on their revealed preferences. We present nine methods in a unified structure — the commonly used MPL format — to our subjects, taking one of the most cited methods as a basis: Holt and Laury (2002). The MPL table structure is as follows: Each table has multiple rows, and in each row all subjects face a lottery (two columns) on one side of the table, and a lottery or a certain payoff (one or two columns) on the other side, depending on the particular method. Then, from row to row, one or more of the parameters change. The methods differ from each other by the parameter which is changing. As the options on the right side become strictly more attractive from row to row, a subject indicates the row where he/she wants to switch from the left option to the right option. This switching point then gives us an interval for a subject’s risk preference parameter according to Table 1,Footnote 1 assuming EUT and CRRAFootnote 2. Note that several other representations of risk elicitation methods exist besides the MPL such as the bisection method (Andersen et al. 2006), the trade-off method (Wakker and Deneffe 1996), questionnaire-based methods (Weber et al. 2002), willingness-to-pay (Hey et al. 2009), etc., but the MPL is favored because of its common usage. Andersen et al. (2006) consider that the main advantage of the MPL format is that it is transparent to subjects and it provides simple incentives for truthful preference revelation. They additionally list its simplicity and the little time it takes as further benefits. As far as the specific risk elicitation method in the MPL framework designed by Holt and Laury (2002) is concerned, it has proven itself numerous times in providing explanations for several phenomena such as behavior in 2x2 games (Goeree et al. 2003), market settings (Fellner and Maciejovsky 2007), smoking, heavy drinking, being overweight or obese (Anderson and Mellor 2008), consumption practices (Lusk and Coble 2005) and many others. Early studies document violations of EUT under risky decision making and provide suggestions how these differences can be reconciled (Bleichrodt et al. 2001). In addition, recent studies (Tanaka et al. 2010; Bocqueho et al. 2014) document potential empirical support for prospect theory (PT, Kahneman and Tversky (1979))Footnote 3 when it comes to risk attitudes: Harrison et al. (2010) found that PT describes behavior of half of their sample best. There is also evidence that subjective probability weighting (PW) (Quiggin 1982) should be taken into account. In addition, cumulative prospect theory (CPT) — PT combined with PW (Tversky and Kahneman 1992) — might also be a candidate that can explain the documented anomalies under EUT. Wakker (2010) provides an extensive review on risk under PT. We justify using CRRA as Wakker (2008) claims that it is the most commonly postulated assumption among economists. Most recently, Chiappori and Paiella (2011) provide evidence on the validity of this assumption in economic-financial decisions.Footnote 4 Nevertheless, alternative functional forms have been introduced, e.g. CARAFootnote 5 (Pratt 1964). It was also questioned whether social status — and mostly the role of wealth or income — might shape risk attitude, which would lead to functions which are increasing or decreasing in these factors such as IRRA and DRRA (Andersen et al. 2012)Footnote 6 or IARA and DARA (Saha 1993).Footnote 7 A review of these functions is provided by Levy (1994). In our robustness analysis, we relax our original assumptions on EUT and CRRA and incorporate all of the above mentioned alternative theories and functional forms. Note that even though we calibrated our parameters to accomodate EUT and CRRA, one is still able to calculate the risk parameter ρ using the aforementioned alternative specifications.Footnote 8
 We group our aforementioned nine risk elicitation methods into two categories: 
 The standard gamble methods (SG methods), where on one side of the table there is always a 100% chance of getting a particular certain payoff and on the other side there is a lottery. The paired gamble methods (PG methods), with lotteries on both sides. We therefore primarily conduct a comparison of different MPL risk elicitation methods. What we do not claim, however, is that the method devised by Holt and Laury (2002) (or MPL in general) is the most fitting to measure people’s risk preferences — we strive to give a recommendation to researchers who already intend to use Holt and Laury (2002) in their studies, and provide a better alternative that shares its attributes with the original MPL design. It should be mentioned that there is an alternative interpretation of our study: The different MPL methods can also be conceived as a mapping of existing risk elicitation methods (from other frameworks) to the MPL space. Several methods exist where the risk elicitation task is provided in a framed context — such as pumping a balloon until it blows (Lejuez et al. 2002) or avoiding a bomb explosion (Crosetto and Filippin 2013). Similarly, some methods differ due to the representation of probabilities with percentages (Holt and Laury 2002) or charts (Andreoni and Harbaugh 2010). All these methods can be displayed with different MPLs by showing the probabilities and the corresponding payoffs in a table format. We provide a complete classification of these methods in the Literature Review section. Up to now, different risk elicitation methods were compared by keeping the original designs, but this approach comes at a price: As the methods differ in many dimensions, any differences found can be attributed to any of those particular characteristics. Our approach can be understood as a way to make all risk elicitation methods as similar as possible, with the drawback of losing the direct connection to the original representation. This paper should therefore primarily be seen as a comparison of different MPL risk elicitation methods, and the resulting comparison of existing risk elicitation methods by mapping them into the same space is only reported for the sake of completeness. We will now discuss the different methods in greater detail and how they are embedded in the literature, if at all. Table 2 provides a summary of the exact parameter that is changing across methods.Footnote 9
 Among the SG methods, there are four parameters that can be changed: The sure payoff (sure), the high payoff of the lottery (high), the low payoff of the lottery (low) or the probability of getting the high payoff (p) (or the probability of getting the low payoff (1−p), respectively). The parameters must of course be chosen in such a way that h
i
g
h>s
u
r
e>l
o
w always holds. For instance, we denote the SG method where the low payoff is changing by “SGlow”, the SG method with the varying certainty equivalent by “SGsure” or the standard gamble method where the probabilities are changing as “SGp”. Binswanger (1980) introduced a method (SGall) where only one of the options has a certainty equivalent. The other options consist of lotteries where the probabilities are fixed at 50-50, but both the high and the low payoff are changing. Cohen et al. (1987) used risk elicitation tasks in which probabilities and lottery outcomes were held constant and only the certainty equivalent was varied. These methods have later been redesigned and presented in a more sophisticated format as a single choice task by Eckel and Grossman (2002, 2008). A recent investigation by Abdellaoui et al. (2011) presents a similar method (SGsure method) in an MPL format with 50-50 probabilities. Bruner (2009) presents a particular certainty equivalent method, where the certainty equivalent and the lottery outcomes are held constant, but the corresponding probabilities of the lotteries are changing (SGp method). Additionally, Bruner (2009) introduces another method where only the potential high outcomes of lotteries vary (SGhigh method). Although not present in the literature, we chose to include a method where the potential low outcome varies for reasons of completeness (SGlow method).Footnote 10
 Holt and Laury (2002, 2005) introduced the most-cited elicitation method under EUT up to now, where potential outcomes are held constant and the respective probabilities change (PGp). Drichoutis and Lusk (2012, 2016) suggest a similar framework where the outcomes of different lotteries change while the probabilities are held constant. We differentiate these methods further into PGhigh and PGlow depending on whether the high or the low outcome is varied in the MPL. Additionally, the PGall method varies both the probabilities and the potential earnings at the same time. Many risk elicitation tasks used in the literature fit into the framework of choosing between different lotteries. Sabater-Grande and Georgantzis (2002) provide ten discrete options with different probabilities and outcomes to choose from. Lejuez et al. (2002) introduce the Balloon Analogue Risk Task where subjects could pump up a balloon, and their earnings depend on the final size of the balloon. The larger the balloon gets, the more likely it will explode, in which case the subject earns nothing. Visschers et al. (2009) and Andreoni and Harbaugh (2010) use a pie chart for probabilities and a slider for outcomes to visualize a similar trade-off effect in their experiment. Crosetto and Filippin (2013) present their Bomb Risk Elicitation Task with an interesting framing which quantifies the aforementioned trade-off between probability and potential earnings with the help of a bomb explosion.Footnote 11
 In addition to the MPL methods, we chose to also incorporate questionnaire risk elicitation methods into our study. Several methods have been introduced that evaluate risk preferences with non-incentivized survey-based methods, and the questions and the methodology they use are very similar. The most recently published ones include the question from the German Socio-Economic Panel Study (Dohmen et al. 2011) or the Domain-Specific Risk-Taking Scale (DOSPERT) by Blais and Weber (2006). For a more detailed description, see the last paragraph of Section 2. The question arises of which method to use if there is such a large variety of risk elicitation methods and whether they lead to the same results. Comparison studies exist, but the majority compare two methods with each other, and thus their scope is limited. The question of within-method consistency has been addressed by some papers: Harrison et al. (2005) document high re-test stability of the method introduced by Holt and Laury (2002, PGp). Andersen et al. (2008b) test consistency of the PGp (2002) method within a 17-month time frame. They find some variation in risk attitudes over time, but do not detect a general tendency for risk attitudes to increase or decrease. This result was confirmed in Andersen et al. (2008a). Yet there is a gap in the academic literature on the time stability of different methods and their representation that we are eager to fill. Interestingly, more work has been done on the field of between-method consistency. Fausti and Gillespie (2000) compare risk preference elicitation methods with hypothetical questions using results from a mail survey. Isaac and James (2000) conclude that risk attitudes and relative ranking of subjects is different in the Becker-DeGroot-Marschak procedure and in the first-price sealed-bid auction setting. Berg et al. (2005) confirm that assessment of risk preferences varies generally across institutions in auction settings. In another comparison study, Bruner (2009) shows that changing the probabilities versus varying the payoffs leads to different levels of risk aversion in the PG tasks. Moreover, Dave et al. (2010) conclude that subjects show different degrees of risk aversion in the Holt and Laury (2002, PGp) and in the Eckel and Grossman (2008, SGall) task. Their results were confirmed by Reynaud and Couture (2012) who used farmers as the subject pool in a field experiment. Bleichrodt (2002) argues that a potential reason for these differences might be attributed to the fact that the original method by Eckel and Grossman (2008) does not cover the risk seeking domain, which can be included with the slight modification we made when incorporating this method. Dulleck et al. (2015) test the method devised by Andreoni and Harbaugh (2010) using a graphical representation against the PGp and describe both a surprisingly high level of within- and inter-method inconsistency. Drichoutis and Lusk (2012, 2016) compare the PGp method to a modified version of it where probabilities are held constant. Their analysis reveals that the elicited risk preferences differ from each other both at the individual and at the aggregate level. Most recently, Crosetto and Filippin (2016) compare four risk preference elicitation methods with their original representation and framing and confirm the relatively high instability across methods. In parallel, a debate among survey-based and incentivized preference elicitation methods emerged which were present since the survey on questionnaire-based risk elicitation methods by Farquhar (1984). Eckel and Grossman (2002) conclude that non-incentivized survey-based methods provide misleading conclusions for incentivized real-world settings. In line with this finding, Anderson and Mellor (2009) claim that non-salient survey-based elicitation methods and the PGp method yield different results. On the contrary, Lönnqvist et al. (2015) provide evidence that the survey-based measure, which Dohmen et al. (2011) had implemented, explains decisions in the trust game better than the SGsure task. Charness and Viceisza (2016) provide evidence from developing countries that hypothetical willingness-to-risk questions and the PGp task deliver deviating results. A recent stream of literature broadens the horizon of investigation to theoretical aspects of elicitation methods: Weber et al. (2002) show that people have different risk attitudes in various fields of life, thus risk preferences seem to be domain-specific. Lönnqvist et al. (2015) document no significant connection between the HLp task and personality traits. Dohmen et al. (2010) document a connection between risk preferences and cognitive ability, which was questioned by Andersson et al. (2016). Hey et al. (2009) investigate noise and bias under four different elicitation procedures and emphasize that elicitation methods should be regarded as strongly context specific measures. Harrison and Rutström (2008) provide an overview and a broader summary of elicitation methods under laboratory conditions, whereas Charness et al. (2013) survey several risk preference elicitation methods based on their advantages and disadvantages. In addition, there is evidence that framing and representation matters. Wilkinson and Wills (2005) advised against using pie charts showing probabilities and payoffs as human beings are not good at estimating angles. Hershey et al. (1982) identify important sources of bias to be taken into account and pitfalls to avoid when designing elicitation tasks. Most importantly, these include task framing, differences between the gain and loss domains and the variation of outcome and probability levels. Von Gaudecker et al. (2008) show that the same risk elicitation methods for the same subjects deliver different results when using different frameworks — e.g. multiple price list, trade-off method, ordered lotteries, graphical chart representation, etc. This procedural indifference was confirmed by Attema and Brouwer (2013) as well, which implies that risk preferences on an individual level are susceptible to the representation and framing used. The previous paragraphs lead us to the conclusion that methods should be compared to each other by using the same representation and format. This justifies our decision to compare them using the standard MPL framework which guarantees that the differences cannot be attributed to the different framing and representation of elicitation tasks. However, this comes at the price that we had to change some of the methods slightly, which implies that they are not exactly the same as their originally published versions. We certainly do not claim that the MPL is the only valid framework, but our choice for it seems justified by its common usage and relative simplicity. We consider a future investigation using a different representation technique as a potentially interesting addition. Also, we emphasize that the differences in our results exist among the MPL representations of the methods and they can only be generalized to the original methods to a very limited extent. See Table 3 for an overview of the link between the MPL representation and the particular method that was published originally, and Table 12 in Appendix A, where we compared the results from our MPL methods to the results in previous studies — most of the studies deliver significantly different results to the risk parameters measured in our study. This is not surprising given the considerations in Sections 1.2.4 and 1.2.5, as we map all methods to the MPL space. Furthermore, risk elicitation methods are very noisy in general. For example the same method with the same representation delivers significantly different results in Crosetto and Filippin (2013) and Crosetto and Filippin (2016).
",28
53.0,2.0,Journal of Risk and Uncertainty,18 March 2017,https://link.springer.com/article/10.1007/s11166-017-9250-6,Learning under compound risk vs. learning under ambiguity – an experiment,December 2016,Othon M. Moreno,Yaroslav Rosokha,,Male,Male,Unknown,Male,"Decision-making under uncertainty is one of the most essential areas of study in economics, in both its single-agent (decision theory) and multiple-agent (game theory) forms. However, as first noted by Knight (1921), it is important to distinguish between uncertainty with known probabilities (risk) and uncertainty with unknown probabilities (Knightian uncertainty or ambiguity). In particular, a problem is ambiguous if there is not sufficient information to generate a unique objective prior probability distribution over the outcomes. Take, for example, the decision to construct a stock portfolio, where the nature of uncertainty cannot be reduced to odds; or making decisions using conflicting recommendations from two or more experts. This is different from a purely risky problem, such as playing roulette, in which rational players agree on a unique probability distribution generating the outcomes, and the potential gains and losses can be interpreted as odds. Ellsberg (1961) made an important behavioral point: attitudes towards risk are distinct from attitudes towards ambiguity. Recent experimental studies by Halevy (2007), Stahl (2014), and Abdellaoui et al. (2015) show that we can explain aggregate behavior using an ambiguity-averse model. In particular, Abdellaoui et al. (2015) focus on the treatment of compound risks relative to simple risks and ambiguity and find more aversion to compound risk than to simple risk. Additionally, these studies find that there is substantial heterogeneity at the individual level, where a significant fraction of participants are ambiguity-neutral. While these studies focus on decision-making under ambiguity in static environments, in the real world, people often have to revise their decisions (learn) upon arrival of new information. The goal of this paper is to investigate whether the difference in behavior under compound risk and ambiguity is present in the dynamic environment in which agents make decisions over time. In this paper, we ask: Is there a difference between how people learn in ambiguous environments as compared to compound risky environments? Specifically, we focus on the problem of how people incorporate new information, as it becomes available, to refine their assessments of the likelihood of events. While the experiment that we carry out is very abstract, the applications are vast. They include problems such as learning about demand for a newly released product on the firm side; learning about the quality of the newly released product based on expert reviews on the consumer side; rebalancing one’s portfolio in the face of new information; and learning about an opponent’s type in a strategic setting upon observing the decisions made. In particular, we focus on learning about the signal-generating process and not on detecting when underlying processes change, as in Massey and Wu (2005). In order to assess our question, we design and conduct an economic experiment to compare the learning process in compound versus ambiguous environments. In our experiment, the participants are required to choose between pairs of lotteries involving urns of black and white marbles of unknown proportions. In order to identify the difference between learning in the two environments, we use two types of urns: “compound” and “ambiguous.” The composition of the “compound” urn is the result of a known randomization device with which the objective probabilities are presented to the subjects. The composition process of the “ambiguous” urn is unknown to the participants; that is, the subjects do not know the objective probabilities. Repeated sampling with replacement from such urns allows the participants to learn — or update their beliefs — about the composition of the urn. In economics modeling, the classic paradigm for the agent’s decision under uncertainty over time relies heavily on the Bayesian updating rule, which specifies how the new information is incorporated into the decision-making problem. The results of experimental testing in the past decades, however, are unsettling (see Camerer 1995 for a comprehensive survey of these results). In fact, the recurrent violations of Bayesian updating have been fertile ground for alternative decision-making models, usually labeled as bounded rationality models (Rabin and Schrag 1999; Kahneman and Frederick 2002). In their seminal article, Kahneman and Tversky (1973) present evidence that individuals over-value new information relative to Bayes’ rule (a judgment bias known as representativeness). El-Gamal and Grether (1995), using a compound lottery setup, show that the most important rules that subjects use are: (a) Bayes’ rule; (b) representativeness (over-weighting the new signal); and (c) conservatism (under-weighting the new signal). Both of these studies, however, consider learning only in risky or compound environments. A large body of literature in psychology also investigates belief updating. One of the most prominent models in that literature is Hogarth and Einhorn (1992). In their model, people handle belief-updating tasks by an “anchor-and-adjustment” process in which they adjust their current belief after new evidence is presented. While Hogarth and Einhorn (1992) focus on order effects — specifically, on the conditions under which early information (primacy) or later information (recency) has more influence on beliefs, our objective is to investigate whether there are behavioral differences in belief updating between compound and ambiguous environments. To achieve our goal, we develop and estimate a behavioral model of subjective belief updating that combines features of the “anchor-and-adjustment” type models with Bayesian updating type models. An assumption that permits us to relate the two is that the subjective prior probability distribution over the outcomes is distributed according to a Beta distribution. The properties of a Beta distribution lead to a learning process that is analytically tractable and can be estimated using standard optimization methods. The two key elements of the behavioral model are: i) weight of the initial belief; and ii) weight of the new signal. The weight of the initial belief provides insight into the belief formation process, while the weight of the new signal captures learning behavior. The distinction is important because only after accounting for the weight of the initial belief can we say whether the subject over- or under-weights the new signal relative to the Bayesian updating. The prior studies on learning in compound environments (e.g., El-Gamal and Grether 1995) typically assume that subjects form beliefs that are consistent with the information presented — in other words, the mean and the weight of the prior are fixed exogenously — and then compare the results with the Bayesian case. We do not make this assumption; instead, we estimate the subjective prior probability distribution even in the compound risk setup. The main finding of the paper is that, after controlling for the subjective initial prior probability distributions, there is a significant difference at the aggregate level between learning in ambiguous and in risky environments. Specifically, we find that when subjects learn under ambiguity, they significantly over-weight the new signal, and when they learn under risk, their behavior is consistent with Bayesian updating. We also find that the initial prior is not the one that participants “should have formed” as an outcome of the composition process. Instead, participants use a prior with a mean that is lower than the one that is consistent with the composition process of the compound urn, or with Laplace’s principle of insufficient information for the ambiguous urn. Additionally, we find no difference in belief formation between the compound and ambiguous urns. These behaviors are generally the same across both genders, except that men place more weight on the initial belief, which is consistent with overconfidence. There have been recent efforts on both the theoretical and the experimental fronts to understand the learning process under ambiguity. On the theory side, papers by Epstein and Schneider (2007) and Hanany and Klibanoff (2009) develop models to incorporate new information in dynamic problems involving ambiguous beliefs. The papers related to ours on the experimental side are Dominiak et al. (2012), Corgnet et al. (2013), and Ert and Trautmann (2014). Dominiak et al. (2012) conduct a dynamic extension of Ellsberg’s three-color experiment and find that a large fraction of participants violate either consequentialism (only outcomes that are still possible matter for updated preferences) or dynamic consistency (ex ante contingent choices are respected by updated preferences), or both. Since dynamic consistency and consequentialism are required for Bayesian updating (Ghirardato 2002; Siniscalchi 2011), we did not expect our behavioral estimates for the ambiguous setting to be in line with Bayesian updating. However, whether subjects would under- or overreact to new information under ambiguity, and how their behavior differs between compound risk and ambiguity, are questions that we would like to address. Corgnet et al. (2013) experimentally study the reaction to new information under ambiguity in financial market settings. They find that there is no under- or over-price reaction to news and that the role of ambiguity in explaining price anomalies is limited. Our study is more fundamental in nature: specifically, instead of looking at the market outcomes that are affected by the market structure and agent interactions—such as price and quantity traded—our goal is to investigate belief formation and updating directly. Finally, the study most closely related to ours is by Ert and Trautmann (2014), who find that sampling experience reverses the pattern of ambiguity attitude observed in the static case. There are several important differences between their work and ours: first, both the compound and ambiguous scenarios are uncertain with respect to the probability of success or failure. This is important because the objective of our paper is to determine how subjects learn about this probability. In Ert and Trautmann (2014), the risky scenarios correspond to the simple risk under which the probability of success is known. Second, in our setting, subjects do not choose the number of samples drawn. Third, we carry out the experiment with physical randomization devices (urns) as opposed to computer-generated random numbers. Finally, the type of analysis is different in that we structurally estimate and test a model of learning. The rest of the paper is organized as follows. In Section 2, we describe the experimental design and belief elicitation procedure. In Section 3, we introduce the learning models and present the estimation procedure used. In Section 4, we test hypotheses of interest and discuss the results. Finally, in Section 5, we conclude.",14
53.0,2.0,Journal of Risk and Uncertainty,08 March 2017,https://link.springer.com/article/10.1007/s11166-016-9246-7,Education and anomalies in decision making: Experimental evidence from Chinese adult twins,December 2016,Soo Hong Chew,Junjian Yi,Songfa Zhong,,Unknown,Unknown,Mix,,
54.0,1.0,Journal of Risk and Uncertainty,26 May 2017,https://link.springer.com/article/10.1007/s11166-017-9253-3,Improving one’s choices by putting oneself in others’ shoes – An experimental analysis,February 2017,Zhihua Li,Kirsten I. M. Rohde,Peter P. Wakker,Unknown,Female,Male,Mix,,
54.0,1.0,Journal of Risk and Uncertainty,08 May 2017,https://link.springer.com/article/10.1007/s11166-017-9254-2,Bounded awareness and anomalies in intertemporal choice: Zooming in Google Earth as both metaphor and model,February 2017,Stein T. Holden,John Quiggin,,Male,Male,Unknown,Male,"Anomalies in intertemporal choice include hyperbolic discounting, quasi-hyperbolic discounting (present bias), and magnitude effects (Chung and Herrnstein 1967; Thaler 1981; Ainslie 1991; Loewenstein and Prelec 1992; Laibson 1997) and represent deviations from the well-known discounted utility model (Samuelson 1937). Magnitude effects in intertemporal choices in the form of systematically lower discount rates associated with prospects with larger monetary amounts appear as an accepted empirical regularityFootnote 1 with few convincing explanations. It may not be explained by the functional form of the utility function and is called the “increasing proportional sensitivity property” (Prelec and Loewenstein 1991). Hyperbolic discounting has been a popular way to model anomalies in intertemporal choice associated with intertemporally inconsistent behavior with potential negative externality effects. While hyperbolic discounting has been accepted as a widespread behavioral characteristic there is no generally accepted account of its sources or its relationship to other aspects of choice, notably including choice under uncertainty. The aim of this paper is threefold. First, we link anomalies in intertemporal choice to the literature on unawareness that has arisen mainly in the context of choice under uncertainty. Second, we introduce a visual metaphor for bounded awareness as ‘zooming’, derived from visualization software such as Google Earth. Third, we present the results of experimental studies designed to test the zooming hypothesis. The approach used in the present paper draws on recent developments in the theory of choice under uncertainty, in which the standard assumption that decision-makers are aware of all possible states of nature is relaxed. Models incorporating various forms of ‘unawareness’ have been developed by Grant and Quiggin (2013), Halpern and Rego (2014), and Heifetz et al. (2006) among others. Schipper (2015) provides a bibliography. The standard assumption of full awareness may be violated in two ways. First, decision-makers may limit the set of contingencies they consider, effectively imputing zero probability to some possible states of nature. This form of unawareness may be referred to as ‘restriction’. Alternatively, decision-makers may lump together contingencies which are, in reality, distinct, as in Heifetz et al. (2006). This form of unawareness may be referred to as ‘coarsening’.Footnote 2
 Given bounded cognitive resources, decision-makers must engage in both restriction and coarsening if they are to reduce even relatively simple decision problems to a manageable scale. Moreover, there is a trade-off between the two processes: the fewer possibilities that are excluded from consideration, the coarser must be the aggregation of those that remain. Visualization software inspired by Google Earth provides a metaphor that is familiar to many. With use of such tools one chooses an area to ‘zoom in’ on as well as the degree of zooming in. As one zooms in, new details appear but the frame becomes much narrower. Zooming permits more focus on the details within a narrow frame but causes the user to lose sight of the larger landscape. In many situations the brain works as a mental zooming device and narrows in the focus on some specific issues or aspects of prospects that are compared rather than evaluating these holistically. Narrow framing (Barberis et al. 2006) or choice bracketing (Read et al. 1999) in some contexts are more specific outcomes of the zooming behavior of the brain. This is therefore a more general theory to attempt to explain what appear to be specific patterns of systematic inconsistent intertemporal choices which have been captured with hyperbolic discount functions or as magnitude effects with higher patience for larger prospects. In the present paper, we apply the ‘zooming’ metaphor in relation to decisions involving intertemporal choice. The main focus of the paper is the relationship between ‘zooming out’ and asset integration. We argue that higher stakes are likely to contribute to ‘zooming out’, which in turn leads to higher levels of asset integration. Section 2 of the paper reviews the literature on hyperbolic discounting and magnitude effects in intertemporal choice including the attempts that have been made at explaining the phenomena. Section 3 outlines the interpretation of zooming in relation to hidden framing and partial asset integration. Section 4 describes field experiments used to obtain data for testing the consistency of the theory with the data. Section 6 uses the standard along with the zooming model of bounded awareness to demonstrate and discuss the predictive power of the model. The final section concludes.",6
54.0,1.0,Journal of Risk and Uncertainty,07 June 2017,https://link.springer.com/article/10.1007/s11166-017-9252-4,The effect of fast and slow decisions on risk taking,February 2017,Michael Kirchler,David Andersson,Daniel Västfjäll,Male,Male,Male,Male,"Many economic and financial decisions appear to be taken rather automatically without much effortful reasoning. For instance, trading decisions on financial markets are taken within seconds after new information arrival (Busse and Green 2002). It is important to understand to what extent such fast decisions differ from more deliberative decisions. In this study, we experimentally test whether financial risk taking is systematically affected by the decision time available. We implement a series of four experiments in three different countries on about 1700 subjects (a student population in Sweden, a general population sample in the US, and two student populations in Austria). Risk attitudes are elicited for both gains and losses by letting subjects choose between a sure gain (loss) or a 50% chance to win (lose) a larger amount. Subjects are randomly allocated to deciding within 7 seconds (time pressure) or waiting 7 or 20 seconds before deciding (time delay). The purpose of the experimental manipulation is to invoke relatively more intuitive decisions with time pressure and relatively more deliberative decisions with time delay. Dual-process models are frequently used to explain differences between intuitive and deliberative decisions in psychology (Epstein 1994; Evans and Stanovich 2013; Glöckner and Witteman 2010; Kahneman 2003, 2011; Pham 2007; Stanovich and West 2000). In dual-process models, intuitive decision-making processes (System 1) are typically characterized as being fast, automatic, effortless, and emotional whereas deliberative decision-making processes (System 2) are characterized as being slower, more controlled, effortful, and deliberative (Epstein 1994; Kahneman 2003, 2011).Footnote 1 Several different experimental manipulations have been used to invoke intuitive versus deliberative decision making. One of them is manipulating the timing of decisions (Finucane et al. 2000; Kocher and Sutter 2006; Kocher et al. 2013; Rand et al. 2012; Sutter et al. 2003; Young et al. 2012; Tinghög et al. 2013), which is the approach we use here. Other studies apply cognitive load or cognitive depletion tasks (Gilbert and Osborne 1989; Gilbert et al. 1995; Greene et al. 2008; Hagger et al. 2010; Shiv and Fedorikhin 1999; Schulz et al. 2014; Xu et al. 2012). According to Kahneman (2011), the characteristics of System 1 are reflected in the features of choice behavior predicted by the S-shaped value function of Prospect Theory—risk aversion for gains, risk taking for losses (the reflection effect) and loss aversion. This interpretation is also supported by the results of Frederick (2005) who found that subjects who score low on the Cognitive Reflection Test (CRT) act more in line with Prospect Theory (more risk averse for gains and more risk seeking for losses) than high scorers. We therefore hypothesize that subjects (i) will be more risk averse for gains and (ii) more risk taking for losses with time pressure compared to time delay (i.e. we expect a larger reflection effect with time pressure than with time delay).Footnote 2
 We find that, on aggregate, subjects are significantly more risk averse for gains and more risk taking for losses in the time-pressure treatment than in the time-delay treatment. Moreover, we observe that the effect in the loss domain is significant in all four experiments separately. The effect in the gain domain, however, is slightly weaker and less robust. To control for different effects of time-pressure and time-delay on measurement noise, we estimate separate parameters for noise and risk preferences within a random utility framework. As expected, there is more measurement noise in the time-pressure treatment than in the time-delay treatment. Following the argument made by Kahneman (2011) about the value function in Prospect Theory capturing the characteristics of System 1 decision making also implies that loss aversion should be higher with time pressure than with time delay.Footnote 3 This is tested in one of the four experiments, but we cannot reject the null hypothesis of no difference across the two treatments. In one of the four experiments, we also include a treatment without time constraints. With this treatment, we test whether our results are primarily driven by forcing subjects to decide quickly or by forcing them to wait and think about their decision compared to unconstrained decisions. Our results for losses suggest that our treatment effect is driven by forcing subjects to take slow decisions rather than forcing them to respond fast. For gains, our results are inconclusive as the results for the time-pressure and time-delay treatments do not differ in the experiment with the unconstrained treatment. As a related finding to ours, Porcelli and Delgado (2009) found that acute stress increased the reflection effect (increased risk aversion for gains and increased risk taking for losses). They argued that stress disrupts deliberative decision-making processes leading to System 1 thinking. In line with these findings, Cahlikova and Cingl (2017) stated that stress increased risk aversion for gains and Kandasamy et al. (2014) found similar effects when the stress hormone cortisol is administered exogenously.Footnote 4
 The most closely related study to ours is the recent paper by Kocher et al. (2013). They compared time-pressure with a no constraint treatment for pure gains, pure losses and mixed gambles. They found that time pressure decreased risk taking for losses, had no effect for gains and increased loss aversion (the mixed gambles). We find different results, although the comparability across the studies is limited. For instance, we only included a no constraint treatment in one of our experiments and they did not include a time-delay treatment. Furthermore, they collected data for two relatively small experiments (n = 176 and n = 95) and imposed a shorter time to respond in the time-pressure treatment (4 seconds rather than 7 seconds in our study). This may exacerbate problems with measurement error, and they do not separate out the effect of time pressure on risk taking from the effect of time pressure on measurement error.",47
54.0,1.0,Journal of Risk and Uncertainty,05 June 2017,https://link.springer.com/article/10.1007/s11166-017-9251-5,Testing independence conditions in the presence of errors and splitting effects,February 2017,Michael H. Birnbaum,Ulrich Schmidt,Miriam D. Schneider,Male,Male,Female,Mix,,
54.0,2.0,Journal of Risk and Uncertainty,12 June 2017,https://link.springer.com/article/10.1007/s11166-017-9256-0,Elicitation of preferences under ambiguity,April 2017,Enrica Carbone,Xueqi Dong,John Hey,Female,Unknown,Male,Mix,,
54.0,2.0,Journal of Risk and Uncertainty,19 June 2017,https://link.springer.com/article/10.1007/s11166-017-9255-1,Anchoring biases in international estimates of the value of a statistical life,April 2017,W. Kip Viscusi,Clayton Masterman,,Unknown,Male,Unknown,Male,"A substantial U.S. labor market literature on the value of a statistical life (VSL) has established the VSL as a critical policy parameter and has stimulated similar research used throughout the world.Footnote 1 Policymakers use estimates of the VSL to value the economic benefits of reduced mortality risks achieved by government policies. These mortality risk benefits often serve as the most important benefit component. However, the VSL estimates that are published based on both U.S. and international labor market data are potentially subject to publication selection effects. This article demonstrates the existence of such biases and that the extent of such biases is much greater for international studies, which overestimate the underlying VSL. The medical literature first generated evidence of publication selection effects whereby substantial publication biases may affect the research findings submitted to and accepted by journals for publication (Stanley and Doucouliagos 2012; Viscusi 2015). Because neither researchers nor the drug companies that provide funding have an interest in publishing information about unproductive lines of research, the results of clinical drug trials often go unpublished. Studies that fail to yield statistically significant results are also less likely to be accepted for publication. As in the case of medical research, the VSL estimates that are published are potentially subject to publication selection effects that could arise at different stages of the publication process.Footnote 2 Researchers may choose to report results that they believe are most credible or are most consistent with economic theory and the literature. For example, because good health is positively valued, compensating differentials for fatality risks should be positive not negative. Journals may be reluctant to publish VSLs that are inconsistent with economic theory, outside the conventional range, or statistically insignificant. Researchers and journal editors may use the early studies in the literature, which were based on U.S. labor market studies, as guideposts for what constitutes the acceptable range of empirical estimates. In much the same way that anchoring influences and reference point effects affect economic behavior generally (Tversky and Kahneman 1974; Kahneman and Tversky 1979), the U.S. evidence may establish a reference point for subsequent international studies. Such subsequent biases lead to an overestimation of the international VSL as compared to its bias-corrected value. This phenomenon is consistent with U.S. studies generating an anchoring effect that leads to disproportionate reporting of higher VSL estimates from international studies. The concern with possible publication selection effects for the VSL is not new. Ashenfelter and Greenstone (2004) hypothesized that publication bias may be influential in the VSL literature, but they did not present an empirical assessment to document the existence or magnitude of such biases. The meta-analysis by Doucouliagos et al. (2012) found evidence of dramatic publication selection effects in the VSL literature overall, suggesting that the bias-corrected VSL estimates were 70–80% lower than the published values. However, Viscusi (2015) found that studies based on recent occupational fatality data are not subject to such biases. Moreover, much of the apparent bias derives from meta-analysis researchers focusing on only a sample of the single best estimates of the VSL from different studies rather than the full set of estimates in these publications. The evidence of bias is considerably less pronounced for the full set of estimates (Viscusi 2017). In this article, we consider the potential impact of publication selection effects using all estimates from a large sample of studies from the U.S. and other countries. Our empirical estimates adjust for publication selection biases, and these adjustments have especially strong impacts on VSL estimates based on data from outside the U.S. Section 2 presents our meta-analysis dataset, which includes 1025 VSL estimates from 68 studies that we categorize based on five different governmental and non-governmental sources of the occupational fatality data used in these studies. The distribution of the estimates that are reported in Section 3 suggests that researchers are particularly reluctant to report negative or very low VSL estimates. The publication selection biases vary across the distribution of VSL estimates, with the greatest biases being evident at the upper end of the VSL distribution. The regression results in Section 4 estimate the effect of publication selection bias for each subsample to assess whether the biases vary depending on the source of the data and whether it is a U.S. or international study. Only the U.S. studies based on recent fatality rate data are free of estimated biases, and the most substantial biases occur in the international studies. As indicated in the concluding Section 5, the considerable biases in international estimates may reflect a more general economic phenomenon in which empirical evidence from the U.S. establishes a reference point that serves as an anchor for determining which economic parameter estimates from other countries are treated as publishable. Fortunately, the presence of such biases need not result in policy paralysis. The U.S. VSL estimates based on recent fatality rate data display no evidence of statistically significant publication selection effects. In conjunction with information about income differences across countries and estimates of the income elasticity of the VSL, it is possible to generate unbiased VSL estimates for use throughout the world.",39
54.0,2.0,Journal of Risk and Uncertainty,12 June 2017,https://link.springer.com/article/10.1007/s11166-017-9258-y,"Allais for the poor: Relations to ability, information processing, and risk attitudes",April 2017,Tabea Herrmann,Olaf Hübler,Ulrich Schmidt,Female,Male,Male,Mix,,
54.0,2.0,Journal of Risk and Uncertainty,16 June 2017,https://link.springer.com/article/10.1007/s11166-017-9257-z,Risk taking after absolute and relative wealth changes: The role of reference point adaptation,April 2017,Hong Chao,Chun-Yu Ho,Xiangdong Qin,,,Unknown,Mix,,
54.0,3.0,Journal of Risk and Uncertainty,27 July 2017,https://link.springer.com/article/10.1007/s11166-017-9259-x,The intrinsic value of choice: The propensity to under-delegate in the face of potential gains and losses,June 2017,Sebastian Bobadilla-Suarez,Cass R. Sunstein,Tali Sharot,Male,,Female,Mix,,
54.0,3.0,Journal of Risk and Uncertainty,26 August 2017,https://link.springer.com/article/10.1007/s11166-017-9261-3,The psychometric and empirical properties of measures of risk preferences,June 2017,Jonathan P. Beauchamp,David Cesarini,Magnus Johannesson,Male,Male,Male,Male,"Preference heterogeneity is a possible explanation for some of the individual-level variation observed in economic behaviors, such as labor supply, saving and consumption decisions, and asset allocation. The fundamental difficulty that arises in testing explanations of individual differences that invoke preference heterogeneity is that preferences are never directly observed. Stigler & Becker (1977) famously argued that economists should assume not only that individual tastes are stable over time, but that tastes are identical across persons. Those who favor this position argue that the problem with preference-based explanations is that defending them is difficult without recourse to tautological, and hence scientifically meaningless, arguments. In recent years, an alternative methodological approach has gained traction in experimental economics and increasingly also in applied empirical economics. According to this view, researchers should try to obtain empirical measures of the fundamental dimensions of heterogeneity using surveys or experiments. Such direct measurement of preferences, sometimes coupled with the assumption that they are stable functions of some observable states of nature, is a way of disciplining preference-based explanations and thus avoiding the problem of ad hoc theorizing that concerned Stigler and Becker. Proponents of this approach argue for the integration of individual-difference psychology into economics (Almlund et al. 2011; Becker et al. 2012; Ferguson et al. 2011; Borghans et al. 2008) and for a sustained effort to learn more about the properties of the measures of preferences that are commonly used in economic research. In this paper, we make several contributions to this effort. First, we examine the consequences of measurement error (or other transitory fluctuations) in some frequently used survey-based measures of risk attitudes and show that accounting for them leads to considerably higher estimates of their predictive power and correlations with other variables. To do so, we adopt a uniform latent variable model that provides a general framework for making inferences about the component of measured risk attitudes that is not due to measurement error. Economists have been aware for some time that measurement error may significantly attenuate the relationship with other variables and lead to mistaken inference (c.f., Solon, 1992). In practice, however, much of the work that uses surveys or experimental tasks to elicit preferences – including work that questions the usefulness of measures of economic preferences for predicting economic outcomes – treats behavioral responses as if they yielded error-free measurements of the underlying preferences. Second, we conduct a detailed examination of the psychometric and empirical properties of some commonly-used measures of risk attitudes using a large, population-based sample of respondents. We document sizable associations between risk attitudes and a host of real-world outcomes and variables, including investment decisions, entrepreneurship, smoking, drinking, gender, cognitive ability, and personality. Our data set is a comprehensive survey of more than 11,000 Swedish twins with rich self-reported data on psychological variables and risky health behaviors, matched to administrative records with information on labor supply and financial portfolio risk. Important for our purposes is that the survey contains five measures of risk attitudes as well as retest data for 500 twins. The first measure is close in spirit to the one developed for the Health and Retirement Survey; it asks people to answer a series of sub-questions about hypothetical gambles over lifetime wealth (Barsky et al. 1997). The next two measures ask, respectively, about risk attitudes in the domain of finance and risk attitudes in general; these measures have been studied by Dohmen et al. (2011), who provide some evidence on their predictive validity. Our final two measures ask about attitudes toward hypothetical gambles over gains and losses. Our third contribution is to introduce and explore concepts and tools from psychometrics – the field of study concerned with the theory and techniques of psychological measurement – and argue that economists have much to learn from that field in their efforts to understand the properties of the measures of economic preferences. Psychometricians who study personality and cognition make a useful conceptual distinction between reliability, construct validity, and predictive validity. Reliability refers to the consistency of individuals’ responses to an instrument across measurement occasions and is a descriptive statistic designed to capture how much measurement error is in a variable; construct validity refers to the degree to which the instrument actually measures the underlying construct it is intended to measure; and predictive validity is the extent to which it correlates with, or predicts, other variables that theory or intuition suggest might relate to the construct purportedly being measured (McArdle & Prescott 1992). Many of the measures of preferences that have come from economics were designed to directly measure the fundamental dimensions of heterogeneity that feature in economic theory, such as a coefficient of risk aversion. In the language of psychometrics, such instruments have, by their very design, strong construct validity. In contrast, relatively little effort has been devoted to studying the reliabilities and predictive validities of the measures used in economics. The research that does address or touch upon these questions suggests the measures are subject to significant measurement error (Barsky et al. 1997; Gillen et al. 2015; Harrison et al. 2005; Sahm 2012; Kimball et al. 2008; Lönnqvist et al. 2014) and have only limited predictive validity. The remainder of the paper is structured as follows. We begin by providing an overview of the data set in Section 2. In Section 3, we introduce the uniform and general latent variable model of risk attitudes that we adopt throughout the paper and that accounts for measurement error (or other transitory fluctuations) and for the ordinal nature of the measures of risk attitudes. Then, in Section 4, we estimate the test-retest reliability of the five measures, using data from approximately 500 respondents who responded to the survey on two occasions. Our estimates – which are measures of the stability of risk attitudes over time – vary between about 0.5 to 0.7 for the different measures, showing some stability in responses over time but also substantial measurement error. Section 5 investigates the predictive validity of the risk measures (focusing on the Risk General, Risk Financial, and Risk HRS variables, which have higher reliability). To do so, we use the GMM estimator proposed by Kimball et al. (2008), which allows consistent estimation of the effect of each individual measure of risk preferences despite the fact that the measures are noisy. We report the first measurement-error-adjusted estimates of the proportion of variation in risky behaviors in the domains of health and personal finance that is explained by the measures of risk attitudes. Our measures of risk attitudes have significant explanatory power for investment decisions, the propensity to run one’s own business, and drinking and smoking behaviors. For example, adding risk attitudes to a rich set of covariates in a regression in which portfolio risk is the dependent variable doubles the R
2, though from a low baseline. As well, after adjustment for measurement error, a one-standard-deviation increase in risk attitudes is associated with an increase of approximately 10 percentage points in the probability of having started a business, as well as with two- to four-percentage-point increases in the probability of being an alcohol consumer and in the probability of being a smoker. We thus find strong support for the proposition that persistent differences are present in risk attitudes across people and that these differences translate into statistically significant and economically important differences in economic choices. Also, we find that adjusting for measurement error substantially increases the estimated effects. An important conclusion emerging from our work is that the low R
2’s reported in previous work (Barsky et al. 1997; Harrison et al. 2005; Dohmen et al. 2011; Dohmen et al. 2012; Kimball et al. 2008; Sahm 2012) – which are sometimes used to discard preference-based explanations – are partly attributable to the relatively low reliabilities of the risk measures. Section 6 examines predictors of the measures of risk attitudes and how the coefficient estimates change once we allow for measurement error in these measures. We document large sex differences in risk attitudes after measurement-error correction and find that cognitive ability – measured about four decades before the risk attitudes – is a strong predictor of risk attitudes. In Section 7, we conduct a factor analysis that shows that all five risk variables load significantly on their first common factor, indicating they share a sizable fraction of their variance, although an important part of the variation is variable-specific. In Section 8, we consider other applications of our framework. We first estimate a sizable and highly significant measurement-error-adjusted correlation between risk attitudes and the personality trait behavioral inhibition. The estimated correlation between behavioral inhibition and risk attitudes increases by about 50% after adjustment for measurement error, and the adjusted estimate of 0.45 is substantially higher than previously reported correlations between risk attitudes and personality traits (Becker et al. 2012; Dohmen et al. 2010; Lönnqvist et al. 2014). We then provide another illustration of the importance of adjusting for measurement error, by reporting rough estimates of the share of the variation of risk attitudes that is attributable to genetic factors. With adjustment for measurement error, the estimates range from 35% to 55% – almost doubling previous estimates in the literature (Cesarini et al. 2009) – compared to 21% to 34% without adjustment for measurement error. We conclude in Section 9.",61
54.0,3.0,Journal of Risk and Uncertainty,22 August 2017,https://link.springer.com/article/10.1007/s11166-017-9262-2,Are the poor worse at dealing with ambiguity?,June 2017,Chen Li,,,,Unknown,Unknown,Mix,,
54.0,3.0,Journal of Risk and Uncertainty,26 August 2017,https://link.springer.com/article/10.1007/s11166-017-9260-4,Measuring ambiguity attitude: (Extended) multiplier preferences for the American and the Dutch population,June 2017,Aurélien Baillon,Han Bleichrodt,Rogier Potter van Loon,Male,,Male,Mix,,
55.0,1.0,Journal of Risk and Uncertainty,17 November 2017,https://link.springer.com/article/10.1007/s11166-017-9266-y,Accommodating stake effects under prospect theory,August 2017,Ranoua Bouchouicha,Ferdinand M. Vieider,,Unknown,Male,Unknown,Male,"Almost any important real world decision involves considerable levels of risk. It thus comes as no surprise that attitudes toward such risks and how to model them have received considerable attention in economics. After the axiomatization by von Neumann & Morgenstern (1944) of Daniel Bernoulli’s 1738 expected utility theory (reprinted in Bernoulli 1954), it soon became clear that a setup richer than a function with one subjective dimension defined over lifetime wealth would be needed to model real world behavior such as the coexistence of insurance uptake and lottery play (for an early discussion of these issues, see Vickrey 1945). Markowitz (1952) provided such a framework by allowing preferences to differ between gains and losses relative to a reference point given by current wealth. Psychologists Preston & Baratta (1948) proposed a different solution, involving the subjective transformation of probabilities instead of outcomes—a solution that they considered to be psychologically more realistic (Lopes 1987). The two approaches of subjectively transforming changes in wealth into utilities and subjectively transforming probabilities into decision weights were finally combined in prospect theory (Kahneman & Tversky 1979; Tversky & Kahneman 1992). Prospect theory is recognized by most scholars to be the leading descriptive theory of decision making under risk today (Barberis 2013; Starmer 2000; Wakker 2010). Nevertheless, its descriptive accuracy continues to be debated. Scholten & Read (2014) recently pointed out how prospect theory has generally neglected the type of changes in risk attitudes taking place purely over outcomes while keeping probabilities constant, as originally proposed by Markowitz (1952). Fehr-Duda et al. (2010) uncovered issues in the separability principle underlying prospect theory—a principle according to which changes in preferences over outcomes ought to be reflected purely in utility, while changes in preferences over probabilities ought to be reflected in probability weighting. We set out to revisit the issue of whether a double-fourfold pattern of risk preferences—changes from risk seeking to risk aversion over the probability spectrum, and changes from risk seeking to risk aversion over the outcome spectrum—can be accommodated within a prospect theory framework without increasing the number of parameters. We thereby expand on the insights provided by Scholten & Read (2014) by generalizing the results to probabilities larger than their p ≤ 0.1 and by discussing the economic and psychological underpinnings of the candidate utility functions. This further allows us to revisit the separability violations pinpointed by Fehr-Duda et al. (2010), and thus to examine the descriptive validity of prospect theory in our setup. Indeed, while a good fit of functional forms is necessary for separability to hold, it is not sufficient, so that separability does not necessarily follow from a good fit of functional forms to the data. We conduct two experiments to investigate these issues. The first one is an incentivized experiment over gains. While this has the advantage of rendering the decision real for subjects, it suffers the drawback of restricting our stake range and to make the investigation of losses problematic. We thus supplement the insights gained from experiment 1 with a second, hypothetical experiment. This allows us to examine the whole fourfold pattern of risk preferences over outcomes, including gains as well as losses. Furthermore, it serves the purpose of testing the stability of the results obtained in experiment 1 to truly large stakes, beyond what we could provide under real incentives. In both experiments, we find relative risk aversion to increase in stakes at all probability levels, with qualitative reversals from risk seeking to risk aversion as stakes increase for some probability levels, as described in Markowitz’s original thought experiment. We also replicate qualitatively similar probability distortions across different stake levels, as originally found by Preston & Baratta (1948). We then proceed to fit functional forms to our data. We find the frequently used power utility function to provide the worst fit to our data—perhaps unsurprisingly, given how that function assumes constant relative risk aversion, while we find relative risk aversion to increase in stakes for gains. The logarithmic utility function proposed by Scholten & Read (2010) for intertemporal decisions, and applied to risk by Scholten & Read (2014), is found to fit our data best. The reason for this may lie in the psychological insight that the subjective sensation derived from a physical stimulus often tends to be proportional to the logarithm of the physical stimulus itself (known as the Weber-Fechner law). From an economic point of view the logarithmic utility function, which can be traced all the way back to Bernoulli’s 1738 essay, incorporates both increasing relative risk aversion and decreasing absolute risk aversion, which constitute the most common empirical finding (Wakker 2010). Our results in terms of separability violations are more nuanced. Examining risk preferences over low and high stakes, Fehr-Duda et al. (2010) found stake effects and the resulting increase in relative risk aversion to be reflected in probability weighting rather than in utility curvature (see also Hogarth and Einhorn 1990). This violates the separability precept of prospect theory, whereby changes in outcomes ought to be purely reflected in utility curvature. We replicate their finding of high stakes shifting probability weighting downwards using a setup similar to the original one. We then show that, when combining our two-parameter weighting function with a logarithmic utility function, these stake effects on probability weighting disappear in experiment 1. This shows that one issue underlying separability violations can be found in the traditional neglect of qualitative changes in risk attitudes over outcomes, and from the inability of traditional power and exponential utility functions to account for such patterns. In experiment 2, however, the use of a logarithmic utility function does not eliminate the issue completely. Indeed, we still observe a separability violation for some stake levels, an issue that seems to be driven by different reactions to changes in outcomes across probabilities.",21
55.0,1.0,Journal of Risk and Uncertainty,16 October 2017,https://link.springer.com/article/10.1007/s11166-017-9265-z,Average willingness to pay for disease prevention with personalized health information,August 2017,David Crainich,Louis Eeckhoudt,,Male,Male,Unknown,Male,"Willingness to pay is usually seen by economists as the most appropriate way to convert into monetary units the benefits that result from investments reducing the probability of death or disease (see for instance the influential contributions of Drèze 1962, Schelling 1968 and Mishan 1971). The measure has been exploited in several studies evaluating the effects of public policies increasing safety. For instance, the willingness to pay for improvements in road safety (Jones-Lee et al. 1985), for pesticide free food (Misra et al. 1991), for poison control centers (Phillips et al. 1997), for improved air quality (Carlsson & Johansson-Stenman 2000), for electronic waste recovering and recycling programs (Nixon & Saphores 2007) and for moving to a neighborhood with less violent crime (Bishop & Murphy 2011) have been assessed. The theoretical foundations of the willingness to pay for reductions in the probability of death or disease have been introduced by Drèze (1962). Since then, a deeper understanding of the properties of this measure has been provided in the literature. For instance, Jones-Lee (1974) describes the form of the functional relationship between the willingness to pay for reductions in the probability of death and the size of these reductions. Jones-Lee (1974) also shows that the willingness to pay should rise with the baseline probability of death, a phenomenon termed the “dead anyway effect” by Pratt & Zeckhauser (1996). These authors also address the concentration of the risk in the population. Specifically, keeping the aggregate risk constant, Pratt & Zeckhauser (1996) determine if the willingness to pay for larger reductions in the probability of death concentrated on a small number of individuals is higher than the willingness to pay for smaller reductions in probability spread within a larger population. Hammitt (2000) examines the effects of the health status and of wealth on the willingness to pay for reductions in the probability of death (expressed as the value of statistical life). He indicates that the first effect depends on the way the marginal utility of wealth changes with health status. Assuming that additional wealth is more valuable in life than as a bequest, Hammitt (2000) also indicates that the value of statistical life increases as wealth rises if individuals are risk averse or risk neutral. Dachraoui et al. (2004) examine the effect of risk aversion on the willingness to pay for self-protection. They show that risk aversion increases (resp. decreases) this willingness to pay Footnote 1 only when the baseline probability of loss is above (resp. below) \(\frac {1}{2}\). Crainich et al. (2015) specify this relationship by showing that the willingness to pay to reduce the probability of disease is the product of the willingness to pay under risk neutrality and an adjustment factor that depends on both risk aversion and downside risk aversion. The impact of background risks on the willingness to pay for self-protection has been analyzed by Eeckhoudt & Hammitt (2001) and by Bleichrodt et al. (2003). Eeckhoudt & Hammitt (2001) examine how various sources of mortality affect the willingness to pay for reducing the probability of death while Bleichrodt et al. (2003) highlights the conditions under which the willingness to pay to reduce the probability of a given disease increases as the probability and severity of comorbidities rise. The impact of information about individuals’ heterogeneity on the willingness to pay for projects reducing the probability of death has been examined by Hammitt & Treich (2007) who show that when the reduction in the probability of death differs across individuals, the average willingness to pay decreases with information about individual risk change. Our paper complements these theoretical developments by examining the effects that personalized health information should have on the average willingness to pay for disease prevention actions. The ever growing availability of personal health related information (due for instance to the development of genetic testing, to the multiplication of public health campaigns and to the increasing use of the internet for medical purposes) changes the way diseases are perceived by individuals and thus modifies their propensity to prevent them.Footnote 2 In this paper, we examine prevention actions reducing either the probability of disease or its severity (self-protection and self-insurance, respectively, in Ehrlich and Becker’s 1972 terminology) and focus on personal information about the baseline probability of disease (provided by genetic predisposition tests for instance).Footnote 3 With such information, individuals do not make medical decisions based on the prevalence of the disease (i.e. based on average information) but according to personal characteristics instead. Whether they learn that their intrinsic probability to develop a given disease is higher or lower than the prevalence of the disease, individuals’ willingness to pay for prevention should, in theory, be modified. Consequently, an interesting question related to the development of personalized health information is whether the willingness to pay for prevention based on the average information (i.e. in the absence of personalized information) is higher than the average willingness to pay for prevention with personalized information. In this paper we address this question, the answer to which determines how the relevance of a prevention program is affected by the development of genetic testing. Note that in contrast to Hammitt & Treich (2007), the heterogeneity we deal with is about the baseline probability of disease (i.e. because this is the information that genetic tests provide) and not about the reduction in the probability of death resulting from a public project. In the first model developed in the paper, we show that personalized information about the probability of disease raises the average willingness to pay for self-protection (Section 2). We then show (Section 3) that the same information raises the average willingness to pay for self-insurance if the marginal utility of wealth rises with health. Section 4 concludes.",5
55.0,1.0,Journal of Risk and Uncertainty,17 November 2017,https://link.springer.com/article/10.1007/s11166-017-9263-1,Dynamics in risk taking with a low-probability hazard,August 2017,Andrew Royal,,,Male,Unknown,Unknown,Male,"According to economic theory, insurance markets emerge in response to the willingness of risk-averse consumers to pay a premium to be protected from costly random events. Low voluntary demand for insurance covering flood (Dixon et al. 2006; Browne & Hoyt 2000) and earthquake (Palm & Hodgson 1992) damages, often in the presence of discounted premiums, however, brings to question whether households are insensitive to rare events. Increasing numbers of homeowners moving to high-risk areas (Kunreuther & Michel-Kerjan 2009) and a greater frequency of extreme weather events (Pielke & Downton 2000) have led to increased damages resulting from natural disasters over the past few decades. This happens as federal and state exposure to disaster risks rises through subsidized insurance programs and a growing market share of state-run residual insurance markets in high-risk states such as Florida (Kunreuther & Michel-Kerjan 2009; Kunreuther & Pauly 2006). A better understanding of how consumers react to repeated low probability risks can contribute to important policy discussions about the proper level of government subsidy and relief funding (Shavell 2014), and the optimal structure of insurance contracts (Michel-Kerjan & Kunreuther 2011). Whether market behavior such as low insurance takeup results from households’ perceptions of risks is difficult to judge using only observational data from the field. Some of the reasons for this include the difficulty in estimating the actual risk faced by individual households, the confounding effect that regulatory institutions have on market outcomes, and the difficulty in determining the causal relationships between risk perceptions and risk mitigation. This paper avoids some of these obstacles by studying risk-taking behavior in a controlled laboratory experiment that uses real monetary risks to motivate thoughtful decision-making. Subjects in the experiment owned multiple digital “properties” for one or more “decision periods” and were told that the value of one of these properties at the end of a given decision period would determine their final payment for participating. Each property had a chance of experiencing a “disaster” that could eliminate either half or all of a property’s value, resulting in the subject earning a significantly lower final payment. Given the information they received about each property, subjects chose whether to purchase insurance coverage by paying a premium that was deducted from their final payment. Subjects in the experiment chose whether to purchase insurance while facing a “one-shot” risk and made a sequence of insurance decisions while facing a repeated risk. In both settings, subjects learned about the risk through experience. However, in the repeated choice setting, subjects learned directly from experience because each experienced outcome had a potential impact on one’s final income; in the one-shot setting subjects learned indirectly by observing a set of past outcomes generated by the risk. I use several different modeling approaches to understand how subjects learned about and respond to risks across different choice environments. Economists traditionally use models of rational belief learning in conjunction with expected utility theory (EUT) to understand consumer behavior in the presence of uncertainty. An EUT decision-maker responds to hazards by first forming beliefs about the probability that the hazard will occur and then weighing the expected benefits of reducing hazard exposure against the cost of taking protective measures. EUT combines naturally with models of belief formation that fall under headings such as rational expectations, adaptive expectations, or Bayesian updating. These common approaches share the assumption that individuals rationally integrate all available information and act on beliefs that minimize expected prediction error. Although this is often a reasonable and parsimonious approach to understanding market behavior, its predictions are sometimes at odds with empirical studies of insurance demand. Browne et al. (2015), for instance, compare the demand for flood insurance and bicycle theft insurance using data from a German insurer and find that there is an unexplained preference toward insuring against the higher-frequency, lower-consequence bike thefts even after conditioning on the expected loss of both hazards. The preference to insure against higher-frequency hazards runs counter to EUT, which typically assumes that individuals are risk-averse and therefore should have greater aversion to low-probability, high-consequence hazards. There is also evidence that housing markets fail to capitalize flood risks into prices of properties that reside in 500-year floodplains and that 100-year floodplains respond disproportionately to recent floods that occurred within the preceding decade (Atreya et al. 2015; Bin & Landry 2013). Insensitivity to low-probability flood risks may be a consequence of households being uninformed about the hazard probabilities and other details that are required to properly identify the costs and benefits of risk mitigation (Palm & Hodgson 1992; Kunreuther & Slovic 1978). These are just some of the reasons one might expect households to fail to live up to the predictions of a rational theory of decision-making. Prospect theory (PT) is a competing model for decision-making under risk that has in many cases proven to be more robust than EUT in explaining decisions made by subjects in laboratory environments. PT emerged out of commonly observed patterns documented in a series of choice experiments (Kahneman & Tversky 1979; Tversky & Kahneman 1992). One important finding supported by PT is that subjects in laboratory experiments tend to overweight low-probability risks. If these findings generalize, then we would expect insurance takeup for low-probability risks to exceed insurance takeup for high-probability ones. More recent laboratory studies, however, show that the pattern of overweighting low probabilities observed in the original PT choice experiments occurs only if subjects learn about risky prospects from description. In Kahneman & Tversky (1979) subjects made a series of choices between two risky prospects, prospect A and prospect B. For every one of these decisions, subjects received a complete description of the risk associated with each prospect (example: “Choose between A: $6,000 with 0.45 probability or B: $3,000 with 0.90 probability). While decisions made in this setting (from description) produced evidence that subjects overweight low-probability risks, Hertwig et al. (2004), Barron & Erev (2003), and Hertwig & Erev (2009) show that this pattern reverses when subjects instead learn about risks from experience. These experiments studied decisions made from experience by observing how subjects behaved as they learned about risk by sampling random outcomes from unknown prospects— prospects for which subjects had no prior knowledge of the underlying probabilities. Subjects in Barron and Erev’s experiment made repeated decisions about whether to be paid from one of two unknown prospects. After each choice, subjects observed the random payment generated by each prospect and used that information to inform their next choice. Hertwig et al.’s experiment allowed subjects to randomly sample outcomes from each prospect before deciding which one to select to count for payment. While Barron and Erev’s experiment studied learning in a repeated choice setting, Hertwig et al. studied learning by observing one-shot decisions. In both settings, subjects underweighted low-probability risks compared with high-probability ones. The contrast between these findings and the overweighting of low probabilities observed in the PT experiments is commonly referred to as the “experience-description gap.” The experiment reported in this paper documents only decisions made from experience: subjects made insurance choices based on an observed sequence of disasters. Because subjects did not make comparable decisions from description and they faced only relatively low-probability risks, the experiment is not a proper test of the experience-description gap. It instead focuses on identifying specific features of the learning process that uniquely influence decisions from experience and, perhaps, explain why individuals underweight low probability risks in this setting. The features I examine most closely are (1) attention to the recency of outcomes, (2) differences in interpretation of direct versus indirect experience, (3) misinterpretation of random outcomes using the representative heuristic, and (4) the tendency to repeat prior choices. Table 1 classifies three environments in which researchers typically study decision-making under risk; it also shows which of these are included in the current study and the responses to low-probability risks observed in prior studies pertaining to each environment. The experiment reported here does include some element of description (in addition to experience) in that subjects received some information about the risk before observing a random sample of outcomes. This feature is shared by an experiment reported by Jessup et al. (2008), who find that presenting prior information to subjects makes them more sensitive to low-probability risks in early decision periods, though underweighting nevertheless persists in later decision periods.
 There are some existing studies that draw a similar comparison between one-shot and repeated choices. Erev et al. (2010) and Hertwig & Erev (2009) both document consistent underweighting of low-probability risks across one-shot and repeated choice environments. Erev et al. (2010) present results from a competition to determine which model offers the best out of sample predictions of behavior in an experiment where subjects chose among risky prospects in all three of the environments outlined in Table 1. They discover that a stochastic version of PT offers the best predictions for decisions made from description and a weighted combination of several different heuristic decision rules gives the best predictions for decisions made from experience. In a similar study, Hau et al. (2008) compare model predictions for decisions made from experience, giving special attention to heuristics subjects may use for responding to repeated risks. They find that the most successful predictions come from a two-stage version of PT and a maximax decision heuristic in which the decision-maker simply selects the risky prospect with the most favorable experienced outcome. The current study focuses primarily on models that fit into the Bayesian expected utility framework because these sorts of models serve as the foundation for the majority of economic analysis used to inform policy. I combine Bayesian learning models with a constant-relative-risk-aversion (CRRA) utility function to predict subjects’ choices. I propose several modifications of the Bayesian model that are motivated by either empirical regularities or potentially fruitful theoretical hypotheses. Each modification nests the standard Bayesian model as a special case. Beyond simply determining which decision model provides the best statistical fit to subjects’ behavior, I use the models to interpret how decisions and risk preferences are affected by the choice environment. The focus on modeling risk taking in a repeated exposure setting is what distinguishes this study from most previous experiments on insurance takeup. Examples of studies that examine insurance choice in descriptive settings are Slovic et al. (1977) and Laury et al. (2009). Interestingly, Laury et al. (2009) find that subjects in their experiment preferred to insure against low-probability risks. This is perhaps a manifestation of the experience-description gap, as participants in an experienced risk insurance experiment reported in Ganderton et al. (2000) did the opposite by insuring more against high-probability risks. Other experiments that examine insurance choices in an experienced risk setting include Shafran (2011) and Meyer (2012). The study by Meyer finds that recent losses are responsible for reductions in risk taking, particularly if losses were unprotected. Subjects in Shafran’s experiment engaged in greater risk taking for low-probability, high-consequence hazards than for high-probability, low-consequence hazards with the same expected outcome. Shafran also finds that recent losses were negatively correlated with subjects’ risk taking in subsequent periods even though prior losses did not convey any information about the underlying risk. Subjects in the current study reduced their risk taking by purchasing insurance more often following disasters, though model estimates show that subjects weighted observed outcomes proportionately whether or not they occurred recently. There is no evidence that subjects exhibited recency bias, holding an irrational belief that more recent events were more informative; increases in insurance takeup immediately following a loss were proportional to the increase one would expect from a Bayesian expected utility maximizer. There is some evidence, however, that subjects weighted directly experienced outcomes more heavily than those experienced indirectly. Unexplained autocorrelation in subjects’ choices, called inertia, also predicted decisions in the repeated choice environment. Subjects were risk-averse on average, though they appear to have been less risk-averse in the repeated choice environment than in the one-shot setting. In Section 3.2.7 I discuss how the lower revealed risk aversion in the repeated choice portion of the experiment could be a consequence of inertia across choices. I conclude by discussing insights for public policy in Section 6.",3
55.0,1.0,Journal of Risk and Uncertainty,11 November 2017,https://link.springer.com/article/10.1007/s11166-017-9264-0,Learning-by-doing in an ambiguous environment,August 2017,Jim Engle-Warnick,Sonia Laszlo,,Male,Female,Unknown,Mix,,
55.0,2.0,Journal of Risk and Uncertainty,03 February 2018,https://link.springer.com/article/10.1007/s11166-017-9270-2,Giving in the face of risk,December 2017,Elena Cettolin,Arno Riedl,Giang Tran,Female,Male,,Mix,,
55.0,2.0,Journal of Risk and Uncertainty,27 January 2018,https://link.springer.com/article/10.1007/s11166-018-9272-8,Time preferences and consumer behavior,December 2017,David Bradford,Charles Courtemanche,Christopher Ruhm,Male,Male,Male,Male,"Time preferences are generally considered to be a primitive of economic decision-making and so are predicted to affect behavior across many dimensions. Economic theories assume that less-patient individuals are less likely to spend money, time, or resources now to yield benefits in the future. Evidence from behavioral economics and psychology suggests that many individuals exhibit present bias, placing a disproportionate weight on immediate costs and benefits compared to more temporally distant outcomes. Present bias can yield time inconsistency and failure to stick to plans; the optimal decision at one point in time may no longer remain optimal as proximate time periods become increasingly salient. For instance, time-inconsistent consumers may plan to invest in energy-efficient technologies (e.g. hybrid cars, home energy improvements), but postpone the investment when the costs become immediate. This inconsistency is a potential explanation of the energy efficiency gap and provides a possible rationale for government intervention (Gillingham and Palmer 2014). Time inconsistency affects health policies, as present-biased consumers may underinvest in health, e.g. by eating too much, exercising too little, neglecting preventive health care, or failing to buy insurance. Policies designed to encourage retirement savings may need to accommodate the potential for time inconsistency (Carroll et al. 2009). This paper investigates how elicited time preferences (defined across both time-consistent and time-inconsistent frames) predict consumer behavior across multiple domains. Since no secondary data exist that contain the wide range of information necessary for this analysis, we field our own survey and measure time preferences with an incentive-compatible choice experiment about intertemporal tradeoffs, paying out for one of the choices for randomly-selected respondents to mitigate any hypothetical bias. We use multiple price list (MPL) questions to compute the standard discount factor, both the present bias and long-run components of a quasi-hyperbolic (β, δ) specification (as in Laibson 1997), and the coefficient of relative risk aversion (CRRA). This allows for an empirical test for associations of both time-consistent impatience and time inconsistency with outcomes related to self-reported health, health behaviors, health insurance, use of energy-efficient technologies, and financial decisions, while holding risk preferences constant to avoid erroneously conflating time and risk preferences. There is debate in the literature about how well discounting measures elicited from small-stakes financial tradeoffs actually reflect the rates of time preference used in real-world decisions. One concern is that the stakes are not high enough for individuals to respond accurately. Additionally, time preferences may vary across different domains (Chapman and Elstein 1995). For instance, questions about intertemporal health choices may predict health behavior better than they predict financial choices. To address these challenges, we consider alternative measures of time preferences, including questions specifically tailored to health decisions. These alternates allow us to assess how measurement error affects the estimates from regressions using elicited time preferences from the monetary domain. Our data show that time preferences elicited using MPL questions over monetary payouts are associated with many outcomes related to health, energy use, and finances. In regressions controlling for demographic variables and risk preferences, the time-consistent discount factor, δ, is significantly correlated in the predicted direction with good or better overall and mental health, activity limitations, having health insurance, exercise, snacking, smoking, binge drinking, seatbelt use, refusal to report weight or height, installing energy-efficient compact fluorescent lighting, credit card balance, and having retirement savings. The present bias discount factor, β, is statistically associated in the expected direction with good or better overall health, excellent overall health, exercise, smoking, binge drinking, seatbelt use, driving a fuel-efficient car, adding weather stripping to one’s home, and having non-retirement savings. The elicited discount factors from the monetary domain are only modestly associated with the alternative time preferences variables, suggesting that our estimated relationships between time preferences and consumer behaviors may be conservative. Our study’s contribution is four-fold. First, we examine how survey-elicited time preferences are related to a heterogeneous set of real-world outcomes, including those across the domains of health, energy, and financial decisions. To our knowledge, we are the first to estimate the link between time preferences and many of our outcomes: self-assessed physical and mental health, health-related limitations, health insurance, snacking, binge drinking, sunscreen and seatbelt use, automobile fuel economy, actual (as opposed to hypothetical) home energy-efficiency investments, thermostat temperature settings, overall home energy use, and having an energy audit. This is important because if the role of time preferences varies by frame, it is not clear to what degree the results for previously-studied outcomes (such as smoking and body mass index) generalize to other settings. We motivate our analyses with testable hypotheses about which types of behaviors are most likely to be influenced by time preferences. Second, our study allows for quasi-hyperbolic discounting and disentangles whether the diverse group of observed relationships is driven by time-consistent preferences (δ), present bias (β), or both. We are unaware of prior research on the relationship between elicited time preference inconsistency and any of our outcomes, with the exception of smoking, BMI, and credit card debt. We hypothesize and test which outcomes are particularly likely to be correlated with present bias, as opposed to general time-consistent discounting. Our third contribution relates to methodological advancements in the survey design. We improve on previous studies linking time preferences to consumer outcomes by surveying respondents that are representative of the U.S. population, using an incentivized measure of time preferences, studying many decision domains simultaneously, and using a greater sample size than studies relying on a student population. While some previous studies have incorporated some of these features, to our knowledge none have featured all these innovations at once. Fourth, we assess how discount factors based on monetary outcomes compare to alternative time preference measures across different domains. These alternate measures include self-reported patience and willpower, as well as both elicited and self-reported measures specific to the health domain. These measures are only modestly associated with elicited money-based time preferences, implying that they provide new information related to intertemporal choices.",57
55.0,2.0,Journal of Risk and Uncertainty,05 January 2018,https://link.springer.com/article/10.1007/s11166-017-9268-9,Regret theory and risk attitudes,December 2017,Jeeva Somasundaram,Enrico Diecidue,,Male,Male,Unknown,Male,"In economics and management, the classic model of expected utility (von Neumann and Morgenstern 1947) is the benchmark for representing preferences under risk and uncertainty. Yet several models have been introduced since the 1970s to accommodate some of expected utility’s descriptive failures (for reviews see Starmer 2000; Wakker 2010). Regret theory (Bell 1982; Loomes and Sugden 1982) is one of the most popular alternatives to expected utility (EU). Regret theory is based on the intuition that a decision maker (DM)—when choosing among risky objects (e.g., lotteries, gambles, and investments)—is concerned not only about the payoff he receives but also about the foregone payoff, i.e., had he chosen differently.Footnote 1 Regret theory has a simple structure: a utility function capturing attitudes toward outcomes and a function capturing the effect of regret. Despite this structural simplicity, regret theory can account for many of the empirical violations of EU (Loomes and Sugden 1982). The key to explaining these violations is the psychological intuition that most decision makers are by nature regret averse. Regret theory’s intuitive content and explanatory power make it well suited to real-world applications. For example, Barberis and Thaler (2006), Gollier and Salanié (2006), Muermann and Volkman (2006), Michenaud and Solnik (2008), and Qin (2015) have applied the notion of regret to financial and insurance decisions. Perakis and Roels (2008) reinterpreted the newsvendor model in terms of regret, and Filiz-Ozbay and Ozbay (2007) and Engelbrecht-Wiggans and Katok (2008) proposed models in auction theory that rely on regret. Diecidue and Tang (2012), Nasiry and Popescu (2012), and Viefers and Strack (2014) focused on dynamic applications of regret. To deploy regret theory for decision analysis, the risk attitudes under that theory should be well understood. Under EU, risk attitudes are fully captured by “utility curvature” (Wakker 2010); in more complex models, such as prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992; Tversky and Wakker 1995), risk attitudes are captured by the interaction between utility and a probability weighting function that yields a fourfold pattern of risk attitudes. Under regret theory, however, it is not clear how the interaction between utility and regret is related to risk attitudes. Bell (1983) formalized the risk premium under regret theory and showed that it consists of two distinct components: a resolution premium and a regret premium. However, Bell (1983) did not suggest an empirical method suitable for measuring these two components (Anand 1985). In this paper, we provide an analytical expression for both the resolution premium and the regret premium. These expressions enable—for the first time—a precise characterization of risk attitudes under regret theory and thus rigorous predictions about the risk attitudes of a regret-averse decision maker. We predict that regret-averse DMs will be risk seeking for low probabilities of gains and risk averse for high probabilities; we also postulate that risk attitudes are reinforced by feedback about the foregone payoff. We introduce a method to measure the risk premium under regret theory. This method allows us then to compute both the resolution and regret premiums and thereby to understand the effect of feedback on regret attitudes. Finally, we design an experiment to estimate empirically the risk premium’s components and to test our predictions about risk attitudes. The experiment serves also as a descriptive test of regret theory. The data support regret aversion as a robust empirical phenomenon. However, we find no significant support for the risk attitude predictions of regret theory. We also discover that immediate feedback polarizes regret attitudes: It increases the regret aversion of regret-averse subjects but it reduces the regret aversion within the entire subject pool. The paper is organized as follows. Section 2 introduces the notation and the definition of regret theory; in Section 3, we first derive analytically the two components of the risk premium under regret theory and then introduce a measurement that distinguishes the two components. Building on this new measurement, Section 4 derives formal predictions for the risk attitudes of a regret-averse decision maker; the experiment described in Section 5 tests those predictions. We discuss the experiment’s results and conclude in Section 6.",25
55.0,2.0,Journal of Risk and Uncertainty,13 December 2017,https://link.springer.com/article/10.1007/s11166-017-9267-x,Baseline risk and marginal willingness to pay for health risk reduction,December 2017,Shelby Gerking,Wiktor Adamowicz,Marcella Veronesi,,Male,Female,Mix,,
55.0,2.0,Journal of Risk and Uncertainty,07 February 2018,https://link.springer.com/article/10.1007/s11166-017-9269-8,Loss aversion leading to advantageous selection,December 2017,Christina Aperjis,Filippo Balestrieri,,Female,Male,Unknown,Mix,,
56.0,1.0,Journal of Risk and Uncertainty,27 March 2018,https://link.springer.com/article/10.1007/s11166-018-9274-6,Complexity in risk elicitation may affect the conclusions: A demonstration using gender differences,February 2018,Gary Charness,Catherine Eckel,Agne Kajackaite,Male,Female,Female,Mix,,
56.0,1.0,Journal of Risk and Uncertainty,23 February 2018,https://link.springer.com/article/10.1007/s11166-018-9271-9,Goals as reference points in marathon running: A novel test of reference dependence,February 2018,Alex Markle,George Wu,Aaron Sackett,Male,Male,Male,Male,"A recent article in The Chronicle of Higher Education documented the enormous scholarly impact of Prospect Theory in economics, psychology, and beyond (Goldstein 2011). Perhaps the main reason Prospect Theory has been so influential is the theory’s proposition that individuals evaluate outcomes in relative terms, as changes from a neutral reference point rather than final states as classical economic models assume (Kahneman and Tversky 1979; Tversky and Kahneman 1992). Indeed, reference-dependent preferences account for a wide range of empirical phenomena, including the endowment effect (Kahneman et al. 1990), status quo bias (Samuelson and Zeckhauser 1988), disposition effect (Shefrin and Statman 1985), equity premium puzzle (Benartzi and Thaler 1995), and extreme risk aversion for prospects involving the possibility of a loss (Kahneman and Lovallo 1993; Rabin 2000). Empirical investigations of Prospect Theory and reference dependence have largely taken the reference point to be the status quo.Footnote 1 In many domains, this choice is sensible because the status quo is the most reasonable or even the only reasonable reference point from which to evaluate a particular outcome. However, in other settings, points of comparison besides the status quo are plausible reference points, such as “aspirations, expectations, norms, and social comparisons” (Tversky and Kahneman 1991). While these alternative reference points have been the subject of some empirical study, such investigations face conceptual and methodological challenges. Conceptually, the field currently lacks the principles to guide a researcher challenged with identifying which of any number of plausible reference points is used to evaluate an outcome. For example, the reference point for selling a house might be the purchase price, an offer that was recently turned down, the sales price of a neighbor’s property, a round number such as $500,000, or the average sales price of comparable houses. Empirically, even if the researcher knows which type of reference point drives evaluation, the level of that reference point may be difficult to observe and measure. Expectations, for example, are generally subjective and thus hidden from the researcher. In this paper, we investigate reference dependence in marathon running.Footnote 2 Although this setting is distinct from those traditionally used to study Prospect Theory, we argue that it provides useful and unique insights into how non-status quo reference points operate. While people run marathons for a variety of reasons (e.g., to finish a long race or to support a charity), surpassing a predetermined time goal defines success for many marathoners. Indeed, marathon training guides often emphasize the importance of time goals as a motivational tool and provide heuristics and calculators for setting appropriate goals (e.g., Higdon 2011). In the present research, we test the specific proposition that time goals serve as reference points in the evaluation of marathon performance.Footnote 3 In support of this proposition, we find that satisfaction as a function of performance relative to the time goal exhibits loss aversion and diminishing sensitivity, mirroring the classic S-shape of the Prospect Theory value function. However, we also document a jump in satisfaction at the reference point that departs from Prospect Theory’s continuous value function. For marathon runners, aversion to losses thus appears to have two sources: a value function that has a steeper slope in losses than in gains, as in the classical Prospect Theory account, and a discrete jump at the reference point itself. Our novel setting also yields a number of conceptual insights, filling gaps in the existing literature and generalizing the scope of Prospect Theory in several ways. First, we employ satisfaction as our dependent measure instead of estimating the value function from choice percentages or cash equivalents. While studies of overt behavior can speak to the role of reference dependence in the determination of decision utility (the weight given to outcomes in choice), they cannot provide insight into how, if at all, reference dependence shapes experienced utility (Kahneman 1999). In our study, we examine both pre-marathon predictions of satisfaction and post-marathon measures of satisfaction and find that loss aversion and diminishing sensitivity are present in both predicted and experienced satisfaction, contrary to the results of Kermer et al. (2006). We also examine the role of multiple reference points. In our context, a time goal is one of several possible reference points runners may use to evaluate their performance. Experienced marathoners, for instance, may compare their current performance against their most recent or best marathon time. Indeed, we find that satisfaction among experienced marathoners is driven by comparing performance with both their time goal and previous marathon times. In sum, this study contributes to multiple strands of research on reference dependence. Empirically, our results add to a growing list of field demonstrations of Prospect Theory (Camerer 2005; Camerer et al. 1997; Fryer et al. 2012; Genesove and Mayer 2001; Pope and Schweitzer 2011; Post et al. 2008; see Allen et al. 2017), for a more extensive list). This body of work suggests that reference dependence extends beyond small-stakes, stylized laboratory experiments, contrary to the criticism of List (2003).Footnote 4 Moreover, we provide an estimation of the Prospect Theory value function using data collected outside of the laboratory. Although many papers have estimated the Prospect Theory value function from laboratory data (e.g., Abdellaoui 2000; Gächter et al. 2007; Ho and Zhang 2008; Wu and Markle 2008), few have done so using field data (for exceptions, see Post et al. 2008; Tovar 2009). Our paper proceeds as follows. In Section 2, we review the existing literature and position this paper’s contributions relative to that literature. In Section 3, we describe the methods for our empirical investigation. In Section 4, we present the basic results of our study. In Section 5, we estimate the Prospect Theory value function to relate satisfaction to relative performance. In Section 6, we discuss some alternative explanations for our results and present evidence that casts doubt on these accounts. We conclude in Section 7, by summarizing our contributions and discussing some implications and future directions for this research.Footnote 5",47
56.0,1.0,Journal of Risk and Uncertainty,27 March 2018,https://link.springer.com/article/10.1007/s11166-018-9276-4,Corporate apology for environmental damage,February 2018,Ben Gilbert,Alexander James,Jason F. Shogren,Male,Male,Male,Male,"“The responsibility for safety on the drilling rig is Transocean. It is their rig, their equipment, their people, their systems, their safety processes.” 
—Tony Hayward, BP CEO
 
CNN interview, April 28, 2010
 “The explosion and fire aboard the Deepwater Horizon and the resulting oil spill in the Gulf of Mexico never should have happened — and Iam deeply sorry that they did. None of us yet knows why it happened. But whatever the cause, we at BP will do what we can to make certain that an incident like this does not happen again.” 
—Tony Hayward, BP CEO
 
U.S. House Testimony, June 17, 2010
 CEOs often make public apologies when their firm’s products cause harm. These apologies are a powerful way to restore trust and reduce punishment costs in bilateral settings such as medical malpractice contracts (see McMichael et al. 2016), food safety scares, product tampering incidents, automotive scandals (e.g., VW emissions, Ford and Firestone tires, GM ignition switches, Toyota accelerator/braking risks), and major security/data breaches (e.g., Target, Equifax, Yahoo mail). Highly visible apologies from CEOs such as Apple’s Steve Jobs and Facebook’s Mark Zuckerberg can take many forms, and they play an important role in dealing with adverse risk outcomes (see for example Hearit 2006). An apology can save a company millions of dollars in punishment and gain more goodwill with the public. But what do we know about consumers’ desire for punishment and compensation given public apologies for large-scale multilateral, man-made disasters? Herein we report on results from a novel experimental survey focusing on apologies within a multilateral setting: a firm-caused environmental disaster. For example, the statements in the epigraph by former BP CEO Tony Hayward following the Deepwater Horizon oil spill differ in some obvious and some subtle ways. The obvious difference is that in the first statement, made soon after the April 20, 2010 explosion causing the spill, Hayward shifts the blame to Transocean, the owner of the drilling rig that exploded. In the second statement, Hayward apologizes for the spill and pledges to avoid future harm. More subtle is that the second statement is only a partial apology; Hayward does not accept full responsibility for the damage on behalf of his organization. A similar and now somewhat infamous statement was made by Exxon Chairman Lawrence Rawl ten days after the 1989 Exxon Valdez spill in Prince William Sound:  “I want to tell you how sorry I am that this accident took place.” 
—Lawrence Rawl, Exxon Chairman & CEO
 
Newspaper ads, April 3, 1989
 Could such statements have affected the demand for punishment and compensation from these firms? Rawl’s statement was seen as stoking public outrage over the spill. The statement does not accept responsibility, was made with significant delay, and was made in a passive medium (newspaper advertisements) in which its sincerity could not be evaluated (O’Hara O’Connor 2011); news outlets reported that 40,000 customers subsequently cut up their Exxon credit cards in order to boycott the company (Behar 1990). Conventional wisdom suggests that apologies help resolve conflict and restore trust at reduced cost. This view is supported by a broad literature in economics, law, management, communications, and psychology discussed in more detail below. But this standard view is based on bilateral interactions, which provide too narrow a perspective for large scale environmental disasters. Environmental damages are inherently multilateral; they involve the loss of a public good, with multiple firms or agents potentially at fault, and incidence of costs borne by a large group of victims. This multilateral setting may create strategic reasons for firms to make only “partial” apologies — such as shifting the blame to a third party — at the risk of being less effective. This paper reports results from an experiment that tests how corporate apologies for environmental damage affect the demand for punishment and compensation. Specifically, we ask whether the apology’s content, and the firm’s reputation, affect the outcome. We study two types of punishment outcomes: (1) “personal responses”, which include boycotting, opposing local development, and signing a petition to urge criminal prosecution of a firm, and (2) fines in excess of victim compensation. We also study the demand for compensation when the subject is part of a large group of victims, and compensation is determined through a hypothetical legal settlement. In a 3×3 design, we presented subjects with an oil spill scenario and randomly assigned them to one of three apology treatments (No Apology, Full Apology, Shift the Blame), and one of three firm reputation treatments (No Reputation Information, Good Reputation, Bad Reputation). We then asked subjects about their likely personal responses to the spill, their preferred fine, and their Willingness to Accept compensation (WTA) as part of a class action lawsuit for lost passive use value of the environmental good. Subjects were not told the scenario and firms were fictional until the survey was complete. We find that full apologies and better reputation reduce the intensity of personal responses. Apologies and reputation seem to be substitutes in affecting personal responses, however, with reputation being significantly more important. The relationship becomes more complicated for the cases of the fine and WTA. For the fine, if the firm has a good reputation, people ask for a smaller fine — especially if the firm shifts the blame. For WTA, if the firm has a good reputation, then giving a full apology has no additional impact on WTA — the impact is the same if the firm remained silent. But we find weak evidence that a person’s WTA increases if (a) the firm accepts blame, irrespective of the firm’s reputation, or (b) the firm has a bad reputation and shifts the blame. Subjects seem to either latch on to the admission of guilt, or they want to punish the clear bad actors. These results help explain why firms make investments in corporate social responsibility (CSR) that provide cover for socially irresponsible behavior, as in Kotchen and Moon (2012), or engage in “greenwashing”, or the deliberate spread of misleading information about their environmental record (Cherry and Sneirson 2011; Delmas and Burbano 2011). The results also illustrate why many corporate apologies are often only “partial” apologies that shift the blame, downplay potential damages, or fail to fully accept responsibility, as was true with the apologies for both the Deepwater Horizon and the Exxon Valdez oil spills. The rest of the paper is organized as follows. The next section discusses related literature on apologies from several disciplines. Section 3 describes the experiment and methods. Section 4 discusses the results, and Section 5 concludes.",10
56.0,1.0,Journal of Risk and Uncertainty,28 March 2018,https://link.springer.com/article/10.1007/s11166-018-9273-7,Making the Anscombe-Aumann approach to ambiguity suitable for descriptive applications,February 2018,Stefan Trautmann,Peter P. Wakker,,Male,Male,Unknown,Male,"This section presents a basic version of the AA model so as to motivate the method that we introduce in the next section. A formalized and general version of the AA model will be presented in the theoretical part of the paper, starting in Section 4. Figure 1a depicts a standard “Savage” act for decision under uncertainty. E1,…,E
n
 denote mutually exclusive and exhaustive events. That is, exactly one will obtain, but it is uncertain which one. Following AA, we assume that a horse race takes place with n horses participating, and exactly one will win. Event E
i
 refers to horse i winning. The act yields consequence x
i
 if event E
i
 obtains. We mostly assume that consequences are monetary, although they can be anything. U(x
i
) is the utility of consequence x
i
. V denotes a general functional that represents preferences. It is increasing in all its arguments. Savage (1954) considered the case where V gives subjective expected utility. Nowadays, there is much interest in ambiguity theories, where V can be any such theory, e.g., a multiple prior theory. Such theories are also the topic of this paper.
 Traditional (one-stage) choice objects In decision under risk, we assume probabilities to be known. Then choices are between lotteries (probability distributions). Figure 1b denotes a lottery yielding x
j
 with probability p
j
. Following AA, we assume that a roulette wheel is spun to generate the probabilities. Besides the expected utility evaluation depicted, many deviating models have been studied (Starmer 2000). Figure 2 depicts an act in AA’s model. Both uncertainty and risk are involved. The act is like a Savage act in Fig. 1a, but now consequences are lotteries, i.e., probability distributions over “outcomes” x
i
j
. Uncertainty is resolved in two stages. First nature chooses which event E
i
 obtains, resulting in the corresponding lottery. Next the lottery is resolved, resulting in outcome x
i
j
 with probability p
i
j
, j = 1,…,m.Footnote 3 In AA’s model, acts are evaluated as depicted. First, every lottery of the second stage is evaluated by its expected utility. Next, an ambiguity functional V is applied to those expected utilities as it was to utilities in Fig. 1a. The evaluation of the ambiguity by the functional V is of central interest in the modern ambiguity literature. The evaluation of the lotteries only serves to facilitate the analysis of ambiguity in the first stage. The evaluation of each lottery in the second stage is independent of what happens at the other branches in the figure. We can, for instance, replace each lottery by its certainty equivalent derived “in isolation” in Fig. 1b, and then evaluate the resulting ambiguous act as in Fig. 1a. That is, we are using backward induction here.
 An Anscombe-Aumann act and its evaluation We list the two assumptions made, and add two more: (1) lotteries, being unambiguous, are evaluated using expected utility (EU); (2) backward induction is used to evaluate the two stages; (3) there is no reference dependence, with gains and losses treated the same; (4) there is universal ambiguity aversion. The last two assumptions concern ambiguity and are, therefore, of central interest. They are called substantive. Assumptions 1 and 2 define the AA model, with its two-stage structure. They only serve to simplify the mathematical analysis and are, therefore, called ancillary. The purpose of this paper is descriptive. We, therefore, wish to avoid descriptive problems of the ancillary assumptions. As regards the first assumption, Allais’ (1953) thought experiment provided the first evidence against EU for risk, later confirmed by many empirical studies. It led to the popular prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992). Surveys of violations of EU for risk include Birnbaum (2008), Edwards (1954), Fehr-Duda and Epper (2012), Fox et al. (2015), Schmidt (2004), Slovic et al. (1988), and Starmer (2000). In view of the many violations of EU found, Assumption (1) is currently considered to be descriptively unsatisfactory. Several authors argued that it is also normatively undesirable (Allais 1953; Machina 1989). Assumption (2), backward induction, is a kind of monotonicity condition. If we only focus on consequences that are sure money amounts (degenerate lotteries; Fig. 1a), then the condition is uncontroversial. However, it becomes debatable if consequences are nondegenerate lotteries as in Fig. 2. Then the condition implies that the decision maker’s evaluation of the lottery faced there, i.e., of the act conditional on the event E
i
 that obtained, is independent of what happens outside of E
i
. This is a form of separability rather than of monotonicity (Bommier 2017 p. 106; Machina 1989 p. 1624), which may be undesirable for ambiguous events E
i
. Although most papers using the AA model do not discuss this assumption explicitly, several recent papers have criticized it (Bommier 2017; Bommier et al. 2017 Footnote 7; Cheridito et al. 2015; Machina 2014 p. 385 3rd bulleted point; Saito 2015; Schneider and Schonger 2017; Skiadas 2013 p. 63; Wakker 2010Section 10.7.3). Dynamic optimization principles such as backward induction that are self-evident under expected utility become problematic and cannot all be satisfied under nonexpected utility (Machina 1989). Several authors have therefore argued against backward induction for nonexpected utility on normative grounds.Footnote 4 Many studies have found empirical violations of backward induction.Footnote 5 We conclude that both ancillary assumptions are descriptively problematic and, according to several authors, also normatively problematic. Our rAA model therefore aims to avoid the problems just discussed. We now turn to a detailed outline of the paper. Section 2 explains the rAA model informally, showing how to test AA theories without being affected by violations of the ancillary assumptions. In particular, no two-stage uncertainty as in Fig. 2 occurs in the rAA model, and we only use stimuli as in Fig. 1. An additional advantage of our stimuli is that they are less complex, reducing the burden for subjects and the noise in the data. Dominiak and Schnedler (2011) and Oechssler et al. (2016) tested Schmeidler’s (1989) uncertainty aversion for two-stage acts, and found no clear relations with Ellsberg-type ambiguity aversion. This can be taken as evidence against the descriptive usefulness of two-stage acts. Section 3 illustrates our approach in a simple experiment. Unsurprisingly, we find that losses are treated differently, with more ambiguity seeking, than gains (reference dependence). We have thus tested and falsified the substantive Assumptions 3 and 4. Many studies have demonstrated reference dependence outside of ambiguity, and several have done so within ambiguity.Footnote 6 Our experiment shows it in a simpler way and is the first to have done so for the AA model. It may be conjectured that AA theories could indirectly model the reference dependence found. This conjecture holds true for the smooth model (Klibanoff et al. 2005) and other utility-driven theories of ambiguity.Footnote 7 However, we prove that it does not hold true for most commonly used AA theories, because weak certainty independence, a necessary condition for most theories,Footnote 8 is violated. Baillon and Placido (2017) also tested this condition and also found it violated. Generalizations of these theories are therefore desirable. We turn to those in the next, theoretical, part of the paper, with definitions and basic results in Section 4 and the reference dependent generalization of Schmeidler (1989) in Section 5. Faro (2005, Ch. 3) provided an alternative ambiguity model with reference dependence. Our generalization of Schmeidler’s model can accommodate loss aversion, and ambiguity aversion for gains combined with ambiguity seeking for losses, as in prospect theory. In many applications of ambiguity (asset markets, insurance, health) the gain-loss distinction is important, and descriptive models that assume reference-independent universal ambiguity aversion cannot accommodate this. As regards our finding of violations of weak certainty independence, reference dependence is the only generalization needed to accommodate these violations. Weak certainty independence remains satisfied if we restrict our attention to gains or to losses. Section 6 analyzes loss aversion under ambiguity. A discussion, with implications for existing ambiguity theories, is in Section 7. Section 8 concludes. A model-theoretic isomorphism of the rAA model with the full AA model is in Appendix E. Its implications can be stated in simple terms for experimentalists, without requiring a study of its formal content: Although the rAA model is a submodel of the full AA model, every ambiguity property that can be defined in the full AA model can be tested in the rAA model using the method explained in the next section. No information on ambiguity is lost by restricting to the rAA model. A simple test such as the one in Section 3 can be devised for every ambiguity condition other than weak certainty independence. The first, empirical part of this paper, preceding Section 4, makes empirical studies of the AA model possible, providing an easy recipe. It is accessible to readers with no mathematical background. We postpone formal definitions and results to the second, theoretical part, in Section 4 and further. Given the negative finding in the first part, with violations of most existing AA ambiguity theories, the second part presents a positive result: the first reference-dependent AA theory.",10
56.0,2.0,Journal of Risk and Uncertainty,09 May 2018,https://link.springer.com/article/10.1007/s11166-018-9279-1,Discounting health and money: New evidence using a more robust method,April 2018,Arthur E. Attema,Han Bleichrodt,Valérie Seror,Male,,Female,Mix,,
56.0,2.0,Journal of Risk and Uncertainty,08 May 2018,https://link.springer.com/article/10.1007/s11166-018-9275-5,Social interaction effects: The impact of distributional preferences on risky choices,April 2018,Anita Gantner,Rudolf Kerschbamer,,Female,Male,Unknown,Mix,,
56.0,2.0,Journal of Risk and Uncertainty,10 May 2018,https://link.springer.com/article/10.1007/s11166-018-9277-3,Responsiveness to feedback as a personal trait,April 2018,Thomas Buser,Leonie Gerhards,Joël van der Weele,Male,Female,Male,Mix,,
56.0,2.0,Journal of Risk and Uncertainty,05 May 2018,https://link.springer.com/article/10.1007/s11166-018-9278-2,Evidence for multiple strategies in choice under risk,April 2018,Giorgio Coricelli,Enrico Diecidue,Francesco D. Zaffuto,Male,Male,Male,Male,"In this paper we report on results of an experiment that focuses on multiple strategies of choice in decision making under risk. In this domain, the most popular models of choice are the normative expected utility (EU) model (von Neumann and Morgenstern 1947) and the descriptive cumulative prospect theory (CPT) model (Tversky and Kahneman 1992). The latter has a mathematical foundation and has received extensive empirical analysis (for a review, see Wakker 2010). According to the terminology of Venkatraman et al. (2014), these theories are “compensatory” in that they evaluate prospects as the sum of the utility of outcomes – as weighted either by probabilities (EU) or decision weights (CPT). Beginning with the seminal contribution of Simon (1955), an alternative stream of research has emphasized the role of aspiration levels in decisions under risk; much subsequent work (e.g., Payne et al. 1980; Lopes 1987; Lopes and Oden 1999) has yielded data that support the relevance of aspiration levels, and of the overall probability of winning, for risky choice. Decisions based on aspiration levels are “simplifying” strategies (Venkatraman et al. 2014) that allow one to reduce a decision problem’s complexity (Payne 2005; Payne et al. 1980). Thus individuals aspire to “satisficing” values (Simon 1956) rather than – as in compensatory theories – optimal values. Diecidue and van de Ven (2008) introduced a model that includes aspiration levels within an expected utility framework, a “hybrid” model incorporating both compensatory (EU) and simplifying (aspiration level) strategies.Footnote 1 In addition, several experimental investigations seem to confirm that aspiration levels and overall success probabilities affect decisions made under risk (Payne et al. 1980; Lopes 1987; Lopes and Oden 1999; Payne 2005; Venkatraman et al. 2009, 2014). However, Diecidue et al. (2015) found no evidence of aspiration levels at zero (which most of the literature assumes to exist) and did report evidence of heterogeneity in aspiration levels. Their aggregate data supported CPT, a compensatory approach. It is therefore clearly desirable to test for whether or not individuals do sometimes “aspire” to a zero outcome and also for whether both simplifying and compensatory decision strategies can coexist in practice. With regard to compensatory strategies, our focus is on skewness preferences and their relation to CPT. A decision strategy based on preference for positive skewness is well modeled by CPT through its probability weighting functions, and empirically derived CPT parameters are compatible with a preference for positive skewness (Spiliopoulos and Hertwig 2015; see also Astebro et al. 2014; Ebert 2015; Ebert and Strack 2015, Thm. 1). Decision makers (DMs) with positive skewness preferences opt for the prospect that could achieve extreme positive outcomes.Footnote 2 Although skewness preferences are often discussed in finance,Footnote 3 they remain relatively unexplored in individual decision making under risk.Footnote 4Grossman and Eckel (2015) reported that nearly 90% of their experimental subjects were classified as positive skewness seeking; of these, more than a third took increased risks because of their preference for skewness. Similarly, Astebro et al. (2014) showed that DMs make riskier choices when prospects have positive skewness. The authors explain this finding via the ability of skewness to capture both optimism and “likelihood insensitivity”, behavioral patterns characteristic of risky choices and well modeled under CPT by overweighting the small probabilities of large outcomes. Yet to the best of our knowledge, no study has addressed the possibility of decision strategies incorporating skewness and aspiration levels. Our experimental study has two goals, the first of which is to disentangle decision strategies based on aspiration levels from those based on skewness preferences. Given two prospects with only one of them providing the possibility or the certainty to achieve aspiration levels, we define a DM’s choice to be based on aspiration levels if the prospect chosen is the one giving the chance to achieve aspiration levels. We therefore assume any non-negative real number to be an aspiration level. For that purpose, we use a large set of prospects with several features such that aspiration levels are predictive of choices in some conditions but not in others. When aspiration levels are predictive, the resulting choice patterns characterize a heuristic for reducing the complexity of risky decisions; when those levels are not predictive, we test for whether choices can be explained by skewness preferences. Given that the outcome zero is central in the studies of aspiration levels (Payne et al. 1980; Lopes 1987; Lopes and Oden 1999) we also investigate what role (if any) is played by the zero outcome – for instance, in contexts where zero might be viewed as a winning opportunity. Our second goal is to compare the fit of the most popular models of risky choices (EU and CPT) and, for the first time, to compare these models when augmented by aspiration levels. We document that subjects employ multiple strategies of choice, either simplifying or compensatory, depending on the type of lottery involved. Our model-fitting exercise corroborates these results by showing that – when choices are driven by aspiration levels – CPT is unable to accommodate our empirical evidence and the best-fitting model is one that incorporates both expected utility and aspiration levels (Diecidue and van de Ven 2008). When choices are instead driven by a preference for positive skewness, it turns out that CPT is the best-fitting model. Thus we offer new insights on the limits of compensatory strategies (e.g., EU and CPT) while identifying circumstances under which simplifying strategies are more likely to be undertaken. The paper proceeds as follows. In Section 2 we describe the experiment, whose results are reported in Section 3. In Section 4 we fit our experimental data to theoretical models of choice under risk. Our findings are discussed in Section 5, and Section 6 concludes.",4
56.0,3.0,Journal of Risk and Uncertainty,04 July 2018,https://link.springer.com/article/10.1007/s11166-018-9282-6,"Defaults, normative anchors, and the occurrence of risky and cautious shifts",June 2018,Stephan Jagau,Theo Offerman,,Male,Male,Unknown,Male,"Many important decisions under risk are taken in small groups. Examples include the investments made in clubs, managing a joint asset portfolio (cf. Barber and Odean 2000), decisions made by a company board or a political committee and the decision of a mountaineering party whether to make the final ascent to the top. It is well-known that in such situations choice shifts may occur. A choice shift happens when individuals advocate a risky (safe) decision when acting as part of a group even though they would prefer a safe (risky) alternative decision when acting as individuals. Although there are many examples of risky and cautious shifts, there is little consensus about the behavioral mechanisms that are driving these shifts. In this paper, we consider two prominent mechanisms that may systematically produce choice shifts: (i) rank-dependent utility and (ii) conformity. The explanation based on rank-dependent utility focuses on the extra layer of uncertainty when people decide in groups while it abstracts from the social aspect of the situation. In contrast, conformity ignores the uncertainty dimension and zooms in on the social aspect that distinguishes a group decision from an individual decision. In a laboratory experiment, we investigate which of these two explanations correctly predicts when cautious and when risky shifts are observed. To the best of our knowledge, we are the first who distinguish between the two explanations. Systematic evidence of choice shifts has been reported in psychology since the early sixties (cf. Stoner 1961; Bem et al. 1962; Pruitt 1971; Isenberg 1986).Footnote 1 The prevalence of risky shifts in the early experiments gave rise to diffusion of responsibility theory (Bem et al. 1962, 1964, 1965), which more recently inspired the formal approach of Eliaz et al. (2006) (ERR) based on rank-dependent utility. Diffusion of responsibility theory argues that individuals ‘voting’ for an outcome in a group might feel less responsible for the outcome than if they directly choose in an individual decision and that this might induce them to push for more risky prospects than they would choose in an individual-decision problem. The psychological cause of this behavior is held to lie in a feeling of disappointment following a failure to realize the good outcome in a risky prospect. When choosing in a group — the argument goes — individuals account less for this potential disappointment since their vote for a prospect matters less for the outcome than their choice in an individual-decision problem would (cf. Pruitt 1971). Starting from Nordhøy (1962) and Stoner (1968), the regular occurrence of cautious shifts in later studies that cannot be accommodated by diffusion of responsibility theory was seen as a strong empirical reason to doubt the validity of this approach. ERR formalize the intuition behind diffusion of responsibility theory and show how a generalized model based on rank-dependent utility (RDU) preferences can explain risky and cautious shifts. They achieve this by considering the compound lotteries an individual expects to result from the group decision conditional on her own vote as the object of evaluation. These compound lotteries account both for the exogenous risk emanating from the random processes described by the prospects, but also the endogenous risk deriving from other group members’ influence on the group decision. E.g., if for a binary group decision between a risky and a safe prospect a person votes for the safe prospect, she effectively chooses a prospect yielding the safe prospect with the probability that the group choice becomes safe if she voted safe and the risky prospect with the probability that the group choice becomes risky if she voted safe. The classical diffusion of responsibility theory ignores this additional layer of uncertainty and implicitly assumes that individuals base their choice on the primitive lotteries presented to the group — even where they do not have full influence regarding the outcome of the group choice. For an exemplary group-decision problem, we demonstrate that ERR’s RDU model accommodates cautious shifts. To see this, suppose an individual expects the safe choice to be implemented with certainty if her decision is not pivotal. This would be the case, for instance, if the decision rule is unanimous decision and the fallback outcome under disagreement is the safe prospect. Then, voting for risky in the group decision generates a compound lottery in which the probability of failing to receive the good outcome of the risky prospect is higher than in the original risky prospect. Now assume the decision maker maximizes RDU preferences with a strictly convex (gain-rank) weighting function such that she overweights probabilities attached to the bad outcomes of a lottery relative to the good outcomes. Then she might well prefer the risky choice if deciding on her own and vote for the safe choice if deciding in the group. The resulting cautious shift in a group decision is in this sense similar to the choice pattern generated in the Allais paradox; both patterns point to a violation of expected-utility theory’s independence axiom. ERR show how rank-dependent utility may systematically produce cautious and risky shifts. While their results imply that assuming a certain type of RDU preferences is sufficient for choice shifts in group decisions, similar results can be achieved using other types of preferences outside the RDU class. Recently, Dillenberger and Raymond (2016) have generalized ERR’s approach. They show that the choice-shift pattern predicted by ERR is exhibited by a larger class of preferences that includes ERR’s RDU preferences, and they provide a set of axioms that is necessary and sufficient for preferences to cause choice shifts in group decisions. In contrast, a competing explanation for choice shifts that has been developed in psychology assumes that people have a taste for conformity. Asch’s line judgments experiments first showed the profound effect that social pressure can have on individuals’ reported judgments. This taste for conformity can arise for a number of reasons that will typically lead to different behavioral predictions. In a survey, Cialdini and Goldstein (2004) emphasize two goals that people may implicitly or explicitly pursue when they respond to social pressure. First, people may pursue an accuracy goal. That is, when they are unsure of the appropriate choice in a social situation, they may revise their intended choice in the direction of the majority of the group when they are informed of the opinions or choices of other group members. Second, people may care about the outcomes for the others in their group and about how the others judge them. Thus, an affiliation goal may also encourage them to conform to the choices and opinions of others in their group. Nordhøy (1962), Brown(1965), and Stoner (1968) first explained how such social pressure may cause risky and cautious choice shifts. They argue that in a group decision, individual votes get shifted towards the choice that most group members would have preferred in an individual decision. In this approach, choice shifts are conceptualized as a drift towards the ex-ante majority preference. Some studies advocate the affiliation goal that people may pursue when they give in to social pressure (cf. Brown 1965). Other studies favor the accuracy goal (cf. Brown 1965; St. Jean 1970; Stoner 1968; Pruitt and Teger 1967; Vinokur1971). Notice that the previously collected evidence in favor of the conformity mechanism does not contradict ERR’s RDU theory. ERR predict that if the default in a group is the cautious decision, choice shifts will tend to go in the cautious direction. Likewise, if the default in a group is the risky decision, choice shifts will tend to go in the risky direction. In a group process, the majority position may easily serve as the default which will be implemented if an individual’s vote is not pivotal. Thus, the two mechanisms may be quite similar in terms of behavioral patterns that are expected in previous designs. Our experimental design is the first to allow for a direct comparison of the rank-dependent utility and the conformity explanations for choice shifts.Footnote 2 We use a simplified setup inspired by ERR’s model of group processes. This amounts to having our subjects choose between a risky and a safe gamble that, conditional on the treatment, we augment with different layers of a real group decision. One treatment gives ERR’s model its best shot. In the group decision of this treatment, subjects are informed of the risky or safe default that will be implemented with exogenous probability, while subjects are not distracted by information about the preferences of their peers and while they also know that their decisions have no consequences for the others. There is also a treatment that gives the conformity approach its best shot. In this treatment, there is no default that causes exogenous uncertainty, while subjects are informed of the preferences of their group members and know that their decisions have payoff consequences for their group members. In between these two extremes, we have some treatments that allow us to systematically study the effect of receiving information regarding the majority preferences, the effect of whether or not the individual decision has payoff externalities to the other group members and the effect of the presence of a default. Thus, a novel feature of our design is that we control the influence an individual’s choice has on her final outcome independently of social aspects of the choice situation such as the degree of responsibility for others’ outcomes and the extent to which subjects learn about others’ preferences. Group discussion is not essential for either of the two mechanisms. To distinguish between the two mechanisms in a clean setting, we do not allow groups in the experiment to explicitly discuss their attitudes toward risk for the specific gambles that they face.Footnote 3 To create a sense of being in a group, our subjects briefly get to know each other before they are informed of the risky decisions that they make.Footnote 4 In agreement with previous work, we find that cautious and risky shifts regularly occur. Our results lend clear support for the conformity mechanism: individuals display a strong tendency to adapt their decisions to the majority preferences in their group. This pattern is strongest when a subject’s decision has payoff consequences for other group members — suggesting that choice shifts are partly driven by the activation of the group affiliation goal of the conformity mechanism, or, in economic terms, by other-regarding preferences. Although shifts are common in both directions, we do find an asymmetry in the occurrence of choice shifts when decisions have payoff externalities. A choice shift is particularly likely when a subject exhibits a preference for the risky option when choosing in isolation while she shifts to the cautious option once she is informed that the majority preferred the cautious gamble.Footnote 5 We find only limited support for ERR’s approach based on rank-dependent utility. Even in the treatment that gives the theory its best shot, the observed pattern of choice shifts does not agree particularly well with their mechanism; shifts are somewhat more often in the direction of the default (like ERR predict), but the difference is insignificant. The only other study that sheds light on the empirical validity of ERR’s model is an (unpublished) paper by Gurdal and Miller (2010). In their implementation of the ERR model, the group decision is the risky decision unless all group members vote for the cautious decision. They never provide subjects with information regarding the preferences of the majority. They find that subjects in the group decision tend to shift in the cautious direction even though they should shift in the risky direction if the ERR model drives the choice shift. They favor the explanation that in groups people are affected by a social norm to behave cautiously. An alternative explanation is that subjects implicitly respected other group members’ preferences. That is, with unanimous decision making and a risky default, a subject’s vote only matters if all the others vote cautiously. Conditional on being pivotal, a voter would know that he imposed the risky lottery on the others who voted cautiously and therefore a desire for conformity with the group preference might make subjects behave more cautiously in the group decision. Gurdal and Miller (2010) do not have observations of group decisions where the default is the cautious decision. Therefore, it is not clear whether subjects become generally more cautious in groups, as Gurdal and Miller (2010) suggest, or whether they move into the direction of the supposed majority preference of the group. Our setup has the advantage of clearly separating between the effects of defaults and information on majority preferences among group members. Possibly as a consequence of this, we find much clearer evidence of choice shifts than Gurdal and Miller (2010). Contrary to Gurdal and Miller (2010), we also find a sizable number of risky shifts, which should not occur if people generally become more cautious when acting in groups. In this paper we focus on the extent to which rank-dependent utility and conformity contribute to the emergence of choice shifts. There may also be other mechanisms that cause choice shifts. When there are payoff externalities of people’s decisions, it may be that individuals change their decisions because they feel responsible for others’ outcomes. If such responsibility matters, then in the presence of payoff externalities people may shift their decisions even if they have no information about the majority preference. Charness (2000) reports an effect of social responsibility in a labor market experiment. Charness and Jackson (2009) find that subjects are somewhat more likely to choose the safe option in a stag-hunt game when they are responsible for the payoff of another group member. More closely related are the recent papers by Pahlke et al. (2015) and Vieider et al. (2016) that investigate the role of social responsibility in individual decision making. Social responsibility sometimes leads to less risk taking and sometimes to more risk seeking. Vieider et al. (2016) find that probability weighting becomes more extreme in the presence of social responsibility.Footnote 6 The remainder of this paper is structured as follows: Section 2 provides the model that we use and explains how rank-dependent utility may produce choice shifts. Section 3 describes and motivates the experimental design we used. In Section 4, we show how the design allows us to derive predictions that distinguish the two candidate mechanisms for choice shifts. Section 5 presents the results of our experiment. Section 6 concludes.",8
56.0,3.0,Journal of Risk and Uncertainty,12 June 2018,https://link.springer.com/article/10.1007/s11166-018-9280-8,Spatial externalities and risk in interdependent security games,June 2018,Stephan Kroll,Aric P. Shafran,,Male,Male,Unknown,Male,"Individuals regularly make decisions to invest in self-protection, expenditures that reduce the probability of a loss without affecting the severity of the loss. Often the effectiveness of self-protection depends on the extent of self-protection among other economic agents. For example, the effort exerted in screening checked bags for bombs not only reduces an airline’s risk but also the risk faced by partner airlines that accept transferred bags without re-screening (Kunreuther and Heal 2003); investment in antivirus software protects data on a shared hard drive (Kearns 2005); and homeowners in the wildland urban interface can create defensible space around their houses to reduce the risk of wildfire destroying their house, but the effectiveness of defensible space also depends on fuel loads at neighboring properties (Shafran 2008). In these cases, the decision to self-protect can be modeled as an interdependent security (IDS) game. Kunreuther and Heal (2003) characterize several classes of IDS games depending on whether the risk interdependencies exhibit strategic complementarity or substitutability. In situations where strategic complementarities exist, it is possible to have two Pareto-ranked equilibria in pure strategies, one in which all agents invest in self-protection and one in which no agent invests. These games are subject to a coordination problem, and agents can get stuck in the inferior equilibrium — even though everyone is better off at the preferred equilibrium, no agent has an incentive to unilaterally deviate from the inferior equilibrium. For example, Taylor et al. (2015) show that, across otherwise similar communities, there are clusters of homes with low levels of defensible space and other clusters with high levels, providing evidence of multiple equilibria in defensible space outcomes. In many examples of IDS games, agents affect the risk of other agents in a heterogeneous manner. For example, an airline’s bag screening decisions have a greater effect on airlines with which they transfer bags more frequently. Similarly, in the context of wildfire risk, the risk at a house depends on defensible space at neighboring houses but not houses that are very far away. We design experiments to test the importance of “spatial” links between agents in promoting coordination on the preferred equilibrium — both in a stochastic IDS game and in a game with equivalent expected payoffs but no noise in the payoffs. Our experimental design varies whether payoffs are determined stochastically or deterministically, and we examine whether agents respond differently to the behavior of other group members in these two situations. This reflects that outcomes in different real-world coordination games can have different levels of noise. Consider two different scenarios regarding the actions of homeowners in the wildland urban interface. In the first example, suppose the good is “reducing the likelihood of a wildfire.” Decreasing fuel loads on one’s own property decreases the probability that a wildfire occurs, but even when all homeowners take this action, it is still possible that a wildfire occurs (and on the other hand, no homeowners taking action does not automatically result in a wildfire occurrence). If, however, the good in question is instead the visual appearance of a neighborhood, then cleaning one’s own property has an immediate (deterministic) impact on the overall appearance of the neighborhood. In this second example, the neighborhood looks the nicest when all owners take the action, the outcome in the better equilibrium. Risk-neutral agents should not behave differently in stochastic and deterministic games if the stochastic game has the same expected value, but anecdotal evidence as well as previous research (Bereby-Meyer and Roth 2006) suggest that many agents’ behavior in stochastic decision environments is influenced more by ex post realizations of previous gambles than by ex ante predictions of future gambles. For example, Gong et al. (2014) report that in stochastic games subjects were more likely to stick with their decision in the previous round when there was a “good” outcome in that round than when they suffered a loss. A similar phenomenon has been observed in earthquake and flood insurance markets, where demand for insurance increases immediately following an adverse event (Kunreuther 1978; Browne and Hoyt 2000). For both the deterministic and stochastic cases, we also compare non-spatial externalities, in which every agent has the same effect on the risk of every other agent, with spatial externalities, in which agents directly affect the risk of only the agents closest to themselves. For the spatial treatments, subjects are placed in a figurative circle and only affect the risk of the two neighboring subjects. We examine if the spatial nature of interdependencies has an impact on whether groups coordinate more easily on the social optimum, which has important policy implications. Adding the spatial component to the IDS environment leads to two potentially competing effects — on the one hand, the spatial nature of interdependencies will make coordination on the optimal equilibrium easier due to the smaller set of neighbors with whom an agent is directly interacting. Van Huyck et al. (1990) observe in their seminal paper on coordination in weakest-link games with several Pareto-ranked equilibria that smaller groups coordinate more easily on the Pareto-optimal equilibrium, and this result has been replicated in many papers with similar (non-spatial) setups since; see the overview of this research in Riedl et al. (2016).Footnote 1 On the other hand, while subjects in a spatial setup interact only in a small group with their neighbors, those neighbors interact with others who in turn interact yet with others, and so on. Different people within the same group face different local conditions and so coordination across different neighborhoods is more complicated. In fact, Cassar (2007) hypothesizes that groups with a smaller “clustering coefficient,” the extent to which one’s neighbors are directly connected to each other, should actually be less likely to coordinate on a payoff-dominant equilibrium than groups with a larger clustering coefficient. In our experiment, the clustering coefficient, which generally ranges from 0 to 1, is 0 in the spatial treatment, since each subject’s neighbors are never connected, and 1 in the non-spatial treatment, since everybody within a group is a “neighbor” to everybody else. Consistent with Cassar’s hypothesis, Keser et al. (1998) observe that subjects in fixed groups of three (clustering coefficient equals 1, similar to our non-spatial treatment) coordinate vastly more often on the payoff-dominant equilibrium than subjects who interact only with two (out of seven) neighbors in a circle (clustering coefficient equals 0, similar to our spatial treatment), even though in both cases a subject interacts with two other subjects.Footnote 2 Given these competing forces, it is then an open research question which effect will be stronger in our experiment where the overall group sizes are equal in the spatial and non-spatial treatments, but the number of neighbors each subject interacts with differs.Footnote 3 There is a growing literature that uses economic experiments to analyze behavior in interdependent security games. The current paper is closely related to Hess et al. (2007), Shafran (2010) and Gong et al. (2014), all three of which study stochastic IDS games with strategic complementarity and multiple equilibria.Footnote 4 The prior experimental literature has found that (pure-strategy) equilibrium outcomes are rarely observed, and play does not appear to converge toward either equilibrium. However, despite the “spatial” nature of many risk interdependencies, most prior IDS experiments have focused on non-spatial interdependencies. Gong et al. (2014) is most closely related to this paper, as they also conduct deterministic vs. stochastic treatments of IDS games, but their experimental design differs from ours in two important aspects: a) they only examine behavior in a non-spatial context, and b) they induce a zero probability of a bad outcome if all group members decide to invest in self-protection. For most relevant examples, however, it is not realistic to think that self-protection can completely eliminate the possibility of a bad outcome. Moreover, precluding a bad outcome if everybody is taking action makes this equilibrium more stable if subjects base their decisions at least partly on previous outcomes. By varying two controls, we are the first to examine the impact of stochastic vs. deterministic payoff structures in spatial and non-spatial interactions (Table 1 presents the four main treatments). To summarize the goals of the paper, we compare stochastic interdependent security games with deterministic versions of the same games and examine the behavioral responses to the introduction of noise. Second, we compare non-spatial externalities with spatial externalities to see if the introduction of a spatial component affects the ability of agents to tacitly coordinate on the payoff-dominant equilibrium, noting that this depends on the relative importance of two effects from the literature: a small-group effect (Van Huyck et al. 1990) and a clustering effect (Cassar 2007). Finally, we are interested in the interaction effects of the spatial component with the stochasticity.
 Our main results are as follows. We find that subjects are likely to coordinate on the preferred equilibrium in the deterministic sessions but not in the stochastic IDS sessions. This is consistent with prior experiments on stochastic IDS games in which equilibrium outcomes were rare. We also find that spatial externalities lead to greater coordination on the preferred equilibrium in the deterministic sessions, providing evidence that the small-group effect outweighs the clustering effect; however, there is no similar effect in the stochastic sessions. We therefore conclude that the structure of interdependencies in deterministic coordination games can affect the ease with which agents tacitly coordinate on the payoff-dominant equilibrium but that the additional complexity introduced in stochastic IDS games overwhelms this effect. To further investigate differences between the stochastic and deterministic treatments, we ran additional sessions of the stochastic treatments in which we provided expected values of every outcome to subjects. The purpose of these sessions is to assess the extent to which differences between stochastic and deterministic treatments stem from an inability of subjects in the stochastic treatments to determine which action is best, conditional on the actions of the other subjects. We also elicited the risk preferences of subjects using the Holt and Laury (2002) procedure to test whether differences in risk preferences explain observed behavioral differences in the stochastic sessions. We find that play is not notably different in the stochastic sessions with or without expected values and that risk preferences are not an important driver of subject behavior. The remainder of the paper is organized as follows. The next section explains the general set-up and experimental design. Section 3 presents the results. Section 4 concludes and suggests policy recommendations.",3
56.0,3.0,Journal of Risk and Uncertainty,26 June 2018,https://link.springer.com/article/10.1007/s11166-018-9281-7,Estimating representations of time preferences and models of probabilistic intertemporal choice on experimental data,June 2018,Pavlo R. Blavatskyy,Hela Maafi,,Male,Female,Unknown,Mix,,
56.0,3.0,Journal of Risk and Uncertainty,29 June 2018,https://link.springer.com/article/10.1007/s11166-018-9283-5,Risk and risk aversion effects in contests with contingent payments,June 2018,Liqun Liu,Jack Meyer,Thomas R. Saving,Unknown,Male,Male,Male,"The participants in a contest know they can win or lose, and they use resources to increase their chance of winning. For some contests the cost of these resources is paid by the contestant at the time the resources are used. For others, however, the contestant only bears the cost after the contest is resolved. This allows the payment to be contingent on the outcome of the contest. Much of the existing research concerning contests assumes that the costs are paid up front regardless of the outcome of the contest. This is an important case, and we modestly extend the known results for contests with upfront payment of costs. The majority of our analysis, however, is for contests with contingent payment of costs where only the winner of the contest pays. A good example of such a contest occurs in professional golf, where new golfers are often sponsored by an investor who agrees to pay all expenses associated with competing, and in return the investor acquires a share of the winnings when the sponsored golfer wins a tournament. From the contestant/golfer’s perspective, payment for resources used to compete is only made after the contest is resolved and only if the contest is won. It is after winning that the golfer pays the investor. Another example of a contest where only the winner pays occurs when a venture capital firm has a fixed budget to complete the research and then production of a new product. Part of the budget is allocated to the invited bidders as seed money to use for prototype development, and the remainder is awarded to the winning contestant to finance the project’s completion. This is a contest where the winner effectively pays for the costs incurred by all the bidders in the prototype phase. A third example of a contest where contestants do not pay unless they win occurs when several firms compete for a project, and the project is only very loosely specified. Each bidding firm then indicates the amenities it would include when completing the project, and only must deliver and pay for those amenities if it wins the contract. Contests by their very nature involve risk. Winning and losing are both possible, and the gain from winning can itself be uncertain. The main focus of this paper is on the effects of risk aversion and risk in contests with contingent payment of costs. These results are then contrasted with those determined for contests with upfront payment of costs. The literature on contests began many years ago and is very extensive.Footnote 1 Many different models of contests exist. In these models the contest success function, which gives the probability of winning, can be assumed to take on a general form, or alternatively, in other instances this functional form is quite specific. For our purposes, a general formulation is best suited for the analysis of risk and risk aversion impacts, so no specific form for the probability function is assumed. Only modest restrictions on how the probability of winning is affected by the efforts of the contestants are imposed. Models of contests also assume different goals for contestants. Here contestants are assumed to maximize expected utility, and the utility functions are assumed to be increasing and concave. Again, for the analysis of risk aversion and risk effects, specifying a particular form for the utility function, or imposing a particular risk aversion property such as constant absolute risk aversion, is not desirable because varying the risk taking properties of the contestants is an important part of the analysis. Finally, many models of contests focus on the symmetric Nash equilibrium resulting from the competition among contestants, and that practice is followed here. As others have noted, participants in contests, when viewed as individual decision makers, face a decision much like the decision to self-protect (Ehrlich and Becker 1972). The self-protection decision has been extensively analyzed, including analysis for situations where the payment for self-protection is contingent on the outcome (Liu et al. 2009). That work proves useful in the derivations presented here. Because contests involve more than one participant, however, the equilibrium condition used to conduct comparative static analysis is a Nash equilibrium condition in a non-cooperative game rather than a first order condition for expected utility maximization. This implies that only some of the results from the analysis of the self-protection decision with a contingent payment carry forward to this analysis. The contingent payments considered here assume that payment for the cost of participation in a contest is made only in the winning state by the winning contestant. Not paying when losing a contest reduces or eliminates many of the anomalies that arise in the basic self-protection decision model. The lowest possible outcome does not get smaller. Paying only when winning also significantly alters the effects of risk and risk aversion on the outcome of the contest.Footnote 2 As a general summary, relative to contests where costs are paid up front, the symmetric Nash equilibrium is unique, the effects of increased risk aversion are reversed, and the effects of increased risk are maintained when costs are paid only by the winner. The details of these claims are presented in the body of the paper. The paper is organized as follows. In the next section, the contest model used throughout the paper is presented, and the known results when costs are paid up front—regardless of whether the contest is won or lost—are reviewed. The model used is very similar to one formulated by Konrad and Schlesinger (1997) and used recently by Treich (2010). In this model, Treich characterizes the symmetric Nash equilibria, and shows that risk-averse and prudent contestants choose to devote fewer resources to winning than do risk-neutral contestants. Treich also shows that risk-averse and prudent contestants devote fewer resources to winning the contest when the gain from winning is random rather than certain. The analysis here adds to these findings by comparing the equilibria reached by two groups of risk-averse and prudent (i.e., downside risk averse) contestants, with one group being both Ross more risk averse and Ross more downside risk averse than the other. It is not surprising that the finding we obtain is in the same direction as that determined by Treich. Section 3 contains the main contribution of the paper. In this section, the contest model is modified so that the payment for the resources used to improve the chance of winning is contingent upon winning. That is, the contestant chooses the quantity of resources to employ to enhance the chance of winning, but pays for these resources after the contest is resolved, and only pays when a winner.Footnote 3 The professional golfer example mentioned earlier is one example of such a contest, but there are many others. The analysis in Section 3 is purposely kept very general, working with a general contingent cost function, and deferring the discussion of several concrete specifications of the contingent cost function and their examples to the next section. Compared with the results presented in Section 2 under upfront payment, the results obtained in this section under contingent payment reverse the effects of risk aversion, but uphold the effects of making the prize random. Section 4 focuses on several specific contingent cost functions that are observed in the contest world. These cost functions specify exactly the algorithm used to determine the size of the payment made by the winner of the contest. These algorithms are discussed so that the general results can be interpreted, and also because each algorithm represents a contingent payment contract that is observed in the marketplace. Included in the discussion are contingent payment contracts that require the winner to pay a fixed share of the prize, that require the winner to pay the costs incurred by all participants, and that require the winner to pay only her own expenses and losers to pay nothing.Footnote 4 Section 5 concludes and summarizes the results presented here.",7
57.0,1.0,Journal of Risk and Uncertainty,24 September 2018,https://link.springer.com/article/10.1007/s11166-018-9287-1,Temporal discounting of gains and losses of time: An experimental investigation,August 2018,Mohammed Abdellaoui,Cédric Gutierrez,Emmanuel Kemel,Male,Male,Male,Male,"Most of the behavioral research on intertemporal choice, under Samuelson’s (1937) discounted utility (DU) model and extensions, has been dominated by studies involving intertemporal tradeoffs of monetary outcomes. Yet, the easier transferability of money over time, due to its intrinsic fungibility and the access to credit that decouples money from consumption, may generate specific findings regarding discounting and utility (see Bleichrodt et al. 2016a; Cubitt and Read 2007). Additionally, empirical investigations have focused mainly on monetary gains, leaving the problems of “asymmetries” between gains and losses, initially detected in decision under risk and reasonably expected in intertemporal choice (Shelley 1994), relatively unexplored. The focus on monetary gains in the literature was favoured due to the complications related to the implementation of real incentives to losses.Footnote 1 Several papers have investigated temporal preferences towards other attributes than money, such as chocolate, beer or drugs (Estle et al. 2007; Read and Van Leeuwen 1998; Tsukayama and Duckworth 2010). These studies reveal that the nature of the consequence can impact discounting even though, like money, the goods considered can be received at a given time period and consumed later. More recently, a few research works have explored the idea that the time dedicated to a specific task or activity may be a better proxy for consumption than money (e.g., Meissner and Pfeiffer 2015). Similarly, Augenblick et al. (2015) measured temporal preferences and dynamic inconsistencies for decisions involving the effort dedicated to a specific task. Unlike money, time can neither be stored nor exchanged on a market: an hour of spare (or working) time at a given period has to be enjoyed (or endured) at the same period. The present paper contributes to this research direction. We report the results of an incentivized experimental elicitation of intertemporal preferences when consequences are gains and losses of time with respect to a given reference point, assuming Loewenstein and Prelec’s (1992) behavioral model. We did not impose constant discounting (see Table 1) or a linear utility function, i.e. we accounted for the subjective value assigned by the subjects to gains and losses of time. Our investigation consisted of a series of situations where subjects faced a choice between temporal prospects involving soon and late gains (losses) of time. The reference point with respect to which gains and losses of time were defined was set up by means of a concrete research assistantship contract that consisted of two working sessions initially planned to last 4 hours each at two different dates (sooner and later). For instance, a temporal prospect that entails gaining (losing) 1 hour now and 1 hour in six months means that the duration of the two sessions will be shortened (extended) by 1 hour each. We managed to have a homogenous sample of subjects with similar time schedule within the time span of the (receipt) delays involved in the experiment. In addition to its reference and sign-dependence (i.e., gain/loss dependence), our elicitation did not impose positive discounting, i.e. subjects were allowed to exhibit preference for the futureFootnote 2 (see Loewenstein and Prelec 1993). We also elicited individual preferences for gains of money, as a benchmark. To our knowledge, this is the first paper that proposes an elicitation of temporal preferences for both gains and losses of time while assuming a general behavioral model of intertemporal choice (Loewenstein and Prelec 1992).
 Many everyday life decisions involve saving or losing time, now or in the future. Understanding how people discount future gains and losses of time is therefore economically important. Gains and losses of time are always defined with reference to a specific duration. For example, an employee with a standard working contract of 40 hours per week would consider any decrease (increase) of weekly working time as a gain (loss) of time. Regarding time gains, any company or state policy of working time reduction raises the question of how employees prefer to allocate their spare time. An employee who is offered 5 hours of extra free time due to a reduction of working time can, for example, choose to leave the office one hour earlier every day or instead have a complete afternoon off at the end of the week. The choice of this employee would depend on the way she discounts gains of time and her preference for smoothing such gains over time. The first aspect is captured by the discounting function while the second is captured by the utility function. Regarding time losses, imagine the situation where the same employee is asked to work overtime due to an increase of activity and has to decide between working 42 hours next week or 44 hours four weeks from now; would the employee prefer to lose two hours in the coming week or four hours next month? Similarly, the decision of when to edit the minutes of a meeting is another example of intertemporal choice involving losses of time that most professionals have experienced. The task can be completed right after the meeting when the memory is still “fresh”, or may be postponed and take longer. Several research works have observed that discounting non-monetary consequences may produce distinctive discount patterns (Augenblick et al. 2015; Bleichrodt et al. 2016a; Prelec and Loewenstein 1997; Read et al. 1999; Winer 1997; Zauberman and Lynch 2005). For instance, while negative discounting (preference for satisfaction later than sooner) is the exception for money, it has been observed for health consequences in a number of experimental investigations (Ganiats et al. 2000; van der Pol and Cairns 2000). Similarly, according to Loewenstein (1987) “the pleasurable deferral of a vacation, the speeding up of a dental appointment, the prolonged storage of a bottle of expensive champagne are all instances of this phenomenon.” Regarding time gains, the behavior of a person attempting to accomplish a time-consuming task as soon as possible and to “save” free time for the future is also considered rational by Adam Smith: “He is enabled gradually to relax, both in the rigour of his parsimony and in the severity of his application; and he feels with double satisfaction this gradual increase of ease and enjoyment, from having felt before the hardship which attended the want of them” [Adam Smith, The Theory of Moral Sentiments, as cited in Loewenstein and Prelec (1991)]. In line with these results, our experiment shows that people do not discount time in the same way they discount money. Overall, at the aggregate level, both the level of impatience and the degree of deviation from constant discounting are higher for gains of time than for gains of money. We also observe a much higher heterogeneity in discounting behavior for time than for money. Furthermore, our main finding is the strong asymmetry in discounting behavior between gains and losses of time: the level of impatience is much higher for gains than for losses of time. More specifically, in contrast to the classical view that people would prefer to experience losses further along in the future, we observe that the majority of our subjects prefers to expedite losses of time, i.e. exhibits negative discounting. On the other hand, a wide majority of the subjects exhibits positive discounting for gains of time, i.e. they prefer to gain time now than in the future. The present paper proceeds as follows. Section 2 introduces the theoretical background and the parametric specifications used in the paper. Section 3 details the experimental design and the incentive system. Section 4 presents the results obtained in a model-free fashion while Section 5 introduces the results of the econometric analyses. The paper ends with a discussion and concluding remarks.",14
57.0,1.0,Journal of Risk and Uncertainty,02 August 2018,https://link.springer.com/article/10.1007/s11166-018-9284-4,Your money and your life: Risk attitudes over gains and losses,August 2018,Adam Oliver,,,Male,Unknown,Unknown,Male,"Neoclassical economics and the standard theory of rational choice under conditions of risk and uncertainty (i.e. von Neumann-Morgenstern expected utility theory)—hereon defined as orthodox theory—postulate that people should adopt a consistent risk attitude across all decisions that offer outcomes that do not occur with certainty. More specifically, it is typically assumed that marginal utility diminishes with increasing money outcomes, as indicated by curve aa in Fig. 1, and thus that an individual will be consistently risk averse—although in several articles Matthew Rabin argues that orthodox theory does not permit risk aversion over anything other than very large outcomes, and that near universal risk neutrality is the more accurate assumption (e.g. see Rabin 2000). However, at least some of the founding fathers of neoclassical economic theory acknowledged that actual choice behaviours regularly reveal inconsistent individual risk attitudes across different circumstances. For instance, Alfred Marshall (1920), in relation to occupational choice, wrote that people will tend to be risk averse in the face of modest outcomes, but will be risk seeking when a few very large outcomes are offered, and we know from common observation that many individuals both gamble and insure. Utility/value curves In 1952, Harry Markowitz presented a more formal challenge to whether the assumption of a consistent risk attitude holds up in descriptive choice. Markowitz posed a series of questions that involved choices between a 10% chance of money gains or losses of $1 to $10,000,000 and the expected values of those lotteries. With these questions, Markowitz discovered that his acquaintances were typically risk averse when faced with a 10% chance of large gains and small losses, and risk seeking when faced with a 10% chance of small gains and large losses. He attributed all of these attitudes to the size of the outcome and to whether the outcomes were perceived as gains or losses—i.e. risk aversion over large gains and small losses and risk seeking over small gains and large losses—with the point dividing gains and losses assumed to be present or customary wealth. The shape of Markowitz’s utility curve is depicted as bb in Fig. 1; intimating loss aversion—i.e. the tendency to feel the pain of a loss more acutely than the pleasure of an equal sized gain—Markowitz suggested that the curve falls faster to the left than it rises to the right of the origin. A generation later, Kahneman and Tversky (1979; 1992), in their work on prospect theory, similarly postulated that utility—or rather, in their words, value—is reference dependent in that it is defined by gains and losses around a reference point, but more so than Markowitz, they specified an empirically supported value curve that is much steeper for losses than for gains, strongly emphasising loss aversion. However, unlike Markowitz’s utility curve, which has concave and convex regions within both the positive and negative domains, Kahneman and Tversky’s value function is entirely concave for gains and convex for losses, giving an S-shaped curve. A typical prospect theory value curve is depicted as cc in Fig. 1. The curve cc implies declining marginal sensitivity to both mounting gains and mounting losses, and hence risk aversion and risk seeking in the domains of gains and losses, respectively. Therefore, unlike orthodox utility theory but in common with the Markowitz model, prospect theory, by allowing people to take risks, is incompatible with the notion that certainty is always desirable. However, prospect theory introduced a further complication that Markowitz avoided, by also positing that people will overweight small probabilities and underweight large probabilities. The underweighting of large probabilities reinforces the risk attitude predictions inferred from the value function, but the overweighting of small probabilities may reverse the risk attitudes in the domains of gains and losses, such that individuals will now be risk seeking in the former and risk averse in the latter. Thus, probability weighting predicts gambling and insurance behaviours in small probability scenarios. Due to the combined effects of the value and probability weighting functions, Tversky and Kahneman (1992, p.306) note that the “most distinctive implication of prospect theory is the fourfold pattern of risk attitudes.” These risk attitudes are summarised in Table 1, for which Tversky and Kahneman, in a non-incentivised study using simple money lotteries, provided empirical support. The top left quadrant in Table 1 describes the prospect theory risk attitude prediction when a person is faced with a large probability of a gain—for example, a 99% chance of winning £1000. If an individual is offered a choice between this risky option and the certainty of its expected value of £990, prospect theory predicts that the individual places a high weight on the certainty and will demonstrate risk aversion. The bottom left quadrant describes the predicted risk attitude when faced with a small probability of a gain, such as a 1% chance of £1000. Here, the prediction is that since the individual will overweight the chance of winning, he or she would prefer the gamble over its expected value of £10 and will therefore be risk seeking. The top and bottom right quadrants can be read similarly, and show that prospect theory predicts opposing risk attitudes for losses as compared to gains for both large and small probability scenarios. This fourfold pattern of predicted risk attitudes is known as prospect theory’s reflection effect.",9
57.0,1.0,Journal of Risk and Uncertainty,24 August 2018,https://link.springer.com/article/10.1007/s11166-018-9285-3,"Age, autos, and the value of a statistical life",August 2018,James O’Brien,,,Male,Unknown,Unknown,Male,"The value of a statistical life (VSL) measures people’s demonstrated willingness to pay for reduced mortality risk and is used to assign a dollar value to the benefits of regulations that protect health and safety. For many such regulations, those benefits accrue disproportionately to older people. But most current VSL estimates come from hedonic wage studies based on samples that include few older workers and no retirees.Footnote 1 Rather than examining the risk-income tradeoff that individuals face in their labor market decisions, I focus on the risk-income tradeoff that individuals face in their automobile purchase decisions. By comparing the relative importance of automobile cost and safety for various age groups, I generate separate VSL estimates for seven age cohorts ranging in age from 18 to 85 years. Policymakers recognize that health and safety benefits often accrue disproportionately to older groups, but this is not currently incorporated into regulatory cost-benefit analyses in the US. Other countries and coalitions such as Canada and the European Commission have at times recommended the use of an age-adjusted VSL for seniors (Aldy and Viscusi 2007). In 2003 the US Environmental Protection Agency (EPA) released a cost-benefit analysis applying a “senior discount” of 37% to the VSL of people over the age of 70 (US Environmental Protection Agency 2002). The political backlash that followed led Congress to prohibit the use of age adjustments in federal cost-benefit analyses.Footnote 2 Currently, federal agencies are instructed not to use age-adjustment factors due to “continuing questions over the effect of age on VSL estimates” (US Office of Management and Budget 2003).Footnote 3 Similarly, the OECD also recommends “no adjustment for adults due to inconclusive evidence” (OECD 2012). In theory, the age-VSL relationship can take any shape (Johansson 2002; Aldy and Viscusi 2003), although models based on life-cycle consumption often suggest an inverted-U shape (Aldy and Smyth 2014; Shepard and Zeckhauser 1984). Empirical work on this topic has also produced mixed results. Table 1 and Fig. 1 summarize several prominent empirical age-VSL studies and their findings.Footnote 4 These can be separated into two types: those that rely on revealed preferences and those that rely on contingent valuation. Among the revealed preference studies, the vast majority employ a hedonic wage framework to estimate a wage-risk tradeoff for workers across different industries, occupations, and demographics.Footnote 5 Although there is considerable variation, hedonic wage studies generally support a flat or inverted-U shaped age-VSL relationship, such as in Aldy and Viscusi (2003, 2008).Footnote 6 The survey-based contingent valuation approach is not limited by labor market participation and captures a more diverse set of individuals. Stated preference studies typically show flat, slightly decreasing, or weakly inverted-U shaped age-VSL estimates, such as in Krupnick et al. (2002), Alberini et al. (2004) and DeShazo and Cameron (2004).Footnote 7 Selected age-VSL results in recent literature In this paper I use a revealed preference approach to estimate the VSL by examining the tradeoffs between safety and cost inherent in consumers’ automobile choices. A key challenge I face is that safety—measured using the number of fatalities in each vehicle—depends on the individuals who drive those vehicles. For example, if Prius drivers and Camaro drivers differ from one another in unobservable ways, estimates based on the relative riskiness of those vehicles could be biased. I control for the effects of drivers’ behavior on vehicles’ measured safety in two ways. First, I use age-specific risk factors for evaluating each individual’s purchase decision. This is similar to Viscusi and Aldy (2007) and Aldy and Viscusi (2007, 2008) who calculate on-the-job mortality risk by industry and age group. Then I further refine my risk factors by adjusting for the behavior of different types of drivers via a correction based on differences between laboratory crash test results and real-world single-vehicle fatalities. Despite the challenges of estimating a VSL using automobile choice, this approach also has several distinct advantages. The first is that automobiles are purchased by adults of all ages. For example, in the 2009 National Household Travel Survey (NHTS) roughly 14% of automobile acquisitions involved someone over the age of 65. Second, this study provides a comparable benchmark to labor market VSL estimates because over 40% of on-the-job fatalities are transportation related, with roadway incidents alone accounting for one out of every four workplace fatalities (US Bureau of Labor Statistics 2016).Footnote 8 Third, by measuring the relative importance of fatality risk and cost in the consumer’s underlying utility function, I separately observe the effects of each as they flow into the final VSL estimate. This will be the first study to identify whether age differences in the VSL are the result of different preferences for risk, cost, or both. I find that the age-VSL function over the life-cycle follows an inverted-U shape. Moreover, this inverted-U is the result of two distinct patterns in the marginal disutility of cost and risk. Seniors and middle-aged drivers have similar disutility from cost, but seniors place less importance on safety, which results in a lower VSL for seniors relative to middle-aged drivers. Young drivers place less importance on safety and more importance on cost relative to middle-aged drivers, which implies a lower VSL for young drivers relative to middle-aged drivers. Overall, the VSL ranges from $2.4 million for the youngest group (18 to 24 years) to $19.2 million for the middle-aged group (55 to 64 years), falling again to $8.2 million for the oldest group (75 to 85 years). This corresponds to a weighted average VSL of $9.2 million. These vehicle-based results are comparable in scale to existing wage-based estimates that range from zero to $19.9 million.Footnote 9",13
57.0,1.0,Journal of Risk and Uncertainty,06 September 2018,https://link.springer.com/article/10.1007/s11166-018-9286-2,Risk taking on behalf of others: The role of social distance,August 2018,Natalia Montinari,Michela Rancan,,Female,Female,Unknown,Female,"Individuals take decisions affecting themselves as well as other people, such as spouses, children, colleagues, employees, shareholders, customers, patients, and citizens. Despite the fact that these situations are extremely common in everyday life, little is known still about how being responsible for another person affects individuals’ decision making (see e.g. Charness and Jackson 2009; Sutter 2009; Bolton and Ockenfels 2010; Pahlke et al. 2015). The aim of this paper is to study how risk taking varies when risky choices are made on behalf of someone else under different levels of social distance compared to the case when decisions do not affect anyone else. We proceed in two steps: first, we focus on how the presence of someone else affects individuals’ risky choices. Then, we concentrate on social distance to investigate whether not only the presence of another person, but also his/her closeness with the decision maker matters. The first step premises the second one, which is relevant only if the decision maker takes into account others’ outcome or feels responsible for others’ gains and losses. Previous studies have investigated decision making in social contexts, and particularly on behalf of others, but we lack a comprehensive framework about risk. While some works suggest that deciding for others is different from the case in which no one else is involved (Sutter 2009), other results do not show any difference (Andersson et al. 2014). Therefore, based on this mixed evidence, it is not obvious a priori how, and to what extent, the presence of another person affected by the outcome of the risky choice enters into the decision process. Our second step of investigation focuses on the social distance that is on the degree of similarity, closeness, or “emotional proximity” between individuals involved in a certain situation (Charness and Gneezy 2008). When deciding for others, social distance becomes an inherent feature: the decision maker is usually aware of the person for whom he is deciding. Furthermore, the level of social distance changes substantially from completely anonymous situations (politicians-citizens) to highly close situations (husband-wife). Thus, it seems an important dimension to explore. This paper aims at improving the understanding of the decision making of risk-seeking choices when deciding for others. With this aim in mind, we designed an experiment to study the behavior of a decision maker (active participant, identified by ‘he’) confronted with a lottery, which has consequences both for himself and another person (passive participant, identified by ‘she’). In order to make the decision made on behalf of others more salient, we choose lotteries in the domain of losses, i.e. yielding negative expected values.Footnote 1 Most of the literature usually considers lotteries in the gain domain (Charness and Gneezy 2010). On the other hand, tasks in the loss domain are rarely analyzed (exceptions are Shupp and Williams 2008; Pahlke et al. 2015). Existing evidence shows that people exhibit risk seeking behaviors, such as taking risky choices with associated negative expected value (i.e. buying lotteries), which are inconsistent with expected value maximization.Footnote 2 Previous studies discussed how the presence of a passive participant may affect the decision making process. In the simplest scenario, the active participant may use his own risk preferences to decide, thinking that the passive participant has the same risk preference. According to others, ‘socialization’ may have induced the idea that, when in a position of responsibility, more caution is necessary (Charness and Jackson 2009), inducing the decision maker to be more risk averse when deciding for others. Also an active participant exhibiting guilt aversion may be concerned about the fact that his choices cause the passive participant to receive less than what she expects to get (see e.g. Charness and Dufwenberg 2006). An alternative view suggests that, in some circumstances, risk can be considered as a value resulting in more risk seeking choices taken on behalf of others (see e.g. Stone and Allgaier 2008). Similar implications are expected according to the theory that people tend to perceive others to be less risk seeking than themselves (Lamm et al. 1972). In addition to cognitive processes, choices are driven by emotional reactions (Loewenstein et al. 2001): when others are involved, the empathy gap hypothesis suggests that there may be an underestimation of the emotional response of others to risk (see e.g. Loewenstein 1996). To study the effect of social distance, we change the identity of the person for whom the active participant decides implementing two polar cases: deciding on behalf of an anonymous stranger or deciding on behalf of a friend who comes to the laboratory together with the decision maker. While a decision for an anonymous stranger represents a situation characterized by high social distance without the possibility of any feedback, a decision on behalf of a friend is characterized by low social distance and, most likely, will be discussed after the experimental session. This implies that when the active participant decides for a friend, compared to an anonymous stranger, being responsible for the outcomes of the passive participant may be more salient. In the same way, guilt aversion may be enhanced by the friendship as he may be more concerned about hurting his friend than hurting a stranger. In both scenarios the decisions of the active participant may be motivated by his beliefs about the passive participant’s choice, and due to a lower social distance, the active participant may have more precise beliefs about his friend’s risk preference than about those of a stranger. Also lower social distance may imply that emotional reactions are underestimated to a lower extent for friends compared to strangers. In a first set of experimental sessions, each active participant decides, in a fixed order, first for himself, then on behalf of an anonymous stranger and finally on behalf of a friend. To control for potential order effects occurring through the three decisions, we collected additional data. Specifically, we investigated i) whether the decisions made by the active participants for themselves change depending on the fact that they are made in the first or last experimental part, and ii) whether the order of the decisions made on behalf of others matters (i.e. whether decisions differ depending on the fact that participants experience an increase or decrease in the social distance). In addition, we study whether decision making on behalf of others is affected by the frequency of the feedback received about the outcome of the risky choice. This allows us to investigate whether being responsible for another person’s earnings affects myopic loss aversion (MLA, henceforth). MLA is a well-documented behavioral bias and it predicts that subjects are willing to invest more money into a lottery the longer the horizon of risky choices and the less often they receive feedback (see, e.g. Gneezy and Potters 1997; Charness and Gneezy 2010). We report three main results. First, we find that when deciding on behalf of others, despite all else being equal, individuals make different and safer choices than when they decide only for themselves. More specifically, when deciding on behalf of an anonymous stranger or a friend, individuals make, respectively, slightly and remarkably less risky decisions. This pattern suggests that being responsible for another person has the strongest effect on the decision-making process when the social distance is shortened. When controlling for order effects, we find that experiencing a decrease in social distance (i.e. deciding for a friend after having decided for a stranger) is crucial for observing this shift in the behavior. One possible explanation is that experiencing a reduction in social distance makes salient the different level of responsibility associated with deciding for a friend rather than for a stranger. Moreover, when deciding on behalf of a friend, we find that the closer the degree of friendship between the decision maker and the passive participant, the safer the choice taken on her behalf. Second, similar to the decision process when no one else is involved, myopic loss aversion is confirmed for decisions made on behalf of a stranger and, to a lesser extent, a friend. Third, we investigate the key factors influencing the decision making on behalf of others. Heterogeneity in individual risk propensity accounts for differences in the risky choices, and personality traits, such as extraversion, also have an impact. Despite the relevance of individual characteristics in explaining risky choices, our evidence still suggests that social distance plays a significant role, highlighting that decision making on behalf of others is a complex process, involving both cognitive and emotional components, which interact differently depending on the social proximity to the person affected by the choice. The remainder of the paper is organized as follows. Section 2 discusses the relevant literature and Section 3 explains the experimental design. Section 4 formulates our research hypothesis. Sections 5 and 6 present the experimental procedures and the main results, respectively. Section 7 discusses the impact of order effects on our results. Section 8 concludes.",18
57.0,2.0,Journal of Risk and Uncertainty,11 October 2018,https://link.springer.com/article/10.1007/s11166-018-9288-0,Valuing the risk of workplace sexual harassment,October 2018,Joni Hersch,,,Female,Unknown,Unknown,Female,"A vast literature documents that workers facing risks of workplace fatality or injury are paid a premium for exposure to these risks (e.g., Viscusi and Aldy 2003; Viscusi 2018). But there is little evidence that workers are paid a premium for exposure to other workplace hazards or undesirable working conditions. In this paper, I establish that white women who are at risk of sexual harassment receive a premium for exposure to this risk. Nonwhite women do not receive this premium. The disparity by race is consistent with a similar lack of compensation for nonwhites at risk of fatality or injury (Leeth and Ruser 2003; Viscusi 2003). Based on the estimated premium for the risk of sexual harassment, I calculate a measure analogous to the value of a statistical life (VSL), which I term the “value of statistical harassment,” or VSH. The VSL monetizes the risk of fatality and is used by government agencies to set regulatory standards for safety. As with the VSL, the VSH can be used to monetize the societal harm caused by workplace sexual harassment and has applicability to policy in setting regulatory standards and damages awards to incentivize a reduction in the risk of workplace sexual harassment. Workplace sexual harassment is illegal under Title VII of the Civil Rights Act of 1964. Nonetheless, survey evidence documents that it is common. The #MeToo movement launched in fall 2017 has graphically revealed decades-long practices of unwelcome and often criminal sexual acts perpetrated by men at the top of their industries. Although the #MeToo movement has raised awareness of sexual harassment and has been costly to some individual harassers and firms, the bulk of the costs is borne by victims in terms of lower job satisfaction, reduced productivity, and higher turnover. There is currently little financial incentive for firms to cease sexual harassment as both the probability of a successful lawsuit and the federal cap on damages awards are low. In contrast, attempts to decrease sexual harassment increase the costs to firms of monitoring workplace behavior and may involve sanctioning or removing from their positions some of the most prominent employees.Footnote 1 The potential role of appropriate financial incentives in deterring undesirable behaviors is well-established. The existence of penalties alone is not sufficient to deter undesirable risky behaviors; the deterrence effect also depends on the magnitude of the penalties. For example, fairly low penalties often deter excessive speeding (DeAngelo and Charness 2012). In contrast, low penalties do not have an appreciable effect in deterring domestic violence (Sloan et al. 2013). My analysis demonstrates that the current federal cap on damages awards is far too low for efficient deterrence. To establish the level of damages awards necessary for efficient deterrence, I use data on sexual harassment claims filed with the Equal Employment Opportunity Commission (EEOC), from which I calculate the rate of sexual harassment claims by industry, age, and gender. Sexual harassment is well-known to be severely underreported; reported claims will account for the most severe cases of sexual harassment and are used as a proxy for the risk of sexual harassment faced by workers in their specific industry, age group, and gender. This rate of claims provides the first comprehensive information on the incidence of sexual harassment by industry and allows making inter-industry comparisons that are otherwise not feasible. By including the risk of sexual harassment claims in wage equations, I estimate the sexual harassment risk–wage tradeoff. I then use the estimated compensating differential for the risk of sexual harassment to calculate the value of statistical harassment in a manner parallel to the calculation of the value of statistical life. This value is about $7.6 million (in $2017) per sexual harassment claim filed with the EEOC, far above the current maximum damages award under Title VII of $300,000 for the largest firms. I conclude with a discussion of the policy implications for establishing incentives for efficient deterrence to reduce the prevalence of workplace sexual harassment.",12
57.0,2.0,Journal of Risk and Uncertainty,08 November 2018,https://link.springer.com/article/10.1007/s11166-018-9290-6,Ambiguity framed,October 2018,Mark Schneider,Jonathan W. Leland,Nathaniel T. Wilcox,Male,Male,Male,Male,"Expected utility (EU) theory (Von Neumann and Morgenstern 1947) and subjective expected utility (SEU) theory (Savage 1954) are widely recognized as the standard models of rational decision making under risk and uncertainty. Both models have also been applied as descriptive theories of actual behavior, although persistent empirical challenges were raised soon after the models were introduced. Allais (1953) devised pairs of choices, one involving a certain outcome and a risky prospect and the other a choice between two risky prospects where people frequentlyFootnote 1 violate the independence axiom of EU. Ellsberg (1961) presented pairs of choices each involving a risky prospect (whose probabilities are given) and an uncertain prospect (whose probabilities are unknown) where people frequently violate the “sure-thing” principle of SEU. In his exposition of subjective expected utility, Savage (1954) digressed to address the Allais-type violations of the independence axiom. He conjectured that these violations might be reduced if the choice situations were reframed in a transparent format. Tests of this prediction, discussed below, have consistently found that the Allais paradox is susceptible to framing, with significantly fewer violations in Savage’s proposed presentation format. Since the Ellsberg paradox also violates an independence condition, we ask whether applying Savage’s presentation format to Ellsberg-style choices leads to fewer violations of SEU. To our knowledge this question has not been previously investigated. We ground our investigation in new and rigorous theory formalizing the notion of a transparent frame (Leland and Schneider 2016) and recent theory formalizing salience (e.g., Bordalo et al. 2012; Kőszegi and Szeidl 2013).",3
57.0,2.0,Journal of Risk and Uncertainty,06 November 2018,https://link.springer.com/article/10.1007/s11166-018-9291-5,Reporting probabilistic expectations with dynamic uncertainty about possible distributions,October 2018,Charles Bellemare,Sabine Kröger,Kouamé Marius Sossou,Male,Female,Unknown,Mix,,
57.0,2.0,Journal of Risk and Uncertainty,25 October 2018,https://link.springer.com/article/10.1007/s11166-018-9289-z,Present bias and health,October 2018,Yang Wang,Frank A. Sloan,,,Male,Unknown,Mix,,
57.0,3.0,Journal of Risk and Uncertainty,28 December 2018,https://link.springer.com/article/10.1007/s11166-018-9293-3,Boundedly rational expected utility theory,December 2018,Daniel Navarro-Martinez,Graham Loomes,Larbi Alaoui,Male,Male,Male,Male,"Economics is often said to be the study of the allocation of scarce resources, of how human beings decide to combine their time and skills with physical resources to produce, distribute and consume. However, economic models may sometimes ignore the fact that arriving at decisions is itself an economic activity and that the hardware and software involved—that is, the human brain and its mental processes—are themselves subject to constraints. Herbert Simon emphasised this point in his 1978 Richard T. Ely lecture, in which he discussed the implications of attention being a scarce resource.Footnote 1 In a world where there are many (often complex) choices to be made, spending time on any one decision entails an opportunity cost in terms of the potential fruits of other decisions that might have been considered instead. Being unable to devote unlimited time and attention to every decision they encounter, humans generally have to satisfice rather than optimise. Simon bemoaned the lack of interest among economists in the processes that individuals use when deciding how to allocate their scarce mental resources, and he advocated “building a theory of procedural rationality to complement existing theories of substantive rationality”. He suggested that “some elements of such a theory can be borrowed from the neighboring disciplines of operations research, artificial intelligence, and cognitive psychology”, but noted that “an enormous job remains to be done to extend this work and to apply it to specifically economic problems” (Simon 1978, pp.14–15). Although there have been some developments along these lines (e.g., Gilboa and Schmeidler 1995; Rubinstein 1988; Gigerenzer and Selten 2001), the decades that followed Simon’s lecture saw the mainstream modelling of individual decision making—especially with respect to choice under risk and uncertainty—take a different direction. Stimulated by experimental data that appeared to violate basic axioms of rational choice, a number of models appeared at the end of the 1970s and in the early 1980s that sought to provide behavioural alternatives to standard Expected Utility Theory (EUT)—see Starmer (2000) for a review. Typically, these were deterministic models that relaxed a particular axiom and/or incorporated various additional features—e.g., reference points, loss aversion, probability weighting, regret, disappointment—to try to account for certain regularities in observed decisions. While such models provided more elaborate descriptive theories of choice, little or no consideration was given to the mental constraints referred to by Simon. His invocation to build boundedly rational procedural models largely fell by the wayside in the field of risky decision making. Thus we now have an impressive array of alternative deterministic models, each of which can claim to accommodate some (but not all) of the observed departures from EUT. However, these models have no intrinsic explanation for at least three other pervasive empirical regularities in the data, which may arise from features of the decision-making process: first, the probabilistic nature of most people’s decisionsFootnote 2; second, the systematic variability in the time it takes an individual to respond to different decision tasks of comparable complexityFootnote 3; and third, the degree of confidence decision makers (DMs) express about their decisions.Footnote 4 In this paper, we propose to explore the direction Simon advocated and investigate the potential for applying a boundedly rational deliberative process to the ‘industry standard’ model of decision making under risk and uncertainty, EUT. We start by identifying in general terms what is required of a procedural model of preferential choice. We then consider how the various components of such a model might be specified in ways that are in keeping with conventional economic assumptions while at the same time allowing for scarcity of time and attention. The resulting model—which we call Boundedly Rational Expected Utility Theory (BREUT)—generates a number of implications, not just about choice probabilities, but also about process measures such as response times and confidence in the decisions made. One striking result is that, despite being based upon EU preferences, the model produces some choice patterns that deviate from EUT in line with several well-known decision-making phenomena. This highlights the influence of the processes that lead from the ‘core’ assumptions about preferences to observable choices. At the same time, there are other choice patterns that are not accommodated by BREUT. So we are not proposing BREUT as a descriptive model that can account for all known empirical regularities; nor is it intended to provide a literal representation of the way the mind actually operates. Rather, this paper may be understood as a ‘proof of concept’ exercise, which demonstrates the implications of embedding a deterministic core in a simple boundedly rational apparatus to generate decisions. As we shall explain in due course, our broad modelling strategy has the potential to be extended to many non-EU core theories, some of which may accommodate more or other known regularities. In the next section, we present our instantiation of BREUT, focusing upon the kind of binary choices between lotteries with monetary outcomes that have been the staple diet of many decision-making experiments. In Section 3, we demonstrate how BREUT provides a parsimonious account of the systematic relationship between choice probabilities, decision time and confidence. We show that the model entails respect for first order stochastic dominance and weak stochastic transitivity, but allows patterns of choice that violate strong stochastic transitivity, independence and (to some extent) betweenness. In the final section, we consider the relationship between our model and others in the psychology and economics literature. We discuss some limitations of the model in its current form, together with what we see as the most promising directions for extending this approach. Some theorems and their proofs can be found in the online appendix.",13
57.0,3.0,Journal of Risk and Uncertainty,10 December 2018,https://link.springer.com/article/10.1007/s11166-018-9292-4,Decision irrationalities involving deadly risks,December 2018,W. Kip Viscusi,Scott DeAngelis,,Unknown,Male,Unknown,Male,"Sequential decisions in many uncertain contexts offer the potential for learning. However, the experimental process in which the decision maker learns about the underlying probability may not be costless. Unsuccessful outcomes may have dire consequences, as with decisions posing risks of death. These adverse effects not only impose costs, but also alter the structure of the decision problem, making it more challenging, and posing additional challenges for individual rationality. There are no such costs in the classic economic paradigm for analyzing optimal economic experiments, the two-armed bandit model. In that basic model, the experimental subject is engaged in a sequence of choices from the two arms, each of which offers some constant probability of success with an associated payoff on each trial and a probability of an unsuccessful outcome with no reward. Following the familiar distinction between situations of risk and those of uncertainty (Knight 1921), these probabilities may be known with precision to the experimenter, which is a situation of risk. Or the experimenter may be in a situation of uncertainty in which the chance of success is not well known. Situations involving only risk generate no opportunity for learning, as the optimal choice is to choose the option with the higher probability of success. But if there is uncertainty, there is the potential to observe the outcome of each trial and then to update one’s beliefs based on these results as in standard Bayesian learning. If the options are independent, the optimal strategy is to stay on a winner but to switch if the experiences are sufficiently unfavorable (Yakowitz 1969; Berry 1972; Berry and Fristedt 1985). Although such models of experimentation have many applications, this experimentation process imposes no permanent cost if the trial is unsuccessful. However, many experimental contexts entail substantial costs associated with unsuccessful trials. In some situations, the experimentation process may end after an unsuccessful outcome. An economic situation that gave rise to analyses of this decision structure is that in which a worker is making choices on the job involving a sequence of lotteries on life and death (Viscusi 1979). The worker has a choice of jobs in each period, and successful outcomes permit the worker to continue making the choice in future periods. Unlike conventional two-armed bandits, after an unsuccessful outcome the experimentation process ends. Other labor market outcomes such as being fired or disabled after an unsuccessful lottery outcome likewise may share a similar decision structure. Another prominent example of the applicability of such models is medical treatments involving alternative drugs (Berry and Viscusi 1981). Suppose that the patient can be treated with one of two alternative drugs, which either leads to survival in that week or death. For this experimental situation, in which the learning process is within a drug trial rather than across trials, the problem structure is that of a two-armed bandit with a chance of termination. Applications to other phenomena, such as environmental decisions with a probability of landing in an absorbing state, may share a similar structure, as do research projects in which funding is terminated after the first unsuccessful research outcome. The optimal strategy for this class of problems differs from that in conventional two-armed bandit models. There is no opportunity to alter decisions after unsuccessful outcomes so that the optimal strategy will never involve switching behavior. The task is to choose wisely in the initial round and to continue with that choice as long as the streak of successes continues. Despite the asymmetric nature of the learning process, ambiguity and learning are consequential. In particular, for any given mean probability of success, greater ambiguity is desirable. Increases in ambiguity with respect to the probability of success offer greater opportunities for long-term gains because of the greater chance that the underlying probability of success for that option offers a high chance of success on each trial (Viscusi 1979; Berry and Viscusi 1981). We can illustrate this result using the following two-period example involving options offering a choice between a risky probability and an uncertain probability, each of which is 0.5 on the initial round. Trials are independent and identically distributed. Suppose the player earns a payoff of $1 for a successful outcome, and play ends after an unsuccessful outcome. The risky option 1 offers a probability of success of 0.5 in each period so that the probability of having a successful outcome in both periods is 0.25. For the risky option 1, the expected reward is $0.50 in period one, and $0.25 in period 2, for a total payoff of $0.75. Let’s assume that the uncertain option 2’s assessed chance of success is governed by the uniform Beta distribution, so that the initial chance of success is 0.5, but after a successful outcome in period one it is updated to 0.67. The ambiguous option 2 offers an expected reward of $0.50 in period one and $0.34 in period two, for a total payoff of $0.84, which exceeds the expected reward from the risky option 1. The advantage of the uncertain option 2 increases with the degree of updating after successful outcomes or, put somewhat differently, with the extent of the risk ambiguity. The problem structure involves a sequence of choices under risk and uncertainty, posing challenges for rational optimizing behavior. For any given mean probability of success, increasing ambiguity boosts the expected value of that option if there are multiple rounds because of the greater influence of learning when initial risk beliefs are less precise. But if individuals are ambiguity-averse, then that influence will be a deterrent to exploiting the gains from increased uncertainty. Ambiguity aversion plays a prominent role in behavioral economics generally (e.g., Ellsberg 1961; Camerer and Weber 1992; Machina and Siniscalchi 2014; Bellemare et al. 2018), as well as in previous experimental studies of optimal experimentation models (e.g., Liu and Colman 2009; Anderson 2012; Trautmann and Zeckhauser 2013; Moreno and Rosokha 2016; Engle-Warnick and Laszlo 2017). Switching behavior would appear to be simpler for the class of two-armed bandit problems with a chance of termination in that the problem ends after an unfavorable outcome and the optimal strategy never involves switching after favorable outcomes. Nevertheless, people may not always stay on a winner, creating a clear-cut departure from optimal behavior if their initial choice was their expected value maximizing decision. Previous studies of risky decisions in models without the chance of termination and for which switching may be optimal have often found that observed switching behavior was often not expected value maximizing, both for studies in which there was no opportunity for learning as well as studies in which learning plays an important role (Charness and Levin 2005; Charness et al. 2007; Trautmann and Zeckhauser 2013). Given the distinctive characteristics of the problem structure considered here and the difference in the optimizing strategies from the more basic class of two-armed bandit problems, previously documented shortcomings in rational behavior may arise to a different degree, and new departures from optimizing behavior may be evident. Section 2 summarizes the principles governing the optimizing criteria for sequential decisions with the chance of termination. The optimal strategy never involves switching even if the choices are interdependent. A single-arm strategy is always optimal. Consequently, the stay-on-a-winner rule continues to hold for this class of problems, and the opportunity to switch after an unsuccessful trial never arises. We examine how people make these decisions using the experimental design described in Section 3. Participants engaged in a series of choices from two alternatives, where the designs included both independent choices and interdependent choices. In each instance, it is possible to assess the optimality of the choices, the patterns of errors in switching behavior, and whether participants improve their performance with successive repetitions of the game. The experimental results in Section 4 consider the simpler case in which the choices involve lotteries that are independent. Although the experimental subjects did not always avoid the ambiguous option, they failed to fully exploit the gains from ambiguity even when offered a more ambiguous alternative that offered the same initial mean probability of success. Participants sometimes switched away from the optimizing ambiguous choice, but there was more evidence of switching to correct mistaken initial decisions than switching away from the optimal choice. Consequently, within a particular session of play, the experimental results were broadly consistent with key aspects of rational learning behavior. However, there was no evidence that participants improved their performance across different sessions in which the experimental structure was replicated. The introduction of interdependent choices in Section 5 leads to different optimizing criteria and behavior than in conventional two-armed bandits. Unlike conventional two-armed bandits with interdependent choices, for two-armed bandit models with risks involving the chance of termination, the optimal choice never involves switching behavior. However, if the person made a non-optimal choice initially, successful trials with this less desirable option may provide information about the underlying state of the world that is shared between the options, making switching desirable so as to correct the initial mistaken choice. Many participants corrected their decisions in this manner. There was also evidence that increasing the mean probability of success matters, consistent with the usual rationality requirements regarding monotonicity. Another favorable result was that switching from the very desirable uncertain option was rare. The most glaring failure was the same as in the case of the independent choice experiments in that there was no evidence of learning across repeated sessions. Although each of the 10 sessions for independent and interdependent choices followed the previous session close in time, performance did not improve across sessions. The concluding Section 6 recounts the mixed performance of the experimental subjects. Participants were more deterred by risk ambiguity than they should be in their initial choice since ambiguity is a desirable feature that enhances the chance of long-term survival. This concept is not particularly intuitive, and tendencies toward ambiguity aversion may deter experimental subjects from learning about the potential dividends from ambiguous probabilities. Participants also switched more often than would be consistent with optimal behavior since optimal decisions in these models never entail switching. The most prominent departure from rationality is that over the course of ten sessions with each of the four treatments, there was no improvement in average performance. While there is frequent evidence of some aspects of rational optimizing behavior within particular experimental sessions, across sessions there was no evidence that participants benefited from learning. This result is counter to economists’ frequent assumption that even if people err, with additional experience and repetitions of the decisions, they will alter their decisions to eliminate behavioral failures. However, this negative result is consistent with other findings in which subjects display very limited improvement across sessions when confronting challenging tasks (Charness and Levin 2005, 2009).",3
57.0,3.0,Journal of Risk and Uncertainty,22 December 2018,https://link.springer.com/article/10.1007/s11166-018-9294-2,Dinner with Bayes: On the revision of risk beliefs,December 2018,Christoph M. Rheinberger,James K. Hammitt,,Male,Male,Unknown,Male,"People often respond to public health policies in ways that are inconsistent with economic theory. They overreact to some risks while they ignore others (Slovic et al. 2000); they are reluctant to give up unhealthy behaviors though they know it would be better for them (O’Donoghue and Rabin 2001); and they take healthy behaviors as an excuse for indulging in unhealthy ones such as eating more when foods are low in calories (Wisdom et al. 2010) or smoking cigarettes down to the nub while trying to quit (Adda and Cornaglia 2006). Reasons for these obvious deviations from the rational consumer model are manifold and include cognitive and attentional limitations, emotional arousal, various forms of procrastination, and difficulties in processing probabilistic information (McFadden 2006). In this paper, we study how consumers perceive an everyday health risk—contracting a foodborne illness—before and after the provision of relevant information. Since risk perception is a crucial link in the causal chain between consumer information and behavioral responses, understanding better how consumers form their beliefs and how they adjust them to new information is of considerable interest to policymakers (Magat and Viscusi 1992; Viscusi 1998). In the context of food safety, the interest is fueled by its implications for the evaluation of existing policies and by regulatory needs to accurately predict behavioral responses to new consumer information and awareness campaigns. Success or failure of such campaigns matters because each year foodborne pathogens cause billions of episodes of illness worldwide; in the U.S. alone, the annual welfare cost of foodborne illness is estimated to exceed $50 billion (Scharff 2012). Two questions emerge. Do public information programs affect health risk perceptions? And if so, do they alter consumer behavior in the predicted manner? Answers to these questions require a better understanding of the processing of risk-related information. Indeed, the uptake of information plays a key role in studies of risk perception. Both economists and psychologists have long recognized that people make a number of common mistakes when they update risk beliefs with newly available information: small risks tend to be overestimated, while large ones tend to be underestimated (Kahneman 2003); risks are assessed based on emotions rather than cognitive evaluations (Loewenstein et al. 2001; Slovic et al. 2004); and more attention is given to bad news than to good news (Viscusi 1997).Footnote 1 Over the past 30 years, a substantial number of empirical studies have addressed the impact of information on subjective risk beliefs. Here, we summarize the most important insights gained. In a landmark study, Viscusi and O’Connor (1984) elicited chemical workers’ perception of job hazards based on warning labels and found that most workers displayed the capacity to consistently update their probabilistic beliefs with new information. In subsequent work, Viscusi (1997) studied location decisions in the presence of ambiguous information about air pollution and discovered that in contexts with multiple and conflicting sources of information respondents place disproportionate weight on alarmist information. Smith and Johnson (1988) studied the effect of public information programs on homeowners’ attitudes toward the health risk associated with radon exposure. Their results support a modified form of a Bayesian learning model for best describing individuals’ response to information about the risk of radon. Dickie and collaborators explored public perceptions of skin cancer (1996, 2007) and leukemia (Gerking et al. 2014) and consistently found respondents accounting for personal risk factors (e.g., complexion and sunlight exposure history) when assessing their own risk. They also observed that less knowledgeable and more concerned individuals demonstrated a greater propensity to use information provided. While previous studies concluded that most people revise their risk beliefs in a manner broadly consistent with Bayesian inference, they largely ignored the endogenous nature of health risks with people often having private information about their health and taking precautionary measures to reduce the likelihood or severity of bad health outcomes. Private information and precautionary measures are typically unobserved in observational studies, but may systematically affect both perceptions and actions. For example, consumers choose the quality, storage place, and preparation of their foods, thereby affecting the likelihood of contracting a foodborne illness (Shogren and Stamland 2007). It might thus be perfectly rational for a consumer to believe her risk differs from the population average risk even if she is otherwise similar to the average consumer. Such an individualization of risk information may, however, be a major source of error because many information structures generate correlated rather than mutually independent signals. As Enke and Zimmermann (2018) demonstrate, many people fail to realize these dependencies and overshoot when forming or revising their beliefs. This paper presents a belief-elicitation protocol that permits capturing the impact of precautionary behaviors and other idiosyncratic factors affecting both the formation and the revision of risk beliefs. In what is essentially a panel structure, a representative sample of French consumers stated their perceived chance of contracting a foodborne illness from eating fish. We first elicited subjects’ risk beliefs without providing any specific information. We repeated the elicitation after informing subjects about the average consumer’s fish consumption, the corresponding population average risk, and the prevalence of various risky and risk-averting behaviors. The chained elicitation procedure allows us to explore subjects’ responses to risk-related information, heterogeneity in the revision of risk beliefs, and deviations from the Bayesian rationality assumption that underlies the design of most—if not all—consumer information campaigns. In a nutshell, we find that there is heterogeneity in belief revision but the majority of subjects updated their beliefs consistently with the Bayesian learning hypothesis. These subjects responded to information about the population average risk by reducing their prior beliefs if these were above the population average risk and by increasing their prior beliefs if they were below the population average risk. Precautionary effort in handling and preparing food reduced prior risk beliefs, but did not affect the belief updating process. This finding underpins the importance of controlling for confounding factors in understanding how individuals form and revise their risk beliefs and has implications for predicting the effectiveness of health and consumption advisories. For example, many respondents seem able to draw reasonable inferences from their precautionary behavior about their own risk and to use information about average risk and precautionary behavior in the population to update their beliefs in a Bayesian fashion. However, it is less clear that they can combine these pieces of information in a coherent manner, leading many respondents to update their beliefs quite drastically. Others do not consistently update at all and may be better served by more direct messages about the risks of certain behaviors. Using finite mixture models we decompose the heterogeneity in belief revision and find four distinct updating patterns: (1) subjects who aggressively adjust their beliefs toward the population average risk; (2) subjects who modestly revise their beliefs toward the population average risk; (3) subjects who ignore the new information altogether; and (4) subjects who update in a manner inconsistent with the information provided. The first two patterns are entirely consistent with the Bayesian updating hypothesis, the third pattern may or may not be consistent depending on the reason to ignore new information, and the fourth pattern is clearly inconsistent. The mixture modeling approach allows us to link the emerging patterns to personal characteristics. We find that older, less educated and less numerate subjects are more likely to adapt either strategy (3) or (4) when updating their risk beliefs. Both refusal of information and lack of numeracy are problematic from the regulator’s point of view as they are associated with violation of Bayesian updating and undermine the efficacy of public health policies that seek to change behavior by informing consumers. Our paper makes several contributions. We analyze a rich data set which enables us to disentangle the effects of endogenous precautionary effort on the formation versus the revision of risk beliefs. Unlike the studies cited above, our experimental task elicited risk beliefs conditional on a future foodborne illness, which requires respondents to evaluate the relative probabilities of multiple causes of foodborne illness rather than the marginal probability of illness. We believe this makes the updating less onerous and more tractable for subjects. Another innovation is that we estimate beta regression models to account for the theoretical underpinnings of the Bayesian learning model (Viscusi 1979). Finally, we account not only for observable, but also for unobservable heterogeneity in belief updating. The paper proceeds as follows. In Section 2, we operationalize the Bayesian learning model and derive a formal definition of rational updating that is conditional on the precautionary effort expended by the updater. Section 3 provides details of the belief-elicitation task and the sample characteristics. Section 4 summarizes our econometric approach (details are given in the Appendix). Section 5 presents the results of our study. In Section 6, we discuss the response to information at the individual and the aggregate level. Section 7 concludes.",8
57.0,3.0,Journal of Risk and Uncertainty,29 December 2018,https://link.springer.com/article/10.1007/s11166-018-9295-1,Subjective beliefs and confidence when facts are forgotten,December 2018,Igor Kopylov,Joshua Miller,,Male,Male,Unknown,Male,"Forgetting is manifested in many distinct contexts and behaviors. Misidentifications in eyewitness testimonies played a role in over 70% of 358 wrongful convictions overturned by DNA evidence in the Innocence Project.Footnote 1 Military officers forget how to identify threat vehicles (Rowan 2015), radiologists cannot recall details of processed exams even after short interruptions (Froehle and White 2014), and occupational first aiders exhibit a severe loss of skill with less than 20 percent being able to perform CPR one year after training (McKenna and Glendon 1985). Besides the accuracy (or reliability) of memory, the confidence that people have in their recall is essential for their own actions and others’ decisions. Memory failures are especially harmful when coupled with high confidence. For example, juries are more likely to believe witnesses who appear very confident and excuse inaccuracies in their testimony compared to witnesses who appear less confident but give accurate testimonyFootnote 2 (Brewer and Burke 2002; Lindsay et al. 1981). Various links between the accuracy and self-reported confidence of people’s memories have been observed in legal and experimental contexts (see reviews in Wixted and Wells 2017; Roediger et al.2012). In this paper we present new empirical evidence on the behavioral consequences of forgetting in decision making under uncertainty. We run several experiments where people bet on propositions (facts) that they cannot recall with certainty. Forgetting is induced via interference tasks and time delays of up to one year. Our main methodological novelty is that we capture memory failures via betting preferences rather than via direct verbal queries. We hypothesize that this choice-based design should identify some forms of memory decay, over/under confidence, ambiguity attitudes, and provide additional insights on the evolution of beliefs and confidence of forgetful agents. More formally, we elicit our subjects’ monetary evaluations of binary bets. Any such bet 
  delivers a fixed prize of Z = 11 euros if the corresponding proposition B is true and nothing otherwise. Prior to evaluating tB, each subject is instructed to memorize whether B is true or false. Assuming perfect memory, her evaluation of tB should be Z = 11 if B is true, or zero if B is false. Naturally, forgetting should decrease the value of tB below Z if B is true, but increase it above zero if B is false. This methodology should be more suitable than verbal queries in various economic settings where incentives are monetary and forgetting can be significant.Footnote 3 Moreover, our framework allows us to define revealed beliefs as in de Finetti (1937) and Savage (1972), and then analyze the accuracy of these beliefs. It is well-known that memory becomes less accurate over time. In the extensive psychological literature (see Rubin and Wenzel 1996; Brown et al.2007 and references therein), this phenomenon is called memory decay and quantified with forgetting curves. The effect of time delays on confidence is more controversial. In particular, Shaw and McClure (1996) showed that repeated questioning increased witnesses’ confidence without increasing their accuracy. In our experiments, beliefs become less accurate over time, and subjects’ level of confidence in their beliefs decreases. To elaborate, all participants in our experiment first learn a list \(\mathcal {M}\) of true propositions. The contents of this list are novel to all participants and do not pertain to their general knowledge, skills, or previous actions. We find that for true (false) propositions B, the average monetary value of tB diminishes (increases) after interference tasks, and then diminishes (increases) further with a time delay of one year. To analyze memory decay in more detail, we use revealed beliefs. Given any true proposition A in the list \(\mathcal {M}\), a subject reveals a positive belief for A if she values the ticket tA more than t¬A. Similarly, her revealed belief is called negative if she values t¬A more than tA, or indeterminate if she values tA and t¬A equally. Thus we partition the entire array \(\mathcal {D}\) of data points into three disjoint parts \(\mathcal {P}\), \(\mathcal {N}\), and \(\mathcal {I}\) that correspond to positive, negative, and indeterminate beliefs respectively. We observe that the proportion of positive beliefs \(\frac {|\mathcal {P} |}{|\mathcal {D} |} \) decreases after interference tasks and then decreases further one year later. Conversely, negative and indeterminate beliefs became more common over time. A more intriguing finding is that negative beliefs are highly transient as if most of them are based on random spontaneous hunches rather than some more permanent convictions. Next, we find overconfidence on the subdomain \( \mathcal {P} \cup \mathcal {N}\) where revealed beliefs can be either positive or negative, but not indeterminate. On this subdomain, subjects behave as if they overestimate the average accuracy of their strict beliefs \(\frac {|\mathcal {P} |}{|\mathcal {P} |+ |\mathcal {N} |}\). On average, they overpay (underpay) for tickets tB that they find more (less) valuable than the opposite bets t¬B. In bookmaking terms, the monetary odds that are required to bet on B are respectively too short or too long. This pattern is roughly in line with many studies where subjects are found to overestimate their own knowledge (e.g. Fischoff et al. 1977; Keren 1991) ability, performance, and level of control (see Moore and Healy 2008 for an overview). However, our design can be distinguished from this literature by the use of binary bets and two other features. 
 (i) The truthfulness of all relevant propositions is stated explicitly before evaluation stages rather than derived from general knowledge or intellectual efforts. Moreover, a subject’s beliefs about the same proposition is elicited at several points in time. By contrast, Lichtenstein and Fischhoff (1977) and other studies of overconfidence use questions about general knowledge and do not observe how subjects’ responses change over time (presumably, because general knowledge is unlikely to vary in the short run). (ii) Subjects evaluate propositions that pertain to objective facts, rather than to assessments of their own performance, ability, or actions. Thus we observe overconfidence without above-average effects studied by Camerer and Lovallo (1999), Kirchler and Maciejovsky (2002), and Hoelzl and Rustichini (2005), and others. In a different vein, we use our data to identify a new kind of ambiguity aversion. Roughly speaking, our experiments suggest that people may be even less willing to bet when they cannot recall the truth rather than when they never have learnt it at all. We attribute this form of the Ellsberg Paradox to comparative ignorance. Fox and Tversky (1995) use this term broadly to describe ambiguity aversion driven by a “feeling of incompetence” and a contrast with more knowledgeable decision makers (see also Heath and Tversky 1991; Trautmann et al. 2008). Our subjects may exhibit comparative ignorance because of the contrast between having forgotten the truth and knowing it in the past. Obviously, overconfidence and ambiguity aversion are inconsistent with the expected utility model and well-calibrated beliefs that are typical in game theory (e.g. Piccione and Rubinstein 1997a, b). To accommodate the observed behavioral patterns, we formulate a two-signal model of forgetting in Section 4. We assume that agents can receive signals of two types: a memory signal that points to the true proposition with probability \(\alpha \geq \tfrac 12\) and a noisy hunch that is objectively uninformative, but affects subjective evaluations nonetheless. We assume that the memory signal becomes less common over time. In the absence of the memory signal, subjects use their transient hunches, but become ambiguity averse. We run a simulation of the model that generates memory decay, overconfidence, and comparative ignorance. Moreover, our model suggests the following two hypotheses: 
 (H1) negative beliefs should be more transient than positive ones, (H2) subjects should be more confident in positive rather than negative beliefs when their memory retains statistical accuracy. We find both patterns in the data. There are other models (e.g. Erev et al. 1994; Costello and Watts 2014) that introduce noise into probabilistic assessments to generate overconfidence and related biases. However, we are not aware of any such models that accommodate ambiguity aversion together with overconfidence. Moreover, our two-signal approach can be additionally motivated by the good fit that it achieves for our aggregate data and its testable hypotheses H1 and H2 that we confirm empirically. Blavatskyy (2009) studies overconfidence via evaluations of monetary bets on general knowledge questions. He also uses a different incentive compatible scheme where subjects make probabilistic rather than monetary evaluations. He finds little overconfidence regardless of ambiguity attitudes. Ericson (2011) observes a strong overconfidence about one’s own future memory accuracy. Only 53% of his subjects remembered to send an email to claim a twenty-dollar payment six months after the experiment. The beliefs revealed by their previous choices imply a forecast of a 76% claim rate. Inattention provides another explanation for the failure to respond rationally to some fees and incentives (e.g. the operating costs of mutual funds in Barber et al.2005 and the sales tax in Chetty et al. 2009). In our experiment we did not have an independent measure of attention and therefore we cannot rule out the possibility of inattention explaining some of the heterogeneity in choices between subjects. Nevertheless, inattention cannot account for the key within-subject differences between treatments. In particular, inattention cannot account for the persistent pattern of decreasing accuracy and confidence in beliefs over time. In more abstract settings, memory limitations have been used to explain dynamic inconsistency (Piccione and Rubinstein 1997b; Battigalli 1997), inertia and impulsiveness in decisions (Hirshleifer and Welch 2002), availability heuristics and confirmation biases (Wilson 2003; Mullainathan 2002), sunk cost fallacy (Baliga and Ely 2011) and other patterns. In such applications, memory failures are interpreted as a separate source of uncertainty that distorts previously available information and signals.",2
58.0,1.0,Journal of Risk and Uncertainty,24 April 2019,https://link.springer.com/article/10.1007/s11166-019-09296-3,The value of a statistical life under changes in ambiguity,February 2019,Han Bleichrodt,Christophe Courbage,Béatrice Rey,,Male,Female,Mix,,
58.0,1.0,Journal of Risk and Uncertainty,28 February 2019,https://link.springer.com/article/10.1007/s11166-019-09297-2,Interpersonal discounting,February 2019,Rong Rong,Therese C. Grijalva,W. Douglass Shaw,,Female,Unknown,Mix,,
58.0,1.0,Journal of Risk and Uncertainty,30 March 2019,https://link.springer.com/article/10.1007/s11166-019-09298-1,Looking ahead: Subjective time perception and individual discounting,February 2019,W. David Bradford,Paul Dolan,Matteo M. Galizzi,Unknown,Male,Male,Male,"Time discounting is considered a fundamental characteristic of human decision-making (Frederick et al. 2002). For example, individuals with higher discount rates may be less willing to invest in painful activities in the present even if such investments yield substantial benefits in the future (Barsky et al. 1997; Chapman and Coups 1999; Chabris et al. 2008; Bradford 2010; Sutter et al. 2013). Multiple empirical methods have been developed over the past decades to estimate individual levels of discounting, ranging from survey questions involving hypothetical payouts (Dolan and Gudex 1995; Van der Pol and Cairns 2001), to laboratory or field experiments using incentive-compatible methods (Coller and Williams 1999; Harrison et al. 2002; Andersen et al. 2008, 2014; Epper et al. 2011; Andreoni and Sprenger 2012; Laury et al. 2012; Attema et al. 2016). Perhaps the most widely debated finding in the literature is that individuals do not appear to always discount the future at a constant rate: discount rates tend to be higher for more proximate time periods and lower for more distal ones (Thaler 1981; Kirby and Marakovic 1995; Coller et al. 2012). This phenomenon has been typically explained in terms of hyperbolic or quasi-hyperbolic time discounting (Loewenstein and Prelec 1992; Laibson 1997). Several alternative accounts have been proposed to explain non-constant discounting (Holden and Quiggin 2017). Ainslie (1975) relates it to individual impulsivity, whereas Loewenstein (1996) to the tempting influence and temporal proximity of “visceral factors” such as hunger, sexual arousal, cravings, and physical pain. Trope and Liberman (2003, 2010) point to different representations of near and distant future events in terms of cognitive concreteness. Others argue that declining discounting rates could also be due to “sub-additive discounting”: the fact that the overall time horizon is partitioned into subintervals can increase the salience of the partitioned time components, and lead to higher discounting (Read 2001; Scholten and Read 2006). Prelec and Loewenstein (1991) and Frederick et al. (2002) observe that, as future payouts are inextricably associated with uncertainty, our valuation of inter-temporal tradeoffs not only depends on our “pure” time preferences, but also on perceived risks associated with the delay (Chesson and Viscusi 2000; Andersen et al. 2008; Coble and Lusk 2010). Epper et al. (2011) find that hyperbolic discounting is significantly associated with non-linear probability weighting in the subjective perception of probabilities (“sub-proportionality”). The presence of hyperbolic and quasi-hyperbolic discounting, moreover, is not ubiquitous in the literature. Andersen et al. (2008, 2014) elicit and jointly estimate risk and time preferences for representative samples of the Danish population and find clear evidence of constant discounting. Also Andreoni and Sprenger (2012) find no evidence of present bias in their experiment. Negative evidence about hyperbolic or quasi-hyperbolic discounting has also been documented by Anderhub et al. (2001), Harrison et al. (2002), and Laury et al. (2012), among others. In a recent literature review, Andersen et al. (2014) notice that most incentive-compatible evidence of hyperbolic and quasi-hyperbolic discounting occurs in samples of college students, while there is little evidence from real money choices of adult respondents over typical horizons of months (Andersen et al. 2014). Attema et al. (2016) compare discounting measured by the conventional “utility method” with a new “direct method” which measures discounting directly without the need of measuring utility, and find that, while there is prevailing evidence of decreasing impatience using the utility method, there is only weak evidence of decreasing impatience using the direct method, which explains the poor performance of the hyperbolic discount function to explain their data. Attema et al. (2018) extend the “direct method” to compare discounting for money and health, and find that constant discounting fit the data better than hyperbolic discounting. There is a separate branch of the literature that suggests that the observed behavior may be less based in inconsistencies in actual time preferences than in perceptions about time duration (Read 2001; Prelec 2004; Zauberman et al. 2009). In particular, there is evidence suggesting that human understanding of time—either retrospectively or projected forward in time—is not a linear mapping from calendar time: e.g., 2 years are subjectively perceived as less than twice as far away as 1 year (Hornik 1984; Kim and Zauberman 2009). That people would perceive time in this manner can be explained by appealing to what is known as the “Weber-Fechner law”, a fundamental principle in the psychology of perception that—together with “Stevens’ power law” (Stevens 1957)—has been widely documented for other neuro-physiological stimuli such as heat, sound, and light. If these psychophysics laws also hold for time perception, then calculations of the discount rates based on the assumption that perception of time duration is linear may be distorted. In this paper, we simultaneously elicit and estimate respondents’ subjective perception of time and implied discount rates. We innovatively build on the literature, and in particular on the method developed by Kim and Zauberman (2009) and Zauberman et al. (2009) to measure time perception, in four ways: i) we elicit time discounting using state-of-the-art experimental tests with real monetary rewards; ii) we consider different time intervals spanning from 1 day to 1 year to isolate “first period” effects and hyperbolic versus quasi-hyperbolic discounting; iii) we adjust for individual heterogeneity in the subjective time scales to draw cross-subjects comparisons; iv) we look at the links between retrospective and prospective perception of time. We find that individuals do indeed compress future time perception in ways very reminiscent of the general Weber-Fechner and Stevens principles. Further, once that compression is taken into account, we find evidence that discount rates are higher for today versus later times, but essentially constant and statistically undistinguishable for all later times from 1 week onwards. While discounting rates based on “objective time” replicate the usual hyperbolic preferences, much of the hyperbolic pattern is absent in the curve of the discounting factors obtained from “subjective time”, which closely resembles the shape of a quasi-hyperbolic function. Thus, we argue that there is good reason to suspect that considering direct measures of subjectively perceived time instead of objective time can lead to different conclusions on the estimated discount rates even when these are elicited through experimental tests with real monetary rewards. The rest of the article is structured as follows. In Section 2 we review the background, the objectives, and the main features of our study, and its contributions to the literature. Section 3 describes the experiment, while in Section 4 we present the econometric model. Section 5 discusses the results, while in Section 6 we conclude by discussing some of the implications of our findings.",23
58.0,1.0,Journal of Risk and Uncertainty,22 April 2019,https://link.springer.com/article/10.1007/s11166-019-09299-0,Measuring ambiguity preferences: A new ambiguity preference survey module,February 2019,Elisa Cavatorta,David Schröder,,Female,Male,Unknown,Mix,,
58.0,2.0,Journal of Risk and Uncertainty,28 June 2019,https://link.springer.com/article/10.1007/s11166-019-09307-3,Risk guideposts for a safer society: Introduction and overview,June 2019,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
58.0,2.0,Journal of Risk and Uncertainty,11 May 2019,https://link.springer.com/article/10.1007/s11166-019-09300-w,Ruining popcorn? The welfare effects of information,June 2019,Cass R. Sunstein,,,,Unknown,Unknown,Mix,,
58.0,2.0,Journal of Risk and Uncertainty,11 May 2019,https://link.springer.com/article/10.1007/s11166-019-09301-9,Utility functions for mild and severe health risks,June 2019,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
58.0,2.0,Journal of Risk and Uncertainty,15 June 2019,https://link.springer.com/article/10.1007/s11166-019-09305-5,Valuing mortality risk in China: Comparing stated-preference estimates from 2005 and 2016,June 2019,James K. Hammitt,Fangli Geng,Chris P. Nielsen,Male,Unknown,,Mix,,
58.0,2.0,Journal of Risk and Uncertainty,15 June 2019,https://link.springer.com/article/10.1007/s11166-019-09303-7,Birds of a feather: Estimating the value of statistical life from dual-earner families,June 2019,Joseph E. Aldy,,,Male,Unknown,Unknown,Male,"Individuals reveal their preferences over fatality risk and income in an array of market contexts. Economists have long employed empirical tools to estimate the compensating differential for bearing risk in these markets (Viscusi 2018). Specifically, economists investigate the trade-offs between wages and occupational fatality risks in labor markets to estimate the willingness to pay for marginal changes in mortality risk. The value of statistical life (VSL) derived from these estimates informs evaluations of public policies and proposed regulations intended to reduce the mortality risk associated with air pollution, transportation safety, consumer product safety, food safety, and other risks (Viscusi and Aldy 2003; Robinson 2007). For example, the Environmental Protection Agency uses a VSL of $8.8 million and the Department of Transportation uses a VSL of $9.6 million in their regulatory impact analyses (EPA. 2018; DOT. 2016).Footnote 1 This paper presents a novel empirical strategy for estimating the value of statistical life from labor market data. In particular, I exploit the assortative matching on risk attitudes, experience with risky behaviors, and physical characteristics within married couples to derive a modified hedonic wage estimator. Focusing on dual-earner households in which head of household and spouse each work full time, I take the within-household difference in wages, occupational fatality risk, and all other independent variables common in hedonic wage analysis. I estimate how the differenced wage varies with differenced fatality risk and other differenced controls using the Current Population Survey Merged Outgoing Rotation Group (CPS MORG) data. If a married couple has common unobserved attitudes about and skills in mitigating exposure to occupational fatality risks, then this differencing can remove this unobservable element that could be correlated with the observed occupational fatality risk (and hence bias the coefficient estimate for this variable). This is analogous to panel methods that use individual fixed effects to control for the unobservable, time-invariant characteristics of the individual worker (Kniesner et al. 2012). It is also similar to the approach taken in “twins” analyses that estimate within-twins differences in returns to schooling in order to remove the impacts of genetic endowments (Behrman et al. 1994). I estimate occupational fatality risk using the Census of Fatal Occupational Injuries (CFOI) and construct a variety of industry, industry-by-age, and industry-by-occupation measures averaged over the preceding three years to address concerns about measurement error. I find that the value of statistical life varies from $9 to $13 million. These estimated VSLs are similar in magnitude from what I would estimate from conventional hedonic wage models (using a log-wage specification with the same sample of heads of households as in the spousal-differences model), but they are much more precise than the log-wage models, most of which produced statistically insignificant coefficient estimates for my suite of risk measures. I examine the stability of the VSL estimates to variation in the specification of controls in the regression model. I find that the VSL estimates in the within-couple differenced models are robust across the full range of models—from inclusion of a full set of socio-demographics, state and year fixed effects, and industry and occupation fixed effects to the most parsimonious model that simply estimates the differenced wage as a function of the differenced occupational fatality risk and a constant. This contrasts with the conventional hedonic wage models that differ in statistical significance and sign—with several models yielding statistically significant negative VSLs. These comparisons suggest that this differenced estimator purges the model of many of the correlated unobservables that may bias the estimated compensating differential in the conventional empirical framework. I also find that the value of statistical life appears to take an inverted-U shape with respect to age. In specifications that allow the compensating differential for occupational fatality risk to vary by age group, I find that the VSL increases up to the 45-54 year old age group and then declines. This is generally consistent with many theoretical, revealed preference, and stated preference studies (Aldy and Viscusi 2008; 2007; Krupnick 2007; Shepard and Zeckhauser 1984; Aldy and Smyth 2014). The next section reviews the literature on assortative matching that serves as the motivation for the empirical strategy. Section three presents the empirical strategy and data. The fourth section presents the results from estimating the modified hedonic wage models. And the fifth section concludes with research and policy implications.",10
58.0,2.0,Journal of Risk and Uncertainty,27 May 2019,https://link.springer.com/article/10.1007/s11166-019-09302-8,Behavioral economics and the value of a statistical life,June 2019,Thomas J. Kniesner,,,Male,Unknown,Unknown,Male,"I have a long-standing personal interest in the value of a statistical life (VSL), not only as a researcher but also as a son and a grandson. In addition to being hooked by the early work of Schelling (1968), Thaler and Rosen (1976) and Viscusi (1979) there is a family history to share. Since about 1940 many of my closest family members have worked in a chemical plant in Cleveland, Ohio. The list includes my maternal grandfather, my mother, my uncle (her brother), my father, and most recently me. While my father was away in the Army during World War II, my mother and my grandfather had the misfortune of working in the chemical plant when it was (unknown to them but known to the government) contaminated by radiation from producing part of the atomic bomb (Eisler 2000a, b). My grandfather ultimately died of colon cancer and my mother ultimately died of lung cancer, both of which were covered by a special federal government program that compensated their closest survivors, my uncle and me, because of our respective parents’ radiation exposure.Footnote 1 The amount was maybe 10–15% of the values of their statistical lives (VSL). Because VSL is a main component of the benefit side of a benefit-cost calculation of a program intended to save lives, my family’s situation furthered my interest in understanding, calculating, and using VSL in private and public decisions. Moreover, my family was exposed to carcinogens due to government subterfuge, misperceptions, inattention or possibly non-salience of their workplace hazards (myself included when I worked there). All are core issues in behavioral economics. Moreover, VSL got its first policy application with respect to OSHA hazard communication regulation, and without this fact workers would still not be getting the information that I and my family were deprived of (Viscusi 2018, Chapter 1). In what follows I focus on connecting the results of the emerging area of behavioral economics to the estimation and applications of VSL with a focus on identifying connections already made and ones that are the most promising areas of future research based on their respective relative importance to economists and policymakers. I begin by noting in Section 2 the connection between VSL and the development of the field of behavioral economics. I emphasize how early work on VSL suggesting a possible difference in willingness to pay for slightly more safety and willingness to accept slightly less safety is supposedly what got Richard Thaler to focus more of his attention on behavioral economic issues.Footnote 2 In Section 3 I flesh out the reasons why we care about how behavioral economics may be important to VSL research and its uses. I note the large amount of VSL research done by the conference honoree, Kip Viscusi, that can be considered behavioral economics (Viscusi 2018). Section 4 develops a list of topics in behavioral economics that might be the most fruitful connections between it and VSL research. Section 4 then argues that the core behavioral economic issues of risk salience, ambiguity aversion, present bias, reference group effects, reference point effects and experienced versus decision utility are where future VSL research may benefit most, including possible past and future nudge policies that connect to estimating or using VSL in government decisions. Section 5 considers briefly that there are two first cousins and one second cousin of behavioral economic research that are also of interest. They include research involving experiments, neuroeconomics, or beauty/personal attractiveness. Section 6 concludes by summarizing the effects of the behavioral economics movement on VSL as of now, via reference effects, and possible high value future impacts of behavioral economic ideas such as the distinction between decision and experienced utilities, for how economists estimate and use VSL.",10
58.0,2.0,Journal of Risk and Uncertainty,13 June 2019,https://link.springer.com/article/10.1007/s11166-019-09304-6,Empirical evidence of risk penalties for NTI Drugs,June 2019,Elissa Philip Gentry,,,Female,Unknown,Unknown,Female,"In order to provide faster access to necessary drugs, the FDA allows certain drugs to go through abbreviated approval processes. The abbreviated nature of these approval processes, while encouraging broader drug access to consumers, may result in riskier drugs being available on the market. In such situations, physicians’ ability to assess—and the market’s ability to price—such risk is gravely important. Viscusi (2018) summarizes evidence on compensating differentials from markets for risky jobs and products. While there is considerable evidence of compensating differentials from markets for products like cars or houses, this analysis becomes more complicated for pharmaceutical products, a market in which third party payments play a dominant role. This article examines such tradeoffs for a subset of drugs associated with risky switching and finds evidence of the expected price-risk tradeoffs as in other product markets. Moreover, this article finds evidence of the same kind of learning and switching behavior documented in the job safety context with respect to drug usage decisions over time. In order to incentivize generic manufacturers to apply for approval, the FDA allows follow-on products to go through an abbreviated new drug approval (ANDA).Footnote 1 Unlike original approvals, which require clinical evidence of safety and effectiveness, generic products are merely required to be bioequivalent to an already-approved reference drug. Bioequivalence does not require exact replication of the reference drug. Instead, two one-sided tests are conducted, measuring if the test drug is significantly less bioavailable than the reference drug, and vice versa. Bioequivalence is satisfied if there is no more than a “difference of 20% in the area under the curve (AUC) and peak concentration (Cmax) between the reference and test drugs” (Singh et al. 2014). While such generic approval processes are generally considered sufficient for most types of drugs, some are doubtful that they adequately controls risk for a group of drugs characterized by a narrow therapeutic index (NTI). The FDA’s proposed definition of NTI drugs are “those drugs where small differences in dose or blood concentration may lead to dose and blood concentration dependent, serious therapeutic failures or adverse drug reactions” (Yu 2011). Given the narrow range in which the drug provides therapeutic benefit and the allowed difference between products that are technically bioequivalent, changes from a brand-name to a generic or from a generic to another generic can result in adverse events. This perceived heterogeneity in risk was not previously regulated by the FDA, as the FDA treated all generic drugs as equivalent to brand-name drugs. However, the FDA has recently acknowledged the increased risk associated with NTI drugs. In 2010, the FDA Advisory Committee for Pharmaceutical Science proposed instituting stricter regulations for NTI bioequivalence (Yu 2011). More recently, scientists at the FDA have published new bioequivalence limits for NTI drugs (Yu et al. 2015). NTI drugs are just one example of areas in which the FDA has declined to regulate a perceived residual drug risk. Critics have suggested that traditional generic approval processes may be insufficient to evaluate generic versions of nonbiological complex drugs (NBCD), making the risks associated with them more uncertain (Gottlieb 2014).Footnote 2 This seems to be confirmed by FDA actions, as the FDA recently tried to approve generic versions of NBCDs but found it difficult to verify therapeutic equivalence. While the FDA approved a generic version of IV iron medication in 2011, it requested an independent study in 2013 to verify that the generic version was safe and effective (Gottlieb 2014). Seeing this as evidence that the bioequivalence process is insufficient to guarantee safety and effectiveness for NBCDs, some critics have called for the FDA to develop a better generic approval process for these types of drugs. Given that other FDA follow-on processes are perceived to create risk in generic products, this paper examines NTI drugs as an example of how the market deals with such unregulated risk. The structure of the market for pharmaceuticals attenuates the link between medical risks and willingness to pay. Physicians have the most information about medical risks but never personally experience the medical risk; more importantly, they rarely know the cost of a given drug. Third-party payers predominantly shoulder the costs but do not experience the medical risk. Consumers bear a portion of the costs and the entirety of the medical risks. This fracture in incentives to pay for less risky drugs might result in market insensitivity to NTI risk and provide a strong rationale for stronger ex-ante FDA regulation. If, however, there is evidence of market sensitivity to this risk despite this fracture, abbreviated approval processes might provide broader access to drugs without imposing uncompensated risks. The rest of the paper proceeds as follows: Section 2 explores the current literature on narrow therapeutic index substitution in relation to the broader issue of generic substitution. Section 3 outlines the conceptual model underlying consumer choice when switching is costly. Intuitively, given that NTI drugs are associated with both an initial risk of adverse events and with a future risk due to the lack of risk-free drug substitutes, the expected compensating differential consists of two parts. Not only would one expect a categorical penalty associated with NTI status, but the lack of risk-free substitutability would also produce a smaller gap in price between brand-name and generic versions. Section 4 describes the Medical Expenditure Panel Survey (MEPS) data used to test this model, and Section 5 discusses the results. Section 5.2 discusses the results of a hedonic price regression for NTI drugs, which both allows for a penalty associated with NTI status, as well as for differential effects on brand-name and generic versions. A benefit of the data is that these regressions can be applied to each portion of payments made by private insurance, Medicare, and the patient individually. The regression results provide evidence of a significantly negative price penalty for NTI drugs. Moreover, the results also indicate that the gap between brand-name and generic drugs is smaller for NTI drugs than for non-NTI drugs, consistent with consumer preferences when switching is costly. These results hold for total payments, third-party payments, and self-payments. Section 5.3 supplements these findings with evidence on differences in consumption bundles by NTI status. Given costly switching, theory suggests that consumers would find a drug version they can tolerate and subsequently remain loyal to that specific version. The results from the consumption bundle analysis confirm this pattern of learning and loyalty, concluding that consumers make purchasing decisions that are consistent with awareness of this costly switching risk. Section 6 concludes that the generic process for NTI drugs allows for broader access to drugs without imposing uncompensated risks on the public. These results have ramifications beyond the context of NTI drugs and might inform by analogy the appropriateness of requiring extra FDA regulation on generic versions of other product types.",1
58.0,2.0,Journal of Risk and Uncertainty,21 June 2019,https://link.springer.com/article/10.1007/s11166-019-09306-4,Can a ‘veil of ignorance’ reduce the impact of distortionary taxation on public good valuations?,June 2019,Morgan Beeson,Susan Chilton,Jytte Seested Nielsen,,Female,Female,Mix,,
59.0,1.0,Journal of Risk and Uncertainty,16 August 2019,https://link.springer.com/article/10.1007/s11166-019-09308-2,Endogenous attention to costs,August 2019,Linda Thunström,Chian Jones Ritten,,Female,Unknown,Unknown,Female,"A puzzling empirical finding is that people often do not pay attention to prices.Footnote 1 Allen et al. (1976) and Conover (1986) find about half of shoppers do not know the price of recently purchased products. To examine if the lack of price knowledge is due to the short time lag between the purchase and the inquiry, Dickson and Sawyer (1990) asked supermarket customers about the price of products just placed into their shopping cart. They find more than half of shoppers could not identify the item price. The authors suggest several potential explanations for this price unawareness, including consumer time constraints, habitual purchases, expectations that the price is ‘good enough,’ or high price search costs. Clerides and Courty (2017) find a majority of consumers do not choose the cheaper option amongst identical products, which the authors ascribe to price inattention. The authors show price search costs may cause such inattention, suggesting the observed price inattention is ‘rational.’ Others argue price inattention occurs because some parts of price information are hard to process, such as taxes (Chetty et al. 2009) and shipping costs (Hossain and Morgan 2006). Feldman and Ruffle (2015) find that the way in which sales taxes are presented to consumers impacts consumer price inattention. They find consumers downplay sales taxes added to the price, while fully accounting for sales tax rebates.Footnote 2 In this study, we propose a behavioral explanation to price inattention—we argue some of the observed price inattention may be willful. Previous studies show willful, or strategic, ignorance of risk information is prevalent for other types of behavior when there is a trade-off between what one thinks one should do and what one would like to do. People are found to use ignorance of the consequences of their choices on others as a way to allow themselves to be more selfish (Broberg et al. 2007; Dana et al. 2007; Larson and Capra 2009; Lazear et al. 2012; Grossman 2014; Onwezen and van der Weele 2016; Conrads and Irlenbusch 2013; Grossman and van der Weele 2017). People may also ignore information likely to entail bad news on asset performance, even if such ignorance may reduce decision utility (Karlsson et al. 2009), and ignore health outcomes to reduce health-related anxiety (Kőszegi 2003; Barigozzi and Levaggi 2010; Oster et al. 2013). Further, people choose ignorance of consequences of current choices to allow over-engagement in enjoyable behavior that causes future risks to one’s self (Ganguly and Tasoff 2016; Thunström et al. 2016). Specifically, in a closely related study, Thunström et al. (2016) find that people may be strategically “self-ignorant”—people may willfully choose ignorance of calorie information, and use ignorance as an excuse to follow their short-term preference to over-consume calories. They find willful ignorance of calories is particularly prevalent in subjects with low self-control, a group for which short- and long-term preferences differ. Similar to the trade-off between current enjoyable consumption and future health consequences from over-consuming calories, spending money poses a trade-off between enjoyable consumption today and the size of future budgets, i.e., the opportunity cost of today’s consumption. Previous studies show pain of paying is particularly important for responsible spending decisions, since it acts as a proxy for the opportunity cost of current consumption (Prelec and Loewenstein 1998; Loewenstein and O’Donoghue 2006; Rick 2013). Rick et al. (2008) develop a scale that classifies consumers based on their level of pain of paying. They define people who spend too much for their own liking as “spendthrifts,” and those who spend too little for their own liking as “tightwads.” In between are “unconflicted” people, who are satisfied with their own spending behavior. Suboptimal spending by tightwads and spendthrifts arises due to their suboptimal pain of paying. Dittmar and Drury (2000) provide empirical support of spendthrifts feeling conflicted about their immediate urge to (over)spend today and their long-term goal to ensure sustainable personal finances. Further, studies find that spendthrifts experience regret when deviating from their long-term goal (O’Guinn and Faber 1989; Horváth et al. 2015).Footnote 3 Both Festinger’s (1957) theory on cognitive dissonance and regret regulation theory suggest that people are motivated to reduce such regret (Pieters and Zeelenberg 2007; Zeelenberg and Pieters 2007). When provided full information of their risks to spending, anticipated regret may help spendthrifts regulate overspending. We develop a model based on the above findings. In our model, the pain of paying for unconflicted consumers accurately reflects their opportunity cost to current consumption, while the pain is too low for spendthrifts and too high for tightwads; causing over- and underspending, respectively. Spendthrifts experience regret from overspending. Our model shows that if anticipated regret from overspending is reduced under ignorance of risks to spending (i.e., costs), people particularly prone to overspending benefit from willfully ignoring cost information, even though ignorance leads to suboptimal consumption. Our model further shows that spendthrifts who only moderately overspend, unconflicted consumers, and tightwads do not benefit from ignoring price information. We design two complementary experiments to perform our empirical analysis. First, we conduct an economic field experiment to test for the correlation between spendthriftiness and price ignorance. Second, we design a hypothetical experiment that allows us to study whether spendthrifts willfully ignore prices. We find empirical support for our model—spendthrifts are more likely to willfully ignore price information, i.e., short-term gain from willful ignorance of prices. This may help explain previous findings of price inattention. Further, our results are in line with findings of willfull/strategic ignorance of consequences to self in the health domain (Ganguly and Tasoff 2016; Thunström et al. 2016). Indicative evidence from previous studies suggests that spendthriftiness correlates with price inattention in the way predicted by our model. Rick et al. (2008) find that spendthriftiness negatively correlates with the price consciousness scale developed in Lichtenstein et al. (1993). Berman et al. (2016) find that spendthrifts are more likely to underestimate future expenses, and Horváth et al. (2015) find that spendthrifts spend less time thinking through their purchases. The structure of the paper is as follows. Section 2 provides a theoretical framework, Section 3 describes our two datasets, Section 4 presents our empirical results and Section 5 concludes.",5
59.0,1.0,Journal of Risk and Uncertainty,04 September 2019,https://link.springer.com/article/10.1007/s11166-019-09309-1,Correlation neglect and case-based decisions,August 2019,Benjamin Radoc,Robert Sugden,Theodore L. Turocy,Male,Male,Male,Male,"Most theories of choice under uncertainty that have been proposed by economists or decision theorists are closely related to expected utility theory, and often are generalisations of that theory. (For one survey, see Machina and Viscusi 2014, chapters 12-14.) In these theories, uncertainty is represented by a set of states of the world, any one of which might obtain. Alternative acts available to an agent are represented as different assignments of consequences to states. Decision-making is conceptualised as a process of evaluating acts in terms of the subjective values that the agent attributes to their consequences and the probabilities or subjective weights that he or she assigns to the events in which those consequences occur. However, case-based decision theory (CBDT), proposed by Gilboa and Schmeidler (1995, 2001), is based on a fundamentally different representation of decision problems. In broad terms, CBDT is in the tradition of psychological theories of reinforcement learning (e.g. Bush and Mosteller 1953). In CBDT, there is no state space and no concept of probability. The agent is not assumed to know anything about the outside world except what he has actually experienced as the results of previous decision-making. The agent uses neither forward-looking hypothetical reasoning (“What will happen if I choose X?”) nor backward-looking counterfactual reasoning (“What would have happened if I had chosen X?”). He simply evaluates each currently available act in terms of the consequences he has in fact experienced as a result of choosing that act (or in some variants of the theory, choosing similar acts) in previous decision problems that he perceives to be similar to the problem at hand. Gilboa and Schmeidler (1995, pp. 606, 622) present CBDT and expected utility theory (EUT) as “complementary theories”. They argue that CBDT is normatively most defensible and descriptively most plausible when “states of the world are neither naturally given, nor can they be simply formulated”. Decision-making in such circumstances is decision under ignorance, as contrasted with decision under risk (i.e., with known probabilities) and decision under uncertainty (i.e., with known states of the world but unknown probabilities). For decision under ignorance, Gilboa and Schmeidler argue, “the very language of expected utility models is inappropriate”. If this complementarity claim were taken at face value, any idea of testing CBDT and EUT against one another would be out of place. Up to now, most experimental tests of CBDT have been designed on the premise that CBDT and EUT are complementary theories. For example, Ossadnik et al. (2013) set up an experimental environment of “structural ignorance” (p. 212), and compare the explanatory power of CBDT with that of three alternative criteria for decision under ignorance – maximin, maximax, and the pessimism-optimism criterion of Arrow and Hurwicz (1972). Similarly, Grosskopf et al. (2015) start from the explicit premise that CBDT “is not proposed as an alternative to or a generalization of [EUT]” (p. 640), and use an experimental environment in which “EUT is not a reasonable alternative decision-making procedure” (p. 652). They test CBDT against the null hypothesis of random choice and against a very simple heuristic, related to the “Take the Best” algorithm of Gigerenzer and Goldstein (1996). Unlike Ossadnik et al. and Grosskopf et al., who test specific parameterised forms of CBDT, Bleichrodt et al. (2017) report experimental tests of predictions derived from a very general, non-parameterised version of CBDT. But they too presuppose that CBDT is intended to be applied to situations in which states cannot be specified (p. 2), and design their experiment accordingly.Footnote 1 In contrast, the starting point for our experimental research was the conjecture that CBDT might have predictive power in situations in which states are well-defined and objective prior probabilities are known, but expected-utility decision-making requires the construction of posterior probabilities by Bayesian inference from observations of random processes. In terms of a distinction introduced by Hertwig et al. (2004), these are situations in which decisions are made from experience (i.e., the properties of alternative options must be inferred from previous experience), rather than from description (i.e., those properties are known a priori). In such situations, the “correctness” of Bayesian reasoning about probabilities is uncontroversial. Nevertheless, such reasoning can be cognitively demanding and its implications can be counter-intuitive. It is well-known that human judgements about probability often contravene Bayesian principles in predictable ways, for example because of the use of availability and representativeness heuristics (Tversky and Kahneman 1973; 1983). Charness and Levin (2005) report evidence of deviations from Bayesian reasoning that are consistent with one of the simplest reinforcement learning rules, the “win-stay-lose-shift” heuristic. Given that case-based reasoning requires much less cognitive sophistication and is well-adapted to naturally-occurring problems of decision under ignorance, the hypothesis that human beings are predisposed to use it is psychologically plausible and worthy of investigation. Since our conjecture has not been endorsed by the proposers of CBDT, we cannot structure our enquiry as a test of that theory. Our methodological strategy is to test predictions of EUT in situations in which there are intuitive reasons, derived from the underlying principles of CBDT, for expecting those predictions to fail in specific ways. This general strategy, used in combination with disparate intuitions, has led to many important developments in decision theory. The Allais paradox, common ratio effect, Ellsberg paradox, and preference reversal phenomenon were all first discovered by researchers who recognised potential limitations of EUT but who, at the time of discovery, were not in a position to propose a comprehensive alternative theory. These robust violations of EUT achieved the status of “exhibits” which informed the subsequent development of alternative decision theories.Footnote 2 These exhibits show non-random patterns in deviations of actual behaviour from the predictions of EUT. (For example, the Allais paradox involves comparisons between responses to two binary choice problems that EUT treats as equivalent. Individuals’ choices are systematically more risk-averse in one problem than in the other.) Such an exhibit provides evidence that some non-random mechanism, not encompassed by EUT, is at work, but is not to be interpreted as confirming any fully-specified alternative theory. Our research was designed to have the potential to create exhibits of this kind which might inform the development and application of CBDT. Our experiment tests two related intuitions about how case-based reasoning might lead to systematic deviations from the behaviour predicted by EUT. The first of these intuitions derives from a fundamental property of CBDT – act separability. In CBDT, experiences are encoded in memory as cases; each case consists of a problem, the act that was chosen in that problem, and the result of that choice (measured in utility units). Given a new problem, a decision-maker assesses each available act by recalling the previous cases in which that act was chosen, and weighting the result of each of those choices by a measure of the similarity between the problem in which that choice was made and the new problem. In this algorithm, when any given act is assessed, the only items of memory that are used are those that record results that have actually been experienced as a result of the choice of that act. If memory is used in this way, information that could show positive or negative correlation between the results of different acts is never retrieved. Thus, one might expect case-based reasoners to differ from Bayesian reasoners by neglecting information about correlation. The second intuition derives from the fact that probability judgements have no role in CBDT. Case-based reasoning does not lead to the formation of probability judgements that can be classified as “correct” or “incorrect” according to Bayesian principles. Instead, by moving directly from memory (encoded without reference to states or probabilities) to decisions, it circumvents the whole process of forming probability judgements. When an agent’s case-based reasoning leads to a violation of EUT, an outside observer may be able to conclude that the agent has behaved as if she were trying to maximise expected utility but had made erroneous probability judgements, but the agent herself may have no perception of making or endorsing the judgments that the observer attributes to her. This raises the possibility that agents might in fact endorse probability judgements that are systematically different from those that are revealed in their decisions when those decisions are analysed in the theoretical framework used by EUT. It is of course debatable whether such a difference can properly be called a violation of EUT. (Opinions differ about whether “probability” in EUT refers to an agent’s actual beliefs, or is merely part of a formal representation of her decision-making behaviour.) But a pattern of predictable differences between stated and revealed probabilities would be a surprising phenomenon calling for explanation. These two lines of investigation have the potential to be mutually corroborating. Suppose that, in some experiment, participants’ decisions are found to be insensitive to variations in relevant information about correlation. CBDT would offer a possible explanation for that observation. However, another possibility might be that the participants were Bayesian reasoners who had misunderstood the information given to them, perhaps because of weaknesses in the experimental design. But suppose it were also found that participants’ stated probabilities showed Bayesian sensitivity to information about correlation. That would suggest that participants understood that information, used it when forming probability judgements, but failed to use in when making decisions. That would give additional credence to CBDT’s explanation of correlation neglect in decisions. The remainder of the paper is organised as follows. Section 2 describes act separability under CBDT, and Section 3 discusses stated and revealed probabilities under EUT. Section 4 presents the experimental design; Section 5 discusses the hypotheses, and Section 6 presents the results. Section 7 concludes with a further discussion.",2
59.0,1.0,Journal of Risk and Uncertainty,04 October 2019,https://link.springer.com/article/10.1007/s11166-019-09311-7,An experimental test of the predictive power of dynamic ambiguity models,August 2019,Konstantinos Georgalos,,,Male,Unknown,Unknown,Male,"Many everyday economic life activities involve decision making under some form of uncertainty, either objective (risk) or subjective (ambiguity). Moreover, decisions usually are characterised by a dynamic (sequential) nature, in the sense that the decision maker needs to make a series of choices, while gradually she acquires additional, partial information about the state of nature, between choices, that allows her to update her prior beliefs. The standard theory of behaviour under uncertainty in a dynamic framework, Subjective Expected Utility (SEU), is based on three main assumptions regarding decision makers’ behaviour: (1) they form well-defined subjective beliefs to describe ambiguous situations; (2) they are ambiguity neutral, in the sense that ambiguity has no impact on their choices; and (3) they update their subjective beliefs applying Bayes’ rule. Nevertheless, the thought experiments proposed in Ellsberg (1961) challenged the two first assumptions. The direct consequence of this was a growing strand of literature to emerge, aiming to theoretically model non-Expected Utility (see Etner et al. 2012 and Machina and Siniscalchi 2014 for a review of the theoretical models) along with the generation of considerable experimental evidence confirming the existence of non-neutral ambiguity attitudes (see Hey 2014 and Trautmann and van de Kuilen 2015 for a review of the experimental literature). However, Gilboa and Schmeidler (1993) argue that in order for a model to be theoretically valid, it should be able to successfully cope with the dynamic aspect of the choices. Consequently, a complementary line of research appeared in the literature, trying to reconcile ambiguity non-neutral attitudes and conditional preferences. A common feature of all these non-Expected Utility models is that they relax in one way or another one of the principle axioms that Savage (1954) used in order to characterise preferences, the famous postulate P2, also known as the Sure-Thing Principle. When one considers decisions in a dynamic framework, the Sure-Thing Principle is equivalent to two rationality axioms, dynamic consistency (the ex-ante preferences coincide with the ex-post) and consequentialism (only residual uncertainty and available outcomes matter). Furthermore, Ghirardato (2002) provides a well-established result, that when both axioms are simultaneously satisfied, preferences are represented by the Subjective Expected Utility model, and conditional beliefs are formed according to Bayes’ rule. Consequently, violation of the Sure-Thing principle in a dynamic context implies violation of either the dynamic consistency or the consequentialism axiom, and thus deviations from Bayesian updating. As a result, and since there is still no consensus regarding which axiom should be adopted, two kinds of extensions of the static ambiguity models have been proposed in the literature to capture dynamic choice and conditional beliefs. Those that satisfy consequentialism (Gilboa and Schmeidler 1993; Pires 2002; Wang 2003; Eichberger et al. 2007, 2010 and Siniscalchi 2011among others) and those that satisfy dynamic consistency (Epstein and Schneider 2003; Klibanoff and Hanany 2007, 2009 and Klibanoff et al. 2009 among others).Footnote 1 It is however interesting, that while the experimental literature comparing the various theoretical models of choice under ambiguity in an atemporal framework is quite developed (see Section 2), there is a shortage of empirical evidence comparing dynamic ambiguity models. This is a gap that the present study aspires to bridge. To this end, we report the results from an economic experiment where using a sequential decision task, we compare dynamic ambiguity models. Representing ambiguity with the aid of a transparent and non-manipulable device, a Bingo Blower, and using two-stage allocation questions with an interim stage that allows for updating of beliefs, we are able to compare the predictive (out-of-sample fit) power of various dynamic ambiguity models. The predictive performance of the models is based on structural parameters which are estimated for each model and for each subject, using hierarchical Bayes estimation techniques on a subset of the data. The out-of-sample fit comparison tests the ability that each specification has to predict behaviour (allocations) in the subset of the data that has not been included during the estimation of the parameters. We can then rank models based on their relevant predictive power. We therefore compare the predictive power of the dynamic version of the most commonly used models, including the Savage (1954) Subjective Expected Utility (SEU), which we use as a benchmark model, the Gilboa and Schmeidler (1989) Maxmin Expected Utility (MEU), the Ghirardato et al. (2004) α-Maxmin Expected Utility (α-MEU), the Schmeidler (1989) Choquet Expected Utility (CEU), the Chateauneuf et al. (2007) CEU model with neo-additive capacities (CEUNEO) and the Tversky and Kahneman (1992) Cumulative Prospect Theory (PT). For all the aforementioned models (with the exception of PT), well-defined update rules have been proposed in the literature in order to define conditional preferences. Hence, our analysis allows us to conduct comparisons across different theoretical models,Footnote 2 as well as across different update rules within a particular theoretical model. Adopting the results from the recent experimental literature, in our analysis we do not consider models that preserve the assumption of dynamic consistency. Dominiak et al. (2012) reported extensive violations of the dynamic consistency axiom. Furthermore, our data show that for at least 70% of the observed choices, subjects behave in a dynamically inconsistent way and therefore such models would poorly describe or predict behaviour. To the best of our knowledge, this is the first study to experimentally test the predictive power of dynamic models under ambiguity. Our contribution can be summarised as follows: (1) we provide the first experimental test to compare the various ambiguity models in a dynamic framework; (2) we provide the first experimental comparison on the various rules that have been proposed for the updating of Choquet capacities as well as neo-additive Choquet capacities; (3) we perform the first empirical test of Prospect Theory in a dynamic framework under ambiguity; and (4) we use the estimates obtained by the fitted models in order to test the predictive power of the various theories. The following are our key results: (1) the rank dependent family of models (PT and CEU) performs much better compared to the multiple-prior models (MEU and α-MEU) in terms of their predictive capacity; (2) Prospect Theory wins all other models as far as out-of-sample fit is concerned; (3) within the Choquet Expected Utility model, the Generalised Bayesian updating rule best predicts behaviour while the Dempster-Shaffer rule performs better within the neo-additive Choquet capacities specification; and (4) within the Prospect Theory family, the specification assuming a Prelec (1998) 2-parameter weighting function provides the best predictions.",4
59.0,1.0,Journal of Risk and Uncertainty,02 October 2019,https://link.springer.com/article/10.1007/s11166-019-09310-8,Learning from extreme catastrophes,August 2019,Shinichi Kamiya,Noriyoshi Yanase,,Male,Male,Unknown,Male,"How do direct and indirect loss experiences due to an extreme catastrophe affect individuals’ expectation of the likelihood of future catastrophes, and how do these experiences affect the adjustment of their risk beliefs when they face future catastrophes? Such questions are not easy to answer due to the limited observations of extreme disasters but are important to understand because of an increasing trend in natural catastrophes and the widespread negative consequences of catastrophic events (Swiss Re 2016). Although major disasters tend to trigger public efforts to provide protection against catastrophes at an affordable rate,Footnote 1 homeowners have limited interest in mitigating disaster losses (Kunreuther and Pauly 2004). For instance, the take-up rate of the California Earthquake Insurance Policy is 13.3% in 2017,Footnote 2 and the 2016 take-up rate of the National Flood Insurance Program is roughly 50% even in coastal areas, but it is much lower inland.Footnote 3 Although significant demand surges are reported after loss experiences (e.g., Atreya et al. 2015), studies also show that a post-loss demand surge is only temporary (e.g., Gallagher 2014; Aseervatham et al. 2013; Atreya et al. 2015). Overall, recent empirical findings concerning catastrophe insurance suggest that loss experience has a limited effect on individuals’ risk beliefs about the likelihood of future catastrophes, which might occur because they forget the experience (Gallagher 2014). However, most disasters studied in the literature, such as floods causing state-of-emergency disaster declarations by the President of the United States (e.g., Gallagher 2014), are not extreme in terms of either the death toll or the amount of economic damage.Footnote 4 Therefore, whether and how extreme events affect risk beliefs differently remains an empirical question. This article considers household take-up of Japanese residential earthquake (EQ) insurance, one of the largest catastrophe insurance programs,Footnote 5 and focuses on the effects of the two costliest catastrophes in history, the 1995 Kobe EQ and the 2011 Tohoku EQ (see Fig. 1 for the epicenters of these earthquakes).Footnote 6 Studying Japanese EQ insurance has several advantages for investigating the revision of risk beliefs observed through changes in the purchase of insurance. First, for the two most extreme EQs, people did not self-select into vulnerable areas because the damaged regions have never been recognized as particularly risky relative to other regions. Thus, concerns about the causal relationship are reduced. Second, private insurers do not advertise EQ insurance because it is priced under a no-profit principle and is fully ceded under its reinsurance scheme, which mitigates a concern about the effect of insurers’ marketing efforts. Third, the coverage supply constraints discussed by Kunreuther (1996) have never been imposed. The premium rates, determined solely by the General Insurance Rating Organization of Japan (GIROJ), have remained stable due to its rating policy of focusing on equilibrium in the extraordinarily long term, a large government reinsurance-coverage program, and its geographically diversified portfolio. Fourth, households make informed decisions on purchasing EQ insurance coverage. Therefore, the supply-side effect on insurance purchases during a post-quake period is reduced. Details of the Japanese EQ insurance program are provided in Section 2.2. Epicenters of the 1995 Hanshin EQ and the 2011 Tohoku EQ Using the prefecture-level aggregate EQ insurance take-up rates as a proxy of the households’ expectation of the likelihood of future major earthquakes,Footnote 7 we empirically assess whether and how direct or indirect experience due to an extreme EQ affected households’ risk beliefs during the post-EQ period. The objective of assessing the effect of indirect loss follows Viscusi and Zeckhauser (2015) for environmental risk and Trumbo et al. (2011) for Hurricanes Katrina and Rita. The empirical approach of focusing on “distance” from a catastrophe is analogous to Fischhoff et al. (2003) for the September 11th attacks and Trumbo et al. (2011) for Hurricanes Katrina and Rita, although we examine the effect of indirect loss experience in several respects including physical distance from the epicenter, social distance measuring personalized loss information, and temporal distance. Our primary findings are summarized as follows. First, we confirm nationwide and persistent revisions in risk beliefs after extreme EQs. For instance, immediately after the 1995 Kobe EQ, on average, the EQ insurance take-up rate spiked up 76% (median 116%) in 47 prefectures.Footnote 8 Thus, the post-quake demand surge was not only limited to prefectures that suffered damage but was also observed in those that did not. To illustrate the spillover of demand surge, households living in the Hyogo Prefecture, which was the most severely damaged prefecture (the most severely damaged area was the capital city of Kobe), increased insurance take-up by 185% after the EQ. The insurance take-up in Hokkaido, the capital of which is located 1087 km away from the epicenter and was not affected by the EQ, also increased by 120% immediately after the EQ. The nationwide post-quake surges of take-up continued to increase for at least 4 years after the EQ, even after controlling for economic and demographic factors. Focusing on the effect of an indirect loss, we consider several potential channels. First, we examine the effect of ground movements by hypothesizing that intense ground movement makes it easier for people to consider the deadly consequences even without their own loss experiences. In contrast to our hypothesis, we find that seismic intensity was not a strong determinant of the indirect effect. Similarly, physical distance from the epicenter does not sufficiently capture the nationwide increase of take-up. Second, we examine the effect of information sharing; in particular, we examine whether the loss experience of a relative or friend would facilitate information sharing and affect adjustments of risk beliefs. Our tests measure the migration to/from severely damaged regions and support this hypothesis, implying that a source of information affects heuristics. Overall, a large portion of post-quake demand surges cannot be explained by heterogeneity among prefectures, suggesting that it is a consequence of countrywide adjustments of risk beliefs. Furthermore, we find that the severity of experiences with the Kobe EQ differentiates risk belief adjustments when an individual is faced with new indirect experiences. Specifically, our evidence suggests that households living in Hyogo reacted positively following major EQs before the 2011 Tohoku EQ more than the average household in the country did but less than did those households in neighboring prefectures. Thus, the severe direct loss experience of the 1995 Kobe EQ helped households in Hyogo insulate their risk beliefs from the shock of subsequent major EQs (i.e., weaker representativeness), although they also reacted positively after subsequent EQs. However, after the 2011 Tohoku EQ, often referred to as a 1000-year EQ, households in Hyogo and its neighboring regions reacted quite differently. Households in the neighboring prefectures marginally reduced their take-up of insurance by 4–6% during the post-Tohoku period, but there is no difference between households in Hyogo and the country average. The marginal negative adjustment in the neighboring prefectures is considered a dominating effect of the gambler’s fallacy, as proposed by Tversky and Kahneman (1971). Households that experienced the 1000-year EQ believed that the next extreme EQ would not occur in the near future and reduced their protection against future EQs. In contrast, no difference of households in Hyogo from the country average may be explained by offsetting effects: a positive effect of availability bias and a negative effect of a 1000-year event. The stark contrast between neighboring prefectures suggests that the severity of past experiences affects the updating of beliefs. Overall, this study contributes to the literature by adding new findings on the effect of direct and indirect catastrophe experience on insurance purchases. First, both direct and indirect experience of an extreme EQ could substantially increase insurance purchases. The spatially and temporally persistent surge of the take-up contrasts with most prior findings of temporary effects on insurance purchasing (e.g., Trumbo et al. 2011; Aseervatham et al. 2013; Gallagher 2014; Atreya et al. 2015) and on risk perception (e.g., Fischhoff et al. 2003; Trumbo et al. 2011) in the aftermath of a disaster. Second, our results add a new finding of a strong peer effect through personalized information, which sharply contrasts with the insignificance of peers’ experiences (e.g., Viscusi and Zeckhauser 2015; Krawczyk et al. 2017). Additionally, the nationwide reaction of the take-up rate does not fit the social comparison effect that inequality aversion makes insurance less attractive for correlated risks (e.g., Friedl et al. 2014). Finally, to our knowledge, this study is the first to address how a repeat extreme catastrophe experience (direct or indirect) affects demand for insurance. Specifically, we compare reactions to a new extreme EQ of those who had past direct and indirect EQ experience and show that some groups of households significantly reduced their insurance purchasing after observing the new EQ. That would suggest an interaction between availability heuristics and the gambler’s fallacy as a potential explanation for belief updating after a low probability event. The remainder of this paper is organized as follows. A brief description of extreme earthquakes in Japan and the Japanese residential EQ insurance program is provided in Section 2. The related literature is reviewed and hypotheses are discussed in Section 3. Section 4 describes our methodology and data. In Section 5, we discuss the estimation results. Section 6 concludes.",16
59.0,2.0,Journal of Risk and Uncertainty,03 December 2019,https://link.springer.com/article/10.1007/s11166-019-09313-5,The framing of nothing and the psychology of choice,October 2019,Marc Scholten,Daniel Read,Neil Stewart,Male,Male,Male,Male,"Mutable-zero effects in choice constitute violations of descriptive invariance (Tversky and Kahneman 1986), because preference between objectively the same options depends on how the options are described. The goal of Experiment 1 is to establish the mutable-zero effect as a violation of descriptive invariance in risky choice. It is not possible to examine the mutable-zero effect in direct choice between a gamble affording a possibility of “losing $0” and an equivalent gamble entailing a possibility of “winning $0.” So the method is to have each zero-outcome framing compared with a common standard. We use a sure thing as that common standard. Illustrating this with the gambles used by Bateman et al. (2007), the options could be as follows: LP A 7 in 36 chance of winning $9, and a 29 in 36 chance of losing $0. WP A 7 in 36 chance of winning $9, and a 29 in 36 chance of winning $0. SP Receiving $2 for sure. One group chooses between LP and SP, another group chooses between WP and SP. The mutable-zero effect occurs when LP is chosen more frequently than WP. We will examine this in unincentivized risky choice (Experiment 1a) and in incentivized risky choice (Experiment 1b). Upon reversing the sign of the nonzero outcomes, the options would be as follows: LN A 7 in 36 chance of losing $9, and a 29 in 36 chance of losing $0. WN A 7 in 36 chance of losing $9, and a 29 in 36 chance of winning $0. SN Paying $2 for sure. The mutable-zero effect occurs when LN is chosen more frequently than WN. We will examine this only in unincentivized risky choice (Experiment 1a). The stimuli are shown in Table 1. Each choice is between a Gamble (G) and a Sure thing (S). The zero outcome of G is described either as “winning £0” or “losing £0.” The stimuli were adapted from Bateman et al. (2007), changing the currency from dollars to pounds. With an eye on the incentivized experiment, we magnified both nonzero outcomes by a factor of 10, in order to make it more interesting to receive either amount if eligible, and we rounded the probabilities to 1/5 and 4/5, in order to make the risk more transparent. Participants were 602 British residents, recruited through Maximiles, an Internet service that awards its members points for completing online surveys. The sample was 34% male, with an average age of 38. Most were employed (66%, among whom 45% worked full time), and some had an academic degree (38%, among whom 26% had a bachelors, 10% masters, and 2% PhD) or were still studying (3%). The zero outcome was described as “losing £0” for half the sample (n = 297), and as “winning £0” for the other half (n = 305). Within each subsample, the order of the choices, and the order of the options within each choice, was randomized across participants. We obtain a choice probability, denoted p1, when a zero event is described as “winning” or “receiving” £0, and, from an independent sample, a choice probability, denoted p2, when the zero event is described as “losing” or “paying” £0. We report the 95% confidence interval for the difference between the two proportions as described by Agresti and Min (2005), i.e., d = p2 – p1, and CId = \( \left({p}_2-{p}_1\right)\pm {Z}_{\alpha /2}\sqrt{p_1\left(1-{p}_1\right)/{n}_1+{p}_2\left(1-{p}_2\right)/{n}_2} \), where α = .05, and the corresponding effect size as described by Kline (2004), i.e., ESd’ = logit(d´) / (\( \pi /\sqrt{3} \)), where logit(d´) = ln(p2 / (1 – p2)) – ln(p1 / (1 – p1)). For instance, the difference between proportions .9–.6 = .3 would emerge as the same effect size as the difference between proportions .6–.2 = .4, because the former is closer to the upper bound of 1 than the latter is to the lower bound of 0. The effects that we obtain are generally smaller than 0.20, which, by informal standards (Cohen 1988), are small effects. Table 1 shows the results. Small minorities choose the gamble, but, consistent with prospect theory’s reflection effect (Kahneman and Tversky 1979), choice of the gamble is more likely for losses than for gains. Moreover, there is a reliable mutable-zero effect for both gains and losses: In both outcome domains (gains and losses), choice of the gamble is more likely when the zero outcome is described as “lose £0” rather than “win £0,” although the effect is smaller for losses than for gains. To submit the main effect and the moderating effect of outcome domain to an inferential test, we ran a multi-level logistic regression analysis, using choice of the gamble (yes-no) as the dependent variable, and using as independent variables: Outcome domain (losses [1] versus gains [−1]), zero-outcome framing (“lose £0” [1] versus “win £0” [−1]), and their interaction (the moderating effect). Choice of the gamble was more likely for losses than for gains, p < .0001, 95% CIb = [0.57; 0.86; 1.15], and more likely when the zero outcome was described as “lose £0” rather than “win £0,” p < .0001, 95% CIb = [0.31; 0.66; 1.01]. The effect of zero-outcome framing (which is the mutable-zero effect) was smaller for losses than for gains, but the moderating effect of outcome domain was not reliable, p > .20, 95% CIb = [−0.45; −0.16; 0.14]. Altogether, the mutable-zero effect is reliable for both gains and losses, and although the effect is smaller for losses than for gains, the moderating effect of outcome domain is not reliable. In Experiment 2, we will find that the mutable-zero effect is reliable for gains, but unreliable for losses, and that this difference between outcome domains (the moderating effect of outcome domain) is reliable. Furthermore, we will derive from prospect theory that both realities are perfectly well possible. The mutable-zero effect proved subtle in Experiment 1a, and one may wonder whether it can withstand incentivization. For economists, the rationale of incentives is that judgment and choice require cognitive effort, a scarce resource that people allocate strategically. If participants in an experiment are not paid contingent on their performance, they may not invest the necessary effort to avoid making judgment or decision errors, whereas contingent payoffs would move judgment and choice closer to the economic model, and help to remove errors (e.g., see Camerer and Hogarth 1999; Hertwig and Ortmann 2001; Read 2005). Therefore, as a robustness check, we conducted an incentivized experiment on the mutable-zero effect in risky choice. It is a single-shot choice experiment, with the zero outcome described as “win £0” for some, and as “lose £0” for others, and the single choice is incentivized by randomly selecting one participant to receive the sure amount (£10 for sure) or to play out the gamble (a 20% chance of £90, and £0 otherwise) for real, depending on the choice made. The chances of being the winner were very low, but the effort required by the experiment was as well. As a matter of fact, the Expected Value of participating in the experiment varies between EVS = (1 / N) × £10, if all participants were to choose the sure amount, and EVR = (1 / N) × 0.20 × £90, if all participants were to choose the gamble. Thus, with the sample size being N = 761, the Expected Value varies between £0.0131406 and £0.023653, which, if reading the instructions and making the single choice takes 15 seconds, amounts to a net hourly wage between £3.15 and £5.68, when the minimum wage in the United Kingdom is, at the moment of writing, £3.70. Table 1 shows the stimuli. Relative to Experiment 1a, the magnitude of the sure outcome was decreased, from £20 to £10, to increase the likelihood of choosing the gamble, which was very low in Experiment 1a. Participants, 761 in total, were recruited through Prolific Academic. Each participant made a single choice. Before doing so, they read the following: “When the whole experiment is finished, we will be selecting one person to play for real and, if that is you, you could win a lot of money.” They were then reminded that, at the end of the experiment, one participant would be randomly chosen to play his or her choice for real. This was concretized with the options on offer: “If you are selected and you have chosen Option A, you have a 1 in 5 chance to win £90 and a 4 in 5 chance to [win / lose] £0. If you are selected and you have chosen Option B, you will receive £10.” The participants then made their choice, and left the survey. No demographic information was collected. Table 1 shows the results. As intended, the gamble is more likely to be chosen than in Experiment 1a, but still only small minorities chose it. We do, however, obtain a reliable mutable-zero effect, in that the gamble is more likely to be chosen when the zero outcome is described as “lose £0” rather than “win £0.” The mutable-zero effect therefore extends from unincentivized to incentivized risky choice.",3
59.0,2.0,Journal of Risk and Uncertainty,12 November 2019,https://link.springer.com/article/10.1007/s11166-019-09312-6,Protecting against disaster risks: Why insurance and prevention may be complements,October 2019,W. J. Wouter Botzen,Howard Kunreuther,Erwann Michel-Kerjan,Unknown,Male,Male,Male,"Seminal theoretical papers highlight that insurance and risk reducing protective measures are substitutes (Ehrlich and Becker 1972; Arnott and Stiglitz 1988). According to the theory, insurance would discourage individuals from investing in loss reduction measures unless they are rewarded with a reduction in their premiums. This behavior may lead to moral hazard when individuals take fewer risk-reducing measures after purchasing insurance, and to adverse selection when it is mainly individuals with a high risk who demand insurance but the insurer cannot distinguish between high and low risk individuals. These two problems arise from information asymmetries in the sense that the higher risk-taking by the insured is not observed by the insurer and, therefore, not reflected in a higher risk based premium (Akerlof 1970; Rothschild and Stiglitz 1976). Recent work over the past decade reveals that some insured individuals may view additional measures to limit risk ex ante as complements to insurance and thus implement more of these measures than the uninsured do, for example, because they are (highly) risk averse. This behavior can lead to both insurance purchases and investments in risk reduction; this has been termed advantageous selection (de Meza and Webb 2001) or preferred risk selection (Finkelstein and McGarry 2006). Finkelstein and McGarry (2006) find that individuals with private long term care insurance in the United States are more likely to engage in activities that reduce health risks, which in turn makes it less likely that they will ever use long-term care. Cutler et al. (2008) also show that positive relationships exist between individuals in the United States purchasing term life, annuities, medigap and long term care insurance and adopting risk reduction activities. The datasets used in these studies do not allow for examining the exact behavioral mechanisms behind the observed preferred risk selection, though. Other empirical research has also shown that there can be significant heterogeneity in the relation between insurance coverage and risk reduction (Cohen and Siegelman 2010). Einav et al. (2013) found that some individuals who engage in moral hazard have a higher demand for health insurance coverage, except for highly risk averse individuals with a high perceived health risk who do not engage in moral hazard and have a high willingness-to-pay for insurance coverage. The lack of empirical evidence on adverse selection in some insurance markets may be consistent with the hypothesis that buyers are not maximizing their expected utility, but it is also consistent with the hypothesis that little information asymmetry exists. In this paper we examine the relationship between individual risk reduction activities and natural disaster insurance coverage as our field case by identifying behavioral mechanisms that may explain preferred risk selection. In particular we use the U.S. flood insurance market and the decisions by homeowners to reduce flood risk by investing in loss reduction measures as our context. Floods are the most costly source of natural disasters in the U.S.,Footnote 1 and there is an expectation that the frequency and severity of flooding will increase in the future as a result of climate change and the accompanying sea level rise (IPCC 2014). Flood insurance for residential properties is almost exclusively purchased through the federally-run National Flood Insurance Program (NFIP), which covers more than $1.2 trillion of assets. This makes the NFIP the largest flood insurance program worldwide. We make a temporal distinction between risk reduction activities that are normally adopted well before the risk (i.e. a flood disaster) materializes, such as dry proofing walls of a building to make them impermeable to water, and emergency preparedness measures that are undertaken during an imminent threat of a disaster, such as moving contents to higher floors to avoid them suffering flood damage. Loss reduction measures often have a high upfront cost with an uncertain benefit, while emergency preparedness measures are generally less costly and have more certain risk reduction benefits. Recent experiences with low-probability/high-impact events have given rise to an increasing interest in the economics literature in how people prepare for, and respond to, disasters (Barberis 2013). In particular, Hurricane Katrina in 2005 and Hurricane Sandy in 2012, which combined caused more than $150 billion in economic losses in the United States, showed the importance of undertaking protective measures to reduce future disaster damage (Munich Re 2015). Moreover, such disasters highlight the need to provide recovery funds through insurance should one suffer losses from a disaster and to find ways to improve disaster preparedness in the future. Only a handful of studies have examined the relationship between investment in risk reduction and insurance purchase decisions for natural disasters (for a review see Hudson et al. 2017). Carson et al. (2013) show that homeowners in Florida who have high deductibles on their windstorm insurance are also more likely to take windstorm risk reduction measures. This suggests that insurance coverage and wind risk reduction measures act as substitutes, at least in terms of the deductible amount. On the other hand, Petrolia et al. (2015) find a positive relation between the decision to purchase windstorm coverage and investment in measures that limit windstorm damage based on a sample of U.S. households along the Gulf coast. A follow-up study by Hudson et al. (2017) using a different U.S. sample from the mid-Atlantic and Northeastern U.S. revealed that households with homeowner’s or flood insurance that are threatened to be hit by a hurricane are also more likely to engage in activities that minimize windstorm risks. With respect to flood risk, Thieken et al. (2006) and Hudson et al. (2017) find that insured households in Germany take more flood risk mitigation measures than households without flood insurance. These studies, however, did not identify the behavioral mechanisms behind the relations between insurance and risk reduction activities, which we aim to do here. Our study uses data from a survey we conducted of more than 1000 homeowners who live in flood-prone areas in New York City (NYC). This dataset includes individual level information on implemented flood risk reduction measures and flood insurance purchases from the NFIP as well as a range of variables that influence these decisions, such as psychological characteristics, risk perceptions, experience of past flood damage and receipt of federal disaster assistance. Our individual level data is especially suitable for determining whether the relationship between insurance purchase and risk reduction activities by individuals are substitutes or complements, and for identifying the behavioral mechanisms behind these relationships. The NFIP regulates development in the 1 in 100 year flood zone via specific elevation requirements and by restricting new construction in floodways (Aerts and Botzen 2011; Dehring and Halek 2013). Here we focus on voluntary flood risk reduction measures that households can undertake to prevent flood water from entering a building or to minimize damage once water has entered the structure. These flood-proofing measures can be especially attractive for existing structures that are very expensive to elevate. Cost-benefit analyses have shown that elevation can be cost-effective for new structures, but not for existing buildings (Aerts et al. 2014). The paper is organized as follows. Section 2 presents the data and the empirical methods. Section 3 presents the results. We find that insurance and long-term risk reduction measures taken ex ante a flood threat are complements, which is opposite to a moral hazard effect. In contrast, we find that people with insurance coverage are less likely to take short-term emergency preparedness measures during a flood threat. An examination shows that individuals both insure and take risk reduction measures for financial reasons, like experiencing high flood damage in the past and not having received federal disaster assistance for damage. Interestingly, we find that behavioral motivations to reduce risk outside of the standard economic model also play a role. Section 4 concludes and provides policy recommendations. The databasesFootnote 2 of our survey consists of a random sample of homeowners in NYC that face flood risk who live in a house with a ground floor. This means that renters and those living in apartments above the ground floor level are not included in our sample. The survey was implemented over the phone by a professional survey company about six months after NYC was flooded by Superstorm Sandy in October 2012. 1035 respondents completed the survey (73% completion rate). See the Electronic Supplementary Material (ESM) for more details about the survey method and survey questions. We use probit models of (flood) insurance purchases and explanatory variables of flood risk reduction activities to examine the relationship between flood insurance purchases and flood risk reduction measures. Treating the purchase of insurance as a dependent variable is consistent with related research that examines the relationship between insurance coverage and risk reduction by policyholders in the context of health risks (e.g. Finkelstein and McGarry 2006; Cutler et al. 2008) and natural hazard risks (Hudson et al. 2017). Voluntarily purchasing flood insurance is a personal decision that is made annually and can be cancelled at any time, which also justifies having it as the dependent variable. Most flood risk mitigation measures are long-term adjustments to the structure of a home, with some being implemented by a previous owner of the house. On the other hand, some are a temporary response to a known threat, such as emergency preparedness measures. This is why we also estimate models with implemented flood risk mitigation measures as the dependent variable and insurance and other relevant factors as explanatory variables, to examine if the main results for our main hypotheses are robust to this alternative specification. Our basic probit models are: where Yi is a binary variable indicating whether respondent i has flood insurance (Yi = 1) or no flood insurance at all (Yi = 0),Footnote 3Mi are variables of implemented risk reduction measures. Two empirical models are estimated for the subgroups for which (Yi = 1) since some individuals are required to have flood coverage (1) if they have a federally insured mortgage and live in a designated high-risk flood zone (the 1 in 100 year flood zone defined by the Federal Emergency Management Agency (FEMA)), while others purchase it voluntarily (2). The relationship between insurance coverage and risk reduction may differ between people who bought flood insurance voluntarily or mandatorily, which is why we make this distinction. The mandatory insurance model also enables us to determine whether homeowners who are required to purchase flood insurance implement more or fewer risk reduction measures than those who are uninsured. Mi consists of two separate variables of the number of implemented ex ante risk reduction measures at the household level (i.e. structural measures implemented in the home to limit flood damage) and emergency preparedness measures (which also limit flood damage but require a behavioral response of the homeowner in the immediate time period before the flood occurs). We make a distinction between these two types of protective measures throughout our analysis because decision processes of taking these measures are likely to be different. Risk reduction measures taken ex ante a flood threat have relatively high upfront costs with uncertain risk reduction benefits that materialize if a flood occurs. Emergency preparedness measures taken during a flood threat, like moving contents to higher floors and installing flood shields, are often relatively less expensive, but require the household to take action in a situation of emergency when the likelihood of a flood occurring is now almost certain. We specify two hypotheses (H1 and H2) depending on whether coefficients α1 and α2 are negative, pointing toward households’ viewing insurance and risk reduction measures as substitutes (H1) or whether these coefficients are positive, revealing that these measures are viewed as complements (H2). Although our research design cannot directly prove causality between insurance and risk reduction measures, evidence supporting H1 is consistent with a moral hazard effect, while evidence supporting H2 is consistent with a preferred risk selection effect. Estimating Eqs. (1) and (2) provides insights into how the insurance purchase decision is related with investments in loss prevention. To examine how other variables influence the decision to buy insurance and potentially affect the relationship between Yi and Mi, two other models are estimated: a traditional economic model and a behavioral economic model, respectively. The traditional economic model is: The following new variables are introduced in Eqs. 3 and 4: Ri reflects either homeowners risk perceptions (model variants 3a and 4a) or experts estimates of the flood risk (model variants 3b and 4b), Fi characterizes federal disaster assistance received by homeowner i, and Xi are socio-demographic variables, including income which we capture by including dummy variables of four categories of total household income of which the coefficients show the effects on insurance purchases versus the excluded dummy variable of the very high income category (>$125,000). Those required to purchase flood insurance reside in areas designated by FEMA to have a higher flood risk than those having the option to buy coverage. A principal reason for distinguishing between these two groups is determining whether their relationship between having flood insurance coverage and investing in risk reduction measures (Mi) differ as well as whether their perceptions of the risk and those of the experts differ in their insurance purchase decision. Definitions and coding of all variables are provided in ESM Table 1. If a question was not answered by a respondent, then this resulted in missing observations for the variable that is based on this question. This implies that the number of observations in our statistical models varies dependent on the number of non-missing observations for the explanatory variables included in that model.Footnote 4 Ri are either variables of risk perception measured as perceived flood probability and consequences in line with subjective expected utility theory (Savage 1954) or expert estimates of these variables. These expert indicators of flood risk faced by the respondents have been derived from a probabilistic flood risk model developed for NYC. A detailed description of this model and all flood modelling results can be found in Aerts et al. (2014, including online material). Based on a large set of 549 simulated hurricanes from a coupled hurricane hydro-dynamic model (Lin et al. 2012) the probability that the property of each survey respondent will experience inundation from a flood has been derived (Aerts et al. 2014). An indicator of flood damage was calculated for each respondent based on the mean expected flood inundation level at the respondent’s location and the value of a respondent’s property that are input to a depth-damage function for each specific building category derived from the HAZUS-MH4 methodology (HAZUS stands for Hazards United States). Such depth-damage curves are commonly used in flood risk assessments, and represent the fraction of a building and its content value that is lost in a flood based on the flood water level present in the census block (Aerts et al. 2013, 2014). Fi is a dummy variable representing respondents who have received federal disaster assistance for flood damage in the past. They may expect the government to compensate them for damage suffered from a future flood, which can lower their demand for insurance and risk reduction measures. This crowding out effect has been called the Samaritan’s dilemma or charity hazard (Buchanan 1975; Browne and Hoyt 2000; Raschky et al. 2013). The traditional economic model is also estimated for mandatory flood insurance purchases, because budget constraints due to low income and the receipt of federal disaster assistance in the past may be reasons for not adhering to the mandatory purchase requirements of the NFIP. Next, a behavioral economic model is estimated to examine factors and motivations that are likely to lead to purchasing insurance voluntarily: Variables Mi, Fi and Xi are similar to those for Eq. 4. A difference in (5) is that Ri in (4) is now represented by Ti which is a variable indicating respondents who think that the flood probability is below their threshold level of concern. This variable is included since other studies have found that individuals may use a threshold model in assessing low-probability/high-impact risk (Slovic et al. 1977; McClelland et al. 1993; Kunreuther et al. 2001; Botzen et al. 2015). This model implies that many individuals choose not to insure because they ignore the flood risk. Bi consists of behavioral variables that examine whether individuals purchase insurance because it gives them peace of mind, and whether their decision to purchase coverage is affected by their locus of control, their own internal values or a social norm. The peace of mind variable is included because affect and emotion-related goals appear to have an important influence on decision making under risk (Loewenstein et al. 2001). Individuals may purchase insurance to reduce anxiety, and to avoid anticipated regret not to have bought it should a disaster happen and consolation (Krantz and Kunreuther 2007), which is captured by this variable. Locus of control is a personality trait which reflects a belief about the degree to which an individual exerts control over his or her own life, in contrast to external environmental factors, such as fate or luck (Rotter 1966). It has been shown that locus of control influences economic decision making in various domains such as earnings (Heineck and Anger 2010), entrepreneurship (Evans and Leighton 1989), investments in education (Coleman and Deleire 2003) and in health (Chiteji 2010). It can be expected that individuals with an external locus of controlFootnote 5 think they have little influence over outcomes in their life and are less likely to prepare for disasters and purchase flood insurance (Baumann and Sims 1978; Sattler et al. 2000). Moreover, norms may be a motivation for people to prepare for disasters, as has been shown for the influence of norms on other economic decisions, like consumption, work effort, and cooperation in public good provision, and perceived fairness of income distributions and uses of (public) money, as reviewed by Elster (1989). Being adequately prepared for a specific risky situation may be regarded as a social norm, so that households do not need to rely on others for assistance during and after a disaster. In another context, namely recycling decisions by individuals, Viscusi et al. (2011) and a more detailed follow up study by Huber et al. (2017) show that it is important to distinguish between a person’s behavior due to the actions of others (i.e. social norms) and private values. We realize that recycling decisions can follow a different behavioral process than flood preparedness decisions, but the relevance of distinguishing between different types of social norms has been found in a variety of contexts such as littering and energy savings (see the review in Huber et al. 2017). Whether private values are stronger predictors of behavior than social norms may depend on the type of decision. Since in principle, both social norms and private values may be positively related to taking flood risk reduction measures and purchasing flood insurance, we examine the influence of both variables.Footnote 6 In our study, a social norm refers to approval of others of being well prepared for flooding, while a private value refers to behavior that the respondent finds to be personally important.Footnote 7 Furthermore, we estimate two variants of (5) which include interaction terms with ex ante risk reduction measures in order to examine how behavioral and financial mechanisms relate to the adoption of both risk reduction measures and purchase of insurance. First, we examine how the behavioral characteristics Bi influence the decision to both purchase flood insurance and adopt risk reduction measures by creating interactions terms of these Bi variables with the ex ante risk reduction variable. In particular, the norm and locus of control variables can reflect internal preferences of the individual with regard to risk preparedness which may affect decisions to both insure and implement risk reduction measures. Second, a model is estimated to examine how the interaction between risk reduction and flood insurance purchases is related to financial incentives through previous flood damage and past federal disaster assistance. Experiencing severe flood damage may trigger the adoption of ex ante risk reduction measures and the purchase of insurance when individuals perceive that insurance coverage alone is insufficient for coping with future flood events. Individuals who have received federal disaster assistance may expect the federal government to cover their future losses. They therefore are less likely to invest in risk reduction measures and purchase insurance than if they believed they would be responsible for the costs of repairing their damage after a disaster.",25
59.0,2.0,Journal of Risk and Uncertainty,02 December 2019,https://link.springer.com/article/10.1007/s11166-019-09315-3,"Gender effects for loss aversion: Yes, no, maybe?",October 2019,Ranoua Bouchouicha,Lachlan Deer,Ferdinand M. Vieider,Unknown,Male,Male,Male,"Gender effects in risk taking behaviour are a much-debated topic. The interest in differential risk taking by the sexes can be ascribed to its role as a potential explanation for gender differences in investment behaviour (Sunden and Surette 1998; Dwyer et al. 2002), or for the differential willingness to compete by men and women (Niederle and Vesterlund 2005; Balafoutas and Sutter 2012). There is, however, little agreement on whether women truly have less appetite for risk than men. While some findings indeed suggested that this is the case (see Croson and Gneezy 2009, for a review), a recent meta-analysis has cast doubt on the universality of gender effects (Filippin and Crosetto 2016). Loss aversion—the stylized finding that a given loss provides more disutility than a monetarily equivalent gain provides utility (Markowitz 1952; Kahneman and Tversky 1979)—is a central component of risk aversion. Loss aversion has been used to explain a wide variety of empirical phenomena (Benartzi and Thaler 1995; Gneezy and Potters 1997; Schmidt 2003). It is thought to be the driving factor of any risk aversion in small-stake decisions (Rabin and Thaler 2001; Köbberling and Wakker 2005). It further remains debated in the literature to what extent a gender effect exists for loss aversion, with existing studies finding effects in different directions (Schmidt and Traub 2002; Brooks and Zank 2005; Abdellaoui et al. 2008; Harrison and Rutström 2009; Booij et al. 2010; Gächter et al. 2010; Holden 2014; Andersson et al. 2016a). There is currently no agreement on the correct definition of loss aversion (Schmidt and Zank 2005). We illustrate how even subtle differences in the definition of loss aversion may result in the estimation of contradictory gender effects. Using four definitions of loss aversion commonly used in the literature, we show that according to one definition women are more loss averse than men, whereas according to another there is no gender effect; using the remaining two, we find women to be less loss averse than men. These radically different conclusions are all the more remarkable since they are obtained i) based on the same data; ii) based on the same functional forms and econometric setup; and iii) using definitions that are commonly employed in the literature, and the subtleties of which can easily escape scrutiny when presented in isolation. It should thus be clear that the four definitions we present serve only illustrative purposes, and that additional variation could result from changes in some of the elements we hold constant across our definitions. We further show that the differences in the inferences we report originate from the derivation of loss aversion from the combination of parameters estimated over pure gains and over pure losses, and from the use we make of them to identify loss aversion from decisions in mixed gain-loss tasks. Since we observe gender differences in risk preferences over pure gains, different ways of capturing risk aversion over gains across the different models can result in radically different conclusions about loss aversion. Ultimately, one’s interpretation of gender effects will thus crucially depend on what one considers the ‘correct’ interpretation of loss aversion—an issue on which scholars disagree. The discordant findings can be conceptually organized by measurement error, which in this case may result from modelling assumptions.",21
59.0,2.0,Journal of Risk and Uncertainty,30 November 2019,https://link.springer.com/article/10.1007/s11166-019-09314-4,Some implications of common consequences in lotteries,October 2019,David Crainich,Louis Eeckhoudt,Mario Menegatti,Male,Male,Male,Male,"There are many instances in life where individuals or organizations undertake an action, which, in the case of success, will improve their status. Many examples can be provided. A student may wish to obtain a university degree or apply for a grant. A firm may invest in a new production line or apply for public funding. A researcher may start a new project with the aim of solving a problem or inventing a new product. An individual may take part in a sports competition or place a bet on it. In all these cases, agents face a “challenge”, where they can succeed and thus improve their status, or fail and leave their status unchanged. Such challenges have various facets. In particular, there may be more than one possibility, so that a relevant question may be which of the possible challenges the agent chooses to take up. In the case where the agent has to choose between different challenges a specific but very significant question can be asked: Does the agent prefer to face a challenge where the possible reward is high but probability of success is low (“hard challenge”) or a challenge where the reward is lower but the probability of success is higher (“soft challenge”)? Or, alternatively, what type of agent prefers the hard challenge and what type of agent prefers the soft challenge? In fact, when a firm compares different choices for new production lines or different kinds of public funding, some will have higher returns in the case of success, but others will have a higher chance of success. Comparing different universities, a student may choose a more selective institution where succeeding ensures a better status but probability of failure is higher, or a less selective institution where it is easier to succeed but the degree obtained is less prestigious. In a sports competition, an individual can take part in an international championship with high reward but strong competitors, or in a local tournament with lower reward but weaker competitors. And placing bets, people may choose to bet on a more probable event for a lower reward, or on a less probable event for a higher reward.Footnote 1 Considering a context completely different from that described above, there are many situations where an individual faces the risk of incurring a future financial loss. Many examples of this kind of situation can be provided. The owner of a property may face the risk of fire damage. A driver may face the risk of a car accident. The belongings of an agent may be subject to the risk of burglary. When facing these situations, an agent can act in different ways. In particular, ignoring the instrument of market insurance, the agent can implement many activities in order to deal with the risk of financial loss. One strategy may be to generate a reduction in the probability that the loss occurs. Acting in this direction, the owner of a property can improve the safety of the electrical system in order to reduce the probability of a fire accident. Similarly, a driver can make periodical maintenance of his car in order to reduce the probability of accidents. Lastly, the installation of an alarm may reduce the probability of burglary. The same risks, however, may be dealt with by means of the different strategy of reducing the size of the possible loss, leaving the probability that it occurs unchanged. Acting in this direction, the owner of a property can reduce the damages in case of a fire accident by installing a fire extinguishing system. A driver can reduce the negative effects of a car accident by fastening the safety belt. The loss in case of a burglary may be reduced if some valuables are put in a safe-deposit box. The two instruments used to deal with the risk of incurring a loss described above are typically called in risk theory “self-protection” and “self-insurance”.Footnote 2 These two instruments may be used together (as, for instance, in the case of both maintenance and safety belt in the example of car accident risk) but, in some cases, an agent has to choose between them because of budget constraints, time constraints or other kinds of constraints. For instance, because of budget constraints, the owner of a property may be forced to choose between installing a new electrical system and a fire extinguishing system or an agent may have to choose between installing an alarm and paying the rent of a safe-deposit box. The issue of the choice between risky challenges and the issue of the choice between self-protection and self-insurance are different problems, but they share an important element: both problems are described by lotteries exhibiting a kind of “common consequence” in the two choices. In particular, when an agent compares two different challenges, he compares two different binary lotteries, where the two possible events are “success” and “failure”. The probability of success is different in the two lotteries and, in the case of success, the agent gains a reward which is also different. On the other hand, in the case of failure, agent wealth is unchanged, and it is thus the same in the two lotteries. Similarly, when an agent compares self-protection and self-insurance, he compares two lotteries where the two possible events are “loss occurrence” and “no loss”. The probability of loss occurrence is different in the two lotteries and the size of the loss in case of occurrence is different too. On the other hand, if the loss does not occur, agent wealth is unchanged and it is thus the same in the two lotteries. The existence of a common consequence, which is present both in the choice between risky challenges (in the case of “failure”) and in the choice between self-protection and and self-insurance (in the case of “no loss”), generates a significant linkage between the two problems described above, which is the issue analyzed in this work. In particular, this paper aims at showing that the presence of a common consequence in the comparison between two lotteries generates clear specific results which relate this comparison to the degree of agent risk aversion. Moreover, the application of these results show their implications for the choice between risky challenges and for the choice between self-protection and self-insurance. The problems described above present situations where an agent either obtains a reward in the case of success in a challenge or suffers a loss in the case of a bad event, and both the reward and the loss are known with certainty. In many cases, however, successes generate rewards whose values are not exactly known, just as the consequences of losses may not be precisely known. In these cases, we have respectively random rewards in the choice between risky challenges and random losses in the choice between self-protection and self-insurance. Both these cases will be analyzed in the paper. Lastly, a different kind of uncertainty may also be considered. When choosing between two challenges or between self-protection and self-insurance, an agent may face uncertainty on his background conditions, a further kind of uncertainty, which is independent of both the risk associated with the challenges and the risk of incurring financial loss. This kind of uncertainty, related to the presence of what is usually called in risk theory a “background risk”, is also analyzed in the present work. The paper proceeds as follows. Section 2 examines the comparison between lotteries with a common consequence. Section 3 studies the application to the choice between risky challenges. Section 4 analyzes the application to the choice between self-protection and self-insurance. Section 5 concludes.",
59.0,3.0,Journal of Risk and Uncertainty,04 February 2020,https://link.springer.com/article/10.1007/s11166-019-09319-z,Risky health decisions under regulatory constraints: Abortion tourism in Switzerland,December 2019,Annette Hofmann,Julia K. Neumann,Peter Zweifel,Female,Female,Male,Mix,,
59.0,3.0,Journal of Risk and Uncertainty,08 February 2020,https://link.springer.com/article/10.1007/s11166-019-09318-0,Resolving Rabin’s paradox,December 2019,Han Bleichrodt,Jason N. Doctor,Peter P. Wakker,,Male,Male,Mix,,
59.0,3.0,Journal of Risk and Uncertainty,09 January 2020,https://link.springer.com/article/10.1007/s11166-019-09316-2,Common genetic effects on risk-taking preferences and choices,December 2019,Nicos Nicolaou,Scott Shane,,Unknown,Male,Unknown,Male,"The description of people as risk-taking or risk-avoiding types is not a rhetorical flourish. Some people regularly speed when driving, take up risky sports, choose financial investments that incur a high chance of loss, and select risky occupations, like starting their own businesses. Others always drive below the speed limit, pick tame leisure activities, pick investments that preserve their capital, and choose stable occupations. Prior research has shown that preferences for high risk activities tend to be correlated across multiple domains, from financial investments to decisions about health to job choice to leisure activities (Barsky et al. 1997; Beauchamp et al. 2017; Dohmen et al. 2011; Donkers et al. 2001; Ekelund et al. 2005; Halek and Eisenhauer 2001; Zhong et al. 2009a). In addition, prior research has shown that risk-taking preferences are correlated with risk taking choices, such as undertaking entrepreneurship and investing in the stock market (Dohmen et al. 2011). While academics have long known about these correlations, little research has explored their source. Some researchers have suggested that innate differences may account for at least some part of these correlations, with variation in genetic composition predisposing some people to have more general risk taking preferences, to be more prepared for risk taking in multiple domains and to select higher risk choices than other people (Beauchamp et al. 2017; Dreber et al. 2009). However, the degree to which the correlation in risk taking behavior across domains is accounted for by genetics has received very little attention because identification is not easy (Karlsson Linnèr et al. 2019). Any observed statistical association between general risk-taking preferences, domain-specific risk taking preferences, and risk taking choices may occur because of genetic factors, environmental factors, or both. This study uses the natural experiment of twins to identify the extent to which common genetic factors account for the association (i) between general risk-taking preferences and domain-specific risk taking preferences in five domains: car driving, financial matters, sports and leisure, career, and health, (ii) between general risk-taking preferences and approaches taken to financial investments, (iii) between general risk-taking and stock market participation, and (iv) between general risk-taking and the choice of entrepreneurship as an occupational choice. Specifically, we examine 1898 monozygotic (MZ) and 1344 same-sex dizygotic (DZ) twins from the United Kingdom, who were surveyed in 2011. By comparing the correlations between different measures for pairs of MZ and DZ twins, we can decompose phenotypic correlations into those correlations that occur for genetic and environmental reasons. We find that a substantive part of the variation in: general risk-taking preference; domain-specific risk taking preferences in the five domains of finance, health, career, driving, and sports and leisure; stock market participation; financial investment choices; and the decision to be self-employed are explained by genetic factors. Moreover, our study indicates that general risk-taking shares a common genetic component with risk taking in the five domains and the three risk-taking choices we examined. These effects persist after separating out the effects of the Big Five personality traits, sensation seeking, age, and gender. These findings are in line with the work of Beauchamp et al. (2017) who find sizable associations between risk attitudes and investment decisions, personality, smoking, drinking and cognitive ability. Beauchamp et al. (2017) advanced the seminal work of Cesarini et al. (2009) and Cesarini et al. (2010) who had found a genetic predisposition for risk taking and financial decision making. Our paper extends the study of Beauchamp et al. (2017) by investigating the drivers of these associations through examining the genetic covariations between general risk-taking preferences and financial investment choices, domain-specific risk preferences, stock market participation and business formation. Our paper also adds to the excellent recent work of Karlsson Linnèr et al. (2019) who conducted a genome-wide association study of over one million individuals and found that general risk tolerance is genetically correlated with risky behaviors including speeding, adventurousness and number of sexual partners. Our findings have several implications. First, our study contributes to a biosocial perspective on risk taking (Cesarini et al. 2009). Our findings that a genetic predisposition to risk taking preferences and choices exists in many different domains, from financial investment choices to taking career risks, offers a novel explanation for the similarity in risk-taking behavior between parents and children. Children may adopt similar approaches to driving, and choose similar sports, occupations or ways of investing because they share the same genetic composition as their parents, rather than because they learned these preferences and approaches from their parents or because they share a common culture.Footnote 1 Second, our results suggest that people with a greater preference for risk are more likely to undertake risky behaviors because of a common genetic aetiology between general risk-taking preferences and domain-specific risk preferences, between general risk-taking and financial investment choices, between general risk-taking and stock market participation, and between general risk-taking and the choice of entrepreneurship as a career. Different variants of genes predispose people to develop different risk preferences across a variety of settings. These genetically-influenced preferences lead the same people to be disproportionately likely to make risky financial investment choices, participate in the stock market and choose entrepreneurship as an occupation (and make other similar decisions we cannot observe), as they seek to fit their behavioral choices to their innate tendencies. Third, our study shows that the entire environmental influence in the risk taking behaviors we examined is accounted for by non-shared environmental factors. These results suggest that factors, such as the work environment and work colleagues, are more important for risk taking than shared environmental influences, such as parental upbringing and shared family experiences. Fourth, our results have important implications for the ability of managers and policy makers to influence human behavior. That ability depends on the levels of genetic and environmental correlations between variables. Our results suggest that researchers need to think more carefully about the ways in which interventions might be used to increase the level of risk taking behavior. Even if risk taking preferences, the decision to start a business and the choice to undertake certain financial investment strategies are (phenotypically) correlated, interventions to increase risk taking preferences may not be likely to increase several of these outcomes, unless the correlations are largely environmental. Because we find that the non-shared environmental portion of the correlation between risk preferences and risk taking choices varies across different risk taking choices, interventions designed to influence general risk preferences should not be expected to affect financial investment choices, stock market participation and occupation choice equally. The paper proceeds as follows. The next section discusses related research. The third describes our empirical research methodology. The fourth section presents our results. The final section discusses the results and provides our conclusions.",8
59.0,3.0,Journal of Risk and Uncertainty,05 February 2020,https://link.springer.com/article/10.1007/s11166-019-09317-1,Pricing risk in prostitution: Evidence from online sex ads,December 2019,Gregory DeAngelo,Jacob N. Shapiro,Gary Shiffman,Male,Male,Male,Male,"The movement of many human interactions to the internet has led to massive volumes of text that contain high-value information for social scientists. For example, online illicit sex markets have yielded tens of millions of sex provider advertisements and over one million customer reviews of those providers. These online texts describe prices, locations, personal characteristics, preferences about the commercial encounter, and other information that is useful for social science but otherwise difficult to obtain at a large scale.Footnote 1 Unfortunately, these texts are intended for individual human, not analytical, consumption: they are casually-written and contain nonstandard usage and slang. To date, preparing such data for statistical analysis has required substantial human annotation, with the concomitant expense and necessary reduction in data size. Recent advances in computer science enable macroscopic analysis of such data at finer resolution than previously possible by extracting high-quality structured analysis-ready information from text and images with minimal human annotation. In this paper we employ the DeepDive system to create a structured database of facts recovered from human-written source texts in the online illicit sex market. DeepDive uses large-scale probabilistic inference in a user-enabled feedback loop, thereby avoiding problems common to most standard annotation techniques, such as reliance on brittle rules or fixed grammars. In a number of applications, DeepDive obtains accuracy that is similar to that of a human annotator (Callaway 2015; Peters et al. 2014). We are thus able to obtain a very large and high-quality database about subtle concepts, derived from an extremely messy collection of documents (Appendix Table A1 provides precision/recall figures). Data on this market are relatively easy to access in small quantities because much of the activity happens in public web fora (e.g. www.backpage.com). The postings on these sites are text advertisements written in informal English, akin to classified ads, often accompanied by images. As with classified ads, there is informal broad agreement about the kind of information to provide (prices, locations, etc) but diversity in both mode of expression (slang, colloquial usage, nonstandard usage) and in exactly which data values are provided. Figure 1 displays a full example of an online advertisement as well as two examples of ad text to illustrate the linguistic challenges in this space. Our text collection contains information from almost 30 million text ads for sex services, scraped from 19 distinct websites between early-2011 and January 2016 by IST Research.
 Sample ad from backpage.com Little is known about the market for sex services, but there are many reasons to want to know more. The sex services market is a two-sided market where buyers aim to connect with providers of sex services and providers wish to offer their services. Given the illegal nature of the services, both the service provider and John are concerned with the possibility that they could be matching with a law enforcement agent. Additionally, service providers also face a risk of encountering a violent John.Footnote 2 In fact, small scale surveys have found that as many as two in three sex service providers have been assaulted by customers or pimps (Weitzer 1999). Sex services were traditionally solicited in outdoor spaces, which resulted in the creation of red-light districts (Hubbard and Sanders 2003). In the presence of mounting social pressure and threat of arrest, sex service workers were largely relegated to specific locations in urban areas where illegal activities were more tolerated. However, the introduction of the internet fundamentally changed the nature of the initial interaction between clients (the demand side of the market) and service providers (the supply side). Instead of face-to-face interactions, the initial interactions between potential clients and service providers began when a client responded to an advertisement, which offered sex services. The movement to arranging services online versus through face-to-face interactions is thought to result in more safety for service providers (Bass 2015), as service providers can screen potentially violent clients.Footnote 3 Additionally, the movement to online advertisements enabled service providers to coordinate appointments and have more control over the location where services are to be performed, rather than waiting at specific outdoor locations or propositioning potential clients in public locations (e.g. bars, casinos). So, service providers can now determine whether they are willing to travel to a potential client’s location (outcall), whether the client must come to the location of the service provider’s choosing (incall) or if the service provider can accommodate either situation. Despite these differences in the arrangement of services, payment for services has largely remained unchanged. Cash is still the currency of such transactions, which is most often paid upon completion of services. Violence against the service provider is thought to be most likely to occur at the point of payment, creating a market for additional security.Footnote 4 While product differentiation existed in this market prior to online platforms, it was more difficult for such differentiation to occur. Service providers could place advertisements in alternative weeklies (e.g. The Village Voice) or they could work with an escort agency, which would coordinate service providers and potential clients for a fee. The movement to online advertisements enabled sex service providers to become more entrepreneurial and independent, enabling them to keep a greater share of the proceeds from the services that they offered. The movement online also enabled further horizontal and vertical differentiation of services. Vertical product differentiation was enabled, as idiosyncratic preferences for services (e.g. massage, erotic massage, escort, BDSM, “girlfriend experience”, etc.) could be catered to and advertised across providers who are willing to perform such acts. Horizontal product differentiation was also further enabled, as the costs of advertising and searching for one’s ideal variety were both reduced through online platforms. Moreover, advertisements could link to review web sites that enabled a client to see testimonials of the service provider’s quality. Thus, a service provider could generate a reputation, which enabled the service provider to command higher rates for services performed (Cunningham and Kendall 2016). The movement of sex service advertisement online also resulted in significantly new knowledge about the market for sex services, as measuring the supply for these services was nearly impossible prior to its movement online. Although crude, we can now measure the number of sex service advertisements by city on a given day. Table 10.4 of Cunningham and Kendall (2011c), for example, reports the average number of sex services advertisements on one platform per MSA population across 31 of the largest municipalities in the US that are offered in a day range from 0.36 (Cleveland) to 18.34 (San Francisco). Unfortunately, such robust measures of the demand for sex services do not exist. In one of the only studies that attempts to estimate the demand for sex services, Roe-Sepowitz et al. (2016) estimate the demand for sex services in 15 large municipalities in the US. On average, the study finds that 1 out of every 20 males over the age of 18 years old in these jurisdictions was soliciting online sex ads. From an academic perspective, the online market for sex is representative of the broader class of markets in which regulation and contract enforcement are decentralized because the underlying activity is illegal.Footnote 5 From a policy perspective, an increasingly large share of commercial sex transactions are coordinated through online markets (Cunningham and Kendall 2011b). The emergence of robust online markets for sex have been associated with a range of social ills including child prostitution (Hughes 2002; Mitchell et al. 2010), human trafficking (Latonero et al. 2011), and a drop in the average age of prostitutes (Cunningham and Kendall 2011b). At the same time, these markets may reduce transaction costs in the market for sex and enable better use of reputational mechanisms, both of which can be welfare enhancing for buyers and sellers (Cunningham and Kendall 2011b). Cunningham et al. (2018) also notes that the introduction of online sex service clearinghouses (namely, the erotic services section of www.craigslist.com) significantly reduced female homicides. Using the data from these markets to better understand the commercial sex trade therefore has great potential. Our analysis makes a concrete methodological contribution as well. Because only some online fora are well-structured, and text ads have nonstandard content that is difficult for traditional natual language processing (NLP) methods, past sex market researchers have used relatively small amounts of data.Footnote 6 As a result of these relatively small data sizes, relevant statistics must be aggregated into coarse geographic or temporal regions in order to be statistically useful. For example, a traditional small nationwide sample might yield only a few advertisements from a given city, forcing the analyst to aggregate advertisements at a state level in order to retain a minimal number of counts in each aggregated group. In contrast, our extracted database is significantly larger than even the largest previous effort. We extract price/location tuples for 4.5M ads, of which 2.1M occurred in locations for which we have the full set of covariates.Footnote 7 Elements in this large and high-accuracy dataset do not have to be aggregated into very coarse groupings in order to retain statistical validity: the data is higher-resolution than past efforts. For example, there may no longer be any need to aggregate advertisements to the state level; many cities will retain sufficient counts. This high resolution data enables us to control for local-level variation in contextual factors (such as local wages or commute times) that would have been impossible with data aggregated at coarser levels. Using these unique data we find that pricing in the market is broadly rational from an economic perspective. This is not surprising; previous survey-based research has shown that prostitutes charge a premium for risky behaviors and that the size of the premium is greater for more attractive sex workers (Gertler et al. 2005). Exploiting within-period/within-city variation in the pricing structure across different service venues shows that services performed at a location of the buyer’s choosing (so-called ‘outcall’) earn an estimated 18% price premium, approximately $23 more for an hour-long session, controlling for a wide range of factors. There could be several reasons for this premium. On the supply side, allowing the buyer to choose the location entails both additional physical risk and additional travel time. On the demand side, customers may be willing to pay a premium to reduce their risk and travel time. To assess the magnitude of these sources of variation in pricing we compare how prices vary as the physical size of the MSA for which services are offered expands and as the rate of violent crime in an area changes. Critically, the difference in those correlations across incall services, i.e. service at the provider’s chosen location, and outcall services, i.e. services at the customer’s chosen location, provides a way to sort out supply from demand elasticities. We find that prices for incall services are uncorrelated with MSA size and violent crime rates once some basic controls are added. Prices for outcall services, however, are strongly positively correlated with MSA size, though they are not correlated with violent crime rates once MSA fixed effects are accounted for. These results are consistent with the incall and outcall markets being relatively segmented markets. If incall/outcall were one market then we should see prices moving in opposite directions regardless of whether supply is elastic, demand is elastic, or both. That is women living in larger areas who do not want to travel should compensate men to come to them by offering lower prices and they should charge more for travel. That we primarily observe movement in the outcall market across city size suggests that both supply and demand are fairly inelastic with respect to distance in the incall market but not in the outcall market. The magnitude of the increase for outcall as city size increase indicates a price for providers’ travel time of $36 per extra hour of average commute time in a city, much smaller than the $151/hour mean price for incall time with a client, but much larger than the average female wage of $14/hour in our sample.Footnote 8 That difference is consistent with workers in this market demanding substantial compensation to make up for the distastefulness of time spent with clients, a compensating differential very large compared to the differentials that are easily measured.Footnote 9 These results have a number of policy implications. Most importantly, improved labor market opportunities for women appear to change the composition of suppliers in the market but does not necessarily reduce the volume of activity, at least as proxied by ad postings. Secondly, the large difference between compensation required for travel and that for time spent with clients implies that many workers in the market would happily shift to other activities given the opportunity. Finally, with regards to some risks associated with prostitution (i.e. violence), it appears that sex workers advertising online may have sufficient market power to demand compensation for those risks in the outcall market, implying that the supply of workers in that market is inelastic. The remainder of this paper proceeds as follows. Section 2 provides background on the online market for sex services. Section 3 outlines the technological innovations that enable this research. Section 4 briefly introduces the data. Section 5 analyzes the relationship between pricing and social conditions, including economic opportunities for potential providers. Section 6 concludes.",3
60.0,1.0,Journal of Risk and Uncertainty,19 March 2020,https://link.springer.com/article/10.1007/s11166-020-09322-9,Thoughts and prayers – Do they crowd out charity donations?,February 2020,Linda Thunström,,,Female,Unknown,Unknown,Female,"What motivates prosocial behavior? For centuries, this question has been important to scholars in a range of disciplines (Sachdeva et al. 2009). Numerous meta-analyses, containing decades of research, demonstrate the persistent efforts devoted to identify factors that affect prosocial actions (e.g., Underwood and Moore 1982; Steblay 1987; Carlson et al. 1988; Shariff et al. 2016). In the U.S., prosocial behavior is routinely accompanied by intercessory thoughts and prayers (thoughts and prayers conducted on behalf of others). Thoughts and prayers are often a “first response” to major public risks, such as hurricanes, wild fires, mass shootings or terrorist attacks. These gestures are conducted in both private and public spaces, by citizens and policy makers alike. For instance, in response to the devastating impact of Hurricane Harvey (a category 4 storm that made landfall in Texas August 25, 2017), President Trump declared a National Day of Prayer. In response to the same catastrophe, former President Clinton tweeted: “Our thoughts and prayers continue to be with all of the people affected by Hurricane Harvey and with those helping them.” (August 30, 2017). After a mass shooting in Las Vegas on October 1, 2017, Senator Catherine Cortez Masto (D-Nev) said in a statement that “All of those affected are in our thoughts and prayers,” while after a mass shooting at a school in Parkland, Florida, Florida Governor Rick Scott tweeted: “My thoughts and prayers are with the students, their families and the entire community.” (February 14, 2018). Despite the common usage of intercessory thoughts and prayers as a means to express sympathy with those directly affected by major public risks, their impact on accompanying prosocial behavior is unknown. In this knowledge vacuum, people have formed strong, and diverse, preconceived notions of what might be the impact of thoughts and prayers on prosocial behavior and policy reform aimed at public risk reduction, as shown by a heated public debate in the U.S. Critics perceive intercessory thoughts and prayers as excuses not to take action. This implies not only that the gestures themselves are unhelpful, but also that they may be substitutes for other helpful actions (e.g., financial aid or policy reforms). For instance, former President Obama repeatedly stated that “thoughts and prayers are not enough” when addressing mass shootings during his presidency, implying that policy action is needed to reduce the risk of such catastrophes. This sentiment was echoed by the substantial backlash amongst Twitter users against people sending “thoughts and prayers” after the mass shooting in LasVegas (hashtag #ThoughtsandPrayers). For instance, Senator Chris Murphy tweeted on October 2, 2017, “…your cowardice to act cannot be whitewashed by thoughts and prayers.” In response to such criticism, Vice President Pence said on Fox News November 7, 2017, that praying takes nothing away from efforts to understand what causes mass shootings. This study is the first to explore the impact of thoughts and prayers on prosocial behavior. We focus our study on the effects of thoughts and prayers on material help (charity donations) in the aftermath of a natural disaster. For practical purposes, we limit our study to examine the effect on donations from religious Christians and non-religious people (atheists and agnostics). Christianity is the majority religion in the U.S., with around 65% of Americans identifying as Christians, while atheists and agnostics make up around 10% of the population (Pew Research Center 2019). We develop an analytical framework for the impact of intercessory thoughts and prayers on charity donations to victims of natural disasters. Multiple sources inform our framework (Christian religious scripture, insights from consumer research, and a complementary survey conducted on Amazon Mechanical Turk). We find that intercessory thoughts and prayers increase empathy and saliency of the well-being of natural catastrophe victims, which has a positive effect on donations. Unlike thoughts, intercessory prayers are regarded by senders as directly helpful to recipients (i.e., we identify them as a “moral action”), such that they may be a substitute for monetary donations and thereby reduce donations. The act of praying crowds out charity donations if the negative substitution effect dominates the positive empathy effect. Our analytical framework predicts that intercessory prayers crowd out donations in the wake of major, highly salient, public catastrophes, while thoughts do not. We empirically test the impact of thoughts and prayers on donations in economic experiments with actual monetary donations to natural disaster victims, made via the Red Cross. Participants in our main experiment are recruited from Qualtrics’ market research panel. Consistent with the predictions from our analytical framework, our main experimental study suggests that conducting an intercessory prayer crowds out donations, while donations are unaffected by taking a moment to think of the hurricane victims. Given experimental results are often sensitive to framing, we designed two follow-up experiments to examine the validity of our findings across frames (location of the hurricane and the donation elicitation mechanism). The results from Follow-up experiment 1 suggest the main findings are robust across the location of the hurricane in the continental USA. In Follow-up experiment 2, maximum possible donations are ten times smaller than in the other two experiments and the donation decision is cognitively less demanding. Here, we do not detect an impact of intercessory thoughts and prayers on donations. The crowding out effect of prayers may therefore be sensitive to the stakes at hand. This is consistent with prior research that shows the complementarity between a costless moral action and a subsequent moral action is higher when the cost of the second task is small (Scott 1977). Our results offer first insights into the effect of intercessory thoughts and prayers on prosocial behavior. Our findings underline the importance of examining how these gestures (while undertaken by individuals, policy makers or the nation as a collective, e.g., national prayer days) might impact accompanying material aid and investments in public risk reduction, given we find (albeit not robustly) that they may have undesired effects. Further, our survey responses imply that an intercessory prayer is regarded as a moral action, suggesting we may have identified one of the (possibly the) most frequently used moral actions to accompany material help offered by Americans to those in need.Footnote 1 Identifying intercessory prayers as moral actions is significant, given several influential consumer behavior theories, dating back more than half a century (e.g., Heider 1946; Festinger 1957; Freedman and Fraser 1966; Monin and Miller 2001), that suggest moral actions are related. If prayers are moral actions, these theories suggest that we should expect the act of praying to correlate with subsequent prosocial behavior, even though they may disagree on the direction of the correlation. Our finding that prayers are perceived as directly helpful in the wake of disasters relates to recent research showing that religiosity (the extent to which a person engages in religious behavior) increases in the wake of natural catastrophes (Zapata 2018; Sinding Bentzen 2019). Sinding Bentzen (2019) finds that the increase in religiosity is manifested primarily by people turning to prayers and seeking closeness to God. Ager and Ciccone (2017) find religious participation increases when the risk of destructive rainfall increases, and Auriol et al. (2017) find that church donations may be used as insurance, mainly due to the expectation of divine intervention in the event of catastrophes. These studies, in conjunction with our finding that prayers are perceived as helpful, suggest religiosity, including prayers, may be important parts of private agents’ risk reduction strategies. They may apply both to private and public risk. Viscusi et al. (1988) find that altruistic agents value public risk reductions, and Jones-Lee (1991) shows that safety concerns for others increases the value of public safety. Further, Thunström and Noy (2019) find religious Christians value receiving others’ prayers in times of hardship (e.g., after a natural disaster), as suggested by a positive willingness-to-pay (WTP) for prayers. Religious Christians also value receiving thoughts, if provided by fellow religious Christians. Atheists and agnostics, however, are averse to both thoughts and prayers from religious Christians, while indifferent to thoughts from non-religious. Our overall results, however, suggest that using prayers as risk reduction strategies might come at the expense of lower material aid. More broadly, our study relates to the literature on prosocial behavior and religion or religiosity. Shariff et al. (2016) find that prosocial behavior increases when people are subjected to religious primes (e.g., participants are flashed with religious words), and Benjamin et al. (2016) find religious primes impact public goods contributions. Studies relying on self-reported data typically find that religiosity is positively associated with prosocial behavior (e.g., Monsma 2007), while studies based on observed prosocial behavior are inconclusive about the relationship between religiosity and general prosocial behavior (e.g., Darley and Batson 1973; Batson et al. 1989; Batson et al. 1993; Saroglou et al. 2005; Norenzayan and Shariff 2008). Studies generally find that religiosity is positively associated with volunteer work in the community (Wilson and Musick 1997; Youniss et al. 1999; Ozorak 2003). Religion has also been found to be an important determinant of economic behavior and preferences, see e.g., Iannaccone (1998), Bénabou and Tirole (2003), McCleary and Barro (2006), Noussair et al. (2013), Campante and Yanagizawa-Drott (2015), Minton and Kahle (2016), Noy and O’Brien (2016) and Karlan et al. (2017).",4
60.0,1.0,Journal of Risk and Uncertainty,26 March 2020,https://link.springer.com/article/10.1007/s11166-020-09321-w,Risk attitudes and digit ratio (2D:4D): Evidence from prospect theory,February 2020,Levent Neyse,Ferdinand M. Vieider,Ulrich Schmidt,Male,Male,Male,Male,"Prenatal testosterone exposure (PTE) has organizational effects on the fetus’ brain and endocrine system development and it may thereby have a systematic influence on subsequent behavior (see Manning 2002, for a review). This relationship has attracted the interest of economists, who studied its role in various contexts including social preferences (Buser 2012; Brañas-Garza et al. 2013), financial trading (Coates et al. 2009), competitive bidding (Pearson and Schipper 2012) or managerial activities (Guiso and Rustichini 2018). Although studies investigating the association between risk aversion and 2D:4D are the most widespread in this literature, the results are not conclusive. While several studies showed that higher PTE yields lower risk aversion, many others reported null results. There are two underlying research questions behind increasing interest on PTE and risk attitudes. First, with the rise of neuroeconomics and interdisciplinarity, economists are trying to understand whether human biology has a partial impact on economic behavior (see Robson 2001, for a thorough discussion). Secondly, Paul et al. (2006) also argue that 2D:4D might be inherited. Research on risk attitudes and 2D:4D aims to shed a light on heritability of risk preferences. Such a relationship would be supported by the dual inheritance theory (gene-culture coevolution), which argues that human behavior is a combination of genetic and cultural evolution (Boyd and Richerson 1988; Richerson et al. 2010). This study investigates the relationship between financial risk taking and PTE in the framework of prospect theory. According to prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992; Wakker 2010), risk preferences may differ between gains, losses and mixed prospects, as well as for different probability levels. In his recent study, Hermann (2017) showed that higher PTE correlates with lower degrees of loss aversion. This might suggest that the inconclusive results in the literature may be due to the domain-dependent nature of risk preferences. The evolutionary perspective confirms this argument as survival decisions such as foraging or reproduction involve risk taking both in gain and loss domains. As a result, prospect theory has turned to shaping foraging models as well (McDermott et al. 2008). We find no significant relationship between PTE and risk-aversion in any of the domains. This null result is consistent both for Caucasian and Asian samples in our study.",7
60.0,1.0,Journal of Risk and Uncertainty,27 February 2020,https://link.springer.com/article/10.1007/s11166-020-09320-x,Opting out of workers’ compensation: Non-subscription in Texas and its effects,February 2020,Lu Jinks,Thomas J. Kniesner,Anthony T. Lo Sasso,,Male,Male,Mix,,
60.0,1.0,Journal of Risk and Uncertainty,01 April 2020,https://link.springer.com/article/10.1007/s11166-020-09323-8,"Private security, maritime piracy and the provision of international public safety",February 2020,Gregory DeAngelo,Taylor Leland Smith,,Male,,Unknown,Mix,,
60.0,2.0,Journal of Risk and Uncertainty,18 July 2020,https://link.springer.com/article/10.1007/s11166-020-09325-6,Do measures of risk attitude in the laboratory predict behavior under risk in and outside of the laboratory?,April 2020,Gary Charness,Thomas Garcia,Marie Claire Villeval,Male,Male,Female,Mix,,
60.0,2.0,Journal of Risk and Uncertainty,13 August 2020,https://link.springer.com/article/10.1007/s11166-020-09331-8,The uncertainty triangle – Uncovering heterogeneity in attitudes towards uncertainty,April 2020,Daniel R. Burghart,Thomas Epper,Ernst Fehr,Male,Male,Male,Male,"The analysis of decisions under risk involves prospects that have fully known probabilities. Real-world settings, however, rarely involve full knowledge about prospects. When individuals do not possess full knowledge about prospects they are said to make choices in the face of Knightian uncertainty (Knight 1921). While individual variability in risk attitudes is a well-established finding in that domain, heterogeneity of uncertainty attitudes is not as widely studied. One reason is that studying uncertainty attitudes poses additional challenges. The goal of this paper is to tackle some of these challenges and provide a characterization of individual variability in attitudes towards uncertainty. The first step we take is to assume that the objects of choice are two-outcome lower envelope lotteries.Footnote 1 Lower envelope lotteries specify lower bounds on probabilities, \(\{\underline {p}_{h}, \underline {p}_{l}\}\), for a high and low outcome, zh and zl with zh > zl, and the amount of unassigned probability mass, \( y = 1 - (\underline {p}_{h} + \underline {p}_{l})\), an objective quantity henceforth called uncertainty.Footnote 2 Formally, we denote a lower envelope lottery as \(L = \left (\underline {p}_{h}, \underline {p}_{l}, y \right )\). Analogous to the probability distributions studied in risk, the entries in a lower envelope lottery must be non-negative and sum to one. To parsimoniously model choices in this setting we propose a one parameter extension of Expected Utility. We call this model Partial Ignorance Expected Utility (PEU) and it evaluates a lower envelope lottery (L) asFootnote 3 In terms of notation, \(\underline {L}\) is a transformation of the lower envelope lottery L into a risky lottery with all of the uncertainty in L added to the minimum probability of the worst outcome: \(\underline {L} = \left (\underline {p}_{h}, \underline {p}_{l} + y \right )\). In contrast, \(\overline {L}\) is a transformation of L into a risky lottery with all of the uncertainty added to the minimum probability of the best outcome: \(\overline {L} = \left (\underline {p}_{h} + y, \underline {p}_{l} \right )\). The parameter α controls a mixture of these two lotteries and is interpreted as a choosers attitude towards uncertainty. Values of α > 1/2 place more weight on \(\underline {L}\) with, correspondingly, less weight on \(\overline {L}\). This indicates a pessimistic attitude towards the uncertainty in L and we label choosers with α > 1/2 as uncertainty averse. Values of α < 1/2 put more weight on \(\overline {L}\) and less weight on \(\underline {L}\). This indicates a more optimistic attitude towards uncertainty and we label choosers with α < 1/2 as uncertainty seeking. Choosers with α = 1/2 weight equally between \(\underline {L}\) and \(\overline {L}\) and are called uncertainty neutral. Whatever a chooser’s α, the mixture \(\alpha \underline {L} + (1 - \alpha ) \overline {L}\) is assumed to be evaluated with a von Neumann-Morgenstern Expected Utility, \(\text {EU} \left [ \cdot \right ]\).Footnote 4 To help illustrate how α captures uncertainty attitude consider Fig. 1a. We call this the uncertainty triangle and it is a graphical depiction of all two outcome lower envelope lotteries.Footnote 5 The vertices of the uncertainty triangle represent three special cases: (i) certainty of receiving the good outcome (\(\underline {p}_{h} = 1\)), (ii) certainty of receiving the bad outcome (\(\underline {p}_{l} = 1\)), and (iii) a situation in which nothing is known about the probability distribution over the two outcomes (y = 1). Lower envelope lotteries on the hypotenuse of the triangle are analogous to fully specified lotteries because they have no uncertainty (y = 0). Horizontal movements in the triangle result in a one-to-one tradeoff between uncertainty (y) and the minimum probability for the bad outcome \((\underline {p}_{l})\) while holding the minimum probability for the good outcome \((\underline {p}_{h})\) constant. Vertical movements trade off y and \(\underline {p}_{h}\) holding \(\underline {p}_{l}\) constant while movements parallel to the hypotenuse hold y fixed while trading off \(\underline {p}_{h}\) and \(\underline {p}_{l}\).
 A triangular diagram can be used to plot two outcome lower envelope lotteries and indifference curves for various uncertainty attitudes The Partial Ignorance Expected Utility model can be written as:
 Normalizing utilities such that uh = 1 and ul = 0 gives:
 This means PEU produces indifference curves that are linear and parallel in the uncertainty triangle. The slope of these indifference curves is determined solely by the parameter α ∈ [0,1]. To see this consider a choice between a lower envelope lottery with 100% uncertainty, (0,0,1), and a lower envelope lottery analogous to a 50/50 lottery \(\left (1/2, 1/2, 0\right )\). Figure 1b plots these two alternatives in the uncertainty triangle. The 100% uncertain alternative is at the lower-right vertex and the 50/50 lottery is halfway between the endpoints of the hypotenuse. The PEU for the 100% uncertain option is \(\text {PEU} \left (0, 0, 1\right ) = 1 - \alpha \). The PEU for the 50/50 lottery is \(\text {PEU} \left (1/2, 1/2, 0\right ) = 1/2\). A PEU maximizer is indifferent between these two options if and only if they are uncertainty neutral (i.e. α = 1/2). Indifference curves for an uncertainty neutral chooser are depicted in  in Fig. 1b. An uncertainty averse chooser (i.e. α > 1/2) would prefer the 50/50 lottery to the 100% uncertain option. Indifference curves for an uncertainty averse chooser are depicted in  in Fig. 1c. Someone who is uncertainty seeking (α < 1/2) would prefer the 100% uncertain option to the 50/50 lottery. The indifference curves for an uncertainty seeking chooser are shown in  in Fig. 1d. The PEU model places significant structure on preferences. To explore whether actual behavior is consistent with this structure we designed and conducted an experiment in which the objects of choice were two-outcome lower envelope lotteries. Our experiment systematically varied the tradeoffs between minimum probabilities and uncertainty. This design permits a non-parametric assessment of the PEU model. This assessment indicates that about 60% of participants made choices consistent with PEU maximization. For these PEU maximizers we empirically characterize the heterogeneity of their uncertainty attitudes with finite mixture models. Finite mixture methods provide an endogenous classification of individuals to different preference types and estimate precise preferences \((\hat {\alpha })\) for each type. Our estimation results show that there is substantial heterogeneity of uncertainty attitudes amongst the PEU maximizers. This heterogeneity is best characterized by the existence of three distinct types that, coincidentally, correspond to uncertainty aversion, seeking, and, approximately speaking, neutrality. We also obtain estimated proportions of participants that belong to each of these distinct types. One type is comprised of 30% of the PEU maximizers and corresponds to (near) uncertainty neutrality \((\hat {\alpha } = 0.517)\). Roughly 48% of PEU maximizers exhibited uncertainty aversion \((\hat {\alpha } = 0.583)\). The third type, comprising roughly 22% of PEU maximizers, display uncertainty seeking behavior \((\hat {\alpha } = 0.395)\). Importantly, the finite-mixture approach does not assume the ex-ante existence of these three types. Instead, these three types emerge endogenously from the finite mixture methodology. In addition, almost all of the PEU maximizers are cleanly assigned to one distinct type or another (i.e. the subjects’ posterior probabilities of belonging to, for example, uncertainty aversion, is almost exclusively close to one). About 40% of experimental participants cannot be considered PEU maximizers because they exhibited non-constant uncertainty attitudes. That is, in the uncertainty triangle, their indifference curves are linear but non-parallel.Footnote 6 To parametrically explore preferences for this group we introduce the β-PEU model. The β-PEU model has one more parameter than PEU. It retains the linear indifference curves of PEU but allows indifference curves to ‘fan-in’ (β > 0) or ‘fan-out’ (β < 0) across the uncertainty triangle. Indifference curves that ‘fan-in’ imply increasing aversion to uncertainty when moving northeast in the triangle (i.e. as the minimum probability of the good outcome increases). Indifference curves that ‘fan-out’ imply the opposite – decreasing aversion to uncertainty with northeast movements in the triangle. Again using finite mixture models we show that all of the β-PEU maximizers in our sample have indifference curves that ‘fan-in’ (i.e. increasing aversion to uncertainty, or β > 0). But again there is substantial heterogeneity mainly with regard to their baseline uncertainty aversion: About 62% of β-PEU maximizers exhibit indifference curves that fan in yet are, on average, not significantly different from uncertainty neutrality. The remaining 38% exhibit very similar fanning behavior but, their average uncertainty attitudes are best characterized as uncertainty seeking. The next section of the paper lays out our experimental design and methods in more detail. Section 3 presents our empirical tests and results. Section 4 discusses the relationship between (i) the PEU model and other theoretical approaches towards uncertainty and (ii) our empirical results and other empirical studies of uncertainty and ambiguity attitudes. This section also discusses recent empirical studies of uncertainty/ambiguity. Section 5 concludes and provides directions for future work.",5
60.0,2.0,Journal of Risk and Uncertainty,07 June 2020,https://link.springer.com/article/10.1007/s11166-020-09324-7,The representative heuristic and catastrophe-related risk behaviors,April 2020,Randy E. Dumm,David L. Eckles,Jacqueline Volkman-Wise,,Male,Female,Mix,,
60.0,2.0,Journal of Risk and Uncertainty,03 August 2020,https://link.springer.com/article/10.1007/s11166-020-09327-4,Linking subjective and incentivized risk attitudes: The importance of losses,April 2020,Johannes G. Jaspersen,Marc A. Ragin,Justin R. Sydnor,Male,Male,Male,Male,"Risk attitudes play a major role in almost all economic decisions. They determine consumer preferences over products such as health care, insurance, and investment alternatives. This makes them crucial for policy analyses and highlights their importance in the design of social welfare systems. They also play a vital role in a variety of managerial decisions such compensation schemes, optimal investments, make-or-buy decisions or supply chain design. One attractive method for assessing attitudes toward risk comes from Dohmen et al. (2011), who developed and validated a simple psychometric scale measuring willingness to take on risk. This measure is often called the “general risk question” (henceforth GRQ, Charness et al. 2013).Footnote 1 The GRQ has been found to correlate well with a number of real-world behaviors involving risk (Dohmen et al. 2011) and has high test-retest stability (Lönnqvist et al. 2015). Vieider et al. (2015) also found that the GRQ correlated with certainty equivalents for lotteries in populations across many countries. It has since been applied to various different economic questions, such as in extending the work on state-dependent utility functions by Evans and Viscusi (1991) to the domain of state-dependent risk preferences (Decker and Schmitz 2016). Hence, the GRQ seems to be a useful simple index of risk attitudes for both applied and theory oriented work. There are, however, two caveats to using the GRQ to measure risk attitudes. First, the GRQ measures risk taking rather than risk preferences. In addition to risk attitudes, other factors such as wealth or liquidity could influence responses to the GRQ. Second, even when focusing on the attitudes measured by the GRQ, it would be useful to know more about what aspects of attitudes toward financial risk are captured by it. The GRQ asks respondents how willing they are to take risks. Any answer to this question thus implicitly involves the respondent’s interpretation of the term “risk.” The decision-theoretic literature, namely due to the contribution of Rothschild and Stiglitz (1970), defines risk as variation in an outcome – a concept that is linked to the curvature of the utility function in an expected utility framework. The more colloquial definition in the Oxford English Dictionary defines “risk” as exposure to “the possibility of loss, injury, or other adverse or unwelcome circumstance” – a notion that is not automatically linked to utility curvature and requires differentiating possible outcomes into gains and losses. Since respondents are probably more likely to interpret risk by its colloquial meaning than to adopt the decision theoretical definition, it is possible that the GRQ does not measure risk attitudes according to the expected utility definition. This study focuses on determining which risk attitudes are most closely associated with GRQ responses. The literature on financial risk attitudes has shown that they can involve a range of underlying preferences and biases. They are not limited to attitudes toward the variation in monetary outcomes, but can include concepts such as aversion to losses, preference for certainty, and non-linear weighting of probabilities (Kahneman and Tversky 1979). Additionally, some of these attitudes may differ when considering gains or losses relative to a reference point (Harbaugh et al. 2010). These different underlying sources of risk attitudes can have different implications for the outcomes of decision models (Wang and Webster 2009; Barseghyan et al. 2013; Jaspersen and Peter 2017). Knowing more about the links between the GRQ and different sources of risk attitudes can help determine whether the GRQ is likely to be a relevant index for particular applications. In this study, we investigate the correlations between the GRQ and underlying financial risk preferences using an experimental study carried out both online and in the laboratory, with 1,730 total participants. We measure the five preference motives inherent in a full prospect theory preference functional (Tversky and Kahneman 1992). Specifically, we examine utility curvature in the gain and loss domain, likelihood insensitivity in the gain and loss domain, and loss aversion. We also elicit one additional preference motive – the subjects’ preference for certainty (Schmidt 1998). Our experimental design is a modification and extension of the risk-preference elicitation techniques established by Tanaka et al. (2010) that requires few parametric assumptions. This allows us to create indices for each of the six preference motives. We find that the GRQ correlates modestly – but statistically significantly – with preference motives involving losses, specifically aversion to variation in the loss domain (i.e., utility curvature in the loss domain) and loss aversion (i.e., difference in marginal utility between the gain domain and the loss domain). In contrast, we find no significant correlations between the GRQ and other preference motives and biases, including gain-domain utility curvature, likelihood insensitivity, and preference for certainty. Our results suggest that individuals seem to interpret the concept of “risk-taking” in the GRQ as involving behavior towards losses. This is in line with several studies of managerial risk preferences, which similarly highlight the role of losses in the managerial interpretation of risk as a concept (March and Shapira 1987; Koonce et al. 2005; Gómez-Mejía et al. 2007). The implication for practical applications is that the GRQ will most likely be a useful index of risk attitudes for settings in which financial risk involves a loss component. The GRQ may be less useful, though, for purely gain-side risks or in populations and settings where likelihood insensitivity is expected to be a main driver of behavior. These results also contribute to a broader literature that seeks to determine the properties of different elicitation procedures for risk preferences (Charness et al. 2013, offer a review). The GRQ has been shown to correlate with lottery-based risk preferences in some prior work, including Dohmen et al. (2011), Vieider et al. (2015), and Koudstaal et al. (2015). Some prior studies have also shown no significant correlation between the GRQ and financial risk-taking for lotteries in the gain domain (Lönnqvist et al. 2015; Csermely and Rabas 2016). The paper by Koudstaal et al. (2015) has some of the closest results to ours, finding correlations between the GRQ and measures of risk aversion over gains, loss aversion, and ambiguity aversion. They focus on the differences in risk attitudes between entrepreneurs and other workers. Their primary finding is that entrepreneurs report themselves as less risk averse in the GRQ and show lower loss aversion in the incentivized task. This importance of loss aversion is consistent with the findings in our study. They find overall that gain-domain risk aversion in their incentivized task is correlated with the GRQ. We also find positive one-way correlations between the GRQ and gain-domain risk aversion, but we find that the correlation is much stronger and more robust between the GRQ and loss-domain risk aversion. Our study is the first to investigate the links between the GRQ and financial risk taking using a full prospect-theory functional that captures the primary motives identified in the literature on risk attitudes. Our findings that the GRQ correlates more strongly with loss aversion and risk aversion in the loss domain may help explain why the prior literature has found mixed results when investigating correlations of the GRQ and lottery decisions primarily in the gain domain.",6
60.0,3.0,Journal of Risk and Uncertainty,16 July 2020,https://link.springer.com/article/10.1007/s11166-020-09326-5,E-cigarettes and adult smoking: Evidence from Minnesota,June 2020,Henry Saffer,Daniel Dench,Dhaval Dave,Male,Male,Unknown,Male,"A number of battery-powered devices on the market today deliver nicotine to the user in an aerosol or vapor form and are referred to as electronic cigarettes (e-cigs). Use of e-cigs is often called vaping in contrast to smoking conventional combustible cigarettes.Footnote 1 Because e-cigs are a relatively new product, there is no research on the long-term health effects of use. Nevertheless, e-cigs are generally considered to be less harmful than combustible cigarettes because the vapor produced by them does not contain the toxins and nitrosamines that are found in tobacco smoke (Goniewicz et al. 2013; Czogala et al. 2014). The U.S. National Institute on Drug Abuse states that because e-cigs deliver nicotine without burning tobacco, they appear to be a safer, less toxic alternative to conventional cigarettes.Footnote 2 Public Health England, a public health agency within the U.K.’s Department of Health and Social Care, has taken a more definitive position and stated that e-cigs are significantly less harmful to health and are about 95% safer than smoking (McNeill et al. 2015). The public health debate surrounding the regulation of e-cigs has centered on harms to non-smoking adolescents and harm reduction for adults who smoke. For adolescents the concern is that e-cig use may have negative effects on cognitive development, result in long term nicotine addiction, and may lead to conventional cigarette use. For those adolescents who wish to experiment with nicotine, e-cigs may be a safer option than cigarettes and may have contributed to the decline in adolescent smoking. E-cigs may be effective in helping adult smokers to quit the habit. Currently between 14 and 19% of adults continue to use cigarettes (2017, National Health Interview Survey, NHIS and National Survey of Drug Use and Health, NSDUH), and interest in quitting smoking remains high. Almost two-thirds of current smokers report that they want to quit smoking completely, and among those who expressed such an intent about 60% follow-up with an actual cessation attempt (NHIS 2015). However, most attempts end in relapse, and less than one in ten smokers overall successfully quit in the past year (Babb et al. 2017).Footnote 3 E-cigs may be an effective substitute for smoking, particularly for smokers who have had a difficult time quitting in the past through other methods. Thus, the accessibility of e-cigs might enhance smoking cessation rates. On the other hand, it is also possible, as some contend, that e-cig use may adversely impact smoking cessation by undermining smoking restrictions and providing smokers with an alternative nicotine source for situations where smoking is not permitted. This paper focuses on the potential for harm reduction for adults. There is very little causal evidence to date on how e-cig use impacts smoking cessation among adults. Acknowledging the potential for e-cigs to help smokers quit along with limited empirical evidence on this issue, the Food and Drug Administration (FDA) has thus far refrained from regulating their access for adults. For instance, unlike conventional cigarettes, e-cig manufacturers continue to be able to advertise in broadcast media, and the FDA has resisted banning or restricting such advertising. The FDA has also postponed for now the requirement that e-cig manufacturers submit marketing applications, a condition which would otherwise have effectively banned all e-cig products from the market until the FDA reviewed and approved the applications.Footnote 4 In contrast to the FDA’s relatively more accommodating stance at least with respect to adult access, a growing number of state and local governments have taken steps to more forcefully regulate the sale, marketing, and use of e-cigs. Attorneys General for 29 states signed a letter in 2014 urging the FDA to regulate the sale of e-cigs and restrict its advertising and marketing.Footnote 5 By the time the federal e-cig minimum legal sale age law of 18 went into effect in August of 2016, all states but two had a similar law in place. As of June 2019, 15 states raised their e-cig minimum purchase age to 21. An increasing number of states are also requiring licenses for retail sales of e-cigs and are expanding their smoking bans and clean indoor air laws to include vaping. Several states have also banned sales of flavored e-cigs and Walmart has announced that it will end sales of all e-cigs. There is no federal tax on e-cigs, unlike on cigarettes and other tobacco products. With e-cigs being relatively new, states have struggled to determine whether and how to tax them. As of the end of 2018, ten states (in addition to several cities and counties) had started to levy taxes on e-cigs or the liquid nicotine used with e-cigs. Nine additional states began to do so in 2019 and two more will follow suit in 2020.Footnote 6 Given that one aspect of tobacco taxes is to improve public health and reduce tobacco-related health expenditures, there exists a key knowledge gap in the literature to inform policymakers contemplating taxes on e-cigs. It remains unclear how e-cig taxes impact smoking cessation. If higher e-cig taxes dissuade adult smokers from shifting to vapor products and from quitting smoking in the process, the forgone harm reduction must be taken into account; this would provide justification for taxing e-cigs less than traditional tobacco products, if at all. Similarly, if e-cig taxes promote smoking cessation, by making it more difficult for smokers to circumvent smoking restrictions and by reducing the overall addictive stock of nicotine, then this would provide additional rationale for levying taxes on e-cigs at the federal and state levels. Our study directly addresses this knowledge gap, and makes several contributions in the process. We provide some of the first rigorous evidence on how taxing e-cigs impacts smoking cessation among adults. The empirical analysis exploits the large e-cig tax hike in Minnesota (MN), the first state to tax e-cigs, in conjunction with a synthetic control difference-in-differences approach to identify plausibly causal effects of e-cig use on adult smoking. In addition to providing direct estimates of the cross-effects of e-cig taxation, we also add to the very limited evidence base on the substitution and complementarity between e-cigs and cigarettes. We find consistent evidence that higher e-cig taxes increase adult smoking rates and reduce quits, implying that e-cigs are a likely substitute for conventional cigarettes among current smokers. We also provide the first estimate of the price elasticity of smoking participation with respect to the price of e-cigs implied by the impact of the first imposition of and subsequent large increase in an excise tax on e-cigs in the U.S. in the literature. Estimates suggest that the e-cig tax increased adult smoking and reduced smoking cessation in Minnesota, relative to the control group, and imply a cross elasticity of current smoking participation with respect to e-cigarette prices of 0.13. Our results suggest that in the sample period about 32,400 additional adult smokers would have quit smoking in Minnesota in the absence of the tax. This estimate is an important input towards evaluating the costs and benefits of e-cig taxation and the harm reduction debate. In the process, we add to the limited literature on how e-cig use is impacting adult smokers, drawing on the Minnesota tax hike as a natural experiment to drive exogenous variation in e-cig use. The remainder of the paper proceeds as follows. The next section briefly provides some background on the previous literature. Section 3 details the data and the empirical methods that we apply to this question, followed by a discussion of the results. The concluding section summarizes our findings and places them in context along with some policy implications.",28
60.0,3.0,Journal of Risk and Uncertainty,24 July 2020,https://link.springer.com/article/10.1007/s11166-020-09330-9,The effects of traditional cigarette and e-cigarette tax rates on adult tobacco product use,June 2020,Michael F. Pesko,Charles J. Courtemanche,Johanna Catherine Maclean,Male,Male,Female,Mix,,
60.0,3.0,Journal of Risk and Uncertainty,22 July 2020,https://link.springer.com/article/10.1007/s11166-020-09328-3,Electronic cigarette risk beliefs and usage after the vaping illness outbreak,June 2020,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
60.0,3.0,Journal of Risk and Uncertainty,22 July 2020,https://link.springer.com/article/10.1007/s11166-020-09329-2,News that takes your breath away: risk perceptions during an outbreak of vaping-related lung injuries,June 2020,Dhaval Dave,Daniel Dench,Hua Wang,Unknown,Male,,Mix,,
61.0,1.0,Journal of Risk and Uncertainty,25 November 2020,https://link.springer.com/article/10.1007/s11166-020-09333-6,Decisions under risk: Dispersion and skewness,August 2020,Oben K. Bayrak,John D. Hey,,Unknown,Male,Unknown,Male,"We present a new model of decision making under risk. Crucial to our story is that the decision-maker (henceforth DM) considers not only the expected utility of a lottery, but also the dispersion and skewness of the utilities. Our new theory explains how an individual values lotteries and hence takes decisions under risk. The theory is based on a behavioural description of the evaluation process. In this theory, evaluation is thought of as a two-stage process: first, the DM formulates an interval for the value of each lottery; secondly, the DM takes a weighted average of the extremes of this interval. Crucially, the interval depends upon the dispersion of the lottery, while the weights in the weighted average depend upon the skewness of the lottery and the optimism/pessimism of the individual. Let us break this down into its two stages. As to the first stage, the literature suggests that many individuals find it difficult to state a precise Willingness-to-Pay (WTP) or Willingness-to-Accept (WTA) for a good (Bayrak and Kriström 2016; Dubourg et al. 1994, 1997; Morrison 1998). Studies show that if subjects are given the option of stating their subjective valuations in terms of a single amount or an interval, more than half of subjects prefer to state their valuations in terms of an interval (Banerjee and Shogren 2014; Håkansson 2008; Bayrak and Kriström 2016). However, because of the problems in incentivising the true revelation of intervals (if they exist), this evidence does not prove that people think in terms of an interval, but only suggests it. But this seems a natural phenomenon: if asked to state their valuation for some lottery, individuals usually find it difficult to specify a precise number. Of course, this depends upon the lottery: if it is a certainty, then there is no difficulty; if however the lottery is risky then there is, and it becomes more difficult the more dispersed is the lottery. This is consistent with findings of Butler and Loomes (1988) and Cubitt et al. (2015), who conclude that, on the basis of their experimental evidence, the higher the variance of a lottery, the broader the imprecision range for a lottery.Footnote 1 Consider a lottery that pays either x-d or x + d each with probability one-half. The individual might, for example, say that the value is between x-ad and x + ad where a < 1, and where a depends upon the confidence of the decision-maker. We formulate this more precisely shortly. After the formation of an interval, the second stage sees the individual selecting a single value from the interval. This is done by taking a weighted average of the extremes of the interval, where the weights depend upon the skewness of the lottery and upon the optimism/pessimism of the decision-maker. We shall explain in more detail in the next section. Readers should note that we are not presenting a new normative theory. Instead we focus on the descriptive side of the problem: we distill our new model from the accumulated experimental findings in the literature to explain observed behaviour. The paper is structured as follows: Section 2 formalises our theory; Section 3 describes how our model explains some typical ‘anomalies’ of Expected Utility Theory (EUT) and Rank Dependent Utility Theory (RDUT) found in the literature. Section 4 describes the ‘horse race’ that we conducted, comparing our model to six others familiar in the literature, with our methodology and stochastic assumptions described in Section 5. Section 6 details the results of the ‘horse race’. Section 7 concludes.",7
61.0,1.0,Journal of Risk and Uncertainty,26 September 2020,https://link.springer.com/article/10.1007/s11166-020-09332-7,Dual choice axiom and probabilistic choice,August 2020,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"Economic theory is traditionally based on deterministic preferencesFootnote 1 leaving little room for probabilistic choice.Footnote 2 Empirical research, however, strongly backs probabilistic choice.Footnote 3 Probabilistic choice may occur for a variety of reasons such as unobserved attributes of choice alternatives (e.g. McFadden 1976), imprecision of preferences (Falmagne 1985; Butler and Loomes 2007, 2011), random errors/noise in decisions (e.g. Fechner 1860; Hey and Orme 1994). Models of probabilistic choice originated in mathematical psychology and include inter alia random utility (also known as random preference or random parameter) approach (e.g., Falmagne 1985; Loomes and Sugden 1995), the Fechner (1860) model of random errors (or strong utility)Footnote 4 and the Luce (1959) choice model (strict utility or multinomial logit).Footnote 5 These models are often used in econometric estimation on microeconomic data. Yet, psychological models of probabilistic choice may not always suit economic data, which creates a demand for new models of probabilistic choice designed for economic applications. Random utility/preference/parameter approach can rationalize the preference reversal phenomenon in choice under risk (e.g. Loomes 2005, p.310–311) but it cannot account for rare violations of dominance that are occasionally observed in the data (e.g. Loomes and Sugden 1998, p. 585). Fechner (1860) and Luce (1959) models can rationalize some instances of the common ratio effect (e.g. Loomes 2005, p.305) or violations of the betweenness axiom (e.g., Blavatskyy 2006) in risky choice and some instances of the common difference effect (e.g., Blavatskyy 2017, Section 4, pp. 144–145) in intertemporal choice but they generate too many violations of dominance (Loomes and Sugden 1998, p. 593).Footnote 6 Luce (1959) developed a general theory of probabilistic choice from the choice axiom, which is also known as the independence from irrelevant alternatives. Luce (1959) choice model is often used for eliciting risk preferences (e.g., Camerer and Ho 1994; Wu and Gonzalez 1996; Holt and Laury 2002, Eq. (1), p. 1652) and time preferences (e.g., Andersen et al. 2008, p. 599, Eq. 9; Meier and Sprenger 2015, p. 276, Eq. 1). Luce (1959) choice axiom postulates that the ratio of the probability of choosing one alternative to the probability of choosing another alternative is not affected by the presence (or absence) of other “irrelevant” alternatives in the choice set. This axiom is simple and intuitively appealing in choice situations when there are no substitution/complementary effects among choice alternatives. In Luce (1959) choice model every alternative is chosen with a strictly positive probability. This property may be undesirable in economic applications where clearly inferior alternatives are never chosen. Luce (1959) addressed this problem via a two-stage procedure: first, dominated alternatives are discarded from a choice set; and then a decision maker chooses in a probabilistic manner among the remaining alternatives. Yet, this two-stage procedure creates a discontinuous choice function. This paper proposes a new model of probabilistic choice in which a decision maker may never choose some alternatives (that are sufficiently undesirable compared to other available options). This paper demonstrates that one can formulate a dual version of Luce (1959) choice axiom that is arguably as intuitively appealing as its classic sibling. The dual choice axiom proposed in this paper postulates that the ratio of the probability of not choosing (i.e. rejecting) one alternative to the probability of not choosing (rejecting) another alternative is not affected by the presence (or absence) of other “irrelevant” alternatives in the choice set. In case of a binary choice, the dual choice axiom simply mirrors Luce (1959) choice axiom. The choice of alternative A over alternative B in a direct binary choice is equivalent to B being rejected in favor of A. Yet, when the choice set contains three or more alternatives, the dual choice axiom has different implications from Luce (1959) choice axiom. This paper derives a new model of probabilistic choice from the above dual choice axiom. In this model, the likelihood that a decision maker does not choose one alternative from some choice set is proportionate to the ratio of the disutility of this alternative to the sum of disutilities of all alternatives in the choice set. For example, consider a country that is about to leave the European Union and faces three mutually exclusive scenarios: to leave the union without any agreement (“No Deal Exit”), to leave the union with an agreement (“Deal”) and to stay in the union (“No Exit”). According to the classic choice axiom, the relative likelihood that one country leaves the union without any agreement (“No Deal Exit”) rather than stays in the union (“No Exit”) is not affected by the characteristics of any exiting agreement that the country negotiated with the union for an orderly exit (“Deal”). According to the dual choice axiom, the relative likelihood of either “No Deal Exit” or “Deal” rather than either “No Exit” or “Deal” is not affected by the characteristics of the exiting agreement. In other words, the dual choice axiom postulates that the relative chances that the country leaves the union (with or without agreement) rather than does not leave without an agreement are independent from the agreement itself. When choices are hard, a decision maker can formulate the independence from irrelevant alternatives for a probability of rejecting rather than choosing certain options. This paper is closest to Marley and Louviere (2005) who consider a model of probabilistic choice where a decision maker selects the worst alternative from the choice set.Footnote 7 Their model is similar to Luce (1959) choice model: the higher is the disutility of a choice alternative the more it is likely to be selected as the worst alternative. Marley and Louviere (2005) effectively impose the independence from irrelevant alternatives on the relative probability of selecting the worst choice alternative whereas the dual choice axiom proposed in this paper imposes the independence from irrelevant alternatives on the relative probability of rejecting a choice alternative. In other words, we consider alternatives that are not chosen (rejected) from a choice set, which does not necessarily imply that these alternatives are the worst available alternatives (considered in Marley and Louviere (2005)). In case of a binary choice, when an alternative that is not chosen is automatically the worst available alternative, the model of probabilistic choice derived in this paper from the dual choice axiom coincides with the model of Marley and Louviere (2005) and a binary Luce (1959) choice model. The remainder of the paper is organized as follows. Section 2 presents mathematical notation, formulates classic Luce (1959) choice axiom and a new dual choice axiom. Section 3 presents a new model of probabilistic choice derived from the dual choice axiom. Section 4 compares the new model with Luce (1959) choice model using an example of ternary choice. Section 5 concludes with a general discussion.",1
61.0,1.0,Journal of Risk and Uncertainty,21 November 2020,https://link.springer.com/article/10.1007/s11166-020-09335-4,Risk awareness and adverse selection in catastrophe insurance: Evidence from California’s residential earthquake insurance market,August 2020,Xiao Lin,,,,Unknown,Unknown,Mix,,
61.0,1.0,Journal of Risk and Uncertainty,12 November 2020,https://link.springer.com/article/10.1007/s11166-020-09334-5,Natural disaster and risk-sharing behavior: Evidence from rural Bangladesh,August 2020,Asadul Islam,C. Matthew Leister,Paul A. Raschky,Unknown,Unknown,Male,Male,"The vast majority of households in developing countries have no access to formal types of insurance. Therefore more informal types of risk-sharing are often the only methods to insure against consumption shocks (Townsend 1994; Holzmann et al. 2000). There is a substantial amount of literature that documents the use of informal risk-sharing networks to counter the adverse income effects associated with sickness and unemployment (Ravallion and Dearden 1988; Fafchamps and Lund 2003), weather shocks (Morduch 1991), and natural disasters (Freeman and Kunreuther 2002; Zylberberg and Gröger 2016). Considering that informal risk-sharing networks do not rely on formal contracts, an interesting question is whether commitment to informal risk-sharing is stable over time and, in particular, if individual risk-sharing behaviour is affected by exposure to large negative shocks, such as natural disasters or violent conflict. This question is important for various reasons. Informal mechanisms are often insufficient to cover the damages from large negative shocks because the individual risks are often highly correlated, resulting in very large cumulative damages. In addition, large negative shocks could potentially distort or fully eradicate the only available mechanism that allows households to smooth consumption in poor societies. Eventually, this might have longer lasting adverse effects on the individuals’ livelihoods, even after the initial losses of that shock have been absorbed. There are a number of studies that hint towards the idea that there could be a link between large negative shocks and risk-sharing behaviour. First, large losses affect individual risk preferences, which, in turn, can impact individual risk-sharing behaviour. A number of empirical studies have confirmed that risk attitudes tend to be systematically affected by large negative events (e.g., Eckel et al. 2009; Malmendier and Nagel 2011; Voors et al. 2012; Haushofer and Fehr 2014; Page et al. 2014; Cameron and Shah 2015; Hanaoka et al. 2018). So far, there is no clear consensus in the empirical literature about the effect of natural disasters on risk aversion. One group of studies (Cameron and Shah 2015; Cassar et al. 2017) has found increased risk aversion, while another set of studies (Eckel et al. 2009; Page et al. 2014; Hanaoka et al. 2018) finds that people tend to become more risk tolerant after experiencing a natural disaster and that this effect can even persist over multiple years (Hanaoka et al. 2018). Further, work by Ghatak (1999), Ahlin (2009), and Attanasio et al. (2012) provide evidence of the link between individual risk preferences and the formation of informal risk-sharing groups. Although Ahlin (2009) finds evidence of assertive matching based on risk preferences in risk-sharing groups in general, Attanasio et al. (2012) reveal that this is only the case for groups with strong social networks. Second, large negative shocks are believed to have a systematic impact on social networks and social capital (e.g., Fleming et al. 2014; Cassar et al. 2017; Toya and Skidmore 2014; Yamamura 2016). Fleming et al. (2014) find that reciprocity among rural villages decreased a year after the 2010 Chilean earthquake. In contrast, using a number of lab-in-the-field experiments, Cassar et al. (2017) show that trust and pro-social behavior increased among respondents in areas affected by the 2004 tsunami. Relying on cross-country data, Toya and Skidmore (2014) find that the frequency of major natural disasters has a positive effect on social capital accumulation. Our paper links this literature with existing studies showing that social capital is important for the formation and stability of informal risk-sharing networks (e.g., Weerdt and Dercon 2006; Fafchamps and Gubert 2007; Mazzocco and Saini2012; Munshi and Rosenzweig 2016). A third link between large negative shocks and risk-sharing behavior is that exposure to a disaster can affect individuals’ risk perceptions (e.g., Botzen et al. 2009; 2015), which, in turn, can increase the demand for private protective measures and risk-sharing mechanisms such as disaster insurance (e.g., Botzen and Van Den Bergh 2012b; Bubeck et al. 2012; Botzen et al. 2019; Thieken et al. 2006). What is, however, absent from the literature is an empirical analysis that directly investigates the relationship between large negative shocks and risk-sharing behaviour. The purpose of the current paper is to fill this gap using data from a field experiment in Bangladesh. More specifically, we exploit the random cut-offs of inundation zones resulting from Cyclone Aila in 2009 in the districts of Khulna and Satkhira to identify the effect of disaster exposure on risk-sharing commitments. Villages that were hit by the cyclone-related flooding are the treatment villages, and the adjacent villages that were unaffected by the disaster serve as the control villages. To aid in the interpretation of our findings, we also develop a theoretical model of risk-taking, risk-sharing, and defection. The model highlights the role of social costs in deterring defection. Precisely, in villages that exhibit a combination of (i) more risk-taking, (ii) smaller risk-sharing groups, and (iii) less defection, the within groups must face stronger social norms to prevent defection. As discussed below, this mix of behaviors is exactly what we observe among our treatment villages. Our identifying assumption is that the individuals in the flood-affected villages are comparable to the individuals in the unaffected villages. A common concern among studies that use natural disasters as a natural experimental setting (e.g., Page et al. 2014; Cameron and Shah 2015; Hanaoka et al. 2018), however, is that the population in affected and unaffected villages might differ because of some unobservable factors, which simultaneously drive the selection into the treatment group and their risk-taking behaviour. For example, more affluent villages could afford better flood protection and also have a different demographic composition, which affects their members’ risk-taking commitments (e.g., Kousky et al.2006). With respect to this concern, Cyclone Aila and the resulting inundation of Khulna and Satkhira provide a unique setting for a natural experiment. First, man-made protective measures are largely absent, and the few structural measures (i.e., elevated roads) were no match for Aila’s storm surge. Second, the study area is located in a large delta on Bangladesh’s coast on the gulf of Bengal, and the entire area is flat and lacks significant elevational changes. As such, the topography does not offer any elevated places for settlement to be more protected in the case of flooding. Therefore, the selection into the treatment group is solely driven by the physical magnitude of the cyclone and the resulting storm surge and is, as such, random. To provide further auxiliary support that our identification assumption is valid, we show evidence that the participants in the adjacent villages are similar regarding a set of observable characteristics. To measure risk-sharing behaviour, we invited a random subset of households in both the treatment and control villages to participate in a series of lab-style experiments in makeshift laboratories set up in each village. The design of this lab-in-field type of experiment closely follows the design of Barr and Genicot (2008). In the first step, participants had to choose from different lotteries in a standard risk-taking game.Footnote 1 In the second step, they were asked to form a risk-sharing group that pools and shares the gains from the group members’ gambles. Then, the participants were assigned into one of the three different information treatments. Treatments varied with respect to the level of exogenous commitments that allowed individuals to defect and whether such a defection was public or private information. The main findings are that the participants in disaster-affected areas are more risk loving, form smaller risk-sharing groups, and are less likely to defect in risk-sharing commitments, regardless of the level of exogenous commitment and information. Viewed through the lens of our theoretical model, this suggests a strengthening of the social norms supporting risk-sharing commitments in the wake of natural disasters. Our study’s major contribution is that it builds an empirical link between the literature on the impact of large negative shocks on risk preferences (e.g., Loewenstein and Angner 2003; Eckel et al. 2009; Malmendier and Nagel 2011; Page et al. 2014; Cameron and Shah 2015; Hanaoka et al. 2018) and the literature on the nexus between individual risk preferences and risk-sharing behavior (e.g., Ahlin2009; Attanasio et al. 2012). To the best of our knowledge, our paper is the first one that combines the use of disasters as a natural experiment from the former strand of literature with the methods for investigating risk-sharing behaviour in an incentivized manner from the latter strand. Our findings are of particular importance for the literature that deals with decision making under uncertainty with respect to low-probability-high-loss (LPHL) events (e.g., Kunreuther 1996; Kunreuther et al. 2001; Kunreuther and Pauly 2002; Browne et al. 2015). In line with existing studies (e.g., Viscusi and Zeckhauser2006; Page et al. 2014; Hanaoka et al. 2018), we confirm that the recent exposure to a natural disaster makes people less risk averse. This could be problematic if this adversely affects the individuals’ willingness to prepare for such LPHL events, which are already prone to under-insurance. However, the result of our second experiment suggests that despite becoming more risk tolerant, individuals are also more likely to commit to risk-sharing institutions. This could be an indication that the affected individuals do not necessarily shirk risk-sharing responsibility despite becoming less risk-averse. As such, our results also provide new insights for economic research on the demand for disaster insurance. In particular, we offer an additional explanation why individuals tend to underinsure against LPHL events such as natural disasters (Botzen and Van den Bergh 2012a; Gallagher 2014; Kousky 2010; Kunreuther 1996; Kunreuther et al. 2009; Landry et al. 2016; Petrolia et al. 2013; Raschky et al. 2013). More broadly, our paper also contributes to the broader theoretical and empirical literature about individual decisions to join risk-sharing groups (e.g., Pratt and Zeckhauser 1989; Tausch et al. 2014; Cettolin and Tausch 2015). We extend this strand of literature by looking specifically at the effect of experiencing a LPHL event on one’s decision to join a risk-sharing group. In addition, our field experiment builds on existing evidence in this area, which is predominately based on laboratory experiments. Our study can inform economists and policy-makers trying to explain why the level of disaster insurance is below an optimal social level (Kunreuther 1996; Kriesel and Landry 2004; Kunreuther et al. 2009; Raschky and Weck-Hannemann 2007; Kousky et al. 2018) and can help those want to design more efficient financial risk-transfer mechanisms against natural disasters, in particular in developing countries. The remainder of the current paper is organized as follows: In Section 2, we provide a brief description of the disaster in question and the physical impact it had on the community. In Section 3, we discuss the experimental design that we adopt, which closely follows Barr and Genicot’s (2013) design. Section 4 summarizes the theoretical model. Section 5 presents the field experiment and data. Section 6 discusses the results, and Section 7 concludes.",9
61.0,2.0,Journal of Risk and Uncertainty,02 November 2020,https://link.springer.com/article/10.1007/s11166-020-09337-2,Pricing the global health risks of the COVID-19 pandemic,October 2020,W. Kip Viscusi,,,Unknown,Unknown,Unknown,Unknown,,
61.0,2.0,Journal of Risk and Uncertainty,11 November 2020,https://link.springer.com/article/10.1007/s11166-020-09338-1,Valuing mortality risk in the time of COVID-19,October 2020,James K. Hammitt,,,Male,Unknown,Unknown,Male,"As societies try to judge what restrictions on normal activities should be taken to reduce the spread of pandemic SARS-CoV-2, a key parameter is the appropriate tradeoff between wealth or income and mortality risk. The usual approach to quantifying this tradeoff is the value per statistical life (VSL). In the United States, a value of about $10 million is currently used when evaluating government regulations that affect environmental, health, and safety risks (Robinson et al. 2019). Using this value suggests it would be worthwhile for the U.S. population to sacrifice $1 trillion, nearly 5% of U.S. GDP, to reduce the number of U.S. deaths from the pandemic by 100,000. With projections of U.S. deaths from COVID-19 in the absence of any control that have ranged as high as 2.2 million (Ferguson et al. 2020), it is possible that many hundreds of thousands of deaths could be prevented. Using figures like these (or smaller), Greenstone and Nigam (2020) estimated the benefits of reduced mortality from three to four months of moderate social distancing as $8 trillion and Thunström et al. (2020) estimated the net benefits of social distancing in their base case as $5 trillion, more than one-fifth of GDP. Hall et al. (2020) estimated the average American would be willing to give up more than one-third of a year’s consumption to eliminate the mortality risk created by the pandemic if the average risk is 8 in 1000 and almost one-fifth if it is 3 in 1000. Is the conventional VSL appropriate for evaluating the benefits of preventing deaths from the pandemic? Is it too large? There are at least three reasons to think so: (1) VSL is the marginal rate of substitution of wealth for current mortality risk but the risk reductions achieved by controlling the pandemic are non-marginal and are valued at a lower rate; (2) COVID-19 mortality risk seems to be strongly concentrated among the elderly for whom VSL is arguably much smaller than for younger people; (3) the pandemic has caused substantial income losses, especially in some sectors, that decrease VSL. In contrast, other factors suggest the appropriate VSL should be larger for COVID-19 than for other risks, including the catastrophic and novel nature of the pandemic and ambiguity about the risk. I address these issues in the following sections. Section 2 discusses the valuation of non-marginal risks. I derive a novel closed-form solution for the value of a non-marginal risk reduction as a function of baseline survival probability, income, and the income elasticity of VSL. The solution reveals that the standard economic model underlying VSL, combined with estimates of how VSL varies with income, implies that the rate of willingness to pay for non-marginal risk reductions is well-approximated by VSL until the risk reduction is so large that willingness to pay accounts for a substantial fraction of income. Section 3 describes how VSL varies with age and finds that, while evidence is inconsistent, VSL is plausibly smaller at advanced ages. In addition, the value of a continuing risk reduction is smaller for people with shorter life expectancies who will expect to benefit for a shorter period. Section 4 evaluates the effect of lost current income and concludes the appropriate VSL for evaluating response policies is more sensitive to whether the pandemic causes sustained income losses than to any transient effects. Section 5 discusses some of the evidence about how VSL varies with qualitative attributes of how risk is perceived such as dread, uncertainty, and ambiguity; it suggests these factors may justify using a larger VSL to evaluate COVID-19 risks than for other risks but the evidence about how large an adjustment may be warranted is weak. Section 6 concludes. Note that some of the results, especially those concerning willingness to pay for non-marginal risk reductions, the smaller value of a continuing risk reduction to people with shorter life expectancies, and the effects of transient or permanent income losses on valuation, are applicable to other hazards and other contexts, as well as to the COVID-19 pandemic.",31
61.0,2.0,Journal of Risk and Uncertainty,04 November 2020,https://link.springer.com/article/10.1007/s11166-020-09339-0,The forgotten numbers: A closer look at COVID-19 non-fatal valuations,October 2020,Thomas J. Kniesner,Ryan Sullivan,,Male,,Unknown,Mix,,
61.0,2.0,Journal of Risk and Uncertainty,18 November 2020,https://link.springer.com/article/10.1007/s11166-020-09336-3,"Political polarization in US residents’ COVID-19 risk perceptions, policy preferences, and protective behaviors",October 2020,Wändi Bruine de Bruin,Htay-Wah Saw,Dana P. Goldman,Unknown,Unknown,Female,Female,"When COVID-19 started its spread across the United States, many states initially announced school closures and bans of large gatherings (Yeung et al. 2020). To limit disease transmission, the Centers for Disease Control and Prevention (2020a) recommended protective behaviors such as hand hygiene and social distancing. Mass adoption of these behaviors is especially important when pharmacological interventions are not yet available (Bruine de Bruin et al. 2006). According to theories of decisions about health behavior, people who perceive greater risks are more willing to implement protective behaviors and more likely to prefer government policies designed to mitigate risk (Fischhoff 2013; Rosenstock 1974; Rogers 1975). Links between perceived risks and protective behaviors have traditionally been studied for familiar risks like seasonal influenza (Brewer et al. 2004; Bruine de Bruin and Carman 2018). With emerging diseases like COVID-19, objective risk information is, at least initially, scarce, uncertain, and subject to change. Yet, as the COVID-19 crisis progressed in the United States in March 2020, risk perceptions of getting COVID-19 and dying if infected were already associated with taking more protective actions, such as handwashing and social distancing (Bruine de Bruin and Bennett 2020). However, perceptions of risk may be socially constructed, and therefore vary with political inclinations (Kasperson and Kasperson 1996; Leiserowitz 2006; Sjöberg 2000). A recent paper in Nature Human Behaviour warned that political polarization during a pandemic may lead individuals with different political inclinations to arrive at different conclusions about the level of threat and appropriate actions to be taken (van Bavel et al. 2020). In a risk perception study conducted across ten countries, individualistic worldviews and prosocial values were a stronger predictor of risk perceptions of getting COVID-19 than were knowledge about the risk (Dryhurst et al. 2020). Especially in the United States, self-identifying as Democrat rather than Republican has been associated with higher perceived risk of getting COVID-19, perhaps reflecting differences in worldviews and values (Dryhurst et al. 2020). These differences were also reflected in the political discourse about COVID-19 in the United States: Republican President Trump initially aligned COVID-19 risks with seasonal influenza risks and argued that the country did not shut down for 36,000 influenza deaths per year (National Public Radio 2020). As the economic impacts of the COVID-19 pandemic became clear, calls for re-opening the economy started emerging, especially from Republican politicians (New York Times 2020a). Political polarization of risk may be amplified due to different news sources being preferred by individuals varying in political inclinations (Iyengar and Hahn 2009). Exposure to media coverage has been linked to risk perceptions and policy-specific knowledge (Barabas and Jerit 2009; Combs and Slovic 1978). In the United States, self-identified Democrats tend to prefer CNN while self-identified Republicans tend to prefer Fox News (Iyengar and Hahn 2009). Indeed, television news coverage may be relatively partisan, with Fox News having the most conservative audience among major media outlets (Hamilton 2006; cf. Lott and Hassett 2014). Polls have shown that Fox News viewers were less worried about COVID-19 than CNN viewers, as early as March 2020 (Motta et al. 2020). Although it has been documented that the political inclinations and associated media use of US residents predict their risk perceptions and worry about getting COVID-19 (Dryhurst et al. 2020; Motta et al. 2020), less is known about their contribution to risk perceptions for experiencing negative economic consequences during the pandemic, their preferences for the timing of opening the economy, and their tendencies to implement protective behaviors. Before the COVID-19 pandemic, a majority of US residents across the political spectrum viewed individuals as personally responsible for their own protective health behaviors to mitigate their health risks (Robert and Booske 2011). In contrast, there was a political divide in regard to implementing societal change to improve health outcomes, for which Democrats showed more support than Republicans (Robert and Booske 2011; see also Gollust et al. 2009). Thus, people’s political inclinations may be especially important for understanding their policy preferences as compared to risk perceptions and protective behaviors. While the COVID-19 outbreak was affecting the United States in April–May 2020, we sought to understand how people’s political inclinations and media use were associated with (1) their risk perceptions for getting infected with COVID-19, getting hospitalized or dying from it, and running out of money; (2) their preferences for government policies, and (3) their tendencies to implement protective behaviors such as wearing face masks, hand washing and social distancing.",134
61.0,3.0,Journal of Risk and Uncertainty,10 December 2020,https://link.springer.com/article/10.1007/s11166-020-09341-6,Robust inference in risk elicitation tasks,December 2020,Ola Andersson,Håkan J. Holm,Erik Wengström,Male,Male,Male,Male,"To err is human, as the proverb says, but the empirical fact is that some people are more likely to err than others. Previous research has shown that error propensities are related to observable characteristics such as cognitive ability, education, age and gender (Andersson et al. 2016; Choi et al. 2014; von Gaudecker et al. 2011; Burks et al. 2009; Eckel 1999). Since decision noise leads to bias in most elicitation tasks (see, for example, Crosetto and Filippin 2016), there is a risk of falsely interpreting noise-driven relationships as preference driven. In Andersson et al. (2016), we show that this danger is real by demonstrating that cognitive ability can be both positively and negatively correlated to estimated risk preferences, depending on how the risk elicitation task is constructed. This suggests that the relationship between cognitive ability and risk preferences reported in the earlier literature may be spurious (see, e.g., Benjamin et al. 2013; Dohmen et al. 2010). The previous evidence shows that decision errors are heterogeneous, which may lead to spurious inference regarding the relationship between risk preferences and personal characteristics. This problem should be taken seriously but it does not necessarily imply that any attempt to measure risk preferences and relating preferences to observable characteristics is futile. Instead, the findings in this paper highlight that using appropriate elicitation tasks and econometric methods may help to overcome this bias. In regard to econometric specifications, it is worth noticing that simple OLS estimations cannot handle such error structures. Instead, we need methods that take the heterogeneity of noise into account. A promising approach is to use “off the shelf” structural econometric specifications that take the heterogeneity of the noise into account in combination with multiple elicitation tasks so that the error structure can be estimated with precision. In this paper, we demonstrate the usefulness of this approach. In particular, by utilizing data from an experiment with a random sample from the Danish population, we estimate a random parameter CRRA utility function (Apesteguia and Ballester 2018).Footnote 1 One appealing feature of structural models is that a range of parameters, including noise parameters, can be estimated jointly and be allowed to correlate with covariates. We estimate the models using experimental data from the iLEE panel, with subjects from the Danish adult population from all walks of life.Footnote 2 To elicit risk preferences, we use two Multiple Price Lists (MPLs) that differ with respect to the implied switch point for a given risk preference, and this difference allows for a precise estimation of the error structure. We find that cognitive ability is significantly negatively related to risk aversion if we do not allow cognitive ability to correlate with the noise parameters, which corroborates previous findings (e.g., Benjamin et al. 2013; Dohmen et al. 2010; Beauchamp et al. 2017). However, when we do allow for such correlation, we find no significant relationship between risk aversion and cognitive ability. Instead, we observe that cognitive ability is negatively correlated to the amount of noise.Footnote 3 Our analysis corroborates the findings of Andersson et al. (2016) by using structural estimation techniques. In that paper, we showed, using an experimental design, that there is a spurious relation between cognitive abilities and estimated risk preferences due to how preferences are elicited. One potential mechanism behind the spurious relation is noisy decision making. However, as noted by Dohmen et al. (2018) it may still be that cognitive ability is related to risk preferences but that this is masked by decision noise. Hence, we cannot conclude that there is no underlying relation between cognitive ability and risk preferences. By structurally modelling both risk and noise we take the analysis one step further to understand these relationships. Indeed, in the current paper, we find that cognitive abilities are more correlated with noisy behavior than to modelled risk preferences. The potential problem of spurious relationships naturally extends beyond cognitive ability. To investigate this issue, we use measures of age, gender and personality characteristics (Big Five inventory) of the subjects on whom we elicit risk preferences. Letting these measures correlate with the noise measures as well as the preference parameters, we find that age and education are more closely related to noise than risk preferences. Yet, other variables are much less related to decision noise. In particular, several Big Five personality traits are strikingly robust to our different noise specifications and significantly correlated with risk aversion even after allowing for heterogeneous noise. These findings add to the literature on the relationship between economic preferences and personality measures (see Almlund et al. 2011). In a study using a representative sample from the German population, Becker et al. (2012) find that Big Five personality measures, as well as measures of educational attainment, correlate with risk preferences.Footnote 4 However, they do not control for noisy decision making. Our results suggest that only Big Five but not education robustly relates to risk preferences. One natural question is whether it suffices to use multiple elicitation tasks to mitigate the bias problem without resorting to structural estimations. In this paper, we show that using a balanced design (in our case pooling two skewed MPLs) mitigates the bias somewhat but does not entirely eliminate it. Naturally, it is inherently hard to construct a fully balanced design since subjects’ risk preferences are unknown ex ante. Our results highlight that it is important to employ structural estimation techniques that allow noise to depend on covariates (such as age, education and cognitive ability) in addition to using a balanced design. While this approach requires an extensive set of choice tasks, it enables the researchers to obtain signal-to-noise ratios for a given set of choices. The paper contributes to an old but recently resurrected literature on measurement errors in experiments (see Gillen et al. (2019) for an overview of this literature). However, Gillen et al. (2019) review recent experimental papers published in the top 5 economics journals and show that very few of them try to handle measurement errors. An exception is Beauchamp et al. (2017) who use a latent variable model approach to handle potential measurement errors in estimated risk preferences. They assume homogeneous noise across individuals. However, as we have shown in Andersson et al. (2016), less cognitively able subjects tend to be less consistent; hence it is not clear that homogeneous noise is a good assumption. It may improve the precision of aggregate estimates, but is not well suited for inference regarding preference heterogeneity. Another approach is to collect several measures and use an instrumental variable strategy to reduce the effects of measurement error. Recently, Gillen et al. (2019) use such an approach to re-examine the effect of risk preferences on competitive behavior. Both these studies consider homogeneous noise (classical measurement error) whereas we allow noise to be heterogeneous. In particular, our results show that only allowing homogenous noise may not be sufficient to control for noise in the estimated parameters. Chapman et al. (2018) propose a dynamic estimation method that tries to minimize the effect of noise on estimated preferences by using Bayesian methods to optimally select decision tasks. They find that the consistency of subjects’ choices in MPLs affects the correlation between cognitive abilities and estimated risk preferences which supports the results presented here. When using their proposed method they find that cognitive ability and risk preferences are correlated. However, as with the previous methods, they do not allow noise to be heterogeneous which can potentially explain why our results differ. Finally, in a paper developed subsequently to ours, a factor analysis approach is used in combination with a random parameter utility specification, as in this paper, to reduce noise in preference elicitation (Jagelka 2020). In line with the findings presented here, he finds that cognitive ability is mostly correlated with noisy behavior, while his measured psychological traits are correlated with both risk preferences and noisy behavior. Unlike us, he does not use multiple MPLs to debias the estimation, which may explain why a significant correlation between cognitive abilities and risk aversion is found. The rest of the paper is structured as follows. Section 2 explains the basic intuition for why noisy decision making may create biased inference. Section 3 describes the experiments and our measures of cognitive ability and personality. Section 4 presents the results from the main specification and Section 5 concludes.",13
61.0,3.0,Journal of Risk and Uncertainty,29 January 2021,https://link.springer.com/article/10.1007/s11166-020-09343-4,Broad bracketing for low probability events,December 2020,Shereen J. Chaudhry,Michael Hand,Howard Kunreuther,Female,Male,Male,Mix,,
61.0,3.0,Journal of Risk and Uncertainty,14 January 2021,https://link.springer.com/article/10.1007/s11166-020-09342-5,Liking the long-shot … but just as a friend,December 2020,Matthew P. Taylor,,,Male,Unknown,Unknown,Male,"There is substantial evidence that the skewness of a gamble affects people’s decisions, and that people tend to have a preference for skewness (see Santos-Pinto et al. 2009 for a summary of this literature). Building on several experimental studies that find a preference for skewness (Santos-Pinto et al. 2009; Brünner et al. 2011), Grossman and Eckel (2015) introduce a new instrument that efficiently measures subjects’ risk preferences and skewness preferences simultaneously with a straightforward and easily comprehensible task. The GE skewed lottery task (referred to as GE Task hereafter) measures a subject’s skew and risk preferences using a three-round task, in which the lotteries added in each subsequent round have greater skew than the prior round but identical expected values and standard deviations. This cleverly-designed task, which only requires the subject to make three decisions, allows them to test whether the introduction of skewness affects risk preferences, and they find that it does. In fact, they find that more than 88% of subjects prefer a lottery with positive skew relative to a lottery with zero skew, and, more importantly, they find that nearly 38% of subjects increase risk taking when presented with skewed lotteries. Based on these finding, they conclude that “skewed, long-shot payoffs entice decision makers to higher levels of risk taking than they otherwise would prefer.” Grossman and Eckel (2015, p. 197). The implication of this finding is that some individuals may be easily manipulated to take more risk if choices are structured so that they have more skew (e.g., state lotteries). However, it is possible that the GE Task overestimates preferences for long-shot payoffs because of the impact of loss aversion, the changing nature of the risk proposition implied by shifts in the bounds for the coefficients of constant relative risk aversion (CRRA) when the long shot is added, and framing effects. Loss aversion may confound their findings because, by necessity, increasing skewness while holding expected value and standard deviation constant requires that the worst possible outcome improves. This is particularly problematic for the most risky lotteries because their worst possible outcomes are losses when skew equals zero and these potential losses decrease in magnitude as skew increases. In fact, the worst possible outcome switches from a loss to a gain for one option. Given that individuals tend to weigh losses significantly greater than gains, the increase in risk taking and the preference for the long-shot lotteries may have been primarily driven by the reduction in potential losses (Kahneman et al. 1991; Rabin and Thaler 2001). The improvement in the worst option also means that subjects may not be revealing “higher levels of risk taking” when they choose an option with a greater standard deviation because, even though the standard deviation is held constant when skew is added to a particular lottery, the addition of the third potential outcome (the long shot) shifts the implied bounds for the CRRA coefficients derived from the choices. This shift in CRRA coefficients means that it is possible for a subject to choose an option with greater skew and a greater standard deviation and not, in fact, be revealing a greater tolerance for risk tasking. In addition to the potential effects of loss aversion, the order in which the lotteries are presented to subjects may result in framing effects—order and position effects, in this case. The literature exploring order and position effects in choice behavior increasingly indicates that framing effects are context dependent. If choices in the skewed lottery task suffer from a framing effect, they are most likely to suffer from recency and edge biases because they are presented sequentially, they require cognitive processing, and the unique feature that delineates the new options has a positive valence (Bruine de Bruin and Keren 2003; Bruine de Bruin 2005; Unkelbach et al. 2012; Bar-Hillel 2015). Thus, subjects may appear to have a greater preference for skew and risk taking because they are biased toward the lotteries that are presented last and toward the edges of the choice set. I investigate the importance of these effects by modifying the GE Task to eliminate loss aversion and shifting CRRA coefficients as an explanation for the increase in risk taking or preference for skewness, and also include an additional treatment in which the choices are presented in reverse order to test for framing effects. To eliminate loss aversion as a confounding factor, the modified GE task holds the worst possible outcome constant across the three rounds. The trade-off for this modification is that the standard deviation increases slightly because the expected value is held constant, but it also keeps the CRRA coefficients relatively constant. Moreover, Brünner et al. (2011) and Cubitt et al. (2015) find that changes in standard deviation of this scale are unlikely to significantly affect subject choices. To test for framing effects, half the subjects were presented the no-skew lotteries first and then lotteries with more skew were added (like GE2015), and the other half were presented with the most skewed lotteries first and less skewed lotteries were added. Like Grossman and Eckel (2015), I find that most subjects have a preference for skewness: 78% opt for a skewed lottery even when the worst possible outcome and the implied CRRA coefficients are held constant. However, presenting the lotteries in reverse order decreases the fraction who select a lottery with positive skew to about two-thirds (67.9%). This implies that a non-negligible proportion of the apparent preference for skew may be driven by a recency effect when the lotteries are presented in order of increasing skew. Still, it is clear that a majority of subjects prefer lotteries with skew. More importantly, introducing a long shot outcome is unlikely to be the cause of the increase in risk taking observed by Grossman and Eckel (2015). When the lotteries with more skew are added, like in GE2015, 29.4% of subjects choose a more risky lottery in the third round than the first round. However, when the lotteries are presented in reverse order, the average level of risk taking increases as the skew of the newly-introduced lotteries decreases—17.9% of subjects in this treatment choose a more risky lottery in the third round than the first round. Although this increase in risk taking is not statistically significant, it is clearly inconsistent with the hypothesis that the presence of a long shot is causing individuals to increase risk taking. Furthermore, the level of risk taking is nearly identical across treatments in the first round regardless of whether the lotteries have the most skew or they have zero skew, and a slightly greater proportion of subjects decrease risk taking when skew increases (13.2%) than when skew decreases (11.9%). An additional 10% of subjects in both treatments increase and decrease risk taking over the three rounds. These results indicate that there is a substantial level of adjustment in risk taking in both directions, regardless of the order in which the lotteries are presented. It is possible that these adjustments may be reflecting subjects’ true preferences (adding skew while reducing risk seems reasonable), or it may simply be the type of noise that we would expect to see when subjects have imprecise preferences (Butler and Loomes 2007; 2011; Cubitt et al. 2015). It also suggests that presenting the lotteries so that skew increases appears to introduce a framing bias that encourages subjects to move down and to the right. Although it appears unlikely that skew induces subjects to increase risk taking, the GE Task, and the modified version presented here, can be valuable tools that provide an efficient method to measure both skew and risk preferences simultaneously and distinguish between them. The preponderance of evidence suggests that risk preferences and skew preferences are distinct, and individuals tend to be both risk averse and skew seeking. Most multiple price lists, such as the commonly-used MPL introduced by Holt and Laury (2002) are not designed to account for the impact of skew preferences and, thus, do not effectively measure and disentangle these distinct preferences (Drichoutis and Lusk 2016). To be sure, the decisions from MPLs can be used to estimate parameter values for functionals derived from prospect theory, which allow for the possibility that subjects overweight small probabilities for the long shot, but parameter estimates from maximum likelihood estimation using experimental data are often not robust to the various assumptions that must be made to estimate them (Harrison 2006). The GE Task eliminates the need to make these assumptions, but experimenters must be cautious of the framing effects that may cause individuals to appear to be more risk tolerant and skew seeking. Section 2 describes the skewed lottery task introduced by Grossman and Eckel (2015) in more detail and provides a more detailed discussion of the potential biases. Section 3 introduces the modified task used in for this study. Section 4 summarizes the results and Section 5 concludes.",2
61.0,3.0,Journal of Risk and Uncertainty,06 January 2021,https://link.springer.com/article/10.1007/s11166-020-09340-7,The development of risk aversion and prudence in Chinese children and adolescents,December 2020,Timo Heinrich,Jason Shachat,,Male,Male,Unknown,Male,"Economic models of decision making in risky settings largely assume that a decision maker’s preferences are exogenous to the problem and exhibit a few key properties. Perhaps the most commonly asserted property is risk aversion, which in the expected utility framework implies a concave utility function. Increasingly researchers have identified a wide class of problems in which prudence (Kimball 1990) is also a key property of preferences. In the expected utility framework, prudence implies that marginal utility functions are concave.Footnote 1 We investigated when and how these two key traits—risk aversion and prudence—emerge and are shaped during human development. We did this by experimentally testing for the presence of these traits in 362 Chinese children and adolescents aged 8 to 17 years and then examining the correlation of these results with same tests for their parents—collected as hypotheticals in a survey—as well as with cognitive abilities and household attributes. The direct measurement of higher-order risk preferences, such as prudence, has been sparked by the lottery-based and model-free definition by Eeckhoudt and Schlesinger (2006; see also Eeckhoudt and Schlesinger 2013 for a comprehensive review). For an individual with initial positive wealth W, they define risk aversion and prudence (as well as higher-order risk preferences more generally) in terms of preferences over pairs of lotteries. Each lottery has two potential outcomes x and y and is denoted by [x; y]. Eeckhoudt and Schlesinger (2006) define risk aversion as weakly preferring a lottery [W-k1; W-k2] over a lottery [W-k1-k2; W], where k1 > 0 and k2 > 0 are sure losses. They define prudence as weakly preferring a lottery [W-k1; W + ε] over a lottery [W-k1+ ε; W], where ε is a zero-mean risk (i.e., a lottery with an expected value of zero). While risk-averse individuals like to disaggregate two sure losses, prudent individuals like to disaggregate a sure loss and an additional zero-mean risk. In other words, risk aversion corresponds to a preference for a lower spread in payoffs and prudence to a preference for facing additional risk in better states of the world (prudence is therefore sometimes also called downside risk aversion). Based on these definitions, risk aversion and prudence have been measured in a series of papers (see Trautmann and van de Kuilen (2018) for a survey). Most of these papers elicit risk preferences in the gain domain using choices between lottery pairs (Ebert and Wiesen 2011; Noussair et al. 2014; Deck and Schlesinger 2014; Breaban et al. 2016; Haering et al. 2020; Ebert and van de Kuilen 2017; Baillon et al. 2018). Some alternatively elicit risk and prudence premia using a multiple price list (Ebert and Wiesen 2014; Heinrich and Mayrhofer 2018) or elicit preferences in the loss domain (Maier and Rüger 2012; Bleichrodt and van Bruggen 2018). With respect to the loss domain the evidence is ambiguous, but with respect to the gain domain most studies find a majority of choices to be risk averse and prudent for binary choices, as well as for a multiple price list.Footnote 2 For the sake of comparability with the majority of studies, we opt to elicit risk preferences in the gain domain using binary lottery choices. When comparing risk aversion across age groups, the evidence from previous experiments does not show a clear age effect. In an initial study, Harbaugh et al. (2002) analyze risk aversion of children and adolescents aged 5 to 20 as well as that of adults aged 21 to 64. They observe no correlation between age and a preference for gambles over certain amounts of equal expected value. Levin et al. (2007) compare the risky choices made by 9- to 11-year-old children to the choices the same children made three years earlier. They find a significant within-subject correlation but also no age effects for risk aversion. Furthermore, Sutter et al. (2013) do not observe age effects in their study with children and adolescents ranging from 10 to 18 years. Khachatryan et al. (2015) find a gender-dependent age effect. They study risk aversion of boys and girls in two age groups (7 to 12 years and 12 to 16 years). In their sample, risk aversion of boys decreases with age, while girls’ risk aversion stays constant.Footnote 3 Several survey-based investigations have found positive correlation between the risk aversion of adult children and that of their parents (see, e.g., Kimball et al. 2009, Dohmen et al. 2011, Necker and Voskort 2014), providing evidence for the intergenerational transmission of risk aversion. In addition, experimental studies that correlate decisions in incentivized tasks observe correlations between children’s and their parents’ risk aversion: Levin and Hart (2003) find a significant correlation between the risk aversion of 6- to 8-year-old children and that of their parents (79% of them mothers). However, this correlation is insignificant in their follow-up study with the same subjects three years later, as reported in Levin et al. (2007). Alan et al. (2017) study risk aversion in 7- to 8-year-old children and their mothers. They find that the risk aversion of girls (but not of boys) correlates with their mothers’ risk aversion. There is evidence that a considerable part of variability of decision making under uncertainty is determined genetically, as documented, for example, in twin studies on risk aversion (Cesarini et al. 2009; Zhong et al. 2009; Zyphur et al. 2009) and on financial investments (Cesarini et al. 2010; Barnea et al. 2010). Heritability may explain the transmission of risk preferences from parents to their children. However, the environment also appears to be an important driver of risk-taking. A recent study by Black et al. (2017) on adoptees finds that the portfolio risk adoptees take on is more strongly correlated with that of their adoptive than their biological parents (see also Fagereng et al. 2018). In addition, a number of studies suggest that risk preferences are influenced by the general characteristics of the household within which children grow up. Deckers et al. (2019) observe that 7- to 9-year-old children who grow up in households of parents with low income or low education are less risk averse than other children. Falk and Kosse (2016) interpret breastfeeding duration as a measure of quality of the early childhood environment. In a sample of preschool children aged 5.9 years on average, they find that shorter breastfeeding is associated with lower risk aversion (and lower levels of patience and altruism). In this study, we connect the emerging literature on higher-order risk preferences with previous work on the development and transmission of risk aversion. We measure risk aversion and prudence in two primary schools, one middle school and one high school in the mainland Chinese sub-provincial city of Xiamen. For this purpose, we developed a simple preference elicitation task suitable for young children. We ran all experiments during the usual class time and are thus able to rule out self-selection in the experiment. We also used a survey to collect the stated risk preferences of parents and household information. Furthermore, we obtained additional information from the school records. Our results with respect to the preferences of children and adolescents reveal that subjects from grades 5 to 11 (10 to 17 years) make mostly risk-averse and prudent choices. With respect to risk aversion, behavior of 3rd graders (8 to 9 years) does not differ statistically from risk neutrality. We also find 3rd graders to make mostly prudent choices; however, this effect is driven by one of the two primary schools we sampled from. We also find evidence for a transmission of preferences: children’s risk aversion is significantly correlated with stated preferences of their parents. Also, prudence of girls (but not boys) is significantly correlated with stated preferences of their parents. In Section 2, we will describe our elicitation method and process of data collection. In Section 3, we present the results. After describing the summary statistics, we first focus on the basic question of whether children and adolescents are risk averse and prudent. We then present our results on the transmission of risk preferences and consider the development of risk preferences with age. In Section 4, we present estimates of a structural model of stochastic mean-variance-skewness expected utility to explore whether grade variation in behavior is driven by changing deterministic utility parameters or shrinking decision errors. Section 5 concludes with a discussion of our findings.",5
62.0,1.0,Journal of Risk and Uncertainty,31 July 2021,https://link.springer.com/article/10.1007/s11166-021-09346-9,Prince: An improved method for measuring incentivized preferences,February 2021,Cathleen Johnson,Aurélien Baillon,Peter P. Wakker,Female,Male,Male,Mix,,
62.0,1.0,Journal of Risk and Uncertainty,17 February 2021,https://link.springer.com/article/10.1007/s11166-021-09344-x,The modest effects of fact boxes on cancer screening,February 2021,Michael R. Eber,Cass R. Sunstein,Jennifer M. Yeh,Male,,Female,Mix,,
62.0,1.0,Journal of Risk and Uncertainty,17 June 2021,https://link.springer.com/article/10.1007/s11166-021-09347-8,On the validity of the estimates of the VSL from contingent valuation: Evidence from the Czech Republic,February 2021,Anna Alberini,Milan Ščasný,,Female,Male,Unknown,Mix,,
62.0,1.0,Journal of Risk and Uncertainty,01 May 2021,https://link.springer.com/article/10.1007/s11166-021-09345-w,Risk Taking with Left- and Right-Skewed Lotteries*,February 2021,Douadia Bougherara,Lana Friesen,Céline Nauges,Unknown,Female,Female,Female,"While much literature has focused on preferences regarding risk, preferences over skewness also have important economic implications. Skewness seeking, for example, can explain the overpricing and less than average returns of (right-) skewed securities (Barberis 2013), overinvestment in winner-take-all careers and the high rates of small business failures, the attraction of lotteries (Garrett and Sobel 1999), and even the well-known “favorite – long shot bias” where people overprice long shots and underprice favorites (Golec and Tamarkin 1998). An important and understudied aspect of skewness preferences is how they affect risk taking. That is, are people more or less willing to take risk when facing more skewed outcomes? Answering this question is important for many economic decisions. For example, people may purchase less insurance as skewness increases if they are more willing to take risks, and new crop varieties that reduce downside risk (i.e. technologies that reduce the probability of crop failure) may affect farmers’ willingness to adopt new technologies or to buy insurance.Footnote 1 To study how the skewness of outcomes affects risk taking we design a novel laboratory experiment that elicits certainty equivalents over lotteries where the variance and skewness of the outcomes are orthogonal to each other. Our design enables us to cleanly measure both skewness seeking/avoiding and risk taking behavior, and their interaction, without needing to make parametric assumptions. An important part of our design is the inclusion of both right- and left-skewed lotteries, with the latter less commonly studied. However, left skewness is a feature in many important economic decisions such as financial markets, agricultural production, insurance risks, health outcomes, and employment incomes. In these situations, there is a small likelihood of very unfavorable outcomes such as negative profits, unemployment, and serious illness. As Barberis (2013, p.182) describes, while some individual securities are right-skewed, “the aggregate stock market is negatively skewed: it is subject to occasional large crashes”. Increasing global connectivity magnifies these potential financial risks as exemplified by the recent global stock market crash associated with the coronavirus (COVID-19) pandemic. Similarly, the threat of climate changes brings an increased likelihood of extremely bad (catastrophic even) outcomes (e.g. Hanemann et al. 2016). The most common pattern we observe in our experiment is skewness avoidance and risk taking, which is particularly prevalent when considering lotteries with the same direction of skewness (i.e. all left- or all right-skewed). Correspondingly, subjects also take more risk when facing less skewed lotteries. Nevertheless, we observe considerable heterogeneity in behavior. Our second novel contribution is to link these behaviors to individual structural risk preference parameters estimated using a separate lottery choice task. This allows us to investigate the role of individual heterogeneity, particularly utility curvature and probability weighting, in a manner that previous studies have only hinted at. For this purpose, we use the Harrison and Rutström (2009) protocol, which is specifically designed to measure individual risk preference parameters. For each subject, we estimate their utility curvature and probability weighting parameters in the rank-dependent utility (RDU) model (Quiggin 1982). We find two classes of subjects: skewness avoiders who have an inverse s-shaped probability weighting function, and skewness neutral subjects that do not have an inverse s-shaped probability weighting function.Footnote 2 Our results are the first to demonstrate the relationship between skewness seeking/avoiding and probability distortion at the individual level.Footnote 3 Several other studies investigate how skewness affects risk taking behavior. Grossman and Eckel (2015) use a variation of their Eckel and Grossman (2002, 2008) risk elicitation task and find that when choosing among options with greater skewness, subjects tend to choose riskier options than they did when facing options with lower skewness. In their experiment, subjects choose from six lotteries with the same skewness (and kurtosis) but different expected values or variances. Subjects make (up to) three lottery choices with skewness increasing from zero to two positive levels. However, when controlling for the largest gain in the lottery, their results reverse with subjects taking less risky choices as skewness increases. While Grossman and Eckel (2015) note this is consistent with overweighting the long shot, their experiment is not designed to provide evidence of probability weighting.Footnote 4 Astebro et al. (2015) use a variation of the Holt and Laury (2002) risk elicitation task, modified for different levels of (right) skewness. They find that greater skewness leads to greater risk taking among both students and executives, and with low and high incentives. Using the same choices to estimate average preference parameters for their samples, they rule out risk loving as an explanation but provide support for optimism and likelihood insensitivity. In a different experimental setting using binary lotteries, Ebert (2015) finds that with a symmetric risk, subjects are mostly risk averse but with a right-skewed risk they are mostly risk loving. Thus, similar to these other studies, he finds that risk taking increases with greater skewness. The result in Dertwinkel-Kalt and Köster (2020) is similar, with subjects in their Experiment 1 more willing to choose the lottery over the safe option yielding the lottery’s expected value as the skewness of the lottery increases. In contrast to these studies, we structurally estimate individual risk preference parameters using a standard protocol and relate these to decisions over skewness and risk observed in a separate experimental task. Our results demonstrate the important relationship between probability distortion and skewness seeking/avoiding, as well as revealing considerable individual heterogeneity. In addition to measuring individual risk preference parameters, our experiment also differs by using mixed lotteries that are both left- and right-skewed, whereas the three studies described above use only right-skewed lotteries over gains.Footnote 5 Finally, our design allows us to study not only how skewness affects risk taking but also how the risk of lotteries affects observed skewness-related behavior.Footnote 6 Our paper is also related to the “favorite – long shot bias”; a robust finding in horse betting that long shots (right-skewed lotteries) are overpriced, leading to lower returns on average than favorites. While earlier explanations were that bettors were risk lovers, Golec and Tamarkin (1998) provide evidence consistent with skewness seeking and risk aversion rather than risk loving preferences. In later work, both Jullien and Salanié (2000) and Snowberg and Wolfers (2010) found that models allowing for bettor misperceptions fit the racetrack data better than risk loving preferences do. Importantly, both of these papers rely on a representative agent model and so do not study individual heterogeneity as we do. Further, controlled laboratory experiments can properly isolate factors in a way that is not possible with naturally occurring data where the variance and skewness of bet returns are correlated. Nevertheless, an implication from the long-shot bias is that bettors’ preference for skewness is sufficiently strong to overcome aversion to risk and lower expected returns, which reinforces the importance of studying the interaction between risk and skewness. It is important to note that we study skewness seeking/avoiding behavior rather than prudence. Consistent with Ebert and Wiesen (2011), we define skewness seeking as preferring a lottery with a larger skewness over another lottery with a smaller skewness but the same expected value, variance, and kurtosis.Footnote 7 In contrast, prudence is a stricter feature of preferences, implying skewness seeking behavior that is robust to different levels of kurtosis. Ebert and Wiesen (2011) find evidence of prudence, with most prudent subjects also being skewness seeking but not necessarily vice versa.Footnote 8 In our experiment, we hold kurtosis constant and use the terminology “skewness seeking” to mean subjects prefer a lottery with larger skewness to one with smaller skewness and exactly the same mean, variance, and kurtosis. Similarly, we study “risk taking” rather than risk preference, with the former referring to preferences over changes in standard deviation holding the other moments constant.Footnote 9 As mentioned earlier, most studies include only right-skewed lotteries. Exceptions include Ebert and Wiesen (2011) who study eight pairs of binary lotteries that have the same expected value, variance, (and kurtosis, by definition because they are binary lotteries) and absolute skewness, but one is right-skewed and the other left-skewed. They find significant evidence of skewness seeking (defined here as choosing the right-skewed lottery) with 77% of choices in this direction. Ebert (2015) includes both left- and right-skewed lotteries in his experiment involving binary lottery choices, as well as symmetric (i.e. zero-skew) lotteries. He finds that subjects care about differences in the direction of the skewness but less about the magnitude of skewness, finding no evidence of skewness seeking when comparing two right-skewed lotteries. Ebert (2015, p. 86) finds evidence for skewness preferences, “that individuals both like right-skew and dislike left-skew, and we do not find that one is more important than the other”.Footnote 10 In a striking contrast, Symmonds et al. (2011), in a neuroeconomics study, consider lotteries with both left and right skewness and finds that preference for left skewness is actually more prevalent than skewness seeking. The design is very different in this study, which involves complicated lotteries with between three to nine outcomes.Footnote 11 More generally, these contrasting findings suggest that it is important to study skewness seeking/avoiding behavior across different domains including both left and right skewness.",3
62.0,2.0,Journal of Risk and Uncertainty,20 July 2021,https://link.springer.com/article/10.1007/s11166-021-09350-z,Altruism and efficient allocations in three-generation households,April 2021,Anna Bartczak,Wiktor Budziński,Jytte Seested Nielsen,Female,Male,Female,Mix,,
62.0,2.0,Journal of Risk and Uncertainty,09 July 2021,https://link.springer.com/article/10.1007/s11166-021-09349-6,Simple belief elicitation: An experimental evaluation,April 2021,Karl Schlag,James Tremewan,,Male,Male,Unknown,Male,"Experimental economists are increasingly recognising the value of directly eliciting the beliefs of their subjects. For example, direct measures of a subject’s beliefs can help us disentangle whether deviations from homo economicus behaviour is due to social preferences or bounded rationality, test whether belief updating is Bayesian, and learn about whether peer effects are caused by imitation or information transmission. There are, by now, a large variety of incentive compatible methods for eliciting beliefs, with various strengths and weaknesses.Footnote 1 Some of the most commonly used are not incentive compatible for risk-averse subjects, (e.g. linear or quadratic scoring rules) while those that do not suffer from this flaw tend to be either time-intensive, (e.g. calibrating elicited beliefs; Offerman et al., 2009) or challenging for subjects to fully understand (e.g., variations of the Becker-DeGroot-Marschak mechanism; Karni, 2009). Here we present a procedure for eliciting beliefs about probabilities that is robust to risk-aversion, requires minimal labtime, and is simple for subjects to understand. We refer to this procedure as the “frequency” method. In this paper we lay out the theoretical properties of the frequency method and demonstrate its practical and empirical properties in a laboratory experiment. On the theory side we establish robustness to risk attitudes and point out the inferences that can be made from subjects’ reports. On the practical and empirical side we showcase the benefits of this method by comparing its performance in a laboratory experiment to that of the elicitation method of Karni (2009). In particular we evaluate ease of implementation, understanding of subjects, and reasonableness of elicited reports. We emphasize that we do not conduct a “horse race” designed to determine the “best” method for eliciting beliefs. Instead, our objective is to highlight the properties of the frequency method by comparing it to the popular Karni mechanism. The Karni mechanism is likely to be viewed by experimenters as a leading contender for use in their own experiments, and is therefore an appropriate comparison for us. Using the frequency method, subjects report better understanding of the belief elicitation task and complete it in shorter time. There are fewer reports of the focal probability of 0.5, which in the Karni method is correlated with low cognitive ability.Footnote 2 The two methods do not differ in terms of the proportion of subjects best-responding to their stated beliefs, or average distance from the empirical probability. However, the frequency method results in more correct answers in a Bayesian updating task. Most of the literature on belief elicitation focuses on payments based on the actual outcome of a single event. However, in many laboratory experiments, there will be not just one but many independent realisations of the random variable of interest. Take, for example, a one-shot prisoners’ dilemma experiment where the experimenter is interested in beliefs the subjects hold about the probability of defection. If there are 20 subjects per session, each stated belief can be matched with the 19 realizations of the decisions of others. The two methods we discuss in this paper, the first for eliciting probabilities, the second for quantiles, take advantage of these multiple realisations. In doing so we remove the need to refer in experimental instructions to numerical probabilities of single events, which many subjects may have difficulty understanding (see Section 6 for evidence of this). In the frequency method, the subject is asked to guess the empirical frequency of each outcome. A prize is then awarded if and only if their guess coincides with the realized frequencies. For the case of only two outcomes, this method has been used before (Wilcox & Feltovich, 2000; Bhatt & Camerer, 2005; Hurley & Shogren, 2005; Costa-Gomes & Weizsacker, 2008; Blanco et al., 2010; Le Coq et al., 2015), however its properties do not appear to have been well understood by the experimental community. Wilcox and Feltovich (2000) and Blanco et al. (2010) state only that beliefs about the modal frequency of outcomes are elicited, while Costa-Gomes and Weizsacker (2008) say that it is valid only when the true subjective probability coincides exactly with one of the possible empirical distributions. For the special case of binary outcomes, a correct interpretation was reported in Hurley and Shogren (2005) but not given much prominence in the paper, and as a result appears to have been largely overlooked.Footnote 3 Not only does the frequency method elicit beliefs about modal frequencies, but we also show that it enables the researcher to identify a region in which the belief of the subject should lie. Inference does not require postulating any assumptions on the utility function beyond assuming that the subject strictly prefers getting the prize to not getting it. This method reveals regions of beliefs for events that have an arbitrary number of possible outcomes. With binary events this region is an interval of width 1/(n + 1), where n is the number of realizations of the variable in question. For example, for n = 19, such as in the prisoners’ dilemma example given above, the size of the interval is 5%. In this case, given that subjects tend to answer questions about percentages in multiples of five (Manski, 2004), there is no practical loss of precision. For binary events we show that this method is most precise in a well defined sense. We also show that this method can be used to estimate bounds on subjects’ beliefs about means and variances of distributions. The frequency method stands out for the simplicity of its implementation. We demonstrate its practical and empirical properties in the laboratory using the elicitation method of Karni as a benchmark. We choose the Stag Hunt game as a simple environment in which we can evaluate the relationship between elicited beliefs and actions. Questions are added to compare subjects’ understanding of the two elicitation methods. In an Urn task we compare elicited beliefs to an objectively true probability. We use the Cognitive Reflection Test to provide additional insights into differences in the cognitive requirements of each method. We also test for the first time a novel and related method, using multiple realisations of a random variable to elicit beliefs about quantiles of a distribution. As with the frequency method for eliciting probabilistic beliefs, it is extremely straightforward to explain to subjects, and is equally valid for all non-trivial utility functions. However, in contrast to the probability elicitation method, we find it performs poorly in terms of the internal consistency of the elicited beliefs. The paper proceeds as follows: Section 2 describes the theory underlying the frequency method; Section 3 describes our experimental design and Section 4 provides the results; Section 5 gives a brief outline of the quantile elicitation method and an overview of its performance; in Section 6 we discuss the implications of our findings and conclude.",5
62.0,2.0,Journal of Risk and Uncertainty,31 July 2021,https://link.springer.com/article/10.1007/s11166-021-09351-y,Learning under uncertainty with multiple priors: experimental investigation,April 2021,James R. Bland,Yaroslav Rosokha,,Male,Male,Unknown,Male,"Learning from a signal when the initial information is uncertain is critical for economic success. For example, in an innovation context, firms must decide whether to continue with an R&D project depending on research results; in a consumer choice context, people must decide whether to buy a product, based on the product review; in a retail context, managers must decide which assortment of products to offer depending on the product sales. However, in most situations, a decision maker does not fully understand, has little information about, or considers multiple theories about the process generating the signal. In such cases, it is common practice to model the environment as uncertain and use Bayes’ rule as a way for the agent to learn from a signal. Little is known, however, regarding the learning process under uncertainty with unknown probabilities (henceforth ambiguity). In particular, is it different from the learning process under uncertainty with known probabilities (henceforth compound risk)? And, how well does Bayes’ rule capture the learning process under ambiguity? The difference between ambiguity and risk was first noted by Knight (1921). Later, using a thought experiment, Ellsberg (1961) showed that behavior under ambiguity cannot be explained by the subjective expected utility theory of Savage (1954). Recent experimental studies show that there is substantial heterogeneity in attitudes towards compound risk and/or ambiguity at the individual level (Halevy, 2007; Stahl, 2014; Abdellaoui et al., 2015; Harrison et al., 2015). In addition, a number of studies find an association between attitudes towards the compound risk and ambiguity (Halevy, 2007; Abdellaoui et al., 2015; Dean and Ortoleva, 2015; Prokosheva, 2016; Qiu & Weitzel, 2016; Chew et al., 2017). While the above studies focused on decision-making in static environments, there is scarce evidence on any such relationship about learning under compound risk and ambiguity. In this paper, we present an experiment designed to compare the learning process under compound risk and under ambiguity at the individual level. In our experiment, there are two types of urns composed of black and white marbles. Compound risk urns are constructed by randomly drawing from a set of urns with known composition. Thus, the subjects are provided with the objective prior about the probability that a black (or white) marble could be drawn. Ambiguous urns are constructed so that subjects do not know the exact composition of the urn, but know the total number of marbles, which is kept the same as in the compound case. In other words, subjects are not provided with enough information to form an objective prior. In the experiment, each subject faces decisions regarding both types of urn. The questions that we address in this paper deal with the priors considered by the subjects and the process by which those priors are updated. In particular, the following questions are of interest: Are beliefs consistent with the urn composition process? Are there behavioral differences between learning under the compound risk and ambiguity? Can a multiple priors approach explain the learning behavior under ambiguity and/or compound risk? To answer these questions, we use a mixture model to estimate the proportion of subjects that learn according to Bayes’ rule and the proportion of subjects that learn according to a more general, multiple-priors model of Epstein and Schneider (2007). The multiple-priors model fits within a stream of literature that uses maximum likelihood as a way to discriminate among priors after a signal has been observed (Gilboa & Schmeidler, 1993). In particular, agents consider multiple priors about the signal generating process, and upon realization of the signal agents evaluate which of the priors were “likely” to generate the signal. Then only these “likely” priors are updated according to Bayes’ rule and considered as decision relevant. Importantly, the model can be applied to the compound risk environment – in which case only the single objective prior will be in the set. We find that the majority (60%) of subjects are Bayesian both under compound risk and ambiguity. We also find a substantial fraction (25%) of subjects who are Bayesian under compound risk but not under ambiguity. The challenge in considering the multiple priors model is that the set of possible priors is infinite. We estimate two models with different assumptions on the type of priors subjects may use. The first model assumes that subjects’ priors are over the possible urns that could be generating the signals (i.e. priors over the number of black vs. white marbles in the urn). We refer to these priors as Simplex priors, because they take the form of an element of the 3-dimensional simplex. The second model assumes that subjects’ priors take on a Beta distribution over the probability that a black marble is drawn. This second class of priors was recently used by Moreno and Rosokha (2016) as part of a behavioral model of belief updating. We find that under the assumption of Simplex priors participants’ behavior is in line with the multiple-priors model under both compound risk and ambiguity. At the same time, under the assumption of Beta priors the behavior is more in line with subjects being Bayesian. Our model selection result, however, provides overwhelming evidence in favor of the Beta priors. In particular, while the Simplex priors correspond to the possible urn compositions (and are consistent with the information provided about the uncertain process), they impose implicit restrictions on the strength of priors and the range of the beliefs that subjects could hold. These restrictions prove to be too limiting in describing human belief formation and learning processes as compared to a set of more general Beta priors. Our work contributes to the literature that investigates learning under compound risk and/or ambiguity. In particular, there exists a large body of literature in economics and psychology with focus on learning under compound risk. The conclusions in this literature vary. For example, in a seminal article, Kahneman and Tversky (1973) present evidence that individuals over-value new information relative to Bayes’ rule (a judgment bias known as representativeness). At the same time, other studies (e.g., Buser et al., 2018; Coutts, 2019) find that subjects under-value new information relative to Bayes’ rule (a judgment bias known as conservatism), or that most behavior is well described by Bayes’ rule (e.g., El-Gamal & Grether, 1995). A smaller stream of literature has focused on learning under ambiguity (Cohen et al., 2000; Dominiak et al., 2012; Baillon et al., 2013; Qiu & Weitzel, 2013; Ert & Trautmann, 2014; Moreno & Rosokha, 2016). In this literature, the most closely related study to the current paper is Moreno and Rosokha (2016) who develop a behavioral model of belief updating and then estimate their model at the aggregate level. The authors find that learning under compound risk is consistent with Bayes’ rule, while the learning process under ambiguity is consistent with over-weighting of the new signal. In the current paper we differ in several important ways: First, we use a within-subject design which allows us to address learning by the same individual in the two environments. Second, we consider a multiple-priors model of learning developed for ambiguous environments, rather than using a reinforcement type behavioral model. Third, we investigate two different specifications of subjective priors. Finally, we estimate a mixture model of different types allowing for individual level heterogeneity in preference, learning, and precision parameters. The rest of the paper is organized as follows. In Section 2, we describe the experimental design and elicitation procedure and present an overview of the data. In Section 3, we present the learning model and estimation procedure used. In Section 4, we present and discuss our main results. Finally, in Section 5, we conclude.",3
62.0,2.0,Journal of Risk and Uncertainty,23 July 2021,https://link.springer.com/article/10.1007/s11166-021-09352-x,Efficient Institutions and Effective Deterrence: On Timing and Uncertainty of Formal Sanctions,April 2021,Johannes Buckenmaier,Eugen Dimant,Ulrich Schmidt,Male,Male,Male,Male,"Governments use substantial resources to keep society safe and punish criminal activities. Annually, mass incarceration costs amount to approximately $182 billion in the United States (Wagner & Rabuy, 2017). The economic tradition to understand deviance and deterrence has its origins in the seminal work by Becker (1968), which stresses the importance of severity and certainty of punishments.Footnote 1 More recently, these concepts have been studied in the domain of uncertainty and ambiguity (see, e.g., DeAngelo & Charness 2012; for a recent review of economic research, see Chalfin & McCrary 2017, and for a cross-disciplinary discussion of experimental work, see Engel 2016). Understanding the deterrence mechanisms of deviant behavior yields important policy implications. However, a particular institutional aspect that is implicitly factored in by Becker’s decision framework – the swiftness of punishment – has been under-researched in the economics literature. This concept is often referred to as celerity (see Bailey 1980; Howe & Brandau 1988; Yu 1994; Nagin & Pogarsky 2001; Nagin & Pogarsky2004).Footnote 2 Historically, this concept has its scholarly roots in the writings of Jeremy Bentham and Cesare Beccaria, and represents a fundamental component in deterrence theory that has been referred to as the ‘neglected middle child of the deterrence family’ (Pratt & Turanovic, 2018). However, despite theoretical advancements (see, e.g., Nagin 2013), little empirical work has been conducted regarding the celerity of punishment using observational data since there are very few environments where nearly instantaneous punishment or resolution occurs. For this reason, we capitalize on a stylized experimental setting in which we can systematically vary the structure of celerity. The goal of this paper is to experimentally study the role of timing and uncertainty of formal punishment in deterring deviant behavior - aspects that have received surprisingly little attention in this area of research. Specifically, we are interested in how the timing of formal sanctions (be it conviction or sentencing) and the timing of the resolution of uncertainties surrounding these sanctioning mechanisms affect deterrence. We systematically vary the swiftness of a sanction within a new, stylized, experimental paradigm along the following two dimensions: first, we vary the delay between offense and detection; second, we vary the delay between offense and sanctioning. We also study the combination thereof. Our main objective is to test the behavioral assumption that swiftness matters, which we derived from existing theories (e.g., Loewenstein 1987; Frederick et al., 2002), and to advance our understanding of how to leverage swiftness and the dread of uncertainty as an effective third approach to deter deviance. We argue that swiftness can serve as a useful tool for policy makers to design more efficient and/or potentially also less expensive institutional deterrence mechanisms. Naturally, the concepts of decision-making under risk and uncertainty are not limited to the domain of criminal behavior. Most recently, this has been particularly apparent in the context of the COVID-19 pandemic where individual actions (e.g., to wear masks, to social distance, to obtain a vaccination) are guided by the same principles (Viscusi, 2020). The insights of celerity also find broader application beyond criminal behavior, such as finance and consumption (Palacios-Huerta, 1999; Chesson & Viscusi, 2003). For example, Kreps and Porteus (1978) and Kocher et al. (2014) show that preferences over temporal lotteries also depend on the point in time when the uncertainty is resolved: agents can show a preference for earlier or delayed resolution of uncertainty. Further evidence comes from consumer literature. Anticipatory emotions, compared with outcome-based emotions, are central in prospective consumption situations and the uncertainty associated with anticipatory emotions negatively affects intentions (Bee and Madrigal, 2013). Given the high costs involved in increasing punishment’s certainty (e.g. costs for an executive body) or punishment’s severity (e.g. incarceration costs), we argue that the timing of punishment as well as the timing at which individuals are informed about the consequences, that is, their delay with respect to the transgression in question, can potentially serve as a powerful tool for deterrence. Our findings advocate for changes in institutional structures to increase the effectiveness of deterrence (Bigoni et al., 2015). More generally, from an aggregate perspective, the speed of justice matters because it facilitates economic development and market efficiency (North, 1991; Djankov et al., 2008; Ponticelli & Alencar, 2016). Recent empirical results evaluating the impact of legal reform to increase judiciary adjudication in Senegal find an increase in procedural efficiency without any adverse effects on quality (Kondylis & Stein, 2018). From an individual perspective, existing research in various disciplines indicates that timing affects the association formed in one’s mind between the deviant act and the ultimate punishment: without proper swiftness, sanctions risk losing their bite, regardless of how certain or severe they are Chalfin and McCrary (2017) and Pratt and Turanovic (2018) – a fact already prominently argued a long time ago (Watson, 1924). In reality, the closest we can get to achieving maximal swiftness (celerity) of punishment is by catching deviants in the act and punishing them right away. For example, during the FIFA World Cup 2010 in South Africa, the local government agreed to establish 56 so-called ‘World Cup Courts’ across the country, assigning 1,500 dedicated personnel including magistrates, prosecutors, and public defenders. This was done to achieve speedy justice, in some cases leading to convictions on the same day.Footnote 3 Recently, the concept of celerity has entered the correctional arena through the project HOPE (Hawaii Opportunity Probation with Enforcement) as a new model for probation. “In 2004, First Circuit Judge Steven Alm launched a pilot program to reduce probation violations by drug offenders and others at high risk of recidivism. This high-intensity supervision program, called HOPE Probation [...], is the first and only of its kind in the nation. Probationers in HOPE Probation receive swift, predictable, and immediate sanctions – typically resulting in several days in jail – for each detected violation, such as detected drug use or missed appointments with a probation officer” (Alm, 2014). In a first pilot, the project was found to reduce drug use, crime, and incarceration. Simultaneously, it saved the government approximately $6,000 per participant per year through reduced incarceration (Hawken & Kleiman, 2009).Footnote 4 The existing line of research that has acknowledged the relevance of celerity has often resorted to observational studies with mixed approaches and insights. However, as also recognized by this stream of literature, the common absence of reliable observational data and the ability to account for potentially confounding influences (i.e., perceived or actual certainty or severity of punishment) render the study of celerity methodologically challenging (for a critical discussion, see Pratt & Turanovic 2018). A natural starting point to look for a clean effect is the laboratory environment in which institutional constraints are absent and where we can precisely control the incentives and relevant timings of resolution and punishment (Charness & DeAngelo, 2018). This is in the spirit of the recent surge of experimental economists studying related topics, such as corruption or tax evasion (e.g., Abbink 2018 ; Serra 2011) in controlled laboratory environments. This allows us to control the important elements of celerity: anticipation and revelation of information, timing and severity of punishment, and the opportunity to recidivate. Our experimental analysis is based on a novel cheating game where subjects may cheat in periodic instances to increase payoffs. These cheating periods are followed by an investigation such that cheaters will be detected and fined with a given probability. Across different treatments, we systematically vary the timing of both the resolution of uncertainty about whether one will be punished and actually being punished. We analyze behavior alongside two dimensions: total propensity to cheat and recidivism (cheating conditional on having cheated at least once before). Our results show that delayed resolution of uncertainty has no direct systematic impact on behavior. With respect to the relation between the delay of punishment and deterrence, we observe a u-shaped relationship where deterrence is lowest for delayed punishment combined with no delay in resolution of uncertainty and for delayed resolution combined with no delay of punishment, whereas it is significantly higher for either no delay or a delay combined with delayed resolution of uncertainty. This result is at odds with discounted expected utility and theories of anticipatory utility. From a policy perspective, our results suggest that to improve deterrence mechanisms, punishment should either be swift or delayed and paired with the psychological dread of uncertainty. Section 2 details our experimental design, procedures, and briefly discusses the existing literature on swiftness and deterrence. Results are presented and discussed in Section 3. We conclude in Section 4.",6
62.0,3.0,Journal of Risk and Uncertainty,05 August 2021,https://link.springer.com/article/10.1007/s11166-021-09354-9,Experimental evidence on the effect of incentives and domain in risk aversion and discounting tasks,June 2021,Emmanouil Mentzakis,Jana Sadeh,,Male,Female,Unknown,Mix,,
62.0,3.0,Journal of Risk and Uncertainty,30 September 2021,https://link.springer.com/article/10.1007/s11166-021-09362-9,Stochastic superiority,June 2021,Liqun Liu,Jack Meyer,,Unknown,Male,Unknown,Male,"When choosing between risks a decision maker is often at the same time selecting an overall risk level. Choosing between two risky mutual funds is combined with deciding how much wealth to invest in the selected mutual fund and how much to hold in an insured savings account instead. Buying a car or house is combined with choosing how much auto or homeowner’s insurance to purchase. When selecting a television or washing machine, extending the manufacturer’s warranty is often an option. These secondary decisions are made simultaneously with the primary decision and are ways that decision makers can use to reduce the riskiness of their initial selection. The point being made with these examples is that when a decision maker is choosing between two risky alternatives (the primary decision), the decision of how much of the chosen risk to bear and how much to transfer to others is often made simultaneously. This secondary decision is relevant and important precisely because it is made concurrently with the primary decision. The decision maker’s attitude toward risk affects both decisions, and this interaction can alter the choice between risky alternatives.Footnote 1 This paper explicitly models a simultaneous secondary decision in the stochastic dominance context. Sets of decision makers are asked to rank or choose between two random variables \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\) knowing that a portion of the risk they choose can be transferred to others. The analysis extends the literature on stochastic dominance that was started more than 50 years ago by Hadar and Russell (1969) and Hanoch and Levy (1969). The basic stochastic dominance question they pose is: when is random variable \(\stackrel{{\sim }}{\text{x}}\) preferred or indifferent to random variable \(\stackrel{{\sim }}{\text{y}}\) by all decision makers with a utility function in some set U? This same question is addressed here with the additional feature that both \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\) can be altered by choosing an optimal risk reducing change or transformation. This additional decision is made simultaneously with choosing between \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\), and can lead to a different selection. Because of this extra decision, individual decision makers may rank \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\) differently than without it, and therefore the answer to the basic stochastic dominance question may also be different.Footnote 2 Because the answer to the basic stochastic dominance question can be different when \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\) can be transformed, a new term is required to discuss the partial order that is generated. The term used is “stochastically superior” or “stochastic superiority.” Random variable \(\stackrel{{\sim }}{\text{x}}\) is said to be stochastically superior to \(\stackrel{{\sim }}{\text{y}}\) for a set of decision makers U if every decision maker in U weakly prefers the optimally transformed \(\stackrel{{\sim }}{\text{x}}\) to the optimally transformed \(\stackrel{{\sim }}{\text{y}}\). The transformation applied to \(\stackrel{{\sim }}{\text{y}}\) can be different from that applied to \(\stackrel{{\sim }}{\text{x}}\), and these transformations can and likely do differ across decision makers. Each decision maker in U optimally selects a risk reducing transformation for \(\stackrel{{\sim }}{\text{x}}\) and for \(\stackrel{{\sim }}{\text{y}}\) at the same time he or she is choosing between \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\). All decisions are assumed to be made to maximize the decision maker’s expected utility. When decision makers can each optimally choose how to transform both \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\), the question of which of the two random variables is unanimously preferred to the other by sets of decision makers could well have fewer rather than more clear-cut answers; that is, there could be fewer rather than more unanimous rankings of \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\). The additional complication of a simultaneous risk level decision could make it more difficult for sets of decision makers to agree as to whether \(\stackrel{{\sim }}{\text{x}}\) or \(\stackrel{{\sim }}{\text{y}}\) is best. A main finding here, however, is that for the simultaneous risk level decision considered in this analysis, \(\stackrel{{\sim }}{\text{x}}\) stochastically dominating \(\stackrel{{\sim }}{\text{y}}\) implies, but is not implied by, \(\stackrel{{\sim }}{\text{x}}\) being stochastically superior to \(\stackrel{{\sim }}{\text{y}}\). That is, for sets of decision makers, the inclusion of this simultaneous risk level decision makes unanimity for the primary decision of ranking \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\) more rather than less likely. That stochastic superiority is implied by stochastic dominance is presented as Corollary 1 to Theorem 1 in the paper. Stochastic superiority allows random variables to be ranked when stochastic dominance does not, and therefore leads to smaller efficient sets. As the title of the Hanoch and Levy (1969) paper indicates, reducing sets of random variables to efficient sets, ones containing only elements that at least one decision maker would choose, is one of the main goals of stochastic dominance analysis. This is true not only for first- and second-degree stochastic dominance as defined by Hadar and Russell (1969) and Hanoch and Levy (1969), but also for general nth-degree stochastic dominance and many other forms of stochastic dominance presented in the literature since then. Each new definition refines a previously established form of stochastic dominance in a way that leads to additional rankings and smaller efficient sets. Leshno and Levy (2002), when defining almost stochastic dominance, are also concerned with obtaining additional rankings of random variables. They define a procedure that can be applied to any of the established forms of stochastic dominance such as first-, second- or nth- degree stochastic dominance. Almost stochastic dominance modifies an existing form of stochastic dominance by removing from the associated set of utility functions those utility functions that represent preferences that Leshno and Levy consider to be “extreme, pathological or simply unrealistic.”Footnote 3 By removing these extreme preferences and utility functions, unanimous rankings are obtained that these extreme preferences would otherwise prevent.Footnote 4 The almost stochastic dominance procedure follows a long-established pattern. All stochastic dominance definitions, including those generated using almost stochastic dominance, propose a new form of stochastic dominance that allows more random variables to be ranked and accomplishes this by removing from consideration some of the utility functions previously allowed. This pattern was established at the very beginning by Hadar and Russell and Hanoch and Levy when they defined second-degree stochastic dominance by removing from consideration all increasing utility functions that are not concave. By removing these utility functions from consideration, second-degree stochastic dominance can rank more random variables and generate smaller efficient sets than first-degree stochastic dominance. Like almost stochastic dominance, stochastic superiority is a modification procedure that can be applied to any established form of stochastic dominance and the goal is additional rankings and smaller efficient sets. Stochastic superiority obtains these additional rankings, however, not by removing utility functions, but by allowing decision makers to modify the random variables that are being compared. Stochastic superiority facilitates seemingly reasonable rankings not by discarding “extreme, pathological or simply unrealistic” preferences, but instead by allowing all decision makers, including those whose preferences seem to be extreme, to optimally reduce the risk associated with any random variable. Allowing this additional simultaneous risk reduction decision is often enough to cause the ranking of \(\stackrel{{\sim }}{\text{x}}\) and \(\stackrel{{\sim }}{\text{y}}\) by those with extreme preferences to be aligned with the ranking by decision makers whose preferences are not so extreme. Stochastic superiority and almost stochastic dominance have the same goal, to rank more random variables and generate smaller efficient sets, but the way in which this goal is accomplished is very different. Consider the following example provided by Leshno and Levy (2002). Assume that two random alternatives are available, \(\stackrel{{\sim }}{\text{x}}\) which yields either $0 or $1,000,000 with probabilities 0.01 and 0.99 respectively, and \(\stackrel{{\sim }}{\text{y}}\) which is $1 with certainty. Leshno and Levy point out that although neither \(\stackrel{{\sim }}{\text{x}}\) nor \(\stackrel{{\sim }}{\text{y}}\) dominates the other in the first-degree,Footnote 5 except for very extreme preferences, \(\stackrel{{\sim }}{\text{x}}\) would be chosen over \(\stackrel{{\sim }}{\text{y}}\) by those who prefer more to less. They provide an example of such extreme risk preferences represented by the utility function u(x) = x for x ≤ 1 and u(x) = 1 for x > 1. Leshno and Levy argue that to ensure that \(\stackrel{{\sim }}{\text{x}}\) is preferred to \(\stackrel{{\sim }}{\text{y}}\) by all decision makers, utility functions as extreme as this u(x) must be excluded. The almost stochastic dominance modification procedure excludes these utility functions and thereby ensures that all decision makers unanimously prefer \(\stackrel{{\sim }}{\text{x}}\) to \(\stackrel{{\sim }}{\text{y}}\). To see how stochastic superiority deals with this same example, notice that random variable \(\stackrel{{\sim }}{\text{x}}\) can be interpreted as asset whose value is $1,000,000 but subject to total loss with probability 0.01. Expected loss is then $10,000. Suppose coinsurance is available at a very high price of $100,000, which equals a loading factor of 9. When the chosen share of loss that is reimbursed is θ in [0, 1], the required insurance premium would be $100,000 (θ), and random wealth \(\stackrel{{\sim }}{\text{x}}\) becomes. $1,000,000 (θ)–100,000 (θ), with probability 0.01 $1,000,000–100,000 (θ), with probability 0.99 This transformed \(\stackrel{{\sim }}{\text{x}}\) stochastically dominates $1 in the first-degree whenever ($1,000,000–100,000) (θ) ≥ $1 or θ ≥ 1/900,000. This example shows that every individual who prefers more to less, including the one whose utility function is u(x) given above, would prefer \(\stackrel{{\sim }}{\text{x}}\) to \(\stackrel{{\sim }}{\text{y}}\) when a coinsurance arrangement with a loading factor even as high as 9 is available. After \(\stackrel{{\sim }}{\text{x}}\) has been chosen, individual decision makers will choose a coinsurance rate θ in [0, 1] to maximize their own expected utility. Many would choose not to insure at all, but those with extreme preferences including the example u(x), would choose to coinsure at a rate greater than 1/900,000. The paper is organized as follows. In the next section, the question of how to compare one set of random variables with another set for groups of decision makers, the set dominance issue, is discussed, and the notion of stochastic superiority is formally defined. Set dominance is relevant for the definition of stochastic superiority because allowing a risk reducing secondary decision associates a set of random variables with each original one, and as a result ranking a pair of random variables is transformed into ranking two sets of random variables. In Sect. 3, a simple and seemingly overly strong sufficient condition for stochastic superiority is presented. One important implication of this sufficient condition is that stochastic dominance implies stochastic superiority. In the same section, this same sufficient condition is demonstrated to be necessary for convex sets of concave utility functions. Corollaries are provided that state these general results for the specific cases of first, second and nth-degree stochastic superiority. The set of utility functions corresponding to each of these is a convex set of utility functions, and concavity of utility is assumed for all but first-degree stochastic superiority. An important extension to situations where the cost of risk reduction differs for different random variables is discussed in Sect. 4. This material is particularly relevant when insurance is used to reduce risk since the cost of insurance is typically dependent on the expected loss associated with the indemnification contract. Section 5 is the section with three example applications. The first two show precisely how stochastic superiority can be used to obtain a ranking of \(\stackrel{{\sim }}{\text{x}}\) over \(\stackrel{{\sim }}{\text{y}}\) without removing extreme preferences from the set of utility functions under consideration. The final application is an extensive analysis of the decision to self-protect using stochastic superiority as an analysis tool. A result is presented indicating when all risk averse decision makers choose a higher level of self-protection over a lower level. As those who have examined the self-protection literature know, such a result is impossible to obtain in the standard model without a secondary risk reduction decision. This last example adds significantly to the self-protection literature. Finally, the paper concludes and discusses extensions of this work in Sect. 6.",
62.0,3.0,Journal of Risk and Uncertainty,13 August 2021,https://link.springer.com/article/10.1007/s11166-021-09353-w,Adversity-hope hypothesis: Air pollution raises lottery demand in China,June 2021,Soo Hong Chew,Haoming Liu,Alberto Salvo,,Unknown,Male,Mix,,
62.0,3.0,Journal of Risk and Uncertainty,07 August 2021,https://link.springer.com/article/10.1007/s11166-021-09348-7,Justice in an uncertain world: Evidence on donations to cancer research,June 2021,Tigran Melkonyan,Zvi Safra,Sinong Ma,Male,Male,Unknown,Male,"What a society perceives as fair is at the center stage of public debates surrounding financial bailouts of companies and countries, healthcare policies, and welfare programs. Under a democratic system of governance, these perceptions and policies reflect attitudes of individual society members to different justice principles. Hence, assessment of these individual attitudes is of paramount importance for understanding the drivers of public policies. The present study makes an important step in this direction by using two data sets: a naturally-occurring data on charitable contributions to cancer research in the United Kingdom and data from a representative survey on individual preferences to donate to different cancers. In a non-deterministic world, justice principles can be differentiated according to the degree to which individuals are held responsible for their risky choices vis-a-vis their luck. At one end of this spectrum of justice principles is strict egalitarianism (Nielsen, 1985), which does not hold individuals responsible for any causes of inequality. According to the principle, social redistribution should be based solely on outcomes. At the opposite end of the spectrum is libertarianism (Nozick, 1974), which postulates that individuals should bear full responsibility for their circumstances even if they are caused by bad luck. Some theories of distributive justice combine egalitarian principles with concerns for individual responsibilityFootnote 1. One of the most notable among these is choice egalitarianism (Dworkin 1981a, 1981b; Arneson 1989, Cappelen et al., 2013) that holds people accountable for their risky choices but not for their luck. A related but different channel why decision-makers may favor punishing risky behavior is to mitigate moral hazard. To understand which of these justice principles people endorse, we use charitable contributions to research on cancer treatment and prevention made via an online platform recently developed by the Cancer Research United Kingdom (CRUK). The data collected via the platform offered a unique opportunity to distinguish the incentives to contribute to cancers where chance plays a central role, like hereditary cancers, and cancers where individual choices are more important, namely lifestyle-related cancers. The platform enables potential donors to choose a cancer research project for sponsorship as well as provides them with full flexibility over the donation amount. The list of available projects includes both hereditary and lifestyle-related cancers and this characteristic of research projects is explicitly stated for several of them. Moreover, donors can easily access information regarding each cancer’s prevention rate, which is the probability that the potential cancer victim can avoid the cancer in question by some choice. The naturally-occurring data has certain limitations. Most notable of these is lack of socio-demographic data (except for gender) and lifestyle of donors. Since donations to hereditary versus lifestyle-related cancers may be affected by these variables, such absence of data creates obstacles in unambiguously establishing causality relationships between a cancer’s prevention rate and donations to that cancer. The differences between the descriptions of the projects on the CRUK platform in terms of the degree cancers can be prevented might also not be entirely apparent to some donors. This may also hinder identifying a relationship between prevention rate and donations. To overcome these difficulties and to establish causality, we have created and implemented a survey that mimics key elements of the choice problem faced by actual donors who used CRUK’s online platform. To disentangle different incentives of donors in the empirical part of our paper, we develop a theoretical model of donation behavior for decision-makers who embrace one of different justice principles. Our theory yields a testable hypothesis that is examined in the empirical part of the paper: choice egalitarians’ donations decrease with prevention rates while non-choice egalitarians are insensitive to these changes. The empirical analysis of the naturally-occurring data to elicit attitudes to different justice principles (study 1), a first attempt of this kind, reveals that a non-negligible part of the donors, especially women, in our data set are choice egalitarian. This is supported by a similar pattern of preferences for data obtained from the online survey where respondents faced a series of hypothetical choice questions (study 2) and our companion paper (Safra et al., 2019). The observation that, overall, the donations are higher to hereditary than to lifestyle-related cancers has a different interpretation. As charitable donations may be viewed as contributions towards improved research and the risk involved in preventable cancers may seem lower to individuals who intend to behave responsibly, the observation implies that people are willing to pay more to reduce the risk of hereditary cancers. This is in line with an interesting connection reported in Viscusi et al. (2014), who show that people who view their own cancer risk as high (low) have a higher (lower) willingness to pay for cancer policies. The considerations of fair treatment of risk taking play a central role in a number of contexts. A notable illustration is offered by the functioning of a wide range of healthcare systems (Cappelen & Norheim, 2005). Two very recent examples are offered by policy changes in the operation of the National Health System (NHS) in the United Kingdom. In 2014, the NHS Blood and Transplant Service announced that it was changing its current policy by allowing people with severe drink-related liver diseases to be considered for liver transplants (The Guardian, 2014). Many questioned the appropriateness of this decision mentioning that individuals who are likely to have harmed their own health are not as deserving of treatment. The other example involves a policy that is tilted in the reverse direction. In 2016, hospital leaders in North Yorkshire, UK announced that overweight patients and smokers will be prohibited from most standard hip and knee surgeries for up to a year (The Telegraph, 2016). Redistribution of income and wealth is another important area where individual responsibility for poor choices frequently has a key role. Fong (2001) analyzed a Gallup poll and found that respondents care about justice. According to her findings, “A strong taste for equity or reciprocity is consistent with the basic concept of insuring industrious people against bad luck, but not providing unconditional assistance to the poor if their condition is due to idleness.” More recently, diverse justice views of how inadequate choices should influence redistributive policies reverberated loudly during the last global financial crisis. Many individuals and interest groups vehemently objected to using government resources to help the troubled financial institutions while others defended it on the basis that the alternative was even worse. A group of prominent economists wrote to Congress cautioning against a bailout of “particular investors and institutions whose choices proved unwise”, with justice being their primary concern (Wolfers, 2008). The experimental evidence suggests that a considerable fraction of laboratory subjects tend to accept inequalities reflecting differences in choice (Konow 2000; Frohlich et al., 2004; Cappelen et al., 2007, 2010, 2013; Krawczyk 2010). Cappelen et al. (2013) find that many subjects in their experimental study differentiate between inequalities arising from choice and inequalities arising from luck. Cettolin and Riedl (2017) also examine how people view justice under uncertainty. They conduct an experimental analysis of interactions between uninvolved participants who allocate resources to recipients. Cettolin and Riedl (2017) find that when one out of two recipients faces an uncertain outcome, allocations by uninvolved participants are widely dispersed but have a clear pattern: recipients exposed to higher degrees of uncertainty are allocated less. In our paper we investigate a different question. We are interested in how money is allocated among individuals who are all exposed to the same degree of uncertainty but, for some of them, the uncertainty resulted from their own actions. The laboratory experiments provide important insights into possible behavior in real-life situations and guidance for theoretical developments. However, the context and nature and magnitudes of different outcomes in real-life situations involving applications of different justice principles differ drastically from the environments considered in laboratory experiments. Hence, the potential for generalizability of the predictions obtained from laboratory experiments to a wide range of domains hinges upon a combination of theory and empirical evidence from naturally occurring environments (Winkler & Murphy, 1973; Harrison & List, 2004). We make one of the first steps in this direction in the context of understanding individuals’ attitudes to different distributive justice principles. In addition to providing a positive reconciliation of the findings regarding these attitudes from the laboratory and naturally-occurring environments, we offer a number of additional insights about factors that affect charitable contributions. Thus, the paper also contributes to a burgeoning literature on charitable giving (see, e.g., Auten et al., 2002; Landry et al., 2010) and provides guidance for structuring fund-raising activities. In Section 2, we develop and analyze a model of charitable contributions that yields new insights and a testable hypothesis about the relationship between cancer prevention rate and donations. In Section 3, we develop the empirical model to analyze the naturally-occurring data (study 1) and report its results. In addition to the result mentioned above, we find that provision of information on hereditary causes of a cancer in a project description has a statistically significant positive effect on donations. In contrast, information on lifestyle-related causes negatively affects contributions. These findings suggest that a significant share of the donors tends to favor choice egalitarianism. The estimation results also reveal a considerable amount of “in-gender favoritism” with each gender donating significantly more to cancers that affect only their own gender. There is also considerable difference in how different genders react to the inclusion of information about cancer causes in the project descriptions. Section 4 reports the results of an empirical analysis of the online survey (study 2). These findings provide significant support for the analysis and results of study 1 as they show that donations are higher to the hereditary cancer than to the lifestyle-related cancer. Moreover, we asked individuals to provide a rationale behind their choices and learned that a large share of the respondents revealed choice-egalitarianism as the sole reason for their decision. Similarly to study 1, the respondents in study 2 who had personal or family history with the disease would donate more to it. The final section of the paper concludes and outlines avenues for future research.",
63.0,1.0,Journal of Risk and Uncertainty,06 October 2021,https://link.springer.com/article/10.1007/s11166-021-09355-8,Optimality of winner-take-all contests: the role of attitudes toward risk,August 2021,Liqun Liu,Nicolas Treich,,Unknown,Male,Unknown,Male,"An important question in contest design is whether the winner-take-all arrangement provides a larger incentive for players to expend effort than the alternative arrangements in which rewards are spread out to more players. It has been established in the literature that, from the perspective of a contest organizer with a fixed prize budget, it is optimal to award the entire prize money to the winner and nothing to the others in order to maximize the total effort. However, this result seems inconsistent with the evidence that multiple prizes are often observed in real-life contests. Moreover, the optimality of the winner-take-all contest exhibited in the literature has been typically obtained under the assumption that the contestants are risk neutral.Footnote 1 This is an important limitation since contests are often viewed as risky activities, where an increase in effort permits to increase the probability of the favorable outcome (e.g., in activities such as lobbying, patent races, or sport competitions). In addition, existing empirical/experimental evidence suggests that individual decision makers tend to be both risk averse and prudent.Footnote 2 This paper studies the role of risk attitudes—risk aversion and prudence in particular—in determining the optimality of winner-take-all contests. We compare the winner-take-all contest model of Konrad and Schlesinger (1997) and Treich (2010)—hereafter the KST model—that uses a general contest success function and a general utility function with two types of contests that help spread the rewards to more players. Both multiple-prize models considered here can be looked upon as a generalization of the KST model, and as adding some insurance motive to the winner-take-all design from the perspective of the contestants. In the “multiple-competition contest,” contestants make one-shot efforts but have multiple shots at winning prizes. Examples of contestants who make efforts that are simultaneously aimed at multiple prize-yielding competitions abound. Athletes in various sports undergo rigorous winter/summer training to get ready for a new season of competitions.Footnote 3 New movies produced in a year can compete for various movie awards. Research work of a scientific lab/team can be submitted to many academic/government/industrial organizations for award considerations. For all these examples, contestants make efforts with an eye on multiple competitions, the prizes of which are determined by the same effort inputs in a statistically independent fashion. Assuming that the competitions are statistically independent, it is easy to see that increasing the number of competitions while keeping the total prize budget unchanged helps allocate rewards to more players, and thus has an insurance value to the players. To the best of our knowledge, the multiple-competition contest introduced here has not been formally analyzed in the literature on contests.Footnote 4 In the “contest with a bottom prize,” the single winner is awarded a top prize and each loser is also awarded a bottom prize (or, say, a consolation prize). Shrinking the prize gap while keeping the total prize budget fixed no doubt facilitates a more equal distribution of rewards among players. Although the contest with a bottom prize was already discussed in the ground-breaking work of Lazear and Rosen (1981), it has not been fully analyzed in the literature, probably because it is intuitively appealing to conclude that setting the bottom prize to zero (i.e., making the prize gap as large as possible) would induce the most effort from the players.Footnote 5 As will be shown in this paper, however, shrinking the prize gap in the contest with a bottom prize does not necessarily reduce effort when it comes to risk-averse and prudent players.Footnote 6 The main findings of the paper are the following. First, in the comparison between the multiple-competition contest and the winner-take-all contest, we find that the former is as effective as the latter when the contestants are risk neutral, but the former induces more effort than the latter when the contestants are both risk averse and prudent. Second, when the number of competitions becomes infinitely large in the multiple-competition contest, the model converges to that of a single-competition contest where the contest success functions are interpreted as contestants’ deterministic shares of the total prize instead of their probabilities of winning the prize.Footnote 7 Further, for risk-averse and prudent players, the share contest induces a larger amount of effort than the multiple-competition contest with any number of competitions. This, together with the first finding above, implies that, for risk-averse and prudent players, the share contest induces a larger amount of effort than the corresponding winner-take-all lottery contest. Third, in the comparison between the contest with a bottom prize and the winner-take-all contest, we find that the former is always dominated by the latter when the contestants are risk neutral, but the former could have an advantage over the latter when the contestants are both risk averse and prudent, and it is more likely so as the contestants become more prudent. This finding is consistent with Fu et al. (2019) who investigate the effort effect of multiple prizes in contests with risk-averse players utilizing the nested lottery procedure of Clark and Riis (1996) to allocate the multiple prizes.Footnote 8 Whether the winner-take-all arrangement provides a larger incentive for players to make effort than the alternative arrangements has been an important question in the literature on contest design. Our paper thus sheds new light on this question by studying the roles of risk attitudes (i.e., risk aversion and prudence). It is part of a broader research agenda on optimal contest design given various possible contestants’ preferences (Drugov & Ryvkin, 2021; Schroyen & Treich, 2016). Following a large strand of the contest literature, the goal of contest design in our paper is assumed to be maximizing effort. This objective might be natural in some contexts such as sport events, where effort may be correlated with performance and show quality, and in turn with the revenue of the contest organizer. In other contexts, such as the initial rent-seeking application (Tullock, 1980), the objective may be instead to minimize (unproductive or wasteful) efforts; our result thus also shed some light on those situations. But there exist a wide range of other possible objectives. An alternative objective would be for instance to maximize welfare or efficiency, as in Lazear and Rosen (1981). The quest for the optimal contest design with an eye on welfare when the contestants are risk averse and prudent may be a fruitful avenue for future research. Another interesting research direction would be to consider that participants can choose the optimal dynamic effort in a multi-round competition (and can observe outcomes in intermediate rounds), as in Tsetlin et al. (2004). The paper is organized as follows. The next section presents the basic “winner-take-all” contest model of KST, and discusses the effects of risk and risk aversion on the equilibrium effort level in this model. Section 3 introduces multiple competitions into the basic model so that players can have multiple shots at winning prizes based on the same one-shot effort inputs. Three results concerning the effects of multiple competitions—in which risk attitudes play a critical role—are established in this section. Section 4 introduces a bottom prize into the basic model. Two results concerning the effects of having a larger bottom prize—again with an emphasis on the role of the attitudes toward risk—are obtained. Section 5 concludes with a summary of the findings in the paper.",2
63.0,1.0,Journal of Risk and Uncertainty,08 October 2021,https://link.springer.com/article/10.1007/s11166-021-09356-7,Quantifying loss aversion: Evidence from a UK population survey,August 2021,David Blake,Edmund Cannon,Douglas Wright,Male,Male,Male,Male,"The simplest canonical models in economics assume that agents have identical preferences and that they maximise the expected value of a concave utility function. In this paper, we use survey data to contribute to the literature both on heterogeneous agents and on more sophisticated models of human behaviour. Our data are collected from a survey of 4016 respondents who form a representative sample of individuals in the United Kingdom and we are able to correlate loss aversion and risk attitude with a rich set of demographic and socio-economic variables, as well as self-reported character traits such as optimism and competitiveness. Using the standard parametric model of loss aversion first proposed by Kahneman and Tversky (1979), we show that responses are consistent with loss aversion, but that attitudes to risk in both the gain and loss domains are significantly correlated with reported characteristics of the respondents. We contrast our results with those of comparable studies which are frequently based on the analysis of university students and show that such students are unrepresentative of the population as a whole. For at least fifty years, economists have been aware that the expected utility (EU) model might not fully capture consumer behaviour under risk (Allais 1953; Rabin and Thaler, 2001; Samuelson 1963) and this has led to a range of more general models being proposed. Simply put, the EU model assumes that the objective function depends on two components: first, the value (or utility, loosely defined) of a state depends upon the consumption or wealth in that state without regard to how it was reached; second, when considering more than one possible outcome, the different states of the world are weighted by the subjective probability of each state occurring. In the specific example of the EU model, the objective function is:
 where \(u\left({c}_{i}\right)\) is a standard increasing and concave utility function depending upon consumption (or wealth) in each state \(i\) and \({p}_{i}\) is the associated probability of that state occurring. Kahneman and Tversky (1979) suggested changing both of these components so that the objective function becomes:
 replacing the standard utility function with a more general value function and weighting the outcomes not by the probabilities but by a function of the probabilities. In this paper, we confine our analysis to the standard model proposed by Tversky and Kahneman (1992) and hence use a widely accepted framework to compare the loss and risk attitudes of different respondents. In particular, we use the CRRA (constant relative risk aversion or iso-elastic) form of the value function which depends upon gains and losses, \(x\), relative to the initial position:
 where \(\lambda\) measures “direct” loss aversion, defined as the ratio of the value of a loss of one unit of currency to the value of a gain of one unit of currency. The parameter \(\alpha\) measures risk attitude in the domain of gains. There is risk aversion in the domain of gains if \(\alpha <1\) and this is higher for lower values of \(\alpha\); there is risk seeking in the domain of gains if \(\alpha >1\). The parameter \(\beta\) measures risk attitude in the domain of losses. There is risk seeking in the domain of losses if \(\beta <1\) and this is higher if \(\beta\) is lower; there is risk aversion in the domain of losses if \(\beta >1\). There is risk neutrality in the relevant domain when these parameters take a value of unity. In this paper, we estimate the value function, but we do not attempt to model the more sophisticated treatment of probabilities embodied in Eq. (2), i.e., to estimate the weighting function \(w\left({p}_{i}\right)\). There are two reasons for this. First, existing studies show that the effect of the weighting function is most important when probabilities are close to either zero or unity. Abdellaoui et al. (2008) find w(0.5) = 0.46 in the gain domain and w(0.5) = 0.45 in the loss domain, suggesting we can assume \(w\left({p}_{i}\right)={p}_{i}\) without a serious reduction in accuracy. Bleichrodt et al. (2001) suggest that using probabilities of one-third might reduce bias in parameter estimates, but we chose to use a probability of one half in our survey questions, because the 50:50 scenario is likely to involve the smallest cognitive load for the respondents. Second, there are significant trade-offs that need to be made when calibrating a utility or value function using real-world data. Studies of behaviour in response to loss and risk are usually based on questionnaires of a relatively small number of homogeneous individuals who are typically students of the authors of those studies. For example, Harrison and Swarthout (2016, Table 2) list papers testing or estimating models of loss aversion and the last ten of these references analyse a total of twelve data sets, nine of which are based on students with a sample size ranging from 30 to 177 respondents. The three exceptions are Scholten and Read’s (2014) Yale University data set of 569 online respondents (many of whom may also have been students), Abdellaoui et al.’s (2013) analysis of 65 couples and von Gaudecker et al.’s (2011) survey of a representative sample of 1422 individuals from the Netherlands. Our analysis is closest to that of von Gaudecker et al. (2011) who also choose to ignore the probability weighting issue. There is, however, an important difference in the value functions in the loss domain between their study and ours. In the loss domain, we find that the value function is convex as a function of \(x\), whereas von Gaudecker et al. (2011), using the utility function of Kreps and Porteus (1978), assume that disutility is concave. The advantage of a data set involving students is that the respondents are usually willing (indeed required) to answer a sufficiently large number of questions—often about a hundred—to identify relatively complicated functional forms of both the value function and the weighting function; furthermore, the financial cost of recruiting students is relatively low. The corresponding disadvantage is that the study only reveals information on student-aged individuals selected for university education and whose understanding of risk may be conditioned by what they have already been taught (since they are often Economics, Finance or MBA students). Since estimated utility and value functions might be used to analyse the savings behaviour of poorly-educated individuals or the decumulation behaviour of pensioners, using estimates of risk or loss aversion from such studies may be inappropriate. Our data set is for a representative sample of the UK adult population and contains a large number of variables describing the economic, social, political and personal characteristics of the respondents. The trade-off from having access to such a rich data set is that we were unable to ask a large number of questions because the agency conducting the survey was concerned that if the experiment was too onerous it might put off respondents from completing it. The average time spent by respondents on the questions we asked was 29 min, shorter than the time reported in many experimental studies, which is typically between 40 min and 1 h. We were able to ask sufficient questions to identify the value function but not the weighting function. To give a flavour of the issues that we consider, we summarise some of our findings in Figs. 1 and 2. Figure 1 illustrates our estimate of the value function for our whole sample, ignoring the heterogeneity of respondents: this figure is based entirely on our estimates of \(\alpha\), \(\beta\) and \(\lambda\) in Eq. (3). These estimated values provide evidence for three stylised facts: first, the S-shaped value function posited by Kahneman and Tversky (1979) where the value function is concave in the gain domain and convex in the loss domain (whereas with EU the value function would be concave in the loss domain); second, the older insight (which can be traced back to Samuelson 1963) that the disutility of losses is greater than the utility of gains, commonly known as “loss aversion”; and third, that the value function is less convex in the loss domain than it is concave in the gain domain, i.e., \(\beta >\alpha\), implying that the marginal disutility of losses exceeds the marginal utility of gains.
 The estimated value function for the full sample of respondents Relative loss aversion with a gain or loss of 500, \(\Lambda \left(500\right),\) across gender and age. Note The figure shows the expected value of \(\Lambda \left(500\right)\) and the associated 90% confidence interval. The numbers in this figure come from Table A4 in Online Appendix 2. The graph would have a similar shape if we plotted \(\Lambda \left(x\right)\) for other values of \(x\) We show that these three qualitative findings hold not only for the whole sample but for any sub-sample of the data: for example, they hold for both men and women, at any age, for any level of income, for any level of education, although quantitatively the value functions vary considerably for each subgroup. We now turn to the issue of quantifying loss aversion more precisely, since there is more than one possible definition.Footnote 1 A popular measure of “relative loss aversion” proposed by Köbberling and Wakker (2005) is:
 which measures whether there is a “kink” in the value function at the origin. The hypothetical sums of money considered in our survey (with a minimum monetary value of 10 units of currency) are too large to allow us to analyse with any confidence what is happening for very small values of \(x\) close to zero and so it would be inappropriate for us to use this definition. For larger values of \(x\), possibly the most helpful definition is that of Zank (2010), who notes that the weighting function in the loss domain \({w}^{-}\left(p\right)\) may differ from the weighting function in the gain domain \({w}^{+}\left(p\right),\) suggesting the definition of loss aversion:
 However, as we have already explained, the restrictions placed on our data collection mean that we shall be unable to identify the weighting function(s) and so we use instead the more standard definition originating from Tversky and Kahneman (1992):
 which depends not only on the size of direct loss aversion, \(\lambda\), but, in general, also on the sizes of \(\alpha\), \(\beta\) and \(x\). Only in the cases of \(\alpha =\beta\) or \(x=1\) will \(\Lambda \left(x\right)\) equal \(\lambda\). Figure 2 shows our point estimates and 90% confidence intervals for \(\Lambda \left(500\right)\), estimated separately for our data broken down by gender and into six age groups. There is a strong U-shaped relationship between loss aversion and age; there is also evidence that women have slightly higher (unconditional) loss aversion than men at most ages. It is notable that loss aversion is highest among individuals in the age range 18–24, precisely the age group most likely to be analysed by studies based on university students. While the associations are very strong, we do not claim that they imply a causal relationship since we have not controlled for other factors. To address the association of loss aversion with reported characteristics, we now turn to a detailed description of our study. In Sect. 2, we describe the survey design, elicitation method and sample of respondents. Our results are described in Sect. 3 and Sect. 4 concludes. We also have five online appendices.",6
63.0,1.0,Journal of Risk and Uncertainty,14 August 2021,https://link.springer.com/article/10.1007/s11166-021-09357-6,When risky decisions generate externalities,August 2021,Angela C. M. de Oliveira,,,Female,Unknown,Unknown,Female,"
Much of the existing literature in economics regarding decision-making under risk and uncertainty assumes that an individual bears the consequences of their risky decisions but that there are no spillovers or externalities imposed upon others. For many risky decisions of interest to economists, like own portfolio choice, this is an accurate portrayal. However, other risky decisions generate either positive externalities (e.g., starting a new business, investing in education, vaccinating, or engaging in research and development) or negative externalities (e.g., smoking, reckless driving, unprotected sex). I develop a new experimental framework for evaluating the impact of external costs and benefits on an individual’s risk tolerance. For concreteness, consider the decision to smoke. A ‘rational’ individual deciding to smoke for the first time would weigh the perceived value of the activity with the costs and risk of addiction and adverse health consequences (e.g., Becker & Murphy, 1988).Footnote 1 While this calculus does include the smoker’s value and costs, it ignores the negative externalities associated with smoking, such as health costs to others from second-hand smoke (Max, 2001; Uccello, 2006) and any other disutility of being exposed to the smoke (Rovira et al., 2000). The negative externality creates a difference between the private and social cost, resulting in more of the risky behavior than would be socially optimal. One might then consider this a socially-undesirable risk from the social planner’s point-of-view (Avery et al., 1995). Likewise, risky decisions can generate positive externalities. Consider one of the classic positive externality examples in economics: opening a donut shop in a community. Starting this small business benefits nearby residents through increased police presence in the area and decreased crime. Starting a small business is certainly risky, with only 66% of firms surviving to their second birthday (Knaup, 2005). More generally, small business creation also results in less-direct positive externalities: encouraging more entrepreneurship via network externalities (Minniti, 2005) and also through knowledge spillovers (Acs et al., 2009). While the entrepreneur considers their risk as well as their own costs and benefits, the positive spillovers to the community are not typically modeled as part of their calculus. These externalities create a divergence between the individual and social benefit, resulting in too little of the positive-externality generating activity. This risk might be considered socially-desirable risk because it positively impacts the community. These examples illustrate a more general issue: sometimes externalities are generated as a byproduct of an individual’s risky decision. In this paper, I consider situations that are typically addressed as risky individual decision making problems, such as decisions related to health, education and entrepreneurship. I develop a framework to examine a key aspect of these decision environments: sometimes a risky decision also affects others. In this manuscript, I focus on the simplest case: situations where an individual’s own risky decision generates an externality with certainty. While choices in these environments do affect others through the externality, the traditional theoretical approach posits that individuals only consider their own costs and benefits. This contrasts with a substantial literature indicates that individuals can be other-regarding; motivated by altruism or warm glow (e.g., Andreoni, 1990), efficiency (e.g., Charness & Rabin, 2002; Engelmann & Strobel, 2004). Debate remains regarding whether, and under what conditions, social preferences extend to situations involving risk. Risk exposure for recipients has been shown to reduce giving (Cettolin et al., 2017; Exley, 2015). On a broader scale is the study of environmental risk in social dilemmas. Increasing the riskiness of the decision environment can harm cooperation (e.g., Botelho et al., 2014), but this effect depends on the type of uncertainty and can be reversed in some situations (e.g., Aflaki, 2013; Zhang, 2019).Footnote 2 Their question differs from the one raised in this paper. When making contributions to individuals or to public goods, the contribution is the choice variable. This potentially enhances or changes the impact of social norms and social preferences on behavior. In my framework, individuals make a risky allocation for themselves, which then generates an externality that affects others. A second strand of prior literature examines social aspects of risk by examining whether people make the same risky allocations for others as they do for themselves (e.g., Brennan et al., 2008; Chakravarty et al., 2011; Charness & Jackson, 2009; Humphrey & Renner, 2011; Pahlke et al., 2015; Sutter, 2009) or the impact of social distance on risky decisions for others (Montinari & Rancan, 2018). Unfortunately, these studies do not fully clarify on whether social preferences extend to risky environments in part because the findings are mixed and in part because it is unclear what it means to be ‘generous’ in most of these decision environments: Is the safe choice generous or is the expected value maximizing choice generous? In this paper, ‘generous’ behavior is clear: increasing risky allocations with positive externalities and decreasing risky allocations with negative externalities. Another approach to examining social aspects of risk addresses whether the risks faced by others affect individual risky decisions. For example, Rohde and Rohde (2011) find that individuals make risky choices that exhibit social preferences when their own well-being is not affected but that individual risky choices are generally unaffected by the risks others will have to face. With social information, however, there is significant evidence of peer and group decisions affecting individual decisions (e.g., Lahno & Serra-Garcia, 2015; Linde & Sonnemans, 2012; Viscusi et al., 2011). Li et al. (2017) further find that making predictions for others’ risky choices improved own decision-making in the loss domain. In this paper, rather than examining the impact of others choices on the decision maker, I investigate the impact of knowing that you will directly help or harm others on your own risky choices. In observational data, the analysis of risky decisions that generate externalities is further complicated by data limitations: it is difficult to separate the individual’s subjective value for an activity from the impact of risk preferences on decision-making. Additionally, positive and negative externalities occur in different decision environments, making comparisons of the responsiveness to the externalities across these decision environments difficult. I therefore address the questions of whether and when individuals consider externalities when making risky decisions by taking advantage of the control available with laboratory experiments. I develop a framework for studying risky decisions that generate externalities by modifying an allocation task (Gneezy & Potters, 1997, hereafter GP97). In their task, individuals allocate an endowment between a safe and risky option. I create a ‘risk with externalities’ framework (RwE) where subjects are first paired anonymously and then make a series of risky decisions without feedback. For each experimental dollar a subject allocates to the risky option, an external cost or benefit is imposed on someone else. An individual’s earnings are thus impacted by their own allocation, the payoff from the risk, and (sometimes) the externality imposed upon them by others. All subjects have this information. In addition to this baseline decision environment, I design treatments (described in more detail below) that carefully vary whether individuals impose or receive externalities as well as the role of intent. Combined, the treatments allow an analysis of how the same individuals respond to both positive and negative externalities—both the direction and the marginal external effect. This type of comparison is not possible without contextual confounds outside of the lab. My design also allows for an examination of social preference considerations in a risky environment when the ‘generous’ action is clearly interpretable, an improvement over the existing literature. More specifically, I analyze the deviations from each individual’s risky decision without externalities (their GP97 decision), which, for clarity, I will refer to as their ‘baseline risk preference.’Footnote 3 In the symmetric environment, where individuals both impose and receive externalities, risk taking is lower when a negative externality exists: The reduction in risk taking is over 15 experimental dollars (out of a maximum 100), which is both economically and statistically significant. Risk taking is further significantly impacted by the marginal external effect, with an increase in risk taking over the baseline risk preference by approximately 8 experimental dollars for each 1-unit increase in the external effect for the positive externality, and vice versa for the negative externality. To separate social from non-social motivations, two additional treatments are conducted. In the first of these, the Asymmetric Risk with Externalities treatment, one member of each pair is randomly selected to impose externalities when they make their risky allocation decisions, and so the only way the externality could impact their decision through some sort of social motivation. In this treatment, the other member receives the externalities, and thus their reaction can possibly be attributed to receiving the costs and benefits as well as the perceived intent attributed to those costs and benefits. To cleanly isolate the non-social response to receiving externalities, the No Intent treatment matches participants receiving externalities with a previous player who was imposing externalities on someone else. The subjects in the No Intent treatment then are responding to the expected costs and benefits of the externalities without intent directed at them. For both types of players in the asymmetric case, risk taking is lower when a negative externality exists. Results indicate that the 15 experimental dollar reduction when imposing harm found in the Risks with Externalities treatment can be roughly equally attributed those who impose externalities, which is consistent with a disutility from imposing harm, and those who expect to have to pay the marginal external costs, which is consistent with decreasing relative risk aversion (DRRA). Apart from the reduction in risk taking when imposing any harm, individuals in the asymmetric environment who impose externalities are not sensitive to the magnitude of the marginal external effect in either the positive or negative externality environment. For the individuals who are receiving the external costs and benefits, the impact of externalities depends on whether the intent is directed at them. In the No Intent treatment, individuals exhibit a purely financial response consistent with a traditional self-interested economic agent and DRRA: changes in risk taking behavior are only driven by changes in the marginal external cost or benefit. However, when intent can be attributed with the externalities received, risk taking significantly increases with the marginal external benefit but does not significantly decrease with the marginal external cost. Rather, there is a level reduction in risk-taking when they are in a decision environment where they can receive negative externalities but there is not statistically significant reaction to the degree of harm. This suggests that the underlying decision process for those receiving externalities depends on the attribution of intent. Combined, these results suggest that risky decisions are being driven by a combination of social and non-social factors. In part, there is a disutility associated with imposing or receiving intentional negative externalities. Additionally, the potential external costs and benefits impact risk taking through wealth in a manner consistent with DRRA.",3
63.0,1.0,Journal of Risk and Uncertainty,27 October 2021,https://link.springer.com/article/10.1007/s11166-021-09358-5,Effortful Bayesian updating: A pupil-dilation study,August 2021,Carlos Alós-Ferrer,Alexander Jaudas,Alexander Ritschel,Male,Male,Male,Male,"Incentives are one of the most important drivers of economic behavior. Higher incentives should lead to better performance since larger outcomes offset the additional costs of “thinking harder.” However, the relation between incentives and performance is complex and far from straightforward (Jenkins et al., 1998; Camerer and Hogarth 1999; Gneezy et al., 2011). Known difficulties and paradoxes include ceiling effects (Kahneman et al., 1968; Samuelson and Bazerman 1985), choking under pressure (Baumeister 1984; Baumeister and Showers 1986; Ariely et al., 2009), and crowding out of intrinsic motivation through extrinsic incentives (Deci et al., 1999; Gneezy and Rustichini 2000). These and other phenomena, however, reflect problems at different steps in the assumed chain of implications connecting incentives to performance. The general assumption is that higher incentives increase effort, and increased effort results in higher performance. Thus, whenever performance fails to react to increased incentives, it is unclear which of the two links might have broken down. Did incentives fail to influence effort, or did effort fail to boost performance? Economic policies and interventions designed to improve performance will have to contend with different issues in each case. While effort might be directly observable for tasks requiring physical labor, this is generally not the case for the type of cognitive and analytical, high-skill tasks typical of a knowledge-based economy. In those tasks, actual (cognitive) effort cannot be directly measured, and hence it becomes impossible to distinguish breakdowns of the link from incentives to effort from breakdowns of the link from effort to performance. In this work, we consider a simple, well-established belief-updating task which is representative of this category and has been shown to elicit a large number of errors and be relatively impervious to monetary incentives in the past (Charness and Levin 2005; Charness et al., 2007; Achtziger and Alós-Ferrer 2014; Achtziger et al., 2015; Hügelschäfer and Achtziger 2017; Alós-Ferrer et al., 2017; Li et al., 2019). This task is especially interesting because the information participants receive includes a win-loss component. This captures a characteristic of many economic applications. Projects fail or succeed, firms make profits or losses, and stocks go up or down. This win-loss feedback cues basic reinforcement behavior (“win-stay, lose-shift”) which would not make sense if the win-loss information was absent, and gives rise to well-known phenomena as outcome bias or the focus on past performance (e.g. Baron and Hershey 1988). In the task we focus on, it has been shown that the high error rates originate precisely on the activation of reinforcement behavior due to the win-loss feedback (Charnes and Levin 2005; Achtziger and Alós-Ferrer 2014; Achtziger et al., 2015). We are interested in understanding why incentives do not improve performance in this particular task. Thus, we set out to investigate the origin of this failure in this setting. To solve the problem of the unobservability of cognitive effort in this task, we focus on a type of measurement which goes beyond the type of data usually employed in economics: pupil dilation. It is well-established (see, e.g., Beatty and Lucero-Wagoner 2000, for an overview) that the human eye’s pupil dilates reliably with the amount of mental effort exerted in a task. For instance, a number of early studies (Hess and Polt 1964; Kahneman and Beatty 1966; Kahneman et al., 1968; Kahneman and Peavler 1969) showed that pupil size correlated with the difficulty level in cognitive tasks as multiplication, number and word memorization, or mentally adding one to each digit in a previously-memorized sequence. Eye tracking measurements are relatively common in psychology and neuroscience, but have only recently gained popularity in economics. However, most of those target gaze and fixations patterns to study search patterns or processes of information acquisition (e.g. Knoepfle et al., 2009; Reutskaja et al., 2011; Hu et al. 2013; Polonio et al., 2015; Devetag et al., 2016; Alós-Ferrer et al., 2021), and not pupil dilation. An exception is Wang et al. (2010), who (in addition to fixation patterns) examined pupil dilation in sender-receiver games and found larger pupil dilation when deceiving messages were sent. Pupil dilation can be seen as a neural correlate of decision making, since pupil diameter correlates with activity in brain networks associated with the allocation of attention to motivationally-relevant tasks (Aston-Jones and Cohen 2005; Murphy et al., 2014). As such, our study contributes to the growing literature drawing on neuroscience methods to study behavior under risk and uncertainty. For example, recent research has used functional Magnetic Resonance Imaging (fMRI) to study how different kinds of risk are perceived (Ekins et al., 2014), or the differences between strategic and nonstrategic uncertainty (Chark and Chew 2015). More generally, this literature encompasses novel manipulations, measurements, and correlates of risky choice, thereby expanding the researcher’s toolbox. For example, Wang et al. (2013) used cognitive load to explore the limits of valuation anomalies in risky choice, and Burghart et al. (2013) studied the effect of blood alcohol concentration on risk preferences. We conducted an experiment on the belief-updating task mentioned above, which we focused on precisely because previous research has shown that incentives fail to improve performance in this task. We varied incentives within subjects and measured pupil dilation. As we expected, we found larger pupil dilation for higher incentives, indicating an increase in effort, even though overall performance did not react to incentives. Previous research on this paradigm (Achtziger and Alós-Ferrer 2014; Achtziger et al., 2015) allows us to discard the possibility of ceiling effects and link errors to (fast, impulsive) reinforcement processes. Our analysis also allows us to exclude arousal as a possible alternative explanation for larger pupil dilation. Hence, we conclude that incentives do increase effort in the cognitive task we consider, but this effort might be misallocated to counterproductive processes. Indeed, in an EEG study, Achtziger et al. (2015) found that higher error rates under high incentives in this task were linked to larger amplitudes in (extremely early) brain potentials associated to reinforcement learning. In other words, our results suggest that, in this paradigm, one finds the paradoxical situation that increasing monetary rewards does increase effort, but since it also increases the salience of the win-loss cues on which reinforcement processes operate, the increase in effort is channeled through such processes, counteracting any positive effects of incentives in performance. The paper is structured as follows. Section 2 discusses the belief-updating task and the related literature. Section 3 presents the experimental design in detail. Section 4 discusses the behavioral and pupillary results. Section 5 concludes.",3
63.0,2.0,Journal of Risk and Uncertainty,11 October 2021,https://link.springer.com/article/10.1007/s11166-021-09363-8,Crowded out: Heterogeneity in risk attitudes among poor households in the US,October 2021,Arianna Galliera,E. Elisabet Rutström,,Female,Unknown,Unknown,Female,"A new perspective on poverty, as it relates to risk in income, resources, and needs has recently emerged. Two pathbreaking studies have highlighted the new perspective on poverty: Collins et al. (2009) and Morduch and Schneider (2017). These studies expose the very complex risk management needs that poor households face due to the great variability in both income and spendings that they encounter on a daily basis. Their risk management needs are expressed through a high frequency of micro transactions, such that the total value of transactions during a month greatly exceeds the monthly earnings, often by multiples. Clearly, how well these households manage depend partly on their risk attitudes. This study adds to that literature by investigating the heterogeneity in risk attitudes among the urban poor in a rich country and a novel focus on household composition is introduced. Experimenters by now know a lot about the heterogeneity of risk attitudes, but not so much as it pertains to poor households in rich countries. Measurements of risk attitudes of poor and low-income households exist primarily for less developed countries, going back to Binswanger (1980) in India. This paper provides measurements from a unique data set collected among the urban poor in Atlanta, Georgia.Footnote 1 This data set includes lab-in-the-field experiments on the relationship between risk attitudes and several household characteristics.Footnote 2 We follow in the tradition of Binswanger (1980) and do not make causal claims. Almost all studies that aim to relate risk attitudes to the characteristics of respondents suffer from a lack of clarity about what the underlying causation is, seemingly assuming that risk attitudes depend on other household characteristics, wealth and income in particular. However, we take the view that relationships can be bidirectional. Take income, for example: while it can be argued that a lower income may expose households to more severe consequences of risk, thus perhaps making them more risk averse, it is also the case that risk aversion comes at a cost of foregone expected earnings. Of course, some demographics, such as gender, are clearly exogenous to risk attitudes, but many others reflect at least partly some choices. In this study we look at the usual suspects that may be associated with risk attitudes: income, wealth, education, and employment. However, we also include a novel focus on the composition of the household. We look at both the size of the household, the composition in terms of dependants and non-dependants, and especially children as dependants. This relationship can be bi-directional: on the one hand, the household composition may be determined by the risk attitude of the head, but on the other hand, it is also possible that the household composition affects the risk attitudes of the head. As an example of the former, a household head who is not very risk averse may be willing to take in more dependant household members even if that implies a larger burden of non-discretionary expenses and thus less ability to manage risk. Some of these dependants may be permanent household members while others live only temporarily with the household and are thus members as a result of relatively recent choices by the household head. For any given income, more dependants implies that needs-based consumption such as basic food, clothing and healthcare will take up a larger share of the budget leaving a smaller discretionary share. A smaller discretionary budget increases risk since the resource margins to handle shocks are smaller, so that only heads who are relatively less risk averse may be willing to support a household with more dependants. On the other hand, the increased risk that comes from having more dependants may increase risk aversion under some circumstances or for some household heads. Similar to how other studies associate lower incomes with higher risk aversion due to the reduced ability to manage risk, the reduction in resource margins that come with having more dependants similarly lead to a reduction in the ability to manage risk. This increase in risk may lead to an increase in risk aversion, especially if the number of dependants is not an immediate choice by the household head, but a result of some other forces. One risk management tool that households may use is to take in additional non-dependants who can contribute resources in various ways. However, the use of this tool depends on the availability of bedroom space in the home. The more crowded the house is, the less the household head can depend on this tool. Crowdedness can also have other negative effects on the household, such as stress and depression, that may correlate with risk aversion. Finally, crowdedness may be negatively correlated with other income and wealth that also affect risk management. Understanding the relationship between risk attitudes and household composition, as well as other household characteristics, even in a purely descriptive manner, is informative to the design of public policies and privately provided solutions intended to alleviate poverty. Households that are risk averse are more likely to spend money on products and services that are designed to reduce risk, such as insurance or savings buffers. On the other hand, households that are more risk averse will also be less inclined to try out new and unfamiliar products, such as new types of insurance or credit and savings options, and this may hinder their abilities to improve their lives. We measure risk attitudes using experimental lottery tasks with real money consequences, and characterize the households of the respondents based on survey responses. Respondents for this data set were recruited from low-income, primarily black neighborhoods in Atlanta. This is a population that has received relatively little attention in the experimental literature: the poor in a rich country. Poor households suffer from the inability to buy a lifestyle that prevails in their society (Shipler 2005), they live at the margin even if they have one or multiple jobs. Typical jobs are generally volatile by hours and earnings, while past debts and expenses are ever increasing burdens. The poverty line for a household of four, as defined by the US Department of Health and Human Services (HHS), is an annual income of $25,750 Footnote 3 and several programs base their eligibility criteria on it. Many of the working poor are just above the poverty line when considering their annual earnings, thus ineligible for such program support, although weekly and monthly earnings volatility frequently drag them under the poverty level. Further, a household that loses an income source, thereby falling below the official poverty line, will have to wait for several months for their applications to be processed before receiving any support. Similar to some studies conducted in less developed countries we do not see a significant correlation between risk aversion and several income related variables, and when we do the correlation is negative, consistent with other studies in less developed countries and studies in developed countries on representative, primarily non-poor, populations: the less money you have the more risk averse you are. Specifically, participants with lower hourly earnings tend to be more risk averse. Our most interesting finding is that household composition measures are strongly correlated with risk attitudes. The correlation of the risk aversion with the size of the household is positive but is dampened, and even becomes negative, if the dependants are children. However, the latter negative association disappears when the housing constraint becomes tighter. Our findings cannot be explained purely by a unidirectional relationship between household composition and risk aversion. Some associations are consistent with risk attitudes being determined by the household composition. For example, households with more dependants have heads that are more risk averse, consistent with there being relatively smaller discretionary funds and with how previous studies have shown that less income leads to higher risk aversion. Other associations cannot be explained in this way, such as why heads with lower risk aversion are associated with a larger proportion of children as dependants. This could instead point to a selection effect where heads with lower risk aversion are more willing to take in children, suggesting that being responsible for children comes with greater risks. Section 2 reviews some relevant literature. Section 3 presents the study design. Section 4 gives some descriptive results and Sect. 5 presents results from our estimation using structural maximum likelihood models. Section 6 concludes. Online appendices with additional information referred to throughout the paper are available at https://cear.gsu.edu/category/working-papers/wp-2021/ for working paper WP2021_03.",3
63.0,2.0,Journal of Risk and Uncertainty,22 October 2021,https://link.springer.com/article/10.1007/s11166-021-09359-4,"Risk avoidance, offsetting community effects, and COVID-19: Evidence from an indoor political rally",October 2021,Dhaval Dave,Andrew Friedson,Joseph J. Sabia,Unknown,Male,Male,Male,"“Coronavirus surge in Tulsa ‘more than likely’ linked to Trump rally”              -New York Times headline, July 10, 2020Footnote 1  “Large in-person gatherings where it is difficult for individuals to remain spaced at least six feet apart and attendees travel from outside the local area” are the “highest risk” category of event or gathering for the spread of COVID-19, according to Centers for Disease Control and Prevention (CDC) guidelines (Centers for Disease Control & Prevention 2020a). Indoor gatherings are viewed as problematic as indoor temperature, airflow and humidity are conducive to the spread of COVID-19 (Allen and Marr 2020; Contini and Costabile 2020; Mittal et al. 2020; Setti et al. 2020). Between March 15, 2020 and June 1, 2020, nearly all states and the District of Columbia banned large indoor gatherings such as sporting events and theatre performances (Dave et al. 2020b; Mervosh et al. 2020).Footnote 2 Despite the high-risk categorization of indoor gatherings, some states chose to roll back bans on indoor events. For example, as of June 22, 2020 most counties in Nebraska were allowed to hold indoor events as long as attendance did not exceed the maximum of 50 percent of building capacity or 10,000 individuals (Treisman 2020). Studies of sporting events in March 2020 suggest that additional professional and college level basketball games and additional professional hockey games strongly contributed to the spread of COVID-19 (Ahammer et al. 2020; Carlin et al. 2021).Footnote 3 However, evidence from these events comes from a time period where COVID-19 was circulating less widely – for example, Tulsa County, Oklahoma, which will be a point of focus in the analyses to follow had 32 new recorded cases on April 1, 2020 and over four times as many new recorded cases on June 20, 2020 (our time period of interest), meaning that the large estimates from sporting events may be a lower bound for viral spread due to indoor events during time periods where the virus is circulating more widely. In this study, we examine a special case of President Donald J. Trump’s re-election campaign rally, held on June 20, 2020 at the Bank of Oklahoma (BOK) Center and nearby convention center in Tulsa, Oklahoma. While estimates leading up to the rally estimated that attendance would reach up to 100,000 — well over the capacity of the venue, forcing overflow to the nearby convention center (Murphy and Lauer 2020; Murphy 2020a) — attendance figures reported by Fire Marshalls ranged from 6000 to 7000 and attendance numbers reported by the re-election campaign reached 12,000 (Murphy and Lauer 2020; Wise 2020). Though the turnout of the event was disappointing politically, the crowd size that did materialize is comparable to that seen at many sporting events — including those held by the Women’s National Basketball Association, the National Basketball Association (NBA), and the National Hockey League (NHL) — as well as numerous megachurch services. However, in some ways, this indoor event was also quite different from usual sporting events or church-related gatherings, making the rally a potentially poor bellwether for gauging the dangers of indoor events and reopening policies but at the same time an excellent laboratory in which to observe population reactions to risk. The rally was accompanied by numerous media reports suggesting there could be violent clashes between the president’s supporters and opponents (Baker and Haberman 2020; Bierman 2020; Cohen 2020; Karni 2020; Murphy 2020b; Singh 2020). The National Guard was deployed to maintain order (Murphy 2020c) and numerous businesses and roads closed (Fox23News Staff 2020; Holloway 2020) in anticipation of the event and its size. Thus, the event was coupled with both a local shutdown of many gathering places, including restaurants and bars, as well as signals to deter non-attendees from visiting the area near the event. These factors may have plausibly generated avoidance behavior in the non-attending population, which could have important offsetting effects on population level growth of COVID-19 cases, a point discussed recently in the context of Black Lives Matter protests (Dave et al. (2020b).Footnote 4 In the days and weeks following the campaign event, numerous high-profile media reports anecdotally linked the Trump rally to a surge in new COVID-19 cases in Tulsa, drawing on notable attendees or Oklahomans who had tested positive or drawing on post-rally infection trends in the city.Footnote 5 This included coverage of high profile COVID-19 cases and deaths where the patient had attended the rally, most notably, former businessman and presidential candidate Herman Cain (Ortiz and Seelye 2020). This study is the first to rigorously explore the impact of President Trump’s 2020 presidential campaign kickoff rally on social distancing and COVID-19 related outcomes. To begin, we utilize anonymized smartphone data from SafeGraph Inc. to examine the impact of the Tulsa rally on travel into the Census block groups (CBGs) where the Tulsa rally took place. We document that the Tulsa event increased total cell phone “pings” in the treatment CBGs by 22.4 percent and the number of non-resident cell phone pings by 25.7 percent. Foot traffic at hotels, restaurants, and entertainment venues in the treatment CBGs also increased, consistent with this influx of visitors. However, using synthetic control methods, we find that net stay-at-home behavior in Tulsa County, which drew over half of rally attendees (according to cell phone data), did not change. Moreover, foot traffic at restaurants and bars, and at retail and entertainment venues, in Tulsa County declined on the day of the rally and the preceding day, consistent with avoidance behavior of other residents and displacement of some of their usual weekend activities that would have taken place in the absence of the rally. Such individuals may have chosen to increase stay-at-home behavior to avoid congestion at the rally, owing to road and business closures, or in response to predictions of violent clashes between protesters and rally attendees which precipitated the National Guard being called out on June 19 and 20. Then, turning to data from the Centers for disease control and prevention (CDC), we explore whether the Trump rally ignited COVID-19 growth, examining (i) Tulsa County, (ii) Tulsa County and its border counties, and (iii) the state of Oklahoma. Synthetic control estimates provide no evidence that the Tulsa rally precipitated COVID-19 case growth in any of these jurisdictions during the five weeks following the event. Moreover, a dose–response difference-in-differences approach, which utilizes SafeGraph data on higher “donor” counties to the rally, find no evidence that COVID-19 cases grew more quickly in counties that sent more attendees into the rally census block group and who returned home. An examination of COVID-19 deaths up to eight weeks following the rally similarly produced no evidence of significant increases in the COVID-19 death rate. These findings have important implications for policymakers considering mass gathering bans and reopening policies. Our findings show that this indoor event was likely not as dangerous to public health as sporting events in the earliest days of the pandemic (Ahammer et al. 2020; Carlin et al. 2021). The reasons for this may be twofold, including a) effects from any risk mitigating behavior of attendees and organizers: such as the event being accompanied by substantial publicity surrounding the importance of mitigating behaviors (i.e., mask-wearing), and attendees having their temperature taken upon entry (Murphy 2020d), as well as b) any offsetting community effects in response to perceived risk from the large gathering. To the extent that the Tulsa event displaced mobility that otherwise would have taken place, such as by reducing gatherings of non-household members at restaurants and bars downtown, such compensatory avoidance behavior may have played a vital dampening role in community spread.Footnote 6 As not all future indoor events are likely to generate such avoidance behavior, reopening policies should not dismiss the possibility of disease spread under different circumstances. When assessing the risk of an event, both individual and population level risk mitigating behavior need to be considered.",4
63.0,2.0,Journal of Risk and Uncertainty,25 October 2021,https://link.springer.com/article/10.1007/s11166-021-09361-w,Gender differences in the stability of risk attitudes,October 2021,Anwesha Bandyopadhyay,Lutfunnahar Begum,Philip J. Grossman,Unknown,Unknown,Male,Male,"In this paper, we test if there is a gender difference in risk preferences, if risk preferences are stable across time and stakes and if there is a gender difference in stability, and what role financial and emotional states plays in risk preferences. Subjects play both the Eckel and Grossman lottery task (Eckel and Grossman 2002, 2008a) and the Holt and Laury (2002) multiple price list task to elicit the risk preferences of participants.Footnote 1 Subjects participate in a 12-week, online experiment with low- and high-stakes lottery choices. We also collect self-reported measures of the state of minds of the participants – specifically sources and levels of stress and happiness, as well as financial well-being – to explain the weekly lottery choices. There have been many experimental studies on risk attitudes, using a variety of risk attitude elicitation tasks, and, generally, they report females to be more risk averse than males (see, for example, Johnson and Powell 1994; Eckel and Grossman 2002, 2008a, 2008b; Holt and Laury, 2002; Siegrist et al. 2002; Fehr-Duda et al. 2006; Baucells and Villasís 2010; Charness and Gneezy 2012; Grossman and Eckel 2015). Some find that results depend on the stakes (Filippin and Crosetto 2016; Holt and Laury 2005). Other studies have not found a gender difference (Daruvala 2007; Grossman 2013; Grossman and Lugovskyy 2011; Harrison et al. 2002; Kruse and Thompson 2003; Straznicka 2012). Byrnes’ et al. (1999) meta-analysis of 150 studies finds a significant gender difference in risk attitudes. Empirical studies also report a gender difference in risk attitudes (Jianakoplos and Bernasek 1998; Powell and Ansic 1997; Sahm 2012; Swanson et al. 1995).Footnote 2 Several reasons have been proposed for gender differences in risk taking. Eckel and Grossman (2002) cite the evolutionary reproductive success argument: “[F]or females, the low-risk, steady-return investment in parenting effort often yields the highest returns, whereas for males, the higher-risk investment in mating effort produces a higher expected payoff” (p. 282) (see also, Rubin and Paul 1979; Daly and Wilson 1988; Geary 1998; Low 2000; Fetchenhauer and Rohde 2002). In their survey article, Croson and Gneezy (2009) offer three possible reasons why females are more risk averse than males. The first is Loewenstein’s et al. (2001) risk-as-feeling hypothesis that argues that differences in risk attitudes may reflect a difference in emotional response to risk. It is argued that females experience stress, fear, pessimism and joy at higher levels than males (Fujita et al. 1991; Lerner et al. 2003). The risk-as-feeling hypothesis suggests that the emotional response to a risky situation could cloud cognitive assessments of the risk involved in the situation, thereby leading to emotions taking over cognitive responses. A number of studies offer support for the hypothesis, finding gender difference in experiences of nervousness and fear (Brody 1993; Brody et al. 1990; Fujita et al. 1991; Stapley and Haviland 1989). Sjöberg and Wåhlberg (2002), Fyhri and Backer-Grøndahl (2012), and Weller and Thulin (2012) find that emotionally unstable people perceive risks as higher. The second is a gender difference in confidence (Deaux and Farris 1977; Lundeberg et al. 1994; Niederle and Vesterlund 2007). Finally, Croson and Gneezy cite Arch’s (1993) argument that males are more likely to see risky situations as challenges to overcome while females perceive risky situations as threats to be avoided. If males are more confident of success and see risky situations as challenges, they will be more willing to accept riskier gambles. Whether or not risk attitudes are stable has also received attention; but less studied is whether or not men’s and women’s risk attitudes differ in stability. Stability of risk preferences could be thought of in three ways. Stability might mean a constant level of risk tolerance that is unchanging regardless of stakes and/or lived experiences. Any deviation from this level indicates instability of preferences. Stability might mean that an individual’s underlying level of risk tolerance is constant but is stakes dependent. Finally, at any stake level, an individual’s underlying level of risk tolerance may be constant, but, at any point in time, this tolerance may increase or decrease, depending on what the person is currently experiencing. As the experience fades with time, risk tolerance reverts to the underlying level; a reversion to the mean type process. The malleability of risk attitudes across stakes and contexts has been reported in a number of studies. Using two different elicitation methods – gamble questions and lottery choices – Anderson and Mellor (2009) find instability of average risk preference. Holt and Laury (2002) find that as stakes increase, so does risk aversion and risk aversion is higher when incentives are real. Holt and Laury (2005) find increasing risk aversion as real (but not hypothetical) stakes increase. Holt and Laury (2002) find that females are more risk averse for the lower stakes, however, this gender difference vanishes in treatments with higher payoffs. Brown Kruse and Thompson (2001) report a gender difference in risk preferences when gambles are over class points, but no difference when money is at stake. Barseghyan et al. (2011) find greater risk aversion in home than auto insurance. Schubert et al. (1999) find that context matters. They observe that the gender difference in risk attitudes with respect to abstract gambles – males (females) are more risk tolerant toward gains (losses) – and no gender differences when gambles are framed as insurance or investment. Findings on the stability of risk preferences across time are somewhat mixed. Straznicka (2012), Zeisberger et al. (2012), and Dasgupta et al. (2017) find stability of average lottery choices but evidence of instability at the individual level; some people became more risk tolerant over time while others became more risk averse. Andersen et al. (2008) report changes in their subjects’ risk attitudes over a 17-month period. Baucells and Villasís (2010) report that aggregate patterns of preferences are similar across two sessions, and that preferences in the gain domain are more stable than preferences in the loss domain. Wölbert and Riedl (2013) also find stability of risk attitudes over time. Most of these studies measures risk preferences at a fixed point in time or two points of time and for one set of stakes, thus they are unable to ascertain the nature of the (in)stability they are observing (e.g., extreme stability, stake dependent stability). We designed our experiment to enable us to observe risk attitudes over a relatively longer period of time and also across different stake sizes. Studies in Psychology suggest that males’ and females’ susceptibility to emotions, including stress and joy, may differ and may affect their risk preferences (Fujita et al. 1991; Lerner et al. 2003). Consistent with this are studies reporting evidence that exposure to extreme levels of stress (natural disasters and military conflict) alters individuals’ risk tolerance. Cameron and Shah (2015) find that people in villages that recently suffered a flood or earthquake were less risk tolerant than people in villages that did not experience a disaster, and females were less risk tolerant than males. Beine et al. (2020) collected risk preference data prior and subsequent to the 2019 earthquakes in Tirana, Albania. They report decreased risk tolerance after the first earthquake and risk tolerance further declined after the second. They report less risk tolerance on the part of females. Eckel et al. (2009) compared the risk preferences of survivors of Hurricane Katrina shortly after the event and a year later, as well as a demographically similar sample unaffected by the hurricane. Females survivors were found to be more risk tolerant than males immediately after the hurricane and more risk tolerant than the other sample, but this difference faded with time. Voors et al. (2012) finds that individuals in Burundi who were exposed to violence are more risk-seeking, but they report no gender difference. Cavatorta and Groom (2020), in their study on impact of a wall between Palestinian territories in the West Bank and Israel, find that Palestinians residing close to the wall exhibit relatively higher risk tolerance. They do not observe a gender difference. From a study conducted in Afghanistan, Callen et al. (2014) finds that those exposed to violence have an increased preference for certainty (i.e., lower risk tolerance) but no gender difference. At the aggregate level, we do not observe a significant gender difference in average risk attitudes either within or across stakes. Across time, risk tolerance for low-stakes lotteries increases for both genders; for high-stakes lotteries, only females become more risk tolerant. At the individual level, both genders exhibit a relatively high level of choice stability for all three elicitation tasks. We do observe a gender difference in responses to categories of stress, happiness, and financial well-being. For males, experiencing family-sourced stress increases risk tolerance at both stake levels. For females, for high-stakes lotteries, experiencing university-sourced stress and relationship-sourced happiness increases risk aversion, but feeling more financially stable increases risk tolerance. Males exhibit a decrease in the instability (variability) of lottery choices as time passes, suggesting learning or increasing comfort with riskier choices with experience. Finally, regardless of gender, roughly equal numbers of subjects exhibit risk tolerance that is decreasing, constant, or increasing with stakes, and the risk tolerance of our subjects across stakes are relatively stable.",4
63.0,2.0,Journal of Risk and Uncertainty,22 September 2021,https://link.springer.com/article/10.1007/s11166-021-09360-x,The value of statistical life in the context of road safety: new evidence on the contingent valuation/standard gamble chained approach,October 2021,Fernando-Ignacio Sánchez-Martínez,Jorge-Eduardo Martínez-Pérez,José-Luis Pinto-Prades,Unknown,Unknown,Unknown,Unknown,,
63.0,3.0,Journal of Risk and Uncertainty,26 November 2021,https://link.springer.com/article/10.1007/s11166-021-09364-7,Insurance decisions under nonperformance risk and ambiguity,December 2021,Timo R. Lambregts,Paul van Bruggen,Han Bleichrodt,Male,Male,,Mix,,
63.0,3.0,Journal of Risk and Uncertainty,12 February 2022,https://link.springer.com/article/10.1007/s11166-022-09372-1,Correction to: Insurance decisions under nonperformance risk and ambiguity,December 2021,Timo R. Lambregts,Paul van Bruggen,Han Bleichrodt,Male,Male,,Mix,,
63.0,3.0,Journal of Risk and Uncertainty,26 January 2022,https://link.springer.com/article/10.1007/s11166-021-09367-4,Intransitivity in the small and in the large,December 2021,Sushil Bikhchandani,Uzi Segal,,,Male,Unknown,Mix,,
63.0,3.0,Journal of Risk and Uncertainty,04 December 2021,https://link.springer.com/article/10.1007/s11166-021-09365-6,An experimental study of charity hazard: The effect of risky and ambiguous government compensation on flood insurance demand,December 2021,Peter John Robinson,W. J. Wouter Botzen,Fujin Zhou,Male,Unknown,Unknown,Male,"Individuals typically underinsure low-probability/high-impact natural disaster risks (Kunreuther & Pauly, 2004). These risks tend to be underestimated by individuals (Viscusi & Zeckhauser, 2006). Systematic behavioural biases and heuristics can explain lack of demand for insurance and protective measures, as well as low risk perceptions (Meyer & Kunreuther, 2017; Slovic et al., 1977). However, underinsurance by individuals may also result from rational expectations that governments provide compensation after disaster strikes. The “Samaritan’s dilemma” describes a situation whereby the government cannot credibly commit not to help an individual in case of a loss, even though the receipt of unconditional financial assistance from the government incentivizes the individual not to take protective measures (Buchanan, 1975). Crowding out of private insurance by government compensation for disaster damage has also been termed the “charity hazard” (Browne & Hoyt, 2000). Reliance on government compensation can have negative efficiency effects (Coate, 1995). This is partly due to weak incentives by governments to manage resources carefully, and to examine where disaster relief is most needed (Raschky & Weck-Hannemann, 2007). Another source of inefficiency relates to politically motivated government compensation payments. For example, Garrett and Sobel (2003) find that disaster expenditures made by the Federal Emergency Management Agency (FEMA) as well as U.S. presidential disaster declarations are politically motivated, and in particular depend on election years and states considered important to the outcome of elections. This study focusses on insurance against flood risk, which is the most costly natural disaster risk worldwide (Miller et al., 2008). During the 2017 Atlantic hurricane season, which ranked as one of the most destructive in U.S. history (Chew et al., 2018), National Flood Insurance Program (NFIP) policyholders filed approximately 133,000 claims. Moreover, FEMA paid more than $2 billion in federal disaster assistance, which is a form of ad hoc government compensation for uninsured losses.Footnote 1 The co-existence of the NFIP and disaster compensation by FEMA means that the charity hazard is a potential issue for the flood insurance market in the U.S., as is also the case in many European countries (Porrini & Schwarze, 2014). In the Netherlands (our policy context) the government may provide partial compensation for flood damage via the Calamities and Compensation Act (WTSFootnote 2) (Botzen & van den Bergh, 2008). However, the WTS has no established funds and no clear rules outlining under what circumstances flood damage will be compensated and by how much. Moreover, there is no legal obligation for the Dutch government to compensate damages. Therefore, it is currently ambiguous whether households will receive compensation for flood damages in the Netherlands (Surminski et al., 2015). Efforts have been made in recent years to make private flood insurance more widely available, but this insurance is purchased by only a small fraction of the Dutch population (Suykens et al., 2016). In addition to the high costs of offering flood insurance in the Netherlands, the potential for charity hazard may slow the uptake of this insurance by homeowners. Given increasing flood risks from climate change and socio-economic developments (IPCC, 2012), having adequate flood insurance coverage becomes more important for offering financial protection against residual flood risk, which implies that the charity hazard is especially problematic. Therefore, it is relevant to understand under which conditions charity hazard occurs, which is the focus of this paper. Other forms of government compensation for natural disaster damages exist across Europe. Contrary to the ad hoc Dutch compensation scheme, Austria accumulates funds through mandatory taxation, to be used for financing relief payments to cover flood damages (Schwarze et al., 2011). Although individuals have no legal entitlement to government compensation, the well-functioning nature of the Austrian catastrophe fund generates certainty about compensation receipt according to Raschky et al. (2013). In other countries like Germany, relief is not controlled by formal legislation, and payout can depend on factors like media coverage and election years (Thieken et al., 2006). Nevertheless, high levels of compensation have typically been granted in Germany following flood events in the past (Surminski & Thieken, 2017). Other examples of high levels of government relief to homeowners can be found in Hungary, where extensive compensation was provided after the 2001 Tisza flood (Vari et al., 2003). Similar to the U.S. but in contrast to the other European examples, France requires an official natural disaster declaration before individuals can receive compensation. However, this is not based on pre-defined levels of flood damage, so compensation is also ambiguous in France (Paudel et al., 2012). Given the apparent differences in the extent and degree of riskiness and/or ambiguity in government compensation across different countries, it is relevant to examine which forms of compensation crowd out private demand for insurance the least. According to Jaspersen (2016) and Robinson and Botzen (2019), we define a decision under risk as a situation where the probability of each possible outcome is known. If the probabilities are not known, and a distribution of probabilities over possible probabilities is not known either, the decision is considered one under ambiguity. Ambiguity and/or riskiness in government compensation is perhaps also relative to the number of times individuals have received flood-related compensation in the past. If individuals have been flooded many times in the past, it may be easier for them to accurately assign a probability to the likelihood of receiving compensation (it becomes riskier vs. more ambiguous). On the contrary, if somebody has never been flooded in the past it may be very difficult to assign a precise probability to the likelihood of receiving government compensation. The latter is more relevant to the Dutch context where experience with flooding is scarce due to high levels of flood protection. So far empirical evidence on the charity hazard is rather mixed (Andor et al., 2020). Contrary to expectations, Browne and Hoyt (2000) showed with NFIP policies-in-force data, that disaster relief expenditures by FEMA positively relate to flood insurance demand. The authors proposed that their positive result can arise because their analysis insufficiently controls for risk exposure which affects both demand for insurance and the receipt of government relief. Another potential source of endogeneity in their dataset concerns reverse causality, i.e., the more insured an area is, the less government compensation may be required after a flood. Kousky et al. (2018) control for endogeneity by employing a two-stage least squares analysis.Footnote 3 Their instrumental variable is an interaction term between timing of presidential elections and states considered important for the outcome of elections. According to Garrett and Sobel (2003), the variable provides a useful exogenous source of variation in relief payments. Kousky et al. (2018) showed that individual assistance grants have a negative impact on flood insurance demand once endogeneity has been controlled for. Survey research conducted in coastal regions by Petrolia et al. (2013) in the U.S. find that perceived eligibility for post-disaster relief has a positive effect on the probability of holding flood insurance, in contrast to the charity hazard. In light of these findings, the authors suggested that their measure of disaster assistance expectations may be biased if individuals relying on this assistance are ashamed to admit it. In a follow-up study using the same survey data, Landry et al. (2019) instrumented for post-disaster relief expectations using data on congressional members that served on subcommittees which have direct oversight of FEMA spending, as well as payment history of the FEMA public assistance grant program. They found that perceived eligibility for post-disaster relief has a negative effect on flood insurance demand in the follow-up analysis. In another survey study by Botzen et al. (2019), the purchase of flood insurance in the U.S. is negatively related to previous receipt of federal disaster assistance. Moreover, Botzen and van den Bergh (2012) reported in a stated preference study in the Netherlands that when hypothetical flood insurance demand is elicited in a survey version which may grant government compensation, demand is less than a version in which compensation is not available. Raschky et al. (2013) conducted a survey about flood insurance demand in Austria where partial certain government compensation is provided, and in Germany which has granted full ambiguous government compensation in the past. Their survey results show that expectations about disaster relief crowd out insurance demand more in Austria than in Germany. We aim to re-examine this finding in an experimental setting, allowing for better control over extraneous factors, which are typically challenging to control for in the field. For example, other factors of influence on flood insurance demand, like objective risk levels, may differ between Germany and Austria and partly drive the results by Raschky et al. (2013), while our experimental setting controls for such factors. In general, experimental studies which have an explicit environmental context can be useful to study the impact of certain types of variables (Gsottbauer & van den Bergh, 2011), like the influence of government compensation on insurance demand. Despite the relatively large literature on insurance demand in experimental research (Jaspersen, 2016), to our knowledge Brunette et al. (2013) are the only ones to have directly incorporated government compensation into their design. They also find that partial certain government compensation crowds out demand for insurance.Footnote 4 However, their evidence is based on a hypothetically incentivized experiment, even though incentives have been shown to significantly reduce insurance demand choice anomalies (Jaspersen, 2016; Laury et al., 2009). Moreover, Brunette et al. (2013) implement uncertainty in the probability of loss, whereas we investigate how ambiguity in government compensation affects insurance decisions. This is relevant because in practice government compensation for disaster damage is often ambiguous, albeit to different degrees. We employ an incentivized experiment to study several theoretical predictions related to the charity hazard hypothesis, risk preferences, ambiguity preferences and insurance pricing. Our theory analysis is informed by previous studies by Kelly and Kleffner (2003) and Raschky and Weck-Hannemann (2007), who investigated the effect of government compensation on insurance demand in an Expected Utility framework. However, our analysis also examines the charity hazard under imprecise knowledge about government compensation, for which we utilize the Klibanoff et al. (2005) smooth model of decision making under ambiguity. Some examples of ambiguity preference elicitation underthis model are Chakravarty and Roy (2009) and Attanasi et al. (2014).Footnote 5 An examination such as ours highlights the usefulness of an experiment to disentangle the effect of different schemes of government compensation (certain, risky and ambiguous compensation), which is a challenge when using data for actual insurance purchases as well as hypothetical survey methods. Our experimental findings show that flood insurance demand is negatively impacted by anticipated government compensation, except when the compensation is ambiguous. We also find that ambiguity averse subjects have higher demand for insurance when government compensation is ambiguous relative to risky, according to ambiguity preferences elicited using multiple price list tasks. Furthermore, ambiguity preferences elicited in the gain domain predict a unique effect on insurance demand better under ambiguous government compensation, relative to those elicited in the loss domain. Regarding risk preferences, a stated risk aversion measure which has been shown to correlate well with risk taking behaviour in practice better predicts flood insurance demand than risk preferences elicited in multiple price list tasks. Moreover, we do not find that risk averse subjects demand more insurance when the compensation provided is risky as opposed to certain. Lastly, the insurance loading factor has a negative impact on flood insurance demand, as expected. The paper is structured as follows: Section 2 outlines the hypotheses. Section 3 describes our experimental design and implementation. Section 4 reports the experimental findings based on a non-parametric and parametric analysis. Section 5 discusses these findings in relation to the hypotheses, and suggests several recommendations for policy. Section 6 concludes the paper.",4
63.0,3.0,Journal of Risk and Uncertainty,24 January 2022,https://link.springer.com/article/10.1007/s11166-021-09366-5,How serious is the measurement-error problem in risk-aversion tasks?,December 2021,Fabien Perez,Guillaume Hollard,Radu Vranceanu,Male,Male,Male,Male,"Economists explain individual heterogeneity in observed behavior by appealing to a number of key individual characteristics, such as risk attitudes or time preferences. For many years the gold standard in experimental economics consisted in eliciting risk-aversion by means of incentivized experiments, where choices have material consequences (Schildberg-Hörisch, 2018). A common research practice is to elicit this kind of individual characteristic via an initial task, and then use the resulting figure as an explanatory variable in subsequent regressions. One source of intellectual discomfort with this method is the substantial within-individual variability in these incentive-based measures. For example, the correlations between different measures of risk attitudes for the same individual are typically small, even when the same task is repeated within a short period of time (Csermely & Rabas, 2016; Dulleck et al., 2015). From an econometric perspective, within-individual variability can be interpreted as measurement error (Hey et al., 2009), which has well-known negative consequences: in OLS regressions, the coefficient on the explanatory variable that is measured with error is attenuated and, in multivariate regressions, other variables may falsely appear as significant, as the measurement error in one explanatory variable renders all of the estimates inconsistent (Pischke, 2007). Another difficulty stems from the fact that a majority of popular elicitation methods yield a discrete approximation of a continuous variable (e.g. risk-aversion or the discount rate). Rounding elicited measures will mechanically generate some imprecision. Last, the risk-aversion estimated in laboratory experiments often comes from relatively small samples, in particular in between-subject designs (e.g., N=100 or 200). Small samples will only amplify the measurement-error problem, as the variance of the estimated coefficients will be larger. Measurement error, coupled with small sample sizes, raises questions regarding the robustness of the econometric analyses such as: What is the degree of attenuation of the coefficients in OLS regressions? and How often will significant coefficients actually appear to be insignificant? Furthermore, elicitation methods may differ in their test/retest stability; Is there a method that stands out because of its low measurement error as compared to other methods? Our analysis here proceeds in four steps. We first provide estimates of the extent of measurement error using both parametric (maximum-likelihood, ML) and non-parametric (NP) estimation methods, for 16 test/retest datasets covering four different risk-elicitation tasks. In a second step, we compare the size of the measurement error across the samples and methods. The third step consists of the simulation a large number (100 000) of times of a univariate linear stochastic model. We carry out OLS regressions with the independent variable being either the “true” risk-attitude measure, or noisy and/or rounded measures, over a variety of sample sizes. The simulations are calibrated using the parameters of the distributions as determined in the second step.Footnote 1 This allows us to disentangle the impact of measurement error and rounding on the size and significance of the estimated coefficient. In the last step, the simulations allow us to analyze and compare potential remedies for measurement error, such as increasing the number of observations, IV estimation, or using the Obviously Related Instrumental Variables (ORIV) method developed by Gillen et al. (2019). In summary, we find that:  Somewhat surprisingly, the four elicitation tasks considered, and the different datasets, generate similar levels of noise, as measured by the ratio of the variance of the error term to the variance of the observed risk-aversion measure. This result is robust to different estimation methods: in both maximum-likelihood (ML) and non-parametric estimations the variance of the measurement error is similar to that of the latent risk-aversion variable in all 16 datasets. The difference between the parametric and non-parametric estimates are only small, suggesting that the normality assumptions involved in the ML estimates (and neglecting the rounding effect in the non-parametric estimations) play only a marginal role in the results. Our simulations show that the discrete transformation of the variable of interest (i.e. rounding) affects the attenuation bias and the variance of the estimators only little. By way of contrast, the measurement error arising from within-subject variability is responsible for much of the attenuation effect. The attenuation factor is approximately 0.5 in all four of the elicitation methods considered. In line with theory, this holds regardless of the size of the sample. Our subsequent simulations confirm that the typical amount of noise in the risk-elicitation task divides the estimated coefficient on the variable of interest by around 2. Small sample sizes (e.g. \(N=100\) or \(N=200\)) produce a large proportion of (falsely) insignificant coefficients at the standard significance levels. Increasing the sample size up to \(N=1000\) is sufficient for the coefficient to become significant almost every time. Intermediate values, such as N=500, already reduce the significance bias to a considerable extent. As expected, the ORIV method almost completely removes the attenuation bias, although the ORIV estimates do have larger variances than the true OLS estimates. ORIV may therefore not suffice to remove the significance issue resulting from measurement error in small samples. Two contributions in the related literature have addressed the issue of measurement error in experimental data. Gillen et al. (2019) replicate with a 6-month lag three classic risk experiments using an original dataset (the Caltech cohort survey), and show that the results can change dramatically when measurement error is correctly accounted for. Our analysis addresses two important elements that are not considered there: the impact of the sample size (in particular, the small sample size typical of laboratory experiments) and the rounding issue arising from the use of a discrete measure of a continuous variable. In addition, all test-retest data in our paper are collected within the same experimental session, which rules out any confounds affecting within-subject variability. Engel and Kirchkamp (2019) adopt an alternative method to estimate the measurement error in the classical Holt and Laury (2002) task (or any multiple-price list tasks). Their analysis allows the error term to vary across each line, which may explain inconsistent answers,Footnote 2 which they use to estimate an individual-specific error term. In contrast, we here assume that the error terms are independent between the test and the retest, and are fixed within each task. We furthermore assume that error terms are drawn from the same distribution for all individuals. Under these assumptions, we can use the test/retest data to directly estimate the error variability. Our estimation strategy can be applied to any risk-elicitation task. Many other contributions, as surveyed in Mata et al. (2018), used data collected with a substantial time lag between the test and the retest, spanning from several weeks to one year, and reveal a correlation that falls over time, in particular regarding incentivized tasks but also for survey-based measures (self-reported levels of risk aversion). In general, these results are interpreted as showing the evolution of preferences over the life cycle (Andersen et al., 2008; Lönnqvist et al., 2015; Bardsley et al., 2010; Beauchamp et al., 2017). To rule out this possible source of within-subject variability, in this paper we only use  test/retest measures from the same session, in previously-published work. The remainder of the paper is organized as follows. The next section describes the four elicitation tasks and the corresponding datasets. Section 3 introduces the parametric and non-parametric estimation methods, which are then used to estimate the measurement error, jointly with the mean and variance of the variable of interest. Section 4 presents the simulations. Last section 5 concludes.",
64.0,1.0,Journal of Risk and Uncertainty,18 February 2022,https://link.springer.com/article/10.1007/s11166-022-09368-x,Do people have a bias for low deductible insurance?,February 2022,Howard Kunreuther,Mark Pauly,,Male,Male,Unknown,Male,"The literature on insurance has documented a number of behavioral biases regarding consumer decisions on what type of policy to purchase (Kunreuther et al., 2013). A perennial topic in the economics of risk and insurance is whether, in practice, insurance buyers choose low deductible (LD) coverage when they should have selected a policy with a high deductible (HD) if they were maximizing their expected utility (EU). Based on the relative costs of the two policies, risk averse individuals should often prefer a high deductible, rather than a low deductible plan, but seem to choose the latter (Pashigian et al., 1966). This paper describes the results of an experiment that examines the influence of default options with respect to the choice between a high deductible (HD) health insurance plan and a low deductible (LD) health insurance plan. Our study avoids problems associated with inaccurate information by providing subjects with explicit data on loss probabilities and insurance benefits where they must choose between two plans with modest differences in premiums relative to typical consumer incomes. In our experimental setting, participants are fully informed about loss probabilities and benefits conditional on loss. Our interest is in learning whether individuals maximize their expected utility and what role default options play in influencing individuals’ choices. We explore the following questions which, to our knowledge, have not been addressed in previous research: Do individuals choose the same insurance plan when the default option is altered but the likelihood of an illness and cost of insurance are kept the same for HD and LD insurance plans? Do subjects utilize different strategies when making choices between these two insurance plans (i.e., always choosing the LD plan; always choosing the HD plan; always choosing the default option; or never choosing the default option)? We find no evidence of a preference for the plan with the lower deductible relative to one with a higher deductible. Using several different framings for default options and relative premium variation, a sizeable minority of subjects preferred either HD or LD insurance regardless of premiums, default options, or prior choices, while many had malleable preferences for defaults or rejection of defaults. The coexistence of sizeable fractions of the population with specific and consistent preferences for high or low deductibles coupled with others who seem to be affected by choice architecture (e.g. defaults) presents a policy challenge. Our findings imply that sometimes increased transparency of choices in health insurance and other kinds of insurance should be pursued rather than channeling or nudging individuals toward a specific insurance plan.",
64.0,1.0,Journal of Risk and Uncertainty,10 March 2022,https://link.springer.com/article/10.1007/s11166-022-09369-w,Revisiting the diagnosis of intertemporal preference reversals,February 2022,Zhihua Li,Graham Loomes,,Unknown,Male,Unknown,Male,"The preference reversal (PR) phenomenon is probably best known in the context of risky choice, where it refers to the evidence that individuals often choose an option with lower risk and smaller returns (labelled a P-bet) over an alternative option with greater risk and larger returns (labelled a $-bet), while also placing a higher certainty equivalent value on the $-bet than on the P-bet when evaluating the two options separately. The opposite anomaly – choosing the $-bet over the P-bet in the choice task but valuing the P-bet more highly – is relatively rarely observed. This phenomenon was reported half a century ago by Lichtenstein and Slovic (1971) and Lindman (1971) and has been replicated many times since (see Seidl, 2002, for a review). A number of different explanations for the phenomenon have been proposed. For example, such a pattern of response might be accommodated by some general deterministic theory that allows systematic intransitivity (Loomes & Sugden, 1983). Alternatively, it has been suggested that preferences, far from being deterministic, are often rather imprecise, and that such imprecision may produce the observed asymmetries (Butler & Loomes, 2007; MacCrimmon & Smith, 1986). Another possible implication of the imperfectly formed nature of people’s preferences is that responses to different tasks (e.g. choice as distinct from valuation) are ‘constructed’, often using somewhat diverse cognitive processes subject to different influences or biases (see Tversky et al., 1988). From this latter perspective, valuations may focus more on payoffs, thereby favouring the higher-payoff $-bet, while choices may place more weight on the probabilities of receiving at least some positive return, thereby favouring the P-bet relatively more strongly. Tversky et al. (1990) – hereafter, TSK – tried to disentangle some of the possible causes of PR. Their Study 1 used a number of combinations of $-bets, P-bets and sure amounts, on the basis of which they concluded that nearly two-thirds of the observed reversals were due to overvaluation of the $-bet, with some smaller yet substantial contribution from undervaluation of the P-bet, but with only a minor part of the phenomenon attributable to intransitivity (this latter accounting for no more than 10% of reversals, according to their diagnosis). To investigate the generality of their ‘overvaluation-undervaluation’ diagnosis, TSK conducted a second experiment, looking at time preferences rather than risky choices. Their Study 2 design revolved around five amounts to be received (hypothetically) at various times in the future, ranging from $3550 in 10 years’ time to $1525 in six months, as well as two levels of immediate cash amounts ($1250, $1350). They combined these options to produce four triples, each involving a larger sum to be paid after a longer delay (which we shall refer to as the LargerLater or LL option, the intertemporal analogue to the $-bet), a somewhat smaller sum to be received after a shorter delay (henceforth the SmallerSooner or SS option, analogous to the P-bet) and a lower present cash amount C (analogous to a certainty). They found an even higher rate of reversals, which in this context meant placing a higher present cash value on the LL option than on the SS option but picking the SS option in a straight choice between the two. The diagnosis of causes was much the same as in their risky choice Study 1: that is, while about 15% of reversals were compatible with intransitivity, approximately 55% were attributed to overvaluing LL, with the remaining 30% being due either to undervaluing SS or to some combination of overvaluing LL and undervaluing SS relative to choice. The fact that broadly similar results were found in the domains of risk and time in two large studies using a novel design was highly influential. TSK’s diagnostic procedure has been widely endorsed (see Seidl, 2002 for example), the paper has been cited more than 1,200 times and their conclusions about the causes of PR have become the accepted wisdom. However, we suggest that there may be reasons to be cautious about those conclusions. The high rates of undervaluation of the P-bet/SS options appear to be somewhat at odds with the ‘anchoring and adjustment’ and ‘contingent weighting’ models offered by Lichtenstein and Slovic (1971) and Tversky et al. (1988), These models would entail something more like the results of a risky choice study by Loomes and Pogrebna (2017), who found that both types of bets were overvalued relative to choice, although the degree of overvaluation of $-bets was very much greater than the degree of overvaluation of P-bets, a disparity which was sufficient to produce many reversals. On closer inspection, TSK’s diagnostic procedure appears to be problematic on two counts, each of which will be examined in more detail in Sect. 5. First, TSK’s diagnostic system discarded just under half of the reversals in the risky choice study and almost two-thirds of the reversals in the intertemporal choice study, so that their conclusions were based on a non-random subset of observations. Second, because of the way it is constructed, the system has an inbuilt bias such that it simply cannot detect either undervaluation of the $-bet/LL options or overvaluation of the P-bet/SS options. The present study revisits the accuracy of TSK's attributions of cause, using an experiment which adapts and extends a design developed by Loomes and Pogrebna (2017). We suggest that our study improves on TSK’s classic study in four important ways: (i) it uses all of the observed reversals, not just a subset; (ii) it can measure both undervaluation and overvaluation for both options; (iii) it makes allowance for the noise and imprecision in people’s responses – and indeed, it sheds new light on the degree of such imprecision in the area of intertemporal choice, where there are very few such data at the moment; and (iv) it explores the use of an additional instrument – the choice list – that has become popular among experimenters to supplement (or even replace) standard binary choice and direct valuation methods (Cheung, 2015; Laury et al., 2012). In the next section, we describe the experimental design and its rationale in more detail. In Sect. 3, we present the results, with particular reference to points (i) to (iii) in the previous paragraph as they relate to direct valuation and binary choice, comparable with the original TSK study. In Section 4, we summarise the main additional insights provided by the choice list instrument. Whether we use binary choices or choice lists, our data suggest a substantially different attribution of causality than that proposed by TSK. Section 5 considers why this might be the case. Section 6 concludes with a discussion not only relating to the specifics of this study but also reflecting on some broader implications. The preference reversal phenomenon is more than an experimental curiosity. The systematic disparity between valuation and choice has implications for theory, for the interpretation of experimental and survey data, and for applications to public policy in areas such as health, safety and environmental goods – issues to which we return in the final section.",
64.0,1.0,Journal of Risk and Uncertainty,22 March 2022,https://link.springer.com/article/10.1007/s11166-022-09371-2,A behavioral decomposition of willingness to pay for health insurance,February 2022,Aurélien Baillon,Aleli Kraft,Kim van Wilgenburg,Male,Unknown,,Mix,,
64.0,1.0,Journal of Risk and Uncertainty,25 January 2022,https://link.springer.com/article/10.1007/s11166-022-09370-3,Intertemporal choice as a tradeoff between cumulative payoff and average delay,February 2022,Pavlo R. Blavatskyy,,,Male,Unknown,Unknown,Male,"Intertemporal choice involves outcomes that are received in different moments of time. It arises in many economic situations such as consumption/savings decisions, financial investment, education planning and career choice, dynamic games and bargaining etc. Samuelson (1937) proposed a classic model of time preferences known as discounted utility or constant (exponential) discounting. This model gained momentum in economics after Koopmans (1960) provided its axiomatization. Discounted utility of a stream of intertemporal payoffs is given by the exponentially-discounted sum of utilities of its payoffs. Thaler (1981) argued that a decision maker may prefer to consume one apple today over two apples tomorrow and have a reversed preference when both consumptions are delayed for one year. Such time preferences cannot be rationalized by discounted utility. This descriptive limitation of discounted utility became known as the common difference effect (Loewenstein and Prelec 1992). It motivated the development of numerous generalizations of discounted utility. These include, inter alia, quasi-hyperbolic discounting (Phelps and Pollak 1968), generalized hyperbolic discounting (Loewenstein and Prelec 1992) and liminal discounting (Pan et al. 2015). Yet, these generalizations, like discounted utility itself, share a common counter-intuitive property: splitting one intertemporal payoff into two payoffs, one of which is slightly delayed in time, may increase overall utility. Blavatskyy (2015) gives an example of a decision maker who chooses between receiving two million dollars now and receiving one million dollars now as well as one million dollars at a later moment of time. If this later moment of time is sufficiently close to the present moment, a decision maker with a concaveFootnote 1 utility function prefers the delayed split payment due to Jensen’s inequality. As Baucells and Sarin (2007a, 2007b) put it: “… we would like the intertemporal models to satisfy … local substitutability: as the time distance between periods becomes small, the marginal rate of substitution should approach one. Otherwise, a receipt of two dollars, as compared to one dollar now and one dollar a bit later, could produce a utility jump.” Blavatskyy (2016) proposed a generalization of the present discounted value—rank-dependent discounted utility—which cannot increase when one payoff is split into two payoffs, one of which is slightly delayed in time. This paper presents a new framework for analyzing intertemporal decision making. Any stream of intertemporal payoffs can be characterized by two objective statistics: its cumulative payoff and its average delay. A cumulative payoff is a desirable attribute and an average delay is an undesirable attribute. A decision maker, who decides over time, trades off the cumulative payoff of a stream of intertemporal outcomes against its average delay according to her subjective time preferences. A very patient decision maker opts for a stream of intertemporal payoffs that yields the highest cumulative payoff. A very impatient decision maker opts for a stream of intertemporal payoffs that yields the shortest average delay. Our presented model of intertemporal choice is similar to the mean–variance approach of Markowitz (1952) for modeling risk preferences in financial decision making. Scholten and Read (2010) were the first to consider a tradeoff between differences in valued outcomes and differences in weighted delays. Read and Scholten (2012) extended the model of Scholten and Read (2010) and characterized a sequence of intertemporal outcomes by its cumulative payoff (exactly as in this paper) and its adjusted delay. The average delay considered in this paper is a special case of the adjusted delay (the weighted average) in Read and Scholten (2012). The model of Read and Scholten (2012) overlaps with the model presented in this paper when the adjusted delay in Scholten and Read (2010) is the weighted average and utility function that integrates cumulative payoff and average delay in this paper takes a specific form proposed by Scholten and Read (2010). Scholten et al. (2016) consider a different adjusted delay where time periods are weighted by cumulative utilities of outcomes. Manzini et al. (2010) proposed another model of multi-criteria decision making over time. In their model, a decision maker first compares streams of intertemporal outcomes according to their discounted utility. If one stream is far superior than other streams in terms of discounted utility, then it is chosen straight away. If all streams are similar in terms of discounted utility, a decision maker decides according to the second criterion. This criterion is assumed to be one prominent, possibly context-dependent, attribute (e.g. either outcome or time dimension). Rubinstein (2003) proposed a three-stage procedure for intertemporal choice that closely resembles the similarity approach of Rubinstein (1988) developed for choice under risk. In this procedure, a decision maker first checks for temporal dominance: if one alternative yields a larger monetary outcome at an earlier moment of time, then this dominant alternative is chosen straight away. If neither alternative dominates the other, the decision maker checks if the two alternatives are similar either in temporal or in outcome dimension. If the two alternatives are similar in the temporal dimension but not in the outcome dimension, the alternative with a larger outcome is chosen. If the two alternatives are similar in the outcome dimension but not in the temporal dimension, the alternative with a shorter delay is chosen. If the two alternatives are similar both in the temporal and in the outcome dimension, then a decision maker decides according to another (not specified) criterion. The remainder of the paper is organized as follows. A model of intertemporal choice as a tradeoff between cumulative payoff and average delay is presented in Sect. 2. Section 3 illustrates that time preferences in this model are monotone with respect to advanced payments (unlike in discounted utility theory and many of its generalizations). Section 4 shows that the common difference effect implies a horizontal fanning-out of indifference curves in our proposed model. Section 5 shows that the absolute magnitude effect implies a vertical fanning-in of indifference curves in our proposed model. Section 6 presents an application to the standard consumption-savings problem with a constant interest rate. Section 7 presents an experimental test of the proposed model vs. discounted utility and its popular generalization—quasi-hyperbolic discounting. Section 8 concludes.",1
64.0,2.0,Journal of Risk and Uncertainty,27 May 2022,https://link.springer.com/article/10.1007/s11166-022-09379-8,The limits of reopening policy to alter economic behavior: New evidence from Texas,April 2022,Dhaval Dave,Joseph J. Sabia,Samuel Safford,Unknown,Male,Male,Male,"
""With the medical advancements of vaccines and antibody therapeutic drugs, Texas now has the tools to protect Texans from the virus…. Too many Texans have been sidelined from employment opportunities. Too many small business owners have struggled to pay their bills. This must end. It is now time to open Texas 100%”
 Texas Governor Greg Abbott, March 3, 2021 "" I think it's a big mistake…The last thing -- the last thing we need is Neanderthal thinking that in the meantime, everything's fine, take off your mask, forget it. It still matters.” U.S. President Joseph R. Biden, March 3, 2021 As of October 2021, the COVID-19 pandemic had claimed over 700,00 lives in the United States (Centers for Disease Control & Prevention, 2021a). Non-pharmaceutical interventions (NPIs) such as stay-at-home orders (SIPOs), non-essential business closures, emergency declarations, mask mandates, and limits on in-person gatherings — including capacity constraints at business venues — have been among the most common policy tools used to combat COVID-19 (Courtemanche et al., 2020a, b; Cronin & Evans, 2020; Dave et al., 2020a, 2021,  2022; Friedson et al., 2021; Gupta et al., 2020; Lyu & Wehby, 2020). Many of these policies, while enacted to generate public health benefits through curbing the spread of the pandemic, may also impose economic costs in the short and longer runs due to mobility restrictions and business closures (Viscusi, 2020).Footnote 1 Consequently, the mass distribution of COVID-19 vaccinations by Moderna, Pfizer, and Johnson & Johnson — along with declines in COVID-19 hospitalizations and mortality — created intense pressure on state and local policymakers to begin lifting NPIs, with the goals of improving local labor market conditions and permitting in-person gatherings that would signal a return to pre-COVID normality (Hammer, 2021). At the same time, public health experts have warned that lifting mask mandates or repealing capacity restrictions “too early” relative to the distribution of COVID-19 vaccinations (or progress toward herd immunity) could reverse COVID-19-related health gains. In this vein, Anthony Fauci, Director of the National Institute of Allergy and Infectious Disease, argued that repealing COVID-19 mitigation policies — including mask mandates and limitations on in-person gatherings — would be premature if the rate of decline in a state’s COVID-19 cases had plateaued (Porterfield, 2021).Footnote 2 On the other hand, the effects of enacting or repealing NPIs may be more limited than policymakers or public health officials expect. While there is evidence that particular NPIs — notably, SIPOs and statewide mask mandates — were effective in curbing COVID-19 spread early in the U.S. pandemic (Courtemanche et al., 2020a, b; Dave et al., 2021a,  2022; Friedson et al., 2021; Lyu & Wehby, 2020), a number of studies have documented that NPIs account for a relatively small share of the total variation in individuals’ COVID-19 mitigation behaviors (see, for example, Gupta et al., 2020; Cronin & Evans, 2020). In contrast, most of the variation can be attributed to voluntary (non-policy-related) private demand-side responses, likely due to (i) new or updated information on the novel coronavirus, or (ii) changes in individuals’ assessments of contagion risk and developing serious COVID-19 symptoms. Along the same lines, there is evidence that much of the variation in local unemployment during the pandemic is not attributable to lockdown policies, but rather to voluntary demand-side responses (Chetty et al., 2020; Goolsbee & Syverson, 2021). In addition, the enactment (or repeal) of NPIs could also be accompanied by risk compensating behaviors that may offset expected policy impacts (Dave et al., 2020a, b, c; Yan et al., 2021). Moreover, COVID-19 restrictions (and reopenings) may have very different effects at different phases of the pandemic, in part because the mechanisms through which early policies might have affected behavior (i.e., through information) are less salient late in the pandemic. For instance, a recent study showed that while an initial statewide lockdown in Wisconsin (enacted in late March 2020) increased stay-at-home behavior and curbed the growth of COVID-19 in the state, an unexpected reopening less than two months later had little effect on social mobility or COVID-19-related health (Dave et al., 2020c). The authors attribute this asymmetry, in part, to (i) a smaller role for information shocks in the period following the initial wave of the U.S. pandemic (March–April), and (ii) the elasticity of demand for mitigation behaviors (i.e., mask-wearing, social distancing) with respect to policy becoming smaller (more inelastic) in absolute value over time. With these points in mind, the impacts of a full statewide reopening late in the U.S. pandemic — enacted during a period of mass vaccination — on social mobility, COVID-19 cases (and mortality), and economic activity are not prima facie clear. On the one hand, a reopening may increase population mobility, reduce social distancing, and perhaps even shift individuals’ risk perceptions downward, thereby reducing individuals’ vigilance in engaging in COVID-19 precautionary behaviors. While this may increase economic activity in the short run, effects on COVID-19 spread depend on the extent to which these activities translate into a higher infection risk. As more individuals get vaccinated, for instance, this risk would be moderated, though its degree of moderation could, in theory, be offset by moral hazard effects of vaccinations. Alternatively, it is also possible that the state’s reopening may have much smaller effects on social distancing, COVID-19 cases, and unemployment. If social distancing behavior and economic activity are more a function of the demand shocks caused by the pandemic or more a reflection of voluntary private responses to COVID-related risk assessment rather than a consequence of the mitigation policies per se, then the state’s reopening may do little to change the underlying drivers of individual behavior. This could affect consumers’ willingness to make in-person visits to business establishments and employees’ willingness to work. Furthermore, the generosity of unemployment compensation benefits available to workers — which were expanded as part of President Biden’s March 2021 coronavirus relief bill to a maximum of $300 per week and were extended through September 6, 2021 — could create disincentives for low-wage employment, particularly in industries where the risk of contagion is relatively higher (i.e., indoor bars and restaurants). In addition, if pre-reopening capacity constraints and mask wearing policies were not well-enforced by the state, then the impact of the reopening itself may be muted. Moreover, even if the initial mitigation policies were binding and effective, Bayesian updating of coronavirus risk perceptions mean that if these policies are later lifted, individual behaviors may remain sticky and not respond straightaway (Dave et al., 2020c). The lifting of restrictions may also have little to no effect on population-level social distancing or COVID-19 cases if there are offsetting behaviors among different segments of the population. For instance, while the reopening might cause some residents to increase their mobility and activities outside the home, others may respond by readjusting their perceived infection risk upwards and engaging in greater mitigation behaviors. Support for such compensating responses is found in empirical analyses of Black Lives Matter protests in the summer of 2020 (Dave et al., 2020b), President Trump’s May 2020 campaign rally in Tulsa, Oklahoma (Dave et al., 2020a), and the January 6, 2021 U.S. Capitol Riot (Dave et al. 2021b). Associated with each of these events, there is evidence that local residents increased stay-at-home behavior and reduced their visits to restaurants and bars in response to perceived higher risk of violence and infection (Dave et al., 2020a, b, 2021b).Footnote 3 The net effect on COVID-19 spread, therefore, is unclear.Footnote 4 Finally, the effects of a statewide reopening on population-level health depends on who is nudged by the reopening into altering their social distancing and economic behaviors. While the reopening policy effect we will estimate is an intention-to-treat (ITT) effect (an average population effect), the ITT is identified off a “local” margin, based on those individuals who are actually impacted by the reopening. Effects on community-level COVID-19 spread would then depend on whether these marginal individuals are higher or lower risk for COVID-19 contagion relative to the average individual in the community. This study explores a unique policy shock in Texas to identify the causal impacts of a statewide reopening on public health and economic activity. In many respects, Texas provides an ideal laboratory to help shed light on important questions relating to how private risk-taking behavior responds to the removal of restrictions, during a phase of the pandemic when vaccines are widely available and individuals’ risk-related beliefs may have already been “baked in” and potentially less malleable to public policy actions. Texas was the first state in the United States to enact a “100% reopening”. Executive Order GA-34, issued by Governor Greg Abbott, (i) eliminated statewide capacity constraints on all businesses, and (ii) abolished the statewide mask mandate (Abbott, 2021). Texas’ “first mover” position makes the state’s reopening plausibly exogenous relative to other later-reopening states that followed suit and eased restrictions. Under Governor Greg Abbott’s order, local businesses were free to impose their own voluntary restrictions. Furthermore, unlike the imposition of local shelter-in-place orders which were permitted and widely adopted (Dave et al., 2020a), Governor Abbott advanced the legal position that no local order can supersede the state’s reopening order and legally impose COVID-related capacity constraints on local businesses or fine local residents for not wearing masks.Footnote 5 At the time the reopening was announced, the state of Texas had administered 5.7 million vaccine shots to its residents, fully vaccinating 11 percent of its adult (ages 16 and older) population (Centers for Disease Control & Prevention, 2021b). By March 29, all adults 16 and older were eligible to obtain a vaccine (Harper, 2021) and by April 13, 15.2 million vaccines had been distributed in Texas (Johns Hopkins University, 2021), with 26 percent of the adult population completely vaccinated.Footnote 6 This share had reached nearly 40 percent by mid-May 2021. This study is the first to examine the impact of a statewide reopening in the midst of a mass statewide vaccination effort.Footnote 7 We document three key findings. First, using anonymized smartphone data from SafeGraph, Inc. and a synthetic control approach, we find that the Texas reopening had little impact on stay-at-home behavior or on foot traffic at numerous business locations, including restaurants, bars, entertainment venues, retail establishments, business services, personal care services, and grocery stores. Second, using COVID-19 case and mortality data from the New York Times, we find no evidence that the reopening affected the rate of new COVID-19 cases in the five-week period following the reopening.Footnote 8 In addition, we find that state-level COVID-19 mortality rates were unaffected by the March 10 reopening. These null results persist when we explore heterogeneity in the state reopening by urbanicity and political ideology of Texas counties.Footnote 9 We find no evidence of social distancing or COVID-19 effects of the reopening across more urban versus less urban Texas counties as well as across counties where the majority of residents supported Donald Trump or Joe Biden in the 2020 presidential election. Finally, we explore whether Governor Abbott’s reopening order generated short-run economic growth in Texas. Using weekly state-level data on UI claims per 1000 covered jobs from the Bureau of Labor Statistics (BLS), synthetic control and difference-in-differences estimates show that neither continued UI claims filed nor new UI claims filed (per 1000 UI covered job) fell in the five-week period following the March 10 reopening. Moreover, using state-level data from the St. Louis Federal Reserve Economic Data (FRED), we find no evidence that the Texas reopening reduced the unemployment rate or employment-to-population ratio in the months following the reopening. Supplemental analysis of microdata from the Current Population Basic Monthly Survey (CPS-BMS) show no evidence that that the reopening affected employment-to-population ratios at bars, restaurants, or entertainment venues. Taken together, our findings underscore the persistence of individuals’ risk-related beliefs and behavior, and consequently the limits of late-pandemic era COVID-19 reopening policies to alter such behavior and elicit large responses from the private sector.",
64.0,2.0,Journal of Risk and Uncertainty,02 June 2022,https://link.springer.com/article/10.1007/s11166-022-09375-y,"Fatalism, beliefs, and behaviors during the COVID-19 pandemic",April 2022,Jesper Akesson,Sam Ashworth-Hayes,Itzhak Rasooly,Male,,Male,Mix,,
64.0,2.0,Journal of Risk and Uncertainty,22 March 2022,https://link.springer.com/article/10.1007/s11166-022-09374-z,How does risk preference change under the stress of COVID-19? Evidence from Japan,April 2022,Yoshiro Tsutsui,Iku Tsutsui-Kimura,,Male,Unknown,Unknown,Male,"In this study, we explored if risk aversion changed in a short period of three months in the context of the coronavirus disease (COVID-19) pandemic.Footnote 1 In modern economics, risk preference, time discounting, and social preferences are key concepts that determine the behavior of individuals and, in turn, social outcomes including economic performances. While individuals are characterized as having different preferences, traditional economics assumes that preferences are stable or fixed over their lifespan, which makes the analysis simpler.Footnote 2 With the availability of large-scale panel data on individuals’ preferences, we empirically examined whether the preferences are stable at the level of an individual. While previous studies have reported stable risk tolerance,Footnote 3 we review the recent studies that challenge this assumption. We first classify the studies on change in risk preference into those that employed surveys and those that used economic experiments.Footnote 4 The former are further classified into those investigating the effects of the financial crisis of 2008 and those examining how natural catastrophes or violent conflicts affect risk preferences. The former (on the financial crisis of 2008) largely reported an increase in risk aversion (e.g., Necker & Ziegelmeyer 2016), though Gerrans et al. (2015) reported that the risk tolerance of investors is stable. For example, Guiso et al. (2018) tested whether investors’ risk aversion increased following the 2008 crisis and found that it increased substantially. They examined the cause of this change and concluded that it was mainly caused by changes in the utility function involving emotion (fear). Schildberg-Horisch (2018) concluded from the studies investigating the effect of the financial crisis of 2008 that the “evidence rather consistently documents an increase in risk aversion.” However, she argued, “it is hard to disentangle whether changes in the willingness to take financial risk reflect changes in risk preferences or beliefs about returns.” This is an important critique of these studies, and we examine this point in Subsection 3.3. Since our study investigated the effect of COVID-19, the latter studies (on natural catastrophes or violent conflicts) could be more comparable to ours than the studies of financial crisis. However, the results are inconclusive.Footnote 5 While Voors et al. (2012), in their analysis of the violence between the Hutu and the Tutsi, reported a decrease in risk aversion, Kim and Lee (2014) reported a decrease in risk tolerance while analyzing the effect of the Korean War. Recently, Hanaoka et al. (2018) performed a difference-in- difference (DID) analysis using large-scale annual survey data and found that men who experienced greater intensity of the 2011 Great East Japan Earthquake (GEJE) became more risk tolerant a year after than the group that had not witnessed it. Bu et al. (2020) conducted a survey at the Wuhan University of Science and Technology in two waves in October 2019 and March 2020, in which students were requested to choose between safety and risk assets. A cross-sectional comparison of the data of March 2020 revealed that those who were quarantined in a region more exposed to COVID-19 (Wuhan city) were more risk-loving than those quarantined in significantly less-affected areas. However, the result of the DID analysis using the data of both waves is inconclusive: the cross terms of the greater exposure and Wave 2 dummies were insignificant. Further, we reviewed the studies that used experimental methods. Typically, they collect subjects and expose them to a stress event, such as requiring them to make a speech or showing horror videos and compare their risk preference with regard to a multiple price list, playing a game, or measure the level of cortisol between the treatment and control groups.Footnote 6 Haushofer and Fehr (2014) reviewed seventeen studies and found that fifteen are consistent with the hypothesis that fear and/or stress decrease risk taking, while reduction of fear and/or stress increase it. They concluded, “the majority of the studies show an unambiguous positive effect of fear and anxiety on risk aversion”Footnote 7 Another excellent review article, Chuang and Schechter (2015), examined eight studies, and concluded, “most research shows that risk preferences are not stable across different settings or games.” Compared with the previous studies, our study is unique in that we could measure the risk preference in real-time during the first cycle of the spread of COVID-19. This enables us to address the temporal dynamics of risk preference across different stages of stress exposure. We initiated a survey on COVID-19 and collected data on risk preference as well as perceptions and opinions about COVID-19. The five waves of the survey, conducted from March to June 2020, clearly showed that people became significantly and largely more risk tolerant between March (Wave 1) and April (Wave 3) when the situation was deteriorating. They continued to become more risk tolerant, but the change became smaller between April and May (Wave 4) and between May and June (Wave 5), when the situation became moderate and died down. Our survey may offer novel evidence compared to the previous studies that were based on the survey method because our respondents were under the stress of COVID-19 and on experiments, because the stress is extremely unpleasant. Our results are consistent with those of Hanaoka et al. (2018), but not with the studies on the financial crisis of 2008. We examined the reasons for this discrepancy in Subsection 3.3, focusing on the identification of the change in risk preference due to the change in risk during the event. Our point is that the change in risk attitude in economics should be related to the change in the form of utility functions, which is measured by the change in risk taking behavior to a given magnitude of risks. At a glance, our results seem to be inconsistent with those using economic experiments. We discuss the reason in Section 4 by exploring the difference between acute and repeated stress. While economic experiments studied the effects of acute stress, what we study in this paper is the effects of repeated stress of COVID-19 over three months. Studies in neuroscience have reported that while subjects, either humans or rodents, show strong transient response (e.g., cortisol level) to an acute stress, the response decreases as the stress is repeated (through habituation). Since people are believed to be facing repeated stress under COVID-19, our finding that people become more risk tolerant may be interpreted as the result of habituation to repeated stress. The remainder of this paper is structured as follows: In Section 2, we explain our survey and method of analysis. In Section 3, we present the basic result that people became risk tolerant during the three months of the study period. Further, we show the results on how risk preference differs with regard to mega and moderate risks. Furthermore, we examine the possible cause of the change in risk preference. In Section 4, we discuss why people became more risk tolerant rather than risk averse under COVID- 19, referring to the habituation to repeated stress found in neuroscience. Section 5 concludes.",2
64.0,2.0,Journal of Risk and Uncertainty,02 April 2022,https://link.springer.com/article/10.1007/s11166-022-09373-0,Perceptions of personal and public risk: Dissociable effects on behavior and well-being,April 2022,Laura K. Globig,Bastien Blain,Tali Sharot,Female,Male,Female,Mix,,
64.0,3.0,Journal of Risk and Uncertainty,19 May 2022,https://link.springer.com/article/10.1007/s11166-022-09380-1,Self-serving dishonesty: The role of confidence in driving dishonesty,June 2022,Stephanie A. Heger,Robert Slonim,Franziska Tausch,Female,Male,Female,Mix,,
64.0,3.0,Journal of Risk and Uncertainty,30 April 2022,https://link.springer.com/article/10.1007/s11166-022-09378-9,"Smoking, selection, and medical care expenditures",June 2022,Michael E. Darden,Robert Kaestner,,Male,Male,Unknown,Male,"Cigarette smoking remains the leading cause of preventable mortality in the United States (HHS, 2014). In 2017, there were roughly 34.2 million current smokers and 55.2 million former smokers in the United States over the age of 18 (Creamer et al., 2014). The economic burden of cigarette smoking is thought to be enormous, and it is a burden that motivates public health policies targeted at smoking. For example, Xu et al. (2015, 2021) estimate that 8.7\(\%\) of United States health care spending in 2010 was due to cigarette smoking and that 60\(\%\) of this cost was paid by public programs such as Medicare and Medicaid. These estimates are similar to a large body of work on the social costs of smoking, which are often cited in benefit-cost analyses of tobacco policies and in civil litigation such as the 1998 Master Settlement Agreement (Manning et al., 1991; Hodgson, 1992; Cutler et al., 2000). Yet results in these highly influential studies were estimated using cross sectional data—comparing health care utilization and expenditures of smokers to never smokers at a point in time. The cross-sectional approach does not address the non-random nature of smoking status and, specifically, how health and smoking are dynamically related over the life course. Thus, current estimates of the economic costs of smoking are likely to be significantly biased, perhaps by an order of magnitude (Darden et al., 2018). In this article, we develop a novel theoretical model of smoking and health care expenditures that is rooted in the Grossman (1972) tradition, and we use the model to guide an empirical analysis of the effect of smoking on healthcare expenditures. The model highlights the non-random nature of smoking; how health, health care expenditures, and smoking evolve with age; and the value of using longitudinal information to identify the causal effect of smoking on medical care expenditures. According to the model, smoking generates two fundamental dynamic selection problems. First, smoking causes poor health and increases the probability of death; second, poor health causes smoking cessation.Footnote 1 Both of these model implications imply that cross-sectional comparisons at a given age are comparisons of never smokers and relatively healthy smokers, which suggests that such comparisons may underestimate the economic costs of smoking. Yet, because smoking causes excess mortality, the expected value of the discounted sum of total medical expenditures over many years may be more or less for smokers. Empirically, we test the predictions of our model for a sample of persons age 65 and older using a unique data merge. We link information about participants of the National Health Interview Survey (NHIS) to longitudinal data from Medicare claims.Footnote 2 These data provide information about current and retrospective smoking status, socioeconomic characteristics, and extensive, high-quality longitudinal information on a person’s expenditure on medical care. With these data, we can describe, in detail, the age pattern of the use of medical care by smoking status (current, former, never) for up to 22 years. Importantly, the data allow us to gauge the role of selection by comparing, for example, the medical care utilization of smokers at age 66 to the medical care utilization of smokers at age 75, when they were 66. In this case, our model predicts the latter group to be healthier than the former group and to have lower expenditures at age 66 because the latter group survives to age 75. Because we only observe smoking behavior once, our estimates measure the differences in the age profiles of medical expenditures by smoking status (current, former, and never smokers) at the age of NHIS response. Results of our analysis are consistent with the theoretical model and lead to several novel findings. First, between ages 66 and 84, cross-sectional comparisons of current, former, and never smokers show that, at a given age, former smokers generate between \({\$}\)1,100 and \({\$}\)1,800 more in annual expenditures than current and never smokers, for whom there is little difference. In contrast to our cross-sectional results, results from our longitudinal analysis suggest that, prospectively, smokers generate significantly more expenditures conditional upon being alive, but significantly less when summed from ages 65 to 84. For example, smokers at age 65 who survive to age 75 incur \({\$}\)4,393.25/year in excess expenditures at age 75 relative to never smokers at age 65. However, at age 65, the expected value of the discounted sum of total of expenditures from ages 65 to 84 is \({\$}\)503.14 less (\({\$}\)124,216.03 vs. \({\$}\)124,719.20) for smokers vs. never smokers, which reflects significant excess mortality. Importantly, the expected value of the discounted sum of total of expenditures is less for smokers regardless of survey age (i.e., the age at which smoking behavior is measured). Conditional on living to age 75, the expected value of the discounted sum of total expenditures is \({\$}\)16,354.83 less for smokers relative to never smokers. Furthermore, we find that, prospectively, excess expenditures among smokers are driven largely by inpatient expenditures on the extensive margin of inpatient expenditures - smokers consistently experience inpatient stays at higher rates than never smokers. Interestingly, retrospective comparisons of smokers and never smokers show that smokers generate significantly less in expenditures largely due to lower outpatient, elective expenditures among smokers. This is not true for former smokers; for example, retrospective annual expenditures at age 69 are \({\$}\)1,100 lower for age 75 smokers but they are \({\$}\)589 higher for age 75 former smokers. To summarize, we find that smokers are more expensive in any given year, but because of excess, smoking-attributable mortality, we find no evidence that smoking is a financial burden on Medicare. Our results inform a significant literature on the lifetime social costs associated with smoking, which have been heavily debated in the context of public policy and litigation. For example, Manning et al. (1989) and Manning et al. (1991) find that cigarette smoking generates excess medical expenditures of roughly \({\$}\)0.26/pack (\({\$}\)0.67/pack in 2022) when averaged over smokers of all ages. In spite of this finding, those authors argue that, because smoking causes reductions in expected longevity, smoking “pays for itself” over the life course through significantly lower retirement and nursing home expenditures. Viscusi (2002) revisits these estimates and, in particular, adjusts for changes in the likely health (mortality) effects of cigarettes associated with changes in the amount of tar in cigarettes. Viscusi (2002) concludes that cigarettes pay for themselves while still generating \({\$}\)0.58/pack (\({\$}\)1.08/pack in 2022) in extra medical expenditures. Our contribution to this literature is to demonstrate that smoking actually generates savings on medical care expenditures within the Medicare population. That is, conditional on living to age 65, smokers generate lower Medicare expenditures relative to nonsmokers over their time in Medicare. We also contribute to the literature on the effects of smoking on Medicaid expenditures. For example, Cutler et al. (2000) describe cross-sectional methods that their team used in the 1990s to study the effect of smoking on expenditures in the Massachusetts Medicaid program, estimates of which were influential in 1998 Master Settlement Agreement in which the tobacco industry settled with 46 states that brought suit to recover excess medical expenditures incurred by public payers. They found that in both physician and inpatient services, former smokers generate the highest expenditures. Our cross-sectional results are similar; at a given age, former smokers generate between \({\$}\)1,100 and \({\$}\)1,800 more in annual expenditures than current and never smokers, for whom there is little difference. This result highlights a key prediction from our model that cessation may be preceded by poor health. Cutler et al. (2000) also reported that current smokers have modestly greater healthcare spending than never smokers mainly due to costly inpatient stays. These estimates are likely downward biased because the unmeasured health of smokers is likely better than never smokers. In fact, when we condition on the sub-sample of individuals not observed to die during the period of analysis (i.e. survivors) - as may be the case in a Medicaid population of younger smokers, we observe small to no difference in healthcare expenditures between current smokers and never smokers. These findings demonstrate the importance of unmeasured health and the likely bias of estimates in Cutler et al. (2000). However, Cutler et al. (2000) do not address mortality and they do not measure the difference in the lifetime Medicaid costs of smoking (current or former smokers). We document that cumulative Medicare healthcare expenditures, which are affected by mortality and the number of years of spending, are no different between smokers and never smokers. For any given birth cohort, our results imply that lifetime Medicaid expenditures would be similar if not lower for smokers despite being greater in any given year. While Cutler et al. (2000) study a younger sample, much of the greater costs of smoking they report is associated with long-term care and inpatient care among older persons in their sample, so our results for an older cohort of persons are likely relevant, but some caution is warranted given the difference samples, objectives and, approaches. Our work emphasizes the importance of longitudinal data in the context older smokers who experience significant excess mortality. Our results have several health and economic implications. The Food and Drug Administration (FDA), which has regulatory authority over cigarettes, conducts benefit-cost analyses on proposed cigarette restrictions and on the value of harm reduction related to nicotine-delivery products. The magnitude of the health care cost of smoking is a critical input in FDA cost-benefit analyses. For example, consider the benefit-cost analysis of the recent FDA proposal to ban menthol-flavored cigarettes.Footnote 3 To the extent that such a ban reduces the prevalence of cigarette smoking, the benefit-cost analysis of this regulation requires an understanding of the extent to which changes in traditional smoking due to these regulations will cause changes in future health expenditures (Meltzer, 1997). Our results suggest significantly higher expenditures relative to cross-sectional comparisons. As a result, using cross-sectional estimates will understate the excess healthcare costs associated with additional cigarette smoking among older Americans. However, our results also suggest that lifetime healthcare costs of smoking may be relatively small and that the healthcare benefits (savings) of reductions in smoking may be relatively small too. Results from our study also inform many other benefit-costs analyses, including raising the legal purchase age of cigarettes to 21 (Ahmad, 2005); the benefits of early childhood interventions that affect smoking (Belfield et al., 2006); and the voluminous literature on increases in state and local cigarette taxes (Ahmad & Franz, 2008; Congressional Budget Office, 2012). Our results also inform health policy beyond Medicare. For example, the Affordable Care Act allows insurers to charge smokers up to a 50\(\%\) surcharge on insurance plans and the surcharge is not eligible for federal subsidies (Kaplan et al., 2014). Because the major health implications of smoking are not realized until an individual’s 50s and 60s (Doll et al., 2004; Darden et al., 2018), our results suggest there is little justification for this surcharge for younger smokers - smokers not at risk of death are, if anything, significantly cheaper because of lower outpatient utilization. Furthermore, several studies have shown that surcharges in health insurance premiums for smokers have led to fewer smokers being covered, and smokers, who are of lower socioeconomic status, are an already vulnerable group. An arguably better policy would be to encourage smokers to obtain insurance and make use of subsidized medical services (e.g., physician counseling) that encourage cessation prior to the major health implications of smoking. Even amongst the Medicare population, our results suggest that the overall health care expenditure implications of smoking are economically insignificant.",1
64.0,3.0,Journal of Risk and Uncertainty,20 April 2022,https://link.springer.com/article/10.1007/s11166-022-09376-x,Risk-taking and others,June 2022,Annika Lindskog,Peter Martinsson,Haileselassie Medhin,Female,Male,Unknown,Mix,,
64.0,3.0,Journal of Risk and Uncertainty,23 May 2022,https://link.springer.com/article/10.1007/s11166-022-09381-0,Strength of preference and decisions under risk,June 2022,Carlos Alós-Ferrer,Michele Garagnani,,Male,Female,Unknown,Mix,,
65.0,1.0,Journal of Risk and Uncertainty,27 July 2022,https://link.springer.com/article/10.1007/s11166-022-09385-w,Chance theory: A separation of riskless and risky utility,August 2022,Ulrich Schmidt,Horst Zank,,Male,Male,Unknown,Male,"Suppose your monthly income is made up of a fixed salary component, y, and a bonus, c. The bonus partly depends on factors outside your control (e.g., market demand for the product you are selling) such that it is obtained only with probability \(0<p<1\). Further suppose you are planning a trip to Galápagos Islands next month and need to book a hotel right now as otherwise everything will be booked out. If you can afford a five-star hotel only in case the bonus is actually paid and cancellation is sufficiently expensive, you will end up booking a four-star hotel (which you can afford from the fixed salary), although you might have preferred the better rated accommodation. This describes a situation of a temporal context, i.e., some decisions must be made before the resolution of uncertainty, and the example illustrates an instance where a sure outcomes may yield different utility than the risky ones. For such intermediate decisions, the minimal outcome of an uncertain choice alternative plays a special role as it is a sure payoff that can instantly be incorporated into current wealth holdings and, thus, is available for the execution of part of a plan before uncertainty resolves (e.g., for immediate consumption purposes). Indeed, it is the common practice of most banks to finance loans by mainly considering the sure salary component as basis for an individual’s credibility to pay back a mortgage on a property. We propose a model of decision making under uncertainty called chance theory (CT); it incorporates a differential treatment of sure versus uncertain outcomes. As the sure component of the salary in the above example can lead to higher utility than the risky bonus, CT evaluates the corresponding lottery \((1-p,y;p,y+c)\) by where v is called the utility for wealth and u is referred to as utility for chance.Footnote 1 In agreement with its prominent role, the sure component of an act has special value under CT. All other outcomes of an act are seen as potential extra payoffs which may be available in addition to the sure component. These genuinely uncertain increments for improvement cannot immediately be used, except for planning conditional on the resolution of uncertainty. Consequently, the chances are of a different value to the decision maker.Footnote 2 Note that the minimal outcome of an act is not a reference point in CT. In reference-dependent models like prospect theory (PT; Kahneman & Tversky, 1979; Tversky & Kahneman, 1992) a reference point is formulated for a given choice situation and all alternatives are evaluated with respect to this reference point; indeed, the recent PT-foundation for risk in Werner and Zank (2019) shows how the reference point of PT emerges endogenously from preferences and proves that it must be unique and common to all lotteries. By contrast, in CT, as each alternative may have a different minimal outcome, it is evaluated with respect to that alternative’s own minimal outcome. In this sense CT is also different to behavior where the choice among two lotteries depends on the best of the minimum outcomes of those lotteries, as suggested in the recent study of Baillon et al. (2020) or in other existing reference-dependent models which feature gains and losses (e.g., Sugden, 2003; Bleichrodt, 2009; Schmidt et al., 2008). At best, one may regard the reference-dependence in CT as being lottery dependent.Footnote 3 Having two separate utility function for risky (u) and riskless outcomes (v) in CT may be desirable from a methodological point of view. In expected utility (EU) both outcome types are evaluated using the same von Neumann-Morgenstern utility. As already noted by Savage (1954) and Luce and Raiffa (1957), the latter utility does not necessarily measure strength of preference for sure outcomes as it reflects both attitude towards wealth and risk attitude without the possibility to identify the two components separately. A concave utility in EU may, therefore, merely result from decreasing marginal utility of wealth and need not reflect an intrinsic aversion towards risk (Dyer & Sarin, 1982). Indeed, the main motivation for the dual theory of Yaari (1987) was two develop a model where risk aversion and decreasing marginal utility of wealth are separated. In the dual theory an agent can be risk averse although marginal utility of wealth is constant. In CT this separation is in some sense more general since v is not required to be linear. As under CT risk attitudes in the strong sense of Rothschild and Stiglitz (1970) are captured entirely by the curvature of u, whereas attitude towards wealth are reflected solely through the curvature of v, the separation of both concepts is achieved exclusively in terms of attitudes towards outcomes and without invoking separate measures that capture attitudes towards probability such as in Yaari (1987), Quiggin (1982) or Kahneman and Tversky (1979). Extensions of CT that incorporate probability weighting are feasible, and descriptively they may be desirable; we do not explore them here. The aspect we seek to capture is the separate perception of risky versus riskless outcomes, so we focus only on the utility scale. Indeed, empirical evidence suggests that there may be a fundamental difference between riskless and risky utility. The common consequence and common ratio effect of Allais (1953) rely on the existence of safe options. If these options are moved slightly away from certainty, the violation rate of expected utility (EU) is dramatically reduced (Cohen & Jaffray, 1988; Conlisk, 1989). In general, EU seems to perform rather well when only risky options over common outcomes are considered (Camerer, 1992; Harless & Camerer, 1994; Hey & Orme, 1994; Starmer, 2000). More recently, Andreoni and Sprenger (2012), Andreoni and Harbaugh (2010) and Callen et al. (2014) also find evidence for a disproportionate preference for certainty. They argue for so-called u-v models that also impose a different utility function for risky options (u) than the utility for riskless options (v). Existing u-v models (Fishburn, 1980; Schmidt, 1998; Bleichrodt & Schmidt, 2002; Diecidue et al., 2004; or Neilson, 1992) can be criticized, however, both from a normative and from descriptive perspective. These models either imply violations of (transparent) stochastic dominance or of transitivity, which appers to rule them out as normative models. Violations of transparent stochastic dominance are observed extremely rarely in empirical studies (Loomes & Sugden, 1998; Carbone & Hey, 2000; Hey, 2001). Likewise, the intransitive preference cycles predicted by the model of Bleichrodt and Schmidt (2002) are opposite to those reported in the experimental literature (Starmer & Sugden, 1989). Consequently, also the descriptive validity of existing u-v models can be questioned. By contrast, CT is consistent with transitivity and stochastic dominance and can therefore be regarded as a model that respects many normative desiderata, especially for the context of temporal uncertainty. Regarding optimal planning in a temporal decision making context, the early examples and discussions in Markowitz (1959, Chapter 10 & 11), Mossin (1969), or Spence and Zeckhauser (1972) indicate that, even if a decision maker acts perfectly rational, unresolved uncertainty may make it impossible to have intermediate decisions made such that they turn out to be efficient ex post. Under EU this implies an increased aversion towards risk, a property that is also captured by CT: consistency with stochastic dominance implies that CT-decision makers always disprefer a lottery to receiving its expected value for sure, i.e., they always display weak risk aversion. For non-temporal settings, such globally consistent behavior may, therefore, be somewhat restrictive. As in real life by far most risky decisions are made in a temporal context, people may be set to give specific attention to the worst outcome and do so even in non-temporal settings. In view of the evidence on different evaluations of risky and riskless options, we think that CT may be a descriptively viable model also in such contexts. As we show below, some mileage in accommodating descriptive phenomena is gained by CT’s built-in certainty effect, which can, for instance, explain the EU-paradoxes of Allais (1953) and Rabin (2000). The distinctive role of the minimal outcome of a lottery, often referred to as security level, is not unique to CT. Lopes (1987) showed empirically that decision makers focus on the security level when deciding between uncertain alternatives. Such evidence motivated the development of models by Gilboa (1988), Jaffray (1988), and Cohen (1992), and more recently Diecidue van de Ven (2008), which propose that the utility function in EU should depend on the security level. These models, however, do not achieve a separation of riskless and risky utility. A complementary approach to u-v models, such as CT, is to give extra weight to the minimal outcome without a separation of riskless and risky utility. A focus on the worst outcome also appears in social welfare analysis (Rawls, 1971) and in finance (Roy, 1952); a similar pessimistic outlook reappears in the context of ambiguity aversion (Gilboa & Schmeidler, 1989), where the maximum of the smallest EU-values over a set of priors is decisive for choice behavior. As a consequence, the empirically found ambiguity seeking for unlikely gains or for likely losses (Kilka & Weber, 2001; Abdellaoui et al., 2005; Baillon & Bleichrodt, 2015; Dimmock et al., 2016; Trautmann & Wakker, 2018) cannot be accommodated. To accommodate the latter, it was suggested to include the opposite, an ambiguity seeking multiple prior evaluation, and invoke a parameter that weights ambiguity aversion and ambiguity seeking in some proportion, leading to the \(\alpha\)-Max-Min-model of Ghirardato et al. (2004). Closer to our setting are, however, the models that make adjustments to EU by keeping “the best and worst in mind” (Chateauneuf et al., 2007; Webb & Zank, 2011; Webb, 2015). In these latter theories attention is, in addition to the minimal outcome, also given to the best outcome of a lottery, while intermediate outcome carry a reduced weight in the evaluation of alternatives (see also Lopes, 1987). Our version of CT does not permit special attention to the best outcome, as an extra dimension to also account for optimistic considerations beyond the degree of freedom that we allow for the worst case outcome (e.g., the four-fold pattern of risk attitude in Tversky & Kahneman, 1992) is not incorporated. That said, the tools presented in this paper could in principle be used to develop extensions of CT that incorporate such potential concerns. While we have not progresed in this direction, we note that an approach to separate the best outcome and the worst outcome from the remaining outcomes of an act, keeping the former are evaluated by the same utility, was presented in Alon (2014).Footnote 4 The next section introduces a simple version of CT and shows how the model can explain the afore-mentioned EU-paradoxes. Implications of CT for wealth and risk attitudes are presented in Sect. 3, where also a simple application to intertemporal decision making is considered. Subsequently, CT for the general case of uncertainty, which includes risk as special case, is formally introduced (Sect. 4). Section 5 provides a behavioral foundation for CT and Sect. 6 concludes. Proofs are contained in the Appendix.",
65.0,1.0,Journal of Risk and Uncertainty,02 July 2022,https://link.springer.com/article/10.1007/s11166-022-09386-9,The impact of risk aversion and ambiguity aversion on annuity and saving choices,August 2022,Eric André,Antoine Bommier,François Le Grand,Male,Male,Male,Male,"Risk aversion and ambiguity aversion are two central behavioral traits affecting economic lifecycle problems, such as saving and portfolio choices. A natural question is whether ambiguity and risk aversion have different impacts, or whether they generate qualitatively similar effects, especially when they are considered simultaneously. Although these traits appear to have certain similarities, with both traits aiming to model attitudes towards (objective or subjective) uncertainty, they have generally been studied in two separate strands of the economic literature. As pointed out in Guetlein (2016), the reason for the lack of joint analysis is that doing this is complicated and may in general lead to non-clear-cut results. Some insights have been given in static setups for self-insurance and self-protection questions (see among others, Treich 2010; Snow 2011; Alary et al. 2013), for the value of information (e.g., Hoy et al. 2014), and for portfolio choice problems (see e.g., Dow and Werlang 1992; Gollier 2011). However, the question has never really been theoretically addressed in intertemporal problems. In this paper, we succeed in deriving non-ambiguous results regarding the joint role of risk and ambiguity aversion in a lifecycle model. A lifecycle model with uncertain lifetime is a natural workhorse for such an analysis. Not only is mortality a large risk in life, but life expectancies are also highly heterogeneous, as measured by so-called life disparity. Furthermore, even if life disparity can be partly explained by socio-economic factors, or differences in health systems, significant unexplained factors remain (Shkolnikov et al. 2003; Edwards and Tuljapurkar 2005; Shkolnikov et al. 2011). Mortality can therefore be seen as both risky and ambiguous. Our results indicate that the demand for annuities decreases with both ambiguity and risk aversion, while the demand for (bequeathable) savings increases. The underlying intuition is that annuities are financial products that pay only when the agent survives, that is in good states of the world, while savings pay independently of the agent’s survival. Thus, investing in annuities can be seen as a bet on the agent’s own survival, with positive pay-offs in good states, but no pay-off in bad states. Quite logically, we find that the willingness to take such bets is reduced by both risk and ambiguity aversion. These findings have several interesting implications. First, they highlight that annuities, which are often presented as an insurance against the “risk” of having a long life, also imply the risk of losing wealth in the case of an early death. When living long is seen as a favorable outcome, annuities then appear to be a risk-taking device (when mortality rates are fully predictable), or an uncertainty-taking device (when survival probabilities are ambiguous). This stands in stark contrast with usual insurance products, such as car, health or unemployment insurance, and leads to opposite results regarding the impact of risk aversion and ambiguity aversion. The second implication of our results is that, qualitatively speaking, risk and ambiguity aversion turn out to have similar impacts. What fundamentally matters is not whether probabilities are known (as in a risk setting) or unknown (as in an ambiguity setting) but more fundamentally the possibility of either adverse realizations (like an early death) or favorable outcomes (like a long life). Risk and ambiguity aversion, though formally different, both reflect a similar willingness to “transfer” welfare from good states of the world to bad states of the world. To derive our results, we focus on a smooth ambiguity model à la Hayashi and Miao (2011), which nests some standard models such as that of Klibanoff et al. (2009) or the recursive specification of Epstein and Zin (1989). We analyze the saving decisions of an agent who is both ambiguity and risk averse, and who may invest both in risk-free bonds and in annuities. Mortality is the sole risk faced by the agent, and she can live at most for two periods. The key feature of our approach is that we additionally assume that the agent’s preferences are monotone with respect to first-order stochastic dominance. This property enables us to jointly characterize the respective roles of ambiguity and risk aversion. What does the monotonicity property imply? In a nutshell, monotonicity prevents agents from opting for dominated choices. More precisely, if two choices are available, with the first one yielding preferred outcomes in all circumstances, then an agent with monotone preferences will always prefer the first choice to the second. This property has already been studied in a risk setting. Bommier et al. (2017) show that the only Kreps and Porteus (1978) monotone preferences able to disentangle risk aversion and intertemporal elasticity of substitution are the so-called risk-sensitive preferences introduced by Hansen and Sargent (1995). These preferences have proved to be useful for characterizing the role of risk aversion for various consumption-saving problems. For instance, in a general infinite-horizon setting, Bommier and Le Grand (2019) show that once monotonicity is imposed, risk aversion unambiguously increases precautionary savings. While the impact of monotonicity has already been explored in a risk setting, we are not aware of any extension to an uncertainty setting, apart from the representation results provided in Bommier and Le Grand (2014a) and Bommier et al. (2017). In particular, no application has been developed, leaving the practical implications of using monotone preferences in an ambiguity setting unclear. The current paper fills this gap, showing that monotone preferences can be used to derive clear-cut and intuitive predictions regarding the impact of risk and ambiguity aversion on savings and annuity purchases. As already mentioned, we find that greater risk aversion or greater ambiguity aversion tends to reduce annuity purchases and enhance investments in bonds. To be fully precise, we prove that a higher ambiguity aversion, while maintaining risk aversion constant, leads to higher holdings of riskless bonds but smaller holdings of annuities. This result holds for interior solutions, where agents purchase positive quantities of bonds and annuities. We also derive slightly different results for corner solutions, where either annuity or bond holdings hit non-negativity constraints and are equal to zero. With respect to risk aversion, an increase in risk aversion in the sense introduced by Guetlein (2016) – which does not preserve ambiguity attitudes – typically leads to non-clear-cut results. However, a compensated or “net” change in risk aversion, where the ambiguity parameter is modified together with the risk aversion parameter so as to keep ambiguity attitudes unchanged, is shown to have unambiguous implications, with an impact on asset demands similar to that of an increase in ambiguity aversion. Overall, uncertainty aversion (be it greater risk aversion or greater ambiguity aversion) appears to be a natural candidate for explaining the low annuitization level observed in the data (see Johnson et al., 2004 for empirical evidence in the US). This confirms the early findings of Bommier and Le Grand (2014b), who examined risk aversion, and those of d’Albis and Thibault (2018), who focused on ambiguity aversion in a static, one-period model. To the best of our knowledge, this is the first paper to derive clear-cut results regarding the joint impact of risk and ambiguity aversion in an intertemporal framework. Our article obviously connects to the literature that discusses the roles of risk and ambiguity aversion separately. In the risk setting, there is an abundant literature on precautionary savings (see, among others, Drèze and Modigliani 1972; Kimball 1990; Bleichrodt and Eeckhoudt 2005; Courbage and Rey 2007; Kimball and Weil 2009; Jouini et al. 2013; Nocetti 2016) and on annuity choices (Yaari 1965; Davidoff et al. 2005; Lockwood 2012; Pashchenko 2013, among many others). The role of risk aversion is, however, not often studied because most of the literature relies on the standard time-additive setup, which makes it impossible to isolate the role of risk aversion, since intertemporal elasticity of substitution and risk aversion are intertwined. Among the exceptions, we find papers that use recursive frameworks, such as van der Ploeg (1993), Weil (1993), Kimball and Weil (2009), and several others. Nevertheless, most of these contributions rely on non-monotone preferences (notably those using the most popular Epstein-Zin specification with an intertemporal elasticity of substitution different from 1), with recursivity and monotonicity being combined in very few papers, including van der Ploeg (1993), Tallarini (2000) and Bommier and Le Grand (2014b, 2019). None of these articles feature ambiguity aversion. The literature on ambiguity aversion developed after that on risk aversion, but grew very rapidly.Footnote 1 Most theoretical and experimental contributions initially focused on static settings. Intertemporal problems in an ambiguity setting are typically addressed using recursive extensions of standard static ambiguity models, though without having enough flexibility to change both risk and ambiguity aversion. We note, among others, the analyses of Osaki and Schlesinger (2014), Berger (2014), and Kajii and Xue (2016), who examine the precautionary savings of ambiguity-averse agents, or the study of Collard et al. (2018), who investigate whether ambiguity aversion can explain historical values of the equity premium. All of these papers focus on ambiguity aversion, while restricting their analyses to models that reduce to the standard time-additive model when uncertainty is purely objective (in cases where there is no ambiguity). In other words, they retain the lack of flexibility of the standard time-additive model, which makes it impossible to explore the role of risk aversion. An exception is Peter (2019), who considers a more flexible framework, relying on a non-separable, two-period utility function, although the author does not investigate the role of risk aversion. The current paper introduces flexibility by using the recursive framework of Hayashi and Miao (2011), a route also followed by more quantitatively oriented papers (see e.g., Backus et al. 2015). Our contribution is notable for its use of monotone preferences, which in addition to being an intuitive assumption, affords great tractability and the ability to derive formal results.",1
65.0,1.0,Journal of Risk and Uncertainty,26 July 2022,https://link.springer.com/article/10.1007/s11166-022-09384-x,Personalized information and willingness to pay for non-financial risk prevention: An experiment,August 2022,Yves Arrighi,David Crainich,Sophie Massin,Male,Male,Female,Mix,,
65.0,1.0,Journal of Risk and Uncertainty,16 June 2022,https://link.springer.com/article/10.1007/s11166-022-09387-8,Choice uncertainty and the endowment effect,August 2022,Christina McGranaghan,Steven G. Otto,,Female,Male,Unknown,Mix,,
65.0,2.0,Journal of Risk and Uncertainty,25 October 2022,https://link.springer.com/article/10.1007/s11166-022-09393-w,An inquiry into the nature and causes of the Description - Experience gap,October 2022,Robin Cubitt,Orestis Kopsacheilis,Chris Starmer,,Unknown,,Mix,,
65.0,2.0,Journal of Risk and Uncertainty,15 October 2022,https://link.springer.com/article/10.1007/s11166-022-09392-x,Risk and rationality: The relative importance of probability weighting and choice set dependence,October 2022,Adrian Bruhin,Maha Manai,Luís Santos-Pinto,Male,Female,Male,Mix,,
65.0,2.0,Journal of Risk and Uncertainty,06 October 2022,https://link.springer.com/article/10.1007/s11166-022-09391-y,Do people care about loss probabilities?,October 2022,Stefan Zeisberger,,,Male,Unknown,Unknown,Male,"To some extent, people try to avoid failures under nearly all circumstances. As a consequence, decision makers pay explicit attention to the probability of such failures. (Cumulative) Prospect Theory (CPT) provides a possible explanation with loss aversion in the form of a kink in the value function or extreme risk aversion by strong curvature. An alternative explanation is that decision makers pay explicit attention to losses and their probabilities, irrespective of the loss sizes, which is neither fully captured by loss or risk aversion in CPT. The contribution of the current paper is testing whether people have an aversion to the probability of unwanted events (failure) such as financial losses; this is done by using various different settings, including typically analyzed choice tasks, but also allocation and investment tasks, with and without prior information about the outcome probabilities in repeated decisions. To illustrate our main idea, consider, for example, two similar prospects (lotteries) or financial assets—A and B—that differ in only one small outcome ε, which is negative (–ε) for A and positive (ε) for B. For a very small ε, all major decision theories (e.g., expected utility theory or CPT) would predict that A and B will become equally attractive. If, however, decision makers pay explicit attention to the overall probability of losing, B is valued higher than A, and the magnitude of this difference depends on the probability of that outcome occurring. Such preferences are not part of the traditional decision theories, such as mean–variance, or even descriptively powerful ones, such as CPT, at least not for the typically elicited preference parameters and functional forms (and as we show later on, not even for the more extreme cases). Despite the potentially consequential effects of explicit attention to loss probabilities, it is extremely difficult to analyze such preferences using field data. Even in finance, in which there is a lot of data available, for example, in the form of individual trading data, there is hardly any case in which there are objective, known, and/or stable probabilities. Such a setting, however, is necessary to differentiate between competing explanations based on preferences, in particular, to separate our hypothesis from the pure effect of loss aversion. As an aggravating factor, it is challenging to disentangle the effects of beliefs (e.g., investors’ forecasts about future return distributions, including probabilities) and preferences (e.g., loss, or risk aversion). Similar arguments hold true outside the finance domain, maybe except for casino gambling. Therefore, we use a series of experiments that allow us to control for beliefs by holding outcome distributions constant over time. Roy (1952) was the first to propose the idea that decision makers try to maximize the likelihood of success (hence minimizing the probability of failure).Footnote 1 Some implicit experimental evidence for an aversion to loss probabilities in one-shot tasks has been presented by Payne et al. (1980, 1981), Lopes and Oden (1999), Payne (2005), Sokolowska (2006), Levy and Levy (2009), and Qiu and Weitzel (2012). Many of these tasks can be interpreted as relatively complex, which might be a driver for the observed behavior.Footnote 2 Zeisberger (2022) and Holzmeister et al. (2020) provide evidence that investor risk perception in multi-outcome return distributions is linked to loss probabilities. It can be argued that experimental results lack external validity because of low incentives, but Levy and Levy (2009) demonstrate that this preference pattern also holds true if relatively large real stakes of up to $1,500 are involved, as well as for financial professionals, such as mutual fund managers and financial analysts. Some broader field evidence for preferences that are consistent with our hypothesis (but not allowing for a clear distinction with other hypotheses) has been provided by Camerer et al. (1997), who show that New York cab drivers try to achieve a daily income target, Lopes (1987) for farmers, and Payne et al. (1980, 1981) for investment managers. Shefrin and Statman (2000) provide a “behavioral portfolio theory” based on these insights. However, the literature is not as conclusive as what it may seem to be. Diecidue et al. (2015) do not find evidence for loss probability aversion in a task of certainty equivalent elicitation, even for complex lotteries. Complexity might promote the use of heuristics in decision making, and the studies manipulated it by the number of outcomes and use of round versus nonround probabilities. Generally, previous studies have often used rather complex decision situations with, for example, two options and five different outcomes and probabilities each. In the current paper, we present a series of experiments with different decision tasks and rather low complexity. To explore the generalization and boundaries of our findings, we investigate choice, allocation, and investment tasks. In the choice tasks, the participants make choices between two lotteries. In the allocation tasks, the participants are required to allocate a monetary endowment between prospects. In the investment tasks, the participants must decide how much to invest in a lottery. We begin with choice tasks in a one-shot setting, as is often used in the literature. We then explore repeated decisions because choice behavior has been found to differ substantially between one-shot and repeated decisions (see, e.g., Erev et al., 2010; Hoffmann et al., 2013; Klos et al., 2005; Lopes, 1996; Wulff et al., 2015). In particular, when it comes to repeated decisions, decision makers are acting more according to Expected Utility Theory (Wedell, 2011), and they also show a greater preference for options with higher expected values (e.g., Montgomery & Adelbratt, 1982); both effects work against our hypothesis. Probability judgments are improved for repeated decisions (Fox & Hadar, 2006). We also address the question of whether the results are transferable to a situation of (gradually resolved) uncertainty as opposed to pure risk. In the case of uncertainty, people must infer probabilities and outcomes from feedback rather than receive full distributional information beforehand. The latter case resembles the tasks that are typically used in the decisions from experience (DfE) literature. In the DfE paradigm, decision makers experience outcome distributions rather than having them described, and in most studies, they make choices between binary risky options (see, e.g., Erev & Barron, 2005; Wulff et al., 2018; Kaufmann et al., 2013). Importantly, the DfE literature has documented considerable deviations from tasks with descriptive decision problems. Much attention has been given to the overweighting of rare events, which is lowered or turns into the underweighting of rare events (e.g., Abdellaoui et al., 2011; Hertwig et al., 2004). Although part of the effect is driven by a sampling error in the DfE paradigm, the effect does not completely vanish if controlling for it (e.g., Camilleri & Newell, 2010). Interesting for our analysis is the finding that when only experiencing the outcomes in choice tasks, decision makers tend to prefer the option providing a higher frequency of better outcomes, the option that minimizes the likelihood of losing (e.g., probability matching), or the options that have a lower recalled loss frequency (Hertwig & Erev, 2006; Erev et al., 2017), which is consistent with our loss probability aversion hypothesis. In a number of aspects, our study differs from DfE tasks and goes beyond them to more broadly explore possible aversion to loss probabilities, thus also extending the findings of the DfE paradigm. First, as outlined above, next to classically analyzed choice tasks, we additionally analyze investment and allocation tasks and find support for our hypothesis in all these settings. Second, although we analyze situations in which the outcomes and probabilities must be experienced over time as in DfE, in many of our settings, the outcome distribution is clearly communicated beforehand (decisions under risk). Furthermore, we focus on the tasks in which there are no rare events, and by this, we circumvent the discussion on the over- and underweighting of probabilities (and differences therein between different settings) by design. Our results are as follows: In all tasks—choice, allocation, and investment—we find evidence for decision makers paying explicit attention to loss probability. In choice tasks, the participants tend to prefer options with lower loss likelihoods, even if these are less attractive under other measures. In the allocation and investment tasks, the participants allocate or invest significantly more in lotteries with low probabilities of losing, even though these lotteries are dominated in a mean–variance framework and are also less attractive in a CPT evaluation for the typically assumed preference parameters. To guarantee the robustness of our results, we allow for large variations of typically assumed preference parameters in the CPT framework. We find our hypothesized effects are independent of whether the participants are informed about the outcome distribution (“risk”) or whether they have to infer it from the outcome feedback/experience (“ambiguity”). A “classical” loss aversion explanation (kink at the reference point of the value function or elevated weighting function for losses) does not provide a convincing alternative explanation, nor do the different shapes of the probability weighting functions in CPT. We therefore suggest to have an adaptation of CPT to capture explicit attention toward gain and loss probabilities. Diecidue and Van De Ven (2008)Footnote 3 propose a model in which the value of a prospect X is calculated according to where the probabilities pi are associated with the outcomes xi. The important terms of Eq. (1) are the latter two, with \({\mu }^{+}\ge 0\) and \({\mu }^{-}\ge 0\) as additional decision weights to account for the overall gain and loss probabilities, P(\({x}^{+}\)) and P(\({x}^{-}\)). The parameters \({\mu }^{+}\) and \({\mu }^{-}\) determine the degree to which decision makers take gain and loss probabilities into account. A higher value \({\mu }^{-}\) compared with \({\mu }^{+}\) implies an aversion to the overall loss probability. v(xi) can take, for example, the form of the Prospect Theory value function but is not restricted to just taking this function. In the current study, we test the hypothesis that \({\mu }^{-}\) is larger than \({\mu }^{+}\), that is, that decision makers are averse to the overall loss probability.",4
65.0,2.0,Journal of Risk and Uncertainty,06 October 2022,https://link.springer.com/article/10.1007/s11166-022-09394-9,Risk and time preferences interaction: An experimental measurement,October 2022,Jeeva Somasundaram,Vincent Eli,,Male,Male,Unknown,Male,"Consider a decision maker (DM) choosing between two options: receiving $100 with 50% chance tomorrow or receiving $100 with 75% chance in 1 year. Such choices are not straightforward, because it involves trading off outcomes across both the probability and time dimensions. Many economic decisions, such as an investor investing in a start-up or a mutual fund (with different levels of riskiness and lock-up-periods), a doctor deciding on treatment options (with risky future health outcomes) for his patient, a manager choosing between different research and development (R & D) investments, a policy maker deciding between different climate change abatement policies, involve such risk-time trade offs. This paper aims to empirically characterize how decision makers actually choose between such risky prospects paid at different time points. Future payments are inherently risky. Therefore, when a decision maker chooses between two risky options in the future, the DM’s choices are not only affected by his risk and time preferences, but also affected by the influence of his risk preferences on time preferences.Footnote 1 Empirical investigations also support this interrelationship between risk and time preferences (Keren & Roelofsma, 1995; Anderhub et al., 2001; Weber & Chapman, 2005; Baucells & Heukamp, 2010; Ida & Goto, 2009; Epper et al., 2011; Andreoni & Sprenger, 2012; Cheung, 2015; Miao & Zhong, 2015). Recognizing this interrelationship, Halevy (2008) developed an axiomatic model that can account for risk associated with future payments using a discounted utility with nonlinear probability weighting.Footnote 2 The model jointly accounts for anomalies in risk and time preferences and explains the behavioral deviations from the discounted expected utility model (Fishburn & Rubinstein, 1982). With a similar focus, Baucells and Heukamp (2012) developed the probability-time trade-off (PTT) model to capture the interaction between risk and time preferences accounting also for the magnitude effect (Chapman & Elstein, 1995).Footnote 3 In their model, the interaction between probability and time depends on outcomes through a weighting function but the utility derived from the outcomes is independent of time. The PTT model imposes a constant trade-off between probability and time delay for a specific outcome size. Alternatively, Gerber and Rohde (2018) followed a different approach to capture risk-time preferences interaction: They allowed the weighting function that captures the probability-time interaction to be independent of outcomes but instead imposed the utility to be dependent on time. One other approach developed recently is the range and sign dependent utility (RSU) model (Kontek & Lewandowski, 2017; Baucells et al., 2018), which transforms outcomes (depending on the range) instead of probabilities. Surprisingly, the RSU model generalizes the PTT model to multiple outcome prospects. The different modeling approaches discussed above capture risk-time preferences interaction and are able to explain the behavioral deviations from discounted expected utility (DEU). Existing experimental studies have shown that time delay produces the same change in preference as the reduction in probability of receiving the outcome would induce. In fact this phenomenon explains why common ratio effect (Allais, 1953) and preference for immediate payment (Loewenstein & Thaler, 1989) is reduced when prospects are delayed and are made risky (respectively) (Keren & Roelofsma, 1995; Baucells & Heukamp, 2010; Andreoni & Sprenger, 2012). Such preference patterns can only be explained by a model that combines risk and time preferences in a non-separable fashion (Baucells & Heukamp, 2012). However, more experimental evidence is needed to understand how the risk and time preferences are combined for different outcome levels, probabilities, and time delays. In other words, there is a need to compare and evaluate alternate approaches to modeling and measuring risk and time preferences interaction.Footnote 4 Our paper aims to fill this gap by (i) providing a clean model free evidence on the interaction between the risk and time preferences for different outcome levels, probabilities, and time delays; (ii) understanding how decision makers incorporate risk and time delay into their evaluations, specifically understanding if the time delay affects the taste (utility) or the probability processing; (iii) comparing and evaluating alternate approaches to modeling risk and time preferences. In order to achieve the objectives, we conducted an experiment where we elicited present certainty equivalents of risky prospects paid at different time points in the future.Footnote 5 We used the certainty equivalents to estimate a simple model (with minimal parametric assumption), which treats time delay as a source of uncertainty (Fox & Tversky, 1995; Abdellaoui et al., 2011a). Our experimental results indicate that there is a significant interaction between the probability and the time delay of receiving the outcomes: subjects were insensitive to time delay for small probabilities of gains but become progressively more sensitive to time delay as the probability of gain increases. On the other hand, we find that the utility a subject derives from his outcomes was not dependent on time at which the subject received the outcomes. Our results, thus offer support to models that capture risk-time interaction using the interaction between probability and time. Further, our results also offer new insights for modeling probability-time interaction. We also compared the fit of existing decision models that capture risk and time preferences. In particular, we compare the classical models that assume no interaction between risk and time preferences (discounted expected utility model, discounted rank dependent utility model, hyperbolic discounting model), models that capture risk-time preferences interaction using a probability-time interaction approach (Halevy, 2008, PTT model, RSU) and models that capture risk-time preferences interaction using a time dependent weighting and utility function (weighted temporal utility, Gerber & Rohde, 2018). Based on the Akaike information criterion (Akaike, 1998), we find that the models that capture risk-time preferences interaction and account for the magnitude effect (RSU, WTU) fit the data better. Our results also show that, when the risk-time preferences interaction is accounted for, the estimated discount rates capturing pure rate of time preferences are lower. We also replicate the above results in a follow-up incentivized online study. The paper is organized as follows. Section 2 introduces the model. Section 3 discusses the experiment and results. Section 4 fits the existing decision models to the data. Section 5 reports the follow-up study. Section 6 presents the discussion of the experimental results and the conclusions.",
65.0,3.0,Journal of Risk and Uncertainty,07 January 2023,https://link.springer.com/article/10.1007/s11166-022-09397-6,Is survival a luxury good? Income elasticity of the value per statistical life,December 2022,James K. Hammitt,Jin-Tan Liu,Jin-Long Liu,Male,Unknown,,Mix,,
65.0,3.0,Journal of Risk and Uncertainty,06 December 2022,https://link.springer.com/article/10.1007/s11166-022-09399-4,Controlling ambiguity: The illusion of control in choice under risk and ambiguity,December 2022,Alex Berger,Agnieszka Tymula,,Male,Female,Unknown,Mix,,
65.0,3.0,Journal of Risk and Uncertainty,25 July 2022,https://link.springer.com/article/10.1007/s11166-022-09382-z,Biased survival expectations and behaviours: Does domain specific information matter?,December 2022,Joan Costa-Font,Cristina Vilaplana-Prieto,,Female,Female,Unknown,Female,"Rational expectations models (e.g., rational addiction models) are grounded on the assumption that individuals accurately form their survival expectations (Yaari, 1965). Consistently, some research has documented that subjective survival expectations are on average consistent with life table probabilities (Hurd & McGarry, 1995; Hurd & McGarry, 2002; Palloni & Novak, 2016). However, more recent studies show that aggregate life table realizations do not account for individual-specific heterogeneity (Gan et al., 2004).Footnote 1 The availability of end-of-life data makes it possible to precisely compare individual level objective and subjective survival expectation evidence.Footnote 2 After examining subjective longevity expectations over an individual’s life cycle, some studies find evidence of an underestimation (overestimation) of subjective survival at younger (older) age (Elder, 2007; Hurd et al., 2009), suggesting a life course explanation for a bias in survival expectations formation. However, do such biases in expectation formation have an effect on behaviour? Does the specific information domain matter in the formation of biased expectations? Does the domain specific bias reveal a consistent effect across different behaviours? This paper examines the formation of individual-level biased expectations across two specific domains, namely one’s own longevity and the (predicted) weather. More specifically, we measure biased survival expectations (BSE) by studying individual self-reports in both main and in end-of-life exit interviews in Europe, and we then compare their actual survival realizations (and predicted survival for those alive) with their expectations. Similarly, we estimate individual-level ‘biased meteorological expectations’ (BME), that is, how individuals’ weather predictions compare to weather realisations. Next, upon documenting evidence of biased expectations, we then examine the impact of such biased expectations on health and financial behaviours. More specifically, we test whether private information drives such effects by exploiting the individual level variation in an individual’s family longevity history (parental age at death) to identify the causal effect of biased expectations on several health (e.g., preventive actions) and financial behaviours (e.g., saving for retirement). The underlying intuition is that, the overestimation of one’s subjective survival can be explained by an individual’s private information about their objective survival probability. This information can arise either from knowledge of either technological or medical reasons influencing a persons survival, including individual-specific genetic information (Martin et al, 2007).Footnote 3 We study different mechanisms including whether the effect of individual biased expectations varies depending on the extent of individual control over different life domains (e.g., higher control over ones health than over the weather). We add to the literature in several ways. First, we take advantage of unique data from the end-of-life questionnaire of the Survey of Health Ageing and Retirement in Europe (SHARE) and its retrospective wave (SHARELIFE), which allows us to precisely estimate individual level survival expectations and compare them to the respondent’s actual observed survival.Footnote 4 Second, we study BSE using longitudinal data from several European countries, which exhibit a large cross-cultural variation in behavioural reference points. These heterogeneity can influence the formation of expectations compared to similar studies using United States data. Third, unlike previous studies, we are able to identify the bias is expectation formation in two domains, including one's own survival (or health), and the expected weather (or methodological expectations). Finally, we extend the previous literatureFootnote 5 by providing causal estimates of the effect of biased expectations on specific financial and health behaviours.Footnote 6 Hence, we contribute to the still growing literature on the formation of biased expectations and specifically, on the role of private information (Kim et al., 2017), as a driver of biased expectations. The next section describes in more detail how this paper relates to the existing literature. Section three describes the data, and the empirical strategy is reported in section four. We report the main results in section five. Section six reports a battery of robustness check, and section seven documents evidence of other potential pathways. A final section concludes.",
65.0,3.0,Journal of Risk and Uncertainty,22 July 2022,https://link.springer.com/article/10.1007/s11166-022-09388-7,Risky choice: Probability weighting explains independence axiom violations in monkeys,December 2022,Simone Ferrari-Toniolo,Leo Chi U. Seak,Wolfram Schultz,Female,Male,Male,Mix,,
66.0,1.0,Journal of Risk and Uncertainty,22 December 2022,https://link.springer.com/article/10.1007/s11166-022-09398-5,Towards a typology of risk preference: Four risk profiles describe two-thirds of individuals in a large sample of the U.S. population,February 2023,Renato Frey,Shannon M. Duncan,Elke U. Weber,Male,,Female,Mix,,
66.0,1.0,Journal of Risk and Uncertainty,14 January 2023,https://link.springer.com/article/10.1007/s11166-022-09400-0,Safe options and gender differences in risk attitudes,February 2023,Paolo Crosetto,Antonio Filippin,,Male,Male,Unknown,Male,"Risk attitudes are a latent construct, and as such can only be indirectly and imperfectly measured. Given the heterogeneous features of the different elicitation methods, it is not surprising that measures of risk attitudes tend to show low correlations across domains and tasks. Such an instability of results has been emphasized also along a gender dimension. This evidence imposes several key requirements in order to obtain a clean test of the effect of safe option on risk taking by gender. In particular, we need to a) exogenously manipulate the presence of a safe option in a task ceteris paribus, i.e. keeping its structure unchanged; and b) replicate the exercise in more than one task, because the heterogeneity of results renders the generalization of results from a single elicitation method a questionable exercise.Footnote 3 The first question to answer is therefore which elicitation methods should be used. We believe that in order to be robust, our findings must rely upon tasks delivering clear and different results along a gender perspective. Significant gender differences are a systematic finding in an ordered lottery choice task à la (Eckel & Grossman, 2002, henceforth:EG) and in the Investment Game by Gneezy and Potters (1997). In contrast, gender differences are rarely found and, when found, small in magnitude in the most widely used risk elicitation task, (Holt & Laury, 2002, henceforth:HL), as documented by the meta-analysis of Filippin and Crosetto (2016). Finally, the behavior of men and women is indistinguishable when preferences are elicited with the Bomb Risk Elicitation Task (Crosetto & Filippin 2013, henceforth:BRET). The likelihood of observing gender differences strongly correlates with the presence and focality of a safe alternative. EG and the Investment Game by Gneezy and Potters (1997) present a focal safe option in the form of a degenerate lottery yielding the same payoff irrespective of the random event. HL does not provide an explicit safe option but allows the subject to get a minimum payoff with probability 1. Finally, the BRET does not allow the subjects to earn any positive amount of money with probability one. The Investment Game and EG share many characteristics. They both feature fixed \(50\%-50\%\) probabilities, they both cannot identify risk neutral and risk loving preferences, and in both tasks, gender differences are a nearly ubiquitous finding. Since in the Investment Game a safe option is present but virtually never chosen, and given the similarity of the two tasks, we decided to focus on EG (together with HL, and BRET). We create new versions of each task either introducing (HL and BRET) or removing (EG) a safe option. Our aim is to reduce changes to a minimum, in order to preserve all the idiosyncratic characteristics of each task but still be able to causally identify the role played by the riskless alternative. Towards this goal we assume that agents are characterized by classic CRRA preferences: where \(\rho \neq 1\) represents the coefficient of relative risk aversion \(\mathrm{and}\;U(x)=\mathrm{log}\;x\;\mathrm{when}\;\rho=1\). This assumption a) allows us to build a treatment version of each task that is isomorphic to the baseline condition under the null assumption that the safe option is irrelevant; b) helps to make results comparable across tasks. Another source of heterogeneity in the results might stem from the repetition of the choice. It has been shown that (part of) the subjects make choices that are even negatively correlated over time (Isaac & James, 2000).Footnote 4 We hence opt for a pure between-subject experiment in which each subject participates in only one experimental condition. A grand total of 1085 subjects took part to our six conditions (3 tasks times 2 treatments design). The distribution of subjects by condition and the breakdown by gender are detailed in Table 1. The sample sizes have been obtained by weighing two different principles: matching existing samples for the baseline conditions, and getting enough power to detect an effect in the treatment conditions under our hypothesis. Related literature shows us that the gender effect in the BRET is nearly zero (\(d = 0.01\), Crosetto & Filippin, 2013), it is \(d = 0.17\) in HL (Filippin & Crosetto, 2016) and \(d = 0.46\) in EG (Nelson, 2014). Under our conjecture that the presence and focality of a safe option drives gender differences in risk attitudes, we expected hence the effect in the safe version of the BRET and HL to reach EG-like levels of \(d \sim 0.45\). Under this assumption, we need 75 subjects per condition to detect an effect at 80% power. The experimental sessions were run in 2014 (BRETsafe, HLsafe, HL extra sessions) at the Laboratory of the Max Planck Institute of Economics, and in 2016 (EGnosafe and EGsafe extra sessions) at the Laboratory of the Friedrich Schiller University, both in Jena, Germany.Footnote 5 The experimental procedures were identical for all tasks.Footnote 6 Subjects entered the laboratory, and instructions were both read aloud and available on screen. The English translation of the original instructions in German is available in Appendix 1. Control questions about the experimental procedure and tasks were asked, and subjects were allowed to continue only after having replied correctly to all questions. Then the subjects faced the task, one shot.Footnote 7 After all subjects had completed the task, they were exposed to a short questionnaire including demographics and a self-reported measure of the perceived complexity of the task. The randomization of the assignment to the six conditions should guarantee a balanced distribution of risk attitudes. However, in order to allow us to control for possible unbalances in mid-size samples like those we gathered, we exposed the subjects to the SOEP self-reported measure of attitude toward risk (introduced in the risk elicitation literature by Dohmen et al., 2011).",
66.0,1.0,Journal of Risk and Uncertainty,27 August 2022,https://link.springer.com/article/10.1007/s11166-022-09390-z,Effect of a brief intervention on respondents’ subjective perception of time and discount rates,February 2023,W. David Bradford,Meriem Hodge Doucette,,Unknown,Unknown,Unknown,Unknown,,
66.0,1.0,Journal of Risk and Uncertainty,16 August 2022,https://link.springer.com/article/10.1007/s11166-022-09383-y,Individual characteristics associated with risk and time preferences: A multi country representative survey,February 2023,Thomas Meissner,Xavier Gassmann,Joachim Schleich,Male,Male,Male,Male,"Economic preferences, such as risk aversion and time preferences, have been found to predict a wide range of individual decisions, such as savings (e.g. Bradford et al., 2017), environmental choices (e.g. Bartczak et al., 2015), and investments in health (e.g. Galizzi et al., 2018) or in retirement funds (e.g. Goda et al., 2019). For policymakers, it is particularly important to identify individual characteristics associated with such preferences, so that policies can be designed for the appropriate target groups (for instance, offering upfront subsidies to socio-demographic groups known to discount the future highly or warranties to socio-demographic groups known to be particularly risk averse). Relationships between individual characteristics and economic preferences have been studied extensively in empirical research. The results reported in these studies are often inconsistent, however, making it difficult to derive clear insights and policies. The inconsistencies may stem from a variety of factors. First, studies make use of vastly different methods to elicit and estimate preferences; some studies use incentivized experimental methods (e.g. Boschini et al., 2019; l’Haridon & Vieider, 2019), while others rely on self-reported measures (e.g. Falk et al., 2018; Görlitz & Tamm, 2020). Second, studies make use of different samples: while some studies utilize large scale, demographically representative samples, many rely on small samples consisting predominantly of students.Footnote 1 Finally, studies differ in how they account for structural dependencies between different domains of preferences. Andersen et al. (2008), for instance, argue that the curvature of utility should be taken into account when estimating discount rates. Abdellaoui et al. (2007) show that failing to account for loss aversion can introduce bias in the estimated parameter of risk aversion.
 In this paper, we start with a broad review of the empirical literature on the relationships between the most studied individual characteristics with risk aversion, loss aversion, time discounting and present bias.Footnote 2 This literature review enables us to identify relationships for which patterns of findings are clear, ambiguous, inconsistent, or missing. As our main contribution, we then present results from a large-scale multi-country study (with over 12,000 respondents) covering demographically representative samples in eight European countries, eliciting risk aversion, loss aversion, time discounting, and present bias, using state-of-the-art methods for elicitation and estimation as well as a wide range of robustness checks. Preferences are elicited using Multiple Price List (MPL) designs, as introduced by Holt and Laury (2002) for risk preferences, and by Coller and Williams (1999) for time preferences. Multiple price lists are incentive-compatible and easy to explain and understand; they also make it possible to elicit risk aversion, loss aversion, time discounting, and present bias using the same design. We use real monetary incentives and account for stake and order effects.Footnote 3 In our preferred specification, we jointly estimate preference parameters to account for their structural dependencies; we also conduct a variety of alternative estimations to examine the robustness of the findings to different specifications, including different ways of modeling the structural dependencies between risk and time preferences. This study employs a rich set of individual characteristics with a wide range of socio-demographic characteristics as well as psychological characteristics such as cognitive reflection (Frederick, 2005) and cultural values (Schwartz, 2012), allowing us to analyze how these characteristics are related to risk and time preferences. Overall, this paper aims at obtaining a better understanding of relationships between individual characteristics and risk and time preferences. To our knowledge, this is the first effort to elicit risk aversion, loss aversion, time discounting, and present bias jointly using multi-country representative samples. The findings contribute to the lively discussion on how risk and time preferences are associated with individual characteristics. Our literature review contributes to the literature through the identification of relationships for which more knowledge is needed (either because previous results are ambiguous or inconsistent or because these relationships have rarely or never been studied previously). The findings from our large-sample multi-country survey then provide valuable orientation for these relationships. Table 1 summarizes our main findings. To highlight some of the results, we find that age and gender correlate with all considered preferences. Income appears to be negatively correlated with risk and loss aversion. We also find robust negative correlations between cognitive ability and risk aversion and time discounting. Interestingly, loss aversion appears to be positively correlated with cognitive ability. Additional findings suggest that design features appear to have significant effects on the estimated parameters: incentivized respondents appear to be less risk averse, present biased, and loss averse, but they tend to discount the future to a greater extent. Stake size as well as order of presentation are also significantly related to elicited preference parameters. In line with mounting evidence on this topic, we find that controlling for decision noise is important:Footnote 4 Failing to do so may lead to spurious correlations between preference parameters and individual characteristics that are correlated with decision noise. In the following, in Sect. 2 we first systematically review the empirical literature on the relation between individual characteristics and risk and time preferences. We present the theoretical framework used in Sect. 3 and the survey design in Sect. 4. In Sect. 5 we report the findings from the joint estimation of the preference parameters as well as results obtained from a series of robustness checks. We discuss the implications of our findings in Sect. 6.",3
66.0,2.0,Journal of Risk and Uncertainty,25 February 2023,https://link.springer.com/article/10.1007/s11166-023-09403-5,The locus of dread for mass shooting risks: Distinguishing alarmist risk beliefs from risk preferences,April 2023,Rachel E. Dalafave,W. Kip Viscusi,,Female,Unknown,Unknown,Female,"Low probability dreaded events tend to generate an alarmist public response (Chilton et al., 2006; Fischhoff et al., 1981; Lichtenstein et al., 1978; Sunstein, 1997; Sunstein & Zeckhauser, 2011; Viscusi, 1985). What is not well understood is the extent to which the alarmist response stems from changes in risk beliefs or is also incorporated in individual preferences regarding the dreaded outcomes. This article examines these issues using original survey data from three separate surveys before the Buffalo mass shooting in 2022, following the Buffalo shooting but before the Uvalde shooting that occurred 10 days later, and following the Uvalde shooting. Our identification strategy has some parallels with the literature on analyses of unexpected events during survey design, as analyzed by Muñoz et al. (2019). Unlike most such unexpected event studies, the expected mass shooting events in our article did not occur during a period while the survey was being administered. Rather, after these events occurred, we undertook two additional waves of the survey to estimate the influence of the mass shooting incidents. These mass shootings were salient, unexpected events that could not have been anticipated before they occurred. Both quantitative and comparative measures of risk beliefs regarding mass shootings were considerably higher following the mass shootings. We examine whether the alarmist change in risk beliefs after the mass shootings is also manifested in the elicited tradeoff rates between mass shootings and other firearm deaths. A closely related issue is whether exaggerated risk beliefs intrude on estimates of mortality risk benefit valuations. Our findings indicate that the tradeoff rates between reducing the risks of mass shooting deaths and other firearm deaths were relatively stable, although there is a mass shooting valuation premium for those who viewed mass shootings as a greater threat. This influence of risk beliefs on expressed preferences is evidence of a more general concern that risk–risk tradeoff studies, and stated preference analyses more generally, should recognize that respondents may also bring their own risk beliefs to bear when indicating their risk preferences. One would expect people to have greater perceived risks of mass shootings following the Buffalo and Uvalde tragedies. It is well established that highly publicized fatality risks are associated with overestimation of mortality risk probabilities. Increasing the assessed risk value after mass shootings may be entirely rational under standard Bayesian models. However, it is also possible that the greater risk beliefs that are observed following these tragedies are not warranted. Most risk belief question structures do not make it possible to assess whether people overestimate or underestimate the risk level. Survey questions, such as those based on Likert scales, provide a general sense of whether risk beliefs are high or low, but do not permit assessment of whether risk levels are overestimated or underestimated compared to an objective reference point. Using several different measures of risk beliefs, it is possible both to analyze the consistency of the higher risk beliefs across different measures and to determine whether quantitative measures of posterior risk beliefs are too high following the Buffalo and Uvalde mass shootings. The dread that a mass shooting evokes may have implications for preferences as well as risk beliefs. The utility functions with respect to death from dreaded events may differ, both from the standpoint of individual utility functions for one’s own death and the value attached to preventing deaths to others. Evidence of differences in the valuation for mass shootings as opposed to other traumatic deaths could provide the impetus for heterogenous estimates of the value of a statistical life and the application of these estimates to the valuation of the costs of different types of crime (Cohen, 2020; Anderson, 1999; Cook & Ludwig, 2000; Ludwig & Cook, 2001; Cohen et al., 2004; Cohen, 2004; Atkinson et al., 2005; Pope & Pope, 2012; Anderson, 2021). Although government agencies generally do not assign a valuation premium for different causes of death, the public’s preferences for different mortality risks may not be symmetric (Dalafave & Viscusi, 2021; Viscusi, 2009, 2018). This article uses a choice experiment structure to examine risk–risk tradeoff rates between two similar and prominent public risks, mass shootings and non-mass shooting firearm homicides. The risks share the same firearm-related method of death. As a result, to the extent that dread arises because of a difference in the nature of the death, as was found to be relevant in the analysis by Chilton et al. (2006), the influence of dread may not be as evident for the mass shooting-other firearm homicide comparison. Nevertheless, mass shootings and other firearm homicides differ in fundamental ways that could affect preferences regarding these two mortality risks. In the case of mass shootings, multiple people may die in one attack, the context of the shooting is often in a locale perceived as very safe, the injurer is often not known to the victim, and the deaths receive more substantial publicity. The dread evoked by mass shooting risks also creates more pronounced overestimation of the risk than for other firearm homicides, which potentially could be transmitted into preferences for reducing these hazards. Section 2 provides background information on mass shootings and previous analyses of dread risks before providing an overview of the survey structure and the sample characteristics. Section 3 summarizes the conceptual model underpinning the analysis of risk beliefs and preferences. Unlike standard formulations in stated preference studies, our model incorporates a role of individual perceptions of changes in the risk level rather than assuming that respondents are indicating their preferences with respect to the values stated in the survey. The examination of several different measures of risk beliefs in Section 4 has a twofold purpose. First, the question formats that are used make it possible to document the increased overestimation of mass shooting risks following the Buffalo and Uvalde incidents. Second, the question framing in our primary risk belief question is designed to serve as a comparative risk measure that can be incorporated in the empirical analysis of risk preferences. Whether the dread that leads to enhanced beliefs is also reflected in greater valuation of mass shooting deaths is tested using a risk–risk tradeoff analysis in Section 5. As observed in the concluding Section 6, for most respondents the exaggerated level of mass shooting risk beliefs following these prominent tragedies is not also embodied in the relative valuation of mass shooting deaths compared to homicides involving other types of firearm violence. There is, however, an impact of risk perceptions on expressed valuations among those who consider mass shooting risks to be greater than other homicide risks. These results are of potential policy relevance with respect to whether mass shootings should be accorded a valuation premium compared to other firearm homicides. The findings also have implications for the design of stated preference studies used to value risks.",
66.0,2.0,Journal of Risk and Uncertainty,12 December 2022,https://link.springer.com/article/10.1007/s11166-022-09396-7,Seen and not seen: How people judge ambiguous behavior during the COVID-19 pandemic,April 2023,Andras Molnar,Alex Moore,George Wu,Unknown,Male,Male,Male,"On May 13, 2021, the United States Center for Disease Control and Prevention (CDC) changed its guidelines such that: “fully vaccinated people no longer need to wear a mask or physically distance in any setting.”Footnote 1 The new guidelines surprised and confused many observers. The guidance was not only a radical departure from previous recommendations, but the absence of vaccination passports and other methods of verifying vaccination status in the United States made behavior a matter of trust. Lisa Maragakis, a John Hopkins epidemiologist, summed this up: “The guidance shifts all the burden onto individuals to be ‘on their honor’ and choose the appropriate actions when deciding whether to wear a mask. There is no way to know who is vaccinated and who is not in most scenarios.”Footnote 2 The COVID-19 pandemic has undoubtedly transformed life. In much of 2020 and 2021, everyday social activities—as mundane as drinking at a bar to giving a friend a hug—were deemed inappropriate, putting others at unnecessary risk. But as vaccination against COVID-19 has become widespread, these judgments have become contingent on vaccination status. While it was once generally viewed as inappropriate for anyone to go to the bar, for many, it is now inappropriate only for the unvaccinated to do so. Vaccination status, though, is largely private. In most circumstances, people are “seen and not seen,” that is, even though we can clearly observe their behaviors, we cannot tell whether they are vaccinated or not. Given this ambiguity about vaccination status, how do we make sense of, and judge, behaviors that are appropriate when vaccinated but inappropriate otherwise? And how do we think about the risks that these behaviors pose to the people around them? As the world transitions out of the COVID-19 pandemic, the ways people judge others’ behaviors in these ambiguous situations has implications for how individuals engage with each other and whether these interactions are contentious or civil. In turn, perceptions of risk caused by these behaviors will also affect whether people enter or refrain from public spaces in which the vaccination status of others is not known or ambiguous. In this paper, we examine behaviors, such as going to the gym or a bar, which are judged more appropriate if the target of the judgment is vaccinated, as opposed to unvaccinated. For the remainder of this article, when we refer to judgment, we mean judgments of the appropriateness (or riskiness) of a behavior. Normatively, the favorability of these judgments should increase with the likelihood that the target is vaccinated. For example, when \(70\%\) of the population in an area is vaccinated, people ought to judge strangers (whose vaccination status is unknown to observers) drinking at the bar as behaving more appropriately than in an area where only \(15\%\) are vaccinated. We document a pattern of evaluation that departs, at times dramatically, from this normative benchmark. Let b be a behavior, and V and \(\overline{V}\), indicate that a person is vaccinated or unvaccinated, respectively.Footnote 3 We denote J(b) to be the judgment of behavior b, where higher values indicate a more favorable evaluation. A behavior is a vaccination-contingent behavior if \(J(b|V) > J(b|\overline{V})\), i.e., a vaccinated person engaging in behavior b is judged more favorably than someone who is unvaccinated. Furthermore, let \(p=\Pr (V|b)\) denote an observer’s belief about the conditional probability of a target being vaccinated given that they are engaging in behavior b. Note that \(\Pr (V|b)\) may be the same, higher, or lower than the prior or unconditional probability of someone being vaccinated in a target population, \(\Pr (V)\). Importantly, although observers’ subjective probability estimates may or may not be accurate, our comparison of judgments to the normative benchmark merely requires that judgments be consistent with the observers’ own (subjective) probability estimates. In a complex and dynamically changing social situation, such as the current global pandemic, one could argue that a variety of judgments are normative. In this article, we use the probability-weighted average of the judgment of vaccinated and unvaccinated individuals as this normative benchmark of judging a behavior b under uncertainty. We do not imply that all people ought to make these judgments in this fashion, but this approach provides a straightforward way to compare observed judgments to a standard. At the same time, it is reasonable to assume that many normative rules for judgment, particularly consequentialist ones, ought to include the probability of vaccination in some way, and this probability-weighted averaging is the simplest possible rule that reflects this. We define the probability-weighted normative standard in the following way: As p gets close to 1 (an observer believes it is almost certain that a target is vaccinated), Eq. (1) approaches J(b|V), the judgment of someone known to be vaccinated. Similarly, as p nears 0, the target person should be viewed the same as an unvaccinated individual, \(J(b|\overline{V})\). To understand how participants think about appropriateness and risk in uncertain situations, we compare judgments of the appropriateness and the riskiness of behaviors to the normative benchmark defined in Eq. (1). To do this, we compare participants’ judgments when they have full knowledge of vaccination status (either vaccinated in the “vaccinated” condition or unvaccinated in the “unvaccinated” condition) with cases where vaccination status is ambiguous. Specifically, we compare judgments made under full knowledge with those made when nothing about a target’s vaccination status is indicated (“no information” condition), as well as judgments of targets whose vaccination status we explicitly describe as “unknown” (“unknown status” condition) and targets who are equally likely to be vaccinated or unvaccinated (“50-50” condition). We document judgments under uncertainty that differ substantially from the normative benchmark that relies on a linear probability weighting model. Although the “no information” and “unknown status” conditions are identical with respect to the information conveyed, evaluations of the target vary dramatically, with evaluations in the “no information” being significantly more positive (i.e., generally appropriate and posing relatively little risk to others) than evaluations in the “unknown status.” Apparently, the normatively relevant consideration of a person’s vaccination status does not reliably come to mind unless prompted, leading to, in most cases, overly generous judgments of targets engaging in vaccination-contingent behaviors. Judgments are more critical when participants are reminded to think about vaccination status in the “unknown status” condition. These judgments sometimes cohere with the normative benchmark, but when the judgments depart from it, evaluations tend to be disparaging and closer to the evaluations of unvaccinated targets (i.e., generally inappropriate and posing relatively high risk). Our paper proceeds as follows. In Sect. 2, we present the materials and methods, while Sect. 3 describes the results. In Sect. 4, we discuss how our findings build on previous empirical findings about dealing with uncertainty and ambiguity and the prevalence of non-consequentialist reasoning. Finally, in Sect. 5, we conclude by discussing some of the implications of our results.",
66.0,2.0,Journal of Risk and Uncertainty,14 July 2022,https://link.springer.com/article/10.1007/s11166-022-09389-6,Pay every subject or pay only some?,April 2023,Lisa R. Anderson,Beth A. Freeborn,Andrew Turscak,Female,Female,Male,Mix,,
66.0,2.0,Journal of Risk and Uncertainty,20 April 2022,https://link.springer.com/article/10.1007/s11166-022-09377-w,On the role of monetary incentives in risk preference elicitation experiments,April 2023,Andreas Hackethal,Michael Kirchler,Annika Weber,Male,Male,Female,Mix,,
66.0,3.0,Journal of Risk and Uncertainty,02 June 2023,https://link.springer.com/article/10.1007/s11166-023-09411-5,Windfall gains and house money: The effects of endowment history and prior outcomes on risky decision–making,June 2023,Hauke Jelschen,Ulrich Schmidt,,,Male,Unknown,Mix,,
66.0,3.0,Journal of Risk and Uncertainty,07 January 2023,https://link.springer.com/article/10.1007/s11166-022-09401-z,Strategic ambiguity and risk in alternating pie-sharing experiments,June 2023,Anna Conte,Werner Güth,Paul Pezanis-Christou,Female,Male,Male,Mix,,
66.0,3.0,Journal of Risk and Uncertainty,14 January 2023,https://link.springer.com/article/10.1007/s11166-022-09402-y,Effects of e-cigarette minimum legal sales ages on youth tobacco use in the United States,June 2023,Michael F. Pesko,,,Male,Unknown,Unknown,Male,"The FDA is currently assessing whether specific e-cigarette products are sufficiently appropriate for public health to be legally sold in the United States. To date, 23 unflavored e-cigarette products from three companies have been approved, thousands of e-cigarette products remain under review, and more than one million e-cigarettes have been denied (which recently includes Juul e-cigarettes).Footnote 1 Approval can be rescinded at any time if insufficient evidence exists that these products are benefiting public health. E-cigarettes that are under review, or that are denied marketing orders but these orders are being judicially appealed, can often be sold through enforcement discretion. One key question in determining whether e-cigarettes are appropriate for the protection of public health is the impact that e-cigarette use has on combustible tobacco use. If e-cigarettes can be shown to causally reduce use of combustible tobacco, which is more dangerous (National Academies of Sciences, Engineering, and Medicine, 2018), such a finding would demonstrate an important public health benefit of e-cigarettes. Trends in cigarette use and e-cigarette use over time support the notion that e-cigarettes may be reducing youth cigarette use in aggregate. In 2009, public health leaders set a goal of reducing youth cigarette use from 19.5% in 2009 to 16.0% by 2019 (Office of Disease Prevention & Health Promotion, 2021). Youth cigarette use reached 6% in 2019, so this objective was exceeded by 386%, potentially due to e-cigarette availability during the decade. This trend has continued its acceleration, and by 2021, high school student cigarette use reached 1.9% (Gentzke et al., 2022). Cigar use has also declined sizably, to 2.1% in 2021 (Gentzke et al., 2022). While these trends are suggestive of a beneficial effect of e-cigarettes on teen combustible tobacco use, these trends alone are insufficient for establishing e-cigarettes as the causal factor. Natural experiments, such as from policy changes, can be used to provide causal evidence towards the question of the effect of e-cigarettes on teen cigarette use (Pesko, 2022a). In this paper, the gradual roll-out across states of an e-cigarette minimum legal sale age (MLSA) is used as a form of natural experiment. MLSAs prohibit the sale of e-cigarettes to individuals under specific ages; before MLSAs, it was legal to sell e-cigarettes to minors. Five states implemented e-cigarette MLSAs by the end of 2010, seven by the end of 2011, 12 by the end of 2012, 24 by the end of 2013, 39 by the end of 2014, and 47 by the end of 2015, before federal action applied a national MLSA in 2016 (Centers for Disease Control & Prevention, 2020; Pesko & Currie, 2019). Online Appendix Table 1 shows the dates of MLSA implementation. Three studies explore the effect of e-cigarette MLSAs on e-cigarette use in the United States, but each has substantial methodologic limitations: two studies use a single cross-section of data (due to limited data availability at the time of writing) (Abouk & Adams, 2017; Dave et al., 2019b), and the third uses multiple waves of data (through 2014 only) but does not control for state fixed effects to address several likely sources of confounding (Dutra et al., 2018). Additionally, this third study includes cigarette use as a control variable despite evidence that it is endogenously impacted by e-cigarette MLSAs (Abouk & Adams, 2017; Dave et al., 2019b; Friedman, 2015; Pesko & Currie, 2019; Pesko et al., 2016a), and it assumes that no youth used e-cigarettes in 2009 despite e-cigarettes being available in the United States since 2006 (Consumer Advocates for Smoke-Free Alternatives Association, 2022). One study uses Canadian data through 2017 to estimate the effect of staggered adoption of e-cigarette MLSAs using a two-way fixed effect (TWFE) model. This study finds adoption of e-cigarette MLSAs reduces youth e-cigarette use by 4.3 percentage points (ppt), but does not examine effects on combustible cigarette use (Nguyen, 2020). Finally, a study produced concurrently to this published one implements a regression discontinuity design with data on 17- and 18-year-olds from 2014–2017 in the United States, finding that MLSA laws decreased underage e-cigarette use by 15–20% (DeSimone et al., 2022).  This current study estimates the effect of e-cigarette MLSAs in the United States on both e-cigarette use and combustible tobacco useFootnote 2 using multiple waves of National Youth Tobacco Survey (NYTS) data through 2017. By extending the analysis through 2017, this study covers the full time period leading up to a national e-cigarette MLSA in August 2016 (Sharpless, 2019). In addition to estimating a TWFE model, the current study is the first e-cigarette MLSA study to use a method to account for the presence of dynamic heterogeneity in treatment effects (Callaway & Sant’Anna, 2021; Goodman-Bacon, 2021). Additionally, the study improves on the previous study using NYTS data from 2009 to 2014 (Dutra et al., 2018) by not including endogenous control variables nor making assumptions about e-cigarette use in a given year.",3
66.0,3.0,Journal of Risk and Uncertainty,04 May 2023,https://link.springer.com/article/10.1007/s11166-023-09405-3,How risky is distracted driving?,June 2023,J. Bradley Karl,Charles M. Nyce,Boyi Zhuang,Unknown,Male,Unknown,Male,"Vehicle crashes in the United States cause thousands of fatalities, millions of injuries, and substantial economic costs each year. In response, policymakers enact various measures aimed at improving traffic safety, such as incentivizing or mandating vehicle safety features, enacting seat belt laws, and prohibiting impaired driving. In recent years, the ubiquity of cellphones in society has thrust the issue of distracted driving to the forefront of policy discussions and actions surrounding traffic safety. However, both policymakers and researchers are divided on the efficacy and net benefits of laws intended to mitigate distracted driving, especially laws that ban the use of handheld devices while driving. Conflicting results in the literature exacerbate disagreements among policymakers, because both sides can find empirical results to support their positions. For example, published studies report a range of the relative riskiness of distracted driving between 0–no difference between distracted and focused driving (e.g., Bhargava & Pathania, 2013)–and 23 times more risky (e.g., Olson et al., 2009), with most of the positive estimates falling between 2 and 8 times the relative risk of focused driving. In this paper, we develop and apply a new method for estimating the risk associated with distracted driving that overcomes several of the challenges that hinder previous studies. We draw on the work of Levitt and Porter (2001) (henceforth L&P), who develop an elegant and novel strategy for estimating the risk of drinking and driving using only data on fatal crashes. More specifically, leveraging the fact that many fatal crashes involve multiple drivers, L&P show that, for two car crashes, the relative frequency of crashes involving sober drivers and drinking drivers provides sufficient information for estimating the risk associated with drinking and driving. With this novel method, L&P use the information reported in the Fatality Analysis Reporting System (FARS) database to identify crashes involving sober and drinking drivers. They estimate that drivers who are legally drunk pose a crash risk that is 13 times greater than that of sober drivers. In this way, L&P significantly improve on the work (and shortcomings) of prior studies and provide one of the most complete and reliable insights into the risk of drinking and driving. We use L&P’s method as a kernel to develop a radically different strategy for estimating the risk associated with distracted driving. We analyze the uniform and detailed FARS data in five steps. First, we develop a model similar to L&P’s using a different approach that allows us to estimate the relative risk consistently with fewer observations. Second, we validate our changes to the methodology by applying them to drinking drivers and comparing the results to our replication of L&P’s analysis. Third, we apply the validated method to estimate the frequency and relative risk of distracted driving. Fourth, we compare the relative risks of distractions from various sources to that of cell phone use. Fifth, we estimate the externalities and associated costs of distracted driving. Our results suggest that distracted drivers represent between 3.5% and 4% of all drivers (at any time on the road) and are between 2.5 and 3.14 times more risky than focused drivers. Though these numbers are not as high as the relative riskiness of drinking drivers (7.76 to 8.56 times riskier in the 8:00 p.m. to 5:00 a.m. window), they are consistent with a significant risk posed by distracted driving. We also find that cell phone distractions are somewhat less common and less risky than other sources of distraction. Between 23% and 35% of all distracted drivers are distracted by cellphones, and the relative riskiness of cell phone distraction is about two-thirds that of distractions from other sources (e.g., passengers, eating, grooming, etc.). Policymakers should consider this information as they deliberate the costs and benefits of anti-distraction laws.",
67.0,1.0,Journal of Risk and Uncertainty,22 June 2023,https://link.springer.com/article/10.1007/s11166-023-09413-3,Learning your own risk preferences,August 2023,Gary Charness,Nir Chemaya,Dario Trujano-Ochoa,Male,Male,Male,Male,"Risk preferences have an essential role in understanding individuals’ financial and economic decisions. Economic agents must decide how much risk they are willing to take in their daily lives. Given the importance and relevance of risk, many economic models include risk parameters in the agent's utility function in an effort to model an agent’s decisions under risk; one example is the prospect theory in Kahneman and Tversky (1979). Economists have developed many experimental methods to elicit this risk parameter, which can then be used to predict decisions in risky environments. Yet, risk elicitation is challenging and there are unresolved methodological issues. For example, the risk-elicitation puzzle (Pedroni et al., 2017) stems from numerous investigations showing significant inconsistencies in risk preferences when elicited using different or similar methods. It questions the validity of these methods and the degree to which these preferences are stable. In the standard neoclassical view, one has stable risk preferences that are self-known. But there is evidence that factors such as shocks can influence risk-taking preferences in different environments; for example, Beine et al. (2020) finds evidence that exogenous shocks—two earthquakes occurring during their study—can affect risk preferences. Nevertheless, research in this area is still developing. The effect of experience and learning on one’s risk choices should be systematically explored. We advocate first giving people experience with a task or mechanism in the hope of obtaining better comprehension and a more meaningful measure of risk attitude. Furthermore, we suggest that doing this in a manner that is literally hands-on may accelerate the process. We take a step in this direction with a straightforward experiment that gives people experience with risk choices. We offer each participant a choice of six possible gambles in a slight modification of the Dave et al. (2010) version of the original Eckel and Grossman (2002, 2008) mechanism. These gambles reflect trade-offs between expected value and variance so that one should, in principle, choose the gamble that best suits their own self-perceived risk preferences. This method is known for its simplicity in that 1) all gambles are 50%, avoiding probability-weighting issues (see Gonzalez & Wu, 1999), and 2) a subject makes a choice in only one row (instead of in 10, as in Holt & Laury, 2002). The modified version distinguishes between risk-seeking and risk-neutral choices a bit more sharply than the previous payoff numbers used; specifically, we lower the expected value of the riskiest gamble so that choosing this gamble in effect means one is willing to sacrifice expected value to take on more risk. In a nutshell, our question is whether repeated hands-on and unpaid experience with a relatively simple risk-elicitation task affects the choices and, by extension, the implied risk preferences. We first display and explain the gambles and their choices. Each person then chooses one of the rows for a gamble; either this first gamble or the final gamble, but not both, will be paid (50% chance for each; in doing so, we try to eliminate hedging behavior). We then require people to acquire experience by having them execute 24 practice rolls of the dice. To increase engagement, we had them physically make these practice rolls themselves and then record the choice of rows, the outcome of the roll, and the payoff consequences had this roll been chosen for actual payoff. After the practice periods, they then make a final choice of rows. One of the two non-practice choices is then selected by the subject by rolling the dice at the time of payment; the dice are rolled again to determine the outcome of the gamble. All this information was conveyed in the instructions and was read aloud at the beginning of the session. To account for the role of experience and learning in risk elicitation, we consider two effects that might influence participants. First, experience and learning could make participants understand the risk-elicitation task better and thus reduce some errors that players might make in a one-shot task. Being unfamiliar with tasks could make players more risk-averse in their decision simply due to uncertainty about the task structure. It seems natural to be cautious when one is just having one’s first experience with a task. So, an inexperienced participant may display a degree of risk aversion that is transitory. Second, perhaps the most critical information that people fail to understand in the one-shot task concerns the expected payoffs and variance of the lotteries. With more experience with the lotteries, the player might learn or at least have a better feel for these, thereby making a more informed decision . A related approach to overcoming inexperience was used in Engelmann and Hollard (2010) in considering the endowment effect. Their idea was that people who did not have experience with trading might be reluctant to trade their endowed good for another of equal (or even more) value. In their treatment condition, people were endowed with a good that would have no value if not traded for a good that would have value. Doing this trade gave them some experience. While there was a significant endowment effect present in the control treatment, there was no significant endowment effect for the group with trading experience. The conclusion is that providing familiarity and experience with a mechanism can change behavior. It is not obvious that people really know their own preferences, despite the usefulness of this assumption. At the limit, how can we know our feelings about something never experienced? If negative outcomes during the practice periods are experienced as losses compared to a reference point between the high and low payoffs for the gamble chosen, reference dependence and loss aversion (“losses loom larger than gains”) suggest that people should become more risk-averse with experience. But if uncertainty or inexperience leads to people being less willing to take risks than their “true” preferences recommend, one would expect choices to become less risk-averse. Our hypothesis is that the latter force will dominate—experience will lead people to later choose lotteries with higher expected payoffs. Experience could simply allow players to explore their own preferences and perhaps change them. One could consider this to be the case of a player not being fully aware of their own risk preferences, and so potentially benefitting from exploration. In the end, becoming fully aware of one’s risk attitude should be beneficial. The main contributions of this article: People change their risk preferences over the course of a session, in combination with having unpaid practice periods. An unanticipated result is that this significant change is largely driven by males. No shocks are needed, and the structure of the choices and outcomes are clear. This is not just measurement error because the change significantly favors a decrease in risk aversion. The positive or negative outcomes in the practice periods do not affect the final lottery choice, which is comforting in the absence of any psychological affect that could presumably be present for paid rounds. As do other studies, we find that people with more cognitive ability are less risk-averse. Finally, the comments made by the subjects offer evidence that people indeed learned about their preferences and the task. The remainder of this article is organized as follows. Section 2 provides background in the related literature, while section 3 describes the experimental design and hypotheses. We present the experimental results in section 4, and we conclude in section 5.",
67.0,1.0,Journal of Risk and Uncertainty,26 May 2023,https://link.springer.com/article/10.1007/s11166-023-09412-4,Advantageous selection without moral hazard,August 2023,Philippe De Donder,Marie-Louise Leroux,François Salanié,Male,Unknown,Male,Male,"Since the classical contribution of Rothschild and Stiglitz (1976), most of the literature on asymmetric information in insurance markets has highlighted the difficulties associated with adverse selection. As is well-known, these difficulties include the under-provision of insurance, and even sometimes a market breakdown, without trade at all.Footnote 1 By contrast, the seminal paper by Hemenway (1990) argues that sometimes selection can be advantageous, meaning that those agents most eager to buy insurance are also the cheapest ones to insure. In such a case, insurers should be more eager to supply insurance, thereby yielding very different market outcomes, as well as different testable predictions. Hemenway (1990, 1992) and most subsequent papersFootnote 2 envision advantageous selection as arising from differences in risk-aversion among agents: more risk-averse agents buy more coverage, and simultaneously exert more self-protection efforts, so that they may end up receiving indemnities less often i.e., they appear less risky.Footnote 3 We instead argue that advantageous selection may occur under various circumstances, including regular settings where agents differ only in riskiness, exhibiting the same attitude towards risk, and from which moral hazard is excluded. To do so, we rely on the definition of advantageous and adverse selection used in Einav and Finkelstein (2011). This paper mostly focuses on the case where only one insurance contract is offered to agents who are privately informed of their own exogenous riskiness. A change in the contract premium then modifies the agents’ decision to insure, and therefore changes the indemnities the insurer expects to pay. The type of selection (adverse or advantageous) is measured by the sign of the slope of the insurer’s average cost function with respect to the premium choice: in particular, advantageous selection occurs when “ as price is lowered and more individuals opt into the market, the marginal individual opting in has higher expected cost than infra-marginal individuals” (p.124). Hence, the type of selection stems from a composition effect in the pool of subscribers. The effects on equilibrium allocations and welfare are likely to be significant. As an example, consider the case of a single contract.Footnote 4 Under adverse selection, this contract may well be unprofitable whatever the premium, because an increase in the premium selects costlier types, leading to a market collapse.Footnote 5 By contrast, under advantageous selection, asymmetric information per se cannot yield a market breakdown: if a contract is profitable under complete information, then a fortiori it must also be profitable under advantageous selection. This motivates why this search for the sources of advantageous selection may be worth undertaking, and especially so in cases where the demand for insurance is deemed inefficiently low, such as for the market for long-term care insurance, which we shall examine at the end of this paper. In Section 2, we provide the definition of advantageous and adverse selection of Einav and Finkelstein (2011) for a general insurance economy with hidden types and multiple contracts. Section 3 examines the link between advantageous selection and profits. We first study the canonical situation of perfect Bertrand competition, with discontinuous demand functions in equilibrium, and show that this setting is compatible with advantageous selection. We then move to the more realistic case of continuous demand functions and market power and we show that advantageous selection must be accompanied by a high markup rate, set above the inverse of the demand elasticity for this contract. In particular, this can only occur when maximized profit is positive. While Section 3 relied on an aggregate demand function, in the next two sections we formulate settings that allow to build demand functions from the individual choices of privately informed consumers. We start in Section 4 with a classical model à la Rothschild-Stiglitz (1976), in which a single-crossing property ensures that riskier agents end up with higher coverage. This Positive Correlation Property (PCP), defined in Chiappori and Salanié (2000), is a global property comparing two distinct contracts: the better-insured agents should appear riskier than less well-insured agents. Still, even in this very regular case we show that advantageous selection may well occur. Indeed, an increase in the premium of a contract induces some agents to quit, either to subscribe to a contract with less coverage (then these agents must be relatively low-risk agents, according to the single-crossing property), or to a contract with more coverage (for the relatively high-risks agents). The balance between these two opposite effects on the insurer’s average cost is thus ambiguous, and so is the nature of selection. Hence, a contract may well face advantageous selection simply because there exists a contract proposing a higher coverage. Also, it is interesting to note that the definition we have chosen does not always agree with the PCP test, which was designed to detect the presence of asymmetric information.Footnote 6 While Section 4 identified a possible source of advantageous selection in a case with only one risk but multiple contracts offering different coverages, in Section 5 we turn to the case of two exclusive risks, and a unique contract. Once more, there is no moral hazard, and agents differ only in their riskiness, which is now a two-dimensional variable. We first assume that the contract covers only the first risk, while the second risk is a non-insurable background risk (Section 5.2). As a consequence, when buying insurance for the first risk, a consumer should take into account the fact that he pays the premium even in the state when the insurable loss did not occur, while the non-insured background loss did occur. This reduces his willingness-to-pay for an insurance contract covering only the first risk, and more so for an agent whose probability to face the background loss is higher. Therefore, participation in an insurance contract depends on private exposure to the background risk. We show by means of an example that agents with a high probability to incur the insurable loss may choose not to insure, because they also display a high probability of the background loss. This proves that advantageous selection may occur even when a single contract is offered. Note that this requires two conditions: that at the level of an agent the correlation of losses is negative (indeed, we assume for simplicity that the two risks are exclusive); and that the correlation of the two riskinesses over the whole population is positive enough. In the same model, but allowing now both risks to be insured (Section 5.3), we show how a contract bundling together coverage against the two risks may face advantageous selection, even though every contract that would cover only one of these risks would face adverse selection. This leads to the observation that issuing bundled insurance contracts may help insurers avoid the difficulties associated with adverse selection. Section 6 constructs a numerical example of this idea by bundling annuity contracts and Long-Term Care (LTC hereafter) insurance. Annuity contracts ensure a longevity risk, for which empirical studies tend to support the existence of adverse selection.Footnote 7 LTC corresponds to the risk of becoming dependent at old age and of needing “day-to-day help with activities such as washing and dressing, or help with household activities such as cleaning and cooking” (OECD, 2011). While this risk realizes with high probability and implies significant expenses, most people choose not to insure against it. OECD (2011) has estimated that only 2% of LTC expenditures are financed by private LTC insurance (LTCI hereafter) in OECD countries, while the figure is 7% in the US. This is often referred to as the LTCI puzzle. Adverse selection is often mentioned as one reason why this market is so little developed.Footnote 8 Notice that in both cases, moral hazard can be assumed away.Footnote 9 Creating an insurance contract that would insure against both the longevity risk and the LTC risk could alleviate simultaneously both adverse selection problems. Such a contract was proposed as a ‘life care annuity’ contract in Murtaugh et al. (2001) and Brown and Warshawsky (2013). Ameriks et al. (2011) provides survey evidence of a significant potential demand for such a product. Intuitively, agents with poor health face high prices for LTC insurance, while they are less costly on the annuity side because of their reduced life expectancy. Conversely, there exists a group of lucky agents with a high longevity in good health, meaning that they are high risks for annuity sellers and low risks for LTCI sellers. Bundling the two insurance products would thus likely reduce the extent of adverse selection. In this article, we even argue that bundling could generate advantageous selection. Using Canadian survey data, we are indeed able to recover the two-dimensional riskiness probabilities for a dataset of 2.000 individuals, and under reasonable assumptions on preferences we construct a numerical example of a bundled contract displaying advantageous selection.",
67.0,1.0,Journal of Risk and Uncertainty,23 May 2023,https://link.springer.com/article/10.1007/s11166-023-09407-1,Paying for randomization and indecisiveness,August 2023,Qiyan Ong,Jianying Qiu,,Unknown,Unknown,Unknown,Unknown,,
67.0,1.0,Journal of Risk and Uncertainty,04 March 2023,https://link.springer.com/article/10.1007/s11166-023-09404-4,Estimating risk and time preferences over public lotteries: Findings from the field and stream,August 2023,David Scrogin,,,Male,Unknown,Unknown,Male,"Lotteries are commonly used by state and federal resource managers to allocate individual access and usage privileges on public lands and waterways. Examples include the lotteries conducted annually by the National Park Service to allocate Grand Canyon river rafting permits and by state and provincial wildlife management agencies throughout the U.S. and Canada to allocate hunting licenses, from elk licenses in the Rocky Mountain West to alligator licenses in Florida. Though inefficient, as privileges are randomly awarded and rules restrict secondary market resale, public lotteries for such non-market goods offer an unexplored field counterpart to experimental lotteries with monetary payoffs for investigating risk and time preferences. This paper develops a discrete choice framework for jointly estimating risk and time preferences over public lotteries with individual-level field data from a set of natural policy experiments. Two features of public lotteries make them an appealing field setting for this purpose. First, for estimating risk preferences with observational field data one must establish choice occasions in the form of lotteries over which individuals make qualitative or quantitative choices.Footnote 1 As the institutional structure of public lotteries leads to well-defined choice sets with known prices, non-market attributes and regulations, and historic success rates, this design aspect is largely resolved for researchers by resource managers. Furthermore, and similar to parimutuel lotteries, the stakes of public lotteries may be sizable, such as steaks for a household for the year with big-game hunting licenses or a once-in-a-lifetime opportunity to river raft in the Grand Canyon. Second, public lotteries have distinct temporal periods essential for estimating time preferences. In the first period, individuals confront the choice set of lotteries and select one or rank their preferences over two or more through an application process. The random draws occur in the second period, and those who are drawn then exercise the awarded privileges in a later period defined by the lottery, such as participating in a specific activity within a specific geographic area. Considering these stages, a key characteristic distinguishing lottery systems from one another–and the lottery system evaluated here between years–is the pricing arrangement. Lottery rules may require applicants to pre-pay refundable or non-refundable license fees, or instead payment of the license fees may only be required from applicants who are randomly drawn.Footnote 2 Experimental studies of intertemporal choice over lotteries with monetary payoffs have demonstrated the significance of accounting for risk attitudes in estimating time preferences. Andersen et al. (2008a) used a double elicitation experimental design in order to identify and jointly estimate the parameters of the utility function and alternative discount factors. Their design and estimation strategy were evaluated against a single elicitation method proposed by Laury et al. (2012) that is invariant to structural assumptions about the utility function. Ferecatu and Önçüler (2016) also used the double elicitation method together with Bayesian hierarchical modeling to obtain individual level estimates of risk and time preferences. And Abdellaoui et al. (2019) used a variant of the double elicitation method, and, while they report more conservative discount rate estimates than the earlier studies, the overall conclusions are consistent. These efforts were extended outside the laboratory by Andersen et al. (2014a) in order to estimate the risk and time preferences of entrepreneurs with data from artefactual field experiments following the design of Andersen et al. (2008a). The structural methods used in this literature are integrated here within a destination choice framework used in the lottery demand and non-market valuation literature (see e.g., Yoder et al., 2014; Reeling et al., 2020) in order to estimate risk and time preferences with observational field data. The paper proceeds as follows. Section 2 discusses the lottery field setting and choice data. The lottery system is the long-standing New Mexico big-game hunting license lotteries, and the choice data is a balanced panel of 7,924 participants in the state’s 127 lotteries for elk licenses in the year before and after legislation that resulted in changes to the lottery pricing arrangement, license prices, and allocation of licenses between residents and non-residents. In Section 3 the public lottery framework is formalized, and random expected utility (EU) and rank dependent expected utility (RDEU) models of lottery choice are specified for estimation of risk and time preferences. Homogenous and heterogenous preference estimation results are evaluated in Section 4, and the framework is extended in Section 5 to the setting of compound public lotteries and the estimation of the welfare effects from changes in the lottery pricing arrangement. The paper concludes in Section 6. Consistent with experimental findings, participants in the public lotteries considered here are significantly risk averse, and the degree of risk aversion is significantly related to individual age and gender and the outcomes from prior lottery choices. Results from the RDEU choice models indicate applicants significantly nonlinear weight both the probability of being drawn and the probability of harvesting an elk. Across alternative hyperbolic specifications of the discount factor, RDEU estimated annual discount rates are less variable and more conservative than the respective EU estimates, ranging from 10.7% to 12.8%, and changes in the license fees of about $30 under pre-payment pricing and $50 under post-payment pricing are estimated to yield indifference between the two pricing rules for an average lottery applicant.",

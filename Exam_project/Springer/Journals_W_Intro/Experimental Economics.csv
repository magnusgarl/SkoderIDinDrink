Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009966010448,Editors' Preface,June 1998,Charles A. Holt,Arthur J.H.C. Schram,,Male,Male,Unknown,Male,,1
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009970815935,Introduction,June 1998,James C. Cox,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009905800005,Quantal Response Equilibria for Extensive Form Games,June 1998,Richard D. Mckelvey,Thomas R. Palfrey,,Male,Male,Unknown,Male,,291
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009957816843,Axiomatic Characterization of the Quadratic Scoring Rule,June 1998,Reinhard Selten,,,Male,Unknown,Unknown,Male,,145
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009909900914,Sequential Markets: An Experimental Investigation of Clower's Dual-Decision Hypothesis,June 1998,John D. Hey,Daniela di Cagno,,Male,Female,Unknown,Mix,,
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009961917752,A Monte Carlo Analysis of the Fisher Randomization Technique: Reviving Randomization for Experimental Economists,June 1998,Robert Moir,,,Male,Unknown,Unknown,Male,,33
1.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009914001822,Individual and Group Behavior in the Ultimatum Game: Are Groups More “Rational” Players?,June 1998,Gary Bornstein,Ilan Yaniv,,Male,Male,Unknown,Male,,141
1.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026435508449,On the Validity of the Random Lottery Incentive System,September 1998,Robin P. Cubitt,Chris Starmer,Robert Sugden,,,Male,Mix,,
1.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009972125288,What Collusion? Unilateral Market Power as a Catalyst for Countercyclical Markups,September 1998,Bart J. Wilson,,,Male,Unknown,Unknown,Male,,13
1.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009992209358,Numerical Computation of Equilibrium Bid Functions in a First-Price Auction with Heterogeneous Risk Attitudes,September 1998,Mark V. Van Boening,Stephen J. Rassenti,Vernon L. Smith,Male,Male,Male,Male,,4
1.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009903417134,Bonn Workshop 1997—Theories of Bounded Rationality,September 1998,Bettina Kuon,Abdolkarim Sadrieh,Reinhard Selten,Female,Unknown,Male,Mix,,
1.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009944326196,Bounded Rationality in Individual Decision Making,September 1998,Colin Camerer,,,Male,Unknown,Unknown,Male,,127
1.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009996324622,Nash as an Organizing Principle in the Voluntary Provision of Public Goods: Experimental Evidence,December 1998,R. Mark Isaac,James M. Walker,,Unknown,Male,Unknown,Male,,31
1.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009951108693,Measuring Motivations for the Reciprocal Responses Observed in a Simple Dilemma Game,December 1998,Gary E. Bolton,Jordi Brandts,Axel Ockenfels,Male,Male,Male,Male,,51
1.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009903210510,Reinforcement-Based Adaptive Learning in Asymmetric Two-Person Bargaining with Incomplete Information,December 1998,Amnon Rapoport,Terry E. Daniel,Darryl A. Seale,Male,,Male,Mix,,
1.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009915506721,Bonn Workshop 1997—Theories of Bounded Rationality,December 1998,Bettina Kuon,Abdolkarim Sadrieh,Reinhard Selten,Female,Unknown,Male,Mix,,
1.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009911411418,Bargaining and Dilemma Games: From Laboratory Data Towards Theoretical Synthesis,December 1998,Gary E. Bolton,,,Male,Unknown,Unknown,Male,,11
2.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009984414401,Heterogeneity and the Voluntary Provision of Public Goods,August 1999,Kenneth S. Chan,Stuart Mestelman,R. Andrew Muller,Male,Male,Unknown,Male,,69
2.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009925731240,The Effects of Decoy Gambles on Individual Choice,August 1999,Kaisa Herne,,,Female,Unknown,Unknown,Female,,44
2.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009977715310,Continuous-Time Strategy Selection in Linear Population Games,August 1999,Siegfried K. Berninghaus,Karl-Martin Ehrhart,Claudia Keser,Male,Unknown,Female,Mix,,
2.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009929832148,Collusion Without Conspiracy: An Experimental Study of One-Sided Auctions,August 1999,Katerina Sherstyuk,,,Female,Unknown,Unknown,Female,,16
2.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009981800289,Learning in a Laboratory Market with Random Supply and Demand,August 1999,Timothy N. Cason,Daniel Friedman,,Male,Male,Unknown,Male,,14
2.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009986005690,Eliciting Individual Discount Rates,December 1999,Maribeth Coller,Melonie B. Williams,,Female,Female,Unknown,Female,,308
2.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009996122528,What Does it Take to Eliminate the use of a Strategy Strictly Dominated by a Mixture?,December 1999,John Van Huyck,Frederick Rankin,Raymond Battalio,Male,Male,Male,Male,,2
2.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009948206599,A Model of Behavior in Coordination Game Experiments,December 1999,Martin Sefton,,,Male,Unknown,Unknown,Male,,3
2.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009900310537,A Market-Based Mechanism for Allocating Space Shuttle Secondary Payload Priority,March 2000,John Ledyard,David Porter,Randii Wessen,Male,Male,Unknown,Male,,5
2.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009910627375,Good News and Bad News: Search from Unknown Wage Offer Distributions,March 2000,James C. Cox,Ronald L. Oaxaca,,Male,Male,Unknown,Male,,19
2.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009962612354,Hot vs. Cold: Sequential Responses and Preference Stability in Experimental Games,March 2000,Jordi Brandts,Gary Charness,,Male,Male,Unknown,Male,,149
2.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009918829192,Step Returns in Threshold Public Goods: A Meta- and Experimental Analysis,March 2000,Rachel T. A. Croson,Melanie Beth Marks,,Female,Female,Unknown,Female,,121
3.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009925123187,The Impact of Exchange Context on the Activation of Equity in Ultimatum Games,June 2000,Elizabeth Hoffman,Kevin McCabe,Vernon Smith,Female,Male,Male,Mix,,
3.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009989907258,Learning to Accept in Ultimatum Games: Evidence from an Experimental Design that Generates Low Offers,June 2000,John A. List,Todd L. Cherry,,Male,Male,Unknown,Male,,40
3.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009942024096,Robustness of the Incentive Compatible Combinatorial Auction,June 2000,R. Mark Isaac,Duncan James,,Unknown,Male,Unknown,Male,,15
3.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009994008166,Framing Effects in Public Goods Experiments,June 2000,R. Cookson,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1009946125005,"Trust, Reciprocity, and Social History: A Re-examination",June 2000,Andreas Ortmann,John Fitzgerald,Carl Boeing,Male,Male,Male,Male,,53
3.0,2.0,Experimental Economics,01 October 2000,https://link.springer.com/article/10.1023/A:1026572918109,Volunteers and Pseudo-Volunteers: The Effect of Recruitment Method in Dictator Experiments,October 2000,Catherine C. Eckel,Philip J. Grossman,,Female,Male,Unknown,Mix,,
3.0,2.0,Experimental Economics,01 October 2000,https://link.springer.com/article/10.1023/A:1026537302180,Two Experiments to Test a Model of Herd Behaviour,October 2000,Louise Allsopp,John D. Hey,,Female,Male,Unknown,Mix,,
3.0,2.0,Experimental Economics,01 October 2000,https://link.springer.com/article/10.1023/A:1026589319018,An Experimental Analysis of Intertemporal Allocation Behavior,October 2000,Vital Anderhub,Werner Güth,Martin Strobel,Male,Male,Male,Male,,18
3.0,2.0,Experimental Economics,01 October 2000,https://link.springer.com/article/10.1023/A:1026541403088,Elicitation of Strategy Profiles in Large Group Coordination Games,October 2000,Darryl A. Seale,Amnon Rapoport,,Male,Male,Unknown,Male,,7
3.0,2.0,Experimental Economics,01 October 2000,https://link.springer.com/article/10.1007/BF01669307,Call for papers,October 2000,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011481916758,An Experimental Study of the Effect of Private Information in the Coase Theorem,December 2000,Richard D. McKelvey,Talbot Page,,Male,Male,Unknown,Male,,12
3.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011420500828,Control Without Deception: Individual Behaviour in Free-Riding Experiments Revisited,December 2000,Nicholas Bardsley,,,Male,Unknown,Unknown,Male,,49
3.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011472501737,The False Consensus Effect Disappears if Representative Information and Monetary Incentives Are Given,December 2000,Dirk Engelmann,Martin Strobel,,Male,Male,Unknown,Male,,42
3.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011476619484,Dominant Strategy Adoption and Bidders' Experience with Pricing Rules,December 2000,Ronald M. Harstad,,,Male,Unknown,Unknown,Male,,65
4.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011486405114,Does Repetition Improve Consistency?,June 2001,John D. Hey,,,Male,Unknown,Unknown,Male,,71
4.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011493421952,Marketmaking in the Laboratory: Does Competition Matter?,June 2001,Jan Pieter Krahnen,Martin Weber,,Male,Male,Unknown,Male,,8
4.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011445522861,Price Bubbles in Laboratory Asset Markets with Constant Fundamental Values,June 2001,Charles Noussair,Stephane Robin,Bernard Ruffieux,Male,Unknown,Male,Male,,100
4.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011449606931,The Carrot vs. the Stick in Work Team Motivation,June 2001,David L. Dickinson,,,Male,Unknown,Unknown,Male,,50
4.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011433904265,Strength of the Social Dilemma in a Public Goods Experiment: An Exploration of the Error Hypothesis,October 2001,Marc Willinger,Anthony Ziegelmeyer,,Male,Male,Unknown,Male,,16
4.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011480621103,Decision Making Costs and Problem Solving Performance,October 2001,Tanga M. McDaniel,E. Elisabet Rutström,,Unknown,Unknown,Unknown,Unknown,,
4.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011432705173,Endogenous Entry and Exit in Common Value Auctions,October 2001,James C. Cox,Sam Dinkin,James T. Swarthout,Male,,Male,Mix,,
4.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1011484722011,"Value Orientations, Income and Displacement Effects, and Voluntary Contributions",October 2001,Neil Buckley,Kenneth S. Chan,Mohamed Shehata,Male,Male,Male,Male,,8
4.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1013290819565,Tests of Fairness Models Based on Equity Considerations in a Three-Person Ultimatum Game,December 2001,John H. Kagel,Katherine Willey Wolfe,,Male,Female,Unknown,Mix,,
4.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1013265203635,Testing for the Presence of a Tremble in Economic Experiments,December 2001,Peter G. Moffatt,Simon A. Peters,,Male,Male,Unknown,Male,,17
4.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1013217320474,Behavior and Learning in the “Dirty Faces” Game,December 2001,Roberto A. Weber,,,Male,Unknown,Unknown,Male,,10
4.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1013269304544,Laboratory Behavior in Spot and Forward Auction Markets,December 2001,Owen R. Phillips,Dale J. Menkhaus,Joseph L. Krogmeier,Male,,Male,Mix,,
4.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1013221421382,The Endowment Effect and Repeated Market Trials: Is the Vickrey Auction Demand Revealing?,December 2001,Jack L. Knetsch,Fang-Fang Tang,Richard H. Thaler,Male,,Male,Mix,,
5.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1016380207200,Efficient Contracting and Fair Play in a Simple Principal-Agent Experiment,June 2002,Vital Anderhub,Simon Gächter,Manfred Königstein,Male,Male,Male,Male,,51
5.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1016312624038,Price Competition Between Teams,June 2002,Gary Bornstein,Uri Gneezy,,Male,Male,Unknown,Male,,25
5.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1016364608108,Learning to Punish: Experimental Evidence from a Sequential Step-Level Public Goods Game,June 2002,David J. Cooper,Carol Kraker Stockman,,Male,,Unknown,Mix,,
5.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1016316725855,Risk Attitudes of Children and Adults: Choices Over Small and Large Probability Gains and Losses,June 2002,William T. Harbaugh,Kate Krause,Lise Vesterlund,Male,Female,Female,Mix,,
5.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020330820698,Method in Experiment: Rhetoric and Reality,October 2002,Vernon L. Smith,,,Male,Unknown,Unknown,Male,,57
5.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020365204768,The Costs of Deception: Evidence from Psychology,October 2002,Andreas Ortmann,Ralph Hertwig,,Male,Male,Unknown,Male,,106
5.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020317321607,Voluntary Participation and Spite in Public Good Provision Experiments: An International Comparison,October 2002,Timothy N. Cason,Tatsuyoshi Saijo,Takehiko Yamato,Male,Unknown,Male,Male,,69
5.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020369305677,A New Variant of the Winner's Curse in a Coasian Contracting Game,October 2002,Glen Archibald,Nathaniel T. Wilcox,,Male,Male,Unknown,Male,,
5.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020871917917,On the Behavioral Foundations of the Law of Supply and Demand: Human Convergence and Robot Randomness,December 2002,Paul J. Brewer,Maria Huang,Charles R. Plott,Male,Female,Male,Mix,,
5.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020880101987,An Experimental Investigation of Excludable Public Goods,December 2002,Kurtis J. Swope,,,Male,Unknown,Unknown,Male,,23
5.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020832203804,House Money Effects in Public Good Experiments,December 2002,Jeremy Clark,,,Male,Unknown,Unknown,Male,,120
5.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020840420643,Common Value Auctions with Default: An Experimental Approach,December 2002,Matthew R. Roelofs,,,Male,Unknown,Unknown,Male,,8
5.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1020892405622,Tacit Coordination in a Decentralized Market Entry Game with Fixed Capacity,December 2002,Rami Zwick,Amnon Rapoport,,Male,Male,Unknown,Male,,18
6.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1024268907844,"Order of Play, Forward Induction, and Presentation Effects in Two-Person Games",June 2003,R. Andrew Muller,Asha Sadanand,,Unknown,Female,Unknown,Female,,12
6.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1024248608752,Capacity Choices and Price Competition in Experimental Markets,June 2003,Vital Anderhub,Werner Güth,Hans-Theo Normann,Male,Male,Unknown,Male,,24
6.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1024252725591,Coordination and Information in Critical Mass Games: An Experimental Study,June 2003,Giovanna Devetag,,,Female,Unknown,Unknown,Female,,29
6.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1024204826499,The Hot Versus Cold Effect in a Simple Bargaining Experiment,June 2003,Jeannette Brosig,Joachim Weimann,Chun-Lei Yang,Female,Male,,Mix,,
6.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1024209010570,A Common Pool Resource Game with Sequential Decisions and Experimental Evidence,June 2003,Lluis Bru,Susana Cabrera,Rosario Gomez,Male,Female,Male,Mix,,
6.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1025375803912,Explaining Overbidding in First Price Auctions Using Controlled Lotteries,October 2003,Robert Dorsey,Laura Razzolini,,Male,Female,Unknown,Mix,,
6.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1025305020751,Reinforcement and Directional Learning in the Ultimatum Game with Responder Competition,October 2003,Brit Grosskopf,,,Female,Unknown,Unknown,Female,,61
6.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1025357004821,The Right Choice at the Right Time: A Herding Experiment in Endogenous Time,October 2003,Daniel Sgroi,,,Male,Unknown,Unknown,Male,,29
6.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1025309121659,Relative versus Absolute Speed of Adjustment in Strategic Environments: Responder Behavior in Ultimatum Games,October 2003,David J. Cooper,Nick Feltovich,Rami Zwick,Male,Male,Male,Male,,19
6.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1025361105730,Cost Structures and Nash Play in Repeated Cournot Games,October 2003,Douglas D. Davis,Robert J. Reilly,Bart J. Wilson,Male,Male,Male,Male,,11
6.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026209001464,Learning to Open Monty Hall's Doors,November 2003,Ignacio Palacios-Huerta,,,Male,Unknown,Unknown,Male,,17
6.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026221318302,Does Team-Based Compensation Give Rise to Problems When Agents Vary in Their Ability?,November 2003,Claude Meidinger,Jean-Louis Rullière,Marie-Claire Villeval,,Unknown,Unknown,Mix,,
6.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026273319211,Nonparametric Tests of Differences in Medians: Comparison of the Wilcoxon–Mann–Whitney and Robust Rank-Order Tests,November 2003,Nick Feltovich,,,Male,Unknown,Unknown,Male,,76
6.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026277420119,Linear Public Goods Experiments: A Meta-Analysis,November 2003,Jennifer Zelmer,,,Female,Unknown,Unknown,Female,,369
6.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026281504189,Bayes Factors with an Application to Experimental Economics,November 2003,Gary E. Bolton,Duncan K.H. Fong,Paul L. Mosquin,Male,Male,Male,Male,,9
6.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026233605098,Nonparametric Analysis of Longitudinal Binary Data: An Application to the Intergroup Prisoner's Dilemma Game,November 2003,Ronit Nirel,Malka Gorfine,,Female,Female,Unknown,Female,,1
7.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026256404208,"Economic Games Among the Amazonian Tsimane: Exploring the Roles of Market Access, Costs of Giving, and Cooperation on Pro-Social Game Behavior",February 2004,Michael Gurven,,,Male,Unknown,Unknown,Male,,37
7.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026257921046,Timing and Virtual Observability in Ultimatum Bargaining and “Weak Link” Coordination Games,February 2004,Roberto A. Weber,Colin F. Camerer,Marc Knez,Male,Male,Male,Male,,60
7.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026210021955,"On Perceptions of Fairness: The Role of Valuations, Outside Options, and Information in Ultimatum Bargaining Games",February 2004,Pamela M. Schmitt,,,Female,Unknown,Unknown,Female,,54
7.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026214106025,Inefficiency in Earnings Forecasts: Experimental Evidence of Reactions to Positive vs. Negative Information,February 2004,Douglas E. Stevens,Arlington W. Williams,,Male,Unknown,Unknown,Male,,7
7.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1023/A:1026266122863,A Study of Consumer Behavior Using Laboratory Data,February 2004,Philippe Février,Michael Visser,,Male,Male,Unknown,Male,,33
7.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000026975.48587.f0,Experimental Methods and Elicitation of Values,June 2004,Glenn W. Harrison,Ronald M. Harstad,E. Elisabet Rutström,Male,Male,Unknown,Male,,114
7.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000026976.44467.66,"Repetition, Communication, and Coordination Failure",June 2004,Gregory M. Parkhurst,Jason F. Shogren,Chris Bastian,Male,Male,,Mix,,
7.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000026977.32722.f1,An Application of the English Clock Market Mechanism to Public Goods Games,June 2004,M. Vittoria Levati,Tibor Neugebauer,,Unknown,Male,Unknown,Male,,18
7.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000026978.14316.74,Cultural Differences in Ultimatum Game Experiments: Evidence from a Meta-Analysis,June 2004,Hessel Oosterbeek,Randolph Sloof,Gijs van de Kuilen,Male,Male,Male,Male,,481
7.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000026979.14590.3c,How Robust is Laboratory Gift Exchange?,June 2004,Gary Charness,Guillaume R. Frechette,John H. Kagel,Male,Male,Male,Male,,112
7.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000040639.78678.94,Preface,October 2004,Tim Cason,Arthur Schram,,Male,Male,Unknown,Male,,
7.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000040558.49037.7d,Charitable Lottery Structure and Fund Raising: Theory and Evidence,October 2004,Donald J. Dale,,,Male,Unknown,Unknown,Male,,17
7.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000040559.08652.51,Rewards and Sanctions and the Provision of Public Goods in One-Shot Settings,October 2004,James M. Walker,Matthew A. Halloran,,Male,Male,Unknown,Male,,92
7.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000040560.94572.60,Individual Decision Making in a Negative Externality Experiment,October 2004,J. Spraggon,,,Unknown,Unknown,Unknown,Unknown,,
7.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1023/B:EXEC.0000040561.02540.3f,How Applicable is the Dominant Firm Model of Price Leadership?,October 2004,Stephen J. Rassenti,Bart J. Wilson,,Male,Male,Unknown,Male,,7
8.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1407-5,Learning Direction Theory and the Winner’s Curse,April 2005,Reinhard Selten,Klaus Abbink,Ricarda Cox,Male,Male,Female,Mix,,
8.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0435-5,The Limitations of Experimental Design: A Case Study Involving Monetary Incentive Effects in Laboratory Markets,April 2005,Steven J. Kachelmeier,Kristy L. Towry,,Male,Female,Unknown,Mix,,
8.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0436-4,Heterogeneous Agents in Public Goods Experiments,April 2005,Roberto M. Burlando,Francesco Guala,,Male,Male,Unknown,Male,,163
8.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0437-3,Competing Against Experienced and Inexperienced Players,April 2005,Robert L. Slonim,,,Male,Unknown,Unknown,Male,,37
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1845-0,Editors’ Preface,June 2005,Arthur J. H. C. Schram,Timothy Cason,,Male,Male,Unknown,Male,,
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0867-y,Subsidy Schemes and Charitable Contributions: A Closer Look,June 2005,Douglas D. Davis,Edward L. Millner,Robert J. Reilly,Male,Male,Male,Male,,53
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0869-9,On the Design of Peer Punishment Experiments,June 2005,Marco Casari,,,Male,Unknown,Unknown,Male,,39
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0872-1,Joining a Queue or Staying Out: Effects of Information Structure and Service Time on Arrival and Staying Out Decisions,June 2005,Darryl A. Seale,James E. Parco,Amnon Rapoport,Male,Male,Male,Male,,30
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1469-4,Bargaining Outcomes with Double-Offer Arbitration,June 2005,David L. Dickinson,,,Male,Unknown,Unknown,Male,,25
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1846-z,Dissertation Abstracts,June 2005,Editorial Board,,,Unknown,Unknown,Unknown,Unknown,,
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0875-y,Dissertation Abstract: Decision-Theoretic Models of Market Equilibration,June 2005,Sean M. Crockett,,,Male,Unknown,Unknown,Male,,
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0878-8,"Dissertation Abstract: “Rationality, Minds, and Machines in the Laboratory: A Thematic History of Vernon Smith’s Experimental Economics”",June 2005,Kyu Sang Lee,,,,Unknown,Unknown,Mix,,
8.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-0881-0,Dissertation Abstract: Towards a Positive Economic Theory of Indecisiveness,June 2005,Eric Danan,,,Male,Unknown,Unknown,Male,,
8.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1463-x,"Choosing Bargaining Partners—An Experimental Study on the Impact of Information About Income, Status and Gender",September 2005,Hå kan Holm,Peter Engseld,,Unknown,Male,Unknown,Male,,20
8.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1464-9,Demographics and Behaviour,September 2005,Enrica Carbone,,,Female,Unknown,Unknown,Female,,13
8.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1465-8,Do Subjects Separate (or Are They Sophisticated)?,September 2005,John D. Hey,Jinkwon Lee,,Male,Unknown,Unknown,Male,,94
8.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-1466-7,The Insiders’ Dilemma: An Experiment on Merger Formation,September 2005,Tobias Lindqvist,Johan Stennek,,Male,Male,Unknown,Male,,16
8.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-5366-7,Introduction,December 2005,Chris Starmer,Nicholas Bardsley,,,Male,Unknown,Mix,,
8.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-5372-9,Modelling the Stochastic Component of Behaviour in Experiments: Some Issues for the Interpretation of Data,December 2005,Graham Loomes,,,Male,Unknown,Unknown,Male,,87
8.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-5373-8,Why We Should Not Be Silent About Noise,December 2005,John D. Hey,,,Male,Unknown,Unknown,Male,,61
8.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-5374-7,Regular Quantal Response Equilibrium,December 2005,Jacob K. Goeree,Charles A. Holt,Thomas R. Palfrey,Male,Male,Male,Male,,108
8.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-005-5375-6,Stochastic Choice and the Allocation of Cognitive Effort,December 2005,Peter G. Moffatt,,,Male,Unknown,Unknown,Male,,58
9.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-1467-1,An experimental examination of the house money effect in a multi-period setting,April 2006,Lucy F. Ackert,Narat Charupat,Richard Deaves,Female,Unknown,Male,Mix,,
9.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-1468-0,From ultimatum to Nash bargaining: Theory and experimental evidence,April 2006,Sven Fischer,Werner Güth,Andreas Stiehler,Male,Male,Male,Male,,22
9.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-4310-9,Exploring group decision making in a power-to-take experiment,April 2006,Ronald Bosman,Heike Hennig-Schmidt,Frans van Winden,Male,Female,Male,Mix,,
9.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-4307-4,Internet cautions: Experimental games with internet partners,April 2006,Catherine C. Eckel,Rick K. Wilson,,Female,Male,Unknown,Mix,,
9.0,1.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-4313-6,A simplified test for preference rationality of two-commodity choice,April 2006,Samiran Banerjee,James H. Murphy,,Unknown,Male,Unknown,Male,,12
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-4309-2,The value of reputation on eBay: A controlled experiment,June 2006,Paul Resnick,Richard Zeckhauser,Kate Lockwood,Male,Male,Female,Mix,,
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-5385-z,Neutral versus loaded instructions in a bribery experiment,June 2006,Klaus Abbink,Heike Hennig-Schmidt,,Male,Female,Unknown,Mix,,
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-5386-y,Trust and trustworthiness in games: An experimental study of intergenerational advice,June 2006,Andrew Schotter,Barry Sopher,,Male,Male,Unknown,Male,,51
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7049-4,The breakdown of cooperation in iterative real-time trust dilemmas,June 2006,Ryan O. Murphy,Amnon Rapoport,James E. Parco,,Male,Male,Mix,,
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-5387-x,Dissertation abstract: The influence of culture on economic behavior with applications to food and the environment,June 2006,Mariah D. Tanner Ehmke,,,Female,Unknown,Unknown,Female,,1
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7044-9,Dissertation abstract: Exclusion and cooperation in networks,June 2006,Aljaž Ule,,,Male,Unknown,Unknown,Male,,1
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7045-8,Comparing group and individual decision-making in risky environments,June 2006,Ronald J. Baker II,,,Male,Unknown,Unknown,Male,,1
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7046-7,"Trust, trustworthiness, and risk in rural Paraguay",June 2006,Laura Schechter,,,Female,Unknown,Unknown,Female,,5
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7047-6,R&D cooperation and strategic decision-making in oligopoly: An experimental economics approach,June 2006,Sigrid Suetens,,,Female,Unknown,Unknown,Female,,1
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7048-5,"Dissertation abstract: The effect of involvement, time, and vividness on consumers’ value judgments: A test of prospect theory",June 2006,Najam U. Saqib,,,Unknown,Unknown,Unknown,Unknown,,
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7051-x,"Trust and reciprocity in inter-individual versus inter-group interactions: The effects of social influence, group dynamics, and perspective biases",June 2006,Fei Song,,,,Unknown,Unknown,Mix,,
9.0,2.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7052-9,Dissertation abstract: Reputational benefits of altruism and altruistic punishment,June 2006,Pat Barclay,,,,Unknown,Unknown,Mix,,
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9121-5,“Behavioral experiments” in economics,September 2006,Roberto A. Weber,Colin F. Camerer,,Male,Male,Unknown,Male,,12
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9122-4,Decomposing trust and trustworthiness,September 2006,Nava Ashraf,Iris Bohnet,Nikita Piankov,Female,Female,Male,Mix,,
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9123-3,Barking up the right tree: Are small groups rational agents?,September 2006,James C. Cox,Stephen C. Hayne,,Male,Male,Unknown,Male,,61
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9124-2,Induced heterogeneity in trust experiments,September 2006,Lisa R. Anderson,Jennifer M. Mellor,Jeffrey Milyo,Female,Female,Male,Mix,,
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9125-1,A network experiment in continuous time: The influence of link costs,September 2006,Siegfried K. Berninghaus,Karl-Martin Ehrhart,Marion Ott,Male,Unknown,Female,Mix,,
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9126-0,“Si él lo necesita”: Gypsy fairness in Vallecas,September 2006,Pablo Brañas-Garza,Ramón Cobo-Reyes,Almudena Domínguez,Male,Male,Female,Mix,,
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9127-z,Can second-order punishment deter perverse punishment?,September 2006,Matthias Cinyabuguma,Talbot Page,Louis Putterman,Male,Male,Male,Male,,164
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9128-y,Misperceiving the value of information in predicting the performance of others,September 2006,George Loewenstein,Don A. Moore,Roberto A. Weber,Male,Male,Male,Male,,19
9.0,3.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-9129-x,Does seeing more deeply into a game increase one’s chances of winning?,September 2006,C. Nicholas McKinney Jr.,John B. Van Huyck,,Unknown,Male,Unknown,Male,,9
9.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-0064-7,Testing subgame perfection apart from fairness in ultimatum games,December 2006,James Andreoni,Emily Blanchard,,Male,Female,Unknown,Mix,,
9.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7050-y,Experimental internet auctions with random information retrieval,December 2006,Doron Sonsino,Radosveta Ivanova-Stenzel,,Male,Unknown,Unknown,Male,,
9.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7053-8,Tax compliance and obedience to authority at home and in the lab: A new experimental approach,December 2006,C. Bram Cadsby,Elizabeth Maynes,Viswanath Umashanker Trivedi,Unknown,Female,Unknown,Female,,44
9.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7054-7,EDGAR on the internet: The welfare effects of wider information distribution in an experimental market for risky assets,December 2006,David Bodoff,Hugo Levecq,Hongtao Zhang,Male,Male,Unknown,Male,,2
9.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7055-6,Elicitation using multiple price list formats,December 2006,Steffen Andersen,Glenn W. Harrison,E. Elisabet Rutström,Male,Male,Unknown,Male,,359
9.0,4.0,Experimental Economics,,https://link.springer.com/article/10.1007/s10683-006-7056-5,Observability and overcoming coordination failure in organizations: An experimental study,December 2006,Jordi Brandts,David J. Cooper,,Male,Male,Unknown,Male,,55
10.0,1.0,Experimental Economics,17 February 2007,https://link.springer.com/article/10.1007/s10683-007-9160-6,Editors’ preface,March 2007,Jordi Brandts,Tim Cason,,Male,Male,Unknown,Male,,
10.0,1.0,Experimental Economics,08 February 2007,https://link.springer.com/article/10.1007/s10683-006-9139-8,The rise of cooperation in correlated matching prisoners dilemma: An experiment,March 2007,Chun-Lei Yang,Ching-Syang Jack Yue,I-Tang Yu,,Unknown,Unknown,Mix,,
10.0,1.0,Experimental Economics,01 February 2007,https://link.springer.com/article/10.1007/s10683-006-9133-1,Policy making and rent-dissipation: An experimental test,March 2007,David S. Bullock,E. Elisabet Rutström,,Male,Unknown,Unknown,Male,,12
10.0,1.0,Experimental Economics,01 February 2007,https://link.springer.com/article/10.1007/s10683-006-9130-4,Framing the first-price auction,March 2007,Theodore L. Turocy,Elizabeth Watson,Raymond C. Battalio,Male,Female,Male,Mix,,
10.0,1.0,Experimental Economics,31 January 2007,https://link.springer.com/article/10.1007/s10683-006-9131-3,Are we nice(r) to nice(r) people?—An experimental analysis,March 2007,Max Albert,Werner Güth,Boris Maciejovsky,Male,Male,Male,Male,,38
10.0,1.0,Experimental Economics,31 January 2007,https://link.springer.com/article/10.1007/s10683-006-9132-2,Bertrand colludes more than Cournot,March 2007,Sigrid Suetens,Jan Potters,,Female,Male,Unknown,Mix,,
10.0,1.0,Experimental Economics,07 February 2007,https://link.springer.com/article/10.1007/s10683-006-9134-0,Searching for the sunk cost fallacy,March 2007,Daniel Friedman,Kai Pommerenke,Bernardo A. Huberman,Male,Male,Male,Male,,96
10.0,2.0,Experimental Economics,13 February 2007,https://link.springer.com/article/10.1007/s10683-006-9135-z,Mental processes and strategic equilibration: An fMRI study of selling strategies in second price auctions,June 2007,David M. Grether,Charles R. Plott,John M. Allman,Male,Male,Male,Male,,11
10.0,2.0,Experimental Economics,15 February 2007,https://link.springer.com/article/10.1007/s10683-006-9136-y,Stability of risk preference parameter estimates within the Becker-DeGroot-Marschak procedure,June 2007,Duncan James,,,Male,Unknown,Unknown,Male,,20
10.0,2.0,Experimental Economics,31 January 2007,https://link.springer.com/article/10.1007/s10683-006-9137-x,Contracting inside an organization: An experimental study,June 2007,Paul J. Healy,John O. Ledyard,Giulio Varsi,Male,Male,Male,Male,,5
10.0,2.0,Experimental Economics,05 April 2007,https://link.springer.com/article/10.1007/s10683-007-9170-4,Experimental tools,June 2007,,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Experimental Economics,07 February 2007,https://link.springer.com/article/10.1007/s10683-006-9159-4,z-Tree: Zurich toolbox for ready-made economic experiments,June 2007,Urs Fischbacher,,,Male,Unknown,Unknown,Male,,6506
10.0,2.0,Experimental Economics,13 February 2007,https://link.springer.com/article/10.1007/s10683-006-9141-1,Dissertation abstract: The experimental analysis of the political economics of fisheries governance,June 2007,Samuel Mulenga Bwalya,,,Male,Unknown,Unknown,Male,,1
10.0,2.0,Experimental Economics,15 February 2007,https://link.springer.com/article/10.1007/s10683-006-9142-0,Dissertation abstract: Three essays on the determinants of behavior in the commons-experimental evidence from fishing communities in Colombia,June 2007,María Alejandra Vélez,,,,Unknown,Unknown,Mix,,
10.0,2.0,Experimental Economics,13 February 2007,https://link.springer.com/article/10.1007/s10683-006-9147-8,Dissertation abstract: Contribution to a public good - Theoretical analysis and experimental evidence,June 2007,Walid Hichri,,,Male,Unknown,Unknown,Male,,
10.0,2.0,Experimental Economics,13 February 2007,https://link.springer.com/article/10.1007/s10683-006-9148-7,Dissertation abstract: “Essays in applied economics on the intervention of a third player in agency relationships”,June 2007,Nicolas Jacquemet,,,Male,Unknown,Unknown,Male,,
10.0,2.0,Experimental Economics,13 February 2007,https://link.springer.com/article/10.1007/s10683-006-9150-0,Dissertation abstract: A status theory of collective action,June 2007,Robb Willer,,,Male,Unknown,Unknown,Male,,
10.0,2.0,Experimental Economics,08 February 2007,https://link.springer.com/article/10.1007/s10683-006-9151-z,Dissertation abstract: Voting in the laboratory,June 2007,Jens Großer,,,Male,Unknown,Unknown,Male,,
10.0,2.0,Experimental Economics,08 February 2007,https://link.springer.com/article/10.1007/s10683-006-9154-9,Dissertation abstract: Contestability and the significance of the entrant’s home market,June 2007,Utteeyo Dasgupta,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Experimental Economics,28 March 2007,https://link.springer.com/article/10.1007/s10683-006-9158-5,Dissertation abstract: Essays on veto bargaining games,June 2007,Hankyoung Sung,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Experimental Economics,23 March 2007,https://link.springer.com/article/10.1007/s10683-007-9161-5,Dissertation abstract: Essays on pricing in experimental duopoly markets,June 2007,Shakun Datta,,,Unknown,Unknown,Unknown,Unknown,,
10.0,2.0,Experimental Economics,28 March 2007,https://link.springer.com/article/10.1007/s10683-007-9163-3,Dissertation abstract: Experiments on fairness and reputation,June 2007,Maroš Servátka,,,Male,Unknown,Unknown,Male,,
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9177-x,Introduction to Issue of Experimental Economics in Honor of Raymond C. Battalio,September 2007,John H. Kagel,John B. Van Huyck,,Male,Male,Unknown,Male,,1
10.0,3.0,Experimental Economics,25 August 2007,https://link.springer.com/article/10.1007/s10683-007-9175-z,Evidence on learning in coordination games,September 2007,John B. Van Huyck,Raymond C. Battalio,Frederick W. Rankin,Male,Male,Male,Male,,24
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9183-z,Equilibrium selection through incomplete information in coordination games: an experimental study,September 2007,Antonio Cabrales,Rosemarie Nagel,Roc Armenter,Male,Female,Unknown,Mix,,
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9176-y,Coordination of strategic responses to security threats: Laboratory evidence,September 2007,Rachel O. Hess,Charles A. Holt,Angela M. Smith,Female,Male,Female,Mix,,
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9181-1,Communication and coordination in the laboratory collective resistance game,September 2007,Timothy N. Cason,Vai-Lam Mui,,Male,Unknown,Unknown,Male,,17
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9182-0,Leadership and overcoming coordination failure with asymmetric costs,September 2007,Jordi Brandts,David J. Cooper,Enrique Fatas,Male,Male,Male,Male,,47
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9179-8,Solving coordination failure with “all-or-none” group-level incentives,September 2007,John Hamman,Scott Rick,Roberto A. Weber,Male,Male,Male,Male,,43
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9180-2,Hierarchical thinking and learning in rank order contests,September 2007,Octavian Carare,Ernan Haruvy,Ashutosh Prasad,Male,Unknown,Male,Male,,3
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9185-x,Social learning in coordination games: does status matter?,September 2007,Catherine C. Eckel,Rick K. Wilson,,Female,Male,Unknown,Mix,,
10.0,3.0,Experimental Economics,09 August 2007,https://link.springer.com/article/10.1007/s10683-007-9178-9,When and why? A critical survey on coordination failure in the laboratory,September 2007,Giovanna Devetag,Andreas Ortmann,,Female,Male,Unknown,Mix,,
10.0,4.0,Experimental Economics,01 February 2007,https://link.springer.com/article/10.1007/s10683-006-9138-9,Incentive effects and overcrowding in tournaments: An experimental analysis,December 2007,Donald Vandegrift,Abdullah Yavas,Paul M. Brown,Male,Male,Male,Male,,23
10.0,4.0,Experimental Economics,01 February 2007,https://link.springer.com/article/10.1007/s10683-006-9140-2,Can intertemporal choice experiments elicit time preferences for consumption?,December 2007,Robin P. Cubitt,Daniel Read,,,Male,Unknown,Mix,,
10.0,4.0,Experimental Economics,15 February 2007,https://link.springer.com/article/10.1007/s10683-006-9143-z,Information acquisition in the ultimatum game: An experimental study,December 2007,Anders U. Poulsen,Jonathan H. W. Tan,,Male,Male,Unknown,Male,,11
10.0,4.0,Experimental Economics,01 February 2007,https://link.springer.com/article/10.1007/s10683-006-9144-y,The impact of endowment heterogeneity and origin on contributions in best-shot public good games,December 2007,Stephan Kroll,Todd L. Cherry,Jason F. Shogren,Male,Male,Male,Male,,43
10.0,4.0,Experimental Economics,02 February 2007,https://link.springer.com/article/10.1007/s10683-006-9145-x,House money effects in public good experiments: Comment,December 2007,Glenn W. Harrison,,,Male,Unknown,Unknown,Male,,75
10.0,4.0,Experimental Economics,15 February 2007,https://link.springer.com/article/10.1007/s10683-006-9146-9,Amended final-offer arbitration over an uncertain value: A comparison with CA and FOA,December 2007,Cary Deck,Amy Farmer,Dao-Zhi Zeng,,Female,Unknown,Mix,,
11.0,1.0,Experimental Economics,23 March 2007,https://link.springer.com/article/10.1007/s10683-006-9156-7,"Implications of trust, fear, and reciprocity for modeling economic behavior",March 2008,James C. Cox,Klarita Sadiraj,Vjollca Sadiraj,Male,Female,Female,Mix,,
11.0,1.0,Experimental Economics,02 February 2007,https://link.springer.com/article/10.1007/s10683-006-9155-8,On recombinant estimation for experimental data,March 2008,Jason Abrevaya,,,Male,Unknown,Unknown,Male,,8
11.0,1.0,Experimental Economics,01 February 2007,https://link.springer.com/article/10.1007/s10683-006-9153-x,On the (in)effectiveness of rewards in sustaining cooperation,March 2008,Jana Vyrastekova,Daan van Soest,,Female,Male,Unknown,Mix,,
11.0,1.0,Experimental Economics,07 February 2007,https://link.springer.com/article/10.1007/s10683-006-9152-y,Experimental evidence on coverage choices and contract prices in the market for corporate insurance,March 2008,Gautam Goswami,Martin F. Grace,Michael J. Rebello,Male,Male,Male,Male,,6
11.0,1.0,Experimental Economics,15 February 2007,https://link.springer.com/article/10.1007/s10683-006-9149-6,A live experiment on approval voting,March 2008,Jean-François Laslier,Karine Van der Straeten,,Unknown,Female,Unknown,Female,,56
11.0,2.0,Experimental Economics,23 March 2007,https://link.springer.com/article/10.1007/s10683-006-9157-6,Revealing the depth of reasoning in p-beauty contest games,June 2008,Patrizia Sbriglia,,,Female,Unknown,Unknown,Female,,11
11.0,2.0,Experimental Economics,29 September 2007,https://link.springer.com/article/10.1007/s10683-007-9172-2,Dictator game giving: altruism or artefact?,June 2008,Nicholas Bardsley,,,Male,Unknown,Unknown,Male,,398
11.0,2.0,Experimental Economics,23 March 2007,https://link.springer.com/article/10.1007/s10683-007-9162-4,Increases in trust and altruism from partner selection: Experimental evidence,June 2008,Robert Slonim,Ellen Garbarino,,Male,Female,Unknown,Mix,,
11.0,2.0,Experimental Economics,14 July 2007,https://link.springer.com/article/10.1007/s10683-007-9164-2,Learning under supervision: an experimental study,June 2008,Raghuram Iyengar,Andrew Schotter,,Unknown,Male,Unknown,Male,,14
11.0,2.0,Experimental Economics,14 July 2007,https://link.springer.com/article/10.1007/s10683-007-9165-1,Individual sense of fairness: an experimental study,June 2008,Edi Karni,Tim Salmon,Barry Sopher,Male,Male,Male,Male,,45
11.0,2.0,Experimental Economics,14 July 2007,https://link.springer.com/article/10.1007/s10683-007-9166-0,Bidding ‘as if’ risk neutral in experimental first price auctions without information feedback,June 2008,Tibor Neugebauer,Javier Perote,,Male,,Unknown,Mix,,
11.0,3.0,Experimental Economics,24 May 2008,https://link.springer.com/article/10.1007/s10683-008-9201-9,Introduction to field experiments in economics with applications to the economics of charity,September 2008,John A. List,,,Male,Unknown,Unknown,Male,,27
11.0,3.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-008-9196-2,Peter Bohm: Father of field experiments,September 2008,Martin Dufwenberg,Glenn W. Harrison,,Male,Male,Unknown,Male,,10
11.0,3.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-007-9191-z,The impact of downward social information on contribution decisions,September 2008,Rachel Croson,Jen (Yue) Shang,,Female,Female,Unknown,Female,,174
11.0,3.0,Experimental Economics,08 March 2008,https://link.springer.com/article/10.1007/s10683-008-9198-0,Subsidizing charitable contributions: a natural field experiment comparing matching and rebate subsidies,September 2008,Catherine C. Eckel,Philip J. Grossman,,Female,Male,Unknown,Mix,,
11.0,3.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-007-9190-0,Matching and challenge gifts to charity: evidence from laboratory and natural field experiments,September 2008,Daniel Rondeau,John A. List,,Male,Male,Unknown,Male,,99
11.0,3.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-007-9192-y,Do people behave in experiments as in the field?—evidence from donations,September 2008,Matthias Benz,Stephan Meier,,Male,Male,Unknown,Male,,250
11.0,3.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-007-9193-x,Altruistic behavior in a representative dictator experiment,September 2008,Jeffrey Carpenter,Cristina Connolly,Caitlin Knowles Myers,Male,Female,Female,Mix,,
11.0,3.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-007-9194-9,Does context matter more for hypothetical than for actual contributions? Evidence from a natural field experiment,September 2008,Francisco Alpizar,Fredrik Carlsson,Olof Johansson-Stenman,Male,Male,Male,Male,,43
11.0,4.0,Experimental Economics,14 July 2007,https://link.springer.com/article/10.1007/s10683-007-9167-z,Coordinated voting in sequential and simultaneous elections: some experimental evidence,December 2008,Sugato Dasgupta,Kirk A. Randazzo,Kenneth C. Williams,Unknown,Male,Male,Male,,5
11.0,4.0,Experimental Economics,14 July 2007,https://link.springer.com/article/10.1007/s10683-007-9168-y,Voice matters in a dictator game,December 2008,Tetsuo Yamamori,Kazuhiko Kato,Akihiko Matsui,Male,Male,Male,Male,,89
11.0,4.0,Experimental Economics,14 July 2007,https://link.springer.com/article/10.1007/s10683-007-9169-x,Time is money: The effect of clock speed on seller’s revenue in Dutch auctions,December 2008,Elena Katok,Anthony M. Kwasnica,,Female,Male,Unknown,Mix,,
11.0,4.0,Experimental Economics,29 September 2007,https://link.springer.com/article/10.1007/s10683-007-9171-3,A comparative statics analysis of punishment in public-good experiments,December 2008,Nikos Nikiforakis,Hans-Theo Normann,,Male,Unknown,Unknown,Male,,278
11.0,4.0,Experimental Economics,29 September 2007,https://link.springer.com/article/10.1007/s10683-007-9173-1,Incremental approaches to establishing trust,December 2008,Robert Kurzban,Mary L. Rigdon,Bart J. Wilson,Male,,Male,Mix,,
11.0,4.0,Experimental Economics,29 September 2007,https://link.springer.com/article/10.1007/s10683-007-9174-0,Counterintuitive number effects in experimental oligopolies,December 2008,Henrik Orzen,,,Male,Unknown,Unknown,Male,,24
11.0,4.0,Experimental Economics,07 November 2007,https://link.springer.com/article/10.1007/s10683-007-9184-y,Fixed price plus rationing: an experiment,December 2008,Veronika Grimm,Jaromir Kovarik,Giovanni Ponti,Female,Male,Male,Mix,,
12.0,1.0,Experimental Economics,02 November 2007,https://link.springer.com/article/10.1007/s10683-007-9186-9,An experimental analysis of cooperation and productivity in the trust game,March 2009,Cary Deck,,,,Unknown,Unknown,Mix,,
12.0,1.0,Experimental Economics,07 November 2007,https://link.springer.com/article/10.1007/s10683-007-9187-8,Do people plan?,March 2009,John Bone,John D. Hey,John Suckling,Male,Male,Male,Male,,22
12.0,1.0,Experimental Economics,02 November 2007,https://link.springer.com/article/10.1007/s10683-007-9188-7,Group polarization in the team dictator game reconsidered,March 2009,Wolfgang J. Luhan,Martin G. Kocher,Matthias Sutter,Male,Male,Male,Male,,115
12.0,1.0,Experimental Economics,17 November 2007,https://link.springer.com/article/10.1007/s10683-007-9189-6,A comparison of first price multi-object auctions,March 2009,Katerina Sherstyuk,,,Female,Unknown,Unknown,Female,,5
12.0,1.0,Experimental Economics,15 February 2008,https://link.springer.com/article/10.1007/s10683-008-9195-3,"Re-matching, information and sequencing effects in posted offer markets",March 2009,Douglas Davis,Oleg Korenok,Robert Reilly,Male,Male,Male,Male,,8
12.0,1.0,Experimental Economics,10 April 2008,https://link.springer.com/article/10.1007/s10683-008-9197-1,Measuring conditional cooperation: a replication study in Russia,March 2009,Benedikt Herrmann,Christian Thöni,,Male,Male,Unknown,Male,,113
12.0,1.0,Experimental Economics,10 April 2008,https://link.springer.com/article/10.1007/s10683-008-9199-z,Are women expected to be more generous?,March 2009,Fernando Aguiar,Pablo Brañas-Garza,Luis M. Miller,Male,Male,Male,Male,,54
12.0,1.0,Experimental Economics,10 April 2008,https://link.springer.com/article/10.1007/s10683-008-9202-8,Reciprocity in the workplace,March 2009,Abigail Barr,Pieter Serneels,,Female,Male,Unknown,Mix,,
12.0,1.0,Experimental Economics,18 July 2008,https://link.springer.com/article/10.1007/s10683-008-9207-3,Subject pool effects in a corruption experiment: A comparison of Indonesian public servants and Indonesian students,March 2009,Vivi Alatas,Lisa Cameron,Lata Gangadharan,Female,Female,Female,Female,,81
12.0,2.0,Experimental Economics,13 June 2008,https://link.springer.com/article/10.1007/s10683-008-9203-7,Expected utility theory and prospect theory: one wedding and a decent funeral,June 2009,Glenn W. Harrison,E. Elisabet Rutström,,Male,Unknown,Unknown,Male,,253
12.0,2.0,Experimental Economics,01 August 2008,https://link.springer.com/article/10.1007/s10683-008-9206-4,The control of game form recognition in experiments: understanding dominant strategy failures in a simple two person “guessing” game,June 2009,Eileen Chou,Margaret McConnell,Charles R. Plott,Female,Female,Male,Mix,,
12.0,2.0,Experimental Economics,27 September 2008,https://link.springer.com/article/10.1007/s10683-008-9208-2,Would I lie to you? On social preferences and lying aversion,June 2009,Sjaak Hurkens,Navin Kartik,,Male,Male,Unknown,Male,,200
12.0,2.0,Experimental Economics,11 October 2008,https://link.springer.com/article/10.1007/s10683-008-9209-1,A note on peer effects between teams,June 2009,Rupert Sausgruber,,,Male,Unknown,Unknown,Male,,17
12.0,2.0,Experimental Economics,30 October 2008,https://link.springer.com/article/10.1007/s10683-008-9210-8,Uniform price auctions and fixed price offerings in IPOs: an experimental comparison,June 2009,Ping Zhang,,,,Unknown,Unknown,Mix,,
12.0,2.0,Experimental Economics,09 December 2008,https://link.springer.com/article/10.1007/s10683-008-9211-7,Enjoy the silence: an experiment on truth-telling,June 2009,Santiago Sánchez-Pagés,Marc Vorsatz,,Male,Male,Unknown,Male,,84
12.0,2.0,Experimental Economics,03 February 2009,https://link.springer.com/article/10.1007/s10683-008-9212-6,Testing theories of behavior for extensive-form two-player two-stage games,June 2009,Dale O. Stahl,Ernan Haruvy,,,Unknown,Unknown,Mix,,
12.0,2.0,Experimental Economics,13 June 2008,https://link.springer.com/article/10.1007/s10683-008-9205-5,A simplified test for preference rationality of two-commodity choice,June 2009,Samiran Banerjee,James H. Murphy,,Unknown,Male,Unknown,Male,,1
12.0,3.0,Experimental Economics,24 June 2009,https://link.springer.com/article/10.1007/s10683-009-9218-8,Experimental investigation of stationary concepts in cyclic duopoly games,September 2009,Sebastian J. Goerg,Reinhard Selten,,Male,Male,Unknown,Male,,7
12.0,3.0,Experimental Economics,14 February 2009,https://link.springer.com/article/10.1007/s10683-009-9214-z,What norms trigger punishment?,September 2009,Jeffrey Carpenter,Peter Hans Matthews,,Male,Male,Unknown,Male,,56
12.0,3.0,Experimental Economics,29 April 2009,https://link.springer.com/article/10.1007/s10683-009-9216-x,Price leadership and firm size asymmetry: an experimental analysis,September 2009,Shakun Datta Mago,Emmanuel Dechenaux,,Unknown,Male,Unknown,Male,,15
12.0,3.0,Experimental Economics,09 June 2009,https://link.springer.com/article/10.1007/s10683-009-9217-9,Gender pairing and bargaining—Beware the same sex!,September 2009,Matthias Sutter,Ronald Bosman,Frans van Winden,Male,Male,Male,Male,"Supplementary material accompanying “Gender pairing and bargaining – Beware the same sex!” by Matthias Sutter, Ronald Bosman, Martin Kocher and Frans van Winden (doc 117KB)",71
12.0,3.0,Experimental Economics,27 February 2009,https://link.springer.com/article/10.1007/s10683-009-9215-y,Range effects and lottery pricing,September 2009,Pavlo R. Blavatskyy,Wolfgang R. Köhler,,Male,Male,Unknown,Male,,11
12.0,3.0,Experimental Economics,03 February 2009,https://link.springer.com/article/10.1007/s10683-009-9213-0,Expectation formation and regime switches,September 2009,Otwin Becker,Johannes Leitner,Ulrike Leopold-Wildburger,Male,Male,Female,Mix,,
12.0,3.0,Experimental Economics,13 June 2008,https://link.springer.com/article/10.1007/s10683-008-9204-6,Elicitation using multiple price list formats,September 2009,Steffen Andersen,Glenn W. Harrison,E. Elisabet Rutström,Male,Male,Unknown,Male,,4
12.0,4.0,Experimental Economics,21 July 2009,https://link.springer.com/article/10.1007/s10683-009-9219-7,Changing the probability versus changing the reward,December 2009,David M. Bruner,,,Male,Unknown,Unknown,Male,,24
12.0,4.0,Experimental Economics,29 July 2009,https://link.springer.com/article/10.1007/s10683-009-9220-1,How effectively do people learn from a variety of different opinions?,December 2009,Andrew Healy,,,Male,Unknown,Unknown,Male,,7
12.0,4.0,Experimental Economics,21 August 2009,https://link.springer.com/article/10.1007/s10683-009-9221-0,Risky procurement with an insider bidder,December 2009,Jan Boone,Roy Chen,Angelo Polydoro,Male,Male,Male,Male,,6
12.0,4.0,Experimental Economics,25 August 2009,https://link.springer.com/article/10.1007/s10683-009-9222-z,Promoting justice by treating people unequally: an experimental study,December 2009,Alice Becker,Luis M. Miller,,Female,Male,Unknown,Mix,,
12.0,4.0,Experimental Economics,16 September 2009,https://link.springer.com/article/10.1007/s10683-009-9223-y,Learning and sophistication in coordination games,December 2009,Kyle Hyndman,Antoine Terracol,Jonathan Vaksmann,,Male,Male,Mix,,
12.0,4.0,Experimental Economics,09 October 2009,https://link.springer.com/article/10.1007/s10683-009-9224-x,How certain is the uncertainty effect?,December 2009,Ondřej Rydval,Andreas Ortmann,Ralph Hertwig,Male,Male,Male,Male,,30
12.0,4.0,Experimental Economics,03 October 2009,https://link.springer.com/article/10.1007/s10683-009-9225-9,The effects of externalities and framing on bribery in a petty corruption experiment,December 2009,Abigail Barr,Danila Serra,,Female,Male,Unknown,Mix,,
13.0,1.0,Experimental Economics,08 October 2009,https://link.springer.com/article/10.1007/s10683-009-9226-8,Are ‘true’ preferences revealed in repeated markets? An experimental demonstration of context-dependent valuations,March 2010,Fabio Tufano,,,Male,Unknown,Unknown,Male,,28
13.0,1.0,Experimental Economics,03 October 2009,https://link.springer.com/article/10.1007/s10683-009-9227-7,Do preferences for charitable giving help auctioneers?,March 2010,R. Mark Isaac,Svetlana Pevnitskaya,Timothy C. Salmon,Unknown,Female,Male,Mix,,
13.0,1.0,Experimental Economics,13 October 2009,https://link.springer.com/article/10.1007/s10683-009-9228-6,"Cooperation without coordination: signaling, types and tacit collusion in laboratory oligopolies",March 2010,Douglas Davis,Oleg Korenok,Robert Reilly,Male,Male,Male,Male,,15
13.0,1.0,Experimental Economics,20 October 2009,https://link.springer.com/article/10.1007/s10683-009-9229-5,Saliency of outside options in the lost wallet game,March 2010,James C. Cox,Maroš Servátka,Radovan Vadovič,Male,Male,Male,Male,,14
13.0,1.0,Experimental Economics,30 October 2009,https://link.springer.com/article/10.1007/s10683-009-9230-z,Experimenter demand effects in economic experiments,March 2010,Daniel John Zizzo,,,Male,Unknown,Unknown,Male,,708
13.0,1.0,Experimental Economics,09 December 2009,https://link.springer.com/article/10.1007/s10683-009-9231-y,Dominated choices in a simple game with large stakes,March 2010,Egil Matsen,Bjarne Strøm,,Male,Male,Unknown,Male,,3
13.0,2.0,Experimental Economics,18 December 2009,https://link.springer.com/article/10.1007/s10683-009-9232-x,Fragility of information cascades: an experimental study using elicited beliefs,June 2010,Anthony Ziegelmeyer,Frédéric Koessler,Eyal Winter,Male,Male,Male,Male,,26
13.0,2.0,Experimental Economics,11 February 2010,https://link.springer.com/article/10.1007/s10683-010-9233-9,An experimental test of Taylor-type rules with inexperienced central bankers,June 2010,Jim Engle-Warnick,Nurlan Turdaliev,,Male,Male,Unknown,Male,,17
13.0,2.0,Experimental Economics,11 March 2010,https://link.springer.com/article/10.1007/s10683-010-9234-8,Veto power in committees: an experimental study,June 2010,John H. Kagel,Hankyoung Sung,Eyal Winter,Male,Unknown,Male,Male,"Instructions for “Veto Power in Committees: An Experimental Study, J. Kagel, H. Sung and E. Winter (Instructions for veto game with δ=.95)”. (DOC 36.5 KB) Instructions for “Veto Power in Committees: An Experimental Study, J. Kagel, H. Sung and E. Winter (Instructions for control treatment—no veto player—with δ=.95)”. (DOC 35.5 KB)",53
13.0,2.0,Experimental Economics,17 March 2010,https://link.springer.com/article/10.1007/s10683-010-9235-7,My money or yours: house money payment effects,June 2010,Larry R. Davis,B. Patrick Joyce,Matthew R. Roelofs,Male,Unknown,Male,Male,"Below is the link to the electronic supplementary material. 
                        ",18
13.0,2.0,Experimental Economics,10 March 2010,https://link.springer.com/article/10.1007/s10683-010-9236-6,Do people make strategic commitments? Experimental evidence on strategic information avoidance,June 2010,Anders U. Poulsen,Michael W. M. Roos,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",13
13.0,2.0,Experimental Economics,19 March 2010,https://link.springer.com/article/10.1007/s10683-010-9237-5,"Cheating, emotions, and rationality: an experiment on tax evasion",June 2010,Giorgio Coricelli,Mateus Joffily,Marie Claire Villeval,Male,Male,Female,Mix,,
13.0,3.0,Experimental Economics,01 May 2010,https://link.springer.com/article/10.1007/s10683-010-9239-3,Maximum effort in the minimum-effort game,September 2010,Dirk Engelmann,Hans-Theo Normann,,Male,Unknown,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",34
13.0,3.0,Experimental Economics,30 April 2010,https://link.springer.com/article/10.1007/s10683-010-9240-x,Similarities and differences when building trust: the role of cultures,September 2010,Fabian Bornhorst,Andrea Ichino,Eyal Winter,Male,Female,Male,Mix,,
13.0,3.0,Experimental Economics,06 May 2010,https://link.springer.com/article/10.1007/s10683-010-9241-9,Bubble measures in experimental asset markets,September 2010,Thomas Stöckl,Jürgen Huber,Michael Kirchler,Male,Male,Male,Male,,164
13.0,3.0,Experimental Economics,18 May 2010,https://link.springer.com/article/10.1007/s10683-010-9242-8,Gender and generosity: does degree of anonymity or group gender composition matter?,September 2010,C. Bram Cadsby,Maroš Servátka,Fei Song,Unknown,Male,,Mix,,
13.0,3.0,Experimental Economics,21 May 2010,https://link.springer.com/article/10.1007/s10683-010-9243-7,The irrelevant-menu affect on valuation,September 2010,Doron Sonsino,,,Male,Unknown,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",6
13.0,3.0,Experimental Economics,19 May 2010,https://link.springer.com/article/10.1007/s10683-010-9244-6,Bounding preference parameters under different assumptions about beliefs: a partial identification approach,September 2010,Charles Bellemare,Luc Bissonnette,Sabine Kröger,Male,Male,Female,Mix,,
13.0,3.0,Experimental Economics,09 June 2010,https://link.springer.com/article/10.1007/s10683-010-9245-5,Recommended play and performance bonuses in the minimum effort coordination game,September 2010,Ananish Chaudhuri,Tirnud Paichayontvijit,,Unknown,Unknown,Unknown,Unknown,,
13.0,3.0,Experimental Economics,13 July 2010,https://link.springer.com/article/10.1007/s10683-010-9246-4,The effects of (incentivized) belief elicitation in public goods experiments,September 2010,Simon Gächter,Elke Renner,,Male,Female,Unknown,Mix,,
13.0,4.0,Experimental Economics,16 July 2010,https://link.springer.com/article/10.1007/s10683-010-9247-3,Information aggregation in experimental asset markets in the presence of a manipulator,December 2010,Helena Veiga,Marc Vorsatz,,Female,Male,Unknown,Mix,,
13.0,4.0,Experimental Economics,13 July 2010,https://link.springer.com/article/10.1007/s10683-010-9248-2,An experimental analysis of team production in networks,December 2010,Enrique Fatas,Miguel A. Meléndez-Jiménez,Hector Solaz,Male,Male,Male,Male,"Below are the links to the electronic supplementary material. 
                           
                        ",23
13.0,4.0,Experimental Economics,22 July 2010,https://link.springer.com/article/10.1007/s10683-010-9249-1,Belief elicitation in experiments: is there a hedging problem?,December 2010,Mariana Blanco,Dirk Engelmann,Hans-Theo Normann,Female,Male,Unknown,Mix,,
13.0,4.0,Experimental Economics,27 July 2010,https://link.springer.com/article/10.1007/s10683-010-9250-8,Competition and innovation: an experimental investigation,December 2010,Donja Darai,Dario Sacco,Armin Schmutzler,Unknown,Male,Male,Male,"Below are the links to the electronic supplementary material. 
                           
                        ",25
13.0,4.0,Experimental Economics,28 July 2010,https://link.springer.com/article/10.1007/s10683-010-9251-7,A finite mixture analysis of beauty-contest data using generalized beta distributions,December 2010,Antoni Bosch-Domènech,José G. Montalvo,Albert Satorra,Male,Male,Male,Male,,20
13.0,4.0,Experimental Economics,26 August 2010,https://link.springer.com/article/10.1007/s10683-010-9252-6,Scheduling with package auctions,December 2010,Kan Takeuchi,John C. Lin,Thomas A. Finholt,,Male,Male,Mix,,
13.0,4.0,Experimental Economics,16 September 2010,https://link.springer.com/article/10.1007/s10683-010-9253-5,‘Give me a chance!’ An experiment in social decision under risk,December 2010,Michal Krawczyk,Fabrice Le Lec,,Male,Male,Unknown,Male,,105
14.0,1.0,Experimental Economics,15 September 2010,https://link.springer.com/article/10.1007/s10683-010-9254-4,Classification of natural language messages using a coordination game,March 2011,Daniel Houser,Erte Xiao,,Male,Unknown,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",91
14.0,1.0,Experimental Economics,16 September 2010,https://link.springer.com/article/10.1007/s10683-010-9255-3,Does trust extend beyond the village? Experimental trust and social distance in Cameroon,March 2011,Alvin Etang,David Fielding,Stephen Knowles,Male,Male,Male,Male,,58
14.0,1.0,Experimental Economics,24 September 2010,https://link.springer.com/article/10.1007/s10683-010-9256-2,“A profit table or a profit calculator?” A note on the design of Cournot oligopoly experiments,March 2011,Till Requate,Israel Waichman,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",28
14.0,1.0,Experimental Economics,29 September 2010,https://link.springer.com/article/10.1007/s10683-010-9257-1,Sustaining cooperation in laboratory public goods experiments: a selective survey of the literature,March 2011,Ananish Chaudhuri,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Experimental Economics,20 October 2010,https://link.springer.com/article/10.1007/s10683-010-9258-0,Learning to respect property by refashioning theft into trade,March 2011,Erik O. Kimbrough,,,Male,Unknown,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",6
14.0,1.0,Experimental Economics,27 October 2010,https://link.springer.com/article/10.1007/s10683-010-9259-z,"Pricing accuracy, liquidity and trader behavior with closing price manipulation",March 2011,Carole Comerton-Forde,Tālis J. Putniņš,,Female,Male,Unknown,Mix,,
14.0,2.0,Experimental Economics,03 November 2010,https://link.springer.com/article/10.1007/s10683-010-9260-6,"Hidden information, bargaining power, and efficiency: an experiment",May 2011,Antonio Cabrales,Gary Charness,Marie Claire Villeval,Male,Male,Female,Mix,,
14.0,2.0,Experimental Economics,12 November 2010,https://link.springer.com/article/10.1007/s10683-010-9261-5,The role of role uncertainty in modified dictator games,May 2011,Nagore Iriberri,Pedro Rey-Biel,,Female,Male,Unknown,Mix,,
14.0,2.0,Experimental Economics,07 December 2010,https://link.springer.com/article/10.1007/s10683-010-9262-4,Do price-tags influence consumers’ willingness to pay? On the external validity of using auctions for measuring value,May 2011,Laurent Muller,Bernard Ruffieux,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",21
14.0,2.0,Experimental Economics,13 November 2010,https://link.springer.com/article/10.1007/s10683-010-9263-3,"Peer pressure, social spillovers, and reciprocity: an experimental analysis",May 2011,Luigi Mittone,Matteo Ploner,,Male,Male,Unknown,Male,,24
14.0,2.0,Experimental Economics,26 November 2010,https://link.springer.com/article/10.1007/s10683-010-9264-2,Experimental asset markets with endogenous choice of costly asymmetric information,May 2011,Jürgen Huber,Martin Angerer,Michael Kirchler,Male,Male,Male,Male,,33
14.0,2.0,Experimental Economics,26 November 2010,https://link.springer.com/article/10.1007/s10683-010-9265-1,On the incentive effects of monitoring: evidence from the lab and the field,May 2011,Amadou Boly,,,Unknown,Unknown,Unknown,Unknown,,
14.0,2.0,Experimental Economics,09 December 2010,https://link.springer.com/article/10.1007/s10683-010-9266-0,Framing and free riding: emotional responses and punishment in social dilemma games,May 2011,Robin P. Cubitt,Michalis Drouvelis,Simon Gächter,,Male,Male,Mix,,
14.0,2.0,Experimental Economics,06 January 2011,https://link.springer.com/article/10.1007/s10683-010-9267-z,Remain silent and ye shall suffer: seller exploitation of reticent buyers in an experimental reputation system,May 2011,Robert S. Gazzale,Tapan Khopkar,,Male,Male,Unknown,Male,,10
14.0,3.0,Experimental Economics,10 December 2010,https://link.springer.com/article/10.1007/s10683-010-9268-y,"Public goods provision, inequality and taxes",September 2011,Neslihan Uler,,,Female,Unknown,Unknown,Female,,14
14.0,3.0,Experimental Economics,18 December 2010,https://link.springer.com/article/10.1007/s10683-010-9269-x,Are benevolent dictators altruistic in groups? A within-subject design,September 2011,Lucy F. Ackert,Ann B. Gillette,Mark Rider,Female,Female,Male,Mix,,
14.0,3.0,Experimental Economics,14 January 2011,https://link.springer.com/article/10.1007/s10683-010-9270-4,Matching markets with price bargaining,September 2011,Philipp E. Otto,Friedel Bolle,,Male,,Unknown,Mix,,
14.0,3.0,Experimental Economics,20 February 2011,https://link.springer.com/article/10.1007/s10683-010-9271-3,Saving behavior and cognitive abilities,September 2011,T. Parker Ballinger,Eric Hudson,Nathaniel T. Wilcox,Unknown,Male,Male,Male,"Below is the link to the electronic supplementary material. 
                        ",53
14.0,3.0,Experimental Economics,21 January 2011,https://link.springer.com/article/10.1007/s10683-011-9272-x,The strategy versus the direct-response method: a first survey of experimental comparisons,September 2011,Jordi Brandts,Gary Charness,,Male,Male,Unknown,Male,,474
14.0,3.0,Experimental Economics,20 February 2011,https://link.springer.com/article/10.1007/s10683-011-9273-9,The online laboratory: conducting experiments in a real labor market,September 2011,John J. Horton,David G. Rand,Richard J. Zeckhauser,Male,Male,Male,Male,,1029
14.0,3.0,Experimental Economics,27 January 2011,https://link.springer.com/article/10.1007/s10683-011-9274-8,Measuring altruism in a public goods experiment: a comparison of U.S. and Czech subjects,September 2011,Lisa R. Anderson,Francis J. DiTraglia,Jeffrey R. Gerlach,Female,Male,Male,Mix,,
14.0,4.0,Experimental Economics,18 March 2011,https://link.springer.com/article/10.1007/s10683-011-9275-7,"So you want to run an experiment, now what? Some simple rules of thumb for optimal experimental design",November 2011,John A. List,Sally Sadoff,Mathis Wagner,Male,Female,Male,Mix,,
14.0,4.0,Experimental Economics,18 March 2011,https://link.springer.com/article/10.1007/s10683-011-9276-6,The sustainability of the commons: giving and receiving,November 2011,Nuria Osés-Eraso,Montserrat Viladrich-Grau,,Female,Female,Unknown,Female,,11
14.0,4.0,Experimental Economics,29 March 2011,https://link.springer.com/article/10.1007/s10683-011-9277-5,What brings your subjects to the lab? A field experiment,November 2011,Michal Krawczyk,,,Male,Unknown,Unknown,Male,,24
14.0,4.0,Experimental Economics,31 March 2011,https://link.springer.com/article/10.1007/s10683-011-9278-4,"Heterogeneous preferences for altruism: gender and personality, social status, giving and taking",November 2011,Michael S. Visser,Matthew R. Roelofs,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",38
14.0,4.0,Experimental Economics,06 April 2011,https://link.springer.com/article/10.1007/s10683-011-9279-3,Separating real incentives and accountability,November 2011,Ferdinand M. Vieider,,,Male,Unknown,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",29
14.0,4.0,Experimental Economics,07 May 2011,https://link.springer.com/article/10.1007/s10683-011-9280-x,The dynamics of responder behavior in ultimatum games: a meta-study,November 2011,David J. Cooper,E. Glenn Dutcher,,Male,Unknown,Unknown,Male,,64
14.0,4.0,Experimental Economics,03 May 2011,https://link.springer.com/article/10.1007/s10683-011-9281-9,Coordination and cooperation in asymmetric commons dilemmas,November 2011,Marco A. Janssen,John M. Anderies,Sanket R. Joshi,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",46
14.0,4.0,Experimental Economics,30 April 2011,https://link.springer.com/article/10.1007/s10683-011-9282-8,Outrunning the gender gap—boys and girls compete equally,November 2011,Anna Dreber,Emma von Essen,Eva Ranehill,Female,Female,Female,Female,,91
14.0,4.0,Experimental Economics,20 May 2011,https://link.springer.com/article/10.1007/s10683-011-9283-7,Dictator games: a meta study,November 2011,Christoph Engel,,,Male,Unknown,Unknown,Male,"Below is the link to the electronic supplementary material. 
                        ",1003
14.0,4.0,Experimental Economics,17 May 2011,https://link.springer.com/article/10.1007/s10683-011-9284-6,"Kindness, confusion, or … ambiguity?",November 2011,Carmela Di Mauro,Massimo Finocchiaro Castro,,Female,Male,Unknown,Mix,,
15.0,1.0,Experimental Economics,18 May 2011,https://link.springer.com/article/10.1007/s10683-011-9285-5,Are happiness and productivity lower among young people with newly-divorced parents? An experimental and econometric approach,March 2012,Eugenio Proto,Daniel Sgroi,Andrew J. Oswald,Male,Male,Male,Male,"Below is the link to the electronic supplementary material. 
                ",8
15.0,1.0,Experimental Economics,17 May 2011,https://link.springer.com/article/10.1007/s10683-011-9286-4,Revisiting strategic versus non-strategic cooperation,March 2012,Ernesto Reuben,Sigrid Suetens,,Male,Female,Unknown,Mix,,
15.0,1.0,Experimental Economics,26 May 2011,https://link.springer.com/article/10.1007/s10683-011-9287-3,Behavior in second-price auctions by highly experienced eBay buyers and sellers,March 2012,Rodney J. Garratt,Mark Walker,John Wooders,Male,Male,Male,Male,,38
15.0,1.0,Experimental Economics,07 June 2011,https://link.springer.com/article/10.1007/s10683-011-9288-2,Health effects on children’s willingness to compete,March 2012,Björn Bartling,Ernst Fehr,Daniel Schunk,Male,Male,Male,Male,,20
15.0,1.0,Experimental Economics,10 June 2011,https://link.springer.com/article/10.1007/s10683-011-9289-1,Perfect and imperfect real-time monitoring in a minimum-effort game,March 2012,Cary Deck,Nikos Nikiforakis,,,Male,Unknown,Mix,,
15.0,1.0,Experimental Economics,19 July 2011,https://link.springer.com/article/10.1007/s10683-011-9290-8,The impact of instructions and procedure on reducing confusion and bubbles in experimental asset markets,March 2012,Jürgen Huber,Michael Kirchler,,Male,Male,Unknown,Male,,57
15.0,1.0,Experimental Economics,28 June 2011,https://link.springer.com/article/10.1007/s10683-011-9291-7,Market composition and experience in common-value auctions,March 2012,Johanna M. M. Goertz,,,Female,Unknown,Unknown,Female,"Below is the link to the electronic supplementary material. 
                ",2
15.0,1.0,Experimental Economics,23 July 2011,https://link.springer.com/article/10.1007/s10683-011-9293-5,Risk aversion and framing effects,March 2012,Louis Lévy-Garboua,Hela Maafi,Antoine Terracol,Male,Female,Male,Mix,,
15.0,1.0,Experimental Economics,30 July 2011,https://link.springer.com/article/10.1007/s10683-011-9294-4,Whose money is it anyway? Using prepaid incentives in experimental economics to create a natural environment,March 2012,Mosi Rosenboim,Tal Shavit,,Unknown,,Unknown,Mix,,
15.0,1.0,Experimental Economics,05 August 2011,https://link.springer.com/article/10.1007/s10683-011-9295-3,Self-interest and fairness: self-serving choices of justice principles,March 2012,Ismael Rodriguez-Lara,Luis Moreno-Garrido,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                ",64
15.0,1.0,Experimental Economics,04 August 2011,https://link.springer.com/article/10.1007/s10683-011-9296-2,Belief formation: an experiment with outside observers,March 2012,Kyle Hyndman,Erkut Y. Özbay,Wolf Ehrblatt,,Male,Male,Mix,,
15.0,1.0,Experimental Economics,26 August 2011,https://link.springer.com/article/10.1007/s10683-011-9297-1,Does competition foster trust? The role of tournament incentives,March 2012,Steffen Keck,Natalia Karelaia,,Male,Female,Unknown,Mix,,
15.0,1.0,Experimental Economics,20 August 2011,https://link.springer.com/article/10.1007/s10683-011-9298-0,Decomposing desert and tangibility effects in a charitable giving experiment,March 2012,David Reinstein,Gerhard Riener,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                ",62
15.0,2.0,Experimental Economics,21 February 2012,https://link.springer.com/article/10.1007/s10683-011-9292-6,Peer punishment in teams: expressive or instrumental choice?,June 2012,Marco Casari,Luigi Luini,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                ",33
15.0,2.0,Experimental Economics,08 September 2011,https://link.springer.com/article/10.1007/s10683-011-9299-z,Experts with a conflict of interest: a source of ambiguity?,June 2012,Douglas A. Norton,R. Mark Isaac,,Male,Unknown,Unknown,Male,"Below are the links to the electronic supplementary material. 
                  
                  
                ",2
15.0,2.0,Experimental Economics,06 September 2011,https://link.springer.com/article/10.1007/s10683-011-9300-x,"It is Hobbes, not Rousseau: an experiment on voting and redistribution",June 2012,Antonio Cabrales,Rosemarie Nagel,José V. Rodríguez Mora,Male,Female,Male,Mix,,
15.0,2.0,Experimental Economics,09 September 2011,https://link.springer.com/article/10.1007/s10683-011-9301-9,Discrete clock auctions: an experimental study,June 2012,Peter Cramton,Emel Filiz-Ozbay,Pacharasut Sujarittanonta,Male,Female,Unknown,Mix,,
15.0,2.0,Experimental Economics,01 October 2011,https://link.springer.com/article/10.1007/s10683-011-9302-8,Hidden costs of control: four repetitions and an extension,June 2012,Anthony Ziegelmeyer,Katrin Schmelz,Matteo Ploner,Male,Female,Male,Mix,,
15.0,2.0,Experimental Economics,24 September 2011,https://link.springer.com/article/10.1007/s10683-011-9303-7,A field experiment on the impact of weather shocks and insurance on risky investment,June 2012,Ruth Vargas Hill,Angelino Viceisza,,Female,Male,Unknown,Mix,,
15.0,3.0,Experimental Economics,29 September 2011,https://link.springer.com/article/10.1007/s10683-011-9304-6,Two heads are less bubbly than one: team decision-making in an experimental asset market,September 2012,Stephen L. Cheung,Stefan Palan,,Male,Male,Unknown,Male,"Below is the link to the electronic supplementary material. 
                ",32
15.0,3.0,Experimental Economics,15 October 2011,https://link.springer.com/article/10.1007/s10683-011-9305-5,Social communication and discrimination: a video experiment,September 2012,Ben Greiner,Werner Güth,Ro’i Zultan,Male,Male,Unknown,Male,"Traditionally, most human interactions throughout history were done face-to-face. With the advance of communication technology, however, many interactions have become more anonymous and impersonal. Although communication can play a crucial role in strategic interactions, game theoretical models focus on the content of the communication, so that the act of face-to-face communication in itself has no impact on the theoretical results. Moreover, cheap talk or costly messages should have no effect on rational players when the preferences of the players are strictly opposed and commonly known (cf. Austen-Smith and Banks 2000; Crawford and Sobel 1982). Nonetheless, it seems well established that people in fact behave differently when interacting with others following pre-play face-to-face communication (e.g. Dawes 1990; Frohlich and Oppenheimer 1998). Specifically, in simple bargaining games, face-to-face communication has been shown to induce generosity (Roth 1995; Schmidt and Zultan 2005).Footnote 1
 Explanations for these effects can be broadly categorized as belonging to two general types. The first type attributes communication effects to changes in preferences, triggered by learning about attributes of others. Examples are group identity or empathy.Footnote 2 The identifiable victim effect (that people give more to identified receivers, see Schelling 1968; Small and Loewenstein 2003) gets stronger when more information is provided about the receiver (Bohnet and Frey 1999; Charness and Gneezy 2008), suggesting a genuine change in the social preferences of “dictator” participants. Face-to-face communication automatically implies identifiability and vividness, and therefore is likely to have an effect on social preferences. However, previous studies have failed to find an effect for mere visual exposure or vividness (Bohnet and Frey 1999; Jenni and Loewenstein 1997). Another branch of the literature sees pre-play communication effects as caused by strategic aspects: since verbal and non-verbal channels of communication eliminate anonymity, players are confronted with something like a repeated game where their reputation is at risk. Additionally, face-to-face communication serves to support and enhance the strategic aspects of the communication, making promises, threats, or coordination proposals strategically meaningful (Brosig et al. 2003). Roth (1995) referred to these explanations as the Uncontrolled Social Utility Hypothesis and the Communication Hypothesis. He used an ultimatum bargaining experiment to compare two conditions of pre-play face-to-face communication: unrestricted, and restricted to non-game topics. Both communication treatments were equally successful in inducing nearly equal splits, thus rejecting the Communication Hypothesis of additional strategic effects.Footnote 3 Conversely, Brosig et al. (2003) observed in 4-person public goods experiments that lifting anonymity (via video screen) does not enhance contributions, and therefore did not find support for pure Social Utility. However, none of the existing studies was aimed at disentangling the different types of processes, making it hard to rule out either one. On the one hand, even under restricted bilateral communication, strategic effects of non-verbal communication and reputation concerns may still be present in ultimatum and public good games. On the other hand, social utility theories require more than pure visual identification to stimulate social processes. In this paper we aim to more clearly distinguish how communication influences generosity, by introducing an experimental design ruling out strategic effects in order to examine purely social effects of communication. Specifically we implement a three-person dictator game with unilateral video-based pre-play communication from receivers to dictators.Footnote 4 In this environment, communicators have no strategic power. Therefore, the design does not allow for (explicit or implicit) strategic information such as threats or promises in the video messages. Furthermore, the addressed powerful player is not susceptible to (out-of-the-lab) reputation effects since the one-sidedness of communication makes her completely anonymous. Thus, all communication effects obtained in this experiment can be construed as purely social effects and attributed to changes in the preferences of the dictators due to the communication from the receivers. Our game is actually very much alike the TV ads published by charities in which potential receivers directly speak into the camera. Thus, our results are also applicable to the question of how to increase generosity using one-sided communication channels. In real life, communication between two persons is seldom totally isolated. People are embedded in social networks, or there are even unrelated bystanders present. Those third parties may be indirectly affected by the communication between the two direct communicators. In particular, we hypothesize that there are not only direct social effects of communication, but also indirect positive or negative external spillover effects. Social communication between two persons might weaken social ties and lower generosity to third parties, e.g. in the sense of crowding out, or it may increase such generosity as a by-product of increased sociability. Having two receivers in our 3-person dictator game allows us to identify effects of communication on dictator discrimination, which are absent in the standard two-players dictator game. We distinguish three communication treatments: a no-communication baseline, a video-only treatment where both receivers are seen, but not heard by the dictator, and an audio-visual treatment where additionally one receiver is heard, but not the other.Footnote 5 To control for social perceptions, we elicit social ratings of receivers in the communication treatments utilizing the semantic differential of activity, evaluation, and potential (Osgood et al. 1957). We complement this data with similar ratings obtained from external judges, who are either informed or uninformed about the experimental game. Thus, our paper contributes to the literature by (1) testing the robustness of existing results in a new experimental framework utilizing video-technology and controlling for strategic effects of communication, (2) studying communication effects not only on (average) donations but also on discrimination between potential receivers, and (3) analyzing how the formation of social evaluations and impressions based on received communication can explain dictators’ donation and discrimination decisions. According to our experimental results, donations are higher when the receiver is both seen and heard. In line with the existing literature, mere visual exposure is not enough to induce dictator generosity (Bohnet and Frey 1999). However, while only audio-visual communication is sufficient to increase average donations, discrimination between receivers is already observed with video messages only. In addition, we find that social ratings of receivers (measured in an activity/potency and an evaluation dimension) are highly correlated with generosity towards them and discriminate both within and between receiver pairs, in both visual and audio-visual treatments. An analysis of the external ratings of the video messages, showing a similar correlation, establishes a causal relationship between the impression made by a receiver and what she receives. However, we find no evidence that specific content of the messages (i.e. whether communicators discuss the game or refer to specific distributions) plays a systematic role in the dictators’ decisions. Thus, our results allow us to establish and characterize previously unstudied effects of communication on dictator game behavior. Purely social factors play a role in communication in bargaining, at least when strategic issues are absent. Unilateral communication generates social ties towards communicators, even when the audio channel is omitted. With the audio channel these ties translate to significantly higher donations to (some) receivers, but in a discriminatory way. Dictators’ generosity seems to be driven by a general impression formation rather than the game-related content of the messages. The paper proceeds as follows: Sect. 2 introduces our experimental design and procedures in detail. Section 3 presents our results, and Sect. 4 concludes.",11
15.0,3.0,Experimental Economics,22 October 2011,https://link.springer.com/article/10.1007/s10683-011-9306-4,Random incentive systems in a dynamic choice experiment,September 2012,Guido Baltussen,G. Thierry Post,Peter P. Wakker,Male,Unknown,Male,Male,"RISs are known under several names, including random lottery incentive system (Starmer and Sugden 1991), random lottery selection method (Holt 1986), random problem selection procedure (Beattie and Loomes 1997), and random round payoff mechanism (Lee 2008). The different names apply to particular types of experiments (risky choice or social dilemma), rewards (lotteries or outcomes), or tasks (composite or single-choice). We use random incentive system, because it can be used for any type of experiment, reward, and task. Holt (1986) raised a serious concern about WRISs and stated that subjects may not perceive each choice in the experiment in isolation. Rather, they may perceive the choices together as a meta-lottery, or a probability distribution over the different choices in the whole experiment and their resulting outcomes. Such a perception may lead to contamination effects between tasks if subjects violate the independence condition of expected utility. A large body of research indicates that people indeed systematically violate this condition (Allais 1953; Carlin 1992; Starmer 2000). In the literature, the extreme and implausible case where subjects perfectly integrate all choices and the RIS lottery into one meta-lottery is known as reduction. Milder forms are also conceivable, where subjects do not combine all choices and the RIS lottery precisely, but where they do take some properties of the meta-lottery into account. Contrary to what has sometimes been thought, independence (together with appropriate dynamic principles) is sufficient but not necessary for the validity of RISs. The case where subjects take each choice in the experiment as a single real choice is called isolation. Isolation leads to proper experimental measurements under RISs, also if independence is violated on other occasions. The validity of the WRIS has been investigated in several studies. In a cleverly designed experiment based on simple, pairwise decision problems, Starmer and Sugden (1991) found isolation verified. However, in a direct comparison of the choices in a RIS treatment with those in a sample with guaranteed payment, they found a marginally significant difference. This difference has not been confirmed in later studies. Using more subjects, Beattie and Loomes (1997) and Cubitt et al. (1998a) concluded that there is no evidence of contamination for simple, pairwise decisions. Camerer (1989) also found that WRISs elicit true preferences. After the gamble to be implemented for real had been determined, virtually every subject in Camerer’s experiment abided by her earlier decision when given the opportunity to change it. Hey and Lee (2005a, 2005b) compared the empirical fit of various preference specifications under reduction with the fit under isolation, and concluded in favor of isolation. All in all, these studies are supportive of the WRIS for simple binary choices. In a pure between-subjects experimental design, each subject performs only one single task. In some cases, such a design is more desirable than a within-subjects design (Ariely et al. 2006; Greenwald 1976; Kahneman 2002; Keren and Raaijmakers 1988). When a RIS is employed in a between-subjects experiment, only a fraction of the subjects are paid for their task. Holt’s (1986) concern about meta-lottery perception can also be raised for BRISs: biased risk preferences may similarly result if subjects integrate the choice problem they face with the RIS lottery. The BRIS is particularly susceptible to reduction, which here only involves a straightforward multiplication of the probabilities of the choice alternatives by the probability of real payment. Studies into the performance of the BRIS for risky choices are scarce. The only test we are aware of is in Cubitt et al. (1998b). Using a simple binary choice problem, they found a marginally significant difference, with lower risk aversion in the RIS treatment. Harrison et al. (2007, footnote 16) found no difference between a hybrid RIS and a WRIS for static risky choice. A concern about WRISs is the possibility of carry-over effects from outcomes of prior tasks. With multiple tasks per subject, the outcomes of prior tasks may affect a subject’s behavior in the current task in several ways. First, any experiment with multiple tasks and real payment of each task is vulnerable to an income effect. Outcomes of completed tasks accumulate and may distort subsequent choices (Cox and Epstein 1989). This effect may be limited when outcomes are revealed only at the end of the experiment. However, this is usually not possible. Many actual choice problems are dynamic, consisting of multiple sub-problems with intermediate decisions and intermediate outcomes. In this respect, the WRIS has a clear advantage over using rewards for every task. Because only one task is for real, there is no accumulation of payoffs, and thus no income effect. Grether and Plott (1979, p. 630), however, took the income effect one step further and argued that a subject’s risk attitude may even be influenced by a pseudo-income effect from changes in the expected value of a subject’s payment from the experiment. Second, modern reference-dependent decision theories such as prospect theory suggest that the outcomes of prior tasks can also generate a reference-point effect in a WRIS experiment. If subjects continue to think about results from a previous task, they may evaluate their current choice options relative to their previous winnings. That is, a previous task would set a reference point or anchor for the current task. A favorably completed prior task, for example, then places many outcomes in the domain of losses or makes many current possible outcomes seem relatively small, and consequently encourages risk taking.Footnote 1 Gächter et al. (2009) showed that even experimental economists can be subject to framing effects. Third, in a design with multiple tasks and where all tasks are paid for real, a subject may also be more willing to gamble in order to compensate previously experienced losses or because she feels that she is playing with someone else’s money after previous gains (Thaler and Johnson 1990; Kameda and Davis 1990). Under a WRIS, however, subjects know that only one task is for real. Logically, they would thus understand that gains or losses experienced in prior tasks cannot be lost, undone or enlarged in the current task. In this sense, the WRIS would avoid this kind of reference dependence. Still, even if subjects understand the separation, we cannot exclude that they carry over their experiences and that behavior is affected. Fourth and last, carry-over effects may also result from misunderstanding of randomness. It is well documented in the literature that subjects’ subjective perceptions of chance can be influenced by sequences or patterns of outcomes they observe (see, for example, Rabin 2002). Like the tendency of basketball spectators to overstate the degree to which players are streak shooters (Gilovich et al. 1985; Wardrop 1995; Aharoni and Sarig 2008), subjects in our WRIS treatment who avoided the elimination of large prizes in a previous game may be too confident about their chances of avoiding the elimination of large prizes in the current game. Hardly any empirical research has been done on potential carry-over effects from outcomes of previous tasks in a RIS experiment. Only Lee (2008) partially touched upon this topic, and found that the WRIS avoided an income effect. Our study examines the effects of RISs for risky choices. Various other studies analyzed RISs in other fields. Bolle (1990) reported that behavior under a BRIS is not different from that for real tasks in ultimatum games. Sefton (1992) found that a BRIS does affect behavior in dictator games. Armantier (2006) concluded that ultimatum game behavior under a hybrid RIS is similar to that under a WRIS. Stahl and Haruvy (2006) found that a hybrid RIS does lead to differences in dictator games. Our study of RISs differs from previous studies in three respects. First, we examine whether outcomes from prior tasks in a WRIS experiment affect choice behavior in subsequent tasks. Second, we use a dynamic task that requires more mental effort than the choice problems of previous studies, allowing us to explore whether RISs increase decision errors. Prior analyses of the validity of RISs typically concerned static risky choice problems, in which each task requires the subjects to choose between two simple lotteries.Footnote 2 Finally, we also consider the BRIS. The validity of this design has hardly been investigated before.",57
15.0,3.0,Experimental Economics,05 November 2011,https://link.springer.com/article/10.1007/s10683-011-9307-3,Circadian effects on strategic reasoning,September 2012,David L. Dickinson,Todd McElroy,,Male,Male,Unknown,Male,"The Beauty Contest game has garnered significant attention because of how it captures important features of strategic reasoning that are building blocks for decision making outside the lab.Footnote 1 Recent research documents neural correlates of Beauty Contest decisions in the medial prefrontal cortex, which has been implicated in the key cognitive process of mentalizing or anticipation of others’ actions (Coricelli and Nagel 2009, and references therein). Burks et al. (2009) show that a key determinant of the between-subject heterogeneity in similar strategic reasoning environments appears to be one’s trait-level cognitive skills. In this paper, we consider that state-level cognition, which can exhibit short-run temporal variations, may also impact strategic reasoning, and so this study helps fill an important gap in our understanding of cognition and strategic reasoning in economic decision making. If even a temporary depletion of cognitive resources harms the ability to anticipate others’ actions, this puts one at a strategic disadvantage and higher risk of failure in competitive decision environments. Specifically, we consider circadian mismatch—decision making at an off-peak time-of-day relative to one’s diurnal preference of mornings versus evenings—as a pervasive modern societal example of temporal variation in one’s cognitive resources. We implement a randomized circadian mismatch manipulation on 102 subjects to explore the hypothesis that this common real-world cause of temporal variations in cognition can harm one’s ability to strategically reason. The Beauty Contest we implement is repeated, and the information feedback given after the initial round allows one to mimic others’ choices (see Lakin et al. 2008)Footnote 2—this is not possible when the decision environment is novel, such as a new game or new parameterization of the game. Thus, the repeated Beauty Contest allows us to explore the effects of circadian mismatch on both strategic reasoning during initial task performance and also on more automatic response processes engaged when a task is no longer novel and mimicry is possible. This is an important distinction in our hypotheses given that researchers have suggested that automatic thought processes are less affected by cognitive resources depletion than controlled thought processes (Ferreira et al. 2006; Posner and Snyder 1975). In other words, certain decision domains may be less susceptible to temporal variations in cognition, depending on how much they require controlled thought processes. Our study is highly relevant in a world where most individuals fall prey to modern scheduling demands that require decision making at various times of the day. Recent U.S. data also show that over 21 million wage and salary workers annually (≈18%) performed some type of shift work, which implies that performance and high-level thinking at off-peak circadian hours is not uncommon. Furthermore, a disproportionate percentage of shift work is present in occupations where decision outcomes have significant safety implications, such as health care support (27.9%), protective services (50.4%—includes police and firefighters), and transportation or material moving (29%) (McMenamin 2007). Though our study focuses on circadian mismatch as the cause of depleted cognitive resources, we believe it sheds light on the more hidden decision consequences of temporal variations in state-level cognition, in general.",21
15.0,3.0,Experimental Economics,16 November 2011,https://link.springer.com/article/10.1007/s10683-011-9308-2,Inequality aversion and the house money effect,September 2012,Astrid Dannenberg,Thomas Riechmann,Carsten Vogt,Female,Male,Male,Mix,,
15.0,3.0,Experimental Economics,11 November 2011,https://link.springer.com/article/10.1007/s10683-011-9309-1,Session-effects in the laboratory,September 2012,Guillaume R. Fréchette,,,Male,Unknown,Unknown,Male,"In experiments, where subjects in the same treatment are usually separated in a number of individual sessions, there may be correlations between observations of subjects who participated in the same session. Similarly, when analyzing data from multiple members of a family, observations from siblings might exhibit more correlation than those from individuals across households. In experimental economics this is sometimes referred to as the session-effects problem. To date, there is no explicit articulation of this problem, thus making it difficult to know if a given solution is appropriate. Nonetheless, the session-effects problem has become important in experimental economics in the sense that it influences how data analysis is performed, how experiments are designed, and what questions can be asked. Given the increasingly widespread use of experimental techniques and the fact that many non-experimenters now rely on experimental results, issues central to experimental methods and the way data analysis is performed are of more general interest. Unfortunately, since there is no clear articulation of the problem, it is difficult to formulate an appropriate response to it. The present paper has two aims. First, to formulate more clearly what session-effects are and how they could arise. In particular two different types of session-effects will be identified: static and dynamic session-effects. Second, to explore the implications for experimental design and data analysis. Note that the focus of this paper is not on how to deal with session-effects. Rather, the aim is to define more clearly the issue. This, in turn, will help to evaluate our current practices and to provide a basis for future work. Without having defined session-effects precisely, it is nonetheless clear that correlation in subgroups of a given population are frequent in many other areas beside experimental economics. For instance analysis of survey data involves clustering issues (Sakata 2002). Experimental data in ecology (Warton and Hudson 2004), biology (Williams 2000), medicine (Altman and Bland 1997) and other areas have repeated measurements per treatment which induce similar correlations, and the analysis of genes and their comparison across subpopulations generates the same type of problems (Excoffier et al. 1992). Each of these fields has developed different, although often closely related, methods for dealing with these problems given the particular details of their applications. The problem of session-effects is often addressed in one of the two following ways in experimental economics.Footnote 1 One solution is to use session averages of the variable of interest. The intuition for why this could eliminate the problem is fairly straightforward. Imagine a data set of observations from multiple subjects divided in sessions. Many statistical tests assume that the observations are independent, an assumption that is violated if there are session-effects. If the session averages are treated as the unit of observations, then the correlation that existed because of the session-effects is believed to be no longer present. The other solution is not to replicate the task of interest in an experimental session, i.e. play the game or have subjects make the relevant decision only once. Why these two methods are thought to resolve the problem will be discussed in more detail below. It should be clear that these solutions are not without costs. Both reduce the number of observations available (for a given number of sessions) and thus increase the actual cost of running experiments. These solutions also reduce the power of the statistical tests that can be performed. To illustrate this point, consider an experiment with two treatments, four sessions per treatment, 16 subjects per session, and each subject plays 10 repetitions of the same game (that’s a total of 1280 observations, but only 8 sessions). For simplicity, imagine that the pooled standard deviation of the variable of interest is the same if each data point is treated separately or if the data is first averaged by session. In that case, the smallest treatment effect that would lead to a rejection of the null hypothesis that there is no treatment effect needs to be more than 15 times greater using a t-test at the 5% level if the data is first averaged by session as opposed to if each observation is used separately. Clearly, this is an important loss in power. Furthermore, if the researcher believes that the behavior of interest is the one which occurs after the subjects have understood the game and that this is only possible through practice, then the second solution (having subjects take a single decision) is not possible. Thus if the question of interest requires controlling for observables, this could make it extremely difficult to ask some types of questions since averaging by session does not allow to keep track of subject specific variables. A simple example of this is how can one properly estimate a bidding function in an auction experiment using session averages if the height of the subject is a key determinant.Footnote 2 Also, if one is interested in the interaction between a variable and the treatment, then one is forced to study it within the second setting (a one-period experiment) by introducing variation in the variable of interest within each session. This constrains the experimenter to using cross-sectional analysis and thus limits his ability to control for other factors which might be relevant. Moreover, it will be shown that both methods can create new problems and that depending on the source of the session-effects, they may not even address the problem they are meant to solve.",139
15.0,3.0,Experimental Economics,03 December 2011,https://link.springer.com/article/10.1007/s10683-011-9310-8,The effect of payoff tables on experimental oligopoly behavior,September 2012,Özgür Gürerk,Reinhard Selten,,Male,Male,Unknown,Male,"Payoff tablesFootnote 1 are widely used as an informational aid in experimental economics since its beginnings. Some of the pioneering studies on oligopolies adopt payoff tables (Fouraker and Siegel 1963; Sauermann and Selten 1967; Dolbear et al. 1968) as well as recent ones (e.g., Abbink and Brandts 2008, the majority of the studies reviewed in Huck et al. 2004). The influence of this device on subjects’ behavior, however, has not yet been explored systematically. With this study, we try to fill this gap. We conduct a series of Cournot market experiments with two presentational settings that differ slightly. In one setting named TAB, subjects are provided with payoff tables whereas they are not in the other setting (noTAB). Our main research interest concerns whether subjects in the two settings behave differently. In the context of an oligopoly, we may re-formulate our research question: Do competitors with an information processing aid tend to be more collusive than competitors without such an aid? To check whether the possible effects of payoff tables are robust with respect to market size, we conduct experiments with two, three, and four competitors.Footnote 2
 Previous studies show that slight differences in information presentation may indeed have effects on subject’s behavior. Pruitt (1967), for instance, reports more cooperation in the prisoner’s dilemma game if the payoff structure of the game is presented to subjects in the decomposed form. In a public goods experiment, Saijo and Nakamura (1995) provide subjects either with a “rough” table containing basic payoff information or a “detailed” table that is comparable to the payoff table we provide. They find, if the marginal capita per return is high (and resp. low), average contributions to the public goods are higher with detailed tables (resp. lower) than the investments with rough tables. Huck et al. (1999) find that markets tend to become less competitive if more information about demand and cost conditions are present, while more information about competitors’ quantities and profits yields more competitive behavior. Bosch-Domènech and Vriend (2003) investigate imitation behavior in Cournot markets by varying the presentation of market information. They observe that the imitation frequency does not increase when the information retrieval gets more complex. In a gift-exchange experiment, Charness et al. (2004) find significant reductions in both wages and worker effort when subjects are provided with payoff tables compared to the baseline treatment without payoff tables. Requate and Waichman (2011) report no differences in behavior in Cournot duopoly experiments whether subjects are provided with payoff tables or use a payoff calculator. The studies of Charness et al. (2004) and especially, Requate and Waichman (2011) are closely related to ours.Footnote 3 In the last section of the paper, we will discuss and compare the findings of these studies to our results. The payoff table we use in our study (see online supplementary material) reduces the complexity of the payoff structure by presenting all possible payoffs in a crystal clear way. This may help subjects to realize better what alternatives they have and what the consequences of these alternatives are. In particular, subjects may identify collusive quantities more easily. Hence, we conjecture that payoff tables should lead to more collusive behavior. On the other hand, one can think of an alternative conjecture: with payoff tables, subjects could also easily identify best-replies. This could drive the results more in the direction of Nash-equilibrium. Our results show for all market sizes, average total quantities are lower when subjects are provided with payoff tables, i.e., in TAB, the markets are more collusive. In the initial phase of the experiment, the differences between both settings are most pronounced. Subjects provided with payoff tables choose more often collusive quantities. Over time, however, the differences between both settings get smaller. The next section presents the model. Section 3 describes the experimental design and procedure. Section 4 is dedicated to the results. Section 5 concludes.",20
15.0,3.0,Experimental Economics,06 December 2011,https://link.springer.com/article/10.1007/s10683-011-9311-7,The visible hand: finger ratio (2D:4D) and competitive bidding,September 2012,Matthew Pearson,Burkhard C. Schipper,,Male,Male,Unknown,Male,"To what extent are economic behavior and outcomes biologically determined? Which biological factors affect economic behavior? There is a growing literature with empirical evidence that biological factors substantially influence economic outcomes. Apart from the well known gender wage gap (see for instance Blau and Kahn 2000) and evidence that on average tall men earn more than shorter men (Case and Paxson 2008) and that attractive people earn on average more than less attractive people (Hamermesh and Biddle 1994), there is also evidence that points to more specific biological mechanisms such as certain hormones or certain genes that determine economic behavior to some extent. For instance, Apicella et al. (2008) find that risk-taking in an investment decision is positively correlated with salivary testosterone levels in men. In the same investment decision task, Dreber et al. (2009) associate significantly more risk-taking behavior of men with the presence of the 7-repeat allele of the dopamine receptor D
4 gene. Using a lottery choice task in a design with monozygotic and dizygotic twins, Cesarini et al. (2009) conclude that risk preferences are to a certain extent heritable. Finally, Zak et al. (2005) report that blood plasma levels of oxytocin are positively correlated with trustworthy behavior in a trust game and Kosfeld et al. (2005) observes that exposing humans to the hormone oxytocin increases trust. Zak et al. (2009) show that exposing men to testosterone decreases generosity in the ultimatum bargaining game. In this study we investigate to what extent competitive behavior may be influenced by prenatal exposure to hormones such as testosterone and estrogen. That is, we are interested in what sense competitive behavior may be influenced by biological events before birth. We use as a proxy the “visible hand,” that is the ratio between the length of the 2nd (index) finger and the 4th (ring) finger of the subjects’ right hand (so called “digit ratio” or more precisely, 2D:4D). (See Manning (2002), for an introduction.) 2D:4D is positively correlated with prenatal exposure to estrogen and negatively correlated to prenatal exposure to testosterone (Manning et al. 1998; Lutchmaya et al. 2004; Hönekopp et al. 2007). On average, men have lower 2D:4D than women. 2D:4D is to a large extent genetically determined (Paul et al. 2006), but it may also be affected by the environment in utero. In any case, 2D:4D is determined before birth and thus before common economic, social, and cultural factors could shape competitive behavior of the individual directly. There is already some indirect empirical evidence that 2D:4D may predict competitive behavior. Manning and Taylor (2001) and Hönekopp et al. (2006) show that lower 2D:4D predicts more competitiveness in sports, but they do not address whether this result is due to a correlation with physical fitness or mental “competitiveness.” Dreber and Hoffman (2007) and Garbarino et al. (2011) show that risk-taking in lottery tasks is significantly negatively correlated with 2D:4D in White subjects but Apicella et al. (2008) and Schipper (2011b) show that this is not the case in more ethnically mixed samples. It is known that there are differences in 2D:4D between ethnic groups (Manning et al. 2002, 2004). Sapienza et al. (2009) do not find a significant correlation between risk aversion and 2D:4D in a lottery choice task except for a marginal significant positive correlation in females of a sample of 550 MBA students. Brañas Garza and Rustichini (2011) study the correlation between 2D:4D, risk aversion, and abstract reasoning ability. They employ two measures of risk aversion in a sample of 188 Caucasian subjects. Their analysis reveals that whether or not one can find a significant correlation between risk aversion and the digit ratio may depend on the measure of risk aversion employed. However, the focus of their study is on how prenatal hormone exposure may affect risk aversion indirectly through abstract reasoning ability. Using mediation analysis, they conclude that the digit ratio effects risk aversion directly and indirectly through abstract reasoning ability. Both “competitiveness” and risk-taking behavior are relevant for our study. We investigate the correlation between 2D:4D and bidding behavior and profits in sealed bid first-price auctions with symmetric independent private values. From auction theory it is known that higher risk-takingFootnote 1 in those auctions amounts to relatively lower bids (see Krishna 2002, Chap. 4.1). A higher bid implies a higher probability of winning the object. Yet, conditional on winning, a higher bid results in a lower profit in the first-price auction. Thus we hypothesize that 2D:4D is positively correlated with bids and negatively correlated with profits.Footnote 2
 The study most relevant to ours is Coates et al. (2009), who find that lower 2D:4D predicts the 20-month average profitability of 44 male high-frequency traders in London. We however, do not find a significant correlation of both competitive bidding and profits with 2D:4D in repeated sealed bid first-price auctions played by 400 college students. While the study by Coates et al. (2009) is clearly related to ours, there are several important differences that may account for the contrasting results. First, Coates et al. (2009) focus on a sample of males in a highly selected profession while we focus on a diverse sample of college students. It could be that among a highly competitive subsample of the population lower 2D:4D is correlated with competitiveness while such an relation is absent in the overall population. Second, high frequency traders compete under time pressure in a complex environment while such an intense pressure and complexity are absent in the sealed bid first-price auction. That is, the social environment and the underlying market game differs from our auction experiment. In Sect. 4, we will discuss in more detail how the differences in the market games may contribute to the different results. Third, the performance index in Coates et al. (2009) is cumulated profits and losses over a period of one year, which tends to reduce the impact of noise.Footnote 3 Forth, the stakes are different. Successful traders earn more than £4 million per year while subjects in our study earned on average about US$19.00. Ours is not the first study of 2D:4D in experimental games. Van den Bergh and Dewitte (2006) report that in ultimatum bargaining games men with lower 2D:4D are more likely to reject unfair offers in neutral contexts but are morel likely to accept unfair offers in sex-related contexts. Using a public good game, Millet and Dewitte (2006) find that men and women with lower digit ratio contribute proportionally, whereas those with higher 2D:4D contributed either more or less. Sanchez-Pages and Turiegano (2010) show that men with intermediate 2D:4D are more likely to cooperate in a one-shot prisoners’ dilemma game. We are also not the first to study how biological factors affect bidding in auctions. Casari et al. (2007) report significantly different bidding behavior of men and women in sealed bid first-price common value auctions. Initially, women bid significantly higher than men and hence are more prone to the winner’s curse. However, women also learn bidding much faster than men, thus eventually their earnings may even slightly surpass those of the men. Ham and Kagel (2006) report that females bid significantly higher than men in two-stage first-price private value auctions. Chen et al. (2009) study the effect of the menstrual cycle on bidding behavior of women in sealed bid first and second-price auctions with independent private values. They report that women bid higher than men in all phases of their menstrual cycle in the first-price auction but not in the second-price auction. Moreover, in the first-price auction, higher bidding in the follicular phase and lower bidding in the luteal phase is driven entirely by oral hormonal contraceptives. No such differences appear for second-price private value auctions. These findings are contrasted in a follow-up paper by Pearson and Schipper (2011) who report that naturally cycling women bid significantly higher than men and earn significantly lower profits than men except during the midcycle when fecundity is highest. They suggest an evolutionary hypothesis according to which women are hormonally predisposed to behave generally more riskily during their fecund phase of their menstrual cycle in order to increase the probability of conception, quality of offspring, and genetic variety. They also find that women on hormonal contraceptives bid significantly higher and earn substantially lower profits than men. Finally, using a subsample of the current paper Schipper (2011a) shows that bidding is positively correlated with salivary progesterone and profits are negatively correlated with salivary progesterone. No significant correlations with salivary testosterone, estradiol, or cortisol are found. In both Pearson and Schipper (2011) and Schipper (2011a), we use the same auction environment as in Chen et al. (2007, 2009) and in this paper. The paper is organized as follows: In Sect. 2 we outline the experimental design. The analysis of the data is presented in Sect. 3. We finish in Sect. 4 with a discussion of our null finding. Access to the Stata datasets and a do-file that reproduces the entire analysis reported here and additional analysis is provided through http://www.econ.ucdavis.edu/faculty/schipper/.",26
15.0,3.0,Experimental Economics,21 December 2011,https://link.springer.com/article/10.1007/s10683-011-9312-6,Do watching eyes affect charitable giving? Evidence from a field experiment,September 2012,Mathias Ekström,,,Male,Unknown,Unknown,Male,"Human behavior may be influenced by the physical presence of others. For example, a worker’s productivity is affected by being observed by colleagues or not (Bandiera et al., 2005; Mas and Moretti, 2009). Likewise, people’s willingness to pay for a public good increases when contributions are possible to observe (e.g. Alpizar et al., 2008; Soetevent, 2005). These findings evidently show that people take observers into account when future interaction is likely, and hence, real reputation concerns are at stake. More remarkably, laboratory studies point out that even subtle cues of being observed influences altruistic behavior. Exposing subjects in a dictator game to either a photo of the recipient (Burnham, 2003), a drawing of a pair of eyes (Haley and Fessler, 2005) or three dots formed as the letter v (Rigdon et al., 2009), increases dictator generosity.Footnote 1 Since the dictator game analyzes unconditional giving (i.e. altruism) by anonymous decision makers, explicit reputation incentives should be irrelevant. A common interpretation of these findings is that the human decision making process is sensitive to implicit observation cues which triggers people to act instinctively as being observed and therefore taking reputation concerns into account. Furthermore it has been argued that if subtle cues like those mentioned can affect pro-social behavior, also other uncontrolled subconscious cues may explain a vast part of the remaining pro-social behavior found in anonymous experimental settings (Bateson et al., 2006; Haley and Fessler, 2005). These scholars therefore question the theoretical and experimental work (e.g., Camerer and Fehr, 2006; Fehr and Schmidt, 1999) that argues that some people have social preferences and that this phenomenon has evolved over time in a group selection process (Gintis et al. 2003). When implicit observation cues are adopted to other settings results diverge. Fehr and Schneider (2010) use the same drawing as Haley and Fessler (2005) in a trust game and Lamba and Mace (2010) let real people serve as implicit cues (being present but without getting information on actual decisions) in an ultimatum game. Both studies report zero-effects and conclude by raising concerns regarding the robustness of the previous dictator game findings, given the accumulating evidence of experimenter demand effects in that specific game (Cooper and Kagel, 2010 gives an overview on the topic, references therein includes Bardsley, 2008 and List, 2007).Footnote 2 On the other hand, two studies which adopt implicit observation cues in field settings (Bateson et al., 2006, 2011), finds large treatment effects on cooperative behavior. Bateson et al. (2006) study money collected to a coffee room “honesty box”Footnote 3 and report that contributions increased by 176 percent during weeks when a black and white picture of human eyes was posted on the payment instruction notice, compared to weeks when a control picture was posted at the same place. Bateson et al. (2011) test how the same manipulation (a picture of human eyes) affects littering behavior among customers in a university cafeteria. By adopting a 2×2 design (picture of eyes or control picture and congruent or incongruent text) they resolve most of the former study’s deficiencies. It turned out that during days when eyes were placed on walls in the cafeteria the probability to leave litter on the table halved, irrespectively of the message accompanying them. Note, however, that the effect in these studies is upward biased if people are conditional cooperators, a bias which I refer to as the social multiplier effect. The argument being that if some fraction does respond to the eyes other subjects will notice this (either by observing more money in the “honesty box” or that more people collect their litter) and respond to the shift in real behavior by peers not because of the picture of eyes.Footnote 4
 From the above discussion the role of implicit observation cues appears unclear to say the least. Another possible explanation for the inconclusive pattern could be due to a confounding interpretation of observational cues. When the counterpart of a specific transaction is another human being an alternative effect of cues such as eye-configurations, a picture of eyes or even a picture of the recipient, is that they remind the decision-maker of the person directly affected by one’s action, i.e. the recipient in a dictator game, a colleague in the “honesty box” experiment or the cafeteria staff who needs to clean up when customers are not doing so.Footnote 5 This reminder effect could then increase giving/cooperation even though the feeling of being observed is unchanged. The reminder effect should naturally be more important in situations where the human counterpart is less salient to start with, but possibly vanish in situations where the counterpart already constitutes an integral part of the decision process. Hence, even though Fehr and Schneider (2010) and Lamba and Mace (2010) stress the possibility of experimenter demand effects in dictator games, another interpretation is that the inherent, and salient, reciprocal element in ultimatum- and trust games diminishes the importance of these sorts of cues. Since research on this topic can provide insights into the determinants of human altruism, it is crucial to gather further evidence to better understand how people interpret implicit observation cues and why they affect behavior. In an attempt in that direction the present paper test if the same trigger used by Bateson et al. (2006, 2011), a picture of human eyes, can affect behavior in a setting that involves a different type of decision problem, and importantly, a setting similar to the dictator game. More specifically, I make use of the situation faced by recyclers in a large Swedish supermarket chain. When customers in this chain have recycled their cans and bottles they have to make a choice, whether to keep the money or donate the amount to a well-known charity organization focused on foreign aid. The choice is made by pressing one of two buttons placed side by side on the recycling machine. In effect this means that the decision problem itself, combined with the non-personal solicitation procedure and the placement of machines into remote areas in the store, provides us with the anonymous and non-strategic situation featured in dictator games, which makes it suitable to study altruistic behavior. Three key elements are important to highlight. First, the fact that the recipient is a charity importantly weakens the direct link between the cue (the picture of eyes) and the counterpart (an organization), and therefore reduces the possibility of the mentioned reminder effect. Instead, if the picture of watching eyes actually trigger something in this context, the most likely candidate is feeling observed by a third party. This argument is given further support by the use of eyes from a native Swede while the final recipients are developing projects in poor countries, in addition, the charity’s non-practice of personal solicitation procedures makes it unlikely that customers have a personal connection to someone from the organization who they could be reminded of. Secondly, since subjects are unaware of participating in an experiment, demand effects are unlikely.Footnote 6 Thirdly, while I in principle cannot reject a social multiplier effect, it should be reduced substantially in magnitude since the flow of recycling customers is such that it is very unlikely that a queue forms where people can observe other recyclers’ behavior. Neither are recyclers given information about total prior contributions during the experimental period. To test the hypothesis that the implicit observation cue consisting of a pair of eyes increases generosity, a natural field experiment in 38 stores belonging to the supermarket chain was conducted. For each store in my sample, a picture of watching eyes and a control picture were consecutively posted on the recycling machines. Pictures were posted close to the decision buttons and can by no means have been neglected by customers. Each picture was posted for six straight days and which picture to be first out in a specific store was randomly determined. Consequently, I obtain both within- and between-store estimates of the treatment effect for two different outcome variables; the proportion of donors and the proportion of money being donated. In total, my sample consists of 12 days, 38 stores and 16775 individual choices. In many dimensions the picture of eyes seemed to have no effect on donor behavior. The average treatment effect is not statistically significant and close to zero in magnitude. Interestingly, however, restricting attention to days when relatively few recycling customers visited a specific store we observe a 30 percent increase in the amount being donated if subjects are exposed to the watching eyes. This increase appears even though the effect on the proportion of donors, while positive in direction, is not significantly different from zero, suggesting that the picture of eyes affected a small fraction of customers who recycled sizable amounts of money. That the effect only is significant on days with relatively low store attendance is in line with results reported in Bateson et al. (2011), since their results also varies according to the number of customers in the cafeteria. One interpretation of the dependence between implicit observation cues and the surrounding environment is that increased noise leads to a higher probability of being distracted and therefore not observing the picture of eyes. Another possibility is that the presence of real people makes implicit observation cues obsolete. An interesting question is if the relatively small and subtle effect reported here is due to exploring a new situation, or if the mentioned confounding factors in previous experiments have generated exaggerated effects. While I am unable to answer this question directly, the results confirm that observational cues possibly can play a role even in situations less prone to induce an upward bias. To learn more, one direction for future research could be to explore these concerns in a more systematic way. With respect to the broader topic about whether social preferences or reputation concerns is most important in determining pro-social behavior, the current experiment is neither better nor worse suited to answer that question.",111
15.0,4.0,Experimental Economics,15 March 2012,https://link.springer.com/article/10.1007/s10683-012-9313-0,The dark side of friendship: ‘envy’,December 2012,Ramón Cobo-Reyes,Natalia Jiménez,,Male,Female,Unknown,Mix,,
15.0,4.0,Experimental Economics,21 February 2012,https://link.springer.com/article/10.1007/s10683-012-9314-z,What you don’t know won’t hurt you: a laboratory analysis of betrayal aversion,December 2012,Jason A. Aimone,Daniel Houser,,Male,Male,Unknown,Male,"First movers in bargaining environments often must decide whether to forgo guaranteed returns in order to trust counterparts to provide them with greater future benefits (e.g., efficiency wages in labor markets; see Fehr and Falk 1999; Rigdon 2002). While experiments in both naturally occurring and designed environments have established that many people do trust, some people of course do not (see, e.g., Berg et al. 1995). Choosing not to trust is consistent with standard economic theory, and perhaps for this reason few alternative explanations for such decisions have appeared. Recently several excellent studies (see, e.g., Bohnet and Zeckhauser 2004; Kosfeld et al. 2005; Hong and Bohnet 2007; or Bohnet et al. 2008) have suggested that betrayal aversion could be an alternative explanation for such behavior in trust games. These studies suggest that a personal betrayal brings about an emotional or psychological cost associated with a betrayal of trust separate from monetary outcomes. We present here a new approach to the analysis of betrayal aversion in order to complement this important growing field of research. The novel design succeeds in identifying effects of betrayal aversion discussed in the past studies. We explain how our procedure can be easily modified for studies directed at institution design, where researchers would want to focus on revealed preferences instead of probability elicitation procedures. Recent interest by economists in the way betrayal aversion impacts trust decisions owes largely to important research by Bohnet and Zeckhauser (2004). Following Rabin (1993), they focus on the aversion an individual agent has to being betrayed by a trusted counterpart. Disutility stems from how the payoff outcomes arise, as compared to outcome based (social) preferences where utility is derived exclusively from absolute and relative payoff outcomes (e.g. Fehr and Schmidt 1999; Bolton and Ockenfels 2000). A person who is betrayed both learns that she has been betrayed and also, usually, suffers the consequences of that betrayal. These are conceptually distinct events and, as will become clear below, in our analysis we find it useful to distinguish between them. In particular, we separate the aversion to discovering one has been betrayed from the aversion to the consequences of betrayal. It is worth noting that this is consistent with but refines the concept of betrayal aversion described by Bohnet and Zeckhauser (2004), who did not distinguish these two effects. Previous experimental studies of betrayal aversion have used a common design (see, e.g., Bohnet and Zeckhauser 2004; Hong and Bohnet 2007; or Bohnet et al. 2008). In brief, the investigators asked each subject to report the “minimum acceptable probability” (MAP) at which she would choose a (trust or risk) “gamble.” In the “trust” MAP game, if the reported MAP is less than the true fraction, ‘p
∗’, of reciprocating trustees the agent enters the trust gamble and is paid according to their own counterpart’s decision in a binary trust game (if the trustee reciprocates, $30 is split evenly, if the trustee betrays trust they keep $22 and the trusting agent receives only $8). If the reported MAP is larger than p
∗ trustees, the agent does not enter the trust gamble and they receive $10 and their counterpart receives $10. In the “risky dictator” MAP game, if the reported MAP is less than an unknown, but preset, fraction, p
∗, the agent and their counterpart enter the risky lottery, where the two monetary outcomes of the lottery are the same as the outcomes of the binary trust game. Consistent with betrayal aversion and the previous research on the distinction between risk and trust games (Houser et al. 2010; Eckel and Wilson 2004, etc.), these papers found subjects report higher average MAPs for trust gambles than risk gambles. However, as noted by Bohnet and Zeckhauser (2004) and Bohnet et al. (2010), there are other factors unrelated to betrayal aversion which could contribute to differences in behavior between treatments. Loss aversion is one factor that might plausibly contribute to differences between treatments in MAP designs. The key issue is that in a MAP design one’s reference point for trustworthiness is the expected fraction of trustees who chose to reciprocate trust, but this value also determines whether the trust gamble will be played. It is easy to see why this can potentially confound inferences regarding betrayal aversion. Consider, for example, a loss-averse but not-betrayal-averse subject participating in the risky dictator MAP game at her university. Suppose she is willing to engage in the risky gamble when the likelihood of an equitable outcome is a third or greater, and so reports p=1/3. On the other hand, when playing the trust version of the MAP game, she may from previous interactions with her fellow students (playing in the role of trustees) expect 2/3 of the trustees will choose to reciprocate. That is, she may plausibly have a reference point at a 2/3 chance of an equitable outcome. It follows that she would perceive a 1/3 chance of an equitable outcome as an expected loss. Consequently, as she is loss-averse, she would report a MAP that exceeds 1/3 in the Trust game, despite the absence of betrayal aversion. Scholars using the MAP design (Bohnet and Zeckhauser 2004; Hong and Bohnet 2007; or Bohnet et al. 2008) have pointed to various other potential confounds including disutility from loss of control, assessment costs associated with calculating trustworthiness, costs of making incorrect assessments, costs from placing trustees in a potentially undesirable decision situation, and disutility from earning money due to other people’s kindness as factors that could lead to differences between treatments. Consequently, there seems to be value in exploring betrayal aversion with an alternative design. Our design circumvents many of the concerns present with the MAP design, and confirms that betrayal aversion is a robust and influential phenomenon. Here we report data from one-shot two-person binary investment games (Tullock 1967)Footnote 1 in which investors can choose not to know the decision of their particular trustee, and instead receive payment according to a random draw from a separate pool of decisions identical to the pool of trustees’ decisions. Note that the probability of receiving the “cooperative” outcome is identical in the two cases, and participants understand this is the case. Our design differs from the MAP-based design primarily in that it does not require one to elicit probabilities: our inferences stem from revealed-preferences. By holding the position of trustees constant between treatments, our experiment design removes concerns associated with investor expectations about trustees’ behavior, such as loss-aversion, assessment costs, altruism, etc. Our main finding is that, due to betrayal aversion, investors systematically prefer to avoid knowledge of their specific trustee’s decision. Moreover, when avoiding this information is not possible investors are substantially less likely to make trusting decisions. These results are convergent evidence that outcome-based models cannot fully explain economic decision making in strategic environments (see, e.g., McCabe et al. 2003), and that betrayal aversion can have a detrimental impact on trust behavior. More specifically, our findings suggest that an effect of impersonal institution-mediated exchange (e.g., lending through financial intermediaries) is the promotion of trust and economic efficiency by shielding investors from knowing whether their particular trustee chose to betray. Our data also reinforce the general importance of emotions to economic decision-making (see, e.g., Fehr et al. 2005; Xiao and Houser 2005), and provide new evidence to the importance of emotion regulation (see, e.g., Gross 1998; Miu et al. 2008).Footnote 2 Moreover, our design suggests a way to construct institutional solutions to inefficiencies stemming from betrayal aversion, an important topic that previous investigations have not addressed. Additionally, our experiments demonstrate that the majority of people choose such institutions when provided institutional choice, suggesting the importance of designing institutions with emotion regulation in mind. Finally, our investigation contributes to the literature on distinctions between trust and risk environments (see, e.g., Houser et al. 2010; Schechter 2007; Kosfeld et al. 2005; Eckel and Wilson 2004; Ashraf et al. 2006; McCabe et al. 2001; Snijders and Keren 1998). This literature provides substantial evidence that people make trusting decisions differently than decisions under risk. However, identifying the source of the differences is difficult. A reason is that trust and risk environments typically differ in multiple ways (e.g., strategic-uncertainty always involves another person, while state-uncertainty need not). As a result, while it is widely accepted that trusting decisions differ between environments of strategic and state-uncertainty, the reason for such differences—and particularly the role of betrayal aversion—remains an important open question. The following section reviews the relevant betrayal literature. Our experiment design is in Sect. 3. Section 4 gives predictions and hypotheses, and Sect. 5 describes our data and results. Section 6 is a concluding discussion.",55
15.0,4.0,Experimental Economics,01 March 2012,https://link.springer.com/article/10.1007/s10683-012-9315-y,Are social preferences related to market performance?,December 2012,Andreas Leibbrandt,,,Male,Unknown,Unknown,Male,"A fundamental question in behavioral economics is which role social preferences play in natural markets. Pro-social behavior is omnipresent in the laboratory environment (Güth et al. 1982; Roth 1995; Fehr and Gächter 2000; Camerer 2003) and these observations have led to the formulation of other-regarding preference theories (Andreoni 1990; Rabin 1993; Fehr and Schmidt 1999; Bolton and Ockenfels 2000; Charness and Rabin 2002; Dufwenberg and Kirchsteiger 2004; Sobel 2005; Falk and Fischbacher 2006; Cox et al. 2007; López-Pérez 2008). Laboratory studies also suggest that pro-social behavior can affect outcomes in market settings, pay off for employers and provide explanations for phenomena such as price rigidities and relational contracts (Fehr et al. 1993, 2009; Brown et al. 2005). There is also suggestive evidence that pro-sociality is a positive predictor for earnings and productivities outside the laboratory environment (Bowles et al. 2001; Barr and Serneels 2009; Dohmen et al. 2009; Carpenter and Seki 2010). There are at least three explanations for the positive impact of pro-sociality on job performance. The first potential explanation is that pro-social individuals are more likely to select into environments where earnings and productivities are higher than selfish individuals. The second potential explanation is that pro-social individuals are more productive because they interact better with their colleagues and are better integrated into the social network at the workplace (Barr et al. 2009). The third potential explanation is that pro-social individuals interact more smoothly with employers/buyers because they are less likely to refrain from opportunistic behavior that hurts the employers/buyers and thus can reap reputational benefits (Bowles et al. 2001).Footnote 1
 This paper investigates professional individual sellers in open-air markets and whether pro-social sellers achieve different prices for their products and have different trade relations than selfish sellers. The hypothesis based on previous evidence is that pro-social sellers outperform selfish sellers. To test this hypothesis and whether differential seller-buyer relations are responsible for such a relationship, I observe the behavior of the same professional shrimp sellers in open-air markets and the laboratory, and in addition collect additional information about them in surveys. More precisely, I use an anonymous laboratory experiment to isolate the sellers’ levels of pro-sociality. On the markets, I record trade outcomes to measure prices, qualities and quality misrepresentations. Finally, I conduct surveys to collect data on the sellers’ trades, trade relations and their characteristics. The data confirm the hypothesis and show that social preferences are positively related to performance in natural markets. Sellers, who are more pro-social in a laboratory public goods experiment, achieve significantly higher prices for goods of similar quality than less pro-social and selfish sellers. The features of the field setting render two of the three mentioned explanations unlikely for the observed relationship between pro-sociality and market performance. First, it is unlikely that this relationship is driven by selection into different job environments because the study investigates one unique job environment. Second, it is unlikely that the relationship is explainable by the idea that pro-social individuals interact better with colleagues because the individuals in this study operate on their own, i.e. they catch and sell shrimp alone. However, as the subjects in this field setting are in steady and direct contact with buyers it seems likely that the third potential explanation is valid, i.e., that pro-social sellers outperform selfish sellers because they interact more smoothly with buyers. I find mixed evidence for the explanation based on differential seller-buyer relations depending on the seller’s level of pro-sociality. On the one hand, the data show that more pro-social sellers have significantly more stable and longer lasting trade relations than less pro-social sellers. Moreover, I observe that more pro-social sellers self-report to be better able to signal trustworthiness to buyers than less pro-social sellers. However, on the other hand, I find no evidence that pro-social sellers misrepresent quality less than selfish sellers. Thus, I find that pro-social sellers interact more smoothly with buyers than selfish sellers but it remains unclear why this is the case. One possibility is that pro-social sellers cheat less on other unobservable dimensions. Another possibility is that buyers know the seller’s type but hold false beliefs about their level of quality misrepresentation. This paper is also related to other studies which combine laboratory data on pro-sociality with field data (Karlan 2005; List 2006; Fehr and Leibbrandt 2011) and studies which observe the link between prices, reputation and trade relations (Weisbuch et al. 2000; Jin and Kato 2006). For example, Fehr and Leibbrandt (2011) study individuals drawn from the same subject pool and find that more pro-social fishermen exploit fishing grounds less. List (2006) studies sellers’ pro-sociality in the laboratory and also relates it to their quality misrepresentation in natural markets. His study suggests that the main determinant for quality misrepresentation in natural markets is reputation and that pro-sociality plays a negligible role. In contrast to List, this studies combines both laboratory and market data from the same individuals to directly study whether the extent of individual pro-sociality is related to individual quality misrepresentation in natural markets. The reminder of this paper proceeds as follows. Section 2 presents the field setting and the collected data. Section 3 links the data on market performance with the laboratory data. Section 4 provides concluding remarks.",13
15.0,4.0,Experimental Economics,02 March 2012,https://link.springer.com/article/10.1007/s10683-012-9316-x,How does voice matter? Evidence from the ultimatum game,December 2012,Qiyan Ong,Yohanes E. Riyanto,Steven M. Sheffrin,Unknown,Unknown,Male,Male,"Deep seated in standard economic theory, utilitarian consequentialism has led economists to evaluate different states of the world based exclusively on their pecuniary outcomes. Non-pecuniary effects such as the fairness of the outcome, or distributive fairness, and the means by which different states are attained are typically ignored in the individual welfare maximization exercise. The failure to incorporate these non-pecuniary effects into economic models has led to poor predictions in some of them; a classic example is the ultimatum game. Inaccurate predictions from this game and similar ones have driven researchers to incorporate concerns for distributive fairness into economic models (Bolton and Ockenfels 2000; Fehr and Schmidt 1999). While there is now a general consensus that concerns for distributive fairness have a non-negligible impact on outcomes, the role that process plays in determining outcomes remains ambiguous (Dolan et al. 2007). One aspect of process which appears to have a profound impact on decision is voice: the act of expressing one’s opinions about how decisions should be made (Hirschman 1970). Studies have found that the opportunity for voice often increases the acceptance of decisions made by authorities even if the outcome is unfavourable (Vidmar 1992; Lind et al. 1993; Peterson 1999) and occurs in a variety of social contexts, such as participatory decision-making (Greenberg and Folger 1983) and performance appraisal and compensation plans (Greenberg 1990; Miceli and Lane 1991). These findings echo a long tradition in social theory—spanning from Jean Jacque Rousseau’s general will to Jurgen Habermas’s (1970) theory of depoliticization—that voice has a strong influence on social decisions. Several explanations have been put forth to explain how the opportunity for voice affects the acceptance decisions of unfavourable outcomes, mainly the consequentialist value of voice—outcomes are improved (Thibaut and Walker 1975), the procedural fairness effect of voice (Lind et al. 1980; Lind et al. 1990; van den Bos 1999; Tyler and Blader 2003) and the punishment effect of voice (Xiao and Houser 2005), which focuses on ex post interactions between individuals. With the exception of few papers where the effect of voice is tested through revealed preferences (such as Xiao and Houser 2005), most existing studies elicit responders’ responses to unfavourable outcomes through the responders’ reports on their satisfaction with the outcomes after experiments or events. We adopt a revealed preference approach and provide comprehensive tests for four separate effects of voice that encompass the ex ante effects of voice on economic and social interactions—the knowledge effect of voice, the value-expressive effect of voice (or the inherent value of voice), the expectation effect of voice and the procedural fairness effect of voice. These four channels for voice that we examine (knowledge, value-expressive, expectation and procedural justice) all represent components of voice that can affect assessments of social situations. To illustrate these channels, let’s consider three examples: student evaluations of professors, the opportunity for homeowners to challenge their property tax assessment, and performance appraisals in many companies. Students may value the knowledge that they can write evaluations of their professors; they may take satisfaction through filling out the evaluation form (value-expressive); they may believe that their evaluations will influence subsequent teaching assignments (expectation); and may in general feel that because of student evaluations the procedures for assigning and evaluating instructors will be fair. Similarly, when a homeowner receives her current property tax assessment, she may feel better knowing that an appeal process exists; may gain satisfaction from submitting an appeal even if it has no immediate effect (value-expressive); may expect her appeal to affect her assessment (expectation); or, more generally feel that the procedures for setting of property tax assessments are generally fair. Finally, in a performance appraisal it is common for employers to give an opportunity to employees to express their opinions on their job performance before employers decide on salary increases or bonuses to be awarded to employees. In this setting, employees may feel satisfied from having been able to express their opinions even though they may not eventually receive the salary increases or bonuses they wish for (value-expressive); may feel that they were being treated unjustly if they know that others, but not them, were given a chance to express their opinions (knowledge); may expect that their opinions would influence employers’ appraisal (expectation); and may in general feel that the performance appraisal procedure is fair because they were given the chance to express their opinions (procedural fairness). To test for these four separate channels for voice, we introduce three additional treatments to the classic ultimatum game. In one of these treatments, we informed the responders that some other responders were randomly selected to express their preferred allocation to the proposers, while they were not selected (no-voice treatment). In another treatment, we informed the responders that they were randomly selected to express their preferred allocation to the experimenters instead of to the proposers (voice-to-experimenter treatment). In our third treatment, we informed the responders that they were randomly selected to express their preferred allocation to the proposers (voice treatment) before the proposers make their allocation decision. Apart from our introduction of three additional treatments to the ultimatum game, we also adopt the strategy method (Selten 1967; Brandts and Charness 2011) to elicit the responders’ acceptance strategy profile for all possible offers that could be made by proposers. It is well known in the experimental economics literature that, in general, proposers in the standard ultimatum game would allocate between 40–50% of their endowment to recipients, and the responders would reject offers that are below 40% of the endowment (Kahneman et al. 1986; Thaler 1988; Roth and Erev 1995; Hoffman et al. 1996; List and Cherry 2000; Camerer 2003). Since we are predominantly interested in the impact of voice on responders’ acceptance decisions of non-egalitarian offers, we choose to employ the strategy method to derive the responders’ minimum acceptable offer (MAO), and evaluate the impact of our treatments on the threshold MAO. The knowledge effect of voice refers to the extent to which that the awareness of the opportunity for voice in one’s social surrounding influences one’s decision. Since the difference between the no-voice treatment and the standard ultimatum game is simply the awareness that other responders were selected the opportunity to voice, we compare the difference in the MAOs in these two treatments to detect the knowledge effect of voice. We find no evidence for the knowledge effect of voice. The value-expressive effect of voice refers to the inherent value that one places on the ability to state one’s case about how decisions should be made, irrespective of its impact on outcomes (Katz 1960). People value the chance to state their case and voice improves people’s satisfaction with the outcomes even when their opinions had no favourable impact on outcomes (Tyler et al. 1985). We test this effect by comparing the MAOs in the no-voice and voice-to-experimenter treatments. In the former, responders knew that some others could express their opinions, but they could not. In the latter, responders had a chance to express their opinions, but not to someone who could influence the outcome. We find support for the value-expressive effect of voice. The responders who had an opportunity to voice their opinions to unintended targets (experimenters) had lower MAOs compared to those in the no-voice treatment. The expectation effect of voice refers to the increase in one’s expectation of favorable outcomes as a result of the opportunity for voice (Köszegi and Rabin 2006). In other words, when an individual has the opportunity to express his opinion about the outcome, she envisages a more favourable outcome. This increase in expectation means that a lower outcome than anticipated would reduce utility. We test the expectation effect of voice by comparing the MAOs of recipients in the voice to experimenter and the voice treatment. We find support for the expectation effect of voice. When the responders could voice to intended targets (proposers) instead of third parties, they had higher MAOs. Finally, previous studies that explored the procedural fairness effect of voice often document the positive relationship between procedural fairness and the opportunity for voice and how this leads to an increase in satisfaction with outcomes (Lind et al. 1980, 1990; van den Bos 1999; Burke and Leben 2007; Tyler and Blader 2003). We explore these channels by examining the reported procedural fairness ratings in each of our groups. We also introduce the procedural fairness ratings into regressions of MAOs in order to establish the effect of perceptions of procedural fairness on decisions. We find no evidence on procedural fairness effects of voice; MAOs are not influenced by the perception of procedural fairness.",20
15.0,4.0,Experimental Economics,10 March 2012,https://link.springer.com/article/10.1007/s10683-012-9317-9,Information and beliefs in a repeated normal-form game,December 2012,David N. Danz,Dietmar Fehr,Dorothea Kübler,Male,Male,Female,Mix,,
15.0,4.0,Experimental Economics,06 March 2012,https://link.springer.com/article/10.1007/s10683-012-9318-8,Multitasking,December 2012,Thomas Buser,Noemi Peter,,Male,Female,Unknown,Mix,,
15.0,4.0,Experimental Economics,21 March 2012,https://link.springer.com/article/10.1007/s10683-012-9319-7,Will you accept without knowing what? The Yes-No game in the newspaper and in the lab,December 2012,Werner Güth,Oliver Kirchkamp,,Male,Male,Unknown,Male,"One aim of this study is to learn more about external validity of laboratory experiments with student participants. Since students are similar in age and education such laboratory experiments leave open the question how far results from the lab can be generalised. To increase the variance of socio-demographic characteristics in the subject pool Roth et al. (1991) study the ultimatum game with students from different nationalities and find clear differences in behaviour between these groups. Murnighan and Saxon (1998) look at the behaviour of children and observe that generosity in the ultimatum game decreases with age. In a similar study with children Harbaugh et al. (2003) find that, once one controls for size, generosity increases with age. In a newspaper experiment with the ultimatum game (Güth et al. 2003) the medium of participation, Internet or postal mail, has an effect on generosity. Güth et al. (2007) look at a three-person ultimatum game and show that fairness and rejection rates increase with age. Köhler et al. (2007) play an ultimatum game with a heterogeneous sample of German adults illustrating that generosity increases with age and income. Integrating their experiment into an existing survey, the Dutch CentER panel, Bellemare et al. (2008) let their participants play either the ultimatum or the dictator game and confirm that generosity increases with age. Using the trust game, Fehr and List (2004) compare the behaviour of students with that of CEOs who turn out to be more trusting, more trustworthy, and who punish less. Fehr et al. (2003) report data from a trust game with a randomly selected sample of German households. Bellemare and Kröger (2007) compare behaviour in the trust game played by students and households of the CentER panel to find a hump-shaped relation between age and trust, and a U-shaped relation between age and trustworthiness. Bornhorst et al. (2010) play a trust game with Ph.D. students of different nationalities and find significant differences in trust and trustworthiness between different regions of origin. Sutter and Kocher (2007) study a trust game with participants from different age groups and observe a hump-shaped relation between age and trust and increasing trustworthiness with age.Footnote 1
 To explain why non-students outside the lab and students within the lab behave differently, Pull (1999) and Selten (2000) argue that student participants in a lab environment react more clearly to subtle strategic details than non-students outside the lab. It is, hence, essential to compare behaviour of student participants with a more heterogeneous population in other games, especially those related to the previously explored ones by differing only in subtle details. This is what we want to do in this paper.Footnote 2 We will use a very simple and abstract game and concentrate on analysing the effect of stakes and the effects of the subject pool on the offers and on the willingness to accept. Our workhorse is the Yes-No game which is a game where proposers suggest how to share a given positive monetary amount and responders decide without knowing the proposal. From ultimatum games (see Camerer 2003, for a survey of ultimatum experiments), Yes-No games differ since responders in ultimatum games know what they accept or reject.Footnote 3 Unlike to dictator experiments (e.g. Forsythe et al. 1994), the responder in Yes-No games still has full veto power in the sense that without his consent the pie of 100€ or 1000€ is lost. We find the Yes-No game an interesting game for several reasons:  Many offers or opportunities in real life contain a certain as well as an uncertain component. E.g. a work contract might specify an explicit salary but might be silent about working hours, pensions, obligations of the worker and much more. Other examples include so-called experience goods whose quality is not known to customers or partnership proposals without knowing how reliable the partner(s) will be, as e.g. in joint ventures or spouse relationships. The ultimatum game studies as one extreme the (artificial) situation of an offer with no uncertainty at all. Everything that can be said about the offer is known to the responder. The Yes-No game looks at the other extreme: A situation where the offer is entirely unknown to the responder. The Yes-No game also sheds new light on motives of behaviour in situations like the ultimatum game. Do proposers make generous offers in the ultimatum game because they fear rejection of a lower offer? Since the offer is unknown to the responder low offers are not more likely to be rejected than high offers. Proposers who make high offers must have other reasons. Even more interestingly, responders who reject unfair offers in the ultimatum game should when they expect to receive unfair offers in the Yes-No game reject these offers, too. We already know from ultimatum games with private information (Güth et al. 1996), where only the proposer knows whether the pie is large or small, that most proposers who could divide the large pie offered only a fair share of the small pie which was never rejected by the responder. Such response behaviour can be explained either by “in dubio pro reo” (the pie could be small) or by “in dubio pro meo” (better little than nothing). In the Yes-No game accepting an unknown offer can be similarly justified (Gehrig et al. 2007). But since in the Yes-No game the proposer can be more exploitative by offering only the smallest positive amount the responder may expect less what could weaken the “in dubio pro meo” argument. Will we therefore observe more rejections (No) by responders who expect a low offer? Will these rejections be more frequent if the pie is small (100€) and punishment is cheap? Another reason for using the Yes-No game is, of course, that it only differs from the ultimatum game by one subtle detail, namely that one does not know the proposal when exercising one’s veto power. Is this a detail overlooked more frequently by non-students than by student participants? Furthermore, is it possible that not only students but also executives are paying more attention to subtleties like these. Do we have reasons to single out important and economically relevant subgroups of non-students?  In this paper we want to describe properties of a heterogeneous population playing the Yes-No game. Will participants with a socio-demographic background different from students as responders also rely to the same degree on “in dubio pro reo” or “in dubio pro meo”? And will they as proposers yield to the exploitation incentive or will they feel more committed to fairness concerns, at least when the pie is small (and exploitation less profitable)? In Sect. 2, we introduce the design of the experiment and discuss some hypotheses which, in Sect. 3, are tested with the help of the rather large data set (involving altogether 1175 participants). Section 4 concludes.",12
15.0,4.0,Experimental Economics,31 March 2012,https://link.springer.com/article/10.1007/s10683-012-9321-0,On the impact of package selection in combinatorial auctions: an experimental study in the context of spectrum auction design,December 2012,Tobias Scheffel,Georg Ziegler,Martin Bichler,Male,Male,Male,Male,"Designing combinatorial auction (CA) markets is a formidable task: Many theoretical results are negative in the sense that it seems quite unlikely to design fully efficient and practically applicable CAs with a strong game-theoretical solution concept (Cramton et al. 2006). Experimental research has shown that iterative combinatorial auction (ICA) formats with linear prices achieve very high levels of efficiency (Porter et al. 2003; Kwasnica et al. 2005; Brunner et al. 2010; Scheffel et al. 2011). Most of these experiments are based on value models with only a few packages of interest. While it is important to understand bidder behavior in small CAs, we need to know whether the promising results carry over to larger auctions, since applications of CAs can easily consist of a larger number of items leading to thousands of possible packages. We ran laboratory experiments with those CA formats that have been used or analyzed for the sale of spectrum licenses, the most prominent and most thoroughly investigated application domain for CAs. Given the importance of spectrum auctions for the telecommunications sector and the significant costs that companies have to bear for licenses, these auction formats demand more empirical results. However, the analysis is not restricted to spectrum sales and the results are also relevant to the design of auctions in other domains, such as procurement and transportation. There has been a long and ongoing discussion on appropriate auction mechanisms for the sale of spectrum rights in the USA (Porter and Smith 2006). Since 2005 countries such as Trinidad and Tobago, the UK, the Netherlands, Denmark, and the USA have adopted CAs for selling spectrum rights (Cramton 2009b), after the simultaneous multiround auction (SMR) has been used worldwide. The USA used an auction format called Hierarchical Package Bidding (HPB) (Goeree and Holt 2010), in which only restricted combinatorial bidding is allowed and bidders can only bid on hierarchically structured pre-defined packages. Other countries used a version of the Combinatorial Clock (CC) auction (Porter et al. 2003) or the Clock-Proxy auction (Ausubel et al. 2006), which extends the CC auction by a sealed-bid phase. The pricing in the CC auction is rather simple, the price of over-demanded items raises by the specified increment and bidders can only place package bids at the current prices. A third type of CA formats, which has been analyzed for spectrum auctions in the US, uses pseudo-dual linear prices, i.e., they use an approximation of the dual variables of the winner determination problem. Different versions have been discussed in the literature (Brunner et al. 2010; Bichler et al. 2009), which are all based on the Resource Allocation Design by Kwasnica et al. (2005) and earlier by Rassenti et al. (1982). We refer to these auction formats as pseudo-dual price (PDP) auctions in the following. Based on the influence of the experiments by Goeree and Holt (2010) on the FCC’s decision, we intentionally decided to partly replicate their experiments but extended them with the CC auction. Since numerical simulations indicate that even without package bids high levels of efficiency can be obtained in their value model, we add experiments with a new value model that has different synergy characteristics and captures the local synergies of licenses, which can be well motivated from observations in the field (Ausubel et al. 1997; Moreton and Spiller 1998). From these extensions we derive new insights on bidder behavior in three important ICA formats: CC, HPB, and PDP. To our knowledge we are the first, who do not only analyze which package bids bidders submit, but also which packages they evaluate. We find that restricted package selection of bidders, not the auction design, is the biggest barrier to full efficiency. Our conjecture is that satisficing behavior is an explanation for the heuristics bidder use to select a subset out of the many possible packages in larger combinatorial auctions. Satisficing behavior describes the tendency to select the options that meet specified criteria, but need not to be optimal (Simon 1959). Addressing this problem could have a significant and positive impact on the efficiency of such auction formats. We find only a few statistically significant differences in efficiency and revenue among the auction formats. We replicate important findings of Goeree and Holt (2010) in their value model with global synergies, namely that HPB achieves higher efficiency than PDP. The results of HPB and PDP were not significantly different in a second value model with local synergies, which is less suitable to a hierarchical pre-packaging. Interestingly, we found significantly higher efficiency of the CC auction compared to HPB in the value model with local synergies. In HPB, regional bidders had to take an exposure risk more often and in a significantly higher number of cases they failed to outbid the national bidder in a threshold problem. The paper is structured as follows. Section 2 describes the related literature and gives an overview of experimental studies on CAs so far. Section 3 describes the experimental environment, and the CA formats analyzed in this paper are introduced. In Sect. 4 we present our results both on an aggregate level and on the level of individual bidder behavior. A summary and conclusions follow in Sect. 5.",22
15.0,4.0,Experimental Economics,24 April 2012,https://link.springer.com/article/10.1007/s10683-012-9322-z,Testing the Modigliani-Miller theorem directly in the lab,December 2012,M. Vittoria Levati,Jianying Qiu,Prashanth Mahagaonkar,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Experimental Economics,21 June 2012,https://link.springer.com/article/10.1007/s10683-012-9329-5,Preference intensities and risk aversion in school choice: a laboratory experiment,March 2013,Flip Klijn,Joana Pais,Marc Vorsatz,Male,Female,Male,Mix,,
16.0,1.0,Experimental Economics,14 June 2012,https://link.springer.com/article/10.1007/s10683-012-9331-y,The pivotal mechanism revisited: some evidence on group manipulation,March 2013,Francesco Feri,Anita Gantner,Rupert Sausgruber,Male,Female,Male,Mix,,
16.0,1.0,Experimental Economics,05 September 2012,https://link.springer.com/article/10.1007/s10683-012-9338-4,Demand reduction and preemptive bidding in multi-unit license auctions,March 2013,Jacob K. Goeree,Theo Offerman,Randolph Sloof,Male,Male,Male,Male,"Following the successful US spectrum auctions in 1994, highly valuable public assets around the world are now assigned by auction. Gas stations, airport slots, phone numbers, and telecommunication frequencies have been put up for bid, mostly using some variant of the open ascending auction. However, despite their popularity, open ascending auctions are known to be vulnerable to collusion, sometimes allowing bidders to split the market at low prices.Footnote 1 Before the 2001 Austrian UMTS auction, for example, the largest incumbent, Telekom Austria, announced it “… would be satisfied with just two out of the twelve blocks for offer and if the [five] other bidders behaved similarly, it should be possible to get the frequencies on sensible terms … but that it would bid on a third block if one of its rivals did…” Other bidders understood the hint and bidding stopped after a couple of rounds at low prices with each bidder obtaining just two blocks (Klemperer 2004, p. 136). Several papers have demonstrated that this type of demand reduction can be supported in an equilibrium of the sealed-bid uniform-price auctions, see, e.g., Noussair (1995), Engelbrecht-Wiggans and Kahn (1998), and Ausubel and Cramton (1998). As Ausubel and Cramton (1998) note, such a result can usually be adapted to apply for the open ascending auction.Footnote 2
,
Footnote 3
 Incentives for demand reduction are likely affected when incumbents compete with possible entrants for a fixed number of market licenses. Entrants typically impose a negative externality on incumbents in the ensuing product market and in an attempt to keep the entrants out, incumbents may engage in “predatory bidding” and drive up license prices beyond their economic values. Open ascending auctions are particularly conducive to predatory bidding as they allow incumbents to coordinate their attempts to keep an entrant out. One example of successful preemptive bidding occurred in the US C&F spectrum auction. After round 14, only Verizon, Cingular, and AT&T were competing for the three available licenses in New York. At this point prices were $782 million but Verizon continued to bid for two licenses until Cingular dropped out, resulting in license prices in excess of $2 billion (Cramton 2002).Footnote 4
 When both incumbent and entrant bidders are present, a revenue-maximizing seller thus has to gauge the likelihood of demand reduction versus preemptive bidding in the open ascending auction.Footnote 5 The drawing power of either equilibrium is essentially an empirical issue, which is complicated by the existence of “cheap preemptive equilibria.” In such an equilibrium, incumbents first try to keep entrants out of the market but when preemption turns out to be rather costly, they reduce demand and split the market at an intermediate price level. In the German UMTS auction, for instance, Deutsche Telekom, one of the incumbents, continued pushing up the price when the market could be split among the six active bidders but it later ended the auction before any of its competitors had conceded, paying an extra $2 billion for the two blocks it could have acquired before. This sequence of events surprised many and some even interpreted it as evidence of irrational behavior by Deutsche Telekom.Footnote 6 Ewerhart and Moldovanu (2001) show, however, that preemption followed by demand reduction can be rationalized as equilibrium behavior.Footnote 7
 This paper presents the first experimental study of the impact of negative externalities on outcomes of the open ascending auction. In this auction, the auctioneer steadily raises the price for the goods for sale and the bidders decide at what prices they want to reduce demand. A bidder’s decision to reduce demand is irrevocable and observed by the other bidders. The auction stops when demand equals supply. For brevity we will simply refer to this auction as the ascending auction. In the environment we consider, two incumbents compete with one entrant for six identical licenses. We assume bidders have flat demands for the licenses offered, i.e. a bidder’s independent private value applies to each license bought. Every bidder can buy at most three licenses and if the entrant acquires one or more licenses, both incumbents potentially incur a negative externality (even when an incumbent buys no license). Hence, the only way to avoid the negative externality is for the incumbents to each buy three licenses and keep the entrant out of the market. We consider three different regimes: no externality, a weak externality, and a strong externality. We prove there exists a continuum of “cheap preemptive equilibria” in all three regimes—ranging from a “pure” demand reduction equilibrium to a “pure” preemptive equilibrium. We thus extend the analysis of Ewerhart and Moldovanu (2001) beyond the particular setting of the German UMTS auction and show that preemptive bidding followed by demand reduction can occur more generally, e.g. with multiple symmetric incumbent bidders. Our experimental setting is conducive to any of these equilibria, all of which prescribe identical strategies to players of the same type. Intuitively, one might expect that demand reduction becomes less focal when the negative externality becomes stronger. Our experiment provides a controlled way to evaluate this conjecture empirically. The equilibrium-selection issue present in ascending auctions translates into large uncertainty about revenues. This may explain why some high-stakes license auctions employ a simpler sealed-bid format where high bidders pay their own bids. In some instances, such discriminatory auctions have performed remarkably well. For example, in the Brazilian auction for wireless telephone services, a consortium including BellSouth and Splice do Brazil submitted the winning bid of $2.45 billion for the license covering Sao Paulo. This bid was 60 % higher than the second highest bid, i.e. about $1 billion was left on the table (Milgrom 2004, p. 17). In another instance, Spain’s biggest bank BSCH won the Sao Paulo state bank Banespa for a bid of $3.6 billion. To the embarrassment of the managers of the Spanish bank, the second highest bid was only $1.1 billion, leaving $2.5 billion on the table (Klemperer 2004, p. 136). While these examples are somewhat extreme, they show that the discriminatory auction may outperform the ascending auction in terms of revenues. Our experiments also compare the ascending auction to the discriminatory auction.Footnote 8 In the setting that we consider the discriminatory auction does not support demand reduction in equilibrium. Moreover, preemptive bidding plays less of a role in that entrants have better chances than in the preemptive equilibrium of the ascending auction. A comparison of the performance of the two formats thus hinges on the type of equilibrium selected in the ascending auction. Our experimental results indicate that demand reduction occurs even when incumbents’ incentives are to keep the entrant out of the market. In particular, while the presence of a negative externality makes strategic demand reduction less focal in the ascending auctions, it is always more prevalent than preemptive bidding. Because demand reduction is so wide-spread, the ascending auction is outperformed by the discriminatory auction in terms of revenue and efficiency while entry levels are similar. The remainder of the paper is organized as follows. Section 2 details the auction formats, the experimental design, and procedures. Section 3 presents the equilibrium predictions for our setup. In Sect. 4 we discuss the experimental results. Section 5 concludes.",19
16.0,1.0,Experimental Economics,27 June 2012,https://link.springer.com/article/10.1007/s10683-012-9328-6,The role of information in different bargaining protocols,March 2013,Rafael Hortala-Vallve,Aniol Llorente-Saguer,Rosemarie Nagel,Male,Unknown,Female,Mix,,
16.0,1.0,Experimental Economics,16 June 2012,https://link.springer.com/article/10.1007/s10683-012-9330-z,Testing revealed preferences for homotheticity with two-good experiments,March 2013,Jan Heufer,,,Male,Unknown,Unknown,Male,"Homotheticity of consumer preferences features importantly in theory and applications. If preferences are homothetic, a consumer’s entire preference relation can be deduced from a single indifference set. Assuming homothetic preferences provides useful restrictions for the analysis of consumer demand.Footnote 1
 In applications researchers often focus on special types of homothetic preferences, like those given by a CES utility function. A researcher who wishes to estimate homothetic demand functions using consumption data might wish to test if the data could have been generated by a homothetic utility function. A common nonparametric test of the utility maximization hypothesis has been developed by Afriat (1967) and refined by Varian (1982, 1983). In applications, especially in laboratory experiments, the commodity space is often only two dimensional.Footnote 2 For the two-commodity case Rose (1958) showed that satisfying the Weak Axiom of Revealed Preference (WARP) as introduced by Samuelson (1938) is sufficient for utility maximization. Banerjee and Murphy (2006) used the result to develop a simplified test for utility maximization. In this article it is first shown that Rose’s result carries over to homothetic rationalization: In the two-commodity case pairwise testing of observations is sufficient to test for consistency with a homothetic utility function. The result is stated as a pairwise version of Varian’s (1983) Homothetic Axiom of Revealed Preference (HARP); testing this new sufficient axiom is much faster, which is useful for extensive Monte-Carlo simulations. The result also provides a direct way to compute scalar factors needed to construct the set of all bundles homothetically revealed preferred to any bundle. Knowing that pairwise comparison is sufficient also allows for the definition of a homothetic efficiency index, which can be usefully applied to screen data for severe violations of homotheticity. The test and measure for homothetic efficiency developed in this article are applied to data sets from two-person dictator experiments by Andreoni and Miller (2002) and Fisman et al. (2007) and to a two-asset risk preferences experiment by Choi et al. (2007). In the first two articles the authors use the collected data to estimate parameters of a CES utility function, so the question of the validity of their (implicitly maintained) hypothesis is quite sensible in this context. The remainder is organized as follows. Section 2 reviews the relevant part of revealed preference theory and introduces the Pairwise Homothetic Axiom of Revealed Preference (PHARP) and shows that in two dimensions PHARP is equivalent to HARP. Section 3 provides a simplified test for homotheticity and a way to measure the extent of deviation from homotheticity. In Sect. 4 these ideas are applied to previously published data sets. Section 5 concludes. The supplementary material contains some illustrations and more details on the data analysis.",14
16.0,1.0,Experimental Economics,19 May 2012,https://link.springer.com/article/10.1007/s10683-012-9323-y,Payment schemes in infinite-horizon experimental games,March 2013,Katerina Sherstyuk,Nori Tarui,Tatsuyoshi Saijo,Female,,Unknown,Mix,,
16.0,2.0,Experimental Economics,01 September 2012,https://link.springer.com/article/10.1007/s10683-012-9337-5,The external validity of giving in the dictator game,June 2013,Axel Franzen,Sonja Pointner,,Male,Female,Unknown,Mix,,
16.0,2.0,Experimental Economics,23 June 2012,https://link.springer.com/article/10.1007/s10683-012-9327-7,Self-selection and variations in the laboratory measurement of other-regarding preferences across subject pools: evidence from one college student and two adult samples,June 2013,Jon Anderson,Stephen V. Burks,Aldo Rustichini,Male,Male,Male,Male,"A considerable body of evidence has now accumulated from economic experiments that many individuals exhibit “other-regarding preferences”: not only do they care about their personal material payoffs from social and economic interactions, but they also care about the payoffs of other agents with whom they interact. The designs of such experiments control, at least to a good first approximation, for potentially confounding reasons—such as repeated interactions or reputation effects—that could lead to what appears to be other-regarding behavior, but is really sophisticated self-interest. For example, in the context of voluntary cooperation games only about a third of the participants in experiments typically behave in accordance with own monetary-payoff maximization. The majority of individuals seem instead motivated by other-regarding considerations. For example, more than half of the participants in public goods game experiments are found to be “conditionally cooperative”—they are willing to forgo material gain and cooperate if others cooperate as well (see, e.g., Fischbacher et al. 2001; Herrmann and Thoni 2009; Kocher et al. 2008). Analogous evidence of the importance of other-regarding preferences has been documented by experimental studies using dictator games, bargaining games, trust games, and gift-exchange games (for reviews see, e.g., Camerer 2003; Fehr and Schmidt 2006). However, most of the economic experiments providing evidence for the importance of other-regarding motives have been conducted using samples of undergraduate college students who self-selected into participation in the studies. Generalizations from studies using self-selected college student samples could be problematic for two reasons. First, experimental studies relying on self-selected samples may overestimate the importance of other-regarding preferences if the process by which participants self-select into experiments is correlated with their preferences. For example, as suggested by Levitt and List (2007, p. 166) “… volunteers … who have social preferences or who readily cooperate with the experimenter and seek social approval might be those who are most likely to participate in the experiment.” If this were the case, the pervasiveness of social and other-regarding behaviors documented in economic experiments could substantially reflect the endogenous process by which the experimental participants were selected rather than the underlying propensities of the population. Second, college students clearly differ in many ways from the general population (e.g. in terms of age, education, social class and experience with markets and economic environments), and in principle it is possible that they may also differ in the strength of their other-regarding concerns. In this paper we address these concerns by examining how other-regarding preferences measured in a laboratory experiment vary across three different samples of experimental subjects. One sample consists of undergraduate students who self-select into the laboratory experiment. The two other samples consist of participants recruited among the non-student adult population. An important difference between these two non-student samples is in the procedures used to recruit participants: in one case the recruitment procedures were similar to those used for recruiting undergraduates, and participants could self-select into the experiment. In the other case, the recruitment procedures allowed for very little self-selection of participants. As described in detail in Sect. 2, we measure other-regarding preferences using a sequential social dilemma game in which players choose between an uncooperative action that leaves earnings unaffected, and cooperative actions that are costly for the player, but benefit their partner and increase total earnings. In the experiment decisions were elicited using the strategy method and subjects played both in the role of first-mover and in the role of second-mover. We use decisions in the role of second-mover to classify subjects in three main categories: Free-Riders, who do not display other-regarding concerns and choose the own-material-payoff maximizing actions, Conditional Cooperators, who behave cooperatively only if the first-mover behaves cooperatively, and Unconditional Cooperators, who behave cooperatively regardless of how the first-mover behaves. The latter two types both exhibit other-regarding concerns in the sense that they choose actions that are inconsistent with own-material-payoff maximization. Finally, our data also include a measure of subjects’ need for social approval (the “Unlikely Virtues Scale”, developed by Patrick et al. 2002), which we use to examine whether, as suggested by Levitt and List (2007), approval-seeking is positively related to decisions to self-select into experimental studies. We report our results in Sect. 3. To examine whether other-regarding preferences are more widespread among self-selected participants than among non-self-selected participants we compare the distribution of cooperation types across the two samples of adult non-students. We find that self-selection does not distort the measuring of other-regarding preferences: the proportions of Free Riders, Conditional Cooperators, and Unconditional Cooperators do not differ significantly between the two groups. We also do not find any difference in the need for social approval of self-selected and non-self-selected adult participants. To examine whether there are differences in the extent to which students and non-students engage in other-regarding behaviors we compare the sample of self-selected college students and the sample of self-selected adults. We find that the share of individuals exhibiting other-regarding concerns is remarkably smaller among college students, even after controlling for observable differences in socio-demographic characteristics between the two subject pools. Our finding that the impact of self-selection on measurements of other-regarding preferences in our two adult samples is negligible is in line with the results of two recent studies that also examine the issue of self-selection in economic experiments among college student subjects (Cleave et al. 2011; Falk et al. forthcoming 2012). The finding that college students are less other-regarding than non-students is also in line with the existing literature comparing student and non-student samples across experimental games. We review and discuss these related literatures in Sect. 4, and briefly summarize our conclusions in Sect. 5.",58
16.0,2.0,Experimental Economics,29 May 2012,https://link.springer.com/article/10.1007/s10683-012-9326-8,"Easy come, easy go",June 2013,Fredrik Carlsson,Haoran He,Peter Martinsson,Male,Unknown,Male,Male,"Laboratory experiments are an important tool to gain various economic insights that cannot easily be obtained using market data or field experiment data. While experiments in the laboratory, with greater control over the situation, give higher confidence in the internal validity, the questions about external validity or parallelism of laboratory experiments remain. One crucial question is whether subjects’ behavior in the laboratory is consistent with their behavior outside the lab. There are of course many differences between the laboratory and the field; therefore it is difficult to compare behaviors in these two settings. For example, if people do not give away a large share of their income to charity, it does not prove that the behavior in a dictator game, where subjects on average give away 20 % of their endowment (e.g., Camerer 2003), is not externally valid. Levitt and List (2007) argue that a number of factors can explain the behavioral differences found between the laboratory and the real world: scrutiny, context, stakes, selection of subjects, and restrictions on time horizons and choice sets. It is therefore important to carry out empirical studies that are able to examine the potential behavioral differences directly, identify factors that can reduce the differences, or do both (see, e.g., Smith 1982; List 2008; Falk and Heckman 2009). One focus in the methodological development of lab experiments is to understand and reduce the differences between the lab and the field by, for example, using non-standard subject pools or having subjects earn the endowment. An important reason for the increased use of earned endowments is the intent to mimic the setting outside the lab, where almost all incomes are earned rather than obtained as windfalls. The evidence of the effect of windfall money on subject behavior in the lab is mixed. In dictator games, the dictators contribute less when the endowment is earned (Cherry et al. 2002; Cherry and Shogren 2008; Ruffle 1998; Oxoby and Spraggon 2008). In a public good experiment, Clark (2002) does not find a significant effect of earned endowment on the share of free-riding subjects, while Harrison (2007) shows that the windfall gain in the experiment by Clark (2002) does have a significant effect in a re-analysis of the data. Cherry et al. (2005) find no significant evidence of a windfall-gains effect on the contributions in a public good experiment, saying that although there seemed to be an effect, it was hidden within the more complex considerations of a public good game. However, in a follow-up paper using the best-shot game, Kroll et al. (2007) find significant differences in a public good experiment with heterogeneous endowment. By and large, previous findings seem to indicate that windfall endowment does have an effect on behavior. In a recent paper, Smith (2010) argues that using laboratory experiments has resulted in many insights into human behavior, but the extent to which these can be carried over to behavior when people’s own money is involved is questionable.Footnote 1 Note that this should not be seen as a general critique against laboratory experiments. In many instances, when researchers would like to test the effects of a certain treatment or stimuli keeping all other factors constant, there are strong arguments for conducting laboratory experiments, not the least the strong degree of control over the environment in which the decision is made (Falk and Heckman 2009). In this paper, we are interested in analyzing the behavioral differences between conducting experiments in the lab and the field, and in particular we investigate the role of windfall and earned endowments in the lab and the field. To do this, we use a 2×2 experimental design. We let the subjects participate in a dictator game with a charity organization as the recipient (see, e.g., Eckel and Grossman 1996, for a similar experiment). In the experiment, we keep all factors such as stake, selection of subjects, and the choice sets and time horizons of the experiment constant, only varying windfall gain and whether the experiment is conducted in the lab or in the field. This means that the main differences between the lab and the field in our experiment are due to the environment per se and the degree of scrutiny.Footnote 2 Thus, we can make two comparisons between the lab and the field. The first one is to what extent they provide similar results in terms of the level of donation, under various conditions. The second one is to what extent a change in the context—in our case a change in how the endowment is obtained—affect behavior differently in the lab and in the field. The difference between lab and field can thus also be seen as part of a broader and more complex area related to how behavior is affected by context. It is evident that subjects are potentially sensitive to the context of the experiment and factors such as the choice set, (e.g., List 2007), social distance (Hoffman et al. 1996), and experimenter demand effects (e.g., Zizzo 2010). For a general discussion on the topic of context see the recent work by e.g. Bardsley et al. (2010) and Smith (2010). The contexts of the lab and the field are in many ways very different. In this experiment we have tried to reduce these differences, but there are indeed some fundamental differences between the lab and the field in our experiment as well. The advantage of using a dictator game is that the game is very easy to understand and there are no strategic motives involved. The game also resembles a charitable giving situation, which means that it is possible for us to compare the behavior with that in a field experiment involving charitable giving. Treatment 1 is a standard lab experiment with windfall endowment, and Treatment 2 is a lab experiment with earned endowment. Treatment 3 is a field experiment with windfall endowment, and Treatment 4 is a field experiment with earned endowment. Our design allows us to make two important comparisons. First, we can investigate the effect of windfall gain in the lab (by comparing Treatment 1 and Treatment 2) and in the field (Treatments 3 and 4). Second, by comparing Treatments 1 and 3, and 2 and 4, we can make an overall comparison between the lab and the field, conditional on the way the endowment is received and earned, and thus also the effect of windfall gains in the lab and in the field. In addition, we also investigate the effect of show-up fees in a traditional lab experiment with windfall gain in order to investigate whether subjects view the show-up fee as a compensation for their time or as a windfall gain by conducting a follow-up experiment with two treatments (treatment 5 and 6).Footnote 3
 Why would windfall money matter in a dictator game? One explanation to the potential difference is that people’s preferences for the distribution of money depend on, among other things, the input of the subjects (Konow 2000). When the endowment is a windfall gain, the dictator prefers to split the money more evenly, since she does not do anything to receive the money. Cherry et al. (2002) make a similar argument: earned money legitimizes the endowment and invokes more selfish behavior. In psychology, it has been suggested that subjects use different mental accounts for earned and windfall money (Arkes et al. 1994). Several previous studies have studied differences in behavior between the lab and the field (e.g., Carpenter et al. 2005a; List 2006; Karlan 2006; Benz and Meier 2008; Laury and Taylor 2008; Antonovics et al. 2009; Carpenter and Seki 2010). However, the only other study we are aware of that makes a direct comparison between lab and field using a dictator game with control for a possible subject effect is the one by Benz and Meier (2008), who use an ingenious within-subject design to compare university students’ donation behaviors in the field and in the lab. They conduct a dictator game with two social funds as external recipients, and compare the behavior in the experiment with actual charitable giving by the same subjects. They find a stronger donation behavior in the lab and that there is a positive correlation between behavior in the lab and in the field. An important reason for the difference between the lab and the field settings could be that the lab experiment uses windfall money while the field experiment does not involve an experimental endowment at all. This is exactly what our experimental design allows us to test. By applying a between-subject design and keeping the difference between the laboratory and field experiments to a minimum, our experiment allows us to make clear comparisons of the behavior in the lab and the field. In addition, there is no significant effect on behavior from offering or not offering subjects in a lab a show-up fee. The remainder of the paper is organized as follows. Section 2 introduces the experimental design, and Sect. 3 reports the experimental results. Section 4 concludes the findings.",62
16.0,2.0,Experimental Economics,25 June 2012,https://link.springer.com/article/10.1007/s10683-012-9332-x,Equity vs. efficiency vs. self-interest: on the use of dictator games to measure distributional preferences,June 2013,Pamela Jakiela,,,Female,Unknown,Unknown,Female,"People are often willing to sacrifice some of their income to increase that of others, and experimental dictator games measure the social preferences which drive such sharing decisions. Individual social preferences provide insights into important economic phenomena such as charitable giving, team production, and political support for redistributive social programs. Yet, several recent studies question the use of dictator games to measure innate preference parameters, suggesting that allocation decisions made in the lab may reveal more about responses to the context and framing of the experiment than they do about altruism. In this paper, we use a within-subjects design in which we vary the price of giving within a dictator game in order to decompose social preferences into two components: a traditional measure of altruism in games where the price of giving is one, and an elasticity of substitution which captures the willingness to sacrifice efficiency to enhance equity.Footnote 1 Across multiple decisions within the experiment, we randomly assign subjects to a Taking treatment, varying whether subjects divide their own earned income or another player’s earnings, and isolate the effects of this change on the two distinct dimensions of social preferences.Footnote 2 We find that the overall level of self-interest—which is typically measured in dictator games when the price of giving is one—is impacted by the Taking treatment. However, our results suggest that individual elasticities of substitution, which identify the willingness to trade off equity and efficiency, are not affected by our change in the source of the dictator’s budget. In any dictator game, subjects divide money between self and an anonymous other within the experiment.Footnote 3 Given the non-strategic environment, dictator games provide an unconfounded measure of altruism. Andreoni and Miller (2002) proposed varying the price of allocating money to other within a dictator game, and used responses to price changes to estimate elasticity of substitution self and other.Footnote 4 The elasticity of substitution is of interest because it measures the willingness to reduce the sum of payoffs in order to equalize them. Thus, it is what distinguishes egalitarians from utilitarians: egalitarians care only about increasing the payoff to the worst off individual, while utilitarians seek to maximize total payoffs, even at their own expense. Varying the price of giving in dictator games allows Andreoni and Vesterlund (2001) to conclude that men and women differ in terms of their willingness to trade off efficiency and equity: women have social preferences which are substantially less elastic than those of men. As a consequence, female subjects appear more generous than men when the price of giving is high, but less generous than men at lower prices.Footnote 5
 Varying the price of giving also allows researchers to test the rationality of choices in dictator games using standard revealed preference tests.Footnote 6 Encouragingly, Andreoni and Miller (2002) and Fisman et al. (2007) find that individual choices in dictator games can be rationalized by an other-regarding utility function that is continuous, strictly increasing, and depends only on the payoffs to self and other. However, though revealed social preferences within experiments appear consistent with utility maximization, there is substantial evidence that the level of giving observed in dictator games depends on the experimental context. Varying the level of anonymity or asking dictators to divide earned income (instead of windfall income) leads to a substantial reduction in the observed level of dictator game giving.Footnote 7 Taken together, these two strands of literature create something of a puzzle: individual social preferences appear rational, but also context-dependent. In light of this evidence, Levitt and List (2007) suggest that the other-regarding utility function may, in fact, depend on the payoffs to self and other plus a “social norm” which varies across experimental designs. This raises an important empirical question: which aspects of social preferences are impacted by changes in social norms across experiments, and how? We explore this issue by introducing within-subject variation in whether dictators are giving from their own earnings or taking from other’s earnings, and estimate the impact of this Taking treatment on both levels of giving when the price of giving is one and the willingness to trade off payoffs to self and other in response to price changes.Footnote 8 Subjects participated in dictator games in which the price of giving and whether the dictator was “taking” varied across rounds. The experimental design allows us to test the robustness of elasticity measures to changes in the source of the dictator’s budget. Our results are consistent with previous work in that we find a significant treatment effect of altering the provenance of the dictator’s budget. However, we find that changes in context do not impact the willingness to substitute between self and other in response to price changes, suggesting that dictator games may reveal true underlying elasticity parameters, or at least the elasticity of substitution is a relatively robust component of individual social preferences. The rest of this paper is organized as follows: in Sect. 2, we outline a theoretical framework for interpreting individual choices in dictator games where the price of giving varies; Sect. 3 details our experimental design and procedures; Sect. 4 presents results; and Sect. 5 concludes.",11
16.0,2.0,Experimental Economics,30 August 2012,https://link.springer.com/article/10.1007/s10683-012-9336-6,Finitely repeated games with social preferences,June 2013,Jörg Oechssler,,,Male,Unknown,Unknown,Male,"A well-known result from the theory of finitely repeated games states that if the stage game has a unique equilibrium, then there is a unique subgame perfect equilibrium in the finitely repeated game in which the equilibrium of the stage game is being played in every period. This result has been much used in applied theory with the most prominent example being the finitely repeated prisoners’ dilemma. It is also frequently being invoked in experimental economics when a stage game is played amongst the same set of players for a finite number of periods. The purpose of this note is to point out that the result described above does in general not hold anymore if players have social preferences of the form frequently assumed in the recent literature, for example in the inequity aversion models of Fehr and Schmidt (1999) or Bolton and Ockenfels (2000).Footnote 1 In fact, repeating the unique stage game equilibrium may not be a subgame perfect equilibrium at all in some examples. The logic is simple. In the standard case of selfish preferences, payoffs are separable across periods in the sense that the optimal choice in the last period does not depend on anything that has happened in previous periods. For most models of social preferences, this no longer holds. What has happened in previous periods influences the relative payoffs and therefore also the optimal choice in the last period, which makes it impossible to treat the last period as independent from the rest of the game.Footnote 2
 (Dictator game) As a simple illustration consider the following example of a dictator game with three options for the proposer: (0,100), (40,40), (100,0), where x in (x,y) denotes the amount of money allocated to the dictator and y denotes the amount of money allocated to the other player, the recipient. Choose a model of social preferences and parametrize it such that the proposer ranks (40,40)≻(100,0)⪰(0,100). Assume further that preferences are monotone in the sense that (a,a)≻(b,b) for a>b. Clearly, the unique Nash equilibrium of the stage game is to allocate the money (40,40). However, in the 2-period repeated game there are two subgame perfect equilibria (SPE) that yield the sequence of allocations (100,0)→(0,100) or (0,100)→(100,0), respectively. Since (100,100)≻(80,80), playing the unique stage game equilibrium twice is not a SPE of the repeated game. In this example as well as in all other examples in this paper, repeating the unique stage game equilibrium is either not a SPE of the T-fold repetition at all or at least not the unique SPE. This finding should have relevance for all experiments with repeated interaction, whether with fixed, random, or perfect stranger matching. Often experimenters seem to (explicitly or more often implicitly) assume that the solution to the stage game is the same as the solution to the finitely repeated game. There are numerous examples for this in the experimental literature. A very incomplete sample includes experiments on labor market relations (e.g. Brown et al., 2004; or Cohn et al., 2011), on public goods experiments (e.g. Masclet and Villeval, 2008; Sousa, 2010; Sutter et al., 2010; or Balafoutas et al., 2010), on screening models (e.g. Teyssier, 2008), on voting and redistribution (Cabrales et al. 2012) and on Stackelberg competition (Huck et al. 2001). In the next section I will introduce notation and the model and present a simple adaptation of the standard result referred to above for the case of social preferences. The key section is Sect. 3 where I present further examples of games frequently used in the experimental literature.",14
16.0,3.0,Experimental Economics,19 May 2012,https://link.springer.com/article/10.1007/s10683-012-9324-x,Why do people tell the truth? Experimental evidence for pure lie aversion,September 2013,Raúl López-Pérez,Eli Spiegelman,,Male,Female,Unknown,Mix,,
16.0,3.0,Experimental Economics,23 May 2012,https://link.springer.com/article/10.1007/s10683-012-9325-9,Asymmetric memory recall of positive and negative events in social interactions,September 2013,King King Li,,,Male,Unknown,Unknown,Male,"Existing studies in psychology suggest that negative emotions can enhance memory accuracy, for example, in recalling visual details of pictures (Kensinger et al. 2006) or news reports on verdicts (Levine and Bluck 2004). Kensinger et al. (2006) conducted an experiment in which participants viewed a series of objects. After a couple of days, they were again shown a series of objects and asked to distinguish if these objects were the studied or new objects. They found that participants’ memory accuracy on specific visual details was higher for objects that are supposed to arouse negative emotions (e.g., snack) than neutral objects (e.g., kettle). In another study, Levine and Bluck (2004) asked participants about events related to the murder trial of O.J. Simpson. They found that those participants who were pleased with the verdict were more likely to falsely judge an event that had occurred than those participants whose reaction to the verdict was negative.Footnote 1
 While there are good reasons (e.g., remembering these encounters help people plan and respond better in case of similar encounters in the future) why negative emotions may lead to better memory recall accuracy in those contexts, it is unclear if the above result can be applied in social interactions where people may be motivated to forget negative events. It seems possible that in social interactions, remembering negative events such as being betrayed by someone, may be emotionally costly, e.g., leading to lower self-esteem. The objective of this study is to understand the nature of memory recall in social interactions involving moral sentiments. In particular, we investigate whether individuals exhibit asymmetric memory recall accuracy on positive and negative events they experienced. To the best of my knowledge, this is the first study using incentivized economic game experiments to investigate memory recall. Recently, some theoretical models in economics have, on the contrary, suggested that people may have an incentive to forget negative experiences. In the decision theoretic model of Compte and Postlewaite (2004), people exhibit biases in information processing. More specifically, their model incorporates the phenomenon that recollection of positive events is easier than recollection of negative ones. The intuition behind this result is that if an individual remembers his past failures, it may make future success more difficult. There are also some evidences that one’s mood may affect memory, also known as mood-dependent memory (see Morris 1999 for an extensive review). Bénabou and Tirole (2002) developed a model of endogenous selective memory where the decision maker may be motivated to forget negative experiences (i.e., self-deception) in order to maintain self-esteem or remain confident to accomplish a task. This study also relates to the “rosy view” (Mitchell et al. 1997), which hypothesizes that “People’s expectations of personal events are more positive than their actual experience during the event itself, and their subsequent recollection of that event is more positive than actual experience.” One explanation for the “bias” is based on the strategic manipulation of perceptions: “People selectively search and recall and change the interpretation of past events so that they and the events are seen as positive” (Greenwald 1980; Taylor and Crocker 1981), or in other words: “distort the truth is to enhance our self-esteem” (Loftus 1980). However, most of the existing experimental studies testing the “rosy view” are about events such as Thanksgiving vacations or a 3 week bicycle trip (Mitchell et al. 1997). Thus, it remains unclear whether people follow the “rosy view” in the context of social interactions involving moral sentiments. Note that the “rosy view” makes the same prediction on positive and negative events and is not conditional on who is responsible for a specific event. In this study, we investigate if the “rosy view” holds in the domain of positive and negative events as well as its relationship with who is responsible for a specific event—an important question that has not been investigated before using the methodology of experimental economics. Social interactions often occur across time. When someone does another person a favor, it often happens that after certain period of time, the beneficiary has a chance to reciprocate. Thus, to understand social preference, it is essential to understand how memory recall and the perception of kindness of the event change over time. With the development of experimental economics, many experimental studies on social preference have been conducted.Footnote 2 However, most of them are based on brief periods of laboratory experiments. Few of them empirically investigate the stability of social preference over time. I conducted incentivized experiments to investigate what determines the accuracy of memory recall in social interactions and how the perception of acts performed by others changes over time. The experiment consisted of two stages. In the first stage, subjects played the simplified trust game (Berg et al. 1995); they were not aware that there would be a second stage. In the trust game, player A would choose between trust (strategy A2) and no trust (strategy A1) (see Fig. 1), and player B would choose between being kind (strategy B1) and unkind (strategy B2). In the second stage, subjects were invited to participate in an incentivized online questionnaire in which they recalled their own and the other player’s decision in the trust game. They also gave a rating of the degree of kindness of the decisions and played a dictator game (Kahneman et al. 1986). We manipulated the timing between the first and the second stage to obtain 3 treatments, namely 0 day, 7 day, and 43 day treatments, where the number represented the number of days that elapsed between the two stages. I also conducted a control treatment to investigate if the memory recall bias was due to the “windfall money” effect, i.e., the idea that the unexpected income in the second stage of the experiment might put subjects in a good mood, hence making them more likely to recall good memories.  Trust game. Note: (amount of euro for player A, amount of euro for player B) There are five main findings. First, in contrast to the findings of previous studies (in nonsocial interaction contexts) that negative emotions can enhance memory accuracy, we find that in the context of social interactions, negative emotions may lead to lower memory accuracy. More specifically, victims of unkind acts have a lower memory recall accuracy, while those who benefit from kind acts remember perfectly. This suggests that individuals may be motivated to strategically manipulate their memory by forgetting an un-pleasant experience. It also suggests that individual’s memory recall accuracy in social interaction depends on the kindness of the acts and who performs them. Second, we find that subjects who have committed an unkind act tend to perceive it as less unkind as time moves on. Third, they also tend to believe that there is a higher percentage of players who have committed an unkind act, relative to the belief of those who performed a kind act and the belief of victims. These two results suggest that these individuals may manipulate their beliefs in order to feel less guilty. For subjects performing or benefiting from a kind act or victims of an unkind act, the perception of kindness does not change significantly across time. Fourth, subjects reciprocated the kind act of others by giving a higher amount in the dictator game, and such reciprocity is robust even after 43 days. Fifth, in the control treatment where there is no windfall money, we observe the same pattern of asymmetric memory recall bias. This suggests that the memory recall bias observed is not due to the “windfall money” effect. Overall, our results suggest that individuals exhibit asymmetric memory recall of positive and negative events in social interactions. We find that subjects who suffered unkind acts may engage in strategic manipulation of their memory by forgetting their unpleasant experience. Furthermore, subjects who committed unkind acts may manipulate their beliefs to maintain self-esteem or feel less guilty about the unkind acts they committed. The remainder of the paper is organized as follows: Sect. 2 presents the experimental design, Sect. 3 presents the experimental results, Sect. 4 presents the control treatment and its result, and Sect. 5 concludes.",19
16.0,3.0,Experimental Economics,30 June 2012,https://link.springer.com/article/10.1007/s10683-012-9333-9,Poverty and probability: aspiration and aversion to compound lotteries in El Salvador and India,September 2013,Dean Spears,,,Male,Unknown,Unknown,Male,"This quotation, from the sociological ethnography Random Family, describes the challenges faced by Coco, a young, unmarried mother who moves among housing projects and temporary jobs with little security or permanence. Coco’s economic future was not merely uncertain, it compounded uncertainty upon uncertainty: she had to “do nearly everything absolutely right.” A daughter’s illness, an ex-boyfriend’s return to cocaine, a late city bus, a delayed appointment with a social worker, an unexpected bill—any of these alone could disrupt any project to which Coco might aspire. Unlike richer people with more slack and more security, Coco needed each step to go her way: any aspiration was a compound lottery. Projects with many random steps are “compound lotteries.” A compound lottery is an objectively risky proposition contingent upon the intersection of multiple, sequential risky events; a simple lottery, in contrast, only depends on one. Therefore “you win a dollar if a coin toss comes up heads” is a simple lottery; “you win a dollar if a coin toss comes up heads and then a die rolls an even number” is a complex lottery. Naively, a compound lottery appears to require more steps to “go your way.” Objectively risky lotteries—unlike subjective uncertainty—come with explicit probabilities. This means that the overall probability of winning a compound lottery can be computed, and the lottery could be treated like a simple lottery with this overall probability of winning. For example “you win a dollar if a fair coin toss comes up heads both times” is a compound lottery that reduces to “you win a lottery with 25 percent probability.” This equivalent treatment is known as the “reduction of compound lotteries,” and the assumption that people reduce compound lotteries to their simple equivalents is embedded in expected utility theory’s ordinary application. Few experiments have studied reduction of compound lotteries explicitly. Ronen (1971) studied the effects of reducing compound lotteries—which he called “sequential aggregation”—on business decisions. He considered the case of building a transistor: “a successful assembling of a transistor is contingent upon a successful fabrication stage.” In other words, two independent steps must succeed: fabricating the parts, and assembling them together. Ronen wanted to know how accountants should explain these situations to executives: “Our decision-maker wishes to know not only the cost, but also the probability of producing a good finished unit. Would it make a difference… if instead of giving him an overall estimate of 54 percent, we break this down and present to him the estimates of, say, 90 percent and 60 percent for fabrication and assembly?” Ronen recruited graduate business students for an experiment. Ronen’s participants did not have the option of choosing a simple lottery. Instead, they had to choose between two compound lotteries: all involving drawing a blue marble out of a first bag, and a red marble out of a second. The business students did not reduce the compound lotteries; instead they generally chose the strategy with the higher probability of winning the first round—sometimes even if it meant a lower probability of winning overall. In a more recent experiment, Halevy (2007) finds that participants who are ambiguity neutral—that is, those who do not especially dislike gambles without explicit probabilities—are more likely to reduce compound lotteries. Many participants were compound lottery averse: they preferred simple lotteries. These results suggest that compound lottery aversion is an economic preference, conceptually comparable to risk aversion, ambiguity aversion, or social inequality aversion. However, no previous research that I am aware of has studied the consequences of this preference for economic outcomes. The opening quotation suggests that life is a compound lottery for Coco. This paper proposes that investing in most projects—for example, going to school, finding a job—is a more compound lottery for poorer people. These investments, and the lives they may improve, are risky and uncertain for everybody, rich and poor. However, because poverty typically reflects multiple dimensions of disadvantage (Sen 1999; Deaton 2007), aspirations often require more steps to succeed for poorer people than they do for richer people. Poorer people may live further away, depend on public resources and transportation, speak different languages or in different accents, face social exclusion, have fewer family resources—and each of these may create another obstacle that must be (partially randomly) overcome, relative to richer people attempting the same project. If so, Table 1 details an economic screening mechanism that could result. Some people reduce compound lotteries, while other people are averse to compound lotteries, and this is true both among the rich and among the poor. However, life is more like a compound lottery for the poor, especially when they consider whether or not to make an investment. Therefore, aversion to compound lotteries does not change richer people’s outcomes because—whether they invest or not—they may face risk, but they do not face particularly compound risk. For poorer people, however, the exact same aversion to compound lotteries will prevent them from making an investment because that investment would be a compound lottery—one requiring finding the right bus to the city, it arriving on time, finding the next bus, arranging child care during all of this, feeling healthy, not being socially or racially excluded, getting to know people in town…  
Aversion to compound lotteries prevents investment among poorer people by more than it does among richer people.
 To be clear, the hypothesis is not that poorer people are more averse to compound lotteries than richer people; rather, the hypothesis is that aversion to compound lotteries has a more discouraging effect on poorer people than it does on richer people, because investments in the context of poverty entail more compound risk. After two initial studies that document that aversion to compound lotteries is common and behaves like an economic preference, two studies in El Salvador find that aversion to compound lotteries screens poorer people out of finding employment and going to university respectively, without having a similar effect on richer people. A further study in India, a conservative society where caste and tradition shape many life choices, finds that choosing compound rather than simple lotteries is associated with having a different career than one’s parents among poorer and lower caste participants, but not among richer and higher caste participants. This is far from the first paper to take a lab experiment to participants in a developing country (Cardenas and Carpenter 2008).Footnote 1 This paper differs from earlier lab experiments on poverty in the field in that it proposes not simply a correlation between preferences and outcomes, but specifically an interaction among preferences and situation that influences economic outcomes. Importantly, because this paper’s central hypothesis is an interaction, the vast range of unobservable heterogeneity that is correlated with poverty is not, itself, a threat to empirical identification, unless it also interacts with compound lottery preference. The key question asked here is whether aversion to compound lotteries has different effects for the poor than for the rich. In contrast, what could be a threat to identification is another economic preference that is correlated with compound lottery aversion and specially interacts with poverty. For example, it could be the case that poverty not only entails more compound risk, but it also entails worse risk, in the sense of a lower probability of success at each stage. This may be true, and if so then ordinary risk aversion would also interact with poverty in decisions of whether to invest. If so, this would threaten this paper’s identification only if risk aversion were correlated with compound lottery aversion as measured here. Study 5 explicitly measures risk aversion in additional games and finds no evidence that it is responsible for this paper’s findings. A more complicated case is the apparent close connection between compound lottery aversion and ambiguity aversion. A growing literature is documenting that ambiguity aversion is important to poor people’s decision-making,Footnote 2 although no existing studies address whether ambiguity aversion interacts with poverty. Halevy’s (2007) demonstration of a correlation across participants of aversion to compound lotteries and ambiguity aversion suggests that these phenomena are similar: perhaps ambiguity is mentally represented as compound risk, or vice versa. It is outside of the scope of this applied paper to attempt to settle the unresolved theoretical possibility that compound lottery aversion and ambiguity aversion are the same or deeply related preferences, explored by Segal (1987) and more recently by Seo (2009). It is perhaps reasonable to imagine that ambitious investments are both more compoundedly risky and more ambiguous for the poor than for the rich, especially if these are subjectively similar categories. If so, then ambiguity aversion would not present a threat to this paper’s identification, but it might offer a complementary framework for interpreting the results.",8
16.0,3.0,Experimental Economics,18 July 2012,https://link.springer.com/article/10.1007/s10683-012-9334-8,Costless discrimination and unequal achievements in an experimental tournament,September 2013,Antonio Filippin,Francesco Guala,,Male,Male,Unknown,Male,"Discrimination is a despicable phenomenon that affects in different forms various aspects of social life. It may concern gender, race, social class, geographic origins, ethnicity, age, and several other social categories (Altonji and Blank 1999; Rodgers 2006). For this reason, discrimination has been studied by different disciplines using different methods, and different theories have been proposed to explain its emergence and persistence through time. In this paper we focus in particular on the effect of group identity on discrimination, in the context of an experimental tournament where bidders invest resources to obtain a prize awarded by an auctioneer who observes their exact bids. Bidders are entirely symmetric, except that some of them belong to the same group as the auctioneer. We investigate whether mere categorization is sufficient to induce discriminatory beliefs, and to generate unequal achievements among perfectly symmetric, economic agents. Several theories posit a link between group identity and discrimination. From an evolutionary perspective, Homo Sapiens may have evolved a propensity to cooperate with the members of her own family or tribe, while competition for territory and resources may have made suspicion and hostility advantageous in the context of inter-group behavior. The propensity to discriminate against members of other groups, then, may be a (maladaptive) trait selected in an ancestral environment that differed radically from the large multicultural societies we currently live in.Footnote 1 The existence of such a propensity is confirmed by a large number of studies conducted by social and cognitive scientists. In a famous field experiment with middle-class teenagers, Muzafer Sherif showed that the mere creation of group identities in the context of a peaceful summer camp increased significantly the level of competition and aggressiveness (to the point that the experiment had to be suspended—see Sherif et al. 1961). Henri Tajfel and his co-authors pursued a similar line of research using the so-called “minimal group paradigm”, an experimental setting where subjects’ behavior is manipulated using artificial groups based on arbitrary criteria and meaningless labels (Tajfel et al. 1971; Tajfel 1982). Again, it turns out that individuals become more cooperative, altruistic, and caring towards the members of their own group than towards the members of other groups. From an economic point of view, it is noteworthy that group members are willing to pay a cost to increase inter-group differences in earnings or achievements (they are inefficiently spiteful towards out-group members, in other words). With the development of experimental economics, some of the concepts and techniques developed by psychologists have been transferred in the economic literature. Variants of Tajfel’s minimal group paradigm for example have been used to study cooperation and social preferences in simple strategic games.Footnote 2 Other studies have used affiliation to real groups to explore a range of issues that go from gender wage-gaps to the effect of ethnic identity, race, and religion in economic contexts.Footnote 3 This empirical work was partly stimulated by a budding theoretical literature that aims at incorporating group identity into economic modeling (e.g. Sugden 2000; Bacharach 2006; Akerlof and Kranton 2010). From a theoretical point of view, group identity may influence the attitude of the decision-maker roughly in two ways: by changing her beliefs about the characteristics of other individuals (for example: “she is Japanese, therefore she is a cooperative, hard-working person”); or by changing her affective orientation (“I like her, because she is Japanese”). These two mechanisms have rather different implications, which economists working on discrimination have been exploring for many years. In a seminal series of papers Gary Becker (1957) proposed an explanation of discrimination in the labor market that is driven entirely by the preferences of employers, customers or co-workers. While the generality of this model makes it applicable to a number of real-world cases (sex, religion, race, etc.) it also raises the puzzle of the persistence of discrimination in a competitive environment. If discrimination implies a competitive disadvantage, it should be wiped away by market pressure in the long run, in spite of abundant evidence to the contrary.Footnote 4 So partly out of dissatisfaction with preference-based models, economists started to devise in the 1970s theories that can explain the existence of discrimination equilibria in competitive markets. Phelps (1972) and Arrow (1973) modeled discrimination as a self-fulfilling prophecy in two seminal papers that started the branch of so-called theories of “statistical discrimination” (for a survey see Fang and Moro 2011). The Arrow approach shows that employers’ beliefs about the existence of different characteristics between groups of workers can be self-fulfilling in equilibrium. The mechanism is the following: an a priori unobservable characteristic of workers (e.g. effort) is endogenously affected by employer’s beliefs (e.g. via lower wages or via worse job assignments), leading to a suboptimal choice of effort that determines an outcome consistent with the beliefs of the employer. Attempts to observe statistical discrimination in the laboratory however have fared rather poorly (e.g. Davis 1987; Feltovich and Papageorgiou 2004; Fryer et al. 2005; Filippin 2008; see also the survey of Anderson et al. 2006). A recent unpublished paper by de Haan et al. (2011) is the only exception so far: de Haan and co-authors report an experiment where symmetric workers labeled with arbitrary colors (“green” and “purple”) compete for a job. Employers observe noisy investment signals and discover the real quality of workers only after they have made their hiring decisions.Footnote 5 Although multiple equilibria are possible, the “fair” equilibrium (where employers treat green and purple workers equally) is fragile to small trembles in employers’ beliefs. As they accumulate experience, employers begin to hire preferentially from the group who has invested more; as a consequence disadvantaged workers revise their expectations downwards, invest even less, and a robust pattern of discrimination persists for up to fifty rounds of the game. An important difference between our experiment and earlier laboratory studies of discrimination is that our auctioneers are affiliated with one of the artificially induced groups (in Davis 1987; Fryer et al. 2005, and de Haan et al. 2011, in contrast, the employers/auctioneers are neutral). Although this is not in principle necessary for discrimination to occur—people may be biased in favor of individuals who belong to other groups—we think it is a very natural and common feature of real-world discrimination. Moreover, inducing group identity in an environment with perfect information allows one to investigate whether group identity can generate unequal outcomes independently of statistical discrimination. As we shall see, discrimination does occur and is explained by a costless bias in auctioneers’ decisions, when they allocate the prize among two or more high bids. The tournament is meant to replicate the essential features of various real-world situations where discrimination may play an important role—from elections to political lobbying, research grant competitions, hiring and promotion decisions. Although we did not impose a specific interpretation on our laboratory task, the analogy with labor markets is quite compelling and we will use it to illustrate the logic of the experiment, its motivation, and its policy implications. The paper is organized as follows: Sect. 2 describes the experimental set up and Sect. 3 provides a thorough analysis of the data. In Sect. 4 we outline a Bayesian model to explain bidders’ learning pattern and to estimate their reaction function to auctioneers’ decisions. Section 5 concludes summarizing the main results and articulating their significance for the study of discrimination.",9
16.0,3.0,Experimental Economics,26 July 2012,https://link.springer.com/article/10.1007/s10683-012-9335-7,Shifting the blame to a powerless intermediary,September 2013,Regine Oexl,Zachary J. Grossman,,Female,Male,Unknown,Mix,,
16.0,3.0,Experimental Economics,15 September 2012,https://link.springer.com/article/10.1007/s10683-012-9339-3,Coordination and learning in dynamic global games: experimental evidence,September 2013,Olga Shurchkov,,,Female,Unknown,Unknown,Female,"Coordination amongst economic agents is an essential element in many macroeconomic events. The ability of individual participants to agree on a specific course of action such as an attack on a currency peg, a bank run, or a riot can determine the ultimate outcome for an economy as a whole and may change the course of a nation’s history. Speculative attacks can lead to an enormous negative impact on economic growth and can cause political change and turmoil.Footnote 1 Recent laboratory experiments have shed light on the relative importance of macroeconomic fundamentals and attacking costs during a one-time decision whether to attack (Heinemann et al. 2004) (hereafter, HNO). However, real-world speculative attacks have a dynamic element. Even if a policy intervention succeeds in curbing a given attack, speculators may coordinate again as new information about the state of the economy arrives over time. This paper uses an experimental approach to explore how the arrival of information in a dynamic setting affects the relative aggressiveness of speculators.Footnote 2
 The starting point is a two-stage variant on a dynamic global game developed by Angeletos et al. (2007) (hereafter, AHP). This model captures the features of currency crises that seem to be essential for understanding these issues: (1) the coordination element of a speculative attack that arises due to strategic complementarities in agents’ actions, (2) the heterogeneity of expectations about the underlying economic fundamentals among the agents, and (3) the fact that the agents’ beliefs about their ability to induce a regime change may vary over time.Footnote 3 The model consists of a large number of agents (15 subjects in the experiment) and two possible regimes, the status quo and an alternative to the status quo. The game continues into the second stage as long as the status quo is in place. In each stage, each subject can either attack the status quo (i.e., take an action that favors regime change) or not attack. The net payoff from attacking is positive if the status quo is abandoned in that stage and negative otherwise. Regime change, in turn, occurs if and only if the percentage of subjects attacking exceeds a threshold θ∈ℝ that parameterizes the strength of the status quo.Footnote 4 The parameter θ captures the component of the payoff structure (the “fundamentals”) that is never common knowledge, as is customary in the global games literature (Carlsson and van Damme 1993a, 1993b; Morris and Shin 1998). In the first stage, each subject receives a private signal about θ. If the game continues into the second stage, subjects may or may not receive more a more precise private signal about θ. In each stage, subjects are asked to state their beliefs about the expected size of the attack. In the laboratory, I conduct multi-round experiments that vary the strength of the fundamentals, the cost of attacking, and the availability of information in the second stage. There are four basic takeaways from the experiments. First, individuals display excess aggressiveness. Second, the extra aggressiveness appears to be driven by beliefs about the aggressiveness of others rather than an intrinsic taste for attacking. Third, individuals appear to successfully learn to be less aggressive following failed attacks, but, fourth, this learning is supplanted by the provision of additional information in the second stage. The first-stage results are consistent with the previous literature on static global games in that the size of the attack/the individual likelihood of attacking decrease in the fundamental/the private signal, as well as in the cost of attacking (Heinemann et al. 2004). Furthermore, I document a systematic deviation from the theoretical prediction toward aggressive attacking behavior, regardless of the cost, which is also consistent with previous literature (Heinemann et al. 2004). However, while previous studies find weak or no evidence of strategic reasoning, I show that 81.4 percent of subjects best respond to their stated beliefs about others strategies.Footnote 5 In particular, subjects expect others to attack aggressively and, in turn, respond with aggressiveness.Footnote 6 The strategic complementarity feature of the environment then rewards this behavior with more successful attacks, thereby partially validating the aggressiveness of the original beliefs and resulting in subjects maintaining the same high level of aggressiveness in subsequent rounds of play. This self-fulfilling nature of beliefs partially explains the relative unimportance of the cost of attacking and the persistence of aggressiveness of beliefs as the game is repeated. Subjects not only believe that the status quo is more likely to be successfully overturned than the theory predicts, but they also experience this to be the case. In contrast to the static global game framework, the two-stage dynamic setting allows me to observe how the arrival of new information affects subjects’ ability to coordinate. If the experiment continues into the second stage in the no new information treatment, subjects learn only that the game has not ended. In the model, this implies that the fundamentals are sufficiently strong and that subjects should refrain from attacking again. Indeed, subjects exhibit this type of learning, since their aggressiveness is greatly reduced in the second stage of this treatment. Furthermore, the reduction in aggressiveness is matched by a significant decrease in the size of the believed attack relative to the first stage. In the new information treatment, subjects receive a new, more precise, private signal in the second stage. In this case, subjects are still able to learn by observing that the game did not end after the first-stage attack, but they can also now learn by incorporating this more precise information into their decision of whether to attack the status quo. In the model, the arrival of this type of precise new information may result in an additional equilibrium where a new attack becomes possible. Experimentally, the arrival of new information significantly increases the probability of attack in the second stage relative to the treatment without new information. Learning, induced by the observation of a failed attack, alone makes subjects relatively less aggressive, but a new attack may become possible as the participants accumulate new information about the strength of the regime. Together, the second-stage findings imply that a policy-maker, having previously successfully defended the regime, cannot be assured that crisis is averted. Although there is a rich experimental literature on coordination games, this paper is the first to study a dynamic global coordination environment in an experimental setting. This study is also the first to characterize the role of self-fulfilling expectations in driving persistently aggressive attacking behavior. Furthermore, this paper shows that, while a failed attack can significantly reduce the aggressiveness of the speculators, the arrival of new private information about the state of the fundamentals can provoke new attacks to occur. This paper is related to the empirical literature on coordination games which begins with experiments using common knowledge in a static environment (Cooper et al. 1990, 1992; Van Huyck et al. 1990). Several studies have explored the predictions of common knowledge games in a dynamic environment (Cheung and Friedman 2009; Brunnermeier and Morgan 2010). Experimental papers that are most closely related to the present study test the predictions of static coordination games with private information (global games). Cabrales et al. (2007) study two-person games with random matching inspired by Carlsson and van Damme (1993a), while Heinemann et al. (2004, 2009) examine the static speculative-attack model of Morris and Shin (1998). These papers focus on the static elements of coordination and do not provide insights into what happens after an unsuccessful attack or upon arrival of new information over time. Another closely related body of literature examines dynamic coordination games (Costain et al. 2007; Schotter and Yorulmazer 2009). These papers provide important conclusions about the outcomes of sequential-move coordination games. However, they do not incorporate the effects of the arrival of new private information over time. In addition, several studies have used field data to test the predictions of static global games (Prati and Sbracia 2002; Chen et al. 2010; Danielsson and Peñaranda 2011). I view these studies as complementary to the experimental approach, although the data in these studies do not enable the researchers to identify the causal effects of the strength of the fundamentals, the cost of attacking, or the arrival of new information. The rest of the paper is organized as follows. Section 2 discusses the experimental design: the theoretical environment and the parameters implemented in the lab, the treatments, and the procedures. Section 3 describes the forces behind attacking behavior in the first stage of the experiment, including the extent of subjects ability to best respond to others’ strategies and beliefs about others’ strategies. Section 4 provides evidence that subjects learn to be less aggressive in the second stage of the no new information treatments. Section 5 focuses on the effects of the arrival of new information on subjects’ aggressiveness. Section 6 concludes and discusses potential implications of the results.",13
16.0,3.0,Experimental Economics,13 September 2012,https://link.springer.com/article/10.1007/s10683-012-9340-x,A portmanteau experiment on the relevance of individual decision anomalies for households,September 2013,Alistair Munro,Danail Popov,,Male,Male,Unknown,Male,"One large portion of the experiments on individual decision-making has been devoted to examining the robustness of the standard models of individual choice. To truncate a huge and ongoing endeavour into a few words: while many results continue to attract controversy, a significant number of experiments have found anomalies—i.e. deviations from the predictions of rational choice models. This paper reports an experiment designed to test for anomalies in the choice behaviour of established couples, married and unmarried. A simple motivation is that the majority of adults actually live with other adults, typically in some form of long-term relationship. Yet despite the importance of multi-person households, there is actually very little experimental evidence on decisions made by natural groups such as couples. Perhaps they do not behave in a similar manner to individuals. Before moving on it is worth stating that the word ‘anomaly’ is used here without its usual connotation of irrationality. With individual decisions, choice models often have clear predictions about what is rational and what is not. An anomaly is then a systematic deviation from rational behaviour. With collective choice, as in the household, behaviour depends on the preferences of the individuals and on the nature of the game that is played between them. Since the exact structure of the latter is often not observable, conclusions about rationality or its absence are harder to obtain.Footnote 1 Here, for want of a better word, we use the term anomaly as shorthand for ‘behaviour such that if it was observed within individual choice it would be typically be interpreted as a deviation from rational choice’. One reason for investigating household choices lies in attempts by behavioural researchers to link experimental data with that drawn from the field—for instance, the Benartzi and Thaler (1995) explanation of the familiar equity premium paradox. Since identification of anomalies from non-experimental data is problematic, usually the empirical strategy is to show that the field data is consistent with the chosen anomaly and then to rule out other possible explanations as far as is possible. A basic problem with such a strategy is that many decisions in the real world are made jointly, by couples (or by a household) and not individually as in the laboratory, or even in typical field experiments. Thus the researchers are trying to tie field data generated from the collective decisions of households to results gleaned from individual choice experiments. There is therefore a missing link in the chain of evidence: experimental tests of decisions made by couples. We aim to narrow the gap between field and laboratory, by reporting an experiment on couples living together in established relationships. Because there is an almost complete lack of findings on the topic we test for several effects in one experiment. Our aim therefore is not to probe the detailed factors which determine the strength or causes of particular anomalies. Rather it is to provide broad-brush evidence on a topic, which though important, currently lacks data. In the experiment we compare the behaviour of couples to individuals with similar backgrounds to the couples (hereafter labelled individuals) and to undergraduate students. Our results are basically that couples exhibit anomalies in more or less the same way as the non-couples. In other words, there is no evidence that two heads are better than one; nor is there any evidence that being in a couple makes anomalous choices more likely. The plan of the remainder of the paper is as follows: in the next section we provide a brief background and consider the relevant literature. In Sect. 3, we describe the experimental procedure with results presented in the following section. Section 5 concludes the paper.",9
16.0,3.0,Experimental Economics,27 September 2012,https://link.springer.com/article/10.1007/s10683-012-9341-9,Do people care about social context? Framing effects in dictator games,September 2013,Anna Dreber,Tore Ellingsen,David G. Rand,Female,Male,Male,Mix,,
16.0,3.0,Experimental Economics,18 October 2012,https://link.springer.com/article/10.1007/s10683-012-9342-8,Is there selection bias in laboratory experiments? The case of social and risk preferences,September 2013,Blair L. Cleave,Nikos Nikiforakis,Robert Slonim,,Male,Male,Mix,,
16.0,3.0,Experimental Economics,08 November 2012,https://link.springer.com/article/10.1007/s10683-012-9343-7,The influence of spouses on household decision making under risk: an experiment in rural China,September 2013,Fredrik Carlsson,Peter Martinsson,Matthias Sutter,Male,Male,Male,Male,"Many important economic decisions—e.g., labor supply, residential location, buying insurance or a new car, investing in stocks and bonds or in children’s education—are often made by households rather than by individuals. This implies that the decisions will be a function of the preferences of the household members and the decision making process. In particular, it has been shown that decisions and outcomes in a household—such as child health, nutrition, and expenditures for different goods and services (e.g., tobacco versus child care)—depend strongly on whether its income is controlled by the husband or the wife (see Thomas 1994; Lundberg et al. 1997; Phipps and Burton 1998; Duflo 2003). Qian (2008), for instance, reports that the relative female income (as a share of total household income) in Chinese rural households has had significantly positive impacts on the survival rates for girls and on the educational attainment of children. In this paper we present an experiment that was run in the homes of 117 randomly selected married couples in rural China. Wives and husbands had to choose between different lotteries first individually and then jointly. Our aim is to provide controlled experimental evidence on two important aspects of household decision making. First, we address how similar the two spouses’ individual decisions are when decisions are made separately, and which socioeconomic factors influence the level of similarity. Second, we study how a couple’s joint decision relates to the spouses’ separate decisions, and which conditions are related to a stronger influence of the wife on the joint decision. Thus, we can study the circumstances that determine the outcome of an implicit bargaining process that is assumed to take place in many household decisions. We find that spouses in richer households have more similar individual risk attitudes. In general, a couple’s joint decision is closer to the husband’s individual decision. However, we show that the preferences of women are better reflected in joint decisions if women contribute relatively more to household income, live in households of higher joint income, or are communist party members. Recently, experiments have become increasingly popular as a method for gaining deeper insights into household behavior by carefully controlling—and varying—the conditions under which household members can make decisions (see, e.g., Peters et al. 2004; Bateman and Munro 2005; Ashraf 2009; de Palma et al. 2011; Abdellaoui et al. 2011). These experiments are a result of a shift in the theoretical modeling of decision making in a household from using unitary models, which assume a unique decision-maker, to collective models (see, for instance, Lundberg and Pollak 1996; Vermeulen 2002; de Palma et al. 2011). Our paper is most closely related to contributions by Bateman and Munro (2005) and de Palma et al. (2011). Bateman and Munro (2005) examine whether decisions made by couples conform more or less to the axioms of expected utility theory compared to decisions made by spouses individually. To do this, they invited 76 couples and let the spouses make risky decisions both separately and jointly. Their results suggest that couples exhibit the same kinds of departures from expected utility theory as individuals. Furthermore, they find joint decisions to be typically more risk averse than the spouses’ individual decisions. de Palma et al. (2011) focus on the question of which spouse has more influence on joint decisions. Based on observations from 22 couples, they conclude that husbands generally have a stronger influence on joint decisions than wives, although wives gain influence if they control the computer keyboard while entering the joint decisions in the experiment. Contrary to Bateman and Munro (2005), de Palma et al. (2011) also report that the joint decision of a couple tended to be less risk averse than the spouses’ individual decisions. Our paper distinguishes itself from these important contributions in several dimensions: First, the subject pools are completely different. While Bateman and Munro (2005) and de Palma et al. (2011) ran their experiments in highly developed countries, ours was conducted in the field in a rather poor area of China, which by many accounts is still a developing country. The experiment was run in the Guizhou province in the southwest of China. This is the poorest province in China. The average yearly per-capita income of the couples participating in our experiment was 570 US-Dollars and they had on average only 4.8 years of schooling, allowing us to study household decision making in a very different environment than in the studies of Bateman and Munro (2005) and de Palma et al. (2011).Footnote 1 Second, our sample is random. This means that the subjects were not invited through flyers or newspaper ads to participate in an experiment, which might give rise to endogeneity effects as to who is going to participate. Instead the participants in our experiment were randomly selected by the village council (as described in more detail in Sect. 3) and then approached by the experimenters in their homes. Although participation was voluntary, no couple refused the invitation to participate. Third, our experiment involves considerably higher stakes than those used in the UK by Bateman and Munro (2005) or in Germany by de Palma et al. (2011). In our experiment, the average earnings from participating in the experiment were roughly equivalent to the average income earned from three days of off-farm work.Footnote 2 Fourth, our research focus is different. Unlike Bateman and Munro (2005), we are not interested in whether couple decisions exhibit more or less so-called anomalies in decision making than decisions made individually; instead we examine the socio-demographic conditions under which (i) a couple’s separate decisions are more similar and (ii) a couple’s joint decision is more likely to be driven by the wife’s individual preferences. Although de Palma et al. (2011) did address item (ii), they only took account of who was holding the computer mouse when entering the joint decision, while we consider a set of socio-demographic variables to estimate how individual decisions of spouses relate to the couple’s joint decision. Studying decision making in couples is also related to a recent strand of literature that addresses how groups make decisions in risky choices. The evidence with randomly formed groups is quite mixed.Footnote 3 There are, however, important differences between randomly formed groups in the laboratory and married couples, making it difficult to draw direct comparisons. Married couples do have a history with repeated interactions, while randomly formed groups in the laboratory only meet for a very short period for a specific (experimental) task, after which they separate immediately. The process of sharing information, the degree of potential altruism and willingness to find compromises are most likely different in real couples than in ad hoc groups as a consequence of repeated interaction in couples. The closest paper on groups and risk taking to our approach is He et al. (2012) who ran risk-taking experiments with student couples to bridge the gap between ad hoc created groups and (long-married) couples, and they find that student couples are more risk neutral than the choices of individual students. The rest of the paper is organized as follows. Section 2 provides background information on our subject pool and on the Chinese province where the experiment was conducted. Section 3 introduces the experimental design and procedure. Section 4 presents the experimental results, and Sect. 5 concludes the paper.",32
16.0,3.0,Experimental Economics,13 November 2012,https://link.springer.com/article/10.1007/s10683-012-9344-6,Estimating depth of reasoning in a repeated guessing game with no feedback,September 2013,Mariano Runco,,,Male,Unknown,Unknown,Male,"In this paper we estimate the level of each subject and analyze its evolution using an Iterative Best Response model with data from Weber (2003) ten-period repeated guessing game experiment with no feedback. In this game each player in a group must simultaneously choose a number between 0 and 100. The experimenter records all choices, computes the average and the player closest to \(\frac{2}{3}\) of that average wins a monetary prize, the rest gets nothing.Footnote 1 These types of guessing games have been extensively used to analyze the way in which individuals form beliefs about the choices of other players. The data is analyzed using the theoretical framework of the Iterative Best Response model, which consists of different types or levels of players. For example, an individual who chooses randomly between 0 and 100 does not best respond to any belief about the choice of other players, we call her a Level-0 subject. It is assumed that L-0 choices are uniformly distributed on [0,100]. A Level-1 individual believes that all other players will randomly choose between 0 and 100, thus she will maximize her expected payoff by choosing \(50 \cdot\frac{2}{3}=33.33\) (if the number of participants is large enough her own choice will have a negligible effect on the average). A Level-2 individual believes that all others are Level-1 and will choose \(33.33 \cdot\frac{2}{3} = 22.22\). More generally, a Level-k player chooses \(50\cdot(\frac{2}{3})^{k}\) to best respond to the belief that all others are Level-(k−1). In the limit, a Level-∞ will choose 0, the Nash Equilibrium strategy. This is a model of boundedly rational agents, individuals maximize their expected payoff given their beliefs about others’ choices but, unlike the Nash Equilibrium concept, beliefs are not consistent with their actions. This might be due to cognitive limitations (people have difficulties with higher order reasoning) and/or overconfidence (individuals think they are on average smarter than the rest). In this paper there are several questions we want to answer: Do subjects increase levels monotonically? Do they skip intermediate levels or the increase is stepwise? Do they revert back to lower levels? Does the distribution of levels stabilize over time? If so, what is that distribution? There is ample evidence that individuals “learn” how to play a game, in the sense of convergence to equilibrium, when they repeatedly participate in it.Footnote 2 Nagel (1995) performed this experiment repeatedly with the same subjects providing feedback on the average after each round and found that the approximation to the Nash equilibrium was very fast. In this case it is not clear whether subjects improved their depth of reasoning with repetitions or they anchored their choices on the feedback of the previous round.Footnote 3 Therefore, in order to isolate the effect of thinking repeatedly about a strategic interaction on depth of reasoning we use data from a guessing game performed repeatedly with subjects receiving no feedback after each round. We estimate each individual’s posterior probability of being Level-k given her choice using four different specifications to test the robustness of our findings. We keep track of each subject’s type and compute the transition probabilities of being Level-j in the current period given that she was Level-k in the previous period. We find that the great majority of individuals either remain in the same level they were in the previous game or advance to the next two levels of reasoning. In particular the lowest levels (Level-0 and Level-1) have a larger probability of transitioning to a higher level than L-2 or L-3. Thus, we can conclude that subjects, through repetition of the task, quickly become more sophisticated strategic thinkers as defined by higher levels. However, in some specifications the highest levels have a relatively large probability of switching to a lower level in the next period. In general, it seems that depth of reasoning increases monotonically in small steps as individuals are subjected to the same task repeatedly. Stahl and Wilson (1995) analyzed 3×3 games to determine what model (Level-k, naive Nash, rational expectations or worldly) best describes the choices of actual experimental subjects, using a logit model to estimate the relevant parameters. Unlike Nagel (1995), in their model a level-k player best responds to a normalized proportion of lower levels. Later Stahl (1996) developed a rule learning model of the Beauty Contest and used the data from Nagel (1995) to estimate it. Players made choices based on an initial disposition to a given rule and later on, based on the feedback they received about the previous average, they updated those propensities. Rules that performed well were more likely to be used and those that performed poorly were less likely to be used. He interprets the evidence, in contrast to Nagel (1995), as providing support for increasing depth of reasoning. However, as mentioned before, the fact that players receive feedback complicates the interpretation of the data. With feedback there might be two sources of reduction in players’ guesses, on the one hand, increased depth of reasoning and on the other anchoring with respect to the average observed in the previous period. By analyzing data of an experiment without feedback we can eliminate the anchoring effect and study the change in depth of reasoning. Nagel et al. (1999) used a mixture of normals and uniform distributions to estimate the model using the Expectation Maximization algorithm with data generated by three newspaper experiments. We follow a similar approach, but unlike them, we estimate the models independently in each round and analyze how players’ levels evolve. It is interesting to note that in Weber’s experiment subjects modify their beliefs as they play the game repeatedly even without feedback. An important aspect of the Beauty Contest is that social norms, social preferences and other non-strategic factors play no role in it, making it a suitable tool to analyze the strategic thinking of subjects. Other well-known concepts of bounded rationality that could be applied to model the choices of a Beauty Contest are the Quantal Response Equilibrium of McKelvey and Palfrey (1999) and Noisy Introspection of Goeree and Holt (2004). In a QRE players “better” respond to the beliefs they have about the actions of their rivals and, unlike a Level-k model, beliefs are consistent with actions. This is a generalization of the Nash Equilibrium concept in the sense that players do not select with probability one the action that generates the highest expected payoff, but assign higher probability to actions generating higher payoffs. This sensitivity of probabilities to payoffs is measured by a sensitivity parameter μ. In a QRE players immediately reach equilibrium, giving little room for analyzing increases in depth of reasoning. On the other hand, NI is a noisy version of the concept of rationalizability. It is not an equilibrium concept and can accommodate the Level-k model as a special case. Recently, Breitmoser (2012) argues that levels higher than 1 in a beauty contest can be modeled more precisely as either QRE or NI. This is an interesting idea that requires further research, however, it lies beyond the scope of this paper since we are interested in analyzing stepwise reasoning. Other papers modeling learning in games are Erev and Roth (1998), Cheung and Friedman (1997), Camerer and Ho (1999) and Stahl (2000). Camerer (2003) provides a survey of models of learning in games. All those models consider a strategic situation where players do learn from past outcomes. However, we are not aware of papers that analyze how subjects change beliefs without feedback. Here we are not interested in modeling the learning process, but in estimating the evolution of players’ levels when they repeatedly play the guessing game without feedback.",
16.0,3.0,Experimental Economics,21 November 2012,https://link.springer.com/article/10.1007/s10683-012-9345-5,Estimating the causal effect of beliefs on contributions in repeated public good games,September 2013,Alexander Smith,,,Male,Unknown,Unknown,Male,"Recent literature on conditional cooperation in repeated public good games finds that contributions are increasing in beliefs about the contributions of others (Croson 2007; Fischbacher and Gaechter 2010; Neugebauer et al. 2009). However, the extent to which the relationship is causal is unclear. Standard regressions of contributions on beliefs overestimate the causal effect of beliefs if there is some tendency for contributions to cause beliefs or if there are time-varying omitted variables which are positively correlated with beliefs. Also, accurately estimating the effect of beliefs is important for making comparisons to the effects of other variables, such as the previous contributions of others. Addressing these issues, this paper uses instrumental variables (IV) for determining the causal effect of beliefs. Research on conditional cooperation goes back as least as far as Sugden (1984), who uses reciprocity for explaining the voluntary provision of public goods. In practice, different methods are used for studying conditional cooperation. Many, however, broadly focus on the tendency for subjects to “match” the contributions of others (Ashley et al. 2010; Croson et al. 2005; Ferraro and Vossler 2010; Guttman 1986; Keser and van Winden 2000). Keser and van Winden (2000), for example, interpret higher cooperation with fixed compared to random matching as conditional cooperation, motivated by forward-looking and reactive behavior. Others find that contributions are related to the contributions of others in the previous round (Croson et al. 2005). Ashley et al. (2010) and Ferraro and Vossler (2010) study deviations from the group average, reporting that subjects decrease their contributions when they contributed more than the average in the previous round. Somewhat differently, Baker et al. (2009) examine the tendency for subjects to match the contributions of an anonymous external donor. Our experiment involves eliciting beliefs about the contributions of others, and is similar to (components of) a number of previous experiments (Croson 2007; Dufwenberg et al. 2011; Fischbacher and Gaechter 2010; Gaechter and Renner 2010; Neugebauer et al. 2009). The most important difference is that our game is repeated for 20 rounds compared to no more than ten for the similar experiments cited.Footnote 1 Creating a longer panel of data helps accommodate using lagged variables as instruments and in theory decreases the bias of fixed effects estimation when contributions are modeled as an autoregressive process (Nickell 1981). Thus, compared to the other papers, our primary contributions are empirical. Contributions are initially regressed on beliefs and other explanatory variables in models estimated using standard techniques, consistently producing highly significant evidence of a positive relationship between contributions and beliefs. We then include the previous contributions of others, attempting to determine their relative effect in determining contributions. However, the estimated effects of beliefs become larger and the effects of the previous contributions of others are negative, which conflicts with prior literature (Croson et al. 2005). Both findings are consistent with the hypothesis that the estimates are biased due to the endogeneity of beliefs. Using IV estimation, we estimate that the causal effect of beliefs is about half as large as suggested by ordinary least squares (OLS). The results are consistent with previous conclusions that reciprocity (Croson 2007; Dufwenberg et al. 2011) and guilt aversion (Dufwenberg et al. 2011) are useful for explaining cooperation, and that beliefs are a key determinant of contributions (Fischbacher and Gaechter 2010; Neugebauer et al. 2009). However, they also suggest that IV methods are the appropriate way of estimating the effect of beliefs. In our context, valid instruments have direct effects on beliefs, but not on contributions. As instruments, we use the second and third lags of beliefs and of the average contributions of others. We avoid using the first lags of both variables out of concern that they may have direct effects on contributions, making them invalid as instruments. Post-estimation diagnostic tests support our choices of instruments, providing evidence that beliefs are endogenous, failing to reject the null hypothesis that the instruments are exogenous, and indicating that we do not have weak instruments. We use the IV strategy for comparing the effect of beliefs to the effect of the previous contributions of others, finding that beliefs have the larger role in determining contributions. A belief-based model also performs well compared to models based on deviations from the average contributions of others in the previous round (Ashley et al. 2010; Ferraro and Vossler 2010). Thus, our analysis sheds new light on the factors affecting behavior in repeated public good games. Specifically, we show that even though the causal effect of beliefs is smaller than suggested by OLS, beliefs are the main determinant of contributions. This paper complements a somewhat limited previous literature addressing endogenous variables in economic experiments. Ham et al. (2005) study the role of cash balances in private value auctions. They add a random component to cash balances in order to introduce exogenous variation, which can then be used as an instrument for estimating the causal effect of cash balances on bidding. Costa-Gomes et al. (2010) similarly add a random component to the amounts that trustees return in trust games in order to create the exogenous variation necessary for estimating the causal effect of trustor beliefs on the amounts that trustors send to trustees. Finally, other papers estimate the effects of beliefs using structural models (Bellemare et al. 2008, 2011).",36
16.0,3.0,Experimental Economics,17 November 2012,https://link.springer.com/article/10.1007/s10683-012-9346-4,Splitting leagues: promotion and demotion in contribution-based regrouping experiments,September 2013,Susana Cabrera,Enrique Fatás,Tibor Neugebauer,Female,Male,Male,Mix,,
16.0,3.0,Experimental Economics,30 July 2013,https://link.springer.com/article/10.1007/s10683-013-9371-y,Erratum to: The gift of advice: communication in a bilateral gift exchange game,September 2013,David J. Cooper,John P. Lightle,,Male,Male,Unknown,Male,,
16.0,4.0,Experimental Economics,12 January 2013,https://link.springer.com/article/10.1007/s10683-012-9347-3,The gift of advice: communication in a bilateral gift exchange game,December 2013,David J. Cooper,John P. Lightle,,Male,Male,Unknown,Male,"Fehr et al. (1993) inaugurated one of the most influential literatures within experimental economics. Standard economic theory built on the assumption that individuals are self-regarding predicts that the relationship between wages and effort in a labor market with incomplete contracts should be flat.Footnote 1 Instead, a strong positive relationship is observed between wages and effort, a finding that has been replicated many times (see Cooper and Kagel 2013). Fehr et al. and most of its numerous successors share two important features which motivate the experiments we report below. First, reciprocity can only occur through a single channel, effort. In reality there are many ways employees can reciprocate the wage choices of their manager. If the existence of alternative means of reciprocation affects the relationship between effort and wages, then experiments lacking alternative channels of reciprocation may misrepresent this relationship. Our experiments add a simple, natural channel of reciprocation to a bilateral gift exchange game: communication. A second feature of experiments on gift exchange plays an important, if generally unrecognized role, in generating the main results. The strong relationship between wages and effort can be seen easily because a wide range of wages are persistently chosen. This is puzzling since the strong positive relationship between wages and effort implies that higher wages yield higher payoffs for managers. For example, the median wage is 75 over the second half of Fehr et al.’s experiments (periods 7–12), but the payoff maximizing wage is 85. Half of the wages chosen in the second half of the experiment are unambiguously below the payoff maximizing level! Moreover, learning is surprisingly weak. The median wage only shifts slightly (70 to 75) between the first and second halves of Fehr et al.’s experiment and low wages (wage≤60) have not disappeared in the second half of the experiment (16 % of observations). Failure to learn the payoff maximizing wage is not unique to Fehr et al. Experiments on gift exchange often feature persistent use of wages below the payoff maximizing wage. A plausible explanation for this failure is that learning to set a payoff maximizing wage is difficult. For most managers, it probably isn’t obvious ex ante that higher wages lead to higher effort. They cannot acquire the necessary information to learn this relationship unless they experiment with high wages, and even if they do experiment with high wages, noisy feedback may obscure the relationship between wages and effort. Our experiments study whether improving managers’ information in a bilateral gift exchange game, either endogenously through advice from their employees or exogenously through additional feedback about the relationship between wages and effort, can help managers learn to take advantage of the strong positive relationship between wages and effort. We explore these two issues through laboratory experiments where subjects assigned the roles of managers and employees play a series of twenty bilateral gift exchange (henceforth BGE) games (Fehr et al. 1998). In each round, managers and employees are randomly matched to play a single BGE game. Play begins with the manager choosing a wage. The employee observes this wage and responds by choosing a costly effort level. The game concludes after the choice of an effort level, giving managers no means of responding after their employees have chosen an effort level. If employees are self-regarding (only concerned with maximizing their monetary payoffs), they have no incentive to choose more than the minimum effort level since the wage cannot be made contingent on the effort. Anticipating this, managers have no reason to provide more than the minimum wage, regardless of their employees’ ability to send a message. However, given ample existing evidence that subjects are not self-regarding, we anticipated a strong positive relationship between wages and effort (i.e. gift exchange). Our initial sessions, henceforth “Experiment 1”, study the impact of allowing employees to send messages to their managers after a wage has been chosen.Footnote 2 As a treatment variable, the employee can enter a text message at the same time he chooses an effort level. This is shown to the manager when he observes the employee’s effort. All sessions of Experiment 1 included ten rounds where messages were available and ten rounds where they were not, with the order of the treatments systematically varied. We take multiple approaches to developing hypotheses about the effects of allowing employee messages on gift exchange. One possibility is that positive messages serve as a substitute for effort as a method of reciprocating high wages. The predicted effect of allowing messages is primarily that effort should be lower controlling for wages. Intuitively, kind messages are cheaper than effort, so employees who wish to reward their manager should substitute away from effort towards messages. Wages are predicted to be higher when messages are allowed. This is a second order effect due to changing the slope of the wage-effort curve and should be very weak. A related approach models employees as using positive messages for “emotional expression”, to borrow a term from Xiao and Houser (2005). This approach predicts that effort and wages will both be lower when messages are allowed. The idea here is that the employee’s intended target might be himself. Sending a positive message provides a release for his gratitude, leaving him free to choose a lower effort. We get distinctly different predictions if employee messages serve primarily as an aid to managers’ learning. Managers cannot observe what their employees would have done if a different wage had been chosen, but employees possess this information. Employees can write almost anything they want in their messages, leaving them free to pass their information on to the managers. If they choose to do so, wages should be higher over time when messages are available as managers learn that setting higher wages leads to higher profits. Unlike the first two approaches, no effect is predicted on effort. The results of Experiment 1 come down clearly on the side of messages affecting gift exchange through enhanced learning. Effort (controlling for wages) is unchanged when communication is possible, contrary to either of the approaches that rely on an interaction between messages and other-regarding preferences. However, wages are significantly higher when communication is possible. This effect has a strong dynamic element, taking several periods to emerge but persisting even after the possibility of sending messages is removed. To understand why messages have a powerful effect on wages, we coded all of the employees’ messages for three broad categories of content: positive responses (verbal rewards), negative responses (verbal punishments), and advice on how to play the BGE game. Of these three categories, only advice has a strong positive effect on future wages chosen by the recipient. This makes sense when the content of the advice is considered. Almost all advice tells the manager to choose a higher wage, usually adding that a higher wage will lead to higher effort. Given the matching mechanism used in our experiments, such advice cannot be interpreted as a promise of a future reward by the employee to the manager. Instead, the receipt of advice prompts managers to experiment with higher wages. Since high wages are strongly associated with high payoffs for managers, managers learn to stick with higher wages. Both the primary treatment effect observed in Experiment 1 and the mechanism underlying this effect are consistent with the hypothesis that allowing employee messages affects gift exchange through improved learning by managers. If messages lead to higher wages by helping managers learn about the relationship between wages and effort, directly providing managers with relevant information about this relationship should also increase wages. In follow-up sessions, henceforth “Experiment 2”, we seek to confirm this prediction and to clarify the mechanism by which advice leads to higher wages. Messages are never allowed in Experiment 2. Instead, for the first ten rounds managers receive additional feedback drawn from a parallel session of Experiment 1. Specifically, at the end of each round they are shown the wage, effort, and payoffs for one randomly chosen play of the BGE game. This comes from the same round of an Experiment 1 session without messages. Showing the managers additional feedback causes wages to increase, as predicted, but the effect is significantly smaller than allowing messages. We argue that this is due to noise in the additional feedback. Put simply, advice is clearer and easier to digest than additional data about the relationship between wages and effort. The feedback is designed to contain roughly the same amount of information that employees would have when providing advice, but the impact of this information is much smaller than the effect of the straight forward advice that employees typically offer.Footnote 3
 To summarize, allowing employee messages changes the nature of gift exchange, but not because it provides a second channel for reciprocation. Instead, allowing messages affects wages through advice from employees to managers. The existing literature pays little attention to the possibility that gift exchange must be learned, but even if managers initially understand that there is a positive relationship between wages and effort, it stretches to credulity to think they would know the exact strength of that relationship. Messages, specifically advice, give managers information they would not otherwise have and make it easier for them to learn the benefits of gift exchange. Giving managers equivalent information directly is actually less effective than allowing them to learn via advice. The existing paper that most closely resembles our work is Xiao and Houser’s (2005) study of ultimatum games.Footnote 4 They study one-shot ultimatum games where responders can send a message at the same time they accept or reject an offer. Rejection rates are lower, controlling for offers, when messages are possible. Pillutla and Murnighan (1996) provide evidence that negative emotions, like anger, are the driving force behind rejections of low offers in an ultimatum game when emotion expression is impossible. Xiao and Houser speculate that responder messages lower rejection rates by allowing responders to vent these negative emotions after receiving a low offer. No longer under the influence of anger, responders then feel free to maximize their monetary payoffs by accepting low offers. It is puzzling that Xiao and Houser find an effect of allowing messages on rejection rates while we observe no effect on effort, but we conjecture this reflects an absence of strong negative emotions in BGE games. Xiao and Houser do not observe a change in offers equivalent to the increase we see in wages, but given the dynamic nature of this effect there was little chance of it arising in their one-shot experiment.Footnote 5
 Taking a normative point of view, the high wages when messages are (or have previously been) possible lead to high effort levels in response, increasing payoffs for both roles. Thus, allowing messages is efficiency enhancing. This suggests that policies designed to increase communication between managers and employees, such as regularly scheduled group meetings, may be Pareto improving. More broadly, information about the relationship of effort and wages is just one of many valuable pieces of information that employees can potentially pass on to their managers when communication is encouraged. The remainder of this paper is organized as follows. Section 2 describes the BGE and lays out the details of the experimental design for Experiment 1. Section 3 develops hypotheses about the effects of adding employee messages to the BGE. The experimental results for Experiment 1, including analysis of the content of messages, are described in Sect. 4. Section 5 gives the design and results for Experiment 2. Section 6 discusses our results.",11
16.0,4.0,Experimental Economics,15 January 2013,https://link.springer.com/article/10.1007/s10683-012-9348-2,Confusion and learning in the voluntary contributions game,December 2013,Ralph-C. Bayer,Elke Renner,Rupert Sausgruber,Unknown,Female,Male,Mix,,
16.0,4.0,Experimental Economics,11 January 2013,https://link.springer.com/article/10.1007/s10683-012-9349-1,"Heterogeneity, Coordination and the Provision of Best-Shot Public Goods",December 2013,Todd L. Cherry,Stephen J. Cotten,Stephan Kroll,Male,Male,Male,Male,"When a public good is provided through a best-shot mechanism (Hirshleifer 1983; Bliss and Nalebuff 1984; Harrison and Hirshleifer 1989), the maximum individual contribution rather than the sum of all contributions determines the level of the good. This supply technology mimics many social problems, such as finding a cure for disease, calling an ambulance for a roadside accident, or providing a tsunami warning system, and is particularly relevant for regional, international, and global public goods (Holzinger 2001; Sandler 2006). Best-shot public goods sit at the intersection of two types of provision problems: free-riding and coordination failure. Free-riding arises because in most constructions each individual is better off if someone else provides the good and coordination failures occur because groups find it difficult to coordinate on one of the multiple equilibria—i.e., identifying which member contributes the optimal amount while all other members avoid duplicating the contribution. Beginning with Schelling’s (1960) seminal work, the literature generally accepts that shared expectations can yield focal points that distinguish one equilibrium from others (e.g., Mehta et al. 1994; Camerer 2003). Such expectations rely on the existence of heterogeneity in one or more dimensions.Footnote 1 For best-shot public goods, two potential sources of heterogeneity are the ability to contribute (endowment) and the benefits received from the good (marginal private return). For example, the ability to contribute to the development of a cure for malaria varies by differences in resources, while the benefits of a cure vary by location-specific environmental attributes (e.g., malaria is concentrated in warm, wet regions). If one potential contributor has more resources than the others and therefore can achieve greater levels of provision, there may be a shared expectation that this agent will be the contributor. Alternatively, a potential contributor that stands to benefit the most from the public good might be expected to be the contributor for self-interested reasons—she has the most to lose from failed provision of the public good. The literature offers some indication that such heterogeneity can facilitate cooperation in the provision of best-shot public goods. Kroll et al. (2007) introduces endowment heterogeneity in a one-shot best-shot public good game, and though the focus of the study is not coordination, it reports that homogeneous treatments suffer from coordination failure while groups with heterogeneous endowments tend to coordinate on the optimal outcome.Footnote 2
 There is an extensive literature that examines coordination problems, including variants of the market-entry game, battle-of-the-sexes game and minimum-effort games, but little attention is directed to exploring coordination in the best-shot public good game.Footnote 3 Yet the game offers a useful framework to investigate coordination and efficiency because coordination failures can occur from people contributing too much (multiple agents making duplicate contributions that are wasted) or contributing too little (the largest contributor giving less than the optimal amount). Indeed, a group can commit both missteps simultaneously. This closely resembles the volunteer’s dilemma, in which each person of a group faces a binary choice on whether to volunteer to provide a public good for her group (Diekmann 1985). Each subject is better off if the good is provided, but would be best off if someone else provided it. Experimental studies of the volunteer’s dilemma generally impose homogeneity in endowments and returns (e.g., Bilodeau et al. 2004; Goeree et al. 2005; Hörisch and Kirchkamp 2010), but a working paper by Healy and Pate (2012) introduces heterogeneity in the cost of provision. Their results indicate that cost heterogeneity improves coordination with group members more frequently selecting an equilibrium in which the low-cost member decides to volunteer while the others do not. Herein we examine the influence of within-group heterogeneity on the coordination and provision of a best-shot public good. We consider heterogeneous endowments and returns, separately and together, and find interesting differences between the two sources of heterogeneity. We find that heterogeneity in endowments is quite effective, having a large positive effect on coordination and efficiency. Results are mixed regarding heterogeneous returns—efficiency improves, but coordination does not. This divergent outcome arises because while heterogeneity in returns does not facilitate better coordination, it does change the nature of the coordination failures.",9
16.0,4.0,Experimental Economics,24 January 2013,https://link.springer.com/article/10.1007/s10683-013-9350-3,Do core-selecting Combinatorial Clock Auctions always lead to high efficiency? An experimental analysis of spectrum auction designs,December 2013,Martin Bichler,Pasha Shabalin,Jürgen Wolf,Male,Male,Male,Male,"There has been a long discussion on appropriate auction mechanisms for the sale of spectrum rights (Porter and Smith 2006). Since 1994, the Simultaneous Multi-Round Auction (SMRA) has been used worldwide (Milgrom 2000). The SMRA design was very successful, but also led to a number of strategic problems for bidders (Cramton 2009b). The exposure problem is central and refers to the risk for a bidder to make a loss due to winning only a fraction of the bundle of items (or blocks of spectrum) he has bid on at a price which exceeds his valuation of the won subset. 
Combinatorial auctions (CAs) allow for bids on indivisible bundles avoiding the exposure problem. The design of such auctions, however, led to a number of fundamental problems, and many theoretical and experimental contributions during the past ten years (Cramton et al. 2006a, 2006b). The existing experimental literature comparing SMRAs and CAs suggests that in the presence of significant complementarities in bidders’ valuations and a setting with independent private and quasi-linear valuations, combinatorial auctions achieve higher efficiency than simultaneous auctions (Banks et al. 1989; Ledyard et al. 1997; Porter et al. 2003; Kwasnica et al. 2005; Brunner et al. 2010; Goeree and Holt 2010). Since 2008 several countries such as the UK, Ireland, the Netherlands, Denmark, Austria, Switzerland, and the US have adopted combinatorial auctions for selling spectrum rights (Cramton 2009b). While the US used an auction format called Hierarchical Package Bidding (HPB) (Goeree and Holt 2010), which accounts for the large number of regional licenses, the other countries used a Combinatorial Clock Auction (CCA) (Maldoom 2007; Cramton 2009a), a two-phase auction format with primary bid rounds (aka. clock phase) for price discovery, which is extended by a supplementary bids round (aka. supplementary phase). The CCA design used in those countries has a number of similarities to the Clock-Proxy auction, which was proposed by Ausubel et al. (2006). It was used for the sale of blocks in a single spectrum band (i.e., paired and unpaired blocks in the 2.6 GHz band) and for the sale of multiple bands in Switzerland.Footnote 1
 Although, spectrum auction design might appear specific, the application is a representative of a much broader class of multi-object markets as they can be found in logistics and industrial procurement. Spectrum auctions are very visible in public and successful designs are a likely role model for other domains as well. The main contributions of this paper are the following:  To our knowledge, this is the first lab experiment on the CCA, which we compare to the SMRA. We used an implementation of the CCA and the SMRA, which mirrors the auction rules used in the field and derive a number of properties of these auction rules. While most experimental studies in this field focus on small markets with a few blocks only, we intentionally used an experimental design which resembles real market environments. This is an important complement to other studies, as results of small combinatorial auctions do not necessarily extend to larger ones (Scheffel et al. 2012).Footnote 2
 We show that the efficiency in the CCA was not higher than that of SMRA and due to the low number of bundle bids actually significantly lower in the multiband setting. Auctioneer revenue was considerably lower than in SMRA which can be explained by the CCA second-price payment rules. However, the auctioneer revenue in CCA was also significantly lower than in CCA simulations where we had artificial bidders submit bids on all possible combinations truthfully with the same value models. We also analyzed bidder behavior in the CCA. In particular, in the multiband value model bidders select only a small fraction of all possible bundles for practical, but also for strategic reasons. While restricted bundle selection has recently been discussed in the experimental literature on other combinatorial auction designs (Scheffel et al. 2012), the paper analyzes the specific effects it can have on the efficiency of the CCA with a core-selecting payment rule. Although bid shading in core-selecting auctions is a concern in the theoretical literature, we found most bundle bids in the supplementary phase of the CCA to be at the valuation and only limited bid shading. In complex environments such as spectrum auctions there is a danger that external validity of lab experiments is not given as bidders in the field are better prepared than in the lab. To address this point to some extent, we also conducted competitions, where bidders had additional information about equilibrium strategies, known auction tactics, and two weeks of time to prepare a bidding strategy in a team of two people. While the bidder payoff in SMRAs was significantly higher than in the lab, in the CCA bidding behavior was not much different to the lab. In the next section, we revisit the existing experimental literature on spectrum auctions and combinatorial auctions. In Sect. 3, we discuss the auction formats and game-theoretical results relevant to our study. Section 4 describes the experimental design, while Sect. 5 summarizes the results of our experiments. Finally, in Sect. 6 we provide conclusions and a discussion of further research in this area.",37
16.0,4.0,Experimental Economics,26 January 2013,https://link.springer.com/article/10.1007/s10683-013-9351-2,"My five pounds are not as good as yours, so I will spend them",December 2013,Sotiris Vandoros,,,Unknown,Unknown,Unknown,Unknown,,
16.0,4.0,Experimental Economics,01 February 2013,https://link.springer.com/article/10.1007/s10683-013-9352-1,How sensitive are bargaining outcomes to changes in disagreement payoffs?,December 2013,Nejat Anbarci,Nick Feltovich,,Male,Male,Unknown,Male,"Theoretical analysis of bargaining has, broadly speaking, proceeded along two lines. One line comprises the axiomatic bargaining solution concepts, such as those of Nash (1950) and Kalai and Smorodinsky (1975). This approach does not attempt to describe the detail of the bargaining process, but rather begins by listing a few desirable properties (“axioms”) that the outcome of bargaining should satisfy. If these axioms are chosen carefully, they imply a unique outcome as the solution of any bargaining situation that satisfies a few weak conditions. The other line of research specifies the structure under which bargaining occurs (e.g., the sequence of proposals and counter-proposals), and uses the tools of non-cooperative game-theory to find solutions. Some of the earliest work along this line was that of Nash (1953), who proposed a non-cooperative game (now known as the Nash Demand Game, which we will abbreviate as NDG) in which two players simultaneously make demands, and where each player receives the payoff they demand if the demands are compatible; otherwise some default “disagreement” outcome is imposed. Axiomatic and non-cooperative game-theoretic analyses of bargaining situations can serve as alternative but complementary ways of understanding the outcome of the bargaining process. Both of these techniques provide testable implications for particular bargaining situations. As a simple example, consider the bargaining problem shown in the left panel of Fig. 1. The set of feasible agreements (bargaining set) S is equal to the convex hull of the points (0, 0), (1, 0) and (0, 1) and the disagreement outcome is (0, 0). In this case, all of S is individually rational: at least as good for both parties as the disagreement outcome. While in general, different techniques for analysing bargaining can yield different solutions, in this case they show a striking amount of agreement. The most prominent axiomatic bargaining solutions—the well-known Nash and Kalai-Smorodinsky solutions as well as others such as equal-gain and equal-sacrifice—make identical predictions in this case: agreement on the (0.5, 0.5) outcome. In addition, (0.5, 0.5) is the unique symmetric efficient Nash equilibrium in the NDG, as well as the outcome implied by Harsanyi and Selten’s (1988) notion of risk dominance, as well as their equilibrium selection criterion.  Bargaining problems and bargaining solutions (S and S′ are sets of individually rational agreements; d and d′ are disagreement outcomes) Now suppose that Player 1’s bargaining position improves, in the sense that her disagreement payoff increases from 0 to 0.5 and Player 2’s remains unchanged. Then the new individually rational portion of the bargaining set S′ is the convex hull of (0.5, 0), (1, 0) and (0.5, 0.5) (see the right panel of Fig. 1), and all of the well-known axiomatic bargaining solutions predict (0.75, 0.25) to be the outcome of this new bargaining situation.Footnote 1 Moreover, risk dominance, the Harsanyi-Selten selection criterion and (if the bargainers focus only on individually rational outcomes) the symmetric efficient Nash equilibrium also predict a shift from (0.5, 0.5) in a Nash Demand Game with the first bargaining set to (0.75, 0.25) in the second. Thus, all of the most commonly used techniques for analysing bargaining agree on how players adjust to changes in the disagreement outcome (i.e., changes in their bargaining position). In the example above, the increase of 0.5 in Player 1’s disagreement payoff, with no change to Player 2’s disagreement payoff, led to an increase of 0.25 in Player 1’s bargaining payoff, and a corresponding decrease of 0.25 in Player 2’s payoff. More generally, given any bargaining set with this isosceles right triangular shape, a unit increase in one player’s disagreement payoff is predicted to lead to an increase in that player’s bargaining payoff of exactly one-half of a unit, with an equal-sized decrease in the other player’s payoff. Whether this theoretically ubiquitous property holds in real bargaining situations is, of course, an empirical question. The goal of this paper is to examine whether and how bargaining outcomes actually are affected by changes to players’ disagreement payoffs. We accomplish this by means of a laboratory experiment, which allows us precise control over both the disagreement outcome and the total amount being bargained over (the “cake size”). For robustness, we use two bargaining games, both of which capture essential features of real-life bargaining. One is the NDG, described above. The other is an unstructured variation of the NDG, which we call the Unstructured Bargaining Game (UBG). In the UBG, the bargaining set is the same, but instead of making simultaneous demands, players are given a fixed amount of time to negotiate a mutually-agreed division of the cake. Both players can make proposals, and either player can accept any opponent proposal up until time runs out; the first accepted proposal is implemented. If no proposal is accepted, both players receive their disagreement payoffs. Also for the sake of robustness, we vary the stakes involved, with a small cake size of £5 and a large cake size of £20. Our experiment is designed with several features that allow a thorough investigation of the effects of disagreement outcomes on bargaining behaviour. First, unlike most previous studies in which bargaining power was manipulated through only a few discrete values (see Sect. 3), our experiment has disagreement payoffs drawn randomly from nearly continuous distributions with roughly two thousand possible values. As a result, most subjects do not face the same disagreement outcome twice during the experiment. Second, unlike the only previous study that did vary disagreement outcomes over multiple values (Fischer et al. 2007, discussed in some detail in Sect. 3), our experiment has the subject’s own disagreement payoff drawn independently of the opponent’s disagreement payoff, allowing us to distinguish between the effect of changes to one’s own disagreement payoff and that of changes to the opponent’s disagreement payoff. Our main finding is that while subjects do take into account their bargaining position—in the sense that increases in one’s own disagreement payoff, and decreases in the opponent’s disagreement payoff, translate into higher bargaining payoffs—they are much less sensitive to changes in their bargaining position compared to the theoretical predictions described above. Specifically, both the own- and the opponent-disagreement-payoff effects are on the order of one-quarter, compared to the theoretically-predicted values of one-half. This result is robust to which bargaining game was played, as well as to changes in the cake size and in the ordering in which subjects faced the cake sizes, and is observed in both early and late rounds. We will show in Sect. 7 that this result cannot be explained solely by subjects’ aversion to risk, and in Sect. 8 that quantal-response equilibrium (McKelvey and Palfrey 1995) does better, but is also unsatisfactory. In contrast, we will show in Sect. 9 that other-regarding preferences can account for our result: specifically, while Fehr and Schmidt’s (1999) original model of inequity aversion cannot explain the result, a slight modification can. As we discuss in Sect. 10, however, other explanations for our results may also be possible.",45
16.0,4.0,Experimental Economics,09 February 2013,https://link.springer.com/article/10.1007/s10683-013-9353-0,An examination of the effect of messages on cooperation under double-blind and single-blind payoff procedures,December 2013,Cary Deck,Maroš Servátka,Steven Tucker,,Male,Male,Mix,,
17.0,1.0,Experimental Economics,15 February 2013,https://link.springer.com/article/10.1007/s10683-013-9354-z,Mixing the carrots with the sticks: third party punishment and reward,March 2014,Nikos Nikiforakis,Helen Mitchell,,Male,Female,Unknown,Mix,,
17.0,1.0,Experimental Economics,05 March 2013,https://link.springer.com/article/10.1007/s10683-013-9355-y,Parental background and other-regarding preferences in children,March 2014,Michal Bauer,Julie Chytilová,Barbara Pertold-Gebicka,Male,Female,Female,Mix,,
17.0,1.0,Experimental Economics,16 March 2013,https://link.springer.com/article/10.1007/s10683-013-9356-x,Is it my money or not? An experiment on risk aversion and the house-money effect,March 2014,Juan Camilo Cárdenas,Nicolas De Roux,Luis Roberto Martinez,Male,Male,Male,Male,"The house-money effect, understood as people’s tendency to be more daring with easily-gotten money, is a behavioral pattern which finds support in incentivized experiments using real money by Thaler and Johnson (1990). Since experiments in economics usually start by handing out money to the subjects so that they never stand to suffer any net monetary losses, the participants’ behavior could be modified as a result of the house-money effect. This poses questions about the external validity of experiments in economics (Guala 2005, p. 231; Guala and Mittone 2005), and particular questions about the incentives used: to what extent do people behave in the experiment like they would have in a real-life situation, given that they play with easily-gotten house money? (Levitt and List 2007) The experimental literature has addressed this question in the context of altruism (Cherry et al. 2002), public goods (Clark 2002), auctions (Ackert et al. 2006) and capital expenditure (Keasey and Moon 1996). The general idea of windfall gains has been also explored in the psychology and economics literature (Arkes et al. 1994; Keeler et al. 1985). Most of these papers deal with the issues arising from having people play with their own money by having participants earn money in an initial stage and then making choices with their earnings. This paper studies the effect of house money on the risk preferences of a group of 122 undergraduate students within an age range of 16 to 28. The students were randomly assigned to a control or a treatment group and given money to participate in the experiment, which they were told involved risky choices and possibly losses. As usual, the money handed out for participating was enough to cover the potential losses. However, while the control group received this initial money just before they made their choices, the treatment group received the money three weeks in advance so that they had time to spend it before making their choices. (A back-of-the-envelope calculation suggests that on average 35 % of the cash in advance was spent.) This experimental design, inspired by Bosch-Domènech and Silvestre (2010), is as close as we can get to having them gamble with their own money. We find evidence of an indirect house money effect operating through the money that participants had with them at the time of choosing between lotteries. More specifically, we find that for the treatment group, each additional thousand Colombian pesos (COP) spent (∼USD$0.50) leads to an increase of 0.0019 in their CRRA risk aversion coefficient. We interpret this finding as evidence of a house money effect on those subjects of the treatment group who actually spent some of the cash provided to them in advance. In our preferred specification, the mean relative risk aversion coefficient equals 0.34 with a standard deviation of 0.09. Therefore, our estimated 35 % expenditure of the endowment would lead to a reduction of 0.3 standard deviations in the risk aversion coefficient. This interpretation rests on two assumptions. First, that the money participants had with them at the time of the experiment is a good proxy for endowment not spent, if compared to the same measure in the control group. Second, and more importantly, we assume that the house money effect only operates for those people who actually spent some of their endowment. We will have more to say about this assumption below. The results that we report here add to a vast literature documenting risk aversionFootnote 1 and suggest that it would be advisable to include credible controls for the house-money effect in experimental work in economics.",30
17.0,1.0,Experimental Economics,08 March 2013,https://link.springer.com/article/10.1007/s10683-013-9357-9,Measuring agents’ reaction to private and public information in games with strategic complementarities,March 2014,Camille Cornand,Frank Heinemann,,,Male,Unknown,Mix,,
17.0,1.0,Experimental Economics,20 April 2013,https://link.springer.com/article/10.1007/s10683-013-9358-8,Intermediaries in corruption: an experiment,March 2014,Mikhail Drugov,John Hamman,Danila Serra,Male,Male,Male,Male,"An intermediary is “an economic agent that purchases from suppliers for resale to buyers or that helps buyers and sellers meet and transact” (Spulber 1996). Intermediaries facilitate the exchange between buyers and sellers by getting expertise in sellers’ goods and buyers’ needs, thus reducing search and bargaining costs while building a reputation for credibility and trustworthiness. Intermediation activities are an important part of the economy. Spulber (1996) computes that in the United States intermediation activities such as retail and wholesale trade, finance and insurance contribute about 28 percent of the GDP. In illegal activities we should expect even more intermediation than in legal ones, due to the higher transaction costs generated by the need for secrecy and the lack of legal contract enforcement. Indeed, anecdotal evidence suggests that intermediaries are ubiquitous in corrupt activities. Bertrand et al. (2007) find that in India while most applicants pay bribes to get a drivers’ license, “there is no evidence of direct bribes to bureaucrats … The extralegal payments are mainly fees to “agents”, professionals who “assist” individuals in the process of obtaining their driver’s licenses. … multiple pieces of evidence suggest that agents institutionalize corruption” (p. 1641). Oldenburg (1987) reports that in the land consolidation process in India in the early 1980s intermediaries were necessary due to their “special knowledge of the procedures, access to officials, time to spend, and dirty hands” (p. 527). Fjeldstad (2003) explains the failure of the anti-corruption reform first implemented by the Tanzanian tax authority in 1996 as a result of the fact that many corrupt officers who had been fired either got employed by firms as “tax experts” or set up their own agencies, and, thus, became “facilitators” of corruption. In a similar study conducted in Uganda Fjeldstad (2006) reaches similar conclusions. In Latin America, “coyotes” or “tramitadores” are often found next to government buildings ready to “help” individuals applying for licenses, permits or documents (see Lambsdorff 2002). In several recent cases of corruption involving large firms bribing public officials in foreign countries, such as the BAE and Chrysler cases, the use of intermediaries is routinely mentioned.Footnote 1
 The economic literature on intermediaries in corruption is surprisingly small. The latest survey book on corruption by Rose-Ackerman (1999) does not cover the problem of intermediation in corrupt exchanges. The recent review of corruption research of Banerjee et al. (2012) highlights that while there exists evidence of the use of agents to intermediate bribe-taking, “the theory of how the use of agents alters the nature of corruption is yet to be developed …” (p. 29). The studies discussed above and a few theoretical investigations (Bose and Gangopadhyay 2009; Bjorvatn et al. 2005; Hasker and Okten 2008) suggest that middlemen are employed in corrupt transactions because they eliminate uncertainty with respect to whom and how much to bribe, guarantee the enforcement of the illegal “contract” and reduce the risk of detection of both briber and bribee. This is due to the intermediary’s expertise and knowledge of both the organization of the corrupt system and the people that ought to be involved in the process. The risk of detection of the client may be further lowered because, even if the intermediary is found guilty of bribery, the client may keep his or her anonymity, or, if exposed, could deny responsibility, arguing the intermediary bribed on his own initiative. In this paper, we focus on an additional channel through which intermediaries may increase corruption. We ask whether intermediaries facilitate corruption by lowering the moral or psychological cost that potential bribers and bribees might suffer when engaging in corruption. Indeed, by acting as professionals with superior information and expertise, and by charging fees—rather than bribes—in exchange for their services, intermediaries might generate the belief that the services in question are neither illegal, nor socially condemned. Such belief may be reinforced by the fact that, by going through an intermediary, the client does not interact with the bureaucrat, and therefore at no time does he or she actively engage in bribery. As observed by Oldenburg (1987), the intermediary in corrupt transactions “… lets it be known that he is willing to dirty his hands: not only is he experienced (knows the subtle hints, knows the techniques of passing money), but making use of him also allows the briber to distance himself from the transaction” (p. 527). As a consequence, intermediaries may decrease the moral or psychological costs that potential bribers may suffer from engaging in corruption. Indeed, evidence from specially designed lab experiments (Hamman et al. 2010; Bartling and Fischbacher 2012; Coffman 2011) suggests that individuals behave significantly more selfishly and they are less likely to be reprimanded when they delegate others to carry on their decisions. Besides creating psychological distance from the corrupt act, the presence of an intermediary, as suggested by Bertrand et al. (2007), may reduce moral or psychological costs of the parties involved by institutionalizing corruption, i.e. by inducing people to see bribery exchanges as ordinary business transactions. There are no theoretical or empirical studies, to the best of our knowledge, exploring whether intermediaries may increase corruption further by lowering the moral or psychological costs that potential bribers and bribees may suffer when engaging in corruption. Given the behavioral nature of our research question, it would be impossible to address it in the field. We use data generated by a specifically designed laboratory experiment that simulates corrupt transactions between “private citizens” and “public officials”. While the transaction benefits a citizen-official pair, it generates negative externalities on an “other member of society”. By conducting different versions of the game, in which we alter the degree of uncertainty and/or the presence of the intermediary, we are able to isolate the moral cost-reducing role that intermediaries may play in corruption exchanges. Our results confirm that the presence of the intermediary significantly increases corruption. While we find evidence that this increase is partly driven by the elimination of uncertainty, there is more to the role of the intermediary. In particular, our data suggest that the presence of the intermediary leads to a reduction in the moral or psychological costs of both private citizens and public officials, and thus further increases corruption. Our findings have implications with respect to possible policies concerning the legitimacy of the use of intermediation for the provision of government goods or services. Whether the services provided by intermediaries should be prohibited is an open question. Lambsdorff (2011) rightly argues that prohibiting the use of intermediation would eliminate the benefits provided by honest intermediaries, i.e., intermediaries that do not engage in corruption; moreover, it is likely that intermediaries would keep operating informally and illegally. On the other hand, our findings suggest that rendering the use of intermediaries for the provision of government services illegal would eliminate one of the channels through which corrupt intermediaries seem to operate, i.e. the reduction of the moral costs associated with corruption of potential bribers and bribees, and therefore could reduce the demand for corrupt intermediaries. Indeed, if the use of intermediaries for the provision of government services is made illegal, the reduction in the moral costs caused by increased psychological distance might be outweighed by the moral cost generated by the awareness that hiring an intermediary is also illegal. We conclude that a thorough examination of the advantages and disadvantages of prohibiting the use of intermediaries for the provision of public services is needed. The paper is organized as follows. Section 2 reviews the related experimental literature on delegation and bribery. Section 3 describes our bribery experiment; Section 4 presents our theoretical framework and predictions. Section 5 describes the parameterization and implementation of the experiment, while Section 6 presents our results. Finally, Section 7 concludes.",56
17.0,1.0,Experimental Economics,01 May 2013,https://link.springer.com/article/10.1007/s10683-013-9359-7,Valuation structure in first-price and least-revenue auctions: an experimental investigation,March 2014,Diego Aycinena,Rimvydas Baltaduonis,Lucas Rentschler,Male,Male,Male,Male,"Auctions for infrastructure concession contracts may be modeled as having both private and common components in the valuation structure. The winner of such an auction receives the revenue generated by the contract (e.g. tolls from highway concessions, energy transmission fees over a high-power grid, generation capacity payments, etc.) which has a common value (Bain and Polakovic 2005; Flyvbjerg et al. 2005). The winner will also incur the cost of fulfilling the contract (e.g. building the highway, constructing the infrastructure for power lines, building and maintaining power plants, etc.). If bidders’ costs of providing the infrastructure differ, these costs may represent a private component of the valuation structure. In auctions in which there is a common value component, it is well known that bidders are prone to the winner’s curse.Footnote 1 That is, bidders bid such that they guarantee themselves negative payoffs in expectation. This is of particular concern in auctions for infrastructure concession contracts, as bidders who go bankrupt may cause costly delays in infrastructure development. In this paper we experimentally compare the standard first-price sealed-bid auction with an alternative auction format which may reduce the prevalence of the winner’s curse, the Least-Revenue Auction (LRA).Footnote 2 We compare these two auction formats in two valuation structures with an uncertain common value. In the first, bidders face a private cost. In the second, bidders face a common cost which is common knowledge. In an LRA, bidders simultaneously make sealed-bid offers which consist of the minimum amount (from the common value of the good) the bidder is willing to accept upon winning the auction.Footnote 3 The bidder who submits the lowest amount wins the auction, and obtains that amount, provided the amount is less than the common value of the good. In the event that the winning amount is greater than the value of the good, the winning bidder only obtains this value. Thus, the winner implicitly pays the difference between the realized common-value of the good and the amount they submitted. This mechanism renders private information bidders may hold regarding the common value of the good strategically irrelevant, provided this information is not correlated with the private information of other bidders. The LRA is then strategically equivalent to a first-price procurement auction. In a purely common-value LRA equilibrium bids are not a function of the private common-value signals that the bidders observe prior to placing their bids. The game is, in effect, a game of complete information. Similarly, in auctions with private and common values, the equilibrium bid function of an LRA maps bidders’ private costs into bids, ignoring privately observed estimates of the common value. The LRA mechanism, in effect, transforms an auction with private and common values into an auction with purely private values. It is important to note that in an LRA, uncertainty regarding the common value of the good is borne by the auctioneer rather than by the bidders. An LRA represents a contract in which the price the winning bidder pays is contingent on the realized value of the good; the auctioneer guarantees the winning bidder that she will earn her bid (provided the winning bid does not exceed the common value of the good). This transfer of risk may be desirable, and provides the original motivation for LRAs: Engel et al. (1997, 2001) first proposed the Least Present Value of Revenue Auction (LPVRA), in which bidders submit the smallest present value of revenue they would require for a contract in which they build, operate and then transfer a highway to the government at the conclusion of the contract term. In an LPVRA, the duration of a contract is contingent on the stream of revenue which is generated by tolls collected on the highway. In particular, the contract lasts until the winning bidder obtains the present value (at a pre-determined discount rate) of the toll revenue that she bid. This flexible-term contract shifts most of the risk resulting from uncertain traffic patterns to the government, relative to a standard fixed-term contract. Engel et al. (1997) estimates that the value of switching to LPVR auctions is about 33 % of the value of the infrastructure investment. More generally, by eliminating ex ante uncertainty regarding payoffs conditional on winning the auction LRAs and LPVRAs reduces the possibility that the winning bidder, having failed to account for the informational content of winning the auction when formulating her bid, will subsequently go bankrupt. An additional potential benefit of using LRAs is that if bidders anticipate that the contract is open to renegotiation ex post, they may bid more aggressively in a first price auction, with no intention of adhering to the contract ex post. Uncertainty regarding the common value component of the good may provide cover for such strategic behavior, since the winning bidder can claim that they simply fell victim to the winner’s curse when demanding that the contract be renegotiated. Indeed, Guasch (2004) reports that over 50 % of concession contracts for transportation infrastructure are renegotiated. Athias and Nuñez (2008) find evidence that is consistent with bidders displaying more strategically opportunistic behavior in auctions for toll-road concessions in weaker institutional settings, presumably due to a higher probability of contract renegotiation. The LRA removes this cover, and thus may reduce reported bankruptcies in practice. We leave this interesting case to future research, and in the current paper focus on the case in which the contract is binding. It is important to note that in both the LRA and the LPVRA, the winning bidder does not have an incentive to maintain the value of the good because winning the auction guarantees the winning bidder her bid, and no more. That is, the benefits of maintaining or improving the value of the good ex post do not accrue to her. Monitoring the ex post behavior of the winning bidder, or imposing an enforceable contract, would be necessary to mitigate this problem. If neither of these are possible, LRAs and LPVRAs may not be ideal. Our work differs from that of Engel et al. (2001) in at least two important ways. First, their focus is on optimal risk-sharing contracts and not on bidding behavior or auction performance. Second, we allow for the possibility of private costs, and we analyze the common value of the good as the realization of a random variable in a single period rather than as a stream of revenue over time (with a high or low realized value in each period). However, the underlying intuition is the same. As such, the main contribution of this paper is to formally analyze and experimentally test bidding behavior and auction performance in an environment consistent with the motivation underlying LPVRAs. Although Chile has implemented LPVRAs on more than one occasion (Vassallo 2006), to the best of our knowledge this is the first formal and empirical analysis of the allocative properties of this auction format and bidding behavior within it. In this paper we test to see if an LRA can reduce the prevalence with which bidder’s guarantee themselves negative profits in expectation. We leave for subsequent experimental research the question of optimal risk sharing between the auctioneer and bidders. In addition, our paper contributes to the small but growing literature regarding auctions with private and common values (see e.g. Goeree and Offerman 2002; Boone et al. 2009). The theoretical analysis of such auctions begins with Goeree and Offerman (2003). We extend this analysis by deriving the cursed equilibrium in first price auctions. Cursed equilibrium was introduced in Eyster and Rabin (2005), and allows for the possibility that bidders do not fully take into account the link between the private information of other bidders and their when formulating their own bid. This model of bounded rationality includes Nash equilibrium and a naive bidding model as special cases. Goeree and Offerman (2002) (henceforth GO) present experimental evidence that first-price auctions with private and common values tend to be inefficient. The intuition behind this inefficiency is that subjects have to combine the information of two signals (the private value and the signal regarding the common value). If subjects were to ignore the common value signal, the auction would be fully efficient. This is precisely what the LRA offers. Ignoring the common-value signal presents a coordination problem for auction participants in a standard auction with private and common values. The LRA avoids this coordination problem by rendering common-value signals strategically irrelevant (provided signals are independent).Footnote 4
 Auctions with purely common value have been studied extensively in the experimental literature. It is typically observed that inexperienced bidders are prone to fall victim to the winner’s curse. That is, inexperienced bidders often bid above a break-even bidding threshold. This observation is robust across numerous auction mechanisms, and these results cannot be explained by risk aversion, limited liability of losses or a non-monetary utility of winning.Footnote 5 This paper provides, to the best of our knowledge, the first attempt to link analysis of the winner’s curse in auctions with only common values to auctions with both private and common value structures. Our most dramatic result is a stark decrease in the frequency with which bidders bid above a break-even bidding threshold in LRAs relative to first-price auctions. Indeed, inexperienced bidders in LRAs very rarely bid above this threshold. Since these bidders do not face any uncertainty regarding their payoff conditional on winning the auction, this is perhaps not surprising. We also find that, when the value of the good has both private and common value components, there is a significant increase in efficiency in LRAs relative to first-price auctions. This is important because, as previously mentioned, efficiency is low in first-price auctions with this valuation structure. Thus, we demonstrate that in this environment increases in efficiency and a reduction in bidding above the break-even bidding threshold can be obtained by changing the auction mechanism. These findings support the use of LRAs or LPVRAs as a way to allocate concession contracts for infrastructure. Contrary to theory, LRA generates less revenue than first-price auctions, regardless of valuation structure. This is largely due to the fact that bidders in first-price auctions tend to overbid aggressively, often to the point of guaranteeing negative profits in expectation. Correspondingly, bidders are better off in an LRA than in a first-price auction. The remainder of the paper is organized as follows. Section 2 provides the theoretical background. Section 3 describes our experimental design. Section 4 provides our results. Section 5 contains the conclusion. Electronic supplementary material contains a sample set of instructions as well as derivations of theoretical predictions.Footnote 6
",3
17.0,1.0,Experimental Economics,01 May 2013,https://link.springer.com/article/10.1007/s10683-013-9360-1,New insights into conditional cooperation and punishment from a strategy method experiment,March 2014,Stephen L. Cheung,,,Male,Unknown,Unknown,Male,"The model of voluntary contribution to a public good provides a simple metaphor for many social dilemmas in which cooperation is socially efficient, but where agents motivated by material self-interest have incentives to free-ride. In this setting, a large body of experimental evidence finds that while many people do indeed free-ride, there are others who contribute a not-inconsequential share of their resources to public goods, even in one-shot interactions.Footnote 1
 A key insight from this literature is that many people are conditional cooperators, who prefer to contribute only when others do so, and even conditional cooperators display a “selfish bias” (Fischbacher et al. 2001, hereinafter FGF).Footnote 2 As a result, there is considerable interest in institutions such as peer punishment (Fehr and Gächter 2000, 2002) that may strengthen cooperation in the face of the temptation to free-ride. Since both conditional cooperation and punishment are at odds with conventional theory, especially in one-shot settings, they have helped to stimulate a lively literature on models of social preferences.Footnote 3
 The aim of this paper is to enrich understanding of how willingness to conditionally cooperate or punish varies in response to the cooperativeness of others. Key to this is a more complete application of the “strategy method” (Selten 1967), in which each subject specifies a complete profile of choices in response to every possible combination of the choices of others. Because FGF and related studies only apply a restricted version of the strategy method based on the average contribution, they overlook important aspects of how conditional cooperation responds to the full distribution of contributions. Likewise, previous studies of punishment in public good experiments have not used the strategy method at all. The results demonstrate clearly that behaviour responds not only to the average level of contributions—as widely presumed in the pastFootnote 4—but also to the distribution of contributions that make up the average. This is the case both for conditional cooperation in a game without punishment, and for conditional punishment decisions in a game with punishment. Moreover, the observed effects are directionally consistent with the predictions of the Fehr and Schmidt (1999, hereinafter FS) model of inequality aversion. In the game without punishment, it is shown that there are two distinct sources of selfish bias in conditional cooperation. Firstly, in cases in which others contribute equally, the finding of FGF that even subjects classified as conditional cooperators fall short of matching others’ contributions is replicated. Secondly, holding the average contributions of others constant, conditional contributions decline even further as other players contribute more unequally. In the game with punishment, there is a substantial positive response of punishment to deviations of the target player below the contribution of the punisher, and a smaller negative response to deviations above the punisher. Holding the contribution of the target constant, punishment responds positively to the contribution of a third player. Finally, the strategy method also detects “antisocial” punishment of high contributors. However, contrary to the suggestion that this may be an expression of disdain toward “do-gooders”, there are remarkably few instances of antisocial punishment directed specifically at higher contributors.",24
17.0,1.0,Experimental Economics,07 May 2013,https://link.springer.com/article/10.1007/s10683-013-9361-0,Gender and competition in adolescence: task matters,March 2014,Anna Dreber,Emma von Essen,Eva Ranehill,Female,Female,Female,Female,"Women’s economic and political opportunities have long been more restricted than those of men. This situation has gradually improved in the Western world: female participation in the labor market has increased substantially during the last decades and women today are in many countries at least as likely as men to complete higher education. Yet, both occupational segregation and gender wage gaps persist. Gender differences in economic preferences provide one possible explanation for the observed gender gaps in the labor market (Croson and Gneezy 2009; Bertrand 2010). Gender differences in preferences are often studied through laboratory and field experiments, and according to this literature men are typically more competitive, more risk taking and less altruistic than men (see, e.g., Eckel and Grossman 2008a; 2008b; Croson and Gneezy 2009; Bertrand 2010; Engel 2011). Moreover, in addition to possible general gender differences in these economic preferences, women and men may also choose different areas for competition. Previous literature suggests that gender differences in competitiveness among adults, but not children, can depend on the nature of the competitive task. Most often, gender-neutral or female-oriented competitive tasks reveal no differences in preferences for competition, whereas men tend to compete more in male-oriented tasks (e.g. Günther et al. 2009; Grosse and Reiner 2010; Shurchkov 2012). This has also been shown in a field experiment in the labor market: Flory et al. (2010) find that women are less likely than men to choose to apply to jobs with highly competitive compensation regimes, but only if the domain of the job has a male stereotype. An open question is how gender matters for competitive preferences in different tasks among adolescents. Competitiveness in certain tasks has been shown to predict educational choices, raising the possibility that gender differences in competition among adolescents might have long-lasting effects. For example, students that self-select into laboratory competitions using mathematical tasks are more willing to take a high school entrance exam than students less inclined to compete (Zhang 2010) and are more likely to choose more math oriented and prestigious university majors (Buser et al. 2012). Similarly, Örs et al. (2008) find that women perform less well compared to men on the very competitive entry exam to one of France’s higher ranked schools, while outperforming men in two less competitive settings. It further seems to be more male associated tasks that are important for labor market outcomes. Favara (2012) finds that, independent of ability, stereotypically male choices lead to higher earnings and that gender stereotypic educational choices are made as early as the age of 14. In a similar vein, mathematical test scores, as opposed to for example verbal test scores, have been found to be a good predictor of future income (Niederle and Vesterlund 2010). If boys and girls choose certain competitive tasks and avoid others differently during adolescence, this may thus have long-term labor market effects, impacting vertical and horizontal occupational segregation by gender, as well as the gender wage gap.Footnote 1 Therefore, we believe it is important to understand the extent to which gender differences in preferences for competition are present among adolescents, and how these differ depending on the task. In this paper we study gender differences in preferences for competition among Swedish adolescents aged 16–18 years. We measure competitiveness both by relative performance in competitive vs. non-competitive settings and by self-selection into a competitive payment scheme. Previous studies mainly focus on competitiveness among adults or younger children, thus we contribute to filling the gap between these two age groups. Importantly, we study competitiveness in two tasks with varying gender associations; a “male” oriented task based on mathematical ability and a “female” oriented one based on verbal ability (e.g. Cvencek et al. 2011; Nosek and Smyth 2011). This has previously not been explored among adolescents. Given previous studies on competitiveness in other age groups, we expect adolescent boys to be more competitive in the mathematical task compared to adolescent girls, whereas we do not have a clear hypothesis for the verbal task in this age group. We find that gender differences in competitiveness exist already among 16–18 year olds, but that it depends on the task. Whereas we find no gender difference in performance change under a competitive setting in comparison to a non-competitive setting, in either a mathematical or a verbal task, female participants are significantly less likely than male participants to self-select into a competitive setting in a mathematical task, but not in a verbal task. The difference between the genders is large and economically relevant. More than twice as many boys as girls choose to enter the competition. This is not true for a verbal task, where adolescent boys and girls are equally competitive in terms of self-selection. The gender difference in competitiveness between the two tasks is driven by a significantly lower number of girls choosing to compete in the math task than in the verbal task. Among boys the number of competitors is stable across the tasks. However, the gender gap in choosing to compete in the mathematical task diminishes and is no longer significant when we control for actual performance and relative performance beliefs.Footnote 2 Our results suggests that if competitiveness matter for labor market choices, then policies addressing gender gaps in this market should account for possible self-selection into and away from competitions already during adolescence, considering that this self-selection varies by gender and competitive domain. Addressing the performance beliefs of females seems to be of particular interest. In our sample, girls are more under-confident in math than boys, thus improving girls’ confidence through for example performance feedback could potentially eliminate the gender gap in competitiveness in math. We also study gender differences in risk preferences and altruism, measured through incentivized behavioral tasks, since these preferences also exhibit gender differences and have been proposed to explain part of the gender gap in labor market outcomes (see, e.g., Bertrand 2010 for further discussion). We hypothesize that if anything adolescent boys in our sample will be more risk taking and less altruistic compared to adolescent girls. Our results confirm this: adolescent girls in our sample are indeed less risk taking and more altruistic than boys in a dictator game in which the recipient is a charity. The outline of our paper is the following. We discuss related previous literature in Sect. 2, present the experimental setup in Sect. 3, and move on to our results in Sect. 4. We finish by a discussion in Sect. 5.",126
17.0,2.0,Experimental Economics,14 May 2013,https://link.springer.com/article/10.1007/s10683-013-9362-z,For those about to talk we salute you: an experimental study of credible deviations and ACDC,June 2014,Adrian de Groot Ruiz,Theo Offerman,Sander Onderstal,Male,Male,Male,Male,"Crawford and Sobel (1982) introduced cheap talk games with asymmetric information, which have found many applications.Footnote 1 Equilibrium selection is important in these games, as they tend to have multiple equilibria with very different levels of information transmission. In this paper, we test the Average Credible Deviation Criterion (ACDC), introduced in De Groot Ruiz et al. (2012a). Many other equilibrium refinements and solution concepts have been proposed for cheap talk gamesFootnote 2 and some of them have found empirical support in the lab. However, these concepts are not able to predict experimental behavior across a wider range of cheap talk games. In contrast, ACDC selects equilibria under general conditions. Its predictions are meaningful in previously analyzed settings and can organize behavior well in existing experiments meant to study other concepts. ACDC builds on binary stability criteria based on credible deviations, in particular neologism proofness (Farrell 1993) and announcement proofness (Matthews et al. 1991). These concepts are based on the observation that out-of-equilibrium messages can have a literal meaning and propose conditions under which messages are credible.Footnote 3 Equilibria that admit such credible deviations are considered unstable. Unfortunately, in many games these criteria do not select a unique equilibrium. The idea behind ACDC is that the credible deviation approach is sound, but that the insistence on a binary distinction between stable and unstable equilibria is problematic. In particular, ACDC assumes that credible deviations matter for the stability of equilibria but that they matter in a gradual manner. On this basis, ACDC can select the most plausible equilibria (ACDC equilibria), even in games where no equilibrium is entirely stable. Moreover, it gives a relative measure of an equilibrium’s instability, so that our approach also predicts for which games the ACDC equilibrium is expected to perform well. In this study, we test the predictions of ACDC in a class of continuous external veto threat games. These games allow for a clean manipulation of the size and frequency of credible deviations. Furthermore, they can have a large equilibrium set, which previous concepts cannot refine. For these games, we show how the ACDC concept is grounded on a simple ‘neologism dynamic’. The neologism dynamic is a best response dynamic with the additional feature that Senders can send credible neologisms, which are believed by Receivers. Two salient initial conditions for dynamic models are the babbling strategy and the naive strategy. Senders who babble refrain from transmitting information by randomizing their report. Naive Senders simply report their type. In contrast to a best response dynamic and a level-k analysis, the neologism dynamic is not sensitive to which of these initial conditions applies. In particular, the neologism dynamic neatly converges to the ACDC equilibrium when its instability measure is low whereas the convergence process is more messy when the instability measure is higher. Our experimental results are the following. First, neologism proofness and announcement proofness perform well in a setting where their prediction is unique. Second, the ACDC equilibrium performs best if all equilibria are unstable according to neologism proofness and announcement proofness, even if there is a large set of equilibria. Third, the ACDC equilibrium in similar games performs worse when its stability according to ACDC decreases. Our findings provide evidence that the neologism dynamic traces experimental behavior quite well. As a consequence, the ACDC concept does a good job predicting which equilibrium attracts behavior best. Turning to the experimental literature, we see that relatively little experimental work exists on equilibrium selection in cheap talk games. Blume et al. (2001) test the predictions of Blume et al. (1993) Partial Common Interest criterion (PCI). This criterion favors the finest partition in which types in each partition element unambiguously prefer to be identified as members of that element (see also Sect. 2). In their series of discrete games, PCI performs better than neologism proofness. In De Groot Ruiz et al. (2012a), we argue that ACDC can explain Blume et al.’s experimental data at least as well as PCI for all games that they tested. ACDC performs better than PCI in continuous games like the Crawford-Sobel game and our veto-threat game in which PCI fails to predict any communication while subjects are actually quite successful in communicating. Experimental work on the Crawford-Sobel game provides evidence that the most informative equilibrium performs best (Dickhaut et al. 1995; Cai and Wang 2006) and Wang et al. (2010).Footnote 4 This is predicted by ACDC (see De Groot Ruiz et al. (2012a)) as well as by No Incentive To Separate (NITS) and influentiality.Footnote 5 Furthermore, the evidence suggests that as the preferences of Sender and Receiver become better aligned, the most informative equilibrium performs better, which is predicted by ACDC but not by NITS or influentiality. The present study is the first systematic experimental test of whether and to what extent credible deviations matter for the stability of cheap talk equilibria. In addition, it presents a rigorous test of ACDC in new experiments.Footnote 6
 Next to equilibrium selection, previous experimental work on cheap talk games primarily deals with two related themes. The first theme concerns the question how much information is transmitted between Sender and Receiver and how this compares to what is expected in the equilibrium in which the most information is communicated. So far the results are mixed. Agranov and Schotter (2012) observe the ‘right’ amount of information transmission in a coordination game whereas Cai and Wang (2006) find that subjects overcommunicate compared to the most informative equilibrium. In our experiment, we find some evidence that Senders’ messages are more informative than would be expected in the ACDC equilibrium benchmark. In this sense our results are in between the findings of Agranov and Schotter and those of Cai and Wang. The other theme in the experimental literature is the extent to which Senders’ messages are driven by a preference to tell the truth or to avoid lies. Sánchez-Pagés and Vorsatz (2007; 2009) show that some subjects are willing to incur costs to punish liars and that these also tend to be the subjects who refrain from lying themselves. Such lying aversion is consistent with the overcommunication result of Cai and Wang (2006). On the other hand, in a leadership game, Agranov and Schotter (2011) find that leaders exhibit a great deal of strategic sophistication and lie easily. In our bargaining game, we framed the Sender’s message as a ‘suggestion for how the Receiver should behave’ instead of as a message about ‘the true state’. In agreement with this framing, our evidence suggests that Senders used the opportunity to communicate in a strategic way. Still, in two of our three treatments subjects deviated in the direction of overcommunication (but not significantly so). Our paper has the following structure. Section 2 presents a simple illustration that explains the main existing equilibrium refinements. In Sect. 3, we discuss the theory we require to derive the hypotheses we wish to test in our experiment. We present the experimental games we study, introduce ACDC and discuss the issue of equilibrium selection. In Sect. 4, we provide the experimental design. In Sect. 5, we present the experimental results. In Sect. 6, we look at the dynamic aspects of our data and discuss the neologism dynamic. In Sect. 7, we present an additional treatment to further test ACDC. Section 8 concludes. Proofs are relegated to online Appendix A.",4
17.0,2.0,Experimental Economics,11 May 2013,https://link.springer.com/article/10.1007/s10683-013-9363-y,Effect of an audience in public goods provision,June 2014,Emel Filiz-Ozbay,Erkut Y. Ozbay,,Female,Male,Unknown,Mix,,
17.0,2.0,Experimental Economics,11 May 2013,https://link.springer.com/article/10.1007/s10683-013-9364-x,"Group status, minorities and trust",June 2014,Kei Tsutsui,Daniel John Zizzo,,,Male,Unknown,Mix,,
17.0,2.0,Experimental Economics,21 May 2013,https://link.springer.com/article/10.1007/s10683-013-9365-9,An experimental study of bidding in contests of incomplete information,June 2014,Philip Brookins,Dmitry Ryvkin,,Male,Male,Unknown,Male,"In contests, agents spend resources to increase the probability of obtaining a valuable prize. Contest models have been applied to describe a variety of economic situations, from rent seeking (Tullock 1980; for a recent review see, e.g., Congleton et al. 2008) to warfare conflicts (e.g., Amegashie and Kutsoati 2007; Chang et al. 2007), R&D competition (e.g., Dasgupta and Stiglitz 1980; Taylor 1995), sports (e.g., Szymanski 2003; Kräkel 2007; Berentsen et al. 2008), and intra-firm promotion tournaments (e.g., Lazear 1999). Empirical investigation of contest phenomena with field data is challenging,Footnote 1 mainly because only outcomes but not the details of the incentive structure, such as players’ abilities and prize valuations, are observable.Footnote 2 Laboratory experiments allow the researcher to have tighter control over incentives and thus serve as a natural alternative research methodology for theory testing. The existing experimental literature on contests focuses mainly on the case of complete information about players’ abilities (e.g., costs of effort or prize valuations).Footnote 3 In many applications, however, the assumption of complete information about rivals’ abilities is not reasonable. For example, in R&D competition it is unlikely that firms perfectly know their rivals’ cost structure. Similarly, lobbyists may have unobserved differences in their lobbying abilities. While the theoretical literature on contests of complete information is well-established (for the most general results on one-shot simultaneous-move contests see, e.g., Malueg and Yates 2006; Cornes and Hartley 2012), there has been an increased interest recently in the theory of contests of incomplete information.Footnote 4 Fey (2008) and Ryvkin (2010) analyzed theoretically bidding in symmetric contests of incomplete information using a formulation similar to independent private value auctions (Klemperer 2004). Specifically, Fey (2008) established the existence of a smooth symmetric equilibrium bidding function in the Tullock (1980) contest model of two players with private uniformly distributed marginal costs of effort. Ryvkin (2010) showed the existence also holds for an arbitrary number of players, arbitrary distributions of costs and more general contest success functions. In this paper, we study experimentally bidding behavior in simultaneous-move contests of incomplete information. To the best of our knowledge, such contests have not yet been explored experimentally, and, unlike in the case of complete information, theoretical predictions for bidding in such contests have not previously been tested. Specifically, we explore bidding in symmetric contests with private marginal costs of effort and compare it to bidding in two types of contests of complete information: symmetric contests where marginal costs of effort are common knowledge and the same for all players, and asymmetric contests where marginal costs of effort are common knowledge but may be heterogeneous. Using a 2×3 between-subjects design, we compare bidding across the three information/symmetry conditions and two group sizes—groups of two and four players. We compare the observed bidding behavior under incomplete information to the theoretical predictions of Fey (2008) and Ryvkin (2010). Additionally, we test the comparative statics results of Ryvkin (2010) who showed that there is a qualitative difference between contests of two and more than two players in relation to how bidding depends on the information condition. Our findings fill in the gap in the literature by providing an experimental test of a theory of bidding in contests of incomplete information. Similar to most of the prior experimental studies on contests, we find significant overbidding in all of our treatments. The comparative statics across the information conditions, however, are partially in agreement with the theoretical predictions. Importantly, the data capture subtle qualitative differences between contests of two and more than two players in the comparative statics between the information/symmetry conditions. At the same time, our results show that bidding in contests is in many ways robust to variations in information conditions. The rest of the paper is organized as follows. Section 2 summarizes the model and theoretical predictions relevant for our study. Section 3 describes the experimental design and procedures. Section 4 presents the results, and Sect. 5 concludes.",36
17.0,2.0,Experimental Economics,07 June 2013,https://link.springer.com/article/10.1007/s10683-013-9366-8,Experimenting and learning with localized direct communication,June 2014,Vincent Mak,Rami Zwick,,Male,Male,Unknown,Male,"In this study, we report an experiment in which subjects may learn from each other. Our setup involves a “queue” of players who are identically informed ex ante and who make decisions in sequence over two lotteries. Whichever lottery is played, either one of two possible payoffs may result. One lottery (the “known lottery”) yields the higher payoff with a fixed, commonly known probability. The other lottery (the “uncertain lottery”) yields the higher payoff with a probability that is not known for sure by the players, who only hold a common prior for that probability at the outset. Every player except the first in the queue observes (only) his immediate predecessor’s (i.e. the player in front) choice and payoff before making his own decision, so that the setup is characterized by localized direct communication. If the predecessor has chosen the known lottery, the realized payoff will not be informative to the decision maker; however, the choice of the predecessor by itself might serve as an informative signal to allow the player to infer what could have happened further up the queue. If the predecessor has chosen the uncertain lottery, both choice and payoff of the predecessor are potentially informative signals to the decision maker. Thus the setup is also characterized by path dependence of signal generation. Our experiment thus captures some features of the processes by which social agents learn from each others’ past behavior and experience when choosing between risky alternatives. For example, after a new product is launched, some consumers might purchase the product relatively early and then relate their own choice and consumption experience (effectively a “payoff”) to their social contacts. This can occur even when all consumers receive similar prior information about the product through the mass media, or when every consumer only communicates about his purchase with a few friends.Footnote 1 As in our experimental setup, it typically exhibits two characteristics: (1) Localized direct communication: every decision maker only directly observes the choices and payoffs of an (often very small) subset of the population; (2) Path dependence of signal generation: the decision maker’s social contacts learn from their own social contacts, who in turn learn from other social contacts; consequently, the information or “signals” received by a decision maker potentially depend on the whole history of choices and payoffs among the population right up to the time of observation. A major reference point of our framework is the social learning literature, which studies how social agents might learn from each other through observing each others’ actions. The classic social learning paradigm (Banerjee 1992; Bikhchandani et al. 1992) describes a process that involves the following: (1) agents make sequential, once-in-a-lifetime decisions, (2) all agents’ choices but not payoffs are publicly observed, and, in addition, (3) every agent holds an exogenously generated private signal. A main outcome of the classic paradigm is that rational agents may end up choosing the same, inefficient action regardless of their private signals. Recent developments in social learning (e.g. Çelen and Kariv 2004a, 2005; Choi et al. 2008; Smith and Sørensen 2008; Acemoglu et al. 2009, 2011; Callander and Hörner 2009; Guarino et al. 2011; Guarino and Jehiel 2013, and Larson 2011) typically keep the assumptions of exogenous private signal and observation of only choices, but limit how much each agent may observe about others’ actions; these studies focus on how agents’ actions might or might not converge when observation of others’ actions is incomplete. In the framework studied here, communication is highly localized but involves both choice and payoff, and there is no exogenous private signal. The payoff signal in our framework, being the realization of a random variable which only one agent may directly observe before decision making, can be compared to the private signal in social learning models; however, the generation of private signals in social learning is independent of the choices of (and signals received by) the agents, while payoff signals in our framework are path dependent.Footnote 2 This implies that our framework presents a different decision context compared with that in previous social learning experiments.Footnote 3 As we shall report, our model and assumptions predict a convergence of choice among rational agents that is analogous to many social learning outcomes, but the behavioral regularities in our experiment—in which subjects’ choices deviate far from theoretical predictions—reveal characteristics that have not been featured in comparable previous studies. Localized direct communication and path dependence of signal generation mean that a subject in our setup has at most two “bits” of information (in addition to the prior) at the time of decision making: the choice and the payoff of his predecessor. Consequently, our research questions are: how would subjects’ decisions depend on the various “bits” of information in the above context? Does decision making follow equilibrium predictions? If not, do subjects exhibit base rate fallacy/conservatism bias by overweighting/underweighting informative payoff signals over the predecessor’s choices (cf. Goeree et al. 2007; Camerer 1995), exhibit social conformity (cf. Hung and Plott 2001 and Goeree and Yariv 2007) or rebelliousness (the opposite of conformity)? Could there be other behavioral regularities that are not often observed in classic social learning experiments? As we shall report, we have indeed found one such behavioral regularity, namely a consistent preference among subjects for experimentation (preferring the lottery with potentially more information spillover value). In the remainder of this paper, we first describe the specific design of our experiment (Sect. 2) and then present an equilibrium analysis based on common knowledge of Bayesian rationality (Sect. 3). We find that decisions in equilibrium would be identical from the first or second player onwards in all experimental conditions. However, as we report in Sect. 4.1, complete adherence to equilibrium play is only occasionally observed in our experiment in short (four-player) queues and never observed in long (12-player) queues. Because our experimental data exhibit large deviations from equilibrium, we need to incorporate the possibility of erroneous moves when analyzing our data. Thus we adopt the approach of quantal response equilibrium (QRE) as we attempt to answer our research questions (see Kübler and Weizsäcker 2004 and Goeree et al. 2007 for examples of applying QRE in experiments on classic social learning). Our estimations following this approach (Sect. 4.2) suggests consistent preference for experimentation across conditions as well as some evidence of conservatism bias, social conformity, and rebelliousness. Further analysis suggests that, in conditions in which experimentation is the a priori preferable action and also informative to others, the preference for experimentation at the front of long queues is higher than at the front of short queues and higher than at the end of long queues. This offers some support for our surmise that subjects’ preference for experimentation is due, in part, to an attempt to influence others behind them in the queue. We conclude our findings in Sect. 5.",
17.0,2.0,Experimental Economics,01 June 2013,https://link.springer.com/article/10.1007/s10683-013-9367-7,The evolution of sanctioning institutions: an experimental approach to the social contract,June 2014,Boyu Zhang,Cong Li,Karl Sigmund,Unknown,,Male,Mix,,
17.0,2.0,Experimental Economics,16 June 2013,https://link.springer.com/article/10.1007/s10683-013-9368-6,"From the lab to the field: envelopes, dictators and manners",June 2014,Jan Stoop,,,Male,Unknown,Unknown,Male,"One of the most influential experiments in economics is the dictator game (Kahneman et al. 1986; Forsythe et al. 1994; Engel 2011). A ‘dictator’ is endowed with an amount of money and is matched with an anonymous recipient. The dictator has to determine how much money to give to the recipient. Behavior in this game is usually explained by altruism or a willingness to conform to social norms (the latter is also referred to as ‘manners’ Camerer and Thaler, 1995). As a result, theorists have altered the standard model. Motivations such as altruism, fairness, inequity aversion, and reciprocity have been incorporated into new models.Footnote 1
 Most evidence regarding behavior in the dictator game has so far come from laboratory experiments.Footnote 2 Critics have argued that laboratory experiments on pro-social preferences produce biased outcomes, because of scrutiny or obtrusiveness by the experimenter.Footnote 3 Some studies have indeed shown that pro-social behavior decreases when subjects are unaware of the presence of an experimenter (List, 2006a; Benz and Meier, 2008, see Bandiera et al., 2005 for a non-experimental study on monitoring). Little is known about whether experimenter scrutiny affects behavior in the dictator game.Footnote 4 In a study similar to this one, Franzen and Pointner (2012) look at the effects of scrutiny for student subjects. They conduct a dictator game in a lab with students and then send ‘misdirected’ envelopes with cash to the same participants. The authors find a positive correlation between dictator gifts and the likelihood that an envelope is returned. Winking and Mizer (2013) conduct a field experiment where they give money to random bystanders, who are unaware that they are taking part in an experiment. Almost none of the subjects choose to share the money they have received with another random bystander. In contrast, when subjects are informed up front that they are taking part in an experiment, sharing rates are much greater. This paper reports results of a dictator game in a natural field experiment. A random sample of subjects in a Dutch city receives a transparent envelope with cash due to a supposed misdelivery. They are thus placed in the role of dictator, because they can decide to return part or all of the cash to the intended recipient. In this experiment, subjects are unaware of the experimenter. More experiments are conducted to identify possible differences between the natural field experiment and behavior in the laboratory. These experiments are conducted with student subjects in a laboratory, and with subjects from the same Dutch city in a laboratory or at their home. Roughly half of the subjects in the natural field experiment return the envelope. The other experiments show similar results. This paper shows that the predictive power of laboratory findings is supported in some settings.",30
17.0,2.0,Experimental Economics,27 June 2013,https://link.springer.com/article/10.1007/s10683-013-9369-5,Price efficiency and trading behavior in limit order markets with competing insiders,June 2014,Thomas Stöckl,,,Male,Unknown,Unknown,Male,"Limit order markets (LOM) are the major trading protocol on financial markets nowadays.Footnote 1 Despite the common application of this trading mechanism, little is known about the process of information aggregation into prices. Two major problems complicate the use of theoretical and empirical methods. Theoretical studies have to deal with extremely large action spaces that originate from the possibility to trade in continuous time and the freedom to choose between limit orders (LO) and market orders (MO).Footnote 2 Empirical studies suffer from the availability of data that reliably identifies persons trading in the asset while in possession of new and relevant information. This problem is mainly driven by legal prosecution of traders holding that relevant information, commonly referred to as (corporate) insiders. In this study we analyze laboratory LOMs that differ in the realizations of two treatment variables. We manipulate (i) the number of insiders in a market and (ii) we vary the subset of traders who receives information on the number of insiders present. With these treatment variations we elaborate on three research questions (RQ).  How does competition among insiders affect price efficiency in limit order markets?  So far, no study systematically investigates this RQ. Consequently, predictions on competition effects can only be deduced from studies loosely related to LOMs. While these studies suggest a positive impact of competition on price efficiency, little can be said about the development of price efficiency over time. Kyle (1985), Holden and Subrahmanyam (1992), and Grossman and Stiglitz (1980) provide some insights but must be interpreted cautiously as these models implement pricing mechanisms other than LOMs. The same constraints apply to experimental studies as no study specifically focuses on competition issues (Plott and Sunder 1982; Friedman et al. 1984; Bloomfield et al. 2005; Huber et al. 2011). Schnitzlein (2002) is an exemption but while he focuses on insider competition, his setup deviates in serval important aspects from LOMs. RQ 2 elaborates on a specific aspect of real world markets that cannot be addressed in theoretical models. These models assume that traders are informed about the underlying structure of the economy. Concealing information on the presence of insiders undermines this assumption causing models to break down. However, real markets are characterized by high uncertainty about the presence of insiders (potentially) limiting the predictive power of theoretical results. By varying the subset of traders who receives information about the number of insiders present we study potential consequences of dropping the assumption of common knowledge about the underlying structure of the economy in LOMs.  How does the subset of traders who receives information about the number of insiders present affect price efficiency in limit order markets?  So far, the literature did not agree on likely consequences. In his market maker experiments, Schnitzlein (2002) is the first who deliberately challenges the common knowledge assumption and finds that price efficiency is significantly lower when the number of insiders must be inferred. However, Camerer and Weigelt (1991), Meulbroek (1992), and Bruguier et al. (2010) challenge the result and argue that human traders are able to infer the presence of insiders from the trading process. Still, we know little about the robustness of these results and how manipulations in the degree of competition affect them. In RQ 3 we elaborate on a specific feature of LOMs: the freedom of choice between limit and market orders to make transactions. The insiders’ choice is of particular interest as it determines the way in which information is reflected in prices.  Which order types do insiders choose to make transactions?  While there is ample evidence that insiders show abnormally high trading activity (Easley and O’Hara 1987; Meulbroek 1992) the literature does not provide clear results on the insiders’ preferred channel and no study explores the insiders’ behavior conditional on competition. Several studies suggest the use of both, LOs and MOs, by insiders (Chakravarty and Holden 1995; Harris 1998; Kaniel and Liu 2006; Bloomfield et al. 2005; Goettler et al. 2009). Barner et al. (2005) emphasize that insiders are the first to enter the market with early contracts initiated by limit orders. To evaluate RQ 1 to 3 we conduct laboratory LOMs. Each market is populated by either 0, 1, 2 or 4 insiders, who learn the asset’s value, and 6 uninformed traders, who do not receive that piece of information. Furthermore, we define three information sets that determine whether none of the traders, only insiders, or all traders learn the number of insiders present in the market. We observe that price efficiency (i) is the higher the higher the number of insiders in the market but (ii) is unaffected by changes in the subset of traders who know about the number of insiders present. (iii) Independent of the number of insiders, price efficiency increases gradually over time. (iv) The insiders’ information is reflected in prices via limit (market) orders if the asset’s value is inside (outside) the bid-ask spread. (v) In situations where limit and market orders yield positive profits, insiders clearly prefer market orders, indicating a strong desire for immediate transactions.",10
17.0,2.0,Experimental Economics,07 August 2013,https://link.springer.com/article/10.1007/s10683-013-9370-z,Elicitation effects in a multi-stage bargaining experiment,June 2014,Swee-Hoon Chuah,Robert Hoffmann,Jeremy Larner,Unknown,Male,Male,Male,"Bargaining situations, where parties make decisions in order to divide a surplus, have a central place in economics. Theoretical research has generated a myriad of models including multi-stage versions of the chicken game (Bornstein et al. 2004), diminishing-pie approaches (Rubinstein 1982), the dollar auction (Shubik 1971) and the war of attrition (Maynard Smith 1974). Studies using these models as experimental platforms are important in identifying how different parameters of the situation and subject characteristics influence the likelihood of bargaining success or failure in practice (see Roth 1995; Camerer 2003, Chap. 4). An issue with experimental work generally concerns two alternative ways of eliciting subject decisions (e.g. Roth 1995; Brandts and Charness 2000). In direct-response or ‘hot’ elicitation, subjects make decisions in real time, i.e. responding when required to any actual decisions of co-subjects similar to decision making in real contexts. In strategy-method or ‘cold’ elicitation, subjects specify an overall strategy ex ante, i.e. record specific responses at every possible decision node which are subsequently used as needed to determine actual outcomes. The advantage of the strategy method is that it generates more data but it is unclear whether there may be elicitation effects, i.e. differences in subject responses compared with direct-response elicitation. They could take two forms (e.g. Brandts and Charness 2011, p. 392): first, measured subject behaviour under otherwise the same experimental conditions could differ depending on elicitation method. Second, an inference about a particular treatment effect or predictive variable (such as demographics) in an experiment may be sensitive to elicitation method. We call these two types first and second-order elicitation effects respectively. They constitute an important issue relating to the validity of experimental data, which warrants further examination (Brandts and Charness 2011, p. 387). The present study is motivated by the thought that bargaining experiments, especially those involving multiple stages, may be particularly prone to elicitation effects. Because instrumental rationality yields the same decisions however elicited, elicitation effects may be rooted in specific aspects of human cognition outside rational choice. Multi-stage bargaining may be associated with departures from rational decision making for several reasons. First, a number of experimental economists have argued that compared with strategy-method choice, direct-response decisions are more strongly associated with affect or emotional responses (Roth 1995, p. 323; Brandts and Charness 2000). Distributive decision making processes such as negotiations and bargaining are particularly associated with risk (de Heus et al. 2010), emotion (Barry et al. 2004) and cognitive biases (Thompson et al. 2004). These effects are particularly pronounced in escalating bargaining processes (see Pruitt and Kim 2003, pp. 88–89), i.e. where players incrementally and irreversibly raise the risk to get others to accede. Second, in contrast to direct responses, the strategy method involves deliberation of all possible decision nodes rather than those actually reached. The higher resulting cognitive demands may entail greater potential for subject error (quantal responses, e.g. McKelvey and Palfrey 1998).Footnote 1 As a result the question arises whether experimentalists are on safe ground availing themselves of the advantages of cold elicitation or whether the method is blind to the effects of important psychological factors which should be examined as an integral part of bargaining processes. Our paper is intended as a step towards a better understanding of elicitation effects in bargaining experiments. We report an experiment with a multi-stage escalating bargaining game conducted under hot and cold conditions to test whether any type of elicitation effect exists. Section 2 outlines the task, experimental design and implementation. Section 3 contains the results. Section 4 discusses our findings.",5
17.0,3.0,Experimental Economics,09 August 2013,https://link.springer.com/article/10.1007/s10683-013-9372-x,Corporate Social Responsibility in the work place,September 2014,Hannes Koppel,Tobias Regner,,Male,Male,Unknown,Male,"Corporate social responsibility (CSR) has been a topic of hot debate in economics, not least because of a provocative article in New York Times—Friedman (1970)—that stated “the social responsibility of business is to increase its profits.” In contrast to Friedman’s view, it is often argued that firms’ responsibility in society goes beyond making profits and that it also includes taking environmental or social aspects into account.Footnote 1 At first glance, these two points of view seem irreconcilable. However, if market reactions to a firm’s stance on CSR are taken into account, it may not necessarily be a contradiction to maximize profits and assume ‘social responsibility’ at the same time. At least this is what recent research on CSR suggests. Costly CSR activities may actually be beneficial to firms as they can have a positive effect on the decisions of socially-/environmentally-minded individuals consuming products of the firm (see, Sen and Bhattacharya 2001, among others), investing in it (cf. Graff Zivin and Small 2005), or working for it (see, e.g., Turban and Greening 1997; Greening and Turban 2000). In labor markets—the focus of our paper—the argument in favor of CSR is that firms’ CSR activities can attract more motivated workers which in turn may lead to increased profits (see, e.g., Brekke and Nyborg 2008). However, little is known about the driving factors at the individual level. Do workers react to CSR activities of a firm, possibly by increasing their effort? This is the concern of our paper. A common finding in recent behavioral principal agent theory (see Charness and Kuhn 2011, for an overview), analyzing the employer-employee relationship, is that alternatives to monetary incentives can also motivate agents. Firms’ investment in CSR may be regarded as such an alternative motivation, especially if workers like the particular CSR activity, i.e., mission, of the firm. Besley and Ghatak (2005) stress the importance of such matching mission preferences in a principal agent relationship and they theoretically show that missions can economize on the need for high monetary incentives if workers subscribe to the firm’s mission. Hence, the aim of this paper is to shed more light on workers’ individual decision making in response to firms’ investment into CSR and possible matching mission preferences. For this purpose we set up an experimental design based on the gift exchange game (a workhorse to analyze labor market settings, see Fehr et al. 1993, 1998, 2007, among others), where firms set wages and then workers select their effort. CSR is introduced into this experimental labor market as a certain share of profits given to a third party, e.g., a “good cause.” In the experiment, this CSR activity is implemented by a donation to charity.Footnote 2 In our main treatment (CSR4) firms can choose a share of profits to be donated to charity (none, 10 %, 20 % and 30 %), and workers are asked about their respective effort choices. This allows us to analyze the effect of introducing CSR on firms’ decisions as well as the response of workers to the extent of firms’ CSR activity. In order to test the importance of matching mission preferences for the motivation of agents, we elicit participants’ preferences for the five charitable organizations that are offered. If the worker’s effort translates into a donation to a cause perceived as good (matching the mission preferences of the worker), this worker might be influenced in a positive way and chooses a higher effort level. In addition to the mere extent of CSR activity, we can thus study the role of matching mission/charity/cause preferences in workers’ decision making. In two further treatments we control for the effect of the mere presence of a CSR investment option (treatment GEG, a standard gift exchange game), and whether CSR is regarded as continuous or categorial (treatment CSR2 in which only CSR levels of 0 or 10 % are possible). Our results confirm a positive effect of CSR activity on worker motivation. The effect depends not only on the introduction of CSR but also on its extent. Moreover, matching mission preferences matter. A shared charity preference of firm and worker results in higher effort, independently of the extent of the CSR investment. Previous studies of CSR activities in the employer-employee context have been either theoretical or empirical (based on secondary data or survey responses). Firms’ ability to use CSR investments as a screening device to attract more motivated workers has been theoretically analyzed by Brekke and Nyborg (2008). They show that CSR investment can pay off even if a substantial amount of workers have no moral motivations. Turban and Greening (1997) combine secondary data from KLD Company ProfilesFootnote 3 with survey measures on the attractiveness of companies for students. They find that organizations utilizing CSR are more attractive to employees and might have a comparative advantage in attracting more productive applicants. Frank (2004) using survey and Nyborg and Zhang (2011) using register data show that CSR is associated with lower wages and therefore reduced personnel costs. To the best of our knowledge, we are the first to analyze effects of CSR and matching mission preferences in a laboratory experiment.Footnote 4 Therefore, the design might serve as a useful tool to investigate individual decision making when CSR aspects play a role. Our main result—on average workers reciprocate not only higher wages but as well investments in CSR and matching mission preferences with increased effort—also contributes to the principal agent literature. It adds to the list of alternative instruments to motivate workers that contrast the monetary incentives approach of ‘standard’ contract theory.Footnote 5 Besley and Ghatak (2005) propose the firm’s mission as one important motivational factor for workers. They use a matching model of the labor market with three types of principals and agents: not caring and caring about one of two particular missions. In equilibrium the concurrence of mission preferences between a firm and a worker leads to higher work motivation, less fixed wage and less bonus pay. Organizations can therefore substitute high monetary incentives by subscribing to a particular mission their workers care about. Our paper complements existing approaches to motivate workers and presents an alternative way to alleviate moral hazard in principal agent settings. It tests the effect of investments in CSR and missions set by firms on workers’ motivation, which may lead to an increase in firms’ profits. Section 2 provides the theoretical background of our labor market setting and the resulting hypotheses. In Sect. 3 we describe the experimental design, our treatments and the experimental protocol. Section 4 presents our results. Discussion and conclusions in Sect. 5 round off the paper.",22
17.0,3.0,Experimental Economics,25 August 2013,https://link.springer.com/article/10.1007/s10683-013-9373-9,Do short-term laboratory experiments provide valid descriptions of long-term economic interactions? A study of Cournot markets,September 2014,Hans-Theo Normann,Till Requate,Israel Waichman,Unknown,Male,Male,Male,"Many economic interactions in the field are long term in nature, whereas typical laboratory experiments only last for an hour or two. For instance, competition between firms may go on for months or even years, and the duration of labor-market interactions or vertical firm relations is frequently measured in decades. By contrast, experiments are usually short. Inspecting the duration of experiments published in the journal Experimental Economics between 2006 and 2010 (issues 9–13), we find that the average duration of an experiment is 68.09 minutes, with a minimum duration of 20 minutes and a maximum of 180 minutes. The discrepancy between the duration of field and lab interactions has given rise to concerns about the external validity of laboratory experiments.Footnote 1 In this respect, Gneezy and List (2006) find that in two controlled field experiments the impact of an unexpected increase in wages had only a short-term effect. After about three hours, subjects adjusted their behavior and the effect vanished. This finding is particularly interesting since there are hardly any economics experiments lasting more than three hours. Moreover, in long-term interactions individuals have more time to analyze the situation and to reflect their decisions. It is therefore surprising that, except for Baik et al. (1999) and Oechssler et al. (2008), no laboratory experiment has been designed to test for the impact of long-term interaction on performance.Footnote 2 Conducting a two-player “endogenous-timing” tournament experiment, Baik et al. (1999) find that more time to think led to more rational behavior. In a “mini ultimatum game” Oechssler et al. (2008) gave responders the opportunity to revise their decisions (whether or not to reject an offer) after 24 hours. They observed fewer rejections when stakes (represented by lotteries) were high, but they found no differences in rejections under standard ultimatum-game stakes. Related to the duration issue is the number of repetitions of the stage game in the laboratory. In fact, there is evidence suggesting that the larger the number of rounds in an experiment is, the more subjects will engage in cooperation (see Normann and Wallace 2012, and the references therein). Recently, Friedman and Oprea (2012) found almost 90 percent cooperation in a prisoner’s dilemma with (nearly) continuous time. While this shows that the frequency of interaction may be significant, it does not answer the question of how duration itself affects the course of play in experiments; after all, even these high interaction-frequency experiments do not last longer than the typical lab experiment. Moreover, it is important to investigate whether short- or long-term interactions in experiments are a potentially more appropriate way of testing theories. In theory, the only thing that matters is the distinction between a finite and an infinite number of repetitions, but nothing is said about the time span of these interactions. The present study investigates the impact of duration in symmetric and asymmetric Cournot duopolies. In particular, we conduct long-term treatments via the Internet, and compare them with the corresponding short-term internet counterparts and the standard lab treatments. There are several reasons why we have chosen that framework. First, when one envisages a situation involving long-term economic interactions, competition between firms is among the first examples that come to mind. Second, this game is essentially a social dilemma, and its results could be generalized to apply to many other economic situations. Third, the Cournot game has been experimentally investigated in the laboratory dozens of times, and the results are fairly robust and well-established. Fourth, testing the impact of long-term interaction is especially promising in such a design since there is much room for enhancing collusion. More precisely, while in symmetric duopolies players cooperate to some extent, the average output is still closer to the Cournot-Nash than to the joint-payoff maximizing output (see, e.g., Huck et al. 2004). In asymmetric duopolies players typically fail to cooperate.Footnote 3
 One possible explanation for why behavior differs between short- and long-term time spans is that subjects are more prone to “hot” emotional states in short-term settings than in long-term interactions.Footnote 4 Researchers in neuroscience find that participation in the ultimatum bargaining game activates regions in the brain known to be involved in negative emotions (e.g., Sanfey et al. 2003; Koenigs and Tranel 2007, etc.), and this is also true of cooperation experiments (see Rilling et al. 2002, 2004). These negative emotions (or more generally “visceral factors”) “often propel behavior in directions that are different from that dictated by a weighing of the long-term costs and benefits of disparate actions” (Loewenstein 2000, p. 426). For that reason, a cooling-off period is often used in negotiations. Hence, it may be easier to cooperate in the absence of immediate negative emotions such as anger, fear, or a feeling of injustice. Moreover, the possibility to think about the experiment for a few days (and maybe consult others) could impact on cooperation as subjects may realize that there are long-term benefits from collusion. On the other hand, in the short term subjects may be more prone to punishing defectors (an action conflicting with their self-interest) than in the long run. An example for such an action is “road rage” a short-sighted response on the road that is not in the interest of the angry driver (Loewenstein 2000). As a result, emotional behavior in the short term may induce cooperation even in the short run. Due to the ex-ante ambiguity regarding the impact of a long-term procedure on behavior, we have a two-tailed hypothesis: an experiment that is conducted over a long time span will yield output decisions that are different from the standard short-term laboratory procedures. The overall findings in our Cournot market treatments are as follows. In the long-term symmetric internet treatments behavior does not differ from the kind observed in their respective short-term lab and short-term internet counterparts. We also find no crucial differences between the two short-term treatments in the lab and via the Internet. In the asymmetric case we observe that, due to the behavior of high-cost firms, in the short-term internet treatment subjects play significantly more competitively than in the long-term internet treatment. These difference are, however, quite small in magnitude. Therefore, our overall conclusion is that, in terms of experimental duration, at least in Cournot duopolies outcomes in short-term laboratory settings do not differ from longer term environments.",13
17.0,3.0,Experimental Economics,22 September 2013,https://link.springer.com/article/10.1007/s10683-013-9374-8,Uncertainty causes rounding: an experimental study,September 2014,Paul A. Ruud,Daniel Schunk,Joachim K. Winter,Male,Male,Male,Male,"Rounding is a common phenomenon when subjects provide an answer to an open-ended question, both in experimental tasks and in survey responses. Examples include the elicitation of willingness-to-pay (dollar amounts) and beliefs (probabilities) in experimental studies, and the measurement of quantities such as income and consumption expenditure (dollar amounts) and subjective expectations (probabilities) in household surveys. From a statistical perspective, rounding implies that the measured variable is a coarsened version of the underlying continuous target variable. Since the coarsening process is non-random, inference from rounded data is generally biased; see Heitjan and Rubin (1991), inter alia.Footnote 1 Despite the potentially severe consequences of rounding, little is known about its causes, and in practice, it is typically ignored. In this paper, we study one important potential cause of rounding: the subject’s uncertainty about the target variable. Uncertainty as a cause of rounding has been suggested in the literature on survey response behavior (e.g., Tourangeau et al. 2000) but the applications studied in that literature are limited in scope, do not cover the important examples that arise in experimental economics and in household surveys given above, and do not provide evidence on the causal determinants of rounding. We present a novel experimental method that induces uncertainty in a controlled way, thus providing causal evidence for the effect of subjects’ uncertainty on the extent of rounding. Our experimental approach has three crucial features. First, we use an experimental task that involves a question on an uncertain quantity about which the subject has no prior information. However, we control the value of this quantity (i.e., we know the correct response to an open-ended question about this quantity). Second, we manipulate the degree of subjects’ subjective uncertainty about this quantity experimentally. We can thus provide causal evidence that uncertainty about the target question determines rounding. To our knowledge, this is the first paper to achieve such a result. Third, in contrast to existing experimental tasks that use lottery questions to study behavior in situations involving uncertainty, our design allows us to investigate the effect of uncertainty on response behavior independently of individual differences in both risk and ambiguity aversion, as well as in participants’ ability to understand probabilities. Another contribution is a structural model of the process that generates rounded responses to survey questions. Our analysis of the experimental data shows that subjects are more likely to round in tasks that involve more uncertainty about the true value of the target quantity. Since our experimental method is able to induce uncertainty in an experimentally controlled manner, this finding provides evidence for a causal effect from the subject’s uncertainty about the target variable on the extent of rounding. We conclude our analysis by specifying and estimating a mixture model for the latent beliefs about the true value of the response. The estimation results of the mixture model shed further light on the response process; specifically, they suggest that an increase in the exogenous level of decision uncertainty translates into higher variance in the subjects’ beliefs, which in turn results in more rounding. We interpret our mixture model as a structural econometric model of the response to an open-ended experimental task or survey question about which the subject is uncertain. Interestingly, there is only a rather limited literature, reviewed in Sect. 2, that explicitly models rounding of responses. In our interpretation, the reason for this state of current research is not a lack of interest. After all, (measurement) error in survey and experimental data on decision behavior has received much explicit and implicit attention not only in econometrics, but also in behavioral and experimental economics (see, e.g., von Gaudecker et al. 2011; Bellemare et al. 2010). Rather, existing knowledge of the processes subjects use when giving numeric responses provides econometricians little basis for building structural models of response behavior. Our experimental design is thus useful not only for the specific analysis of uncertainty and rounding and the development of structural models of rounding in subjects’ response behavior, but also for future research that studies the consequences of uncertainty more generally. It has the advantage of inducing uncertainty in such a way that subjects do not need to understand lotteries or probabilities. The paper is structured as follows. We start by reviewing the literature on rounding from various disciplines (economics, statistics, survey research) in Sect. 2. In Sect. 3, we describe the design of our experiment. We present a descriptive analysis of the data in Sect. 4 and results from regressions that predict whether a subject provides a rounded response in Sect. 5. We then specify and estimate a mixture model in Sect. 6. Section 7 concludes.",14
17.0,3.0,Experimental Economics,28 September 2013,https://link.springer.com/article/10.1007/s10683-013-9375-7,The effects of the take-option in dictator-game experiments: a comment on Engel’s (2011) meta-study,September 2014,Le Zhang,Andreas Ortmann,,,Male,Unknown,Mix,,
17.0,3.0,Experimental Economics,08 October 2013,https://link.springer.com/article/10.1007/s10683-013-9376-6,Communication & competition,September 2014,Jacob K. Goeree,Jingjing Zhang,,Male,Unknown,Unknown,Male,"It is well known that efficient contracting may be hampered by adverse selection problems that arise when outputs depend on privately known talents or types. Besides an impressive theoretical literature that addresses the design of optimal contracts in the presence of adverse selection (e.g., Bolton and Dewatripont 2005), alternative solutions based on insights from behavioral economics and laboratory experiments have recently been proposed (e.g., Fehr et al. 2007). In particular, experimental studies have demonstrated that “cheap talk,” i.e. non-binding and costless communication, can enhance efficiency (Charness and Dufwenberg 2006, 2011) and can be more effective than monetary incentives (Brandts and Cooper 2007). Plausible explanations that have been put forth are that cheap talk messages contain implicit promises that are costly to break when agents get disutility from lying or from letting others down. Much of this recent literature, however, focuses on bilateral relationships between a single principal and a single agent. This is obviously different from many real-world settings, e.g. when multiple job applicants compete for a single job (especially in times of a recession). It is conceivable that competition will change the nature of the messages exchanged, or the propensity with which promises are kept. In addition, implicit promises may have less impact when a principal receives similar messages from more than one agent. It is, therefore, natural to ask whether cheap-talk communication is still effective in promoting efficient contracting when competition exists. To address this question, we vary the possibility of communication in the one-shot principal-agent game studied by Charness and Dufwenberg (2011) and in an extension where the principal selects one of two agents before playing the game. This variation of the game defines our competition treatments. Our experiment replicates the main finding of Charness and Dufwenberg (2011). We find that in the “no-competition” treatments, communication raises efficiency. We also find that in the “no-communication” treatments, competition raises efficiency. Thus, by themselves, communication and competition positively affect efficiency. However, compared to treatments with competition or communication only, efficiency is lower in a treatment with both communication and competition. In other words, competition and communication act as substitutes. Communication raises efficiency without competition but not with competition. Likewise, competition raises efficiency without communication but lowers efficiency with communication. We review related experimental work and several behavioral theories that have been proposed in this context. We find that lie aversion, guilt aversion, inequality aversion, and reciprocity all capture some but not all features of the data. We expressly do not propose an alternative theory but rather hope that our findings will stimulate further theoretical work in this area. The remainder of the paper is organized as follows. Section 2 describes our experimental design based on the principal-agent game with hidden information. In Sect. 3 we report the effects of communication and competition. We also correlate messages with outcomes to provide additional insights into behavior. Section 4 briefly discusses related experiments and evaluates several behavioral theories. Section 5 concludes.",14
17.0,3.0,Experimental Economics,12 October 2013,https://link.springer.com/article/10.1007/s10683-013-9377-5,Favor trading in public good provision,September 2014,Sarah Jacobson,Ragan Petrie,,Female,Unknown,Unknown,Female,"Favor trading is ubiquitous. We lend tools to a neighbor and later ask him to watch our house while we are away. We recommend a friend for a job and then entreat him to give to our favorite charity or cause. Sometimes the exchange is less direct. We may support the school band of a colleague’s child with no obvious opportunity for personal benefit in the near future. Such exchanges between non-kin people are common and may spring from motives such as strategic self-interest, reciprocal altruism, and indirect reciprocity. We know little about which of these factors drive behavior and how they interact to increase or reduce cooperative behavior. This is important to understand because, in institutions in which cooperation is pro-social, the ability to return favors may be an important determinant of efficiency.Footnote 1 In this paper, we study favor trading in the private provision of a public good. We manipulate the characteristics of the institution to study the importance of strategy, direct reciprocity, and indirect reciprocity. We add to a substantial literature showing that people do contribute to public goods, but that cooperation is difficult to sustain (Ledyard 1995; Chaudhuri 2011). Reciprocal-type forces based on subjects’ reputations may sustain giving and other pro-social cooperative behavior (e.g. Trivers 1971; Fischbacher et al. 2001; Milinski et al. 2002; Gächter 2007), including through punishment and reward (e.g. Fehr and Gächter 2000; Andreoni et al. 2003; Masclet et al. 2003; Bochet et al. 2006; Houser et al. 2008; Almenberg et al. 2011). Social forces also may drive giving outside the lab; for example, DellaVigna et al. (2012) find evidence of “social pressure” in door-to-door fundraising. The importance of a social element in giving can also be inferred from the fact that charities frequently encourage supporters to tap into their social networks. A supporter can trade favors with friends by asking them to contribute to his favorite cause with the expectation of being asked to support their causes in the future. Brick-and-mortar institutions like the Girl Scouts of the USA (2011) and the Susan G. Komen Race for the Cure have long used personal contact to fundraise through grassroots networks. Social media have made this process easier than ever with online tools like Facebook Causes and DonorPages. Favor trading in this environment is possible because different people favor different public goods and because institutions often make contribution information public. When reciprocal giving supports public good provision, it may work in many ways.Footnote 2 It may be strategic, in which case a person gives only to garner a reward later and giving declines when future rewards are no longer possible. This motive was studied and compared to other-regarding preferences in Cabral et al. (2012), who find evidence of cooperation driven by strategic concerns but also find evidence of other-regarding preferences. Dreber et al. (2013) also study strategic cooperation as compared to cooperation based on social preferences, concluding that both are important, and Reuben and Suetens (2012a) find that much of the cooperation in their repeated Prisoner’s Dilemma game is strategic. Reciprocal giving may be driven by direct reciprocity (i.e. reciprocal altruism), models of which include Rabin (1993) and Cox et al. (2008), in which case the giver’s preferences are increasing in the payoff of a past benefactor.Footnote 3 Indirect reciprocity may play a role if an actor rewards a good deed done to a third party (e.g. Nowak and Sigmund 2005; Seinen and Schram 2006; Engelmann and Fischbacher 2009).Footnote 4
 Our experimental design identifies which institutional elements allow reciprocity to boost giving to a public good.Footnote 5 It also helps us determine the relative importance of direct reciprocity rooted in other regarding preferences, indirect reciprocity, and strategic self-interest. We induce heterogeneous interests in “causes” by assigning asymmetric returns to the public good. In every round, one person (the “Stakeholder”) gains more from the public good than do the others in the group (it is his “pet cause”). A group with a Stakeholder is similar to Olson’s (1965) “privileged group,” as studied in Reuben and Riedl (2009).Footnote 6 Our privileged group is even more effective because the Stakeholder position rotates from round to round through group members, creating opportunities for targeted reciprocal acts. To examine the importance of reciprocity, we compare behavior when subjects have full information regarding each other’s actions (enabling conditional action) and when they do not have that information. We also examine whether a group member who never has a “cause” will indirectly reciprocate by using his contribution to reward people who have been kind to others. This differs from many studies (e.g. Engelmann and Fischbacher 2009; Seinen and Schram 2006) that have found support for indirect reciprocity in that the people whose actions we study gain very little from a group norm of cooperation. In previous studies, subjects who indirectly reciprocate gain a large benefit if the group establishes a general norm of cooperation. We provide a stricter test. We find that letting people engage in targeted reciprocity increases contributions by a modest but statistically significant 14.4 %, although this effect may decline as rounds progress. More interestingly, we find strong evidence of reciprocal acts. Some may be strategic, but some are more consistent with direct reciprocity. We do not find evidence of indirect reciprocity. When a person’s benefit from a norm of cooperation is indirect, he does not reward kindness with kindness. The paper proceeds as follows. The next section explains the general experimental design. Section 3 describes three experimental treatments. In Sect. 4, we present results. We conclude in Sect. 5.",8
17.0,3.0,Experimental Economics,24 October 2013,https://link.springer.com/article/10.1007/s10683-013-9378-4,Traffic congestion: an experimental study of the Downs-Thomson paradox,September 2014,Emmanuel Dechenaux,Shakun D. Mago,Laura Razzolini,Male,Unknown,Female,Mix,,
17.0,3.0,Experimental Economics,26 October 2013,https://link.springer.com/article/10.1007/s10683-013-9379-3,"Taking, giving, and impure altruism in dictator games",September 2014,Oleg Korenok,Edward L. Millner,Laura Razzolini,Male,Male,Female,Mix,,
17.0,4.0,Experimental Economics,07 November 2013,https://link.springer.com/article/10.1007/s10683-013-9380-x,An experimental study of prosocial motivation among criminals,December 2014,Sigbjørn Birkeland,Alexander W. Cappelen,Bertil Tungodden,Male,Male,Male,Male,"Criminal behavior has significant direct costs for society and considerable resources are used on crime prevention. Close to ten million individuals are held in penal institutions around the world (Walmsley 2009) and in the US alone, 227 billion USD are spent each year to catch, prosecute, and punish offenders (Bureau of Justice Statistics 2010). The prison population is considerably smaller in the European countries than in the US, but even in these countries as many as one third of all young males have been charged with at least one crime (Skardhamar 2004). Criminal behavior is thus a pervasive and costly phenomenon and it is important to understand what explains such behavior. In particular, in the design of policies aimed at crime prevention or reintegration of criminals into society, it is crucial to understand why some people commit crimes while others do not. It is well established that differences in circumstances are important in explaining differences in criminal behavior (Allingham and Sandmo 1972; Becker 1968; Gould et al. 2002; Horney et al. 1995; Levitt 1997; Lochner 2004; McCarthy 2002). The role of individual motivation, in particular prosocial motivation, is less well understood. The fact that criminal behavior typically has negative consequences for others provides a compelling reason to believe that criminals are less prosocially motivated than non-criminals. This hypothesis has not been carefully tested, however. Earlier comparisons of the prosocial motivation of criminals and non-criminals have relied on self-reported measures of moral judgments (Kohlberg 1976, 1984; Palmer and Hollin 1998; Tarry and Emler 2007) or studied prosocial behavior when criminals interact with other criminals while they are imprisoned (Chmura et al. 2010; Gummerum and Hanoch 2012). The present study improves on this literature in a number of ways and allows a clean comparison of prosocial behavior of criminals and non-criminals when they make decision. Importantly, we study the prosocial behavior of criminals both inside and outside of prison and both when they interact with criminals and with non-criminals. We report the results from two economic experiments, a lab experiment and an Internet experiment. Both experiments employ the classical dictator game where one participant, the dictator, is asked to determine how a sum of money should be distributed between himself and another participant, the receiver, with whom he is anonymously matched (Camerer 2003; Engel 2011). The key feature of this game is that it places the dictator in a situation where there is no potential economic benefit from sharing since the recipient has no opportunity to respond to the decision made by the dictator. How much the dictator gives to the receiver, as a share of the available money, is therefore commonly used as a measure of an individual’s level of prosocial motivation. In the lab experiment we find a striking similarity in the prosocial behavior of prisoners and a benchmark group recruited from the general population when making decisions in the dictator game. This result holds both when the prisoners interact with other prisoners and when they interact with participants from the benchmark group. In-group favoritism has been shown to be important for prosocial behavior in many contexts (Akerlof and Kranton 2000; Bernhard et al. 2006; Charness et al. 2007; Chen and Li 2009), but in our experiment we find no significant in-group effect for either of the two groups of participants. Two concerns when interpreting the data from the lab experiment are that there might have been an experimenter demand effect and that imprisonment may have made the criminal identity of the prisoners salient (Cohn et al. 2012). The Internet experiment, which was conducted with a large group of participants from the general population, allows us to address these two concerns. In the Internet experiment there was nothing to indicate that the purpose of the experiment was to compare criminals and non-criminals and all subjects could participate in the experiment from their home. By linking the behavioral data from the Internet experiment to official criminal records, we could compare the average level of prosocial behavior of individuals with and without a criminal record. The results are in line with the results from the lab experiment: we find no significant difference in the prosocial behavior of individuals with and without a criminal record. We interpret the finding in the two experiments as providing evidence of criminals being as prosocially motivated as non-criminals in an important type of distributive situations. If we assume cross-situational consistency in social preferences, this suggests that the prosocial motivation captured by the dictator game is of little importance in explaining criminal behavior. We cannot, however, exclude the possibility that criminal behavior is affected by differences in prosocial motivation that are not captured by the dictator game or that there is weak cross-situational consistency in social preferences (Levitt and List 2007). Section 2 presents the sampling procedure and the design of the lab experiment and the Internet experiment. Section 3 presents the results, while Sect. 4 discusses some implications of our findings.",17
17.0,4.0,Experimental Economics,19 November 2013,https://link.springer.com/article/10.1007/s10683-013-9381-9,A day without a search engine: an experimental study of online and offline searches,December 2014,Yan Chen,Grace YoungJoo Jeon,Yong-Mi Kim,Male,Female,Unknown,Mix,,
17.0,4.0,Experimental Economics,30 November 2013,https://link.springer.com/article/10.1007/s10683-013-9383-7,Beyond choice: investigating the sensitivity and validity of measures of strength of preference,December 2014,David Butler,Andrea Isoni,Kei Tsutsui,Male,Female,,Mix,,
17.0,4.0,Experimental Economics,28 November 2013,https://link.springer.com/article/10.1007/s10683-013-9384-6,An experimental study of the impact of competition for Other People’s Money: the portfolio manager market,December 2014,Marina Agranov,Alberto Bisin,Andrew Schotter,Female,Male,Male,Mix,,
17.0,4.0,Experimental Economics,07 January 2014,https://link.springer.com/article/10.1007/s10683-013-9385-5,Incentive effects of funding contracts: an experiment,December 2014,J. Philipp Reiß,Irenaeus Wolff,,Unknown,Unknown,Unknown,Unknown,,
17.0,4.0,Experimental Economics,18 December 2013,https://link.springer.com/article/10.1007/s10683-013-9386-4,Playing the trust game with other people’s money,December 2014,Ola Kvaløy,Miguel Luzuriaga,,Male,Male,Unknown,Male,"In many situations, people make decisions on behalf of others. Investment managers are managing other people’s money, CEOs take decisions on behalf of boards and shareholders, managers delegate decisions to employees, and so on. Such decision-making often involves decisions to trust or act trustworthily on behalf of others. For example, contracting decisions are often delegated, and they typically involve some notion of trust, in which some of the promises that are made cannot be enforced by a court of law (so-called implicit or relational contracts). A CEO may need to decide on behalf of his or her firm whether to enter into a strategic partnership with a competitor, trusting the competitor not to abuse any confidential information. CEOs may delegate recruitment decisions to section leaders or office managers, who must trust new employees to honor also the non-enforceable part of their employment contract. Sales agents may decide, on behalf of their superiors, to let potential customers test the product, trusting the customers to eventually buy it, or return it in good conditions. The objective with this paper is thus to investigate, by use of a controlled laboratory experiment, how people trust and act trustworthily when decisions involve a third party’s economic interests. It is well established in the literature that trust and reciprocity facilitate bilateral transactions where third parties are not involved. People have a tendency to reward kind actions, and are thus willing to trust others since they expect to be reciprocated.Footnote 1
 But reciprocity is a complicated enforcement device. It is not only a behavioral response to an action; it is also a response to an intention. Some experimental studies suggest that when people evaluate and respond to an action, the intention or kindness behind the action plays an important role.Footnote 2 This is potentially detrimental to situations where people make decisions on behalf of others. The kindness or vulnerability of a trustor’s action is less clear when it is not his or her own money that is at stake, and importantly, the trustee’s possibility to reciprocate the kind action is more limited. We thus ask: Does reciprocity facilitate economic exchange when those who take the transactional decisions are not those who bear the fruits from trust? To investigate this question we adopt the well-known trust game (also referred to as the investment game) by Berg et al. (1995), with the important difference that the trustor (sender) who sends money to the trustee (receiver) does this on behalf of a third party, called a client.Footnote 3 The sender receives a fixed fee and does not have any monetary incentives to care about his or her client’s payoff. The receiver then has to decide how much money to send back to the sender’s client. Clients are anonymous to both senders and receivers. In addition we run a baseline trust game replicating Berg et al. Our main results are as follows: Senders who manage other people’s money (OPM) do not behave significantly different from senders who manage their own money (Baseline). Receivers, however, return significantly less when the money goes to the sender’s client instead of the sender. A consequence is that trust does not payoff in the OPM treatment. The clients’ payoff in OPM is significantly lower than the senders’ payoff in Baseline. The average client would have earned more if senders in OPM sent nothing. In Baseline, however, trust is (on average) profitable. The treatment effect among the receivers turns out to be gender specific. Men are not sensitive to the role of the sender, while women are. In fact, women return on average less than half the amount in the OPM treatment compared to the Baseline, and they return significantly less than men in the OPM treatment. Our design and results relate to several findings in the experimental economics literature. First, our paper is related to a small (but emerging) experimental literature on delegation, studying when and why a principal may benefit from delegating decision rights to an agent. Fershtman and Gneezy (2001) investigate an ultimatum game in which the proposer can delegate the decision to an agent, who is incentivized to make certain decisions on behalf of the proposer. They find that the responder is less willing to reject an unfair offer when the decision is delegated to an agent, and so delegation increases the proposer’s profit. Similarly, Bartling and Fischbacher (2012) and Coffman (2011) find, in different experimental games, that delegation can improve profit because the responder is less willing to punish unfair behavior.Footnote 4
 Similar to their findings, we also find that reciprocal behavior is attenuated when the first action is delegated to an agent. But in contrast to these papers, we study positive reciprocity in which the responder can reward trustful actions. This implies that reciprocity attenuation reduces profit rather than increasing it, i.e. that delegation leads to lower profit. Moreover, we do not study strategic delegation in which the principal (client) has a choice, or can induce the agent (sender) to take certain actions. The agent only gets a fixed payment, but is still fully responsible for the decisions. Hence, the receiver’s behavior only affects the monetary payoff of a person who is not responsible for any decisions. This makes it easier to interpret the receiver’s behavior. In Fershtman and Gneezy (2001) one does not know whether less reciprocity is mainly caused by a lower willingness to punish an “innocent” agent (which they call the hostage effect), or a lower willingness to punish the principal (proposer) who under delegation is only indirectly responsible for unfair offers. Our result (in the domain of positive reciprocity) suggests that less reciprocity is due to a lower willingness to reward the principal since he or she is not responsible for the trustful offer.Footnote 5
 This result also shed light on theories of why people trust and reciprocate in trust games. Distributional theories such as Fehr and Schmidt (1999) and Bolton and Ockenfels (2000) can account for reciprocal behavior in both of our treatments, but have a hard time explaining the different behavior in the two treatments. However, models that incorporates reciprocal preferences (like Falk and Fischbacher 2006), where the perceived kindness of the players matter, can contribute to explain why we see a lower share of money returned in the OPM treatment. The clear gender effect we find on the receivers, which was not hypothesized ex ante, speaks to the literature on gender differences in preferences. Croson and Gneezy (2009) provides a comprehensive overview of this literature, showing that in trust games women tend to be more reciprocal than men.Footnote 6 In light of this, our result in the OPM treatment, indicating that women under some conditions return far less money than men, is striking. But several studies also show that women are more responsive to the context or condition of the experiment than men are.Footnote 7 Croson and Gneezy states that “increased sensitivity of women to the context of the situation is the cause of inconsistent gender differences in social preferences” (p. 461). In our theory section we show that if delegation changes the perceptions about the other players’ kindness, then differences in the change of kindness perceptions, and/or differences in sensitivity to kindness perceptions, may explain our gender results. Finally, since trust also involve risk-taking, our paper is related to a growing literature on risk-taking on behalf of others. Some find that people take less risk on behalf of others than on behalf of themselves (Charness and Jackson 2008; Reynolds et al. 2009; Eriksen and Kvaløy 2010), while others find the opposite (Chakravarty et al. 2011 and Agranov et al. 2010). These results can be compared with the senders’ behavior in our experiment, who trust and thus take risk on behalf of others. The rest of the paper is organized as follows. In Sect. 2 we present the experimental design and procedure. Section 3 offers some behavioral predictions. Section 4 presents the experimental results while Sect. 5 concludes. Instructions for the experiment are relegated to the Electronic supplementary material (ESM).",14
17.0,4.0,Experimental Economics,08 January 2014,https://link.springer.com/article/10.1007/s10683-013-9387-3,Software for continuous game experiments,December 2014,James Pettit,Daniel Friedman,Ryan Oprea,Male,Male,,Mix,,
17.0,4.0,Experimental Economics,15 January 2014,https://link.springer.com/article/10.1007/s10683-013-9388-2,Time and uncertainty in resource dilemmas: equilibrium solutions and experimental results,December 2014,Anabela Botelho,Ariel Dinar,Amnon Rapoport,Female,Male,Male,Mix,,
17.0,4.0,Experimental Economics,18 January 2014,https://link.springer.com/article/10.1007/s10683-013-9389-1,Recognizing contributors: an experiment on public goods,December 2014,Anya Savikhin Samek,Roman M. Sheremeta,,Female,Male,Unknown,Mix,,
17.0,4.0,Experimental Economics,15 January 2014,https://link.springer.com/article/10.1007/s10683-013-9390-8,An experiment on protecting intellectual property,December 2014,Joy A. Buchanan,Bart J. Wilson,,Female,Male,Unknown,Mix,,
17.0,4.0,Experimental Economics,15 January 2014,https://link.springer.com/article/10.1007/s10683-014-9391-2,Dishonesty and charitable behavior,December 2014,Doru Cojoc,Adrian Stoian,,Male,Male,Unknown,Male,"Guilty individuals can typically engage in a wide range of good deeds in order to ease their conscience. For example, sinners seeking penance can volunteer at soup kitchens, donate blood, or raise money for the needy. Attorneys who overbill their clients can soothe their conscience by offering pro-bono legal representation. A CEO whose company pollutes can allocate some funds to environmental causes. Religions (Cassone and Marchese 1999; Kuran 1996) and charities (Hibbert et al. 2007) seem to find the individuals’ desire to atone useful for their fundraising activities. This raises an important concern: might opportunities to do good in the future lead an individual to behave less ethically in the present? If moral decisions are indeed history-dependent, it is also important to know how initial moral choices affect subsequent ethical behavior. Do individuals follow up and behave ethically when the opportunities to do good arrive? Or do their initial moral choices lead them to behave less pro-socially in the future? We conduct an experiment in the laboratory to shed light on the questions above. Specifically, we examine whether people cheat more when they are informed about a future chance to donate some of their earnings to a charity. We run a two stage experiment. In the first stage, we give subjects the opportunity to behave dishonestly without the possibility of being detected, such as in Mazar and Ariely (2006) and Gill et al. (2012), among others, using a design pioneered by Fischbacher and Föllmi-Heusi (2013). Subjects self-report the outcome of a random event that determines their expected payoff. They can misreport their performance in order to increase the likelihood of receiving a greater monetary payoff. In the second stage, subjects decide how much of their earnings to give to the American Red Cross. Subjects in the Treatment group know about the second stage from the beginning. Subjects in the Control group only find out they have an opportunity to donate to charity after they have completed the first stage. Two main results emerge from our experiment. First, we find that more subjects in Treatment behave dishonestly than subjects in Control. The proportion of dishonest subjects in Control implied by their self-reports is 31.7 %. In Treatment, the proportion of cheaters is 49.7 %. We thus find that immoral behavior increases when people can behave morally in the future. Our second set of results is that on average subjects donate less when they know about the opportunity to give from the beginning. Subjects in Control donate an average of $4 out of a prize of $10; subjects in Treatment donate only $2. About one quarter of the difference in average giving is due to more subjects donating positive amounts to charity in Control than in Treatment. The difference in donations between treatments remains significant after controlling for all the available exogenous variables and for the subjects’ behavior in the first stage of the experiment. Both the honest and the dishonest subjects in Treatment give significantly less than the honest ones and than the dishonest subjects in Control. We propose a new psychological mechanism to explain our findings. Initial transgressions numb one’s conscience, causing smaller increases in guilt from additional violations of social norms. We call this mechanism conscience numbing. For example, in our experiment an individual maintains a clean conscience only if she both refrains from cheating and donates to charity. Violations of social norms are therefore (perfect) complements: a subject who cheated feels guilty no matter how much she donates, so she finds no cost in donating little or nothing to charity. Under this assumption subjects are, on average, less honest in Treatment. All subjects who cheat in Control would also cheat in Treatment, but some subjects only cheat when they know they have an opportunity to donate later. These “marginal cheaters” are typically the ones whose conscience would require them to donate considerable amounts to charity. The benefits of cheating in the first stage for these subjects include a numb conscience that prevents them from donating too much to charity. Average charitable donations therefore decrease in Treatment. Conscience numbing predicts that although subjects are less honest in Treatment, they end up donating less to charity. An alternative psychological mechanism that can explain the increase in dishonesty in Treatment is moral cleansing. Conforming to social norms in various moral dilemmas may act as substitutes, so a good deed may cleanse one’s guilty conscience. The moral cleansing hypothesis is at the core of some phenomena documented in the social psychology literature. In the “transgression-compliance” framework put forward by Cialdini et al. (1973), past transgressions enhance one’s conscience leading to a desire to atone through compliance to social norms. Monin and Miller (2001) show that subjects who previously conformed to social norms feel “morally licensed” to violate them later. Closer to the behaviors examined here, Sachdeva et al. (2009) find that subjects whose moral identity was threatened express a stronger desire to donate to charity than subjects whose moral identity was enhanced. Under moral cleansing, some subjects in Treatment may decide to cheat and atone later by donating to charity. The moral cleansing hypothesis therefore predicts less honesty and larger charitable donations in Treatment, the latter being contrary to our findings. Our results have important consequences: it appears that the subjects whose behavior changes in Treatment do not cheat more because they plan to give. Instead, they expect to violate social norms in both moral dilemmas. There is no trade-off between good and bad deeds in our experiment, but rather a bad deed induces people to engage in more bad deeds. The results in this paper suggest that analyzing ethical behaviors separately may miss relevant links between them. In particular, institutions that diminish the guilt associated with previous immoral behavior through good deeds may lead to undesirable outcomes. They may give an excuse for engaging in immoral behavior, and at the same time decrease the likelihood of future ethical behavior both among moral and immoral individuals. Kotchen (2006) finds a similar result: in his model, the introduction of green goods meant to facilitate contributions to the environment may induce some economic agents to decrease their overall public good contribution. We contribute to the literature attempting to understand the causes and consequences of dishonest behavior and to the one analyzing charitable giving. Both dishonesty and giving have been extensively analyzed separately in the past, but rarely together.Footnote 1 An exception is Gneezy et al. (2012), a paper developed independently at the same time as ours. They compare subjects who are not given an opportunity to donate to charity to subjects who can donate; they find that the latter are less honest than the former. They also examine how the amount of time that has elapsed from the initial transgression affects subsequent charitable donations. In their experiment, subjects who can donate right after they cheated donate more than subjects who can donate later. Our distinct contribution is to vary the information subjects have about the future opportunity to donate to charity. All our subjects have the opportunity to donate, but only some subjects are aware of it ahead of time and can adjust their behavior. We observe both the degree of dishonesty and the charitable donations of subjects who were not aware they can donate to charity and we are able to compare both behaviors to these of subjects who knew about the opportunity to give. We are thus able to precisely identify the mechanism that increases dishonesty when subjects have information about future giving. We show that the mechanism driving our results is different from the usual moral cleansing mechanisms discussed in the social psychology literature. Our results are consistent with a new alternative hypothesis: initial transgressions numb one’s conscience and lead to additional transgressions. The rest of the paper is organized as follows. In the next section we describe the experimental design. In Sect. 3 we develop a simple model of behavior and we examine the predictions generated by our new hypothesis, conscience numbing, and by moral cleansing. We present our findings on cheating and charitable donations in Sect. 4. The last section concludes.",27
18.0,1.0,Experimental Economics,26 June 2014,https://link.springer.com/article/10.1007/s10683-014-9414-z,"In Honor of Elinor “Lin” Ostrom: The Vincent and Elinor Ostrom Workshop in Political Theory and Policy Analysis, Department of Political Science, Indiana University",March 2015,James Walker,Mark Isaac,,Male,Male,Unknown,Male,,1
18.0,1.0,Experimental Economics,09 November 2013,https://link.springer.com/article/10.1007/s10683-013-9382-8,Cooperation in small groups: the effect of group size,March 2015,Daniele Nosenzo,Simone Quercia,Martin Sefton,Female,Female,Male,Mix,,
18.0,1.0,Experimental Economics,26 April 2014,https://link.springer.com/article/10.1007/s10683-014-9402-3,Incomplete punishment networks in public goods games: experimental evidence,March 2015,Andreas Leibbrandt,Abhijit Ramalingam,James M. Walker,Male,Unknown,Male,Male,"There is widespread evidence that the availability of costly peer sanctioning can have a large positive impact on cooperation in social dilemma settings (e.g., Ostrom 1990; Ostrom et al. 1992; Fehr and Gächter 2000; Walker and Halloran 2004; Sefton et al. 2007). These findings suggest that self-governed monitoring and sanctioning may play an important role in human cooperation and well-functioning of modern societies. However, the prevailing evidence is mainly based on the comparison of two extreme cases; all individuals can punish and be punished by other individuals in a group versus a situation where no one can punish. These criteria are typically not met in the field where various factors such as physical distance, endowments and status, and the social network of actors regularly limit punishment opportunities. Punishment networks, which define who can punish whom, may play a nontrivial role for inducing more efficient provision of public goods or appropriation from common-pool resources. In particular, it seems plausible that denser punishment networks, where a larger fraction of actors can punish each other, deter actors more effectively from non-cooperative behaviors. This increased deterrence in denser networks may be associated with the threat of being punished by more agents and/or the possibility of larger combined punishment capacity. However, it seems equally plausible that denser punishment networks may deter actors less effectively from non-cooperative behaviors if actors believe that the threat of being punished diminishes as the number of potential targets increases and effective coordination of punishment becomes more difficult. In addition, the increasing number of potential targets and limited individual capacities to sanction may reduce the severity of assigned sanctions. Taken together, there is very little direct evidence on how the network structure and punishment capacity impact public good provision, imposed sanctions and economic efficiency. In this study, we provide new empirical evidence on the role of punishment networks for facilitating cooperation. We employ a public goods experiment in which we manipulate the structure of punishment networks and punishment capacities. Contribution and punishment decisions are examined across twenty rounds of repeated play in groups of four players who have fixed identifiers. Four networks are examined: a complete punishment network, a ‘pairwise’ punishment network, an ‘untouchable’ punishment network and a no-punishment network. In the pairwise network, the group of four is divided into two pairs and punishment can only take place within pairs, although contributions affect the entire group. In the untouchable network, there are three agents that can punish and be punished by each other and one agent who cannot punish or be punished. By reducing the number of players who can punish a player, the two incomplete networks (pairwise and untouchable) reduce the total capacity of players to impose and receive punishment. For this reason, an additional treatment is conducted in each of the incomplete networks such that punishment capacities were as high as in the complete network. Individual punishment capacities are manipulated in these two networks in order to investigate if observed behavior is driven by the structure of the punishment network or punishment capacity. These punishment networks were selected for the following reasons. First, arguably, the pairwise networks constitute the most transparent cases to examine issues of targeting sanctions, reputation formation, and limited scope of sanctions. The untouchable networks were selected based on observations from the field where it is common that some agents are temporarily or permanently isolated from others, but cannot be excluded from the benefits of public goods or common-pool resources. Complete and no punishment network conditions are created as benchmarks and to better link our findings to the existing experimental literature. The investigation of punishment behavior in incomplete networks connects our study to numerous examples of common-pool resource management and public good provision settings where the geographical structure and state borders may limit stakeholders’ opportunities to sanction each other. At the same time, many of the international agreements designed to protect natural resources and curb environmental deterioration implement governance structures that often allow for accurate monitoring of contributions but limited opportunities to punish detached actors. A primary finding of this study is that the greater the number of people who can punish and be punished, the greater the contributions to the public good and the greater the amount of punishment used in the group. Further, high contributions are sustained only in the complete and untouchable networks. In addition, the capacity for one individual to punish another plays a less important role on aggregate contribution levels than the network configuration. In particular, higher punishment capacities are unable to stem the observed decline in contributions in the pairwise network, and also play an insignificant role in the untouchable network. Finally, consistent with previous findings, low and high contributors are punished (Hermann et al. 2008), a finding that is consistent with targeted revenge. This study contributes to the literature testing the effectiveness of various institutional arrangements to overcome the regularly observed sub optimality of voluntary contributions. Among the large body of proposed institutional solutions to the problem of free-riding, opportunities to communicate (Isaac and Walker 1988; Ostrom et al. 1992; Bochet et al. 2006), costly peer punishments (Ostrom et al. 1992, Fehr and Gächter 2000), verbal sanctioning (Masclet et al. 2003), ostracism (Cinyabuguma et al. 2005), combined punishment and reward schemes (Andreoni et al. 2003; Gürerk et al. 2006; Sefton et al. 2007; Leibbrandt and López-Pérez 2014), reputation networks (Milinski and Rockenbach 2006) and leadership structures (Güth et al. 2007) all potentially serve as proximate mechanisms to enhance voluntary cooperation.Footnote 1
 In addition, this study connects to an emerging literature examining the role of social and geographic network structures on public good provision when punishment opportunities are absent. Theoretical investigations (Bramoullé and Kranton 2007) and experimental evidence (Yamagishi and Cook 1993; Fatas et al. 2010) point to the fact that contribution levels may differ significantly across networks. Differences in contributions across such networks are explained by conditionally cooperative responses to the restricted spread of information about individual contributions (Fatas et al. 2010).Footnote 2
 More closely related to our study are experiments in which punishment opportunities in public goods settings are manipulated (Carpenter 2007a; Kosfeld et al. 2009; O’Gorman et al. 2009; Reuben and Riedl 2009; Nikiforakis et al. 2010; Carpenter et al. 2012; Cox et al. 2013). Reuben and Riedl (2009) study the effectiveness of punishment in privileged groups where some group members generate positive returns from public good contributions. Their findings indicate that punishment is less effective in privileged groups as compared to normal groups. Kosfeld et al. (2009) investigate institution formation in social dilemmas where a subset of players can form a sanctioning institution, while their contributions benefit the outsiders who do not enter the institution. Nikiforakis et al. (2010) vary the effectiveness of punishments across individuals. Their results suggest that institutions with asymmetric sanctioning power can be equally successful in fostering cooperation and efficiency than their symmetric counterparts. Carpenter et al. (2012) manipulate monitoring opportunities and show how properties from graph theory can organize the data patterns that arise in their public goods experiments. This study differs in several aspects from the previous literature. First, previously unexplored network structures are examined in settings where decision makers receive complete information about individual contributions, sanctions imposed, and sanctions received for all group members. This contrasts with other studies that investigate the joint effect of information dissemination and punishment opportunities in networks where group members do not receive information on individual behavior outside their network (Carpenter 2007a; Carpenter et al. 2012). Second, we use a partner-matching protocol with fixed identifiers. The advantage of fixed identifiers is that this information condition captures the essence of many real networks where individuals have stable positions within a fixed group, not simply a network architecture describing how a random group of individuals occasionally link.Footnote 3 Finally, individual punishment endowments and total punishment capacities are controlled for across groups. Thus, in contrast to many studies, we are able to identify the role of the punishment network and can rule out potential endowment effects.",22
18.0,1.0,Experimental Economics,05 June 2014,https://link.springer.com/article/10.1007/s10683-014-9405-0,State or nature? Endogenous formal versus informal sanctions in the voluntary provision of public goods,March 2015,Kenju Kamei,Louis Putterman,Jean-Robert Tyran,Unknown,Male,Unknown,Male,"Social dilemmas, in which uniformly self-interested behavior makes a social goal less rather than more attainable, are recurrent problems in modern economies. The common ‘textbook’ solution is the use of what we call formal sanctioning schemes. This solution relies on the coercive powers of the state, which can make contributions (such as tax payments) mandatory and subject to penalties for non-compliance. But formal sanction schemes are costly as they require infrastructure (like a judicial system, police and prisons) and their use seems infeasible or unnecessary in some situations. There are indeed numerous collective action dilemmas, including provision of public goods that fall beneath government notice because they are too local or parochial, for which this solution is unavailable or less efficient than what we call informal sanctions. Informal sanctions are decentralized and horizontal in nature and do not require formal infrastructure as they rely on peer punishment. The willingness to mete out such (costly) informal sanctions and to do so in a well-targeted way depends on the characteristics of group members, in particular their social preferences. While the workings of formal sanctions are rather independent of social preferences once they are in place, their establishment is likely not to be. The very existence of a government that can promulgate and enforce regulations to deter free riding while acting as a faithful agent of its citizens depends on voluntary pro-social acts such as citizen scrutiny of politicians’ actions, self-education about political issues, and making the effort to vote in elections. The relationship between voluntary collective action and fulfillment of obligations under threat of formal sanctions is thus a complex one, and the question of when we can or should rely on formal sanctions to resolve social dilemmas is an important and underexplored issue. Experimental economists have extensively studied public goods problems, with a special focus on voluntary contribution dilemmas with informal sanctions (for overviews, see Gächter and Herrmann 2008; Chaudhuri 2011). More centralized forms of governance, and the conditions, if any, under which informal sanctions regimes are preferable to formal ones, have received surprisingly little attention until the present paper and a companion paper by Markussen et al. (2014).Footnote 1 The reason for this neglect may be that it seems obvious to an economist that appropriately designed formal sanctions dominate informal ones. We argue that which sanctions scheme is the better choice is far from obvious when agents are not fully rational and self-interested. Our results indeed show that apparently dominated informal sanctions are chosen quite often when formal sanctions are available at a moderate fixed cost, and that they are surprisingly successful if chosen. Our results also show that if voters have a say on how formal sanction schemes are to be designed, they design them to target free riders and to be deterrent in strength, which contributes to their efficiency. In a formal sanctions (FS) regime, a group adopts a rule specifying what penalties will be imposed under what conditions and sets up a body (in large group settings, an administration or government) that observes rule violations and imposes the stipulated penalties. In an informal sanctions (IS) regime, group members can punish each other at a cost to both the punisher and the punished. Because such sanctions are costly, a rational and selfish decision-maker does not engage in IS in one-shot (and, by extension, finitely repeated) interaction, and voting for IS is therefore pointless among such agents. In contrast, voting for FS is (at least weakly) dominant for rational and self-interested agents, as long as FS target free riders reliably and the fixed cost of having FS in place does not exceed its benefit. In such a population, FS are the efficient choice. Even when rationality, self-interest, and common knowledge assumptions are relaxed, appropriately designed FS may be preferred by voters. One reason is that in an ideal system of FS, sanctions are meted out automatically but uncertainty prevails about who will impose IS on whom, and when. The predictability of FS may mean that fewer costly sanctions need be imposed under a FS regime, an efficiency advantage. If IS do occur when allowed, there is no way to guarantee that they are not misdirected (“perverse” or “anti-social”) and to rule out attempts at retaliation (see below), whereas rules in the interest of every group member can assure that FS are well targeted. This may help explain why the centralized administration of penalties has traditionally been seen as the hallmark of civilization, whereas the enforcement of rules by individuals is often denigrated as “vigilante justice” or “mob rule.” There are reasons for preferring informal sanctions in some circumstances, however. We think of IS in connection with village management of woodlots and irrigation systems or social pressure in work teams, while FS bring to mind managerial structures and governments. Having an enforcing body in place—for example, a police force and courts—involves enforcement structures that, paradoxically, may need to be used relatively little if their presence suffices to deter the social bads they are meant to prevent. Informal sanctions may be less deterrent because they are less certain, but they may have the advantage of avoiding substantial fixed costs.Footnote 2
 By investigating collective choice between FS and IS, our paper contributes to a recent and rather thin experimental literature on endogenous institutions (see Sect. 2 for references). Our design involves repeated choice between FS and IS, both when having FS in place is costly and when it is not. As a control, we also conduct sessions in which neither FS nor IS are available. These design aspects are also present in a companion paper by Markussen et al. (2014). To our knowledge, these two studies constitute the first experimental research on collective choice between and performance under both FS and IS schemes. The key difference between the two studies is that whereas in Markussen et al. the FS scheme available to a given subject group is exogenously specified, subjects in the present paper not only choose between FS and IS but also determine the parameters of the FS, if used. This additional dimension of choice makes collective choices cognitively more demanding for voters. An important implication of this difference is that we create a more level playing field between FS and IS by allowing subjects to choose inefficient properties of the FS scheme. Voters can choose to implement either deterrent or non-deterrent sanctions, paralleling the fact that IS can be strong or weak. Voters also have the possibility of implementing “perverse” formal sanctions—ones that punish high contributors, similar to the “perverse” sanctions that are frequently observed in experiments with IS. Our new design also adds insight into the roles of experience and individual orientations in voting decisions. Our study and that of Markussen et al. also differ in other details, including the country in which they were conducted, the level of fixed costs for having FS in place, presence or absence of variable costs of formal sanctions, and inclusion of treatments allowing exposure to each institution prior to voting. While strict comparisons are ruled out since multiple dimensions differ, each study’s results can be viewed as a set of informal robustness tests of the other. The main shared results are that informal sanctions are surprisingly popular and effective, and that fixed costs of having FS in place are crucial to subjects’ choices between IS and FS. Novel findings of the present study are that subjects improve the terms of their formal sanctions schemes with learning, that they prefer deterrent over non-deterrent formal sanctions, and that subjects who exhibit cooperative tendencies are significantly more likely to vote for efficiency-enhancing institutions, and vice versa for those with uncooperative tendencies. The finding that the latter are likely to vote against efficiency-enhancing institutions, but usually as a minority, affirms a potentially important insight about heterogeneous social preferences and majority voting (Ertan et al. 2009). To conserve space, we leave details of Markussen et al.’s findings to be discussed in the context of comparisons to our own findings, below. We proceed as follows: Sect. 2 discusses theoretical considerations regarding collective action and formal and informal sanctions, and briefly reviews relevant experimental research. Section 3 spells out our experimental design, the rationale behind the various treatments, and the predictions both under the assumption of common knowledge of rationality and self-interest and under alternative behavioral assumptions. Section 4 summarizes the experimental results. Section 5 concludes the paper with a brief summary and suggestions for future research.",61
18.0,1.0,Experimental Economics,12 February 2014,https://link.springer.com/article/10.1007/s10683-014-9393-0,Promoting cooperation in nonlinear social dilemmas through peer punishment,March 2015,Timothy N. Cason,Lata Gangadharan,,Male,Female,Unknown,Mix,,
18.0,1.0,Experimental Economics,22 June 2014,https://link.springer.com/article/10.1007/s10683-014-9413-0,Experimental departures from self-interest when competing partnerships share output,March 2015,Josh Cherry,Stephen Salant,Neslihan Uler,Male,Male,Female,Mix,,
18.0,1.0,Experimental Economics,22 June 2014,https://link.springer.com/article/10.1007/s10683-014-9412-1,One bad apple? Heterogeneity and information in public good provision,March 2015,Angela C. M. de Oliveira,Rachel T. A. Croson,Catherine Eckel,Female,Female,Female,Female,"
Why some groups succeed in working together for their mutual interest while others fail in overcoming social dilemmas was central to the work of Elinor Ostrom and the generations of scholars she inspired. Several regularities have emerged from this research. For example, there is significantly more cooperation observed than would be predicted if all individuals were purely self-interested, money-maximizing agents. This regularity appears in the field, where individuals voluntarily contribute to charitable organizations, to provide public goods, or to govern the commons (e.g., Andreoni 2006, Ostrom 1990), as well as in context-free laboratory environments (Ledyard 1995; Ostrom 2000). Effectively designed institutions can further foster this cooperation (Ostrom 1990; Ostrom et al. 1999). However, not all groups succeed at overcoming the social dilemma (e.g., Dietz et al. 2003; Gächter and Thöni 2005; Ostrom 1990). We investigate one of the explanations for why some groups fail while others succeed: the existence of ‘bad apples.’ Department meetings, homeowners associations, parent-teacher associations, co-authorships, and classes: one needs only quick introspection to identify the ‘bad apples’ that have harmed group efforts that could have been otherwise very productive. The social wisdom is that “one bad apple can spoil the whole bunch”: it only takes one selfish person to destroy group cooperation. We thus design an experiment to identify the marginal impact of ‘bad apples’ on the group’s ability to voluntarily and efficiently provide a public good. We construct groups that range in composition from homogeneous groups of selfish types to homogeneous groups of conditional cooperators, with heterogeneous groupings in between. In half of the groups, participants are aware of the group composition, in the others, no information is provided. We use a particular social dilemma, often studied in public goods provision, a repeated, three-person linear public goods game (voluntary contribution mechanism, also called VCM). We analyze the impacts of group composition and of revealing information about the group composition of social preference types on the ability to attain the social optimum. By controlling group composition and varying the information provided, we are able to overcome confounds that have existed in previous studies, which used random assignment to groups and no information about heterogeneity (e.g., Fischbacher and Gachter 2010; Kurzban and Houser 2005).Footnote 1
 The closest previous study to ours is Fischbacher and Gachter (2010). While they elicit the social preference types of individuals and use this information to explain contributions, they do not systematically vary the group composition or the information available about the composition, as we do. Our results suggest that ‘bad apples’ harm the ability of groups to overcome the social dilemma through two mechanisms. First, and most obviously, they contribute less to the public good. Second, for our group size and parameters, they reduce the contributions made by conditional cooperators, and this effect increases in the number of selfish individuals in the group. These effects are not sensitive to information about the group composition. Together, our results suggest that the negative impact of ‘bad apples’ is gradual rather than all-or-nothing. These results are, of course, limited to our decision environment and further research is warranted to determine the robustness of these results to different group sizes and parameter specifications. In the next section, we describe the literature that lays the groundwork for this study, including work on social preference types, group composition, and information followed by a description of the design and implementation. Subsequent sections present the descriptive and econometric results as well as a closing discussion.",35
18.0,1.0,Experimental Economics,08 April 2014,https://link.springer.com/article/10.1007/s10683-014-9399-7,Observed punishment spillover effects: a laboratory investigation of behavior in a social dilemma,March 2015,David L. Dickinson,E. Glenn Dutcher,Cortney S. Rodet,Male,Unknown,,Mix,,
18.0,1.0,Experimental Economics,29 January 2014,https://link.springer.com/article/10.1007/s10683-014-9392-1,Gunning for efficiency with third party enforcement in threshold public goods,March 2015,James Andreoni,Laura K. Gee,,Male,Female,Unknown,Mix,,
18.0,2.0,Experimental Economics,13 March 2014,https://link.springer.com/article/10.1007/s10683-014-9394-z,Does anonymity affect the willingness to accept and willingness to pay gap? A generalization of Plott and Zeiler,June 2015,Alexander L. Brown,Gregory Cohen,,Male,Male,Unknown,Male,"There is a great deal of controversy whether owning an item causes an individual to value it more than he would otherwise. This “endowment effect” debate appears in the experimental literature as an argument about the existence of a gap between subjects’ willingness to pay (WTP) for an item versus their willingness to accept (WTA) dispossession of the same item. In their seminal work, Kahneman, Knetsch, and Thaler (1990) find the existence of this gap to be robust to a variety of experimental conditions, and note it is consistent with a manifestation of loss aversion and reference dependence. Plott and Zeiler (2005) argue that a true test of such preferences requires control for all other conceivable explanations. They find that when they add controls designed to foster subject understanding of an incentive compatible elicitation device, as well as anonymity, these gaps disappear. Assuming their differing results are due to their differing procedures, then two possible types of changes may be responsible for the observed WTA-WTP gap in experiments. Either the gaps vary with subject familiarity with an incentive compatible elicitation device, or with perceived subject anonymity within experiments. These two changes imply two very different explanations for their results. In one case, unfamiliarity or misconceptions about how reported valuations map into payouts is altering subjects’ responses, but buyers and sellers value the good similarly. In the other, subjects could be correctly responding to the stimulus, but social pressures concerning how others perceive them might dictate different responses in buyers and sellers. Fremling and Posner (Fremling and Posner 1999) propose this latter explanation. Since “talented negotiators” are known for obtaining items for low prices and selling them for high prices, subjects may wish to signal to others or the experimenter that they are talented. These social pressures cause their actions to diverge from their true individual valuation of items, causing the WTA-WTP gap. While there is no direct experimental evidence suggesting subjects will sacrifice their own earnings to be perceived as talented, several studies on anonymity in dictator games (e.g., Burnham 2003; Charness and Gneezy 2008; Eckel and Grossman 1996; Hoffman, McCabe, and Smith 1996) suggest subjects will sacrifice earnings to be perceived as unselfish by others. With the aim of providing further clarity in this area, this paper examines whether anonymity has any effect on subject responses in elicitation procedures in mug experiments. While it is generally believed that familiarity with the second-price mechanismFootnote 1 and not anonymity is responsible for the differences between the results of Plott and Zeiler and Kahneman, Knetsch, and Thaler,Footnote 2 whether anonymity is responsible for any of the difference has not been tested. To this end, we replicate the general designs of both studies with and without anonymity. If we find that gaps vary with anonymity, then this is consistent with the explanation of anonymity being responsible for the WTA-WTP gap found in experiments. Experimenters will then need to decide when using the second-price mechanism whether or not it is appropriate to use anonymity, depending on the purposes of their design. If instead we find differences between the two procedures regardless of anonymity, then varying degrees of familiarity with the second-price mechanism—not the lack of anonymity—is the likely cause of differing elicitations among subjects. It will be up to future experimenters to decide what level of familiarity with the second-price mechanism is appropriate for their studies, but they likely need not worry about instituting anonymous protocols. Our results show no effect of anonymity on subjects’ willingness to accept and willingness to pay for mugs. Further, consistent with Plott and Zeiler we find a significant WTA-WTP gap using procedures taken from Kahneman, Knetsch, and Thaler, but not when using the Plott and Zeiler procedures. Hereafter, we refer to our version of these procedures as KKT-BC and PZ-BC, respectively. As an added check, we run single-shot dictator games in each anonymity condition in each procedure. While our initial study (see our working paper, Brown et al. 2012) using identical anonymous procedures finds significant differences in giving (consistent with Hoffman, McCabe, and Smith 1996), we do not find it in either our KKT-BC or PZ-BC procedures. Thus, it is possible that subjects in our study are unresponsive to, or unaware of, the differences in levels of anonymity. Nonetheless, the differences in WTA-WTP gaps between our PZ-BC and KKT-BC conditions allow us to conclude that variations in subject familiarity with the second-price mechanism are sufficient to cause the observed differences in WTA-WTP gaps, a generalization of Plott and Zeiler’s main result. We conclude that subject training on the second-price mechanism and not anonymity is likely responsible for differences in the WTA-WTP gap between Kahneman, Knetsch and Thaler, Plott and Zeiler, and other elicitation experiments that use the second-price mechanism. We do not claim we have educated subjects correctly to reveal their preferences in the second-price mechanism. Future research is needed to determine the appropriate way to use such mechanisms. Nonetheless, the result should aide experimenters who wish to use the second-price procedure as they can focus on the proper amount of subject training and not enforcing anonymous procedures in the lab. This paper proceeds as follows: the remainder of this section discusses related literature. Section II outlines our general design; section III provides results and section IV concludes. There is a rich history of experimental economics results involving the WTA-WTP gap. Kahneman, Knetsch, and Thaler (1990) find some of the most well-known and significant evidence in favor of the existence of the gap. They attribute this difference to loss aversion and reference dependence. In a typical experiment, subjects are randomly divided into two equal groups of “buyers” and “sellers.” Each seller is given an item (usually a mug) and told they may trade that item with buyers. Unlike standard economic theory—which predicts about half the sellers should trade their mugs and seller willingness to pay should not differ from buyer willingness to accept—a majority of sellers do not trade their mugs and seller WTA is about 1.5–2 times higher than buyer WTP. In other experiments within the same paper, they find these general results are robust to a variety of experimental changes including clearly defining the market value of the item, removing money, only trading items, and using the second-price mechanism to obtain incentive compatible estimates of WTA and WTP. This WTA-WTP gap is not found, however, when tokens with clear, transferable, monetary value are used instead of a durable item. Plott and Zeiler (2005) show that varying subject familiarity and training with the second-price mechanism and anonymity causes variations in the observed WTA-WTP gaps across experiments. They begin by replicating the experiment of Kahneman, Knetsch, and Thaler that used the second-price mechanism. They then implement a new design with a different elicitation mechanism, extensive instruction and training with that mechanism, and anonymity. They find no significant difference between WTP and WTA for mugs among buyers and sellers. They conclude that observed WTA-WTP gaps in the lab cannot be evidence of loss-aversion and reference dependent preferences because the gaps disappear when controls for anonymity and subject misconception are present. We will use these two experimental designs as the KKT-BC and PZ-BC procedures in our experiments. Our initial study (Brown et al. 2012) provides the groundwork for this paper. We run a similar experiment to Kahneman, Knetsch, and Thaler (1990), but replace their hypothetical practice rounds with paid practice rounds and provide feedback to subjects on how to properly bid under their elicitation mechanism. With and without conditions designed to assure anonymity, we find no evidence of the WTA-WTP gap. Going further, this study uses Kahneman, Knetsch, and Thaler and Plott and Zeiler’s original designs both with and without anonymity to examine the relevance of anonymity to Plott and Zeiler’s main result. Other literature examines whether experimenter language is responsible for the effect (Franciosi et al. 1996), whether results can be explained by a different loss aversion factor (Brown 2005), whether repeated markets cause the WTA-WTP gap to disappear (Loomes, Starmer, and Sugden 2003), whether varying the method in which the mug was given alters the effect (Loewenstein and Issacharoff 1994), and whether imprecise preferences cause the disparity (Dubourg, Jones-Lee, and Loomes 1994). Others still test the hypotheses set forth by Plott and Zeiler (2005), such as Isoni, Loomes, and Sugden (2011) and Plott and Zeiler (2007).",7
18.0,2.0,Experimental Economics,22 March 2014,https://link.springer.com/article/10.1007/s10683-014-9395-y,Do control questions influence behavior in experiments?,June 2015,Catherine Roux,Christian Thöni,,Female,Male,Unknown,Mix,,
18.0,2.0,Experimental Economics,30 March 2014,https://link.springer.com/article/10.1007/s10683-014-9397-9,Self-selection into laboratory experiments: pro-social motives versus monetary incentives,June 2015,Johannes Abeler,Daniele Nosenzo,,Male,Female,Unknown,Mix,,
18.0,2.0,Experimental Economics,10 April 2014,https://link.springer.com/article/10.1007/s10683-014-9398-8,Paradoxes and mechanisms for choice under risk,June 2015,James C. Cox,Vjollca Sadiraj,Ulrich Schmidt,Male,Female,Male,Mix,,
18.0,2.0,Experimental Economics,27 April 2014,https://link.springer.com/article/10.1007/s10683-014-9400-5,Social preferences in the online laboratory: a randomized experiment,June 2015,Jérôme Hergueux,Nicolas Jacquemet,,Male,Male,Unknown,Male,"In the field of experimental economics, it is a long time since researchers called for the development of the “online laboratory” (Bainbridge 2007). The interest in online experimentation has been propelled by the possibility of reaching more diverse samples, recruiting larger subject pools and conducting cross-cultural social experiments in real time at an affordable cost.Footnote 1 Besides this methodological concern, the Internet is becoming an increasingly prominent experimental field for social science research in its own right (see, e.g., Resnick et al. 2006; Chesney et al. 2009), as we live more and more of our social and economic lives online. It is thus essential to conduct experiments directly over the Internet if we are to rely on the experimental method to understand the various types of social and economic activities that people engage in online. Notwithstanding these appealing features, the development of the “online laboratory” still remains in its infancy. The primary goal of this paper is to help fill this gap by conducting a methodological evaluation of an Internet-based experimentation procedure. Horton et al. (2011) underline the difficulty of coming up with procedures for online experiments that ensure their internal validity, i.e. the possibility of confidently drawing causal inferences from one’s experimental design. A number of confounding factors have been identified that have probably prevented researchers from running experiments online: (i) it is difficult to monitor the identity of subjects participating in the experiment, (ii) subjects may read the experimental instructions too carelessly and/or make decisions too quickly and/or get significantly distracted during the course of the experiment, (iii) subjects may selectively drop out of the experiment in ways that the experimenter does not understand, (iv) subjects may not believe that they interact with other human players and/or that they are going to be paid at the end of the experiment as described in the instructions, and finally (v) the issue of reliably and automatically processing the payment of subjects over the Internet in an anonymous fashion appeared to be a major blocker. In this paper, we seek to compare the behavioral results generated both in a traditional laboratory and over the Internet. To do so, we develop an online platform specifically dedicated to conducting social experiments over the Internet that is usable as in the laboratory. To account for the effect of self-selection between implementations, we control the allocation of subjects between treatments. The platform provides controls over many of the above-mentioned confounding factors. In particular we (i) control for differences in response times, (ii) deal with the issues of selective attrition, concentration and distraction and (iii) provide as much control as possible over subjects’ beliefs as regards the experimental instructions. The existing literature has already covered a variety of different games implemented over the Internet (Table 1 summarizes the methodology and main conclusions of this literature). The seminal study of Anderhub et al. (2001) focuses on an individual level decision experiment under uncertainty, both in the laboratory and online. Shavit et al. (2001) compare student bids over buying prices for simple lotteries both in the classroom and online. Charness et al. (2007) also compare classroom experiments with other Internet-based experimental settings to investigate the effect of social distance on trust and reciprocity in a simple lost wallet game. They find that trust and reciprocity both decrease in an Internet-based setting, which they argue is consistent with social distance theory (Akerlof 1997). Fiedler and Haruvy (2009) and Chesney et al. (2009) take an exploratory approach and build a virtual laboratory on the Second Life website. Chesney et al. (2009) recruit subjects from the Second Life community to perform a series of social experiments and compare the results with those of the traditional laboratory literature. Similarly, Fiedler and Haruvy (2009) recruit subjects from Second Life to perform a Trust game, but directly compare their results with those obtained from traditional laboratory subjects playing in the same virtual environment, but in a physical laboratory. They also find trust and trustworthiness to be lower outside the physical lab. Most recently, Horton et al. (2011) and Amir et al. (2012) have used the online labor market platform Amazon Mechanical Turk to conduct a set of classic experiments and replicate qualitatively some general results drawn from the experimental economics literature. We contribute to this burgeoning literature by looking at social preferences and by providing a rigorous comparison of the Internet-based experimentation with traditional lab experiments. We apply our methodology to the measurement of social preferences—combined with a risk aversion task—through a Public Good game, a Trust game, a Dictator game and an Ultimatum game (using a within-subjects design). The main conclusions that we draw from this comparison are twofold. First, the social preferences elicited in the lab and online are qualitatively very similar—all common inferences on social preferences that we replicate in the laboratory would also be obtained based on online data. Second, we do, however, observe some differences in the point estimates between treatments. Social distance theory (Akerlof 1997) predicts that the stronger anonymity that prevails in Internet-based interactions should drive social preferences down as compared with the laboratory setting, where people can (i) see each other before and after the experiment, (ii) recognize that they often come from the same socio-economic background and (iii) know that they are going to be matched with one another during the experiment. On the contrary, we find robust and significant evidence that subjects allocated to the Internet treatment behave more altruistically and, when insignificant, the differences in social preferences always go in the direction of more other-regarding decisions online. We suggest an explanation for our results based on the nature of the social and economic interactions in which individuals tend to engage online, which they are likely to bring to the experiment through its contextual implementation. Our results are important to the community of researchers wishing to develop the online laboratory as a medium for running social experiments over the Internet and to relate their results to the established laboratory literature. They are also important for social scientists wishing to use social experiments to research the Internet as a field: given the observed parallelism between fields, it makes sense for researchers to bring their experimental tools directly to the field, i.e. over the Internet, if they want to learn from subjects’ behavior in this context, rather than sticking with the more difficult approach of trying to bring a subsample of those subjects into a traditional university laboratory. The rest of the paper proceeds as follows. Section 2 documents the design of the experiment, reports on the development of our online experimental economics platform and explains our experimental procedures. Section 3 reports the main results of the experiment. We then move to additional evidence on the reliability of the comparison based on an analysis of the internal validity of the online experiment, secondary outcomes and robustness treatments. We discuss the main outcomes of this comparison in Section 4, and conclude in Section 5.",50
18.0,2.0,Experimental Economics,22 April 2014,https://link.springer.com/article/10.1007/s10683-014-9401-4,Why real leisure really matters: incentive effects on real effort in the laboratory,June 2015,Brice Corgnet,Roberto Hernán-González,Eric Schniter,Male,Male,Male,Male,"Incentive theory plays a crucial role in the study of economic phenomena and is a natural candidate for extensive experimental investigation. As a result, the field of experimental labor economics (Charness and Kuhn 2011) has emerged to test many of the predictions that have been generated by incentive and contract theories (Laffont and Martimort, 2002; Bolton and Dewatripont 2005). These experimental protocols have, however, left aside the study of on-the-job leisure activities which unarguably constitute an important part of the work environment (Malachowski, 2005). In this paper, we propose to fill this gap by integrating on-the-job leisure into the evaluation of incentive effects in the laboratory. From a methodological standpoint, the inclusion of on-the-job leisure activities in laboratory experiments may help attenuate active participation, an issue raised by Lei, Noussair and Plott (2001) in the context of experimental asset markets. Specific details of an experiment protocol, such as availability of alternatives to focal “work”, might encourage or fail to encourage subjects to actively perform effortful work. For example, subjects may engage actively in a focal work task because of expectations, rewards, and lack of desirable alternatives. When desirable alternatives are present, active participation in effortful work may be traded off to some degree, revealing subtle incentive effects such as from small shifts in wage or manipulations of incentive schemes. We suspect that active participation has previously been an issue because, despite predictions that individual incentives should outperform team incentives (Alchian and Demsetz, 1972; Holmström, 1982), several published studies, and perhaps even more unpublished studies, have failed to observe such differences (van Dijk, Sonnemans and van Winden, 2001; Dohmen and Falk, 2011). We investigated the effects of providing a leisure alternative on subjects’ performance in a real-effort work task by either allowing or restricting Internet access during the experiment. We considered a real-effort mental arithmetic (summation) task in the spirit of previous laboratory experiments (Niederle and Vesterlund, 2007; Bartling, Fehr, Maréchal and Schunk, 2009; Eriksson, Poulsen and Villeval, 2009; Dohmen and Falk, 2011).Footnote 1 We manipulated the incentive setting by rewarding work performance according to either an individual pay or team pay scheme. We used Internet browsing to represent on-the-job “real-leisure”Footnote 2 because it is a growing and popular on-the-job leisure activity and a representative feature of the workplace. The availability of on-the-job Internet access has increased, due in part to most Americans now owning a smartphone capable of Internet browsing and less subject to employee usage restrictions (Smith, 2013). According to a 2005 study by American Online and Salary.com, employees spent about 26 % of their time on activities unrelated to their work (Malachowski, 2005). Almost half of this time was spent browsing the Internet. Also, the Internet is frequently browsed by and widely available to university students (our subject pool), providing them with a wide range of activities (Jones et al. 2009). Previous studies have proposed alternative experimental designs to moderate active participation in experiments. For example, Mohnen, Pokorny and Sliwka (2008) gave subjects the option to take a 25-second time-out during which they were not able to work on the counting task and for which they were paid 0.10€. While subjects frequently engage in pleasurable activity during their breaks (e.g. socializing, browsing the Internet), the time-out setting investigated by Mohnen and colleagues rewarded subjects for taking breaks characterized by inactivity. Contrary to these designs with “abstract” leisure, in our environment we expect heterogeneity in derived utility from leisure (here proxied by internet access). However, Internet browsing, by offering an unprecedented range of leisure options, reduces the heterogeneity in the value of leisure compared to other “real leisure” alternatives such as giving subjects access to selected magazines. Also, the advantage of this implementation is that subjects can choose to engage in leisure activity—a more representative feature of the real workplace. In line with the real-effort literature we argue that a real-leisure option cannot be instantiated simply as a decision for a monetary alternative. For example, workers who may be reluctant to steal money from an employer or firm may still be willing to spend time browsing the Internet while paid for being “on-the-job”. They may justify Internet browsing as part of their legitimate search for valuable work information or they may consider paid work to include some real-leisure “breaks”. Internet browsing on the job has been shown to damage employees’ productivity (Young, 2005; 2006) and incurred U.S. corporations at least $85 billion in yearly costs (Alder, Noel and Ambrose, 2006). Consideration of leisure-related issues in the experimental economics literature dates back to Dickinson’s (1999) labor supply study in which subjects working on a typing task were provided an option to leave the laboratory whenever they had achieved a certain output level. Quitting options have since been introduced into subsequent studies on minimum wages and workfare (e.g. Falk and Huffman, 2007, Abeler et al., 2011). Given the lack of control over subjects’ activities and desired alternatives outside the laboratory, heterogeneity in quitting behaviors has been difficult to interpret. Quitting options are also not representative of the typical workplace regime as salaried employees in most organizations are required to comply with a minimum number of completed work hours per pay period. Unlike previous experiments, our design embeds on-the-job leisure alternatives into the work environment, allowing the measurement of each subject’s time allocation to leisure or work activities. While two related experimental studies (Eriksson, Poulsen and Villeval, 2009; Charness et al. 2010) have introduced on-the-job leisure alternatives by giving subjects access to magazines (for browsing or reading), they have not reported effects of on-the-job leisure activity on subjects’ performance.Footnote 3
 To our knowledge, ours is the first experimental work to have measured subjects’ time allocation to real-effort work tasks or on-the-job leisure and to have analyzed the effects of access to leisure on subjects’ performance. We studied the effect of on-the-job leisure in two of the most popular incentive schemes: individual pay and team pay (e.g. Prendergast, 1999; Lazear, 2000; Hamilton, Nickerson and Owan, 2003). We conducted a 2 × 2 design in which we varied the availability of Internet access (available or not) and the type of pay (rewarded according to individual performance or team performance). For each of the four treatments, subjects were matched in groups of seven to ten people and completed the experiments in five periods of 20 min. Subjects used the Internet option when available. They dedicated 28.5 % of their time to browsing the Internet under team pay while only dedicating 11.9 % of their time to Internet browsing under individual pay. Consequently, the impact of Internet browsing on subjects’ performance was different across payment schemes. The availability of Internet browsing reduced production significantly under team pay while it did not reduce production under individual pay. In addition, we observed that incentive effects (measured as the difference in production between the individual pay and the team pay treatments) were more pervasive when Internet browsing was available. In the presence of Internet browsing, incentive effects were significant across all periods of the experiment while no incentive effects were found in the first two periods of the experiment when Internet browsing was not included in the design. This suggests that incentive effects may be sensitive to real-leisure alternatives such as Internet browsing and that previous real-effort experiments lacking this feature may have failed to uncover incentive effects which are reasonable to expect in the workplace.",57
18.0,2.0,Experimental Economics,06 May 2014,https://link.springer.com/article/10.1007/s10683-014-9403-2,Gender differences in the dictator experiment: evidence from the matrilineal Mosuo and the patriarchal Yi,June 2015,Binglin Gong,Huibin Yan,Chun-Lei Yang,Unknown,Unknown,,Mix,,
18.0,2.0,Experimental Economics,17 May 2014,https://link.springer.com/article/10.1007/s10683-014-9404-1,Multi-period experimental asset markets with distinct fundamental value regimes,June 2015,Thomas Stöckl,Jürgen Huber,Michael Kirchler,Male,Male,Male,Male,"The enormous costs imposed on society by the recent financial crisis highlight the importance of understanding conditions under which market prices are efficient and when they may deviate from fundamentally justified values. Multi-period laboratory asset markets are a suitable tool for this exploration as they expose real humans to market settings or regulatory regimes under consideration.Footnote 1 However, inferring likely consequences of various regulatory instruments require a profound understanding of how the fundamental value \((\hbox {FV})\) regime influences market efficiency. The most influential paradigm for multi-period laboratory asset markets was developed in Smith et al. (1988), \(\hbox {SSW}\) henceforth. In their model \(\hbox {FV}\)s decline deterministically over time and market prices usually exhibit strong bubble and crash patterns. Over the last years studies on markets with constant or randomly fluctuating fundamentals have shown that the degree of mispricing might also depend on the \(\hbox {FV}\) regime. Markets with constant or randomly fluctuating \(\hbox {FV}\)s exhibit more efficient pricing compared to \(\hbox {SSW}\) (Smith et al. 2000; Noussair et al. 2001; Kirchler 2009; Kirchler et al. 2012). Recently, some studies conjectured that bubbles in \(\hbox {SSW}\) markets are specific to the declining fundamental value trajectory (Huber and Kirchler 2012; Kirchler et al. 2012). So far, there is uncertainty about the persistence of this bubble pattern in increasing \(\hbox {FV}\) regimes. However, convincing data on this point is missing. We provide evidence on this issue with our treatment \(\hbox{R}3(/)\). Given the inconsistent experimental evidence on the role of \(\hbox {FV}\) regimes, we set up the first study which investigates the influence of \(\hbox {FV}\) regimes on market efficiency in a unified framework. We minimize the effect of between-treatment variations by building each regime on the approach of \(\hbox {SSW}\). Treatment \(\hbox {R1}(\backslash )\) replicates \(\hbox {SSW}\), while Treatment R2(—) replicates earlier studies with constant fundamentals in the broader framework of \(\hbox {SSW}\). Treatment \(\hbox {R}3(/)\) is the main innovation as it provides the first clean test of markets with deterministically increasing \(\hbox {FV}\)s. It is difficult to provide an ex ante prediction on the likely outcome in \(\hbox {R}3(/)\) as previous experimental studies on markets with (partly) increasing \(\hbox {FV}\)s show contradicting evidence (Noussair and Powell 2010; Giusti et al. 2012; Johnson and Joyce 2012; Breaban and Noussair 2014). Considering our unified framework and the fact that markets of \(\hbox {SSW}\)-type typically show relatively constant prices in the beginning with strong price adjustments (crashes) towards the end we conjecture by extrapolation that markets with increasing \(\hbox {FV}\)s exhibit (moderate) underpricing. Thus we expect relatively stable prices initially and stronger upward price adjustments towards the end. Finally, treatments \(\hbox {R4(tri)}\) and \(\hbox {R5}(\mathcal {\hbox {N}})\) provide evidence on regimes where \(\hbox {FV}\)s evolve over time following random walk processes. We find (i) efficient pricing in markets with constant \(\hbox {FV}\)s, (ii) overvaluation in markets with decreasing \(\hbox {FV}\)s, and (iii) undervaluation in markets with increasing \(\hbox {FV}\)s. (iv) Markets with randomly fluctuating fundamentals show overvaluation when \(\hbox {FV}\)s predominantly decline and undervaluation when \(\hbox {FV}\)s are mostly upward-sloping. Finally, we document that (v) bid-ask spreads and volatility of price changes are positively correlated with mispricing across regimes. Our study confirms findings of previous work on declining and on constant fundamental value paths (Smith et al. 1988; Noussair et al. 2001; Kirchler et al. 2012). The results of our random-walk \(\hbox {FV}\)-processes confirm earlier work (Kirchler 2009). However, the main contribution of the paper is to provide clean comparisons between distinct \(\hbox {FV}\) regimes, in particular between markets with increasing \(\hbox {FV}\)s and other regimes.",35
18.0,3.0,Experimental Economics,03 June 2014,https://link.springer.com/article/10.1007/s10683-014-9406-z,Excess information acquisition in auctions,September 2015,Vitali Gretschko,Alexander Rajko,,Male,Male,Unknown,Male,"Typically, by investing resources bidders can gain a better understanding of the valuation of the object being auctioned. For example, in real estate auctions, bidders can acquire expertise about the value of a property. However, to place a bid, it is not necessary to obtain perfect information about ones valuation. If the cost of information is high, bidders may prefer to place bids based on partial information. We examined bidders’ behavior in second-price and English auctions in a setting where information about a bidder’s own private valuation is costly. In an experiment, we varied the cost of information and analyzed the subjects’ bidding and information acquisition behavior. We found that in the majority of cases, bidders acquired too much information. Moreover, bidders who chose to remain uninformed placed bids significantly below the optimal bid. Both observations contradicted the theoretical predictions concerning bidder behavior that we derive from a risk-neutral expected surplus maximization model. However, the general predictions concerning revenue and efficiency remain valid, as higher cost of information is associated with lower revenues and lower efficiency rates. We explore ex-post three alternative models that are frequently used to explain behavior in experimental auctions: regret avoidance, risk aversion, and ambiguity aversion. We show that regret avoidance explains the observed data well whereas risk aversion and ambiguity aversion cannot explain the deviations from standard theory. The acquisition of information about ones valuation is not only relevant for mergers and acquisitions where due diligence is a well-established part of the process.Footnote 1 Most auction environments involve information acquisition. For example, in spectrum auctions, information is usually acquired by means of technical research about the infrastructure, internal reports on future revenues, or the cost of setting up a new network. In preparing a bid for a procurement auction, suppliers may spend a considerable amount of resources estimating their cost to deliver the project. Even simple bidding on eBay for personal objects may require costly information acquisition in terms of cognitive cost.Footnote 2
 The typically large data sets from auction platforms cannot help to explain the effects of information acquisition, as these costs usually materialize outside the auction itself, and hence cannot be observed. Therefore, we use a laboratory experiment designed along the lines of the rational choice model of auctions with information acquisition developed by Compte and Jehiel (2007).Footnote 3 To our predictions concerning the value of information, we analyze two cases: if the cost of information acquisition is low, the model predicts that in both auction formats, subjects should acquire information with positive probability. Contrary to that, if the cost of information acquisition is high, no information acquisition should be observed in either format. Based on the data, we provide three new and robust insights into bidders’ behavior at such auctions. First, subjects acquire information excessively in the sense that information is acquired in more than 50 % of the auctions when the cost of information is high. Second, in terms of the bidding strategies, we find that bidders who remain uninformed bid significantly below the optimal bid. Third, in the English auction, subjects buy information prematurely, and thus the timing of their information acquisition is not optimal. The general prediction for revenue and efficiency still holds true in our experiment, as a higher information cost is associated with lower revenues and lower efficiency. However, an auction designer who is concerned about revenue should worry less about costly information than theory would suggest. This is due to the fact that even with a high cost of information acquisition, subjects overinvest in information. If the auction designer is concerned about efficiency, he should take into account the cost of information acquisition. Even though the allocative efficiency with a high information cost is better than theoretically predicted, the over-investment in information reduces overall efficiency significantly. We shall proceed by providing an ex-post analysis of the behavior we observed. We show that excess information acquisition and underbidding can be explained by incorporating regret into the initial model.Footnote 4 If the bidder is fully informed about his valuation of the object, the ex-ante optimal bids in the second-price auction and the English auction are also optimal ex-post.Footnote 5 Hence, the bidders experience no bidder regret. If, however, the valuation is unknown, the bid placed by the bidder may either lead to a negative payoff or the failure to win the object, even when the price is below the realized valuation. Both of these situations may cause regret. If the bidder anticipates the feeling of regret, his willingness to pay for information increases. If, on the other hand, the bidder remains uninformed, our feedback procedure only induces the regret suffered from overpaying.Footnote 6 In this case, it is optimal for a regret-sensitive subject to shade the bid below the optimal bid. In contrast to anticipated regret, risk and ambiguity aversion cannot explain our observations. To the best of our knowledge, there is only one experimental study that is broadly related to the subject at hand: Davis et al. (2011) compared bidder behavior in an English auction and a sequential mechanism. The bidders had to invest in information about their valuation before entering the mechanism. Contrary to our study, bidders were not able to participate if they did not invest in information that made investing more valuable. Similar to our findings, Davis et al. (2011) found that subjects overinvest in information and enter the mechanisms more often than predicted by theory.",10
18.0,3.0,Experimental Economics,28 May 2014,https://link.springer.com/article/10.1007/s10683-014-9407-y,A caveat for the application of the critical cost efficiency index in induced budget experiments,September 2015,James H. Murphy,Samiran Banerjee,,Male,Unknown,Unknown,Male,"Several studies (Sippel 1997; Harbaugh et al. 2001; Andreoni and Miller 2002; Choi et al. 2007; Banerjee and Murphy 2011 among others) have examined the neo-classical rationality of consumers’ choices by confronting them with experimenter-determined budgets. The resulting choice data is directly testable for consistency with one or several of the axioms of revealed preference such as the Generalized Axiom of Revealed Preference (GARP). Data that is GARP-consistent may be rationalized by a piecewise linear, concave utility function in the appropriate commodity space. A frequent feature of such studies (sometimes termed induced budget experiments) is an attempt to measure the severity of violations by applying the critical cost efficiency index (CCEI) due to Afriat (1973, 1987), “a number between zero and one, where a value of one indicates that the data satisfy GARP perfectly” and where unity minus the CCEI “measure[s] the amount by which each budget constraint must be adjusted in order to remove all violations”, as stated succinctly by Choi et al. (2007 p. 1927). This interpretation holds for certain kinds of violation, but not all. As Andreoni and Miller (2002, p. 742) note, “it is possible for the CCEI to be equal to 1 when moving one choice by an infinitesimal amount would remove the violation”. The analyst applying the CCEI might well incorrectly classify such choice as being GARP-consistent when it is not. Andreoni and Miller’s quote characterizes the phenomenon in a simple, two choice scenario, but the case of multiple violations typically encountered in induced budget experiments requires additional examination. We provide a brief introduction to the relevant axioms of revealed preference in Sect. 2. In Sect. 3, we define the CCEI, distinguish a pathological violation of the axioms—termed a cost efficient violation (CEV)—from a non-pathological one, and delineate the conditions when the presence of CEVs in a data set with multiple violations may lead to the misinterpretation of the CCEI. In Sect. 4, we examine four recent induced budget experiments to determine the prevalence of this phenomenon. Section 5 discusses the implications of our findings on the design of induced budget experiments. Section 6 concludes.",4
18.0,3.0,Experimental Economics,30 May 2014,https://link.springer.com/article/10.1007/s10683-014-9408-x,Laboratory elections with endogenous turnout: proportional representation versus majoritarian rule,September 2015,Melis Kartal,,,Female,Unknown,Unknown,Female,"Voter turnout shows significant variation across countries, and numerous empirical studies have attempted to account for this variation in turnout. One factor that received particular attention in these studies is the voting institution (see, for example, Blais and Carty (1990); Kostadinova (2003); Karp et al. (2008)) The voting institution is likely to shape turnout incentives as it determines how vote shares are mapped to seat shares. Turnout is an important matter because electoral outcomes such as minority representation and social welfare depend on the number of people who participate, not on the size of each political group. In this paper, I study theoretically and experimentally the impact of majoritarian rule (hereafter, MR) and proportional representation (hereafter, PR) on voter turnout and minority representation. I present a framework that builds on the costly turnout literature (Palfrey and Rosenthal 1983, 1985; Ledyard 1984; Borgers 2004; Goeree and Grosser 2007; Krasa and Polborn 2009; Taylor and Yildirim 2010). In this framework, citizens can vote in an election for one of two parties, party A or party B. Each citizen is a supporter of either party A or party B. Thus, the electorate consists of two groups with opposing interests. Casting a vote is costly and voluntary; hence, turnout is endogenous.Footnote 1 As mentioned above, the voting system determines how vote shares are mapped to seat shares. MR is a winner-take-all system; the party that obtains more votes wins under MR. On the other hand, PR results in a more balanced sharing of political power. I derive two comparative static predictions using this framework. My first prediction shows that minority size has an important role in the turnout comparison across PR and MR. In particular, PR generates higher turnout than MR if the minority size is small, and the converse is true if the minority size is large. According to my second prediction, PR improves minority representation regardless of the minority size. The prediction regarding turnout is empirically relevant. An extensive empirical literature has compared turnout across the two systems; however, the empirical evidence is mixed.Footnote 2 As my theoretical result shows, this mixed evidence can be related to variations in group size. Minority size is a factor that cannot easily be taken into account in empirical papers on the topic, often because data is not available. Therefore, I use the controlled laboratory environment to test my comparative static predictions. In the experiment, treatment variables are the voting system and the minority size. There is a small minority treatment and a large minority treatment under both PR and MR; these treatments vary the size of the minority keeping the electorate size fixed. The PR system that I use in my experimental design has an election threshold. This is because PR systems differ vastly in their degree of proportionality, and many PR systems have either partial proportionality or election thresholds (see Blais and Dobrzynska 1998). It is debatable whether small minorities benefit from this type of PR. Therefore, I am particularly interested in the turnout and representation effects of such systems. The data from my experiment confirms that the effect of PR on participation depends on the size of the minority. PR decreases participation if the minority size is large, as predicted. It turns out that PR does not have a statistically significant impact on turnout if the minority size is small.Footnote 3 As discussed above, these results are empirically relevant. Turnout comparison across PR and MR depends on the minority size, in both theory and data. However, empirical studies do not control for it. When I test my prediction about minority representation, I also compare the resulting payoffs across the two systems. The reason for this is as follows. There is typically a trade-off between the protection of majority interests and empowering the minorities. If there is too much focus on the protection of majority interests, then minority members may have insufficient political representation, which violates equity principles. If, however, the minority share in political power exceeds a certain limit, then majority members may suffer a large loss, which is inefficient. Thus, it is important to compare PR and MR in terms of both equity and efficiency. Data from the large minority treatment provides strong evidence that PR improves minority representation, as I predicted. Remarkably, improved equity under PR comes at a minute cost in terms of efficiency: PR reduces the average payoff (net of voting costs), but by a very small amount. PR improves minority representation also in the small minority treatment. However, the statistical evidence for this is weak, and the magnitude of the effect of PR is much less pronounced than the theoretical prediction. This stems from an anomaly in the minority behavior. First, the observed minority turnout rate under PR is drastically lower than the point prediction. Second, there is no statistical evidence that minority turnout rate is higher than the majority turnout rate, in contrast with what the theory predicts. Recall that PR has an entry threshold in the experimental design. I conjecture that small minority members get discouraged and abstain frequently under PR due to their size disadvantage and the entry threshold. There are studies from outside the field of voting experiments highlighting the discouraging effects of competitive disadvantage. In experimental studies on tournaments, it is observed that subjects who are assigned a high cost of effort drop out by not exerting enough effort, and this doesn’t conform to the theoretical prediction (see, for example, Schotter and Weigelt 1992; Müller and Schotter 2010). In a similar vein, many small minority members drop out by not voting enough, even under PR. As a result, PR does not have a remarkable impact on the representation of a small minority. I conclude that the impact of the voting system on the society is very sensitive to both the size of the minority and the degree of proportionality. There is an extensive theoretical literature on endogenous turnout originating from the seminal papers by Palfrey and Rosenthal (1983, 1985) and Ledyard (1984). However, little attention has been paid to the comparative analysis of voting systems with endogenous turnout. I compare PR and MR in the current paper and in a companion paper (Kartal 2014). While this paper derives theoretical hypotheses about turnout and representation that can be tested in a controlled laboratory environment, Kartal (2014) presents a theoretical welfare analysis of various voting systems with endogenous turnout. The independent study by Herrera et al. (2013) is closely related to my work: They also analyze PR and MR with endogenous turnout, show that the turnout comparison of PR and MR depends on the minority size and test this theoretical prediction in a laboratory experiment. However, they do not compare the two systems in terms of representation or electoral efficiency. Further, they use a different PR system. Schram and Sonnemans (1996a) also investigated MR and PR in a costly voting experiment. In their study, they analyze turnout, only. Moreover, their design uses equal-sized groups. So far, no costly voting experiment has analyzed the impact of voting systems jointly on turnout and political representation in the presence of a minority and varying the size of the minority.",15
18.0,3.0,Experimental Economics,04 June 2014,https://link.springer.com/article/10.1007/s10683-014-9409-9,You’ve earned it: estimating the impact of human capital on social preferences,September 2015,Pamela Jakiela,Edward Miguel,Vera L. te Velde,Female,Male,Female,Mix,,
18.0,3.0,Experimental Economics,20 June 2014,https://link.springer.com/article/10.1007/s10683-014-9410-3,Identity changes and the efficiency of reputation systems,September 2015,Matthias Wibral,,,Male,Unknown,Unknown,Male,"Online trade usually takes place between strangers, payment occurs before the good is shipped, and legal enforcement of an agreement may be prohibitively costly. This particular constellation generates moral hazard and adverse selection problems, especially on the seller side. Sellers have an incentive to ship a good of lower quality than promised or not to ship at all. Reputation systems are the most important tool in e-business to address these problems and to induce honest behavior among users. They store information about past conduct of a user provided by other users in a reputation profile and disseminate this information to the whole community. In principle, a reputation profile thus allows buyers to distinguish honest sellers from dishonest ones and to interact only with the former. The successful use of reputation systems in traditional markets with similar moral hazard problems can be traced back at least to the beginnings of long-distance trade in the Middle Ages (Greif 1989; Milgrom et al. 1990). Online reputation in most settings, however, differs from reputation in traditional markets in a very important way. Online reputation is only connected to the virtual identity of a person, i.e., the user name, and not the person itself. After a bad rating, a dishonest seller can comparatively easily abandon his old virtual identity and create a new account under a new user name with no reputation attached to it. The implications of this distinctive feature of online reputation for the efficiency of reputation systems are not well understood. At first sight, identity changes seem to severely weaken the disciplinary power of a reputation system. Dishonest behavior can still be punished with a negative rating but these ratings may lose their edge when they can easily be shed by creating a new identity. Real newcomers and sellers who have changed identity after a bad rating become indistinguishable. This argument suggests a high frequency of identity changes accompanied by opportunistic behavior and lower buyer trust. Theory on the other hand also provides some guidance on why reputation systems might still work effectively (Friedman and Resnick 2001; Ockenfels 2003). If buyers anticipate that dishonest sellers start over as new players, they will not trust newcomers or only interact with them at very unfavorable conditions, e.g., low sale prices. Starting as a new seller may then become so costly that cheating and creating a new identity is not profitable anymore. In this case, we would observe a high level of seller trustworthiness and no identity changes. Buyer trust would be high in transactions with experienced sellers and low towards new sellers. This creates a negative externality for honest new sellers. In this paper, we examine empirically how the option to change one’s virtual identity affects the efficiency of markets and the performance of reputation systems in inducing trust and trustworthiness. We study two experimental markets in which buyers play the trust game (Berg et al. 1995) with varying sellers. Buyers can rate sellers after each transaction. Before deciding whether and how much to trust a seller, a buyer can see the rating profile of this seller. New players enter the market over time in both treatments. The only difference is that sellers in one market can change their identity (change treatment), i.e., erase their rating profile and start over as new players, while in the other market this is not possible (no-change treatment). In view of the discussion above we aim at answering the following questions: Do sellers use the opportunity to change their identity? If so, does this go along with an increase in opportunistic behavior? How is buyer trust affected? In particular, are new members treated differently when identity changes are possible? These questions are of high practical relevance given widespread reports that dishonest behavior remains an important problem for online interaction.Footnote 1 According to a survey conducted by the European Commission, 49 % of internet users in the EU 27 countries are concerned about becoming a victim of online fraud, where goods are not delivered, counterfeit or not as advertised, and 19 % have actually become a victim (European Commission 2012). In an earlier survey (European Commission 2004), 21 % of the EU 15 population reported abstaining from buying online because they did not trust the internet. Recent studies on eBay’s reputation system (Reichling 2004; Jin and Kato 2006; Dellarocas and Wood 2008; Klein et al. 2009; Bolton et al. 2013) also indicate that the low percentages of neutral and negative ratings (e.g., Resnick and Zeckhauser 2002) grossly understate the true extent of problematic transactions since users may be reluctant to provide negative feedback for fear of retaliatory ratings (Masclet and Peenard 2012). Dellarocas and Wood (2008) estimate the true figure of mildly or very dissatisfied buyers at up to 21 %. It is difficult to determine whether these problems (also) arise because sellers can shed a negative reputation comparatively easily. So far, there is little evidence on the effects of identity changes, mainly because identity changes are inherently difficult to observe in field data. In addition, even if one could observe identity changes, comparing the same reputation system with and without the possibility to change one’s identity would be extremely difficult in the field. We therefore use a controlled laboratory experiment to address our research questions. The main findings can be summarized as follows. First, buyer trust and seller trustworthiness are significantly lower when sellers can change their identities. Trust is especially lower for new sellers. However, the reputation system in the change treatment maintains trustworthiness at a level that is high enough to make investing profitable for the buyers. The evidence is at least suggestive that trustworthiness is also higher than in the complete absence of a reputation system. Second, the basic principles of reputation systems function in both treatments. Lower trustworthiness translates into lower ratings, and buyers trust sellers with lower ratings less. Third, in the change treatment sellers with a sufficiently bad reputation circumvent lower buyer investments by switching identity and starting over as a new seller. Incentives to be trustworthy are therefore lower. Sellers seem to choose between two kinds of behavior—being trustworthy and maintaining a fixed identity, and behaving opportunistically and changing identity multiple times. The latter group drives the overall lower trustworthiness in the change treatment. Finally, opportunistic behavior does not pay off in the no-change treatment. In the change treatment, however, opportunistic players do earn more than their counterparts who share more equally with the buyer. We believe that this work contributes on several fronts. The results for the change treatment demonstrate that the efficiency of reputation systems in inducing trust and trustworthiness is reduced if sellers have the opportunity to change their identity. While the negative effects of identity changes have long been discussed theoretically in the literature (e.g. Dellarocas and Wood 2003), this paper provides controlled evidence and establishes a causal link between the possibility of identity changes and a higher incidence of dishonest behavior. This is the main contribution of our paper. Our results for the no-change treatment complement the findings of previous laboratory experiments on reputation systems without identity changes. Keser (2003), Bolton et al. (2004) and Masclet and Peenard (2012) find that a reputation system leads to high levels of trust and trustworthiness. We extend this finding to an environment in which new players enter the market over time: The mere fact that there are new players need not affect the efficiency of a reputation system as long as buyers can be sure that a new seller really is a person who has just entered the market. In addition, the controlled environment of the laboratory allows us to take a closer look at the mechanisms through which identity changes influence market outcomes. Several of our specific findings, which we can causally attribute to potential identity changes, are consistent with observations from online platforms. For example, Ockenfels (2003) finds a very low percentage of sellers with more negative than positive ratings in a sample from the platform half.com. In the same sample, new sellers also ask for lower prices than experienced sellers. Resnick et al. (2006) conduct a randomized field experiment on eBay, in which matched pairs of vintage postcards are sold via a high reputation identity and accounts with little or no previous feedback. Buyers are willing to pay 8 % more to the high reputation seller. While the design of their study nicely controls for many potential confounds that plagued earlier empirical studies, it is still unclear to which degree the effect is driven by personal experience of repeat customers of the high reputation identity. We can rule out this and other potential confounds in our setup. Finally, our results also show that a substantial part of the negative effects of a system which allows identity changes may arise from buyers simply refusing to participate in market interaction, thus effectively dropping out of the market. In this aspect, we complement survey evidence indicating that large efficiency losses occur because consumers do not trust online trade(rs). The rest of the paper is structured as follows. Section 2 describes the experimental design. Section 3 reports and discusses our results. Section 4 concludes.",13
18.0,3.0,Experimental Economics,31 July 2014,https://link.springer.com/article/10.1007/s10683-014-9411-2,Doing good or doing harm: experimental evidence on giving and taking in public good games,September 2015,Menusch Khadjavi,Andreas Lange,,Unknown,Male,Unknown,Male,"Andreoni (1995) has shown that subjects are more willing to cooperate in a giving than in a taking frame even if both are identical with respect to their potential outcomes. Dufwenberg et al. (2011), Park (2000), and Sonnemans et al. (1998) also identify differences between contribution decisions to public goods versus public bads.Footnote 1 Cox et al. (2013) however do not establish these differences when comparing games on public goods provision vs. common pool appropriation. In this paper, we replicate Andreoni’s finding using neutral language instructions. We add further insights into the motivations to give or take in a linear public good setting by varying the extent to which subjects actually can do good or do harm. In particular, we consider a situation where both giving and taking are possible. Examples where individuals may either contribute to a public good or reduce its provision include emitting environmental pollutants or reducing pollutant levels by investing in carbon offsets (Kotchen 2009), paying or evading taxes and taking actions as a manager that enhance the performance of a firm or that benefit only the manager personally while imposing a cost on the firm. A better understanding of individual behavior in both giving and taking domains is therefore crucial for creating institutions that secure a sufficient provision of public goods. With our set of experimental treatments, we confirm Andreoni (1995)’s finding that agents are more cooperative in a giving treatment with zero initial endowment in the public account than in the payoff-equivalent taking treatment where all wealth is initially allocated to the public account. However, when starting with an intermediate initial endowment in the public account, we find that subjects reach similar provision levels as in the giving treatment despite the presence of taking options. This indicates that the difference between a pure giving and a pure taking frame in Andreoni (1995) may be driven by the respective one-sided action spaces. These three treatments kept the set of potential payoffs identical and therefore simultaneously changed the set of available actions and the starting levels in both private and public accounts. To explore the impact of a pure extension of the action space to the taking domain, we add a final treatment that also starts with an intermediate level of the public account, but limits actions to the giving domain. Comparing this treatment with the intermediate give and take treatment, we find that allowing for taking leads fewer individuals to give strictly positive amounts even though the giving domain is kept unchanged. With this result, we extend findings on the impact of adding an opportunity to take from dictator games (List, 2007; Bardsley, 2008) to a linear public good setting. Our findings indicate that subjects evaluate their actions relative to the set of all feasible actions such that giving is not necessarily ‘doing good’ and taking is not necessarily ‘doing harm’. The remainder of this article is organized as follows. Section 2 presents the experimental design of the study. Results are presented and discussed in Sect. 3. Section 4 provides a concluding discussion.",42
18.0,3.0,Experimental Economics,24 July 2014,https://link.springer.com/article/10.1007/s10683-014-9415-y,Time as a medium of reward in three social preference experiments,September 2015,Charles N. Noussair,Jan Stoop,,Male,Male,Unknown,Male,"In economic experiments, participants are typically rewarded with cash payments. The choice of money has clear advantages over potential alternatives such as course grades, food products, or store coupons. Money can be allocated in positive and negative quantities. Indeed, utility is typically defined as equivalent to money income in many fields such as industrial organization and financial economics, and the use of money to incentivize experimental subjects in such cases arguably increases conformity with theoretical models. The use of monetary incentives readily allows a reward structure to be specified that conforms to the precepts of Smith (1982). The requirement that money be used as the reward medium is often stressed as an important feature distinguishing experimental economics from psychology.Footnote 1
 However, money is not the only scarce resource in the economy. Time is scarce for many individuals. It is subject to strict and inflexible global constraints, and the typical economic agent must make very frequent choices about how to allocate it. In this paper, we report three experiments in which we use waiting time as a medium of reward to incentivize subjects. All subjects in a session receive the same monetary payment regardless of their decisions, but the time at which they can leave the laboratory depends on the activity in the experiment. Staying in the laboratory is purposely made a boring activity. Subjects cannot read, use a computer, or listen to music. They all have to wait patiently in their enclosed, individual cubicle until the time they have been designated to wait is over. We study three well-known experimental paradigms, that are employed extensively to study social preferences: the dictator, ultimatum, and trust games. We consider behavior in these games when waiting time is at stake, and compare behavior between games with monetary and temporal stakes. The topic of social preferences with regard to time is of specific interest in its own right. In many situations, one’s own time might be allocated to benefiting or hurting another individual, or behaving pro-socially (anti-socially) can increase (decrease) others’ disposable time. Consider, for example, two employees who are charged with keeping a security watch over a facility. If it is sufficient that only one pays attention at any given time to ensure that the facility stays secure, then they face a bargaining problem over time. Or consider two employees, each of whom is responsible for a project on his own. If one helps the other with his project, and the other reciprocates by helping the first employee, both projects can be completed more quickly, increasing the free time available for both individuals. Economic relevance is not the only reason why the study of time is important. Time as a reward medium is of particular interest for methodological purposes for at least three reasons. First, time can be used to study the robustness of laboratory studies that use money to motivate subjects, to different reward media. In typical experimental studies, greater monetary payments are accorded to those participants who achieve greater levels of payoff in the theoretical model under investigation. Like money, waiting time satisfies the three precepts concerning the relationship between decisions and subject rewards that Smith (1982) proposes as necessary conditions for an experimenter to have control over incentives. The first precept, non-satiation, is the condition that, ‘given a costless choice between two alternatives, identical except that the first yields more of a reward medium than the second, the first will always be chosen over the second by an autonomous individual’ (Smith 1982, p. 931). Although Smith refers to monetary payments, it is reasonable to assume that the same condition holds for waiting time. Given a costless choice between two equally unpleasant alternatives, identical except that the first one takes less time to complete, an individual would presumably prefer the first one. The precept of saliency refers to ‘an individual’s right to claim a reward that is increasing (decreasing) in the goods (bads) outcome of an experiment’ (Smith 1982, p. 931). When the reward medium is time, the experimenter can satisfy this precept by making waiting times depend on choices in the experiment, with better choices translating into shorter waiting times. The condition of dominance is that ‘the reward structure should dominate any subjective costs associated with participating in the activities of an experiment’ (Smith 1982, p. 934). Waiting time can readily be made to satisfy this condition: the time a subject spends thinking about the decisions in an experiment can be made less than the gain in time that a subject can achieve from making a good decision. Second, the use of waiting time in the laboratory can create a special type of real effort experiment, which is especially simple, and in which the experimenter has good control. The use of real effort tasks has sometimes been advocated as a means of providing context and inducing costs of effort, when it is thought that monetary payments are ineffective in inducing the appropriate type of disutility. However, with most real effort tasks, there is scope for heterogeneity in participants’ ability or experience to influence their performance on the task or their cost of completing it. Using waiting time to induce disutility circumvents this problem. Finally, in some settings it is prohibited or not convenient to use money to motivate subjects. Time can offer a solution in those settings. For example, money incentives for minors or public employees are often not allowed. Alternatively, sometimes a researcher may, typically for budgetary planning reasons, be risk averse over the variation in experimental earnings to be paid. A fixed show-up fee and task related incentives in the form of time gives the researcher the ability to plan her monetary payments to subjects, in advance. We are aware of two other experimental studies in the social sciences that use time as a medium of reward. One investigates individual decisions under risk (Bruyneel et al. 2013). In that work, the authors investigate the concavity of the utility of waiting time. They find that subjects’ choices over time and money are similar when aggregate behavior is evaluated. Nonetheless, a considerable percentage of subjects value time linearly, but also value money in a concave manner. The other study is on the ultimatum game (Berger et al. 2012), and thus it has the strongest similarity to ours. The focus of Berger et al. (2012) is the effect of anonymity on proposals. Their results show that proposals are similar in a single- and in a double-blind procedure. They observe a modal proposal of an equal division of the waiting time. The overall pattern in our data is that the results obtained in studies with monetary incentives do extend to the settings with waiting time as the reward medium. This supports the contention that laboratory results have a particular form of internal validity, in that they are robust to changing the medium of reward while holding the rest of the structure of the experiment constant to the extent possible. This paper is organized as follows. Section 2 presents the design of the three experiments. In Sect. 3, the data is analyzed. Section 4 compares the media of reward time with money. Lastly, Sect. 5 concludes.",26
18.0,3.0,Experimental Economics,22 August 2014,https://link.springer.com/article/10.1007/s10683-014-9416-x,A penny for your thoughts: a survey of methods for eliciting beliefs,September 2015,Karl H. Schlag,James Tremewan,Joël J. van der Weele,Male,Male,Male,Male,"Economists are interested in the choices people make as well as the reasons they have for making these choices. Choice data alone are often not enough to understand these reasons and we need information about the beliefs of the decision maker. For example, first movers in ultimatum bargaining experiments may make high offers because they are altruistic, or because they believe the other party will reject low offers. Eliciting beliefs can help disentangle these hypotheses (Manski 2002). Belief measurement is also necessary when beliefs are an object of study in themselves, as in experiments about expectation formation and updating. In keeping with the mainstream models of decision making, economists commonly elicit beliefs as probabilistic statements. The underlying assumption is that beliefs take the form of subjective probabilities, or can be usefully expressed in this form. As Manski (2004) points out, probabilistic statements allow for comparisons with objective frequencies and an evaluation of the consistency of beliefs. A second characteristic of (experimental) economists’ elicitation methods is the use of rewards based on how well the reported belief matches actual events. This practice stems from the old idea that appropriately designed bets give the decision maker an incentive to report subjective beliefs truthfully (Ramsey 1926). In this article we give an overview of the available methods for the elicitation of subjective probabilities. In Sect. 2 we outline more formally the elicitation environment and discuss a class of mechanisms called scoring rules. Scoring rules reward subjects on the basis of the submitted report, and the actual realization of a random variable. In Sect. 2.2 we discuss so-called ‘proper scoring rules’ that have been designed to elicit probabilities, means, modes and various quantiles truthfully, under the assumption that the subject is risk neutral. In Sect. 2.3 we provide a new justification for a particular rule, the Quadratic Scoring Rule. In Sects. 2.4, 2.5 and 2.6 we discuss mechanisms that abandon the restrictive assumption of risk neutrality. In Sect. 2.7 we discuss promising extensions to the standard framework. We evaluate elicitation mechanisms empirically in Sect. 3, by looking at the ‘quality’ of elicited beliefs. Such evaluation is not straightforward, since by the nature of the exercise we do not know the right benchmark, i.e. the true belief of the subject. We discuss several alternative benchmarks that may be used to assess the effectiveness of elicitation mechanisms. In Sect. 4 we discuss practical issues in implementation such as the complexity and presentation of elicitation schemes. Finally, in Sect. 5 we tie our findings together and present some thoughts about the appropriate use of incentives in belief elicitation, and directions for future research on this topic. Our survey is complemented by other reviews of the broader aspects of belief elicitation. Manski (2004) focuses on questionnaires in consumer and household surveys. In this context, incentivized elicitation is typically not possible, since beliefs cannot be immediately verified. Garthwaite et al. (2005) and Jenkinson (2005) present a survey of mostly unincentivized elicitation techniques with a wide range of applications. Delavande et al. (2011) consider belief elicitation in studies with subjects in developing countries who may have low levels of numerical literacy. Gneiting and Raftery (2007) present a technical review of proper scoring rules, including a detailed discussion of decision theoretic foundations and statistical properties. Winkler (1996) focuses on a limited set of proper scoring rules and discusses scoring rules as a tool for ex-post evaluation of forecasts, an issue we ignore in this survey. Finally, there is some overlap between this paper and the survey by Schotter and Trevino (2014), which was written simultaneously and independently. While both studies discuss eliciting beliefs about the probability of an event, we also discuss mechanisms to elicit means, medians and other characteristics of belief distributions. Schotter and Trevino (2014) contains a more extensive treatment of the link between actions and beliefs and of second order beliefs, while this paper pays more attention to the empirical comparisons of different methods and practical issues of implementation.",128
18.0,3.0,Experimental Economics,27 August 2014,https://link.springer.com/article/10.1007/s10683-014-9417-9,Is it a norm to favour your own group?,September 2015,Donna Harris,Benedikt Herrmann,Jonathan Newton,Female,Male,Male,Mix,,
18.0,3.0,Experimental Economics,01 October 2014,https://link.springer.com/article/10.1007/s10683-014-9418-8,Voting with hands and feet: the requirements for optimal group formation,September 2015,Andrea Robbett,,,Female,Unknown,Unknown,Female,"Populations with diverse preferences must often form and sustain heterogeneous groups to take advantage of increasing returns to scale and capture gains from pooling resources. For instance, individuals with differing ideologies often join together to form a single political party or coalition, and neighboring towns often build single shared recreation facilities even though residents disagree on its ideal location. This paper uses laboratory experiments to study how agents partition themselves into groups when there are strong benefits to joining with others whose preferences for a group policy are not perfectly aligned with their own. Overall, the experimental results suggest that the institution determining how group policies are chosen can greatly affect whether agents reach optimal partitions. When group policies are fixed, such that individuals can vote only with their feet by moving between locations, subjects fully segregate by preference type. However, when mobility is combined with voting, such that individuals may influence the policy of the group they join, most subjects succeed in forming groups of the optimal membership composition. Two stability concepts are commonly considered side by side in the theoretical literature concerning the partitioning of agents across communities, clubs, or other groups. The concepts differ in whether a stable partition must be immune only to unilateral deviations or also to coalitional deviations. The first corresponds to the Nash equilibrium: a partition of agents is considered Nash stable if no agent can gain by unilaterally moving to a different group. Many Nash stable partitions typically exist and they are generally inefficient. The second, stronger stability concept corresponds to the strong Nash equilibrium (Aumann 1959) and requires that there is no set of individuals who could each do better by collectively relocating.Footnote 1 Such equilibria are Pareto efficient (though often nonexistent). If participants in a dynamic game myopically best respond to the previous state, then all Nash stable partitions will be absorbing. In other words, agents will be trapped in the first Nash stable state that they reach, even if more efficient outcomes could be possible with coordinated movement. It is unclear, however, whether agents will in fact myopically best respond in a dynamic group formation game, and experimental tests are thus necessary to determine which is the more appropriate stability concept. Will participants always remain in the first Nash stable partition they reach? Or will the system instead reach the more efficient strong Nash stable outcome? The goals of this paper are: first, to provide a preliminary assessment of whether inefficient Nash stable states are, in fact, absorbing in a dynamic group formation game, or whether agents tend to reach efficient stable states when such exist; and second, to consider whether the means of establishing group policies determines into which stable partition agents sort themselves. This paper considers a simple experimental environment in which subjects can move freely between groups with various local policies. There are several available locations that remain fixed for the duration of the experiment and subjects play a 20-period dynamic game. In each period, subjects simultaneously choose their location. They receive a payoff based on the number of other subjects who chose the same location and the group’s policy on some unidimensional issue. The policy is simply a number in the [0,1] interval, and subjects are assigned symmetric, single-peaked preferences over the interval at the start of the experiment. This policy can be interpreted as any outcome that applies indiscriminately to all group members, such as the local tax rate in a community, the platform of a political party or organization, the location of a club facility, or the type of good consumed within a club. The agents thus face a trade-off between being in a group of optimal size versus being in a group where the policy is closest to their ideal. The groups do not experience any congestion as membership grows, and so the optimal group size in this set-up is simply equal to the entire population. However, when agents have sufficiently divergent preferences and all members of a group are bound by the same local policy, they may receive higher payoffs by sorting into smaller groups with policies closer to their ideal. Two conditions are conducted, which differ in how group policies are chosen. In the Fixed Policy sessions, each location is associated with a fixed, posted policy, which all group members experience in each period that they are in the location. In the Voting sessions, the policy is chosen in each period by member vote. In the environment considered in this experiment, all Nash stable partitions will be sorted, in the sense that agents of similar types will locate in the same group. In other words, if one considers the range of ideal points represented in each group, these ranges will not overlap across groups. However, the specification of these ranges may be inefficient, such that some Nash stable partitions are Pareto dominated. In these experiments, the same set of Nash stable partitions exists in both the Fixed Policy and the Voting conditions. Additionally, the same unique strong Nash stable partition exists in both conditions and occurs when two groups form, each comprised of those whose ideal points fall within one-half of the [0,1] interval. In other words, the optimal outcome for the population requires the formation of groups with heterogeneous membership and an intermediate, compromise policy, but there exist other, less efficient, equilibrium partitions in which more groups form. Overall, I find that most subjects sort into a Nash stable partition. However, which partition they reach depends on how group policy is determined. I find that subjects who can vote only with their feet for group policies fail to partition themselves optimally. The subjects in the Fixed Policy sessions never succeed in reaching the optimal partition of two heterogeneous groups, and instead nearly all fully segregate into four homogeneous groups. This suggests that the existence of locations with fixed, posted policies facilitates the rapid sorting of agents by type into homogeneous groups, such as Tiebout envisioned, but may inhibit the formation of heterogeneous groups with compromise policies. Thus, fixing local policies may enable agents to successfully partition only when homogeneous groups are optimal, and may otherwise lead to over-segregation. In stark contrast, subjects who are able to vote on their local policies, as well as move freely between groups, typically succeed in forming groups of optimal size and membership composition: The majority of subjects in the Voting sessions reach the strong Nash stable outcome by the end of the session. The prevalence of optimal, heterogeneous groups in the Voting condition is due both to the ability of larger groups to persist by internally adjusting local policy to changes in their membership composition and to the ability of subjects to merge pre-existing groups by implementing compromise policies. This suggests that the ability of group members to influence local policy, without needing to relocate, may not only be necessary for assuring that a group attains its optimal policy for a given membership composition once it has already sorted, but is also necessary for the population to reach the optimal formation. The difference in outcomes between institutions occurs despite the equivalence in the set of Nash stable and strong Nash stable partitions under each. This suggests that how the population will partition cannot be assessed by solely considering the existence of stable states and that consideration must also be given to the system dynamics. In this case, it is the determination of local policy that alters these dynamics. Overall, subjects exit groups when they could have received higher payoffs elsewhere in the previous period, and unstable partitions rarely persist. Deviation from one of the inefficient Nash stable partitions is uncommon, and equally rare in both conditions. However, when such deviations do occur, the population is far more likely to transition to the efficient partition when the subjects are able to vote on local policy. I find that participants in the Voting condition respond less to the current size and policy of groups than participants in the Fixed Policy condition do, and more to the presence of likeminded types, with whom they might influence local policy. At the group level, groups in the Voting condition are both more likely to grow in population and less likely to shrink than groups in the Fixed Policy condition, controlling for group features. The treatment differences in both the persistence of larger groups and the likelihood of pre-existing groups to merge can be directly traced to the number of moves required to initiate a policy change. In the Fixed Policy condition, the only means by which subjects can build compromise groups or alter the policy within a pre-existing group is to move to a new location and hope to attract members. The system is thus more likely to become “stuck” in an inefficient partition, as even when subjects deviate from an inefficient stable partition the system quickly returns to the same partition. In the Voting condition, less movement is required to reach the same outcomes, as groups can internally adjust their policy as their membership changes without the need to form new groups. The rest of the paper is organized as follows: Sect. 2 reviews the related experimental and theoretical literature; Sect. 3 describes the experimental environment, procedure, and predictions; Sect. 4 presents the final group outcomes; Sect. 5 considers the dynamics that lead to these outcomes; and Sect. 6 concludes.",2
18.0,4.0,Experimental Economics,24 January 2015,https://link.springer.com/article/10.1007/s10683-014-9419-7,"Self-control, commitment and peer pressure: a laboratory experiment",December 2015,Aurélie Bonein,Laurent Denant-Boèmont,,Female,Male,Unknown,Mix,,
18.0,4.0,Experimental Economics,07 April 2015,https://link.springer.com/article/10.1007/s10683-014-9420-1,Eliciting beliefs in continuous-choice games: a double auction experiment,December 2015,Claudia Neri,,,Female,Unknown,Unknown,Female,"Beliefs play an important role in game-theoretic models of strategic decision-making. In equilibrium models, such as the Nash equilibrium, the outcome of a game is interpreted as a steady state where players hold correct beliefs about opponents’ behavior and act rationally by best responding to those beliefs. In learning models, such as the belief learning model, which focus on if and how a steady state outcome is reached, players update their beliefs about opponents’ actions by observing opponents’ past actions and then make their choice by best responding to those beliefs. When game-theoretic models are used to explain empirical or experimental choice data in the absence of beliefs data, proxies are used to represent beliefs. In the equilibrium framework, the proxy is provided by equilibrium beliefs defined in accordance with equilibrium strategies. In the belief learning framework, the proxy is provided by empirical beliefs computed from opponents’ past behavior. In recent years a literature on the probabilistic elicitation of subjective beliefs in games has emerged, with the aim to verify (otherwise non-verifiable) model assumptions and to explore whether elicited beliefs lead to better predictions of choice behavior than alternative proxies do. The literature has focused on normal-form games with a discrete-choice variable, either in a finitely-repeated (Nyarko and Schotter 2002; Rutström and Wilcox 2009; Hyndman et al. 2012; Danz et al. 2012) or in a one-shot setting (Costa-Gomes and Weizsäcker 2008; Rey-Biel 2009). This paper proposes a methodology to extend probabilistic belief elicitation to games with a continuous choice set, thus allowing for the use of beliefs data in a wider range of applications than the one covered by the existing literature. As an illustration, the methodology is applied to a uniform-price double auction experiment where multiple buyers and sellers with independent private valuations submit their bidding choices and state their probabilistic beliefs about other participants’ bidding choices. The elicited beliefs data then allow for the investigation of how beliefs affect bidding choices.Footnote 1
 Moving from a discrete- to a continuous-choice setting requires profound modification to the belief elicitation procedure. In a discrete-choice setting, probabilistic beliefs are represented by a discrete probability distribution and are elicited by having participants assign a probability to each possible element in the set. In a continuous-choice setting, instead, probabilistic beliefs are represented by a continuous probability distribution. In this paper, partial information about the subjective distribution is elicited by partitioning the choice set into intervals and requiring each participant to report the probability that opponents’ choices fall within each interval. A parametric distribution is then fitted to the elicited beliefs data in order to recover beliefs in the form of a fitted continuous subjective distribution. The procedure builds on previous work on non-strategic decisions and a survey setting (see the Survey of Economic Expectations and Engelberg et al. (2009)), modifying them to be appropriate for strategic decisions and an experimental setting. Belief elicitation within the double auction experiment enables us to compare elicited subjective beliefs with its proxies, Bayesian Nash equilibrium (BNE) beliefs and empirical beliefs.Footnote 2 Subjective beliefs differ from BNE and empirical beliefs not only in terms of the forecasts of other agents’ bidding choices but also in terms of the best-response bidding choices prescribed by beliefs. The extent to which elicited subjective beliefs help explain observed choices is evaluated comparing the consistency between choice and subjective beliefs to the consistency between choice and BNE or empirical beliefs. Choices are found to be more often consistent with the best response to subjective beliefs than with the best response to BNE or empirical beliefs. Deviation of choice from BNE best response is related to the deviation of subjective beliefs from BNE beliefs. Neither subjective beliefs converge to BNE beliefs nor choice converges to BNE best response. However, evidence suggests that convergence in beliefs may be necessary but not sufficient for convergence in choice, while convergence in choice may be sufficient but not necessary for convergence in beliefs. Despite helping explain observed choices, subjective beliefs have a low accuracy in predicting other subjects’ behavior, whether accuracy is measured by a scoring rule or by calibration. The belief elicitation design is relevant and applicable, beyond the context of a double auction, to settings that involve multiple players (possibly with different roles and types), a continuous choice set, and a payoff function that depends on a (possibly involved) function of players’ choices. Relevant settings may include market competition in oligopoly with incomplete information about production costs or bargaining with incomplete information about reservation values. The remainder of the paper is organized as follows. Section 2 describes the experimental design and the belief elicitation methodology. Section 3 presents the main results. Section 4 concludes.",6
18.0,4.0,Experimental Economics,06 November 2014,https://link.springer.com/article/10.1007/s10683-014-9421-0,"A survey of experimental research on contests, all-pay auctions and tournaments",December 2015,Emmanuel Dechenaux,Dan Kovenock,Roman M. Sheremeta,Male,Male,Male,Male,"Many economic, political and social environments can be described as contests in which competing agents have the opportunity to expend scarce resources—such as effort, money, time, or troops—in order to affect the probabilities of winning prizes. Examples range from the competition for mates, college admission, patents, or promotions within firms, to the process of litigation or lobbying politicians, to elections, sports competitions, and violent global conflicts (Tullock 1967; Krueger 1974). As is obvious from this list, these environments have attracted considerable attention in applications in a wide range of fields, both in- and out-side of economics. They have also been studied extensively by economic theorists in what has become known as the field of contest theory (Konrad 2009). Although this field continues to attract many young theorists, it has its roots in three models developed in the mid-seventies to early eighties: the Tullock (1980) model of rent-seeking, the Lazear and Rosen (1981) rank-order tournament model, and the all-pay auction (Hirshleifer and Riley 1978; Nalebuff and Stiglitz 1983; Dasgupta 1986; Hillman and Riley 1989). Despite the fact that the three models historically developed somewhat independently, they represent special cases of a general contest model that can be formulated in a unified framework. Despite an extensive and established theoretical literature, much less effort has been devoted to empirically investigate individual behavior in different contests and compare such behavior with theoretical predictions. The main reason is that it is not trivial to measure individual effort in the field since the researcher can only observe the performance of contestants, which is a function of effort, ability and luck (Ericsson and Charness 1994). The majority of empirical studies use either firm level data (Prendergast 1999) or sports data (Szymanski 2003). Because it is typically difficult to measure the actual effort expended by players in the field, almost all of these studies focus solely on investigating whether the pattern of outcomes is consistent with the theoretical predictions. Controlled experiments allow researchers to test contest theory without confounding effects and endogeneity issues. Some experiments allow direct measurement of individual effort, while controlling for the relative abilities of contestants and the amount of noise (luck) in the tournament. The first studies to test contest theory using laboratory methods were conducted by Bull et al. (1987) and Millner and Pratt (1989). These studies have inspired a substantial and rapidly developing experimental literature on contests. The purpose of this paper is to survey this work. We begin by identifying the three canonical contest models and their common applications. The assumptions underlying the three contest models lead to vastly different equilibrium behaviors. Tullock (or lottery) contests and rank-order tournaments usually have pure strategy equilibria for the specifications applied, whereas all-pay auctions have only non-degenerate mixed strategy equilibria (in the case of complete information). Moreover, the models have traditionally been applied to different areas of economic analysis. The term Tullock or lottery contest has been commonly used in the study of R&D races and political or rent-seeking competitions. Rank-order tournaments (or sometimes tournaments) have been used in the principal-agent, contract design and labor literatures. Therefore, resources exerted in the process of competing in these contests are usually called efforts or expenditures. All-pay auctions have been used in the auction literature and in lobbying and military applications. Resources exerted competing in all-pay auctions are usually called bids or expenditures. In any given application, contest expenditures may be viewed as a good or bad from the standpoint of the contest designer. For instance, when modeling political or rent-seeking competition, contest expenditure is often viewed as social waste, in the sense that a welfare maximizing social-planner would seek to minimize it (Tullock 1980). In contrast, in management applications where rank-order tournaments have often been applied, effort is viewed as valuable because it contributes to the firm’s output. Similarly, for patent races a social planner may desire the positive externalities generated from increases in R&D spending. Finally, in many all-pay auction applications expenditure is viewed as desirable, such as the case of charitable fundraising or a seller of an object engaging in an all-pay auction to maximize revenue. Consequently, in some applications of contests the designer may be interested in maximizing expenditure and in some cases minimizing expenditure. Although applications of the three canonical models are usually different, all three models assume that (i) players exert costly irreversible efforts while competing for a prize and (ii) an individual player’s probability of winning the prize depends on the players’ relative expenditures. Obviously, the exact probability of winning the prize is defined differently for the three contests and is determined by a contest success function that maps the vector of player expenditures to the probability of winning. In the all-pay auction, the player exerting the highest effort wins the prize with certainty. In the rank-order tournament, the player with the highest performance, which is the sum of effort and a random component, wins the prize with certainty. Finally, in the Tullock
contest, the probability of winning equals the ratio of a player’s effort to a fixed power r ≥ 0 to the sum of each of the other players’ efforts, each raised to the same power r. The special case where r = 1 is the case of the lottery contest. We survey 231 experimental papers on contests; for a complete list of references see the working version of this paper (Dechenaux et al. 2012).Footnote 1 The majority of these papers are already published or in press (159 papers), while other papers are still cited as working papers (72 papers). More than 90 % of the working papers have been written within the last 5 years. Figure 1 displays the time trend of papers published in academic journals. The figure indicates a dramatic increase in the number of published papers over the last decade, with more than 50 % of the papers published in the last 5 years. The vast majority of published experimental studies are conducted in the lab (87 %) employing chosen-effort experiments (75 %). Some experimental studies are conducted in the field (13 %), with more than 70 % of the field studies published within the last 5 years. Out of 158 published papers, 35 % of the papers are based on lottery contests, 25 % are based on all-pay auctions, 25 % on rank-order tournaments and about 15 % of the studies examine other contest structures (usually using binary decisions or real-effort tasks). Time trend of papers published in academic journals We begin by introducing the three canonical contest models in Sect. 2. There are four important lessons that we learn from studies discussed in Sect. 2: (1) most studies on lottery contests and all-pay auctions find significant overbidding relative to the Nash equilibrium prediction; (2) in contrast to lottery contests and all-pay auctions, there is little overbidding in rank-order tournaments and aggregate effort usually conforms to the theoretical predictions; (3) in all three canonical contests there is significant heterogeneity in the behavior of individual subjects; (4) in lottery contests and rank-order tournaments bids are usually distributed around the equilibrium, while in all-pay auctions the distribution of bids is bimodal, with some subjects submitting very low and others submitting very high bids. In Sect. 3 we present experimental studies investigating the basic structure of contests, including the number of players and prizes, spillovers and externalities, heterogeneity, risk and incomplete information. The main lesson that we learn from the experiments discussed in Sect. 3 is that most studies find support for the comparative statics predictions of contest theory. Some commonly supported comparative statics results are the impact of incomplete information on individual behavior, the “discouragement effect”, and the impact of contest parameters (e.g., the number of players and the number of prizes) on effort. Then we identify some common areas of focus within the literature and present a general review of relevant studies. For example, Sect. 4 reviews contests with a dynamic structure, such as sequential move contests, wars of attrition, multi-stage contests, patent races, best-of-n contests, multi-stage elimination contests, and contests with endogenous entry. Section 5 reviews experimental studies on games of multiple contests with linkages, such as Colonel Blotto games and multi-battle contests. Section 6 presents natural extensions of the experimental analysis of contests, such as sabotage, feedback, bias, collusion, alliances, group contests, and gender effects. Section 7 reviews field experiments on contests. Section 8 discusses a number of applications of contests, including litigation, political campaigning, lobbying, wars, sales, and charity giving. Finally, Sect. 9 suggests several directions for future research, including an examination of (1) the sources of overbidding in lottery contests and all-pay auctions, (2) the reasons for little (or no) overbidding in rank-order tournaments, (3) potential overbridging in field settings with real-effort and high stakes, and (4) potential mechanisms of de-escalation, deterrence, management and resolution of conflicts.",392
18.0,4.0,Experimental Economics,21 December 2014,https://link.springer.com/article/10.1007/s10683-014-9422-z,Balancing the current account: experimental evidence on underconsumption,December 2015,Marcus Giamattei,Johann Graf Lambsdorff,,Male,Male,Unknown,Male,"Current account imbalances are currently an issue of wide political debate among the member states of the Eurozone (European Commission 2013). While some countries tend to consume excessively and generate a current account deficit, others consume little and run a current account surplus. The debate can follow two possible paths. The first option is to put under pressure deficit countries who consume too much, urging them to implement adjustment programs, cut down on public or private expenditures, reduce the import of goods and services, and this way mitigate a current account deficit. This is the current practice in the Eurozone (European Commission 2012). The focus lies on “policies [which] must avoid current account deficits in the future” (Feldstein 2012). This has led to deficit countries burdening the major costs of adjustments within the Eurozone (De Grauwe 2013). The second option is to coerce surplus countries into increasing consumption and demand for products from deficit countries. This would equally balance the current account.Footnote 1 Currently, this is not implemented in the Eurozone; Germany, for example, is hardly sanctioned for its current account surplus. We investigate experimentally whether the choice of either of these options actually impacts consumption in the long-run. Might one of these options bring about a deflationary bias, a quantity of aggregate demand that falls short of desirable or rational levels as suggested by Keynes (1943), Greenwald and Stiglitz (2009), Bibow (2009, pp. 160–161) among others? We ask these questions in light of the current depression in the Eurozone, where levels of inflation and growth of GDP are persistently low. This idea of sanctioning both surplus and deficit countries was first expressed during the Bretton-Woods negotiations in 1944 by John M. Keynes. He advocated what has become known as the “Keynes Plan”, which entailed symmetric penalties that should be imposed on surplus and deficit countries (Keynes 1943; Cartapanis and Herland 2002; Piffaretti 2009; Mateos y Lago et al. 2009). But the Keynes Plan was vetoed by the USA, represented by Harry D. White. Since 1944, the question on how to share the burden of adjustment among deficit and creditor countries has repeatedly been an issue and has gained prominence lately, following the 2007/08 financial crisis (United Nations 2009, pp. 109–112; Mateos y Lago et al. 2009; Stiglitz and Greenwald 2010; Caliari 2011). In his role as Governor of the Bank of Canada, Mark Carney (2009) noted that “it is generally much less costly, economically as well as politically, for countries with a balance of payments surplus to run persistent surpluses and accumulate reserves than it is for deficit countries to sustain deficits”. Observing that pressure is put asymmetrically on deficit countries, the International Monetary Fund (2010, p. 9) concluded that “if the counterpart of reserve accumulation is that many countries pursue current account surpluses, an aggregate deflationary impact may emerge to the extent that the rest of the world is no longer willing to incur balance of payments deficits”.Footnote 2
 We investigate how the institutional arrangement of punishing deficit or surplus countries influences the deflationary bias. For this purpose we designed an experiment where subjects playing in groups of six and assuming the role of heads of state are supposed to set levels of consumption. They seek to set appropriate levels of consumption and balance their current accounts. We either penalize subjects in proportion to the extent of the current account imbalance irrespective of being a deficit or a surplus (Keynes treatment) or impose a penalty only asymmetrically on subjects with a deficit (White treatment). Both treatments have identical dominance solvable Nash equilibria. The way in which they are sanctioned has no impact on equilibrium because rational players seek to avoid any current account imbalance. But balancing the current account is not an easy task. It depends not only on one’s own consumption but also on that of all the others. Consumption in other countries determines aggregate demand and creates opportunities for other countries to export goods and services. The current account as the difference between total domestic demand and domestic production (including exports) will thus only be balanced if beliefs regarding the behavior of others are correct. With many countries interacting with each other the correctness of such beliefs can be a bold assumption (Stiglitz 2011). We thus conjecture rationality to be limited. Subjects may fail in correctly determining the level of consumption or may expect others to fail. Therefore their choices may be anchored by disequilibrium values. In line with this conjecture, we observe persistent underconsumption (a deflationary bias) in the White treatment and a slower adjustment towards a balanced current account.Footnote 3
 We examine these questions experimentally for three reasons. First, this allows for a test of policies while avoiding the macroeconomic costs that might result from a real-world implementation (Cornand and Heinemann 2014, p. 4; Amano et al. 2014). Second, behavior that is outside equilibrium, which is difficult to approach theoretically, can be investigated. Third, real world data on a deflationary bias are commonly contaminated by a variety of ad hoc political measures. Central banks react by adjusting the interest rate, fiscal consolidation might already be taking place and the private sector might be anticipating future reform. This creates problems with endogeneity. For example, pressure on surplus countries may arise in reaction to a severe deflationary bias, such that data on a deflation are not well observable. This renders it arduous to identify the causal effect a policy change. For these reasons, laboratory macroeconomic experiments have gained prominence lately (Duffy 2012; Cornand and Heinemann 2014; Amano et al. 2014). Our study thus covers new ground in identifying how pressure on surplus countries can ameliorate a deflationary bias and speed up adjustment. The experimental design is explained in Sect. 2 and its link to a complete macroeconomic model is explained in Sect. 3 (and Appendix 1). Section 4 derives our hypotheses that depart from Nash equilibrium predictions. Section 5 portrays our procedures. Our data is described and analyzed in Sect. 7. Section 8 concludes and discusses policy recommendations.",3
18.0,4.0,Experimental Economics,11 January 2015,https://link.springer.com/article/10.1007/s10683-014-9423-y,Institution design and public good provision: an experimental study of the vote of confidence procedure,December 2015,Chloe Tergiman,,,Female,Unknown,Unknown,Female,"Delays due to unruly legislative majorities hinder one of legislatures’ primary objectives: appropriating funds to projects. As a result, some legislatures have adopted procedures that increase voter cohesion among members of the majority. The vote of confidence procedure is a mechanism present in many parliamentary democraciesFootnote 1 that gives the governing party leadership more control over the members of the ruling coalition. This procedure allows the proposer of a bill to link the fate of that bill to the fate of the ruling coalition. Thus, if a bill has a vote of confidence procedure attached to it, members of the majority who want to be certain to remain in the majority and who would otherwise vote against the bill may still vote in favor of it. In this paper I show that the vote of confidence procedure has negative consequences on public good provision. Members of the ruling coalition are guaranteed to stay in the ruling coalition only if the bill on the floor passes. Thus, they have additional incentives to vote in favor of a bill and their vote is cheaper in the eyes of the proposer. Therefore, while the vote of confidence increases voting cohesion it also decreases public good provision: because the proposer can extract higher rents from the members of the majority, public goods have to be more attractive to compete with earmarks (legislation that favors the districts of a small number of legislators). Public good provision is an essential and defining component of governance and while policy makers may have private preferences when it comes to earmarks or public good spending, legislative rules may indeed change their voting behavior. The purpose of the present paper is to isolate the role and impact of such a rule and show that while there is some advantage to designing rules that temper personal preferences and allow majority leaders to “control” their majorities, such rules may come at a cost, specifically to public good provision. In order to do so, I extend the Diermeier and Feddersen (1998) model to include public good provision.Footnote 2
 The budget allocation procedure is modeled as a finite period game where a fixed budget is allocated at each period.Footnote 3 In the first period a ruling coalition is proposed and voted on. If the proposal is rejected, in the following period a (possibly) different ruling coalition is proposed and voted on. This repeats until a ruling coalition is accepted. In the period after that, a member of the ruling coalition is randomly chosen to bring a budget spending bill to the floor under closed rule, and all members of the legislature vote on this bill. The proposer of a spending bill can choose to invest the government budget in a public good or use it for earmarked projects, simply dividing the budget among members of the legislature of his choosing. If the bill passes, in the next period a (possibly different) randomly chosen legislator brings another budget spending bill to the floor and so on. If one such bill is rejected, then if there was a vote of confidence procedure attached to it, all legislators go back to choosing a ruling coalition.Footnote 4 If there is no vote of confidence procedure attached to it, then the ruling coalition stays in place and its members continue to propose spending bills until the last period. I derive the equilibrium of the model described above and then test its implications in a laboratory experiment. With the vote of confidence procedure, relative to a situation without it, the model predicts that: fewer public goods are offered; proposers can extract higher rents from other members of the legislature; the distribution of earmarked projects is much more unequal, and that despite that, members of the ruling coalition still vote in favor of such bills. While in both treatments, public goods are offered more often than the model predicts, fewer public goods are offered overall with the vote of confidence procedure, a direct prediction of the model. Thus, while the point predictions are not supported by the laboratory data, the laboratory results robustly support the comparative static predictions of the model. Furthermore, in the treatment with the vote of confidence procedure, proposers extract higher rents, on average about 7 % from each member of majority. In the vote of confidence treatment, members of the ruling coalition are also more likely to vote in favor of a proposal compared to the case where there is no vote of confidence procedure, particularly if they are offered “low” rents. Finally, I use the data to show that the increase in voter cohesion due to the vote of confidence procedure also comes at the cost of a 23 % decrease in public good provision. The question of how the legislative process impacts public good provision is one that has received little attention.Footnote 5 As in Baron and Ferejohn’s (1989) alternating offer of bargaining model, the literature on the impact of the legislative process has mostly focused on earmark policies in the absence of a public good. Only rarely have models explicitly incorporated the tension between using a budget for earmarking versus providing public goods. Volden and Wiseman (2007) and Lizzeri and Persico (2001) are notable exceptions.Footnote 6
 Experimentally, Frechette et al. (2011) test the Volden and Wiseman (2007) model and to my knowledge is the only experimental paper that has looked at public good provision in a legislative setting. There is, however, a larger body of experimental work that examines earmark policies, with the objective of evaluating proposer power, the redistribution of the budget, and more generally the Baron and Ferejohn (1989) model. Such experimental papers include McKelvey (1991), Diermeier and Morton (2004), Battaglini and Nunnari (2012), Agranov and Tergiman (2014, 2015). The theoretical models mentioned above are usually very simplified and not very suitable for empirical analysis. Thus, empirical studies on the topic of government stability and distribution of budgets generally describe stylized facts outside of any modelling framework.Footnote 7 There are two exceptions to this. Diermeier et al. (2003) estimate a bargaining model of government formation to study the effect of various political institutions on government stability. They use their model to show that the presence of votes of no-confidence procedures reduces the number of minority governments, which are less stable.Footnote 8
\(^{,}\)
Footnote 9 Becher and Christiansen (2014) show some empirical support for the fact that the threat of dissolution increases the proposer’s legislative strength. The rest of the paper is organized as follows: in Sect. 2, I present the model as well as the testable predictions. In Sect. 3, I describe experimental design. In Sect. 4, I test these predictions against laboratory data. I offer a discussion of the results in Sect. 5. Section 6 concludes.",10
18.0,4.0,Experimental Economics,18 December 2014,https://link.springer.com/article/10.1007/s10683-014-9424-x,Can you spare some change for charity? Experimental evidence on verbal cues and loose change effects in a Dictator Game,December 2015,David Fielding,Stephen Knowles,,Male,Male,Unknown,Male,"Among Dictator Game studies in which the recipient is a charity, there is significant variation in the reported average level of generosity. Some studies (for example Reinstein and Riener 2012; Tonin and Vlassopoulos 2013) analyze the extent to which this variation can be explained by differences in experimental protocols, finding that even subtle differences can have a significant effect on the amount donated. However, little attention has been paid to the extent to which verbal cues have an effect on experimental behavior that differs from visual ones. In most Dictator Game experiments, the invitation to donate is made verbally. This contrasts with real-world charity fundraising, in which there is sometimes a purely visual cue (for example a charity box standing by the checkout in a store) and sometimes a combination of verbal and visual cues (for example when the checkout operator asks customers if they would like to make a donation). The potential importance of the distinction between visual and verbal cues is suggested by recent research in evolutionary biology. One theory of the evolution of spoken language is that it developed out of a need to co-ordinate mutually beneficial actions (Tomasello 2008), and it is possible that speech is associated with neural mechanisms that also have a role to play in empathy. Gazzola et al. (2006) find that certain areas of the brain respond both during the execution of an action and when hearing sounds associated with that action, an example of “mirror neuron” activity (Kohler et al. 2002) thought to be important both in language acquisition and in the expression of empathy. Gazzola et al. report that auditory mirror neuron activity is stronger in subjects who score more highly on an empathy scale. Moreover, visual cues can be easier to ignore than verbal cues, even in situations where the decision to donate is anonymous, if people wish to preserve an altruistic self-image (Dana et al. 2006, 2007). Therefore, this paper reports the results of an experiment designed to test whether the addition of a verbal cue elicits more generosity than a simple visual one, maintaining donor anonymity in both treatments. We find a very large and statistically significant verbal communication effect. A second overlooked source of variation in the protocol of Dictator Game experiments is the amount of loose change the participants have. There is evidence from other types of experiment that people’s propensity to spend sometimes depends on how much loose change they are carrying (Raghubir and Srivastava 2009; Vandoros 2013) and a similar effect could influence charitable giving. For example, a customer at the checkout might put all (or most) of the change she receives into the charity box, regardless of how much change has been given. Therefore, this paper also reports results from a second treatment designed to measure the size of the “loose change effect” on individual generosity. The results are consistent with such an effect, but suggest that it is smaller than the effect of verbal communication.",13
18.0,4.0,Experimental Economics,30 December 2014,https://link.springer.com/article/10.1007/s10683-014-9425-9,Propose with a rose? Signaling in internet dating markets,December 2015,Soohyung Lee,Muriel Niederle,,Unknown,Female,Unknown,Female,"
In many matching markets, candidates vying for positions inundate employers or schools with applications, making it difficult to decide which candidates to offer a limited number of interviews, job offers, or admission slots. For employers or schools, it therefore becomes important to assess not only the quality but also the attainability of an applicant. To help make this assessment, many markets have formal or informal preference signaling mechanisms in place.Footnote 1 Although a growing number of papers theoretically study preference signaling,Footnote 2 the empirical literature has had difficulty proving that an agent has more success when the agent uses a signal (e.g., Avery et al. 2003, for U.S. college admissions; Coles et al. 2010, for the American Economic Association (AEA) signaling; Roth and Xing 1997, for the U.S. market for clinical psychologists; and Niederle et al. 2006, for the U.S. Gastroenterology Fellowship match).Footnote 3 Furthermore, in order to use preference signaling as a market design instrument, it is also critical to understand the behaviors of market participants in terms of how actively they use preference signaling as well as how effectively they use signaling to improve their outcomes. However, existing studies provide little information on how market participants use preference signaling, largely due to data limitations.Footnote 4 The goal of this paper is to narrow this gap in two ways. First, we provide clean empirical evidence that sending a preference signal can considerably improve one’s chances of success. Second, because we observe all behaviors of market participants in our field experiment setting, we are able to present detailed behavioral patterns regarding the use of preference signaling. These two pieces of information are necessary ingredients for signaling mechanisms to be valuable and to be promoted in market design. We conduct a field experiment in online dating, where individuals can express a non-binding special interest to a limited number of potential dates. A large online dating company organizes two dating events with 613 participants, about 50 % of whom are female. All participants are endowed with two “virtual roses” and a randomly chosen 20 % of participants are endowed with eight. A participant can send dating requests to up to ten different people by sending a pre-made electronic note, a proposal. Participants can attach at most one virtual rose, a digital image icon, when sending a proposal. The roses are described as a way to show special interest. Hence, roses are signals that everyone can send for free to anyone, and roses are costly only because they are in limited supply. If sending a preference signal increases a person’s success in getting a date, then we expect, all else being equal, that attaching a rose to a proposal improves the chance of that proposal being accepted. 
Compared to other environments, our set-up offers three major advantages for testing the impact of preference signaling. First, we have the same information about potential dating partners (and often even more) available to a participant. This is one of the distinguishing features of our environment. In general, market participants such as colleges or universities tend to have more detailed information about an applicant (e.g., application essay, high school performance, job market paper) than the researcher does. Second, even though the market is decentralized, we observe not only accepted proposals but all proposals, because the market operates on the dating company’s website. This is an unusual amount of information for a decentralized market. Third, we are able to randomly select participants whom we endow with eight roses (versus two).Footnote 5 This randomization will allow us to make a clean causal inference of the effect of preference signaling, mitigating the concern over potential endogeneity bias. These three features provide us with cleaner evidence than previous studies to test whether sending a non-binding signal can enhance the signaler’s chance of receiving a coveted date, college admission, a job interview, or a job offer. Furthermore, our study of online dating may itself be economically relevant because an important economic variable, marriage, is a result of dating,Footnote 6 and because online dating services are rapidly growing throughout the world (see Oyer 2014). The experiment consists of two special online dating sessions in South Korea for people who are college-educated, never-married, aged between 26 and 38 for men, and 22 and 34 for women. We impose restrictions on participants’ characteristics to create a thick market because heterogeneity in observables may potentially segment the dating market. For the first 5 day of the event, a participant can browse profiles and send up to ten proposals. A proposal can be sent with at most one rose attached. Participants have two roses they can attach to proposals, with a randomly selected 20 % of participants having eight roses. Once this period ends, each participant receives his or her proposals and observes whether they come with a rose. For the next 4 day, participants decide whether to accept each proposal; they can accept at most ten proposals. After the acceptance phase, an accepted proposal results in the company sending a text message to provide the involved pair with each other’s contact information. Motivated by the discussions in the preference signaling literature, we hypothesize that preference signaling will increase the acceptance rate; the positive effect will be large, particularly if a person sends a signal to an agent whom may think the person is “too good”; and signaling will increase the total number of matches, instead of crowding out other offers. We test these hypotheses using our data and find supporting evidence. To test our hypotheses, we need to define the extent to which a person is desirable as a spouse/dating partner. Then, we can measure the effect of roses, for example, by comparing a dating offer accompanied by a rose with another offer without a rose, when those two offers were sent by two equally desirable persons. In our environment we have a clear desirability measure of participants. This measure is provided by the online dating site and has been validated by Lee (2009) using a much more comprehensive dataset than the one used in this study. We use this desirability measure to classify experimental participants into one of three groups—bottom (the least desirable group), middle, or top (the most desirable group). We also use alternative measures of desirability, and our findings below are robust. We find that, all else being equal, sending a proposal with a rose attached increases the probability that a recipient will accept the proposal by 3.3 % points, which corresponds to a 20 % increase in the acceptance rate. This effect is similar in magnitude to the increase in the acceptance rate when the dating offer comes from a sender in the middle, rather than bottom, category. Furthermore, we confirm the positive of effect of roses by showing that participants endowed with eight roses instead of only two are more successful in that they initiate more dates. Next, we show that every recipient group responds positively to roses when the proposals are made by senders from a higher desirability group. That is, when a sender from the top desirability group makes an offer to a middle or bottom group recipient, this offer is significantly more likely to be accepted when a rose is attached. The same is true for offers from middle senders to bottom recipients. The effect of a rose in all those instances is more than a 50 % increase in the acceptance rate, which corresponds to twice the increase in the acceptance rate when moving the sender from the bottom to middle desirability group. We also show that these positive effects of roses are neither because the roses attract attention from recipients nor because they are associated with unobserved quality. We therefore provide evidence that roses can serve as a way to signal special interest and increase the chance of an offer being accepted. Finally, we find that individuals who received at least one rose accepted more dating offers than their counterparts who did not receive a rose, suggesting that roses may increase the total number of matches, rather than crowding out other offers. Despite the positive effects of roses on acceptance rates, participants in our experiment did not use roses strategically. Thirty-two percent of male participants and 69 % of female participants did not exhaust their rose endowments: they sent some dating requests without a rose, although they had roses left. Furthermore, approximately 30 % of roses were sent in vain to top group recipients who did not positively respond to them. Given our earlier finding that roses do not crowd out other offers, there exists a potentially large gain from educating dating market participants on how to strategically use preference signaling. Our experiment on Internet dating provides clear evidence that by sending a preference signal, a proposer can increase the chance of being accepted when everyone can send signals for free but signals are limited in number. It appears that senders are able to convey information to recipients using preference signals, and recipients react to these signals. These are the necessary ingredients for a signaling mechanism to affect a market. Given the present evidence, as well as the multitude of suggestive evidence that we review in the discussion section, it seems that preference signaling has a place in the toolkit of market designers.",29
18.0,4.0,Experimental Economics,09 May 2015,https://link.springer.com/article/10.1007/s10683-015-9441-4,Erratum to: Multi-period experimental asset markets with distinct fundamental value regimes,December 2015,Thomas Stöckl,Jürgen Huber,Michael Kirchler,Male,Male,Male,Male,"In the original version of the paper (currently available online first) we report incorrect values of RAD, RD, SPREAD, VOLA, and ST for one out of five treatments, namely \({\mathrm{R5}}(\mathcal{N}).\) In this corrigendum we present the corrected values in the respective tables and detail changes in the reported results. Note that we only update those tables containing incorrect values and that only values concerning \({\mathrm{R5}}({\mathcal{N}})\) are subject to changes. Overall, the values of RAD and RD reveal that markets are more efficient than previously reported, while values of SPREAD, VOLA, and ST are close to the values previously reported. The changes necessitate small correction in the results section listed in Table 1.Footnote 1
",
18.0,4.0,Experimental Economics,14 July 2015,https://link.springer.com/article/10.1007/s10683-015-9446-z,Erratum to: Incentives for creativity,December 2015,Sanjiv Erat,Uri Gneezy,,Male,Male,Unknown,Male,"In our paper “Incentives for Creativity” (Erat and Gneezy 2015), we failed to cite some very relevant recent papers in experimental economics. We wish to correct this in the discussion below. Bradler et al. (2014) run a laboratory experiment with over a thousand subjects looking into how rewards affect performance in a creative and in a routine task. They find that tournament incentives work well in both tasks, suggesting that creative performance is not subject to motivational crowding out. They also look at unconditional wage gifts. Interestingly, wage gifts induce reciprocity only in the routine task. They run additional treatments to investigate this asymmetry and find that it is the uncertainty about ones exact performance, and, hence, the lack of control over the back-transfer to the principal, that inhibits reciprocity in the creative task. Charness and Grieco (2012) consider the effect of incentives on individual creativity. They present a series of experiments on creativity where subjects face creativity tasks where, in one case, ex-ante goals and constraints are imposed on their answers, and in the other case no restrictions apply. The effect of financial incentives on creativity is then tested. Their findings provide striking evidence that financial incentives affect “closed” (constrained) creativity, but do not facilitate “open” (unconstrained) creativity. Laske and Schröder (2015) investigate how incentives affect creative performance. They introduce a novel real effort task that allows to objectively quantify performance in multiple dimensions of creative work, i.e. quantity, quality, and novelty. In three treatments and a baseline, they separately incentivize each dimension by introducing piece-rate incentives. They find that incentivizing quantity and incentivizing novelty have a positive effect on both quantity and novelty. They find negative spillover effects of incentivizing quantity on the quality. The increases in quantity is in line with payoff-maximization, while the other effects seem to be due to distortion of effort. Combining all three dimensions of creativity, they find that incentivizing novelty results in the highest overall creative output. Eckartz et al. (2013) compare performance in a word based creativity task under three incentive schemes: a flat fee, a linear payment and a tournament. Furthermore, we also compare performance under two control tasks (Raven’s advanced progressive matrices or a number-adding task) with the same treatments. In all tasks we find that incentives seem to have very small effects and that differences in performance are predominantly related to individual skills.
",4
18.0,4.0,Experimental Economics,23 October 2015,https://link.springer.com/article/10.1007/s10683-015-9471-y,Erratum to: Quantal response equilibria for extensive form games,December 2015,Richard D. McKelvey,Thomas R. Palfrey,,Male,Male,Unknown,Male,"This erratum corrects Tables 7 and 8 of our paper, “Quantal Response Equilibrium for Extensive Form Games,” which appeared in this journal, Vol. 1, No. 1, pp. 9–41 (1998). We have corrected the last column of numbers in each table, which are the estimated take probabilities for the 2-parameter model. There are two other minor changes. The estimated value of lambda for this model has also been corrected. (The likelihood values were reported correctly in the original.) Also, in the fifth row of the first column of Table 7, \(\sigma^2\)  has been corrected to \(\gamma. \) We are grateful to James Friedman and Claudio Mezzetti for discovering these errors and bringing them to our attention.",5
19.0,1.0,Experimental Economics,17 April 2015,https://link.springer.com/article/10.1007/s10683-015-9429-0,Lossed in translation: an off-the-shelf method to recover probabilistic beliefs from loss-averse agents,March 2016,Theo Offerman,Asa B. Palley,,Male,Male,Unknown,Male,"Accurately obtaining subjective probabilistic information about uncertain future events is an essential step in the decision making process in many different economic and public policy settings. In many cases, rather than trying to build a model to estimate probabilities, the best and most informative assessments come from an agent who has a good amount of relevant experience and can use her collected wisdom to estimate a subjective probability. Eliciting this information presents an important and difficult problem in many fields such as finance and macroeconomics (Diebold and Rudebusch 1989; Ghysels 1993), decision analysis (Keeney 1982), and meteorology and weather forecasting (Murphy and Winkler 1984). In addition, probability assessments often comprise an important component of economic experiments. Even when the ultimate objective is not to elicit subjective beliefs, obtaining this information may be a critical secondary step in an experimental procedure. Well-designed scoring rules provide a useful tool for procuring this subjective information by providing an agent with the right incentives to carefully evaluate and quantify her beliefs, and to honestly reveal her subjective assessment of the likelihood of these uncertain future events. The quadratic scoring rule (QSR), a variant of which was first introduced by Brier (1950), is the most commonly used.Footnote 1
 The incentive design of scoring rules implicitly assumes, however, that the agent is risk neutral, which contrasts with how people often behave. Winkler and Murphy (1970) examine the effects of nonlinear utility on the optimal report under a proper scoring rule, showing that risk aversion leads an agent to hedge her reports away from categorical forecasts of 0 and 1 and risk seeking leads the agent to bias her reports closer to 0 or 1. This biasing effect of risk preferences can be easily corrected by applying the inverse utility function to the scoring rule (Winkler 1969). In practice, however, an even more troubling pattern of excessive reports equal to the baseline probability of 1/2 emerges as well, a phenomenon not explained by classical expected utility models. For example, Offerman et al. (2009) tested responses by 93 subjects to a QSR for objective probabilities that ranged from 0.05 to 1 and found that they reported 1/2 more than three times as often as they should have (15.3 % versus 5 %). This particular type of conservatism inhibits the  decision maker’s ability to discern among a broad domain of moderate beliefs and conceals a significant amount of useful information. In this paper, we employ the insights of prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992) to understand the ways in which an agent will distort her report when she receives an uncertain reward from a QSR. Employing Palley’s (2015) model of prospect theory with an endogenous reference point, we highlight how loss aversion can account for why an agent may both report 1/2 for a range of moderate beliefs and bias her reports toward 1/2 for beliefs closer to 0 or 1. 
The main contribution of our paper is the introduction of a generalized asymmetric QSR, the  L-adjusted rule, which eliminates the incentives for conservative reports and enables the elicitation of true probabilistic beliefs. The payoffs in this  L-adjusted rule are the same as in a classical QSR except negative outcomes are shrunk by a factor of  L, a parameter that controls the size of the loss adjustment. We use previous experimental work estimating population parameters to derive an off-the-shelf variant of this  L-adjusted QSR that requires no prior agent-specific calibration. In an experiment, we demonstrate its effectiveness in recovering truthful and precise probability assessments, and show that it alleviates the shortcomings associated with the classical QSR. In agreement with previous results, we find that in response to the classical QSR, agents tend to report the implicit benchmark probability of 1/2 for a wide range of beliefs near 1/2 in order to ensure a certain payoff. By matching the choice of L to previous empirical estimates of parameters for the overall population, we also obtain a modified QSR that recovers truthful beliefs experimentally. In doing so, we provide a practical and simple off-the-shelf scoring rule that encourages agents to report their beliefs truthfully. We want to emphasize that the use of the L-adjusted QSR is as easy as the use of a standard QSR. Exactly as with a standard QSR, each subject receives a table that lists how their payoff varies depending their probability judgment and the actual outcome of the predicted phenomenon. The only difference between a standard QSR and an  L-adjusted QSR is that the actual payoffs in the table are changed to accommodate subjects’ loss aversion. As a result, subjects are encouraged to automatically report judgments that are very close to true objective probabilities. The simplicity of our approach depends to a large extent on the fact that we provide each subject with the same  L-adjusted QSR based on parameter estimates for the general population. A natural question is how much precision is sacrificed by ignoring differences that may exist in people’s loss-aversion attitudes. To investigate this question, we include a treatment in which we adjust the scoring rule separately for each subject on the basis of an individually estimated loss parameter. Interestingly, we do not find better results for this treatment. Recently, several related approaches have been suggested to recover true beliefs from conservative reports. Offerman et al. (2009) propose a revealed preference technique that allows the researcher to correct the reported beliefs of agents who are scored according to a standard QSR. In this method, agents initially provide reports for a range of objective probabilities, which then yields an optimal response mapping that can be inverted and applied to infer subjective beliefs from later reports. In an experiment, Offerman et al. demonstrate the effectiveness of this approach in recovering beliefs from reports that do not equal the baseline probability of 1/2. Kothiyal et al. (2011) extend this method to overcome the problem of discriminating between moderate beliefs in a range around the baseline probability of 1/2, for which agents give the same optimal report. By adding a fixed constant to one of the QSR payoffs, they both eliminate the excess of uninformative baseline reports and yield an invertible response mapping that makes possible the recovery of true beliefs, while maintaining the properness of the original scoring rule. Kothiyal, Spinu, and Wakker do not provide an experimental test of their method. The approaches taken in Offerman et al. (2009) and Kothiyal et al. (2011) are precise and elegant because they do not need to make structural assumptions on how people make decisions under risk. The downside of these methods is that they are laborious to employ, because a sufficiently dense risk-correction map has to be derived for each agent before any inferences can be made. In both decision analysis and many experimental economics applications, the elicitation of beliefs is a secondary goal, and a simpler and quicker approach may be preferred, as long as it does not sacrifice precision. The method presented in this paper pursues this purpose. Other elicitation methods that do not make use of scoring rules exist as well. For example, if the utility function is unknown, Allen (1987) presents a randomized payment method that relies on the “linearization” of utility through conditional lottery tickets to incentivize truthful reports. Alternatively, Karni (2009) proposes a procedure with two fixed prizes where the payment function is determined by comparing the agent’s report to a random number drawn uniformly from [0,1], analogous to the Becker et al. (1964) mechanism. Under this method, if the agent exhibits probabilistic sophistication, she has a dominant strategy to report her true belief, irrespective of her risk attitudes. However, in experiments, subjects have been found to have a hard time understanding Becker-DeGroot-Marschak-type procedures (Rutström 1998; Plott and Zeiler 2005; Cason and Plott 2012), and empirical comparisons of these methods with scoring rules have yielded mixed results (Hao and Houser 2010; Hollard et al. 2010; Trautmann and van de Kuilen 2011). The rest of the paper is organized as follows: Sect. 2 introduces our  L-adjusted QSR and characterizes the corresponding optimal reporting policy under the prospect theory model of risk behavior. We discuss how this predicted behavior provides a parsimonious explanation of previously observed conservative reporting patterns and how the parameter  L can be calibrated to allow for the recovery of estimates of any probabilistic belief. Readers who are interested mainly in how well our method encourages subjects to simply report true probabilities may skim Sect. 2 and refer to Proposition 1 and Corollary 1. Sections 3 and 4 detail the experiment that we carried out to test the usefulness of this adjusted scoring rule in practice and demonstrate its improvements over the classical QSR. Section 5 concludes and Appendix 1 characterizes reporting behavior for the general asymmetric  L-adjusted QSR and contains proofs of all results. Appendix 2 in Supplementary Material provides images and instructions from the experimental interface.",3
19.0,1.0,Experimental Economics,05 March 2015,https://link.springer.com/article/10.1007/s10683-015-9434-3,Complexity and biases,March 2016,Kenan Kalaycı,Marta Serra-Garcia,,Male,Female,Unknown,Mix,,
19.0,1.0,Experimental Economics,10 January 2015,https://link.springer.com/article/10.1007/s10683-014-9426-8,Directed search with heterogeneous firms: an experimental study,March 2016,Andrew Kloosterman,,,Male,Unknown,Unknown,Male,"In many labor markets, whether through firms’ reputations or through actual posting of wage offers in advertisements, workers know the offered wage at various jobs before they decide to which of these jobs they will apply. Directed search models this phenomenon as an alternative to random search in which the application behavior of job seekers is not influenced by wages. The central strategic consideration of directed search, for both firms and workers, is a tradeoff between payoffs and the probability of receiving them. Firms want to post a low wage to get a large profit, but know that doing so may deter applicants. Workers want to apply to jobs that offer high wages, but know that such offers may attract more applicants. If both sides of the market are homogeneous, the symmetric equilibrium predicts that all firms will offer the same wage and that workers, who observe no wage dispersion, apply to each job with equal probability. The equilibrium does not ultimately display any wage dispersion so the firms all settle on the same tradeoff and workers face no tradeoff at all. However, if some firms are more productive than others, then the symmetric equilibrium does create the tradeoff on both sides of the market; the more productive firms offer higher wages and the workers apply more often to these higher wages. Perhaps even more importantly, the symmetric equilibrium with heterogeneous firms introduces new issues for efficiency. Matching frictions lead to efficiency loss regardless of firm productivity. However, for the case of heterogeneous firms only, directed search is a mechanism for more productive firms to signal their productivity to workers. In the equilibrium, these firms do indeed offer higher wages which garner more applications. In consequence, there is less efficiency loss than predicted by random allocation, the outcome of random search in this environment. This paper addresses the tradeoff and efficiency with results from a laboratory experiment on a directed search game that extends the model of Burdett et al. (2001) (hereafter BSW)Footnote 1 by adding heterogeneity to the firm side of the market. The main results in the experiment are as follows. First, workers apply to the job offering the higher wage more often than the job offering the lower wage though not as often as theoretically predicted. Second, more productive firms offer approximately the predicted wage on average while less productive firms offer a little less than the predicted wage so there is significant wage dispersion. There is also much variation in the offers. Finally, the markets fail to decrease efficiency loss from the case of random allocation so the signaling mechanism that directed search introduces is not effectively implemented. An investigation of efficiency reveals that it is variation in firm offers that cause the efficiency loss. As such, policies that target firm behavior, such as a minimum wage, could be effective in this environment. Interestingly, the non-equilibrium behavior of workers is actually mitigating the over-dispersion of wages so policies that target worker behavior, such as unemployment insurance, could actually be detrimental to efficiency. This is the first experiment to test the consequences of firm heterogeneity in a directed search market. Cason and Noussair (2007) (hereafter CN) tests directed search in the laboratory with homogeneous firms. The present experiment focuses on the theoretical consequences of heterogeneity, which, as noted above, are quite extensive and important. However, CN provides a good baseline to which the results can be compared. While CN is by far the most closely related experiment, several other experiments are worth mentioning. A number of experiments investigate similar problems; search with frictions through buyer costs (Grether et al. 1988; Davis and Holt 1996; Abrams et al. 2000; Cason and Friedman 2003) or coordination problems like the workers’ problem (Ochs 1990, 1995a for an overview of market entry games). Other experiments investigate similar questions. Network games limit trade opportunities (Deck and Johnson 2004; Cassar 2007; Charness et al. 2007; Corbae and Duffy 2008; Cassar and Rigdon 2011) while directed search limits matching opportunities. Market games extend the classic ultimatum game by introducing competition either among the proposers (Roth et al. 1991) or the responders (Guth 1997; Grosskopf 2003) while directed search introduces competition to both sides of the market. In addition to BSW, a number of early papers pioneered the theory of directed search (Peters 1991; Montgomery 1991; Moen 1997). More recently, the use of directed search has become quite popular for labor theorists (for example Shi 2009; Menzio and Shi 2010, 2011). While these models are far more complex than the model tested here, the proliferation of directed search suggests that laboratory experiments such as this one can be useful. The paper is organized as follows. Section 2 presents the model and then solves for the equilibrium and efficient solutions. Section 3 describes the experiment. Section 4 presents the results. Finally, Sect. 5 concludes. Instructions for one of the sessions are appended.",7
19.0,1.0,Experimental Economics,08 February 2015,https://link.springer.com/article/10.1007/s10683-014-9427-7,Individual characteristics and behavior in repeated games: an experimental study,March 2016,Douglas Davis,Asen Ivanov,Oleg Korenok,Male,Male,Male,Male,"Laboratory experiments on repeated games have shown that there is substantial heterogeneity across subjects: some cooperate a lot while others hardly do so (e.g., see Dal Bó and Fréchette (2011a, b) and Davis et al. (2010)).Footnote 1 This raises the question of whether the behavior of subjects in repeated games is related to other individual characteristics. We address this question by focusing on an array of individual characteristics that are popular in economics and may plausibly be related to behavior in repeated games: risk attitude, time preference, trust, trustworthiness, altruism, strategic skills in one-shot matrix games, compliance with first-order stochastic dominance, ability to plan ahead, and gender. Research on the relationship between individual characteristics and behavior in repeated games could be useful in at least three ways. First, it could provide insights into what motivates the different kinds of behavior in repeated games. Second, such research could tell us whether (i) at the individual level, a player’s individual characteristics help predict her behavior in repeated games and (ii) at the pair level, the individual characteristics of a pair of players help predict to what extent cooperation would emerge if these players are matched to play a repeated game. Third, such research could be used to guide theoretical developments on repeated games. To better understand the sources of heterogeneous play, we conducted a laboratory experiment in which each subject attended two sessions. In session 1, subjects played for several matches a two-player repeated game based on a “Mini-Bertrand” stage game, a three-price version of the regular Bertrand game.Footnote 2 Following the repeated Mini-Bertrand (RMB) games, subjects played for several matches a repeated Prisoner’s Dilemma (RPD). In session 2, subjects performed an array of tasks meant to measure the individual characteristics of interest. We do find some systematic relationships between individual characteristics and behavior in repeated games. At the individual level, a subject’s compliance with first-order stochastic dominance as well as, possibly, patience, gender, and altruism have some systematic effects on her behavior in repeated games. At the pair level, each subject’s gender as well as, possibly, patience and ability to choose an available dominant strategy in a one-shot matrix game systematically affect the frequency of the cooperate-cooperate outcome. None of the remaining individual characteristics systematically affect behavior at either the individual or the pair level. Overall, the number of systematic relationships we find is surprisingly small. Our paper adds to a small, but growing, experimental literature exploring the connection between behavior in indefinitely repeated games and individual characteristics that are popular in economics.Footnote 3 Dreber et al. (2011) find that, when cooperation is an equilibrium with selfish preferences, behavior in the RPD is largely unrelated to giving in the dictator game, answers to survey questions about prosocial behavior outside the lab, and individual characteristics (such as age, belief in God, and risk attitudes); it also appears that, in line with our study, men are more cooperative than women.Footnote 4 Reuben and Suetens (2012) and Cabral et al. (2012) provide evidence that behavior in indefinitely repeated games is mostly driven by strategic motives rather than by other-regarding preferences or non-strategic reciprocity.Footnote 5
 In contrast with our findings, Sabater-Grande and Georgantzis (2002) find that cooperative behavior in the RPD is negatively correlated with risk aversion. The discrepancy may be due to the fact that Sabater-Grande and Georgantzis used a version of the RPD that is somewhat different from the version used in our study. In their study, the first 15 rounds were played with shrinking payoffs of the stage game and a continuation probability of 1; in the remaining rounds, the stage game payoffs were unchanged and there was a positive probability of the game ending after each round.Footnote 6
 Our study contributes beyond previous studies in that we consider a wide array of additional individual characteristics that are popular in economics. In particular, to the best of our knowledge, no previous study has looked at the connection between, on the one hand, behavior in indefinitely repeated games, and, on the other hand, patience, trust, trustworthiness, strategic skills in one-shot matrix games, compliance with first-order stochastic dominance, and ability to plan ahead. The remainder of the paper is organized as follows. In Sect. 2, we explain the experimental design. Section 3 explains how we quantify subjects’ behavior in repeated games. Section 4 examines the possible connections between the individual characteristics we consider and behavior in repeated games. Section 5 contains the data analysis. Section 6 concludes.",16
19.0,1.0,Experimental Economics,09 January 2015,https://link.springer.com/article/10.1007/s10683-014-9428-6,Trust and reciprocity: extensions and robustness of triadic design,March 2016,Giovanni Di Bartolomeo,Stefano Papa,,Male,Male,Unknown,Male,"In trust and gift exchange games, scholars argue that data indicating that many proposers send and responders return money provide evidence for positive reciprocity. However, as Cox (2004) notes, this conclusion is weak. “The source of the difficulty is that the single-game experimental designs used to generate the data in these experiments do not discriminate between actions motivated by trust or reciprocity and actions motivated by other-regarding preferences characterized by altruism or inequality aversion that is not conditional on the behavior of others.” For instance, in an investment game, investors may send positive amounts because they wish to trigger trust mechanisms (conditional) or simply because they are motivated by altruism or inequality aversion (unconditional).Footnote 1
 Cox (2004) proposes a triadic design to distinguish conditional from unconditional motivations by comparing the outcomes of an investment game to those arising from counterfactual scenarios in which actions can only be unconditional.Footnote 2 Specifically, Cox attempts to obtain evidence for (net or conditional) trust and reciprocityFootnote 3 by assuming that actions in dictator games are exclusively driven by unconditional motivations. Then, evidence for trust is obtained in the form of a positive difference between the average amounts sent by investors and dictators. Similarly, evidence for reciprocity is identified as the difference between the average amounts sent by trustees and dictators in a game in which the experimenter designed the dictators’ initial endowments to replicate those faced by the trustees in the investment game.Footnote 4
 The evidence from triadic designs is somewhat mixed. Although certain studies report evidence of conditional motivations,Footnote 5 experiments based on the triadic design often fail to observe trust and reciprocity.Footnote 6 These results appear to suggest that a substantial share of what has commonly been interpreted as trust-based transfers may be exclusively motivated by altruism (Brulhart and Usunier, 2008, 2012). However, as Cox (2004) notes, the logic of the triadic design is to provide sufficient but not necessary conditions allowing an outside observer to conclude from experimental observations that subjects have exhibited trust or reciprocity. Thus, (Brülhart and Usunier 2008, 2012) argument is not definitive. As Cox (2004) provides sufficient conditions, the aim of our paper is to validate his approach by considering the following two extensions: We introduce the strategy method (SM) in addition to the direct-response method (DM) to collect additional information on the trustees’ actions We collect information on participants’ expectations to assess their coherence with the indications derived from the triadic design Our motivations are explained in greater detail in the remainder of this section. One possible explanation for the mixed empirical evidence obtained from the triadic design is “noise”—at least for reciprocity. The trustee should decide how much to return to the investor given the amount he received (initial conditions). Then, reciprocity can be identified by comparing his action to the choice of a dictator derived from a counterfactual situation: a dictator game with the same initial condition (i.e., initial degree of inequality)Footnote 7 imposed by the experimenter. When aggregating the choices of the experimental and control groups, noise can emerge because the initial conditions are endogenous, as they are determined by the investors’ choices. Specifically, the experimental and control groups are obtained by aggregating the responses of trustees and dictators, respectively. Although the choice of each trustee is, by construction, made while considering the same initial condition of the corresponding dictator, each pair can be associated with a different initial condition according to the choice of the investor. Thus, sample outcomes are conditional to a given vector of initial conditions, which is itself a random variable. If Cox’s experiment is replicated, different trustees’ reactions may simply depend on a different vector of initial conditions. If inequality plays a role,Footnote 8 the results will then be highly dependent on the specific degree of inequality generated in the sample. To eliminate this noise, it is necessary to consider a larger sample or an experimental design that allows for inequality.Footnote 9
 The above problem can be overcome by following Stanca et al. (2009). A more sophisticated approach to collecting information on trustees’ actions is to use the SM proposed by Selten (1967)—in addition to the DM used by Cox (2004). Specifically, the SM consists of asking the trustee to make conditional choices for each feasible investor action (i.e., each possible degree of inequality) before being informed of his actual choice. From the SM, we can then verify the existence of reciprocity by comparing the difference between the average amount sent by trustees and the average amount sent by counterfactual dictators while controlling for each degree of inequality (i.e., initial conditions).Footnote 10
 We also test the robustness of the results obtained from the triadic design using the coherence of participants’ expectations—as suggested, e.g., by Coricelli et al. (2006). The intuition is simple: if investors are involved in a “real” investment, we expect that they would always send an amount lower than their expected payoff. Then, trust observed in the triadic design should be associated with a positive difference between average expected payback and the amount sent by the investors. By contrast, if we do not observe trust in the triadic design, we should also observe that investors’ expected gains are zero or negative to conclude that motivations are exclusively related to altruism. Finally, information derived from expectations can be used to further explore the investors’ and trustees’ choices. For instance, combining observations from SM, DM and expectations, we can investigate issues such as emotional bias due to unfulfilled expectations; i.e., we can check for the existence of a type of punishment (or other-regard) when the trustee receives less (or more) than he expected. The remainder of the paper is structured as follows. Section 2 describes the experimental design. Section 3 presents and discusses our main outcomes. Section 4 concludes.",9
19.0,1.0,Experimental Economics,03 March 2015,https://link.springer.com/article/10.1007/s10683-015-9430-7,Preference submission timing in school choice matching: testing fairness and efficiency in the laboratory,March 2016,Jaimie W. Lien,Jie Zheng,Xiaohan Zhong,Female,,Unknown,Mix,,
19.0,1.0,Experimental Economics,22 February 2015,https://link.springer.com/article/10.1007/s10683-015-9431-6,Past experience of uncertainty affects risk aversion,March 2016,Friederike Mengel,Elias Tsakas,Alexander Vostroknutov,Female,Male,Male,Mix,,
19.0,1.0,Experimental Economics,26 February 2015,https://link.springer.com/article/10.1007/s10683-015-9432-5,An eye-tracking study of feature-based choice in one-shot games,March 2016,Giovanna Devetag,Sibilla Di Guida,Luca Polonio,Female,Female,Male,Mix,,
19.0,1.0,Experimental Economics,05 March 2015,https://link.springer.com/article/10.1007/s10683-015-9433-4,Overcoming coordination failure using a mechanism based on gradualism and endogeneity,March 2016,Yoshio Kamijo,Hiroki Ozono,Kazumi Shimizu,Male,Male,,Mix,,
19.0,1.0,Experimental Economics,28 March 2015,https://link.springer.com/article/10.1007/s10683-015-9435-2,Under- versus overconfidence: an experiment on how others perceive a biased self-assessment,March 2016,Carmen Thoma,,,Female,Unknown,Unknown,Female,"Many earlier psychological and social–psychological studies claim that people are overconfident, which has sometimes been labelled the better-than-average effect.Footnote 1 However, recent economic studies analyzing choice behavior instead of verbal reports, show that individuals are able to estimate their skills correctly or even underestimate it.Footnote 2 A biased self-assessment can lead to systematic biases in individuals’ decision making, thus the topic is of high interest to economists.Footnote 3 Yet, barely any research has been done to analyze how overconfidence in comparison to underconfidence is perceived by others, and whether individuals adapt their reported self-assessment to others’ perception. I use a controlled laboratory study to address this topic, thereby focusing on two aspects: first, I analyze whether underconfident individuals are perceived as more or less likeable than overconfident individuals. Secondly, I explore whether under- or overconfidence is perceived as a stronger signal for ambition and effort. As one’s self-assessment might be a signal of one’s actual performance, and therefore not the self-assessment but the estimated performance might be rewarded, I control for performance as follows: the individuals compared exhibit the same relative rank based on their performance in a task, conducted at the beginning of the experiment. My findings contribute to the expanding literature analyzing the advantages of overconfidence in comparison to underconfidence. Thereby I add to the questions why individuals might (rationally) exhibit a bias in their reported self-assessment and in which situations we should expect individuals to over- or underestimate themselves. How reported over- or underconfidence is perceived by others is difficult to analyze in the field as self-confidence interacts with other characteristics in many ways. The anonymous laboratory setting allows me to separate the causal effects of over- and underconfidence on others’ appraisal, by only varying the accuracy of subjects’ reported self-assessment. The experiment consists of two parts. In the first part all subjects perform an incentivized real effort task which serves as the basis for their self-assessment. In the second part two thirds of the subjects (agents) are assigned a rank based on their relative performance, whereas each rank is assigned to two subjects. The two agents having the same rank are assigned to one of the remaining participants (principals). Both agents estimate their relative rank and the principal learns by how many ranks each of them over- or underestimated himself.Footnote 4 In treatment SYMP the principal chooses to whom of the two agents he wants to give 5 Euros. In treatment PERF the principal has a monetary incentive to choose the agent who performs better in a repetition of the real effort task. The only information the principal gets is the deviation of the agents’ self-assessments and the information that both agents have the same actual rank. This element of the design is essential as subjects on higher ranks are more likely to be underconfident, while subjects on lower ranks are more likely to be overconfident, due to the limited scale for self-assessment. If subjects’ actual ranks differed, principals might choose the underconfident agent not because they prefer underconfidence, but because underconfidence might signal a higher actual rank. The results show that it can be advantageous to be underconfident with respect to the perception of others. In SYMP principals reward the underconfident agent significantly more often than the overconfident agent. In PERF principals bet on the underconfident agent significantly more often than on the overconfident agent. Questionnaire data reveals that underconfidence is preferred over overconfidence, and that the less self-confident agent is expected to exert more effort to improve himself, while the more self-confident agent is expected to rest on his high self-perception. I also analyze whether the antipathy towards overconfidence is anticipated by eliciting agents’ (incentivized) beliefs of principals’ selection choices. Moreover, to test whether agents strategically bias their self-assessment in order to increase their selection chances, I conduct two control treatments without monetary incentives to be selected by the principal. Agents’ beliefs in PERF show that they do not expect underconfidence to signal a higher performance than overconfidence. Correspondingly, there is no difference in self-assessment between PERF and its control treatment. In contrast, subjects anticipate that underconfidence is rewarded significantly more often than overconfidence, and men state marginally significantly lower ranks in SYMP than in the non-strategic control treatment. Yet, there is no difference in self-assessment for women. One possible explanation is that that women do not downgrade their self-assessment strategically. Yet, I rather suggest that they even lower their self-assessment in the non-strategic setting, as its accuracy is still observable. Thus, they might still be afraid that their image might suffer when being overconfident.Footnote 5 Furthermore, women and men might downgrade their self-assessment in non-strategic settings due to an idea which goes back to Myerson (1991). He suggests that people internalize optimal behavior from certain situations and behave the same way in similar but different situations.Footnote 6 Thus, it might be the case that women have somehow imprinted the social norm of modesty and even downgrade their self-assessment in environments in which the (monetary) need for modesty is absent.Footnote 7
 There is an extensive and expanding literature on overconfidence. One strand of this literature analyzes whether people are overconfident (e.g., Clark and Friesen 2009; Hoelzl and Rustichini 2005; Svenson 1981), thereby focusing on the definition of overconfidence, the appropriate measurement, and influencing factors (see e.g., Benoît and Dubra 2011; Moore and Healy 2008). This paper is rather related to the literature identifying potential consequences of a biased self-assessment. Within this literature many papers focus on non-payoff maximizing decisions caused by a biased self-assessment, e.g., overinvestment, value-destroying mergers of CEOs (Camerer and Lovallo 1999; Malmendier and Tate 2005, 2008; Odean 1999), and suboptimal selection of payment schemes (Dohmen and Falk 2011; Niederle et al. 2013; Niederle and Vesterlund 2007), or work environments (Niederle and Yestrumskas 2008). Another strand of the literature, mainly theoretical work, identifies utility enhancing aspects of being overconfident, providing (behavioral) explanations for overconfidence at the same time. Overconfidence may directly enhance well-being (Akerlof and Dickens 1982; Brunnermeier and Parker 2005; Caplin and Leahy 2001; Koszegi 2006), boost one’s motivation and willpower (Bénabou and Tirole 2002; Brocas and Carrillo 2000), or increase performance (Compte and Postlewaite 2004). Only very few recent papers consider the impact of one’s self-assessment on others, and whether individuals account for others’ perception when stating their self-assessment. Ewers and Zimmermann (2012) theoretically and experimentally analyze whether individuals bias their self-assessment due to image concerns. They find that individuals state a higher self-assessment if reports (but not true performances) are observed by an audience (anonymity is lifted) in comparison to the situation in which reports are private.Footnote 8 Yet, they find that individuals do not bias their reported self-assessments if the true performance is also publicly revealed. Thus, in their paper subjects do not try to signal high confidence or modesty if the accuracy of one’s self-assessment is observable. However, in contrast to my study, in Ewers and Zimmermann (2012) individuals have no monetary incentives to strategically bias their reported self-assessment. Moreover, their study does not analyze how one’s reported self-assessment is perceived by others, i.e., if one’s social image or expected ability is actually increased by stating a higher self-assessment. The perception of self-assessment and the strategic incentives to bias one’s self-assessment are addressed in an experimental study by Charness et al. (2012). They investigate whether individuals bias their stated confidence about their performance to deter or motivate others to enter a two-player tournament, and whether others react to it. They find that males inflate their stated confidence when deterrence is strategically optimal, and that men and women deflate their confidence if encouraging entry is strategically optimal. Moreover, they observe that individuals are less likely to enter the competition, the higher the stated confidence of the other person is. In line with these results Reuben et al. (2012) observe that men inflate their self-assessment to be voted as the group leader, which turns out to be a successful strategy.Footnote 9 However, in contrast to my experimental design, in Charness et al. (2012) and Reuben et al. (2012) individuals’ actual performance is unknown and might strongly differ. Expecting all individuals to exhibit the same bias in self-assessment, the ranking of subjects’ self-assessment most likely corresponds to the ranking of subjects’ true performance. Thus, it is not the bias in self-assessment, which reveals information about the actual performance, but the self-assessment per se. This is different in my experiment, in which both agents have the same actual (relative) performance, enabling me to investigate whether the bias in self-assessment serves as a performance signal. In many real-life situations, in which individuals have to assess their performance, e.g., in promotion interviews or wage negotiations, the true performance is somehow appraised or at least partly known. Thus, the accuracy of the self-assessment might also be evaluated. To the best of my knowledge this study is the first, to experimentally test whether a principal prefers an over- or an underconfident agent. Yet, theoretical studies exist, providing different predictions. Gervais and Goldstein (2007) suggest that skill and effort are complements, thus an overconfident agent makes a higher effort choice due to underestimating the cost of effort or overestimating his marginal productivity. Sautmann (2013) suggests that overconfident agents overestimate their expected payoff, thus receiving higher incentives with the same wage. In contrast, Santos-Pinto (2008) suggests that a positive self-image and effort are substitutes, as an overconfident agent thinks that he has to exert less effort for the same outcome than an underconfident agent. This paper might also contribute to the literature on gender differences in self-assessment.Footnote 10 I suggest that the observed antipathy towards overconfidence adds to the explanation of the gender difference in reported self-assessment as women report to experience emotions, i.e., the negative attitude towards overconfidence, more intensely than men (see e.g., Brody 1997; Grossman and Wood 1993). In addition, they might even be punished more harshly than men when being self-confident (Eagly 1987; Rudman 1998). My results could moreover provide an explanation for the findings of Ludwig and Thoma (2012) who observe that women are ashamed to overestimate themselves in public, while men have not. The rest of this paper is structured as follows. In the next Sect. I describe the experimental design and the two different treatments. In Sect. 3 I present the selection behavior of principals. In Sect. 4 I analyze whether agents anticipate principals’ preferences and whether they strategically bias their self-assessment. I conclude in Sect. 5.",8
19.0,1.0,Experimental Economics,05 May 2015,https://link.springer.com/article/10.1007/s10683-015-9436-1,On the interpretation of bribery in a laboratory corruption game: moral frames and social norms,March 2016,Ritwik Banerjee,,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Experimental Economics,03 May 2015,https://link.springer.com/article/10.1007/s10683-015-9440-5,Incentives for creativity,June 2016,Sanjiv Erat,Uri Gneezy,,Male,Male,Unknown,Male,"Creativity is central to many activities, including entrepreneurship and research and development at large corporations. Can incentives be used to improve the creative process? If you are in charge of choosing a compensation scheme for a worker who needs to come up with a creative solution to a problem, should you compensate the worker based on her creative performance? And if so, what type of incentives work best? In this paper, we compare the effect of different incentives for being creative. We start with perhaps the simplest form of incentives, piece-rate incentives based on creativity ratings, and we test whether creativity improves when one introduces such incentives relative to simply asking people to be creative without offering them any extrinsic incentives. Next, we consider competitive incentives, which offer a more convex link between performance and reward than piece-rate incentives, and consequently might have a stronger effect on effort levels (Lazear and Rosen 1981), and thus on (creative) performance. Moreover, because the evaluation of creativity might have a large subjective component, a risk-averse participant might prefer competitive incentives (and put in greater effort and show higher performance) because in such an incentive scheme it is the difference in participants’ performances that determines the reward. On the flip side, while high-powered competition incentives might encourage greater effort, the same incentives also have the potential to cause choking under pressure (Ariely et al. 2009), according to which people may have a harder time coming up with a creative idea when they know they are competing with another person. Our key results are that incentives did not improve the creative output relative to the case in which participants are not offered any external monetary incentives for creativity. Moreover, the type of incentives matter, and competitive incentives reduce creativity relative to piece-rate incentives. A possible explanation for these results is that differences in effort, as measured by time, could explain the differences in creativity between treatments. To test for this explanation we also manipulated how much time participants have to come up with a creative solution (either limited at 10 min or not limited). The effect of changing the time did not interact with the above mentioned results. These results also relate to the discussion of the relation between incentives and creativity in psychology. In particular, the “crowding-out hypothesis” in creativity argues that although intrinsic motivation enhances creativity, extrinsic motivation may actively undermine a person’s intrinsic motivation, and thus reduce overall creativity. Therefore, external incentives such as monetary rewards and competition may lower creativity. A large body of evidence supports the crowding-out phenomenon in multiple other domains (e.g., Deci and Rayen 1985; Gneezy and Rustichini 2000; Frey and Jegen 2001). However, relatively few studies have investigated the effect of monetary rewards and competition on creativity, and to our knowledge, none of these studies have compared different forms of extrinsic incentives to examine the relative benefits of using them to improve creativity. Early studies in psychology have found some evidence to support the hypothesis that monetary incentives may crowd out motivation and thus reduce creativity (e.g., Glucksberg 1962; Amabile 1982; Amabile et al. 1986). However, more recent studies have challenged the finding that financial rewards are detrimental to creativity (e.g., Conti et al. 2001; Eisenberger et al. 1998; Eisenberger and Rhoades 2001; Eisenberger and Shanock 2003). The studies in psychology and management that do find a negative relationship between monetary rewards and creativity have typically used fairly small monetary rewards (and often employed non-standard subjects). For instance, Amabile’s (1982) study raffled off three prizes to the children who made the best collages at a party. As Gneezy and Rustichini (2000) noted, although extrinsic incentives may crowd out certain behaviors, increasing the size of the extrinsic incentives can more than offset this effect. The rest of the article is organized as follows: Sect. 2 elaborates on the experimental design and the specific incentive treatments. Section 3 presents the results and explores alternative explanations. Finally, Sect. 4 closes with the key implications of our findings.",52
19.0,2.0,Experimental Economics,12 June 2015,https://link.springer.com/article/10.1007/s10683-015-9437-0,Intertemporal consumption and debt aversion: an experimental study,June 2016,Thomas Meissner,,,Male,Unknown,Unknown,Male,"The question how people cope with stochastic dynamic optimization problems, such as posed by intertemporal consumption/savings problems, has been repeatedly tested in economic experiments.Footnote 1 The main finding of this literature is that subjects (somewhat unsurprisingly) deviate systematically from optimal behavior. In particular it is observed that subjects tend to over-consume in early periods, i.e. save too little compared to the optimal solution, and that consumption tracks income too closely. When subjects are given the opportunity to learn (either privately through repetition or socially through observing other life-cycle consumption decisions) consumption decisions improve towards optimality.Footnote 2
 With the exception of Fenig et al. (2013), however, all existing experimental studies on dynamic intertemporal consumption/savings problems have strict borrowing constraints.Footnote 3 This implies that only one side of the problem has been examined, i.e. whether subjects save optimally. I believe that the question whether individuals borrow optimally is of similar interest to the understanding of intertemporal consumption decisions. This is because borrowing is as much a means to smoothing consumption (i.e. consuming optimally) as is saving. Therefore, sub-optimal borrowing decisions affect welfare in the same detrimental way as sub-optimal saving decisions. The aim of this paper is to address two main research questions: first, how do people behave in a life-cycle consumption experiment when borrowing is allowed and necessary for optimal consumption? Second, do people treat savings systematically different than debt with respect to consumption smoothing? The experiment conducted for this paper is based on a simple discrete-time, finite horizon life-cycle model of consumption with no discounting and no interest paid on savings or debt. In order to compare borrowing and saving behavior this experiment has two treatments, one in which the income stream has an increasing trend and one in which it has a decreasing trend. Since the experimental environment creates an incentive to smooth consumption, the optimal solution with an increasing income stream requires borrowing and the optimal solution with a decreasing income stream requires saving. Moreover, since the specifics of the underlying model were chosen in such a way that optimal consumption is the same in both treatments, this allows to directly compare borrowing and saving behavior. This is a novel feature compared to previous experimental work where mostly stationary income processes are used, which does not allow for such a comparison.Footnote 4
 With an increasing income stream, the experimental data suggest that subjects significantly underconsume, that is they do not borrow enough compared to the optimal solution. This stands in contrast to evidence from previous experiments on intertemporal consumption/savings problems, where overconsumption is typically observed. In line with previous findings, however, weak evidence is found for overconsumption in the treatment with a decreasing income stream, implying that subjects do not save enough relative to the optimal solution. Comparing consumption decisions across treatments reveals that deviations from optimal behavior are higher in the treatment with an increasing income stream compared to the treatment with a decreasing income stream. This implies that subjects are less willing to borrow than they are willing to save in order to smooth consumption, i.e. they are debt averse. To my knowledge, this paper is the first to provide clear evidence for this asymmetry between borrowing and saving behavior. The experimental literature on intertemporal consumption/savings problems concentrates on two potential explanations for suboptimal behavior. One explanation is bounded rationality or cognitive constraints. Ballinger et al. (2011) for example find that cognitive ability is the best predictor of saving performance. Moreover there exists substantial evidence that subjects typically learn to improve their consumption decisions over time. This points towards bounded rationality since with perfect rationality there would be no room for improvement. In the experiment conducted for this paper, cognitive constraints certainly explain some deviations from optimal behavior. However they cannot explain systematic differences in deviations from optimal behavior in the two treatments of this experiment, since the experimental task is equally demanding in both treatments. This is because subjects face essentially the same optimization problem in both treatments, as debt is mathematically equivalent to savings only with a different sign. Another explanation for suboptimal behavior is a preference for immediacy or present bias. Brown et al. (2009) use immediate rewards in an intertemporal consumption/saving experiment. They find that behavior is consistent with both hyperbolic discounting and dual self models and not with exponential discounting. In this experiment, however, present bias should not play a role since subjects receive their reward at only one point in time—at the end of the experiment. The existing literature on debt aversion primarily analyzes its role in the light of the decision to invest in higher education. Eckel et al. (2007) use survey measures to assess debt aversion and relate this data to lottery choice experiments. They find that debt aversion has little to no impact on the demand for loans for postsecondary education. In field experiments, Field (2009) as well as Caetano et al. (2011) offer differently labeled loan contracts to students and find that debt aversion does influence career and education investment decisions. However, all these studies analyze debt aversion in the context of student loans. This experiment offers a different approach, allowing to identify debt aversion in the context of a stochastic dynamic optimization problem, which in one way or another serves as a basis for household behavior in most modern macroeconomic models. The remainder of the paper is organized as follows. The next section provides the theory and experimental design. Section 3 contains the data analysis and Sect. 4 concludes.",40
19.0,2.0,Experimental Economics,18 April 2015,https://link.springer.com/article/10.1007/s10683-015-9438-z,Confusopoly: competition and obfuscation in markets,June 2016,Kenan Kalaycı,,,Male,Unknown,Unknown,Male,"Competition has been widely regarded as the best protector of consumer interests. This view has led governments to liberalize markets that used to be state monopolies and encourage entry of more firms, sometimes even at the expense of duplicate infrastructure investments (Armstrong 2008). Recently however, governments have raised concerns regarding the consumers’ ability to reap full benefits of competition due to complexity and intransparency of certain markets. In a Common Position document regarding electronic communications networks and services the European Commission (2009) states that “In order to take full advantage of the competitive environment, consumers should be able to make informed choices and to change providers when it is in their interests. It is essential to ensure that they can do so without being hindered by legal, technical or practical obstacles, including contractual conditions, procedures, charges and so on”. Similar concerns has been an important motivation in the Obama Administration’s Health Care ReformFootnote 1 and Credit Card Act.Footnote 2 The recent literature in behavioral economics offers support for such concerns. For example, Choi et al. (2010) show in an experiment that investors fail to minimize on mutual fund fees. Similarly, Brown et al. (2010) show in a field experiment that buyers underestimate the shipping costs on eBay auctions. This literature also suggests that when some consumers are boundedly rational, firms in a competitive market might have an incentive to take advantage of this bounded rationality (Gabaix and Laibson 2006; Carlin 2009). Kalaycı and Potters (2011) and Kalaycı (2015) demonstrate experimentally that sellers in a duopoly market use product and price complexity (PC) to confuse buyers, and that market prices are higher in markets where buyers are susceptible to confusion than in markets with perfectly rational (robot) buyers. A striking theoretical result in this literature is that increased competition may be detrimental for consumers when firms can confuse buyers (Gabaix and Laibson 2004; Carlin 2009). The intuition, according to Carlin (2009) is as followsFootnote 3: If the consumer base is divided between expert buyers who always buy from the cheapest seller and uninformed buyers who just shop randomly, there will be only one (low-priced) seller who will be serving the experts whereas the other (high-priced) sellers will share the demand from uninformed buyers. Therefore, if the number of sellers in a market increases the portion of informed buyers per seller will fall and this will increase the sellers’ incentives to create more confusion to raise the share of uninformed buyers in the market. In this way, increased competition might lead to more complexity and more confused buyers. In this vein, the goal of this paper is to examine empirically how increasing the number of sellers affects market outcomes when sellers can confuse buyers. Specifically, I look at how increased competition affects the amount of complexity generated by sellers and the consequences for buyer surplus. In principle, one would expect that increased competition would lead to a fiercer price competition. This would result in lower prices and higher buyer surplus. However, if the sellers make their goods more complex in response to increased competition the buyers may not be able to identify the best deal in an increasingly complex market. If increased competition leads to more complexity and consumer confusion sellers may not compete in prices fiercely. As a result market prices may not fall. I examine two questions regarding the effects of increasing the number of sellers in a market. First, does increased competition lead to more obfuscation by sellers? And second, how is the buyer surplus affected by increasing the number of sellers in a market? To answer these questions, I use laboratory experiments, designing a setup in which sellers can confuse buyers using product or price complexity. The market institution employed in the experiment is a posted-offer market where the sellers are setting prices and complexity level for their goods and then the buyers make their purchasing decisions. I conduct two studies: A quality complexity (QC) study based on Kalaycı and Potters (2011) and a PC study based on Kalaycı (2015). In the QC study the sellers have vertically differentiated goods with different quality levels. Each seller first chooses the number of attributes for her good which affects the complexity of buyers’ evaluation of the good. Then the sellers decide on prices and after that the buyers make purchases given the complexity and price choices of the sellers. In the PC study the sellers are offering an identical good and they decide on the price and the number of fees for their good, where a higher number of fees potentially makes it harder for the buyer to calculate the price of the good. In both studies, the treatment variable is the number of sellers; there are treatments with two and three sellers in each study, as well as a treatment with five sellers in the PC study. In both QC and PC studies I find that sellers use complexity and this leads buyers to make suboptimal choices. I find that increased competition, in the form of increasing the number of sellers from two to three (and to five in PC study), has no effect on the number of attributes or the number of fees chosen by the sellers, suggesting the incentives to confuse are not affected by the number of sellers in my experiments. Moreover, I find that buyer surplus in the treatments with three sellers is significantly higher than buyer surplus in treatments with two sellers in both studies, and it is highest in the treatment with five sellers in the PC study. The results indicate that, although buyer confusion induced by sellers might be a concern, increased competition does not necessarily exacerbate such concerns and still might benefit buyers through lower prices and higher buyer surplus. This article also contributes to the literature on the number effects in experimental oligopolies. Dufwenberg and Gneezy (2000) find in markets with Bertrand competition that prices are lower when there are four rather than two sellers in a market. Huck et al. (2004) examine the effect of number of sellers on collusion in Cournout oligopolies. They find some collusion with two firms, whereas there is no evidence of collision in markets with three, four and five firms. Orzen (2008) examines the effect of competition on prices in markets where a large portion of the buyers (convenience shoppers) are simulated to purchase randomly. He finds that transaction prices are lower in markets with four sellers than with two sellers under a fixed matching protocol but no difference is observed when random matching is used. Abbink and Brandts (2008) find that prices decrease with the number firms in Bertrand competition with decreasing returns. Articles in this literature use simulated demand schedules. The main methodological difference of my article however is that human buyers are used, which can potentially be confused by sellers.",13
19.0,2.0,Experimental Economics,24 April 2015,https://link.springer.com/article/10.1007/s10683-015-9439-y,"Birth, death and public good provision",June 2016,John Duffy,Jonathan Lafky,,Male,Male,Unknown,Male,"The overlapping generations structure, first introduced by Samuelson (1958), is a standard model in economic theory. Agents of different ages or experience levels coexist with one another at each point in time and this type of heterogeneity provides insights into a variety of dynamic social phenomena, such as the existence of social security systems or money as a medium of exchange. Nevertheless, the overlapping generations structure has seen comparatively little use in the field of experimental economics where researchers tend to work with fixed cohorts of subjects in one-shot or repeated settings.Footnote 1 In this paper, we examine the behavior of overlapping generations of subjects playing a workhorse model in the experimental economics literature—the finitely repeated, linear, voluntary public goods game. Our aim is to better understand how the presence of overlapping generations of agents of different experience levels, as implemented in a deterministic birth—and—death manner, affects contributions to a public good over time relative to the standard, baseline case of a fixed cohort of agents interacting repeatedly with one another over all periods of the experiment. The real-world relevance of heterogeneity in age and experience levels is hard to deny. Residents move into and out of neighborhoods, voters enter and exit electoral districts and new donors may give for the first time to a charity while longtime donors may grow old and die.Footnote 2 Nevertheless, it is not clear, a priori, what effect such turnover in the population of potential contributors should have on contributions to public goods. A robust finding across experimental studies of linear public goods games employing fixed cohorts of subjects is that public good contributions are most generous in the early periods of play and become steadily less generous as subjects gain experience.Footnote 3 If this decay in contributions is an effect of individual-level learning, then we might expect that a dynamic, heterogeneous-agent, overlapping generations environment will yield higher contributions as new subjects with predispositions to contribute to the public good are steadily introduced. On the other hand, it is possible that new subjects entering a preexisting group of experienced subjects may anticipate free-riding behavior on the part of their older peers and behave more selfishly as a result. Even if new subjects do not anticipate such behavior by others, they could learn about low levels of giving after their first period of play and, as a result, drastically reduce their contributions in subsequent periods. A widely accepted model of behavior in public goods games does not give clear predictions in our environment; the Fischbacher et al. (2001) model of conditional cooperation in public goods games could lead to increased or decreased contributions in the dynamic environment we study, depending on the reaction of new subjects to the behavior of existing subjects and vice versa. An overlapping-generations approach also provides nuance to our understanding of end-game effects. In traditional public good game experiments under either fixed or random matching protocols, subjects all “age” at the same pace, always having experienced the same number of periods of play at each iteration. Most importantly, all subjects experience their final period simultaneously. Consequently, a subject in her final period is faced not only with a lack of future interactions and, under fixed matches a related loss of any reputational concerns, but also with the knowledge that every other member of her group of subjects is facing the same final period decision. In other words, a subject not only loses her own strategic concern for cooperation, but also faces other subjects experiencing an identical lack of concern for their own futures. It is hard to think of real-life scenarios with such a common air of finality. By contrast, in the overlapping generations framework we study, every subject in the group is a different age at each point in time. A subject in her final period still lacks reputational concerns for the future, but now faces group members who can have such concerns for the future, as they will outlive her. To preview our results: We find that the dynamic, heterogeneous-agent overlapping generations environment that we implement does work to sustain contributions to the public good over a longer period of time relative to the standard, fixed cohort design. Contributions in our dynamic matching treatment decrease less within each subject’s lifetime and contributions in a subject’s final period of play are significantly greater than those made by subjects in the final period of the fixed matching treatment. Our findings suggest that experimentalists will want to add overlapping generation-matching designs to their toolkit of mechanisms for arresting the decline in cooperative behavior that is frequently encountered in laboratory experiments involving fixed cohorts of subjects interacting with one another repeatedly for a finite length of time. The rest of the paper is organized as follows. The next section discusses related literature. Section 3 lays out our experimental design and hypotheses. Section 4 summarizes our main experimental findings. Finally, Sect. 5 concludes with some suggestions for future research.",8
19.0,2.0,Experimental Economics,19 May 2015,https://link.springer.com/article/10.1007/s10683-015-9442-3,More than words: the effects of cheap talk in a volunteer’s dilemma,June 2016,Christoph Feldhaus,Julia Stauf,,Male,Female,Unknown,Mix,,
19.0,2.0,Experimental Economics,17 May 2015,https://link.springer.com/article/10.1007/s10683-015-9443-2,Information and strategic voting,June 2016,Marcelo Tyszler,Arthur Schram,,Male,Male,Unknown,Male,"Since its introduction in ancient Greece, democracy has always been associated with ‘government by the people’. A widespread view is that the democratic decision process must honor the desire of the majority (Goldfinger 2004). Voting is the tool most often used for this purpose. The underlying assumption is that voting correctly aggregates individual preferences. A sufficient condition for correct aggregation is that every voter casts a vote for her most preferred alternative. Of course, not everyone does so. For one thing, many people abstain from voting (especially in large scale elections). Moreover, voters may strategically vote for an alternative that is not ranked highest in their preference ordering (Farquharson 1969). The reason is that any election is not only a manifestation of individual preferences, but also a multi-person decision process (Downs 1957; Riker 1982a; Blais and Nadeau 1996). In such a strategic interaction a voter may be more interested in optimizing the outcome than in stating her own preference. In this paper, we investigate such strategic voting in a controlled (laboratory) environment. Our aim is to carefully isolate important determinants of the strategic vote. In particular, we are interested in the effect on strategic voting of information about others’ preferences and the relative attractiveness of the second-best alternative. When considering voting as a multi-person decision process it can be analyzed as a strategic game in which distinct strategies might lead to different outcomes and equilibria can be computed. It has long been recognized that strategic voting may be an equilibrium strategy in committees (Austen-Smith and Banks 1996), legislatures (Riker 1982a) and even in large electorates (Palfrey 1989; Fey 1997). Of course, strategic voting equilibria may involve highly complex computations that go beyond the capabilities of most voters. Behaviorally, voters may rely on simple voting heuristics such as always voting sincerely for the most preferred alternative. In addition, some people may object morally to voting strategically (Lehtinen 2007). In the end, the question whether or not voters vote strategically is an empirical one.Footnote 1
 
An example illustrates situations when strategic voting may occur. If the most preferred option does not stand a chance, a voter may vote for her second ranked option in an attempt to avoid even worse outcomes. Such behavior is consistent, for example, with Duverger’s law.Footnote 2 A special case occurs when there is a Condorcet loser (i.e., an alternative that would lose any pairwise vote against any other alternative) supported by a minority while a majority is divided between two other alternatives (Gerber et al. 1988; Forsythe et al. 1993, 1996; Cox 1997; Myatt and Fisher 2002; Palfrey 2006). The majority can avoid a victory by the Condorcet loser if the supporters of one of the two majority alternatives votes strategically for the second most preferred option. Though our goal is to better understand the occurrence of strategic voting, we do not consider situations with a Condorcet loser. Instead, we are interested in situations where there are Condorcet cycles. In our environment, each of three alternatives (denoted by A, B, and C) has a similar a priori chance of winning the election and each voter faces an a priori symmetric strategic problem. A cycle occurs because sincere voting can lead to any of the alternatives winning if they are voted on sequentially in pairwise votes. We will see below, that the occurrence of Condorcet cycles yields incentives to vote strategically when decisions are made by plurality rule. We use laboratory experiments for our empirical analysis of strategic voting. Before doing so, we first model the situation as a strategic game. In particular, we will derive Quantal Response Equilibria (QRE) and use these to formulate behavioral predictions. QRE accurately predicts voter behavior in many environments (Goeree and Holt 2005; Levine and Palfrey 2007; Groβer and Schram 2010). It has the intuitive advantage that it allows for boundedly rational behavior while at the same time assuming that the error people make declines as the stakes become larger. Our QRE predictions will be tested using our experimental data. Laboratory control will allow us to measure the impact of changes in the environment on the decision whether or not to vote strategically. Specifically, we are interested in the two circumstances mentioned above. First, we will study how the relative value attributed to the second preferred option affects voters’ decisions. This is important because, intuitively, voters are more likely to vote strategically when there is little to lose by having their second option chosen (Blais et al. 2001). Second, we will measure the impact of information about others’ preferences. This is important, because whether or not voters vote strategically may depend on how much they know about other voters’ preferences (Forsythe et al. 1993, 1996). Outside the laboratory opinion polls serve to provide such information, which may help voters to coordinate on an alternative and win the election. Voluntary preference revelation in polls may be strategic, however. In order to isolate the effect of information, we therefore opt for a situation in which an opinion poll truthfully reveals the electorate’s preferences (as in Großer and Schram 2010). Perfect information about the other voters’ preferences will in some of our treatments be made available before the election.Footnote 3
 With this information, the decision problem faced by each voter may be even more complex than without. This is because without information all voters face the same a priori situation if every preference ordering is equally likely. Assume for the case with information that supporters of the alternative with the largest support (we call this the ‘majoritarian candidate’) vote sincerely but comprise less than 50 % of the electorate. Which voters should then vote strategically? On the one hand, one may think that the supporters of the alternative with the lowest level of support have an incentive to vote strategically to increase their chances. On the other hand, voters for whom the majoritarian candidate is second best may decide to support this to ensure at least this second-best. Whether or not they do so may depend on the relative value they attribute to this option. We will address these issues theoretically and behaviorally in this paper. When preferences are not revealed by polls, all voters face the same situation. The QRE prediction is then that all voters have the same probability of voting strategically and this probability increases with the value attributed to the intermediate option. This comparative static prediction is confirmed by our data. With information about the other voters’ preferences, the prediction depends on the number of others supporting the same alternative and this alternative’s rank (in terms of support) within the electorate. It also depends on the relative value attributed to the second most preferred alternative. The experimental results are largely in line with the QRE predictions. Two important conclusions for the scenario with information are that (i) a higher frequency of strategic voting is observed, the higher is the relative utility of a voter’s second most preferred option; (ii) there is coordination on the victory of the majoritarian candidate. All in all, our results show that strategic voting is an important phenomenon and follows a pattern that to a large extent can be rationalized using the boundedly rational framework offered by QRE. The remainder of this paper is organized as follows: Sect. 2 presents theoretical analysis and equilibrium predictions. The experimental design is introduced in Sect. 3. Section 4 presents the results and Sect. 5 offers concluding remarks.",7
19.0,2.0,Experimental Economics,20 May 2015,https://link.springer.com/article/10.1007/s10683-015-9444-1,"Why do promises affect trustworthiness, or do they?",June 2016,Huseyn Ismayilov,Jan Potters,,Male,Male,Unknown,Male,"Communication is often found to foster trust and cooperation and many studies have emphasized the role of promises in this respect (Belot et al. 2010; Vanberg 2008; Bicchieri and Lev-on 2007; Charness and Dufwenberg 2006; Sally 1995; Ostrom et al. 1992; Orbell et al. 1988). A prime explanation for the impact of promises is that they create a commitment. In the present paper we set out to explore the nature and force of this commitment. Specifically, we examine whether a promise has commitment power because the promisor makes it or because the promisee learns about it. A preference for promise-keeping may derive from a more general preference for consistency (see Ellingsen and Johannesson 2004; Kerr and Kaufman-Gilliland 1994). If a person has expressed that she will do X, not doing X creates an inconsistency which the person may want to avoid.Footnote 1 Whether or not a person’s statement (promise) is consistent with the person’s action does not depend on whether someone else may be affected by the statement or even learns about it. What counts for the individual is that she has expressed an intention to do something; as a consequence she prefers to take an action which matches that intention. We call this the internal consistency explanation for promise keeping. An alternative interpretation of the commitment-based explanation for promise-keeping is that people feel obliged to fulfill verbal contracts and agreements (Vanberg 2008). This conceptualization of the commitment requires, not only that the promisor made the promise, but also that the promisee learns about it. We call this the social obligation explanation for promise keeping. To assess the empirical support for these alternative forms of commitment, we tweaked the experimental trust game by Charness and Dufwenberg (2006). Trustees had an opportunity to write a pre-play free-form message to trustors, but the message was delivered to the trustor only with probability \(\frac{1}{2}\).Footnote 2 Thus, in our experiment 50 % of the trustees wrote a message that was not delivered to the trustors. The results show that trustees who made a promise were significantly more likely to act trustworthy than trustees who did not make a promise, irrespective of whether the message was delivered or not. Conditional on messages being delivered, promisors were 12 % more likely to act trustworthy than non-promisors (54 vs. 42 %); conditional on messages not being delivered, promisors were 21 % more likely to act trustworthy than non-promisors (35 vs. 14 %). These results are in line with the internal consistency hypothesis that promises create commitment even when not delivered. An important caveat, of course, is that promises are endogenous.Footnote 3 It may be that trustworthy trustees are more likely to make a promise than untrustworthy trustees, in which case self-selection may drive the difference between promisors and non-promisors. To examine whether promises increase trustworthiness, we later ran a control treatment in which trustees could send messages but were not allowed to make promises. It turned out that in the control treatment trustees were not less trustworthy than they were in the base treatment in which they could and did make promises; actually they were somewhat, insignificantly, more trustworthy (49 % in the control treatment vs. 39 % in our base treatment). This suggests that the correlation between promises and trustworthiness is due to self-selection. Promises do not increase trustworthiness in our experiment; they are just more likely to be sent by cooperators than by non-cooperators. The fact that promises are endogenous is not unique to our experiment. Many studies which report an impact of promises on cooperation face a potential problem of reverse causality. The fact that a treatment with communication leads to more cooperation than a treatment without communication does not by itself help to draw causal inferences on the effect of promises. Communication can have an effect per se, just as message delivery increases trustworthiness in our base treatment. There we find that promises were more likely to be kept if they were delivered (54 %) than if they were not (35 %), but trustees who did not make a promise were also more likely to be trustworthy if their messages were delivered (42 %) than if they were not (14 %). This suggests that the positive impact of communication on trustworthiness need not be related to promises, even if there is a positive correlation between promises and trustworthiness.",27
19.0,2.0,Experimental Economics,29 May 2015,https://link.springer.com/article/10.1007/s10683-015-9445-0,The influence of investment experience on market prices: laboratory evidence,June 2016,Jürgen Huber,Michael Kirchler,Thomas Stöckl,Male,Male,Male,Male,"The words “bubble”, “crash” and “crisis” have dominated headlines of newspapers worldwide since 2007. The bursting of a bubble often affects the wider economy, causing unemployment and recession (see, e.g., Kindleberger 2011). Thus, understanding causes of bubbles and crashes is an eminently important challenge for practitioners, regulators and academics alike. Bubbles have arguably existed since the creation of modern financial markets, but reasons for their development are still far from fully understood. There is ample evidence that various forms of experience significantly affects subjects’ behavior in financial markets. By taking a historical perspective Malmendier and Nagel (2011) show that experiencing macroeconomic shocks influences financial risk taking. Gong et al. (2013) conducted laboratory experiments with real investors during a boom and later during a crash of the Shanghai stock exchange. They report that, compared to those in the crash-treatment, subjects in the boom-treatment were much more active in the laboratory markets and preferred to hold more shares than cash. In a very general study not directly related to financial markets, Lejarraga (2010) shows that experience sampling can increase subjects’ accuracy in non-lottery tasks and thus has a positive effect on subjects’ decisions. Taking a step towards financial markets there is evidence from lab and field experiments showing that experienced investors are less likely to exhibit behavioral biases like the disposition effect (Shapira and Venezia 2001; Feng and Seasholes 2005) or the endowment effect (List 2003). However, some studies also find that experienced investors show similar or even more attenuated behavioral biases or deviations from Expected Utility Theory compared to non-experienced subjects. Abdellaoui et al. (2013) report that a sample of private bankers and fund managers clearly behave according to Prospect Theory and thus violate utility maximization. The professionals are risk-averse for gains, risk-seeking for losses and exhibit loss aversion for mixed gambles (though less pronounced than assumed in the literature). Furthermore, Haigh and List (2005) show that professional CBOT-traders actually show a higher degree of myopic loss aversion compared to a students sample pointing at a detrimental effect of experience on decision quality. By using empirical data Greenwood and Nagel (2009) study the behavior of mutual fund managers during the Tech-stock bubble. They show that younger (less experienced) managers, compared to their more experienced colleagues, were more exposed to tech stocks and exhibited trend chasing behavior. The success of younger managers in the build-up phase of the bubble generated a massive inflow of money into these funds—a fact that may have additionally spurred the bubble. Earlier studies by Loewenstein and Lerner (2003) and Nguyen and Noussair (2014) suggest that positive past investment experience influences mood (e.g., feelings like anger and happiness) and this can in turn affect the readiness to take risks. Furthermore, there is evidence that the kind of experience, either positive or negative, also influences behavior and market prices (Lakonishok and Smidt 1988; Shiller 2000; Hirshleifer 2001; Hirshleifer and Shumway 2003; Edmans et al. 2007; Gong et al. 2013). For instance Kaustia and Knüpfer (2008) and Chiang et al. (2011) show that successful investments in IPOs increase investors’ likelihood of participating in future auctions. Intuitively, these findings can be rationalized as follows: positive past experience might result in unduly optimistic expectations as good experiences bolster investors’ confidence (Weinstein 1980; Van den Steen 2004). If many investors develop the same optimistic view about the future based on past experience, markets might exhibit undesirable up and down swings. However, in Chiang et al. (2011) institutional investors do not exhibit such behavior. Additionally, experimental asset market studies also contribute to this strand of literature. Here, studies on bubbles and crashes were pioneered by the seminal model of Smith et al. (1988, henceforth \({\mathrm {SSW}}\)). They report substantial mispricing in their markets, although traders know the distribution of all future dividends, and thus know the expected fundamental value (\({\mathrm {FV}}\)) in advance (see Palan 2013 for a review of studies based on the \({\mathrm {SSW}}\) design). Experience, gained through multiple repetition of the same market, is an important factor that moderates mispricing in \({\mathrm {SSW}}\) settings (Smith et al. 1988; Van Boening et al. 1993; Dufwenberg et al. 2005; Haruvy et al. 2007; Lei and Vesely 2009; Huber and Kirchler 2012; Sutter et al. 2012; Cheung et al. 2014). Corgnet et al. (2010) explore the impact of releasing public messages with different levels of reliability on asset prices. They find that messages can play a significant role in bubble abatement, or rekindling. Hussam et al. (2008) find that the result is fragile to minor changes in the setup and consequently, bubbles can be rekindled even with experienced subjects. However, there is hardly any evidence whether experience from trading on real financial markets impacts price efficiency in the laboratory. There is only anecdotal evidence from one market in the classical study of Smith et al. (1988) that price efficiency is not improved when markets are populated by financial professionals instead of students. Hence, the effect of prior investment experience on subjects’ behavior and consequently on market prices is not unequivocally clear and seems to depend on the kind of experience gained and the market context used. To reduce this research gap we explore how different levels of investment experience (positive, negative, no experience) in an individual investment game influence price formation in subsequent laboratory asset markets. Here we analyze whether different levels of investment experience gained from one particular source of experience (i.e., an investment game) influence price efficiency. Thus, our first treatment variable is the level of investment experience subjects gain before the main experiment. We vary experience in three ways: no, positive or negative experience. Subjects either do not play or do play an investment game first. Those that collect investment experience in the game observe and earn either predominantly positive or predominantly negative returns. In the asset market that follows only subjects with identical experience (no, positive or negative) interact. To get a comprehensive picture on the influence of investment experience on markets, we introduce the salience of the fundamental value as second treatment variable. Both market settings are based on \({\mathrm {SSW}}\) but we vary their proneness to mispricing by varying the degree of \({\mathrm {FV}}\)-salience. Either the \({\mathrm {FV}}\) is shown only on the history screen (low salience treatment) or it is also displayed on the trading screen (high salience treatment). With our 3\(\times \)2 design with the treatment variables “Experience” and “\({\mathrm {FV}}\)-salience”—outlined in Table 1—we are able to test which of two effects, investment experience or mood, dominates. If mood dominates, we should see high (low) prices after positive (negative) investment experience. If experience dominates, we should see efficient prices irrespective of whether the prior investment experience was positive or negative. With our treatment design we are also able to compare the experience effect of the investment game (positive or negative) on price efficiency in a setting that is prone to bubbles with one that is usually efficient, but where bubbles might be triggered through prior positive investment experience. We find that (i) both, positive and negative, experience gained in the investment game lead to more efficient pricing. We conjecture that experiencing changing price paths in the investment game creates a higher sensibility on changing fundamentals and prices among subjects in the subsequently run asset market, as the fundamental value information is more salient. Further, we show that (ii) the experience effect dominates potential effects triggered by positive and negative sentiment generated by the investment game. This finding is remarkable as we show that positive and negative investment experience strongly influence subjects’ mood. Thus, we conjecture that experience and a related stronger focus on the exogenous \({\mathrm {FV}}\)-process dominates sentiment effects on mispricing.",8
19.0,2.0,Experimental Economics,05 June 2015,https://link.springer.com/article/10.1007/s10683-015-9447-y,Cancelling out early age gender differences in competition: an analysis of policy interventions,June 2016,Matthias Sutter,Daniela Glätzle-Rützler,Simon Czermak,Male,Female,Male,Mix,,
19.0,2.0,Experimental Economics,17 June 2015,https://link.springer.com/article/10.1007/s10683-015-9448-x,Social comparisons in wage delegation: experimental evidence,June 2016,Gary Charness,Ramón Cobo-Reyes,Jose Maria Perez,Male,Male,Male,Male,"Akerlof (1982) highlighted the importance of fairness considerations for workers’ effort choices. When workers perceive that they are being paid an unfair wage, they may reduce their effort. In the field, it is likely that people consider their co-workers, who are comparable to them, to be a reference group for social comparisons. That is, workers’ perception of fairness may depend on the wages paid to their co-workers (Frank 1984) and on other social considerations. This notion of social comparison is pervasive in a number of areas in economics, such as with respect to consumer behavior or membership in clubs or organizations. This is a central issue in any labor setting, as workers may readily make comparisons to other workers at their own firms, in the same industry, or even in a similar industry. This article makes two main contributions. First, we test whether social comparisons affect workers’ effort when firms can decide either to choose workers’ wages or to let them choose these. We consider the case where there are two workers per firm, and firms can delegate the wage decision to neither worker, to one worker, or to both workers. To the best of our knowledge, this is the first study to test whether social comparisons affect behavior when a firm can decide either to choose workers’ wages or to let them choose these. In order to test whether social comparisons influence workers’ performance, we conduct treatments that vary the information that workers receive. In one treatment, a worker has no information about either the co-worker’s wage or whether she could choose it. In a second treatment, a worker only receives information on her co-worker’s wage. In a third treatment, a worker knows both her co-worker’s wage and whether or not her co-worker could choose his own wage.Footnote 1 Our data support and extend the results found in Charness et al. (2012). Similar to what was found in their one-employer-one-worker relationship, we find that when an employer delegates the wage choice to their employees, they significantly provide higher effort levels. These results hold in all three treatments. Thus, the benefits of the delegation device in the lab appear to be robust to increases in the size of the workforce. Our second contribution is methodological. A controversial issue in experimental labor economics relates to behavior with stated effort versus behavior with real effort. Since Fehr et al. (1993), most studies in this area have modeled ‘effort’ as a choice of a cost, rather than the physical and/or mental exertion that is required to complete a task. However, in the past decade many experimenters have instead made use of simple tasks such as stuffing envelopes or solving mazes, etc.Footnote 2 An open question is whether these approaches lead to different behavior. We conducted our first set of sessions using stated effort by workers, and we then attempted to reproduce our results with real effort by workers (the specific task consisted of adding up sets of five two-digit numbers during 2 min). We are pleased to report that the main results found with stated effort are qualitatively similar to those found with a real-effort task, suggesting that these approaches may be substitutable to some degree. Our article differs in an important respect from previous studies. We provide firms with two distinct tools with which to discriminate between workers: (i) paying different wages, and (ii) delegating the wage choice to just one of the two workers. As a result, social comparisons are two-dimensional. A worker may be concerned about both his co-worker’s wage and whether his co-worker sets her own wage or not. We find that worker performance is affected by relative wage, at least to some extent. We also see that workers make higher effort choices when they can choose their own wage while their co-workers cannot, and workers make lower effort choices when they cannot choose their own wage while their co-workers can. Thus, it behooves organizations to ensure that employees or members feel that their rights are being respected relative to those of other employees or members. The remainder of the paper is organized as follows. Section 2 provides a literature discussion and Sect. 3 explains the experimental design. We describe and discuss the main results in Sect. 4 and conclude in Sect. 5.",16
19.0,2.0,Experimental Economics,24 June 2015,https://link.springer.com/article/10.1007/s10683-015-9449-9,The gift of being chosen,June 2016,Natalia Montinari,Antonio Nicolò,Regine Oexl,Female,Male,Female,Mix,,
19.0,2.0,Experimental Economics,10 June 2015,https://link.springer.com/article/10.1007/s10683-015-9450-3,Putting a price tag on others’ perceptions of us,June 2016,Yohanes E. Riyanto,Jianlin Zhang,,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Experimental Economics,17 June 2015,https://link.springer.com/article/10.1007/s10683-015-9451-2,Detecting motives for cooperation in public goods experiments,June 2016,Takafumi Yamakawa,Yoshitaka Okano,Tatsuyoshi Saijo,Male,Male,Unknown,Male,"In real life, millions of people give to privately provided public goods such as charity. For decades, the question why people cooperate in a social dilemma situation has been one of the most important research questions, not only for economics but also for all of the social sciences. There may be many factors that induce cooperative behavior such as altruism, warm-glow, reciprocity, other social and psychological motives, and even strategic motives. Laboratory experiments using public goods games address this question. A large body of literature find significant evidence that subjects are too cooperative to be consistent with the economic prediction under the self-interested assumption (see, e.g., Ledyard 1995). Many studies have investigated subjects’ preferences in public goods games, but there is no consensus on what explains the observed behavior. Andreoni (1989) and Andreoni (1990) suggest that subjects are altruistic toward other subjects or possibly that they get a warm-glow from giving to the public good. Palfrey and Prisbrey (1996) and Palfrey and Prisbrey (1997) find little or no pure altruism, but significant evidence of warm-glow and confusion. Goeree et al. (2002) find significant evidence, not only of warm-glow and confusion but also of pure altruism. Croson (2007) suggests that a part of subjects’ preferences may be to reciprocate or match the contributions of others in their groups.Footnote 1 Fishcbacher et al. (2001) emphasize that many subjects are conditional cooperators. Andreoni (1995) and Houser and Kurzban (2002) argue the importance of both confusion and social motives. While Andreoni (1988b) does not support the hypothesis of strategic motives, Croson (1996) does do so using a similar design to Andreoni (1988b). Some studies also examine strategic motives, but there is no general agreement on their significance (Weimann 1994; Cooper et al. 1996; Palfrey and Prisbrey 1996; Sonnemans et al. 1999; Keser and Van Winden 2000; Brandts and Schram 2001; Brandts et al. 2004; Yamakawa 2012).Footnote 2
 
Andreoni (1995) provides an ingenious design to discriminate between cooperation due to social motives and that due to confusion. His experiments include three conditions. The first (“Regular” condition) is a standard public goods game. The second (“Rank” condition) is the same as the first, but subjects are paid based on their rank in the standard game, which generates a zero-sum payoff structure. In order to compare directly with the Rank condition, the third (“RegRank” condition) alrefereso provides feedback about rank, but subjects are paid according to the experimental earnings, just as under the Regular condition. Contributions in the Rank condition are considered to be due to confusion because the condition’s zero-sum payoff structure left no incentive for cooperation. Contributions due to social factors are calculated by subtracting the contributions in the Rank condition from those in the RegRank condition. The difference in contributions between the Regular and RegRank conditions are due to either social motives or confusion. Andreoni (1995) reports that, on average, about half of all cooperative behavior can be classified as social motives. Confusion falls as time passes, while the importance of social motives fluctuates over time. 
Houser and Kurzban (2002) also adopt the subtraction method to separate social motives from confusion. They place individual subjects into groups in which the other players are computers. The human player is told that the computers’ contributions to the public good are independent of the subject’s own play. Since subjects cannot benefit either himself/herself or other subjects, contributions in this condition are attributable to confusion. By subtracting contributions in this condition from those in the regular public goods experiment, they calculate the contributions due to social motives. Houser and Kurzban (2002) report that consistent with Andreoni (1995), on average, about half of all cooperation is due to social motives. In addition, confusion accounts for more cooperation in the early rounds than in the later rounds.Footnote 3
 By using the subtraction method, this study separates motives into three categories, namely confusion, one-shot motives, and multi-round motives. Our design adds one condition to the design of Houser and Kurzban (2002). We call the regular public goods game the H condition, and the computer condition the C condition. The additional condition (called the HC condition) is similar to the C condition, but the earnings from the computer are paid to a real subject. Since the subject can benefit the other member but not himself/herself with public contributions, cooperative behavior can be driven by motives such as altruism and warm-glow, as well as confusion. When the subject is faced with positive contribution to the public good by the computer, cooperative behavior can also be driven by inequality aversion and conditional cooperation. The difference in contributions between the HC and C conditions is considered to be an estimate of these motives in the regular public goods game. We refer to these motives as “one-shot motives.”Footnote 4 The difference in contributions between the H and the HC conditions is an estimate of motives such as the strategic motive under incomplete information suggested in Kreps et al. (1982), failure of backward induction, and reciprocity. We refer to these motives as “multi-round motives.” Since our research focus is one-shot motives and multi-round motives, our experimental design attempts to remove confusion as much as possible, while keeping the important feature of public goods experiments, namely, the social dilemma. Confusion includes a subject’s misinterpretation of instructions, unfamiliarity with the game, and so on, which are effects specific to conducting the experiments. Hence, experimenters should eliminate subjects’ confusion as much as possible in order to evaluate their behavior correctly. In particular, in linear public goods experiments with an equilibrium prediction of a zero contribution to the public good, there is only one way a confused subject can err, which is to contribute too much to the public good. Errors will not be averaged out of the aggregate data, and hence we might misinterpret the contributions that are really due to confusion as cooperative behavior. We use two devices to remove confusion in our experiment. First, we adopt a two-player game that generates a simple strategic environment. Second, subjects are provided with a detailed payoff table that lists the total payoffs gained from the private and public goods when their and their group member’s investment units are determined. This makes the structure of the game clear.Footnote 5
 The results of our experiment are as follows. In aggregate, about 80 % of cooperation is attributable to multi-round motives, while confusion and one-shot motives account for only 2 and 18 %, respectively. In addition, when considering round-level data, multi-round motives is a dominant motive in all rounds but not the last, while confusion and one-shot motives are minor. In the last round, sudden cooperative decay is observed in the regular public goods game. This is consistent with a strategic motive under incomplete information about the player’s rationality, failure of backward induction, and/or reciprocators facing with strategic players. These results suggest that multi-round motives plays an important role in driving cooperative behavior in our public goods game. This paper is organized as follows. Section 2 describes our experimental design, which allows us to separate the motives into three categories, confusion, one-shot motives, and multi-round motives. Section 3 shows the experimental results. We will show that multi-round motives plays an important role in driving cooperative behavior. Section 4 concludes.",19
19.0,3.0,Experimental Economics,29 July 2015,https://link.springer.com/article/10.1007/s10683-015-9452-1,Trait perceptions influence economic out-group bias: lab and field evidence from Vietnam,September 2016,Tomomi Tanaka,Colin F. Camerer,,,Male,Unknown,Mix,,
19.0,3.0,Experimental Economics,26 June 2015,https://link.springer.com/article/10.1007/s10683-015-9453-0,The influence of potential on wages and effort,September 2016,Gary Bolton,Peter Werner,,Male,Male,Unknown,Male,"This study identifies circumstances in which a commonly shared norm for wage differentiation emerges and is acceptable to workers in the sense that lower paid workers expend similar effort as their better paid counterparts. The broad hypothesis that guided our experiment is that wage differentials are more acceptable to the extent the differences are transparently related to particular characteristics of workers and, subsequently, will not hamper motivation and effort. Provided that principals foresee workers’ heterogeneous entitlements, this should lead to generally unequal wage profiles. In recent years, a number of empirical and experimental studies have demonstrated that relative pay has a crucial impact on employee motivation both in the lab and in the field, corroborating the influential fair wage-effort hypothesis by Akerlof and Yellen (1990). According to this notion, an employee withdraws effort if her actual wage falls short of a “fair” level. The focus of this literature has typically been on the norm of pay equality, as most studies in this area investigate the effects of deviations from reference incomes or the introduction of wage dispersion among homogeneous agents. An important finding is that inferior wage or wealth positions have a negative effect on life or job satisfaction measures (see, for example, Clark and Oswald 1996; Luttmer 2005; Ferrer-i-Carbonell 2005; Card et al. 2012; and Ockenfels et al. forthcoming). Controlled laboratory and field experiments provide evidence that disadvantageous relative positions decrease work performance and labor supply (see, for example, Burchett and Willoughby 2004; Clark et al. 2010; Gächter and Thöni 2010; Greiner et al. 2011; Cohn et al. 2014; Bracha et al. 2015). However, a single definition of the term “fair wage” is hardly possible. While wage equality is one potential reference point for fairness considerations, alternative fairness norms can conflict with equal pay. Of particular importance to our study, wage differentiation creates an incentive for effort exertion, and plausibly, employees as well as employers may view this fair, at least when workers are heterogeneous concerning productivity.Footnote 1
 A number of experimental studies have investigated how differences in workers’ characteristics might influence wage offers and effort exertion. In the production game by Güth et al. (2001), a principal interacts with two agents of different productivities. Here, wage transparency leads to less differentiation by principals, but there is little evidence for wage comparisons among agents. Charness and Kuhn (2007) conduct a gift exchange game with productivity differences between agents whose absolute sizes are only known to the principal and observe that agents focus solely on their own wages when deciding about effort levels. Moreover, substantial wage and effort differences are found between high and low productivity workers. Abeler et al. (2010) implement an experimental setting in which workers are ex-ante homogenous, but may differ concerning their efforts chosen in each round. Here, efforts decline over time when the principal is forced to pay identical wages whereas efforts remain constant when the principal can differentiate. As the authors show, the effort decline results from violations of the equity norm in the sense that identical wages are paid when agents differ in their effort levels. An important interaction between wage and effort comparisons is also found in the three-person gift exchange setting by Gächter et al. (2012). If agents can compare themselves via multiple channels, the impact of wage comparisons becomes smaller while the impact of effort comparisons increases: in response to high wages, an agent becomes more likely to choose a high effort level when observing a high effort level also by her co-worker.Footnote 2 In a related paper, Gächter et al. (2013) show that this phenomenon can be explained by social preferences rather than by orientation on peer behavior. Rivas (2009) conducts a multi-person gift exchange game where heterogeneous productivities are assigned on the basis of performances in a task and lets principals choose between three unequal payment schemes. Here, the detrimental impact of pay inequality depends on the skewness of the wage distributions, and agents’ productivities, wages and efforts are positively correlated. In a recent study by Charness et al. (2014) that focuses on interactions of managers with two agents, the authors assign productivity differences on the basis of an aptitude test and additionally introduce uncertainty so that managers are not sure about the productivity level of a particular agent. While high productivity workers generally earn more than low productivity workers, managers tend to compress wages when the uncertainty about true productivity increases. Finally, Englmaier et al. (2014) investigate how principals in a gift exchange environment react to information about an agent’s productivity and trustworthiness which are elicited in a real-effort task and a trust game prior to the actual game. The authors find that wage offers respond to both characteristics. However, as the focus of this study is on how employers respond to multidimensional worker heterogeneity, relative wages and productivity comparisons between the workers do not play a role. Our study contributes to the literature on employee heterogeneity and relative wage comparisons by investigating how principal–agent interactions are affected by exogenously introduced entitlements among workers. In particular, we test if gift exchange based on a commonly accepted norm for wage differentiation can emerge and be sustained in the presence of heterogeneous wage demands. In addition, our design allows us to assess how wage setting and effort exertion respond to different sources of entitlements. Finally, we study how the communication by principals influences gift exchange when both wage demands and the bargaining power of the agents vary. The workhorse of our study is a three-person extension of a gift exchange game similar to Fehr et al. (1993, 1997) in which one principal is matched with one high productivity agent and one low productivity agent. A number of features of our experimental design are of particular importance. First, agents in our setting know the exact productivity differences and therefore can directly assess how her and her co-worker’s efforts translate into outcomes. As Hennig-Schmidt et al. (2010) show, payoff comparisons between principals and agents may be important for effort decisions. This feature of our design differs from previous studies where the agents are informed about the existence of workers of several types, but the size of the productivity differences are unknown (Charness and Kuhn 2007). As our study makes productivity differences among workers transparent, it provides a salient and unambiguous comparison standard that may influence demands for wages or effort levels, reflecting the notion that in close working relationships employees should be able to assess their own productivity relative to colleagues. Also, unlike previous studies that introduce budget constraints (Charness and Kuhn 2007) or predetermined wage profiles (Rivas 2009), we do not restrict principals in their choices of wage profiles, therefore leaving them substantial degrees of freedom as to adjust wage offers in the face of employee heterogeneity. Second, in our three main treatments (BASE, CT and ULT), differences in productivity potential are determined by the performance in a competitive (quiz) task. Our main goal here is to test whether earned ex-ante differences between workers challenge the equal pay norm. If the entitlements arising from performance in the competitive task and the resulting differences in employee potential alter fairness perceptions, equality should not be the predominant norm for wage offers. Studies on free-form negotiation and dictator games implemented an initial production task to create unequal “property rights” among participants and found that this had strong effects on the distribution of outcomes (see Hoffman and Spitzer 1985, for a seminal study and, for example, Cherry et al. 2002; Gächter and Riedl 2005; Bolton and Karagözoğlu 2014; and Karagözoğlu and Riedl forthcoming, for more recent evidence). The introduction of heterogeneous entitlements typically leads to distinct deviations from the prominent 50–50 norm. Third, as agents interact repeatedly in our setting and many of them act both as high performers and as low performers, we can evaluate whether wage demands adjust to variations in agents’ relative productivities. Previous studies have often considered how observed differences in behavior—rather than differences in ex-ante potential—shape the relevant norm (Abeler et al. 2010; Gächter et al. 2012, 2013). In our setting, we focus on ex-ante entitlements that can arise from two sources: agents in our main treatments differ both in past quiz performance and in the ability to produce future profits for the principal. To disentangle the contribution of backward-looking and forward-looking entitlements for the patterns we observe in our main treatments, we conduct a control condition (RANDOM) in which potential is randomly assigned to the agents. The final goal of our study is to investigate how communication influences wage choices and employee effort. There is presently little evidence of the strategic value of communication to mitigate work moral hazard in circumstances where wage comparisons are important. Related work, however, suggests that communication can have a strong impact: laboratory experiments have shown that communication can strongly enhance coordination (Brandts and Cooper 2007; Brandts et al. forthcoming) and strengthen gift exchange in 1-to-1 principal–agent relationships (Cooper and Lightle 2013). In two of our main treatments (CT and ULT) principals have the possibility to send a “cheap talk” text message to employees explaining their wage choices. After learning her and her co-worker’s wages, each employee decides on an effort level. One treatment (ULT) allows us to investigate how heterogeneous wage demands affect wage setting and effort exertion when employees’ have stronger bargaining positions. Here, employees can veto the wage offer, reducing the principal’s payoffs as well as their own by more than providing the minimum effort, an arguably clearer way to signal punishment for a pay norm violation since minimum effort is payoff optimal for the employee. Summarizing, we find that department heads differentiate strongly in wages between high and low performing employees. The results of our control treatment indicate that a substantial part of the wage markup for high performers is related to their higher potential to create revenues for the department head. Moreover, we find robust patterns of gift exchange, and relative wage comparisons seem to have little detrimental impact on effort choices in our setting. Finally, communication by the department head may significantly influence effort exertion, with high and low productivity workers responding to different types of messages. Section 2 describes our experimental design and our hypotheses. Our results with respect to wage setting, effort choices and communication, and our experimental robustness check are presented in Sect. 3; in Sect. 4 we discuss our findings and conclude.",6
19.0,3.0,Experimental Economics,25 June 2015,https://link.springer.com/article/10.1007/s10683-015-9454-z,Keeping others in our mind or in our heart? Distribution games under cognitive load,September 2016,Karen Evelyn Hauge,Kjell Arne Brekke,Henrik Svedsäter,Female,Male,Male,Mix,,
19.0,3.0,Experimental Economics,05 August 2015,https://link.springer.com/article/10.1007/s10683-015-9455-y,How to hire helpers? Evidence from a field experiment,September 2016,Julian Conrads,Bernd Irlenbusch,Dirk Sliwka,Male,Male,Male,Male,"How to motivate voluntary helpers? As argued by Menchik and Weisbrod (1987), the standard economic view would suggest to provide financial incentives. Frey and Götte (1999), however, have observed that monetary incentives can undermine motivation: External financial rewards may backfire as extrinsic incentives could crowd out the intrinsic motives to socially engage [see also Deci and Ryan (1985); Bénabou and Tirole 2003, 2006)]. Crowding-out effects are known from different fields of social engagement. Titmuss (1970), for example, argued that monetary compensation for donating blood might crowd out the supply of blood donors [see also Mellström and Johannesson (2008)]. Lacetera et al. (2012), however, have shown that extrinsic financial incentives can also stimulate pro-social behavior, e.g., to donate blood. Thus, the evidence on the crowding effects of financial incentives on pro-social behavior is still inconclusive [see Gneezy et al. (2011) for a critical review]. But there is also some evidence that different forms of non-monetary incentives can motivate individuals. If an action is pro-social per se, individuals may feel motivated by the action itself as they are doing ’good’ as shown by Andreoni (1998). Based on the formal analysis of Bénabou and Tirole (2006), Ariely et al. (2009) have shown experimentally that this effect increases if the pro-social activity is observed by others, i.e., people receive social recognition for their actions, improving their social- and self-image [see also Akerlof and Kranton (2000)]. In a similar vein, Kosfeld and Neckermann (2011) presented evidence that non-monetary awards can have a strong motivating effect. As pointed out by Clary et al. (1998), another non-directly monetary source of motivation are opportunities for personal and professional development, i.e., individuals gain career-related benefits from voluntary work, like learning new skills, being enabled to signal personality traits or improving their personal or business networks [see also Holmström (1999)]. To shed more light on the effects of non-monetary and monetary rewards on the willingness to help, we ran a field experiment in the course of organizing the ’ESA Europe 2012’ conference hosted by the University of Cologne.Footnote 1 We recruited helpers to provide technical assistance in each of the presentation rooms of the different parallel sessions. To advertise our search for helpers we sent out 2859 emails to a pool of business and economics students enrolled at the University of Cologne. In the email we varied the types of incentives provided.Footnote 2
 In one set of treatments we used three types of incentives that did not involve money. In our baseline treatment we tried to motivate candidates to volunteer by just mentioning that they would have the benefit of attenting the conference during the time they were not working for us. In a second treatment we offered an appreciatory certificate for their service.Footnote 3 In a third treatment we provided information about the exact amount of the regular conference fee, which would be waived for the helpers. In another set of treatments we provided very small, small and slightly above standard wages to motivate possible candidates, i.e., we offered either 1 Euro, 5 Euros and 10 Euros as an hourly wage.Footnote 4
 Subjects were randomly assigned to one of the six treatment groups. Departing from other insightful studies on volunteer work supply that mainly exploit survey data (Frey and Götte 1999) or lab experiments (Linardi and McConnell 2011) and we designed a field experiment [see also Harrison and List (2004)], on the benefits of field experimentation) in line with Gneezy and Rustichini (2000b) and Al-Ubaydli and Lee (2011). In our analysis we focus on two dependent variables of interest. First, we look at whether possible candidates actually applied to volunteer at the conference with respect to the different treatments. As a second measure we asked applicants, during the application procedure, about the working time he or she is willing to help at the conference. Thus, our experimental design allows us to analyze the effects of different incentive schemes both on the extensive margin (number of applicants) and the intensive margin (working minutes offered). We find that participants reacted differently to the different types of non-monetary incentives. Just asking them to volunteer or offering them an additional certificate was significantly more motivating than mentioning that regular attendees would have to pay an expensive registration fee to get access to the conference. One explanation might be that potential helpers were demotivated as their intrinsic motives to help are crowded out when becoming aware that they receive an implicit monetary benefit in terms of a waived fee. To find evidence for this hypothesis, we conduced an additional online-survey experiment. We randomly assigned each survey participant to one of two treatments, showing them either the invitation letter for the waived fee treatment or the baseline treatment in our field experiment. We then asked participants to rate a person who would apply under the given reference letter according to several characteristics. We find that ratings of the applicants’ motivation are higher in the baseline treatment, and applicants in the waived fee treatment receive higher ratings for career orientation and status orientation. These results are well in line with an explanation based on the Bénabou and Tirole (2006) model of image concerns: Mentioning the waived fee may have reduced the scope to signal pro-social motivation and increased the likelihood that an applicant is driven by more selfish reasons. If people value being viewed as pro-social and not too selfish, this reduces the incentive to apply. Once money was offered, participants were also sensitive to increasing monetary incentives, but only when these incentives were sufficiently strong. While we find no significant differences in the reactions to the 1 Euro or 5 Euros hourly wage, both the number of applications and the working time offered significantly increased when the promised hourly wage was 10 Euros instead of 1 or 5 Euros. Hence, paying more money may actually help to attract helpers, but this does not work when the wage level is substantially below the opportunity costs of work.Footnote 5
 Comparing the non-monetary with the monetary incentives, we find no significant effects on both margins between just asking for pure voluntary help compared to offering the 1 Euro or 5 Euros hourly wage. We also do not find significant effects in the extensive margin (i.e., the fraction of applicants) between simply asking for help compared to the 10 Euros treatment. However, there is a significant difference on the intensive margin between these two treatments, i.e., offering a 10 Euros hourly wage significantly increased the working time offered. Hence, our results give some insights on the question of how to hire “volunteers” that can be applied to other contexts. Of course, the optimal strategy depends on the objective function of the “employer”, the size of the applicant pool and the costs of recruiting and training. However, irrespective of these factors, our results indicate that offering small monetary rewards does not help or even becomes detrimental. The question then remains whether and how to appeal to social motives or to offer more generous wages. If, for instance, the available pool of potential applicants is large enough, advertising the position is costless, and the task can be split among more people, our results imply that the “voluntary” announcements as in the baseline condition can work well at low costs. If, however, it is more important to have helpers who offer a sufficient number of hours each (for instance when there are switching costs and costs for training helpers), funds are available, and the applicant pool is restricted or advertising is costly, offering money can be better because social motivation may not guarantee a sufficiently intensive engagement. The paper proceeds as follows. Section 2 introduces the experimental setup and our six treatments. In Sect. 3 we analyze the data. In Sect. 4 we discuss the main finding and analyze the data from an additional online-survey experiment. Section 5 concludes the paper.",7
19.0,3.0,Experimental Economics,11 July 2015,https://link.springer.com/article/10.1007/s10683-015-9456-x,Does money impede convergence?,September 2016,John D. Hey,Daniela Di Cagno,,Male,Female,Unknown,Mix,,
19.0,3.0,Experimental Economics,28 July 2015,https://link.springer.com/article/10.1007/s10683-015-9457-9,A theoretical and experimental appraisal of four risk elicitation methods,September 2016,Paolo Crosetto,Antonio Filippin,,Male,Male,Unknown,Male,"Since uncertainty is a pervasive phenomenon in economic decisions, properly measuring attitudes toward risk is crucial in drawing conclusions from economic theory. Scholars have proposed a strikingly long list of methods to measure risk preferences usually by making subjects choose among lotteries. This is done in a variety of ways. The task can entail a single choice among a set of predetermined prospects presented in an abstract way (Binswanger 1981; Eckel and Grossman 2008a) or be framed as an investment decision (Gneezy and Potters 1997; Charness and Gneezy 2010). Alternatively, subjects might be asked to take multiple decisions between pairs or sets of risky lotteries presented in a structured (Holt and Laury 2002; Garcia-Gallego et al. 2012) or random way (Hey and Orme 1994). Lotteries are sometimes presented by means of visual tasks without making explicit reference to probabilities (Slovic 1966; Lejuez et al. 2002; Crosetto and Filippin 2013). Other designs elicit the certainty equivalent of some lotteries (Becker et al. 1964), let the subjects choose among an increasing sure amount and a fixed lottery (Abdellaoui et al. 2011), or ask subjects to input a value for one of the outcomes of a lottery that would make them indifferent with respect to another offered lottery (Wakker and Deneffe 1996). Risk preferences have also been indirectly derived from bids in first price sealed bid auctions (Cox et al. 1982). All the aforementioned tasks make use of remunerated choices within incentive compatible designs. A different and widely used approach is to ask subjects to directly report their risk preferences. This can be done using a single question such as the one contained in the German Socio-Economic Panel Study (SOEP, Wagner et al. 2007) or asking questions about hypothetical real-life decisions, as done by the Domain-Specific Risk-Taking Scale (DOSPERT, Blais and Weber 2006). Such a florilegium of alternatives can at least in part be explained by different research goals. For instance, different tasks should be used if the researcher wants to investigate risk preferences per se, or if the aim is instead to control for risk attitudes while analysing choices in other contexts that nonetheless involve uncertainty. While some characteristics should be common to both goals, e.g., a sound theoretical underpinning, others are more goal-specific. If the target is just to control for risk preferences, the ideal risk elicitation mechanism should also be easy to understand and fast to implement, possibly paying the lowest possible price in terms of loss of precision. In this paper we focus on a battery of incentivised tasks that are well-suited to elicit risk preferences as controls, i.e., to be used as companion tasks in experimental sessions in which the core treatments deal with other topics involving uncertainty: the multiple price list, in its Holt and Laury (2002) incarnation (henceforth, HL); an ordered lottery choice task, in the version implemented by Eckel and Grossman (2002, 2008a) (EG); the Investment Game by Gneezy and Potters (1997) (GP); the Bomb Risk Elicitation Task by Crosetto and Filippin (2013) (BRET). Moreover, we include two self-reported questionnaire measures, and namely the German Socio-Economic Panel Study risk question (SOEP, Wagner et al. 2007), and the Domain-Specific Risk-Taking Scale (Blais and Weber 2006, DOSPERT).  While many other risk elicitation mechanisms exist,Footnote 1 we focus on the ones mentioned above as they are among the most commonly used, they arguably result in a relatively lower cognitive load for the subjects,Footnote 2 and they are fast and easy to implement. Other scholars have already compared some of these tasks. Deck et al. (2010) compare four common risk elicitation tasks: HL, EG, the Balloon (Lejuez et al. 2002), and a version of the ’Deal or Not Deal’ TV show; Deck et al. (2013) also include the DOSPERT questionnaire. Bruner (2009) uses multiple price-lists. Harbaugh et al. (2010) compare the price-based Becker–DeGroot–Marschack (BDM) mechanism with a choice-based procedure. Reynaud and Couture (2012) elicit risk preferences of a random sample of French farmers using four different elicitation methods (HL, EG, the DOSPERT, and the SOEP). Dave et al. (2010) stress the trade-off between the comprehensibility and precision of the task, comparing HL and EG. Charness et al. (2013) survey the literature to discuss merits and weaknesses of HL, GP, EG and the Ballon. A low correlation, if any, in the observed behaviour across tasks is a recurrent finding; Isaac and James (2000) even find a negative correlation between choices in different tasks. Even abandoning the assumption of a single individual risk attitude and adopting the concept of a rich, domain-specific risk trait does not solve the problem of a low individual correlation, as documented by Deck et al. (2013). In this paper we perform an in-depth comparison of four of the most used risk elicitation tasks, with the aim of investigating if the tasks themselves might be heterogeneous enough to generate (at least some of) the observed instability of behavior. We do so in three ways. First, we run a simulation exercise. Risk attitudes are a latent construct that can only be indirectly and imperfectly measured, and the degree of measurement error is possibly influenced by the characteristics of the elicitation methods. The simulations allow us to measure the bias introduced by the mere mechanics of the tasks in the estimation of an underlying known distribution of risk preferences, imposing that no behavioral artifacts, like framing effects, enter the picture. We find that the different methods do introduce systematic task-specific measurement errors. When coupled with stochastic preferences and trembles, the estimated preferences diverge considerably and in task-specific directions from the underlying true values. Second, we run a between subjects, one-shot replication of the chosen tasks within a homogeneous subject pool. All the aforementioned comparisons opted for a within-subject design, consistent with the focus on individual preference instability.Footnote 3 A between subjects design bars us from any conclusion about preference instability, but fits well with our aim of focusing on the bias introduced by the tasks themselves.Footnote 4
 We find that preferences estimations vary widely across tasks, even if the underlying population self-reported risk attitudes are comparable across treatments. The experimentally observed variation goes in the same direction as the one found through simulations, although sometimes its magnitude does not compare. The variance of the choices reflects instead very closely the pattern determined by the bias induced by the mechanics of the tasks. Third, we investigate the possibility that the tasks elicit different types of preferences, rather than simply provide a different measure of the same preferences. We examine in detail the tasks to find out which of their characteristics might trigger different preferences. We find that some tasks feature a safe option, likely to induce certainty effects (Andreoni and Sprenger 2011, 2012) or to act as a focal reference point against which lower outcomes could be perceived as losses. The presence of a safe option appears to play a role from a gender perspective, too. The outline of the paper is as follows. In Sect. 2 we describe the four risk elicitation tasks that we compare in this paper. The simulations are reported in Sect. 3. Section 4 reports results of the between subjects experimental replication of the tasks. Section 5 discusses the role played by the safe option and by other characteristics of the tasks and concludes.",106
19.0,3.0,Experimental Economics,11 July 2015,https://link.springer.com/article/10.1007/s10683-015-9458-8,Pricing competition: a new laboratory measure of gender differences in the willingness to compete,September 2016,John Ifcher,Homa Zarghamee,,Male,Female,Unknown,Mix,,
19.0,3.0,Experimental Economics,23 July 2015,https://link.springer.com/article/10.1007/s10683-015-9459-7,Social preferences and lying aversion in children,September 2016,Valeria Maggian,Marie Claire Villeval,,Female,Female,Unknown,Female,"Recent research has shown that some people seem to have a preference for honesty based on an internalized moral norm. Contrary to the notion that individuals rationally violate moral norms provided this brings marginal net benefits, people may refrain from lying in such a situation. However, pursuing a moral conduct may sometimes conflict with the desire to improve others’ welfare. When the pursuit of other-regarding preferences implies lying, individuals face a moral dilemma. Conditioning lying behavior on preferences over distribution is therefore fundamental. Analyzing such a dilemma is extremely relevant in childhood, a crucial phase in the process of development of both other-regarding preferences and moral reasoning. Indeed, psychologists have shown that persistent lying at a young age may be associated to a number of disruptive behaviors (Gervais et al. 2000; for a review, see Stouthamer-Loeber 1986). In this paper we analyze whether children’s deceitful behavior depends on their other-regarding preferences in presence of economic incentives. Collecting data both on deceptive behavior and on social preferences in a natural environment is, however, extremely difficult. Using survey data with children on this topic is unlikely to provide reliable information. For that reason, we conducted a controlled experiment in classrooms in Italy. Our experiment involved 637 children in three age groups, from middle childhood (7–8 and 9–10 years old) to early adolescence (11 and 14 years old). We gave children the opportunity to lie in order to achieve their preferred outcome, making them conscious that their choice would influence both their own payoff and their partner’s welfare. We study whether having a preference for an other-regarding or for a self-regarding allocation of resources affects children’s willingness to lie to achieve it, according to their age and gender. Precisely, our experiment consists of a modified version of the dictator game that was played in two stages. The first stage allows us to measure the social preferences of children, while the second stage aims at characterizing the lying propensity. In the first stage each child had to choose between two options for the allocation of resources between himself and an anonymous partner. In the second stage a random device selected one of the two same options. Each child was then asked to report the option that had been randomly drawn. If this option did not correspond to his preferred one, the child had the possibility to lie by reporting his preferred option instead of the observed one. Both decisions were made in private in a separate room. This design allows us to examine whether children’s individual lying behavior is conditional on their preferences over allocations. We implemented three different treatments with different allocation options to study various social preferences (namely, altruism, inequity aversion, and efficiency concerns). Importantly, equal payment does not always represent the fair choice so as to avoid obvious focal points across treatments. Our main contribution is the study of the co-evolution of social preferences and lying behavior, using a combination of simple social preferences and dishonesty measures. The design of the experiment and the use of a computerized recording method allow us to identify honesty at the individual level. It also permits to analyze how often children activate the random device to increase their chance to observe their favorite outcome. This may capture the search for self-justification in the reporting decision, as psychologists have shown the importance of maintaining a positive self-image while lying (Mazar et al. 2008; Shalvi et al. 2011; Pittarello et al. 2015). We hypothesized two main plausible developmental pathways. On the one hand, since unconditional lying aversion has been observed in adults, we expected the internalization of the value of honesty to develop in childhood. On the other hand, since adults are sensitive to the consequences of their lies on others we expected that older children, who are more likely to care about other’s welfare as they age, become also more likely to lie to benefit others. Our results show that a large fraction of children is reluctant to tell lies. They also show that lying behavior does not increase linearly across age groups, as we observe that 9–10 years old children are more likely to lie than the oldest ones. Indeed, while other-regarding preferences develop with age, lying behavior does not develop along the same path. Our analysis at the individual level reveals that ethical preferences are correlated with social preferences. The frequency of lie telling is much higher for children having a preference for an allocation of resources that reveals selfishness or envy, independently on age, than for socially oriented children. For example, when the choice in the first stage of the experiment was between a selfish and an equal allocation of resources, children who exhibited inequity aversion were less likely to lie in the second stage to implement an equal sharing compared to children who made an initial selfish choice. For their part, altruistic children never lied. Finally, when the choice was between an efficient and an equal allocation of resources, children who exhibited efficiency concerns were less prone to lie to increase the recipients’ payoff than envious children who lied to decrease the other’s payoff without increasing their own payoff. Since we find more social preferences in older children, this explains that, on average, the frequency of lies is lower among older children. Moreover, male children tell more lies that hurt others compared to female children, but the gender gap tends to disappear when aging. Interestingly, we find that older children, compared to younger ones, are more likely to lie if they can manipulate the truth at their advantage without altering their self-image, by activating the random device several times until they can observe their preferred outcome. This suggests that older children need more self-justification to deceive. The remainder of our study proceeds as follows. Section 2 briefly reviews the recent literature. Section 3 describes our experimental design. Section 4 develops the experimental results and Sect. 5 discusses these results and concludes.",38
19.0,4.0,Experimental Economics,09 September 2015,https://link.springer.com/article/10.1007/s10683-015-9465-9,Combining “real effort” with induced effort costs: the ball-catching task,December 2016,Simon Gächter,Lingbo Huang,Martin Sefton,Male,Unknown,Male,Male,"Experiments using real effort tasks enjoy increasing popularity among experimental economists. Some frequently used tasks include, for instance, number-addition tasks (e.g., Niederle and Vesterlund (2007)), counting-zero tasks (e.g., Abeler et al. (2011)) and slider-positioning tasks (Gill and Prowse 2012).Footnote 1 In this paper, we present a novel computerized task, called the “ball-catching task”, which combines a tangible activity in the lab with induced material cost of effort.Footnote 2 In the task, a subject has a fixed amount of time to catch balls that fall randomly from the top of the screen by using mouse clicks to move a tray at the bottom of the screen. Control over the cost of effort is achieved by attaching material costs to mouse clicks that move the tray. The ball-catching task shares an advantage of real effort tasks in that subjects are required to do something tangible in order to achieve a level of performance, as opposed to simply choosing a number (as is done in experiments that implement cost of effort functions using a pure induced value method, where different number choices are directly linked with different financial costs). A drawback, however, of existing real effort tasks is that in using them the researcher sacrifices considerable control over the cost of effort function. As noted by Falk and Fehr (2003): “while ‘real effort’ surely adds realism to the experiment, one should also note that it is realized at the cost of losing control. Since the experimenter does not know the workers’ effort cost, it is not possible to derive precise quantitative predictions” (p. 404). Incorporating material effort costs re-establishes a degree of control over effort costs and, as we shall demonstrate, allows researchers to manipulate observable effort costs and to make point predictions on effort provision. Here, we report three studies aimed to evaluate the ball-catching task. In Study 1, we examine individual performance on the ball-catching task under piece-rate incentives. Subjects incur a cost for each mouse click and receive a prize for each ball caught. We first show that clicking behavior corresponds closely to comparative static predictions derived from piece-rate incentive theory. We then estimate the relationship between clicks and catches and use this to predict how the number of clicks will vary as the costs of clicking and the benefits of catching are manipulated. We find that the number of mouse clicks is close to the predicted number of clicks. These findings also add to the literature on empirical testing of incentive theories (Prendergast 1999) by presenting experimental evidence on a tangible task supporting basic piece-rate incentive theory. By comparison, the prominent field evidence reported by Lazear (2000) and lab evidence provided by Dickinson (1999) support comparative static predictions of basic incentive theory, whereas we show that in the ball-catching task the theory also predicts activity levels (number of clicks) accurately. In Study 2, we demonstrate how the task can be implemented in some classic experiments. We administer the task in experiments used to study cooperation, fairness and competition, namely, team production (e.g., Nalbantian and Schotter (1997)), gift exchange (e.g., Fehr et al. (1993)) and a tournament (e.g., Bull et al. (1987)). In all three experiments, the results reproduce the stylized findings from previous experiments that used purely induced values. Moreover, behavior also follows equilibrium point predictions closely in those experiments where point predictions are available. In Study 3, we introduce an online version of the ball-catching task and conduct the same experiment as in Study 1 using Amazon Mechanical Turk workers as participants. Comparative statics results are replicated, which we view as an important robustness check. Behavior is noisier than in the lab, however, which most likely is due to the more varied decision environment online compared to the lab. The remainder of the paper is organized as follows. In Sect. 2 we describe the ball-catching task. In Sects. 3–5 we report the three studies using the task. Section 6 provides a comprehensive discussion of the results of our three studies. Section 7 concludes.",41
19.0,4.0,Experimental Economics,11 August 2015,https://link.springer.com/article/10.1007/s10683-015-9461-0,Who knows it is a game? On strategic awareness and cognitive ability,December 2016,Dietmar Fehr,Steffen Huck,,Male,Male,Unknown,Male,"Many interactions are characterized by rather subtle strategic structures that individuals might easily overlook. For example, a worker in a firm might think that his wage (or a future promotion) just depends on his own effort while, in reality, it is linked to the competition between different units and their managers.Footnote 1 In order to understand such strategic ripple effects it is important to recognize the variability of individual strategic sophistication and to uncover the underlying attributes. In this paper, we link economic behavior in a strategic situation to cognitive ability and beliefs about others’ cognitive ability to explore whether subjects’ behavior displays some minimum strategic sophistication. Our evidence comes from a laboratory study using Nagel’s (1995) classic beauty contest game (BCG). In the standard BCG, subjects have to pick a number \( a \in \left[ {0, 100} \right] \) and the subject who is closest to a fraction p of the group average wins a prize m. For \( p \in \left( {0, 1} \right) \) the unique Nash equilibrium is zero, which is also the only outcome surviving iterated elimination of (weakly) dominated strategies. In the last decade BCGs have been extensively used to examine rationality and belief formation, in particular, in the context of level-k models (see, for example Nagel 1995, Ho et al. 1998, Costa-Gomes and Crawford 2006, and for a review on recent evidence on strategic thinking Crawford et al. 2013). The typical level-k model assumes that higher level players anchor their best responses on the behavior of non-strategic level-0 players and that belief formation proceeds in iterative steps. Thus choices of sophisticated players depend on their model of level-0 play, their own level of reasoning and their expectation about the level of reasoning of others. From the perspective of the vast literature on level-k thinking, we take one step back and pose the question which subjects realize that reasoning about how to play the game requires reasoning about others, i.e., who is aware of the strategic setting. We identify subjects’ awareness of the importance of reasoning about others through a short and reliable measure for cognitive ability and by eliciting subjects’ beliefs about others’ cognitive ability. Intuitively, cognitive ability might be an important determinant of the reasoning process and, recently, several studies related subjects’ behavior in the BCG directly to their cognitive ability (e.g., Burnham et al. 2009, Branas-Garza et al. 2012, Carpenter et al. 2013, Georganas et al. 2015, and Gill and Prowse 2015) or brain activity (Coricelli and Nagel 2009). If a higher level of reasoning is associated with higher cognitive ability, then we expect that those with higher cognitive ability will take their opponents’ cognitive ability into account when reasoning about their opponents’ sophistication. Accordingly, we posit that only subjects with high cognitive ability condition their choices in the BCG on their beliefs about the cognitive skills of opponents. While this identification is indirect, an important advantage of using cognitive ability measures is that it does not conflict with behavior in the game itself as would be the case with direct belief elicitation and targeted questions. We find a strong link between cognitive ability, beliefs about others’ cognitive ability and choices in the BCG. Essentially, our data shows that subjects need to have some minimal cognitive ability to realize the importance of reasoning about others in the BCG. Subjects below a certain threshold play the BCG as if it was a game of luck: there is no evidence for reasoning about others and their choices appear to be randomly distributed over the whole interval.Footnote 2 In contrast, subjects above the threshold avoid choices above 50 and do reason about others. More specifically, their choices depend on their beliefs about others’ cognitive ability. 
Our paper complements the recent literature on cognitive ability and the BCG mentioned above. While Burnham et al. (2009) and (Carpenter et al. 2013) document that lower choices in a one-shot BCG are associated with higher cognitive ability as measured through a common IQ test, other studies find no or little evidence for such a relationship when looking at initial choices (e.g., Branas-Garza et al. 2012; Georganas et al. 2015 and Gill and Prowse 2015).Footnote 3 Using fMRI data, Coricelli and Nagel (2009) find substantial evidence that higher levels of reasoning in BCGs trigger activity in certain brain areas associated with mentalizing. Our study goes further and focuses on subjects’ beliefs about the cognitive ability of their opponents. The findings of our study suggest that cognitive ability is an important ingredient for strategic thinking in novel situations and more importantly that strategic reasoning is endogenously determined by expectations about others’ cognitive skills. 
Two closely related studies attempt to manipulate subjects’ beliefs about their opponents’ sophistication without measuring cognitive ability directly. Agranov et al. (2012) show that behavior shifts to higher observed levels of reasoning, when undergraduate subjects know they are playing the BCG against experienced graduate students instead of other undergraduates. In a similar vein Alaoui and Penta (2015) show that subjects’ level of reasoning in an 11–20 game varies with their knowledge about the inferred sophistication of opponents.Footnote 4
 More generally, our findings add to an emerging literature that explores the impact of cognitive ability on economic decision making. For example, the studies of Benjamin et al. (2013), Burks et al. (2009), Dohmen et al. (2010), Frederick (2005) or (Huck and Weizsäcker 1999) demonstrate that higher cognitive ability is associated with less biased risk-taking and time discounting behavior.Footnote 5 In light of this evidence, our results emphasize the importance of cognitive ability for economic behavior in strategic contexts.",36
19.0,4.0,Experimental Economics,03 September 2015,https://link.springer.com/article/10.1007/s10683-015-9463-y,Fairness is intuitive,December 2016,Alexander W. Cappelen,Ulrik H. Nielsen,Erik Wengström,Male,Male,Male,Male,"A key question in the social sciences is whether it is intuitive to behave in a fair manner or whether fair behavior requires active self-control. One way to approach this question is to study how long it takes a person to make a decision when choosing between alternatives that are more or less fair. Since a decision that relies on intuition is typically made faster than a decision that relies on deliberation, the response time of a fair decision relative to a selfish decision provides an important indication of the intuitiveness of fair behavior; if fair behavior is intuitive, we would expect a fair decision to be made faster than a selfish decision. Recently, several experimental studies have used data on subjects’ response time in economic games to argue that fair behavior is intuitive (Rubinstein 2004, 2007; Rand et al. 2012; Fischbacher et al. 2013; Di Guida and Devetag 2013; Lotito et al. 2013; Nielsen et al. 2014).Footnote 1 In a series of public goods games, Rand et al. (2012) and Lotito et al. (2013) find that the contribution to the public good is decreasing in the participant’s response time. A similar association has been documented in the ultimatum game where the response time of the proposer is negatively correlated with the share offered to the responder (Brañas-Garza et al. 2012). In line with these results, studies that exogenously manipulate the participant’s response time show that people tend to contribute more to the public good under time pressure and less when they are forced to delay making their decision (Cappelletti et al. 2011; Grimm and Mengel 2011; Rand et al. 2012; Rand and Kraft-Todd 2014).Footnote 2 The negative association between response time and fair behavior in these experiments has been interpreted as showing that fair behavior is intuitive. It has been argued that the reason why fair behavior is intuitive in social dilemma experiments is that cooperation has proven a successful strategy in most social interactions outside the lab. This is known as the Social Heuristics Hypothesis (Rand et al. 2012; Rand and Kraft-Todd 2014; Rand and Peysakhovich forthcoming). A few studies have, however, challenged these findings. Tinghög et al. (2013) do not find that time pressure increases public good contributions and Piovesan and Wengström (2009) find that faster subjects more often than slower subjects make egoistic choices in distributive situations. A key problem with the previous studies on response time and fairness, which could explain the conflicting results in the literature, is the fact that the overall response time in such experiments does not only depend on whether the decision is made intuitively. As illustrated in Fig. 1, people can be seen as going through three phases when making a decision in an economic experiment. First, they have to read and understand the decision problem, then they have to make their decision (\(t_2\)), and, finally, they have to implement this decision on the computer screen (T). The response time T will thus not only depend on whether the decision itself is based on intuition or deliberation, but also on the subject’s cognitive ability and swiftness in implementing their decision. This introduces an important potential confound when a short response time is interpreted as indicating intuitive decision-making, since the short response time could also reflect that the participant easily grasps the decision problem (\(t_1\)) or is fast in implementing the decision (\(T - t_2\)). Hence, a negative association between the participant’s response time (T) and the fairness of his or her behavior does not necessarily reflect that there is a negative association between decision time (\(t_1\)-\(t_2\)) and fair behavior; it might only reflect that there is a negative association between cognitive ability, swiftness, and the weight attached to fairness. The components of response time. Note the figure illustrates the three phases constituting a participant’s response time In the present paper, we employ an experimental design with two features that allow us to more clearly identify the association between decision time and fair behavior. The first feature is that we focus on the dictator game.Footnote 3 The advantage of the standard dictator game is that it requires little cognitive effort to understand the game. In particular, it is easy to identify the most selfish alternative as well as the most fair alternative. Thus, the time it takes to understand the decision task (\(t_1\)) is minimized, which reduces the potential confound created by heterogeneity in cognitive ability.Footnote 4 In contrast, the instructions for a public good game are clearly more demanding and it is also non-trivial to identify the selfish and the fair alternative in this game. In the ultimatum game, most people easily identify the fair alternative as a 50-50 split, but it is inherently difficult to identify the selfish alternative since it depends on the participant’s belief about how the other participant will respond. The second crucial feature of our design is that we collect independent measures of each participant’s swiftness and cognitive ability. This enables us to control for any remaining confound created by heterogeneity in subject’s swiftness and cognitive ability. Our experiment was carried out with a large and heterogenous sample of the Danish adult population recruited with the assistance of Statistics Denmark. This means that the participants in this experiment are much more diverse than a typical sample of college undergraduates. The collaboration with Statistics Denmark also allows us to match experimental data with data from the Danish population registers. This enables us to study whether there are systematic differences in the population with respect to what they find intuitive when making a distributional choice. Our first main result, reported in Fig. 2, is that there is indeed a close association between fair behavior and response time. The average response time among the selfish participants (i.e. those who shared nothing with the other participant) was 48.5 sec, whereas it was only 38.4 sec among the fair (i.e. those who split 50–50). We find considerable heterogeneity in both swiftness and cognitive ability among the participants in the experiment. In fact, we find that the observed variance in swiftness is as large as the observed variance in response time, and the differences in cognitive ability are also striking. The association between response time and fair behavior is, however, robust to controlling for these and other factors that could affect the subject’s response time. We thus provide clean evidence of fairness being intuitive. Our second main result is that the association between fair behavior and short response time holds across groups in society when differentiating by age, gender, and length of education. Taken together, our two main results provide compelling evidence suggesting that the predisposition to act fairly is a general human trait. The structure of the paper is as follows: Sect. 2 presents the experimental design and the sample. Section 3 reports the results, while Sect. 4 provides some concluding remarks. Average response time of the selfish and the fair. Note the figure reports the average response time in seconds (top-coded at 120 sec) for participants who shared nothing (the selfish, 25 % of the 1508 participants) or shared equally (the fair, 52 % of the 1508 participants) with the other participant. Standard errors are indicated",53
19.0,4.0,Experimental Economics,11 September 2015,https://link.springer.com/article/10.1007/s10683-015-9466-8,The effects of endowment size and strategy method on third party punishment,December 2016,Jillian Jordan,Katherine McAuliffe,David Rand,Female,Female,Male,Mix,,
19.0,4.0,Experimental Economics,22 September 2015,https://link.springer.com/article/10.1007/s10683-015-9467-7,"Same process, different outcomes: group performance in an acquiring a company experiment",December 2016,Marco Casari,Jingjing Zhang,Christine Jackson,Male,Unknown,Female,Mix,,
19.0,4.0,Experimental Economics,25 September 2015,https://link.springer.com/article/10.1007/s10683-015-9468-6,Not just like starting over - Leadership and revivification of cooperation in groups,December 2016,Jordi Brandts,Christina Rott,Carles Solà,Male,Female,Male,Mix,,
19.0,4.0,Experimental Economics,12 November 2015,https://link.springer.com/article/10.1007/s10683-015-9469-5,Procedural fairness in lotteries assigning initial roles in a dynamic setting,December 2016,Gianluca Grimalda,Anirban Kar,Eugenio Proto,Male,Unknown,Male,Male,"The idea that individuals’ sense of justice encompasses not just inequality in final outcomes, but also the fairness of the process leading to such outcomes, is now widespread in the social sciences and political philosophy. Procedural fairness is made possible by “impartial rules ensuring that each of the agents involved in an interaction enjoys an equal opportunity to obtain a satisfactory outcome” (Krawczyk 2011). Experimental evidence generally shows strong individual preferences for fair procedures and that individuals are willing to accept more unequal final allocations, the fairer the procedures determining such allocations. In this paper, we extend the study of fairness in three directions that have not been explored so far. Firstly, we focus on lotteries determining the initial roles in a two-person game. To the best of our knowledge, all the experimental literature has thus far focused on lotteries determining the final payoffs of a game. Secondly, we study the effect of modifying procedural fairness in a dynamic as well as in a static sense. Finally, we analyse whether assigning individuals a minimal chance of achieving an advantaged role is enough to make them willing to accept substantially more inequality. We take the Ultimatum Game (UG henceforth) as our basic interaction. This game is suitable for our experiment because one role has a clearly identifiable advantage over the other. The proposer has greater bargaining power and hence can expect a greater share of the surplus than the receiver (Oosterbeek et al. 2004). When asked to bid on the two roles of a UG before playing the game, players offer twice as much to occupy the proposer’s role as they do for the receiver’s role (Guth and Tietz 1986). A lottery giving one player higher chances of being assigned the proposer’s role than another player can conceivably be seen as unfair. The main novelty of our experimental design is to make the access to the two UG roles subject to the outcome of one lottery, and to manipulate its degree of fairness. The baseline case is that both players have equal
opportunities, as the lottery assigns both individuals a 50 % chance of acquiring the proposer role. The initial lottery becomes increasingly biased in favour of one of the two players in three other treatments. The favoured player has, respectively, 80, 99, and 100 % probability of becoming the proposer, while the unfavoured player has the residual probability. Receivers who are only concerned with the outcomes of the game should behave in the same way in these treatments. Consequently, we interpret differences in rejection rates across treatments as caused by receivers’ preferences over procedural fairness, as in Bolton et al. (2005). We run 20 interactions of the stage game with random rematching of subjects before each interaction. We define the probability that the unfavoured player has of becoming the proposer within each round as p, where \( p \in \Pi \equiv \left\{ 0\,\%,1\,\%,20\,\% \right\} \). We define the bias in this lottery as measuring static unfairness, because it refers to the unfairness of the lottery within each round. In addition, we also study procedural fairness in a dynamic perspective. In a subset of our treatments, which we call variable position conditions (VPCs), we introduce another lottery preceding the lottery assigning the game roles. This first lottery gives each player equal chances to acquire either the probability p or the probability \((1-p)\) of becoming proposer in the second lottery. In VPCs this unbiased first lottery is run at the beginning of each round. In the alternative subset of treatments, which we call fixed position conditions (FPCs), this first lottery is only run in the very first round. Therefore, in FPCs an unfavoured player keeps the same probability p of being assigned the proposer’s role in each of the 20 rounds. We argue that VPCs guarantee dynamic fairness, because the expected probability of being assigned the advantaged role in the game in future rounds is always the same—namely, one half—for every player, regardless of the round of the experiment. This cannot be said for FPCs (except for the very first round). Again, individuals who are only concerned with final outcomes should be indifferent to this manipulation, while receivers who are concerned with procedural fairness will respond to it. Overall, we have six treatments (in addition to the baseline 50 % condition): one treatment for each of the three \(p\in \Pi \), run under either the FPCs or VPCs. We find that procedural fairness matters under all of the accounts outlined above. A general trend exists such that receivers reject less, ceteris paribus, when p is higher (static fairness), and when the meta-lottery reassigning p at every round is run (dynamic fairness). They also reject less when they have been granted a minimal opportunity to acquire the advantaged position in comparison to being given no chance. However, this result in VPCs is not robust to the introduction of sample demographic controls. We also find unexpected results, in that granting full equality of opportunity does not lead to the lowest rejection rates. Rather, these are obtained in two of the VPCs. Our research enables us to speculate on the shape of individuals’ demand for opportunities’ and suggests that individuals’ subjective perceptions of when the ‘playing field’ is ‘level’ may radically differ from the objective distribution of chances. This calls for more in-depth research on the interaction between the various dimensions of fairness that people perceive. It also suggests that policies aiming at maximizing the opportunities for disadvantaged groups in society should take into account individuals’ preferences over procedural as well as outcome fairness. The paper is organised as follows. We review the existing literature in the next section. We enumerate the main hypotheses and describe the experimental protocol in Sect. 3. Section 4 reports the results. Section 5 discusses the results and concludes the paper.",17
19.0,4.0,Experimental Economics,12 November 2015,https://link.springer.com/article/10.1007/s10683-015-9470-z,Leadership effectiveness and institutional frames,December 2016,Gerrit Frackenpohl,Adrian Hillenbrand,Sebastian Kube,,Male,Male,Mix,,
19.0,4.0,Experimental Economics,04 November 2015,https://link.springer.com/article/10.1007/s10683-015-9472-x,Patience auctions: the impact of time vs. money bidding on elicited discount rates,December 2016,Christopher Y. Olivola,Stephanie W. Wang,,Male,Female,Unknown,Mix,,
19.0,4.0,Experimental Economics,11 November 2015,https://link.springer.com/article/10.1007/s10683-015-9473-9,Experimental evidence that quorum rules discourage turnout and promote election boycotts,December 2016,Luís Aguiar-Conraria,Pedro C. Magalhães,Christoph A. Vanberg,Male,Male,Male,Male,"Many democratic decision making institutions involve quorum rules. The adoption of such rules is commonly motivated by concerns about the “legitimacy” or “representativeness” of decisions reached when only a subset of eligible voters participates. (LeDuc 2003, p. 172; Qvortrup 2005, p, 173). Instead of allowing measures to pass if they simply obtain the support of the majority of those who vote, participation quora require that the total number of votes exceeds a particular threshold. In other cases, the required threshold is applied to the number of votes cast in favor of the proposal (approval quora). In what concerns the regulation of referendums and initiatives, for example, quora have been adopted in many countries across the world (no less than 53, according to International IDEA’s Direct Democracy dataset),Footnote 1 including Japan (Hizen and Shinmyo 2011), Colombia, Venezuela or Taiwan (IInternational IDEA 2008), almost all of Germany for municipal and state referenda and initiatives (Kaufman et al. 2008; Verhulst and Nijeboer 2008), and other seventeen European Union countries, either at the national (Aguiar-Conraria and Magalhães 2010a, p. 70) or the local levels (Schiller 2011, pp. 28–30). To be sure, when issues are placed on a ballot together with concurrent local, state or federal elections, as is common in the United States, the consequences of quora for voting behavior and their generalizability are more difficult to appreciate theoretically or empirically. However, even in countries such as the United States or Canada, special elections called specifically to hold a vote on a particular issue under quorum rules are certainly not unheard of,Footnote 2 and, in general, special elections on single issue ballots are extremely common in most countries with direct democracy.Footnote 3 What do we know about the consequences of quora in those cases? Most of the theoretical and empirical research on this issue supports the same conclusion: participation quora decrease turnout. Theoretically, Côrte-Real and Pereira (2004) examine binary-choice voting rules and find that the addition of a turnout condition such as a participation quorum encourages abstention, allowing those who prefer the Status Quo to manipulate the outcome by failing to turn out. Similarly, Hizen and Shinmyo (2011) find that participation quora create incentives for strategic abstention when the participation quorum is anything other than negligibly low. From a different perspective, that of a group turnout model, Herrera and Mattozzi (2010) argue that participation quora distort the incentives for parties and interest groups to mobilize the electorate. Aguiar-Conraria and Magalhães investigate the matter both from the perspective of a decision theoretic model with exogenous pivotal probabilities (2010a) and in a pivotal voter model (2010b), in both cases suggesting that participation quora are likely to decrease turnout. Although empirical work remains scarce, case studies of particular countries and referenda have provided many illustrations of demobilizing effects of participation quora. Case studies on referenda held in Italy (Uleri 2002), the Weimar Republic (West 1985, p. 247; Suksi 1993, p. 95), Slovakia, and Columbia (International IDEA 2008, p. 182) have suggested that participation quora discourage turnout especially among opponents of the measure under consideration. The resulting differential turnout can lead to outcomes in which a large majority votes in favor of the measure, and yet it fails because the quorum is not reached. Finally, in a more systematic observational study, Aguiar-Conraria and Magalhães (2010a), using data for all referenda held in current European Union countries from 1970 until 2007, conclude that the existence of a participation quorum decreased turnout up to 14 % points. There is a second main question about which results, taken together, are less conclusive: what are the different consequences of participation and approval quora? Herrera and Mattozzi (2010) argue that their analysis of participation quora carries over to approval quora, making them essentially equivalent. Anecdotal evidence does reveal several historical cases of very low levels of turnout on the part of those who prefer the Status Quo in referenda under approval quora, just like what happens with participation quora (Suksi 1993, p. 211; Svensson 1996, pp. 38–40; Verhulst and Nijeboer 2008, pp. 19–21). However, there are also indications that approval and participation quora may not be equivalent. On the basis of a pivotal voter model, Aguiar-Conraria and Magalhães (2010b) suggest that the two types of quora may promote different types of abstention, and describe situations where approval quora actually contribute to increase participation. In a different setup, where they distinguish between staying at home and abstaining, Laruelle and Valenciano (2011 and 2012) argue that the approval quorum is the one with the least distortionary effects, no matter what the majority rule is. Given these contradictory findings about the equivalence of participation and approval quora, one might turn to the only systematic empirical study we know of, Aguiar-Conraria and Magalhães (2010a), which finds that, unlike participation quora, approval quora seem to have no effects on abstention. However, it is well known that the use of field data involves a number of drawbacks. For example, it can easily be argued that the type of quorum rule in use may be endogenous. Another disadvantage is that we can only observe realized outcomes, and do not know the distribution of “actual” preferences. Finally, the outcomes we observe are at the aggregate level, and (so far) we have little information about the separate effects of quora on Supporters and Opponents of proposals. Thus, it is possible that approval quora lead to lower turnout among opponents and higher turnout among supporters, leaving the aggregate level of participation unchanged. Similar concerns have motivated previous authors to conduct laboratory experiments to investigate factors affecting voter participation (Levine and Palfrey 2007; Duffy and Tavits 2008; Palfrey 2009). Our approach in this study builds on prior experiments on voter participation. In particular, we introduce quorum rules into the pivotal voter model first developed by Palfrey and Rosenthal (1985). This model assumes that voting is instrumental, so that an individual’s decision to vote depends on her beliefs about the likelihood of casting the decisive vote. The implications and comparative static predictions of the pivotal voter model have been supported by abundant experimental evidence (Levine and Palfrey 2007; Duffy and Tavits 2008; Palfrey 2009), while field evidence is consistent—at least in the margins—with the model’s predictions about the effects of electorate size and election closeness (Blais 2000; Geys 2006). Our study is, to our knowledge, the first to test the effects of quorum rules under this framework in a controlled laboratory experiment.Footnote 4
 Our experimental design closely resembles that of Levine and Palfrey (2007), who test the effects of “closeness” and electorate size on turnout in an experimental pivotal voter game. The main difference between our setup and theirs is that we consider three different quorum restrictions (no quorum, approval quorum, participation quorum). As in other experiments on the pivotal voter model, we measure an individual participant’s propensity to vote under different conditions by eliciting her willingness to incur costs associated with voting. Our results confirm prior evidence that quorum rules have important consequences for turnout. In addition, our experiment allows us to determine among which type of voters those consequences are stronger and what kind of quorum is more consequential. In particular, both approval and participation quora reduce participants’ probability of voting. However, although the Opponents of the proposed measure are demobilized by both types of quora, this effect is significantly stronger under participation quora. Quora increase the probability that Opponents boycott the election, but do so more massively (increasing that probability by more than 40 % points) in the case of participation quora. Finally, we obtain the novel result that quora have different effects conditional on electors’ preferences: while the evidence clearly shows that they strongly decrease the turnout rate of Opponents (preference for preserving the Status Quo), it also suggests they may increase the turnout rate of Supporters (preference for changing the Status Quo).",8
20.0,1.0,Experimental Economics,07 March 2016,https://link.springer.com/article/10.1007/s10683-016-9477-0,Status quo effects in fairness games: reciprocal responses to acts of commission versus acts of omission,March 2017,James C. Cox,Maroš Servátka,Radovan Vadovič,Male,Male,Male,Male,"Does it make a difference whether a bad or good outcome results from an act of commission or an act of omission by another person?Footnote 1 In this paper we compare reciprocal responses to acts of commission, that actively impose harm or kindness, and acts of omission which represent failures to prevent harm or to act kindly. We use three experiments to test a hypothesis that acts of commission induce stronger reciprocal responses than comparable acts of omission. Each experiment has two treatments in which we compare the behavior in two games that vary in their initial endowments, which creates the distinction between the first mover’s acts of commission that alter the initial endowments and acts of omission that keep them unaltered. Importantly, we keep the terminal payoffs in both games identical. This gives us a clean test of the empirical significance of opportunities and payoffs that result from acts of commission that change the status quo versus acts of omission that preserve it. To investigate reciprocal preferences, we focus on what happens after a first mover chooses to uphold or overturn the status quo, that is, what is the reaction of another person to this choice. Data from the experiments provide support for the importance of discriminating between acts of commission and omission by a first mover in theoretical modeling of reciprocal behavior.",29
20.0,1.0,Experimental Economics,22 February 2016,https://link.springer.com/article/10.1007/s10683-015-9474-8,How private is private information? The ability to spot deception in an economic game,March 2017,Michèle Belot,Jeroen van de Ven,,Female,Male,Unknown,Mix,,
20.0,1.0,Experimental Economics,28 January 2016,https://link.springer.com/article/10.1007/s10683-015-9475-7,"Strong, bold, and kind: self-control and cooperation in social dilemmas",March 2017,Martin G. Kocher,Peter Martinsson,Conny E. Wollbrant,Male,Male,,Mix,,
20.0,1.0,Experimental Economics,23 March 2016,https://link.springer.com/article/10.1007/s10683-016-9476-1,Allotment in first-price auctions: an experimental investigation,March 2017,Luca Corazzini,Stefano Galavotti,Paola Valbonesi,Male,Male,Female,Mix,,
20.0,1.0,Experimental Economics,18 March 2016,https://link.springer.com/article/10.1007/s10683-016-9478-z,Peer effects on risk behaviour: the importance of group identity,March 2017,Francesca Gioia,,,Female,Unknown,Unknown,Female,"
The question of whether and how peers influence an individual’s behaviour has been widely investigated in economics literature. Considerable evidence suggests that individuals who are physically or socially close to a subject influence his/her behaviour and choices. Peers’ influence has been studied in the context of academic achievement, choice of university degree course, worker productivity, cheating behaviour and social outcomes such as joining student societies (Manski 1993; Sacerdote 2001; Zimmerman 2003; Stinebrickner and Stinebrickner 2006; Falk and Ichino 2006; Carrell et al. 2008; Mas and Moretti 2009; Imberman et al. 2012; Falk et al. 2013). Peer effects have also often been mentioned as a leading explanation for why people engage in risk taking activities such as smoking (Alexander et al. 2001), drug and alcohol use (Fergusson et al. 2002; Duncan et al. 2005; Powell et al. 2005; Lundborg 2006; Clark and Lohéac 2007), criminal activity (Fergusson et al. 2002; Bayer et al. 2009), financial decisions (Kelly and O’Grada 2000; Hong et al. 2004; Brown et al. 2008; Bursztyn et al. 2014; Cai et al. 2015) and entrepreneurship decisions (Nanda and Sørensen 2010; Falck et al. 2012; Lerner and Malmendier 2013). Despite their relevance for many social and economic interactions, little is known about the circumstances triggering peer effects. In this paper, we investigate the role of group identity, which psychologists define as “the portion of an individual’s self-concept derived from the sense of belonging to the social group” (Hogg and Vaughan 2002). Group membership is a ubiquitous feature of social and economic life. However, groups vary enormously and so does people’s attachment to different social groups. We hypothesize that the sense of belonging to a social group may affect the realization and magnitude of peer effects. Since the introduction of the minimal group paradigm by Tajfel (1970) and the subsequent development of the social identity theory (Billig and Tajfel 1973), different levels of group identity have been introduced to understand how and why people behave differently towards those that they share a common identity with. In particular, numerous studies document that people tend to behave more prosocially when they interact with members of their own group, but become less generous, less trusting, and less cooperative towards individuals who belong to different groups (Tajfel et al. 1971; Götte et al. 2006; Charness et al. 2007; Chen and Li 2009). The goal of this paper is to study whether and to what extent group identity plays a role in peer effects on risk behaviour. Some recent evidence suggests that not all peers matter and some matter more than others (Vaquera and Kao 2008; Lomi et al. 2011; Lin and Weinberg 2014; Borjas and Doran 2015). The sense of belonging to a group may be a possible explanation for this finding: individuals may only be affected by social groups they feel they belong to and the peers they are particularly attached to may matter more than other peers. Knowing that group identity is one of the mechanisms triggering peer effects may help the design of policy interventions the benefits of which may be increased by choosing the target peer group wisely. Also, considering that risk is at play in a large range of social economic decisions, such as choice of career, university degree course or study effort (Saks and Shore 2005; Belzil and Leonardi 2007; Caner and Okten 2010; De Paola and Gioia 2012), and that recent evidence shows that an individual’s risk behaviour is shaped by the behaviour of others in the immediate social environment, studying the role that group identity has in an individual’s decision-making when faced with risk would appear to be especially worthwhile. To our knowledge, we are the first to study how the degree of group identity interacts with peer effects. In this paper, we use procedures commonly used in the literature to induce different levels of group identity (Tajfel 1970; Chen and Li 2009) with the aim of investigating the impact of group identity on the magnitude of peer effects on an individual’s decisions in a risky setting. We run a laboratory experiment with 255 students. We measure individual risk behaviour by using the Bomb Risk Elicitation Task, an easy task in which subjects have to choose how many boxes to collect out of 100, 99 of which contain £0.10 while one contains a bomb. Earnings increase linearly with the number of boxes collected, but are zero if the bomb is collected. Peer influence is introduced by providing subjects with feedback on fellow group members’ decisions in the immediately preceding performance of the task. The experiment consists of a control group and four treatments. One treatment, called Anchoring treatment, is meant to distinguish peer effects from anchoring effects that may arise if the change in individual behaviour is driven by the exposure to numbers rather than by a desire to be similar to assigned peers. The other three treatments introduce a different level of group identity. The Random treatment matches individuals into groups of three at random. The Painting treatment introduces a less impersonal matching: individuals are first asked to express their painting preferences, by choosing their favourite paintings from within five pairs of paintings, and then are matched according to their painting preferences. Finally, the Chat treatment matches individuals according to their painting preferences and entails a group task which consists of their guessing the name of the artists responsible for two more paintings by using an online chat to ask for help from and offer aid to their fellow group members. This additional task is meant to enhance the level of perceived group identity by letting people interact with fellow group members more. We find evidence of peer effects in risk behaviour and find that they depend on the level of group identity. Individuals who are assigned to groups based on their painting preferences are more likely to conform to their peers than the control group (the group standard deviation falls by 8.5 boxes collected in the BRET) and the anchoring treatment (−7.8 boxes). Also, enhancing the level of group identity, by making people aware that they have the same painting preferences as their peers, significantly increases (by about 4.4 boxes) peer effects beyond those produced through a random group assignment. The chat treatment, which combines a preferences-based matching with a group task, does not induce significantly different peer effects from those found for the painting treatment (−7.8 boxes). We speculate that this may be because the group task has a different effect on perceived group identity as a consequence of the individual experience in the task. Indeed, we find that when interaction in the group task contributes to the enhancing of group identity, the magnitude of peer effects on risk behaviour does increase in comparison with the painting treatment. For example, with regard the control group, groups in the chat treatment whose participants consider their group to be more helpful than the average significantly reduce their heterogeneity in risk behaviour by 5.2 boxes more than groups in the chat treatment who do not find their group very helpful, and by 3.8 boxes more than groups in the painting treatment. Similar results are found for groups whose participants feel more attached than the average to their peers or reach an agreement on the possible answers in the group task very quickly. The relative position of the individual within the group in terms of risk behaviour plays an important role in the individual’s decisions when receiving feedback about peers’ previous decisions. Individuals whose peers are riskier than they are tend to increase their choice by 12.3 boxes compared with individuals with mixed peers, while individuals whose peers are less risky than they are tend to decrease it on average by 5.5 boxes. When ruling out the component of the effect due to regression to the mean, peers’ risk behaviour continues to play a significant role for bottom ranked individuals (+6.9 boxes, significant at the 1 % level) while the effect is very close to zero for top ranked individuals. The paper is structured in five parts. Section 2 presents a brief overview of the related literature. In Sect. 3, we describe our experimental design. Section 4 presents our empirical analysis. Section 5 concludes.",34
20.0,1.0,Experimental Economics,09 March 2016,https://link.springer.com/article/10.1007/s10683-016-9479-y,Clever enough to tell the truth,March 2017,Bradley J. Ruffle,Yossef Tobol,,Male,Male,Unknown,Male,"Is it possible to screen effectively for honesty? This is the million-dollar question for employers interviewing job candidates, investors vetting corporate conference calls, tax auditors, voters watching a political candidates’ debate, potential business partners and courting couples, to name a few examples. Continuing with the workplace example, the U.S. retail industry alone loses $53.6 billion a year to employee theft. Moreover, employee theft is on the rise due to poor pre-employment screening and a decline in supervision (Brooks and Chad 2013). Can employers do better in screening for honest employees? Polygraph tests have been shown to be unreliable and their use by employers is unlawful in North America and Europe. Neuroscience-based lie-detection technologies remain unproven and in their infancy. Consequently, many employers continue to rely on written personality tests consisting of self-report questions and job interviews led by the company’s human resource personnel to evaluate job candidates’ honesty. However, a considerable body of research casts doubt on the usefulness of these methods (see, e.g., Morgeson et al. 2007 for a survey and Ones et al. 2007 for a rejoinder). In this paper, we use the methods of a laboratory experiment on both soldier and civilian populations in the field to determine the effectiveness of the compulsory military entrance exam employed by the Israeli Defense Forces (IDF) to categorize soldiers on the basis of their cognitive ability and honesty, among other traits. We extend Fischbacher and Föllmi-Heusi’s (2013) innovative die-rolling paradigm to the field where 427 soldiers and 156 civilians each rolls a six-sided die in private and reports the outcome. For each additional pip reported on the die, soldiers are rewarded with a half-hour early release from the army base on Thursday and civilians with extra cash payment. We find soldiers with higher exam scores are more honest, that is, report lower die outcomes. On a civilian population that has completed its mandatory military service we again find that a higher entrance exam score predicts a lower die report. The robustness of the relationship between exam score and honesty to a civilian sample eliminates a set of strategic and reputational hypotheses for our result and points to the content of the entrance exam as an invaluable predictor of honesty. However, pinpointing which features of the exam account for honesty is complicated by the diverse number of items that determine a soldier’s score. Our experiments on civilians reveal that self-report honesty questions and a consistency check among them cannot explain subjects’ variation in reported die outcomes.Footnote 1 Instead, higher scores on two cognitive-ability tests (namely, the cognitive reflection task (CRT) (Frederick 2005) and an abbreviated version of the Raven advanced progressive matrices test (Arthur and Day 1994)) predict increased honesty. In the developmental psychology literature, a wealth of correlational studies dating back to the late 1920s explores the observed relationship between intelligence and socially approved or disapproved behaviors. Among the first studies, Hartshorne et al. (1928) collect observational and self-report data from over 10,000 children on three forms of deceptive behavior (cheating, lying and stealing) and attempt to correlate it with numerous traits including intelligence. They provide as their “best estimate of the relation between intelligence and a theoretical combination of all of our deception tests, a correlation of −.50 to −.60” (p. 189) (quoted from Unger 1964, p. 300). In summarizing several reviews of this literature, Unger writes that “brains and character actually tend to go together”, the existence of a positive relation between intellect and morality “is practically unanimous and unequivocal”, “the most helpful and cooperative children [a]re nearly always also among the brightest” and “sixth-grade boys rated to be low in delinquency potential [are found] to have markedly superior IQ scores”. A number of studies have found that cheating on exams is most frequent among students with low grades or low GPA scores (see, e.g., Hetherington and Feldman 1964 and the references therein). Gottfredson and Hirschi (1990) review the cross-cultural determinants of crime and conclude that “the individual-level correlates of delinquency that appear everywhere include sexual precocity, limited scholastic aptitude and drug use” (p. 178). Methodologically closer to our study (i.e., incentivized experiments on dishonest reporting), two recent studies include measures of intelligence as a moderator variable in explaining the relationship between honesty and the variable of interest. Gino and Ariely (2012) find that creativity predicts dishonest reporting across a series of five studies. In one of the studies, the authors also measure intelligence through the CRT and a vocabulary test (verbal intelligence) and show that neither intelligence measure is significantly correlated with dishonesty. Fosgaard et al. (2013) also control for CRT scores in an experiment designed to separate the effect that cheating is an option (i.e., cheating awareness) from the effect that cheating is the norm (i.e., cheating conformity). The authors find that CRT scores are positively associated with the probability of cheating. After finding that kaba scores predict honesty in our soldier experiments, our study shifts to focus on the relationship between intelligence and honesty. Our study differs from a rapidly expanding experimental literature on honesty (see Rosenbaum et al. (2014) for a survey) in two further respects. First, our subject pool: unlike student subject pools or even most field experiments targeted at a particular population, soldiers completing their mandatory military service constitute a representative cross-section of society as a whole.Footnote 2 What is more, this population is particularly well suited to examine the relevance of screening criteria for honesty because military units are highly susceptible to dishonesty. The hierarchical organizational structure inherent in a nation’s military inevitably means that a commanding officer who assigns a duty or issues an order to a soldier has no opportunity to verify whether the soldier has completed the assigned task. Yet any well-functioning military relies on honesty between its troops and even seeks this trait when recruiting and promoting soldiers. Indeed, honesty is among the highest declared values for Israeli soldiers and part of the creed of the Israel Defense Forces. Another source of novelty of our experiment is that, to the best of our knowledge, it is the first to examine honesty toward one’s employer. Soldier subjects in our experiment cheat their boss with whom they interact on a daily basis, rather than an anonymous firm (e.g., Levitt 2006; Pruckner and Sausgruber 2013), anonymous subjects (e.g., Gneezy 2005), wait staff at a restaurant (Azar et al. 2013), or the experimenter (e.g., Fischbacher and Föllmi-Heusi 2013).Footnote 3
",21
20.0,1.0,Experimental Economics,22 March 2016,https://link.springer.com/article/10.1007/s10683-016-9480-5,Advice in the marketplace: a laboratory study,March 2017,Jonathan E. Alevy,Michael K. Price,,Male,Male,Unknown,Male,"There is substantial evidence that the decisions of experienced agents in a marketplace differ systematically from those of inexperienced counterparts. In financial markets, costly errors made by retail traders are often reduced or eliminated amongst market professionals (Odean 1998; Grinblatt and Keloharju 2001; Locke and Mann 2005). In real estate markets, Genesove and Mayer (2001) find that loss aversion is attenuated when agents are handling their own property. Experience has also proven important in a variety of experimental settings (Knez et al. 1985; Smith et al. 1988; Myagkov and Plott 1997; List 2002, 2003, 2004; Alevy et al. 2007). While the evidence that experience matters in the marketplace is compelling, we hypothesize that it is not the only means through which individuals can learn to avoid potentially costly behaviors. In this study, we make use of experimental methods to investigate the impact of information and direct advice from experienced subjects on the behavior of inexperienced market participants and resulting market outcomes. Specifically, we examine this question in the context of asset pricing in an experimental market and find that advice and experience are substitutes. Our inquiry is informed by evidence from field settings in which both observation of others’ behavior and information obtained from outside sources—i.e., others in one’s social network, electronic and print media, paid advisors—affect individual trader behavior and overall market outcomes (see, e.g., Shiller and Pound 1989; Bjerring et al. 1983; Desai and Jain 1995; Antweiler and Frank 2004; Hong et al. 2005; Mizrach and Weerts 2009). Much of this information takes the form of public announcements designed to reach as many market participants as possible (see e.g., Antweiler and Frank 2004; Bjerring et al. 1983; Mizrach and Weerts 2009). Yet, other advisors target more carefully, for example, identifying and suggesting strategies within a proprietary trading firm. We focus on the latter case in which an advisor has a proprietary interest in the trading outcomes of the advisee, and heterogeneous advice is transferred to market participants.Footnote 1
 Although much can be gleaned from the extant field studies, the causal impact of advice and information on the behavior of inexperienced individuals and on overall market performance remains an open question. To identify such impacts, we embed the intergenerational advice framework of Schotter and Sopher (2003) in an asset market modeled on the seminal study of Smith et al. (1988; SSW hereafter).Footnote 2 In this framework, a sequence of non-overlapping “generations” of players participate in a stage game for a finite number of periods and are replaced by other agents who repeat the stage game in the same role for an identical length of time. Players in generation t can “communicate” with their successor in generation t + 1 by leaving them written advice. Compensation is a function of both own performance and the performance of the successor in generation t + 1 which creates an incentive to leave valuable advice. The stage game in our experiment is a 15 period session of the SSW asset market, and we create links connecting up to three stage games comprised of distinct ‘generations’ of traders. Our main results focus on sessions in which all nine traders receive information and/or advice since it is the natural comparison to previous work in which all subjects accumulate experience in repeated stage games. We also conduct a number of sessions in which only a subset of agents in period t + 1 receive advice from a predecessor yielding mixed markets of advised and unadvised traders. Several insights emerge from our experiment. First, intergenerational transmission of advice from experienced to inexperienced traders has a substantial influence on market outcomes. Deviations from fundamental values are significantly smaller in sessions with advice than those observed in our control markets. Futher, changes in pricing dynamics are similar to those that arise in our own-experience markets where a common cohort of traders thrice repeats the underlying SSW stage game—deviations from fundamentals diminish rapidly from generation to generation. Our examination of mixed markets advances the literature by showing that convergence towards pricing at market fundamentals can occur in such a setting. Previous work examined whether convergence is sustained in own-experience sessions in a 4th generation stage game that mixes experienced and naïve participants (Dufwenberg et al. 2005; DLM hereafter). Second, we find that advice is largely reflective, outlining trading strategies that are profitable in markets where prices follow the pattern of a bubble and subsequent crash. As successors follow this advice, they alter the trajectory of prices and also avoid the types of momentum trading strategies that have been shown to yield bubbles in prior studies (see, e.g., Smith et al. 1988; Lei et al. 2001). Moreover, agents in successor markets are more responsive to the opportunities represented by deviations from fundamental values suggesting an important channel through which advice serves to impact pricing dynamics. Finally, our data suggest that the returns to advice accrue primarily at the market rather than at the individual level. In aggregate advised and unadvised agents earn statistically similar amounts. In this regard, our data are at odds with the existing literature on the returns to experience in constant sum games such as asset markets (Dufwenberg et al. 2005) and p-beauty contests (Slonim 2005). Yet we observe a significant reduction in the variance of earnings across traders in markets with advice. Such effects are consonant with prior work by List and Price (2005) showing that the effects of buyer experience in collusive markets accrue at the market level in the form of lower prices and increased surplus (earnings) for both experienced and inexperienced buyers. Taken jointly, these data suggest that the advice of others appear to be a close substitute for one’s own experience. Observed messages condition traders to expect the pricing dynamics of a bubble and draw their attention to strategies that are profitable in such markets—i.e., buy shares in early periods when prices are low and sell shares in the middle periods before prices crash. As such strategies are akin to those followed by a fundamentalist—buy (sell) whenever prices are less than (greater than) expected value—prices converge towards fundamentals and the severity of bubbles diminish. In this regard, our findings share similarity with Haruvy et al. (2007) who show that traders base expectations on prior history and best-respond to these beliefs by adapting trading strategies to account for anticipated pricing dynamics.Footnote 3
",3
20.0,1.0,Experimental Economics,18 April 2016,https://link.springer.com/article/10.1007/s10683-016-9481-4,Patience and time consistency in collective decisions,March 2017,Laurent Denant-Boemont,Enrico Diecidue,Olivier l’Haridon,Male,Male,Male,Male,"Most economic models rely on the assumption that individuals and organizations are consistent in their choices over time. Two central restrictions on time preferences—namely, stationarity and dynamic consistency—guarantee that some form of consistency is achieved in decisions (Strotz 1955; Bleichrodt et al. 2009). These conditions are necessary if consistency is to be exhibited in the choices of an individual decision-maker (Halevy 2015) or of several individuals coordinating on a group intertemporal decision (Jackson and Yariv 2014). The same conditions must also be satisfied by any policy maker (Caplin and Leahy 2004). Households, boards, committees, and teams are examples of groups that need to deliberate and coordinate their actions on important decisions that have a time dimension. Such decisions include those related to retirement and saving, education and health care, investments, providing effort and public goods, and building reputations. Behavioral research on group decision making has shown that, comparatively speaking, groups are more likely to make rational choices whereas individuals are more likely to behave in a boundedly rational manner (Cooper and Kagel 2005; Charness and Sutter 2012; Maciejovsky et al. 2013). Hence the expectation is that intertemporal decisions made by a group are more consistent than those made by an individual. For instance, Charness and Sutter (2012) suggest that an individual who is prone to dynamic inconsistency in saving for retirement might achieve a better retirement outcome through participation in group decision making. Such participation could result in a better intertemporal decision with benefits that might compensate for the costs associated with delegating part of the decision. There is little empirical evidence on group decision making in intertemporal choice. Yet numerous theoretical papers are devoted to group decision making and the aggregation of time preferences. Under such aggregation, these papers predict that a collective decision process will generally yield inconsistent choices over time even if group members are individually consistent (Gollier and Zeckhauser 2005; Zuber 2010; Jackson and Yariv 2015); if so, then the consequence could be inefficient behavior (Schaner 2015). Empirical evidence on the aggregation of individual time preferences supports this view. For example, Jackson and Yariv (2014) report that a large majority of subjects acting as social planners are present-biased and that only 2 % of them exhibit consistent behavior. The within-subject experiment described in this paper yields new lab evidence on the outcome resulting from individual and collective decisions. Following Halevy (2015), we combined static and longitudinal experimental methods to address the issue of stationarity and consistency of time preferences but without committing to a particular functional representation. More specifically, we tested four conditions on time preferences at both the individual and group levels: impatience, stationarity, age independence, and dynamic consistency. For group decisions, we designed a coordination mechanism based on majority voting preceded by a deliberation phase among the participants. Our main results can be summarized as follows. In line with the existing literature on intertemporal choice, individuals were impatient and tended to deviate from consistent behavior. In contrast, groups typically made patient and highly consistent decisions; those decisions were based on majority voting after a long sequence of information exchange between group members through a series of straw polls. We observed that our coordination mechanism helped the groups to converge and to make both stable and dynamically consistent decisions. Because our results could have been driven by several confounding factors, we also implemented a series of additional treatments. These robustness checks showed that no such factors (i.e., repetition, voting, or choosing for others) can explain the high degree of patience and consistency displayed by the groups in our experiment. Following Samuelson (1937) and Fishburn and Rubinstein (1982), a large part of the theoretical literature on time preferences builds on discounted utility and additively separable functional forms that assume a separation between value and delay in assessing temporal sequences of outcomes. A typical example is the exponential discounting utility model, which assumes stationarity of time preferences and serves as the workhorse of many economic models. The discounted utility model’s representation of time preferences has the additional advantage of facilitating empirical measurements. With an extra assumption on the linearity of utility, measures of discount factors and discount rates can be carried out by way of simple experiments (Thaler 1981; Coller and Williams 1999). If one instead assumes nonlinear utility then measurements become more sophisticated yet also more complex (Andreoni and Sprenger 2012b). All these measures share the potential descriptive limitations of the discounted utility model. If this model misrepresents time preferences, then so do all measurements based on that model. An alternative route is proposed by Rohde (2010) and Halevy (2015). These authors demonstrate that several basic properties of time preferences—including conditions on stationarity, dynamic consistency, and age independence—can be inferred empirically from direct conditions on preferences and also without committing to a specific functional representation of preferences. Our experiment follows that route and focuses on the basic conditions of choice over time without assuming any particular functional form. The empirical literature on time preference has elicited an extremely wide variety of discount rates. Frederick et al. (2002) report elicited discount rates ranging from less than 1 % (Thaler 1981) to more than 1000 % (Holcomb and Nelson 1992). Furthermore, individuals often exhibit present-bias and thus violate stationarity (Benzion et al. 1989; Kirby and Maraković 1995; Bleichrodt and Johannesson 2001; DellaVigna 2009). However, more recent studies show that experiments can limit this well-established stylized fact—at least for monetary choices—by controlling for transaction costs and payment risk (Andreoni and Sprenger 2012a; Augenblick et al. 2015). Stationarity is the key axiom underlying the discounted utility model’s standard hypothesis of a constant discount rate. A decision exhibits ’stationarity’ when it does not change in response to uniformly delayed receipts. Stationarity should be distinguished from “dynamic consistency”, under which a decision regarding the future made at one time is not changed at a later time. Testing for dynamic consistency therefore requires a longitudinal experimental design (Horowitz 1992; Sayman and Öncüler 2009; Casari and Dragone 2015; Meier and Sprenger 2015), which explains the relative scarcity of experimental studies devoted to dynamic consistency. Those that do exist report mixed results. On the one hand, Horowitz (1992) and Meier and Sprenger (2015) report time consistency at the aggregate level but not at the individual level. Giné et al. (2014) observe 65 % of study participants to be dynamically inconsistent—despite being reminded of their past choices—and Kang and Ikeda (2014) report non-negligible time variations in longitudinal survey measures of time preferences.Footnote 1 On the other hand, Sayman and Öncüler (2009) find no evidence in favor of time inconsistency for short delays. Halevy (2015) report that 48 % of the subject were time consistent. Augenblick et al. (2015) report that while 60 % of their subjects were time consistent in monetary choices, that percentage declined to 25 % for time consistency in real-effort tasks. Evidence on group choice over time mainly concerns impatience. Available studies suggest that groups are more patient than their individual members. For example, individuals are more patient when making a joint decision with a partner than when making a decision for themselves. This statement holds whether the group consists of a decision-making real-life couple (Carlsson et al. 2012) or an experimental ’artificial’ couple (Shapiro 2010). Carlsson et al. (2012) also find that couple-made decisions violate stationarity. For larger groups, collective patience has been reported in groups of three to seven people (Shapiro 2010; Denant-Boemont and Loheac 2011). Coordination mechanisms are central to group decision making. The most frequently used mechanisms in experiments are majority voting and unanimity. For example, Denant-Boemont and Loheac (2011) implement an unanimity rule in collective choice over time and find that it generates more patient choices than does majority voting. Unanimity does have some undesirable features, however. First, the length of the decision process is unknown; it differs among groups and also by decision. Accommodation is crucial for arriving at a unanimous group decision, and the number of rounds needed to reach that stage is indeterminate. Moreover, as reported by Viscusi et al. (2011), the extent of accommodation is greater with majority than with unanimous decisions. Second, Gerardi and Yariv (2007) show that unanimity restricts the domain of implementable outcomes and eliminates some possible outcomes that could be achieved by other, intermediate coordination mechanisms—for example, the simple-majority voting rule. Majority voting is seldom enough to achieve efficiency. Goeree and Yariv (2011) demonstrate experimentally that collective deliberation can affect collective choice under various voting mechanisms. In particular, voting without deliberation tends to make voters more strategic, in which case voting behavior is more contingent on institutional rules. At the opposite extreme, unrestricted deliberation among group members renders communication itself more important than the issues being voted on. This result suggests that a majority-vote collective decision process incorporating initial communication over voting intentions may help participants coordinate more effectively when making a collective choice. Our experiment implements just such a collective decision process for choice over time. The paper proceeds as follows. Section 2 presents the setting of the experiment and offers some theoretical background on time preferences. Section 3 summarizes the experimental results, and Sect. 4 concludes.",13
20.0,1.0,Experimental Economics,14 May 2016,https://link.springer.com/article/10.1007/s10683-016-9482-3,Risk preferences under acute stress,March 2017,Jana Cahlíková,Lubomír Cingl,,Female,Male,Unknown,Mix,,
20.0,1.0,Experimental Economics,15 April 2016,https://link.springer.com/article/10.1007/s10683-016-9483-2,The importance of higher-order beliefs to successful coordination,March 2017,Steven J. Bosworth,,,Male,Unknown,Unknown,Male,"A vast swathe of economic activity is achieved by coordination among agents, sustained in equilibrium by mutually reinforcing beliefs. Product manufacturers expect inputs to be produced by upstream firms, who in turn must count on the business of these downstream manufacturers to profitably produce those inputs. Potential investors in a start-up venture will only invest if they expect that the entrepreneur will successfully raise capital from other investors like them. Entire economies can fall into traps where expectations of low incomes lead firms and individuals to forego production and investment that could be profitable under a different set of expectations. This paper focuses on a paradigm that captures the strategic incentives present in the above situations in their most essential form: a stag hunt game. In a stag hunt game, players have the opportunity to invest in a speculative venture that will be profitable only if a sufficient proportion of other players do likewise. If a player has a sufficiently strong belief that others will invest, then she will also want to invest, making mutual investment a Nash equilibrium. If she places low probability on others investing, then she will also decline to invest. Thus mutual disinvestment is also a Nash equilibrium. How do people form the expectations that lead them into better or worse equilibria? The introspective player will reason that since potential partners face the same incentives that she does, they will invest when they expect her to invest. In this way, her beliefs about others’ actions (what game theorists term first-order beliefs) are crucially informed by her beliefs about others’ beliefs (second-order beliefs, naturally). Do people make these considerations when deciding whether to invest or not in stag hunt games? Experimental implementations of global games, specifically stag hunt games with uncertainty about payoffs, should provide us some evidence (Heinemann et al. 2004; Cabrales et al. 2007; Cornand and Heinemann 2014). Uncertainty about payoffs to coordination can, under Bayesian reasoning, select a unique equilibrium (Carlsson and Van Damme 1993). This works because when a subject has low beliefs about the fundamental payoffs, she expects others do as well, that those other players are unlikely to invest, and that they are making the same consideration about her. Global games experiments generally do not observe strong differences across information treatments that should operate by differentially shifting higher-order beliefs however. I design a modified stag hunt game that features exogenous uncertainty about actions rather than payoffs. Players’ actions are subject to exogenous perturbations with objectively known probabilities. I experimentally manipulate players’ expectations that their partner will invest in the modified stag hunt by revealing information about the exogenous perturbations (first-order beliefs). This allows me to also manipulate players’ second-order beliefs by revealing what their partners know about their probability of investment. Since the perturbations are independently assigned, I can identify how people respond to induced changes in their second-order beliefs. Careful elicitation of subjects’ first- and second-order beliefs confirms that the experimental treatments operate through the hypothesized belief mechanism. The paper proceeds as follows: existing literature on stag hunt games is surveyed in Sect. 2, Sect. 3 describes the design of the experiment in detail, Sect. 4 summarizes the collected data, Sect. 5 presents the results of the experiment and Sect. 6 concludes.",4
20.0,1.0,Experimental Economics,17 May 2016,https://link.springer.com/article/10.1007/s10683-016-9484-1,Does decision error decrease with risk aversion?,March 2017,David M. Bruner,,,Male,Unknown,Unknown,Male,"While expected utility theory has been the dominant approach taken by economists to modeling decision-making under uncertainty for the last half century, it is not beyond criticism. Among the various discrepancies identified in the experimental literature, several studies (Ballinger and Wilcox 1997; Hey 1995; Hey and Orme 1994; Harless and Camerer 1994; Loomes et al. 2002; Wilcox 1993) have reported results that indicate choice under risk involves a stochastic component, which is unaccounted for by any deterministic choice theory.Footnote 1 For instance, when faced with repeated trials of choices between risky assets, subjects frequently make contradicting decisions for the same choice pair. This has motivated several different approaches to modeling the stochastic error process, originating from psychophysics (Fechner 1860) and psychometrics (Thurston 1927). To date, the stochastic process has been modeled as a ‘trembling hand’ (Harless and Camerer 1994), traditional white noise (Fechner 1860; Luce 1959), and random preferences (Becker et al. 1963). The most popular models, however, fail to translate what it means to be ‘more risk averse’, as defined by Pratt (1964), to stochastic choice under risk; a more risk averse individual is not necessarily more likely to choose the safer of two assets with equivalent expected returns. On the other hand, less popular approaches not only account for random decision error, but predict that it is decreasing in risk aversion. While this is a theoretically appealing property of stochastic models of risky decision-making, it is still an open empirical question whether this is an accurate depiction of individual behavior. By far, the most popular approaches to account for stochastic decision error in the literature are the Fechner (1860) and Luce (1959) models.Footnote 2 These models assume that given a choice over two risky assets, an individual chooses their most preferred asset with some probability determined by the difference in expected utilities. The issue that arises is that for commonly adopted specifications of the utility function, such as constant absolute risk aversion (CARA) and constant relative risk aversion (CRRA), the difference in expected utilities is not always monotonically increasing in risk aversion. This can be problematic when these models are used to estimate risk preferences, since risk preferences may not be uniquely identified in such models; such stochastic models of risk preference are not ‘monontone’ (Apesteguia and Ballester 2016). However, this problem can be avoided by incorporating the less popular mean-variance specification of utility into a standard white noise model or by employing a random preference specification of the stochastic error process in conjunction with CARA or CRRA preference specifications.Footnote 3 In either case, an increase in risk aversion implies a decrease in decision error. This paper reports the results of an experiment designed to test the hypothesis that an increase in risk aversion should result in a reduction in decision error. To do so, the experimental design exploits the theoretical equivalence of risk aversion and second-order stochastic dominance (Hadar and Russell 1969; Hanoch and Levy 1969; Rothschild and Stiglitz 1970). Herein after stochastic dominance refers to second-order stochastic dominance, as opposed to first-order stochastic dominance. That is, given a choice between two risky assets such that one asset is a mean-preserving spread of the other, an individual with risk averse preferences should prefer the safer asset. Accordingly, the experiment presents subjects with a series of pairwise choices between risky assets where one asset stochastically dominates the other. The key feature of this task is that all risk averse subjects should make the same choice for each lottery pair. Hence, the experiment elicits risk preferences to identify risk averse subjects and their degree of risk aversion. This permits an investigation as to whether the error rate is decreasing in risk aversion. The most common mechanism used by experimental economists to elicit risk preferences is a multiple price list (MPL) (Andersen et al. 2006).Footnote 4 The MPL requires subjects to make a series of pairwise choices between a risky and a safe option. As subjects proceed through the series, the expected value of the risky option is increased to induce them to switch from the safe to the risky option. The point at which a subject switches provides an interval estimate of the subject’s underlying risk preference. The experiment implemented two formats of the MPL to elicit risk preferences, a probability variation (PV) format and a reward variation (RV) format. Both formats involved a series of 10 decisions between $5 or a lottery. The PV format increased the expected return of the lottery by increasing the probability of a return from 0.10 to 1.0 in increments of 0.10, holding the possible returns constant at $0 or $10. The RV format increased the expected return of the lottery by increasing the high return from $2.00 to $20.00 in $2.00 increments, holding the probability of a return constant at 0.50.Footnote 5 All subjects were given both the PV and RV formats of the MPL. The analysis identifies both the direction and strength of subjects’ risk preferences using their choices in the PV and RV formats of the MPL. All subjects were also presented with a third MPL that was designed to determine the frequency risk averse subjects choose the stochastically dominant of two risky assets.Footnote 6 In the lottery variation (LV) format, subjects faced a series of 10 pairwise choices between the risky options from the PV and RV formats. Each pair of lotteries had equivalent expected returns but different levels of risk. As subjects proceeded through this series, initially the RV lottery dominated the PV lottery and then vice versa. Thus, risk averse subjects should initially choose the RV lottery for the first four decisions and then switch to choosing the PV lottery for all subsequent decisions. Subjects’ decisions from all three formats of the MPL were then combined to test whether the error rate in the LV format decreased with risk aversion, as elicited by the PV and RV formats. Overall, most subjects were consistently risk averse both across and within the PV and RV formats.Footnote 7 Moreover, the distribution of choices in the LV format for risk averse subjects was heavily skewed towards a preference for dominant lotteries with the modal pattern of behavior being to choose all 10 dominant lotteries. Hence, risk averse subjects exhibited a strong preference for dominant lotteries, as predicted. Still, there were a substantial amount of discrepancies; only about a third of risk averse subjects chose all 10 dominant lotteries. However, such violations of stochastic dominance are to be expected given a stochastic error component to decision-making. More importantly, there is significant negative correlation between the error rate and the level of risk aversion. Hence, observed behavior supports the adoption of ‘monontone’ stochastic error models of risk preference. The remainder of the paper is organized as follows. Section 2 presents the theoretical framework to establish testable hypotheses. Section 3 describes the experimental design. Section 4 presents the results from the experimental sessions. Section 5 summarizes the results and discusses their implications.",8
20.0,1.0,Experimental Economics,16 September 2016,https://link.springer.com/article/10.1007/s10683-016-9495-y,Erratum to: Incentives for creativity,March 2017,Sanjiv Erat,Uri Gneezy,,Male,Male,Unknown,Male,"In the part of the instructions to participants that explains the rebus‐puzzle design task in our paper ‘‘Incentives for Creativity’’ (Erat and Gneezy 2016), there are several shared phrases from Kachelmeier et al. (2008). We failed to cite them, and wish to correct this here. In their paper, Kachelmeier et al. (2008) examine how worker productivity differs when compensation is based on quantity, creativity, or the product of both measures. They find that combining quantity and creativity measures in a creativity‐weighted pay scheme results in creativity weighted productivity scores that are significantly lower than those generated by participants with quantity incentives alone. See also Kachelmeier and Williamson (2010) for a follow-up study on self-selection, which shows that participants choosing a creativity‐based incentive scheme exhibit more creativity initially, but are not any more creative overall.",3
20.0,1.0,Experimental Economics,17 November 2016,https://link.springer.com/article/10.1007/s10683-016-9497-9,Editors’ note regarding citations of other work,March 2017,David J. Cooper,Lata Gangadharan,Charles Noussair,Male,Female,Male,Mix,,
20.0,2.0,Experimental Economics,19 October 2016,https://link.springer.com/article/10.1007/s10683-016-9494-z,Infinitely repeated games in the laboratory: four perspectives on discounting and random termination,June 2017,Guillaume R. Fréchette,Sevgi Yuksel,,Male,Female,Unknown,Mix,,
20.0,2.0,Experimental Economics,12 July 2016,https://link.springer.com/article/10.1007/s10683-016-9485-0,"Social approval, competition and cooperation",June 2017,Xiaofei Pan,Daniel Houser,,Unknown,Male,Unknown,Male,"In social dilemma environments, extrinsic peer monetary rewards (or sanctions) are frequently used to promote cooperation. Individuals can reward or punish their peers by incurring costs to increase or decrease the earnings of their fellow group members. Typical results suggest that such monetary peer sanctions effectively mitigate free-riding behavior (Fehr and Gächter 2000). At the same time, they may also adversely impact economic efficiency and lead to spirals of revenge (Denant-Boemont et al. 2007; Dreber et al. 2008). While peer reward can help to avoid such concerns, it is typically less effective than sanctions at promoting cooperation (Andreoni et al. 2003; Sefton et al. 2007). As a result, recent attention has focused on non-monetary incentives. For example, Dugar (2013) showed that while non-monetary peer approval promotes contribution, it is not sustainable. Some studies have also combined monetary and non-monetary rewards. In this vein, Eriksson and Villeval (2012) studied the manner in which costly symbolic rewards influence effort.Footnote 1
 Our study advances this literature by investigating the impact of symbolic non-monetary rewards in a social dilemma environment where rewards are earned by winning a peer approval competition.Footnote 2 In particular, rewards are provided based on whether one has earned the most approval from his peers. This differs from the literature on competitive rewards, as those rewards are often earned based on directly observable outcomes, like individual performance, rather than peer approval. For example, Charness et al. (2010) awarded statusFootnote 3 (through rank) to subjects based on their performance (in a real effort task); Kosfeld and Neckemann (2011) awarded congratulatory cards based on performance; and Fuster and Meier (2010) awarded cash based on contribution in public goods games. While performance (either effort or contribution) is easily observable in some environments, it can be costly to observe in others. Take, for instance, the example of a cashier. The cashier’s nearby colleagues are in a much better position to easily observe the cashier’s productivity than is his/her manager. Assembly workers are another example. Their efforts may not be evident in the final product of their labors, but their close colleagues are likely in a good position to observe their work. As these examples illustrate, the peer approval mechanism may be particularly useful in environments where cooperation is needed, but each individual’s contribution is difficult to monitor. In light of the foregoing, there exists a need for peer approval mechanisms. Despite the proven effectiveness of such mechanism, the empirical literature has not yet investigated how non-monetary rewards based on peer approval might influence contributions in public goods games. In contrast, economic theory on this topic emerged long ago. In particular, Holländer (1990) provided an early and influential model of voluntary contributions in response to peer-to-peer approval. The model assumes that one’s preference is based on utility from private goods, public goods and social (peer) approval. The theory formalizes the idea that the extent to which one values peer approval can impact one’s cooperative actions in social dilemma environments. Consequently, for the purpose of institution design, it is crucial to know which environmental features might encourage people to highly value peer approval. To study this question, we designed a ten-period public goods game in which each period consisted of two-stages. The first stage was a regular public goods game in which players simultaneously made contribution decisions to the public good. In the second stage, each player had an opportunity, after observing hisFootnote 4 group members’ contributions, to assign costless non-monetary approval points to each of his fellow group members, but not to himself. As suggested by the name, the non-monetary approval points provided no monetary benefits to those who received them. Our experiment included five treatments with competition (Competition-only, Star, Cash, Mug, Ice-Cream) and two treatments without competition. Of these latter two, we denote one the No-competition treatment because it lacks competition for approval. The other is a Control treatment that is identical to a standard public goods game. Details of these treatments are provided below. In brief, we created a competitive environment by ensuring that, in all competition treatments, subjects learned the total approval they received from other group members in each period. However, we varied the consequences of this information. For instance, in the Competition-only treatment, we simply informed the participants as to who won the most approval points in their group in a given period. We did the same thing in the Star treatment; however, we also awarded an electronic gold star to the top earner of approval points in each group for each period.Footnote 5 (It was common knowledge that, in case of ties for highest approval, all who tied were awarded a gold star.) The Ice-cream,
Mug and Cash treatments were identical to Star, except that each gold star earned increased the probability of receiving a final reward by ten percentage points. In the Mug treatment, we created a unique and durable reward of small monetary value: a mug emblazoned with the organization’s logo. The mug could only be obtained by participating in our experiment. In the Ice-cream treatment, we used a non-unique and non-durable reward with the same monetary value: a Haagen-Dazs ice cream bar (see Fig. 5 in Appendix 1). In the Cash treatment, we used a non-unique but durable cash reward about 50 % higher in value than the average of the mug and ice cream rewards. Our No-competition treatment did not include competition for peer approval or reward. In particular, participants assigned approval to others and received approval from others, but were informed only of the number of approval points they themselves received. Finally, our Control treatment is the same as a traditional public goods game, absent approval or competition. We ran these treatments for the following reasons. First, studies have shown that “signaling motives” may directly influence pro-social behaviors (see e.g., Harbaugh 1998; Bénabou and Tirole 2006; Ariely et al. 2009).Footnote 6 Previous studies have shown that unique and durable rewards are good signaling mechanisms (Pan and Houser 2011 and cites therein; Kosfeld and Neckermann 2011). We chose rewards that vary in three key dimensions: uniqueness, durability, and monetary value. The mug reward was unique, durable and of small monetary value; the ice cream reward was non-unique, non-durable, and of a similarly small monetary value; and the cash reward, while durable and of a higher monetary value, lacked uniqueness. The Star reward, which was unique and present in all three treatments with final rewards, thus served to evaluate how effective the other rewards (cash, mug or ice cream) were at promoting cooperation. The Competition-only treatment served as a baseline for these other treatments with competitive rewards. As we detail below, comparing our treatments allowed us to learn about the types of environments in which approval from peers is most likely to be valued and thus increase contributions in equilibrium. Our key finding is that competition for social approval is most effective at promoting cooperation when winners receive non-cash rewards that are both unique and durable (mug). We find that such rewards are as effective as a cash reward with 36 % higher monetary value, and are more effective in triggering high contributions than the cash reward. On the other hand, a non-cash reward with the same monetary value that is neither unique nor durable (ice cream) cannot promote cooperation. We also find that providing social comparison information alone (Competition-only) detrimentally impacts cooperation in comparison to the No-competition treatment. This detrimental impact is not reversed when competition for social approval leads to symbolic rewards (star) or a generic and non-durable reward with monetary value (ice cream). Finally, we find that cooperation in Competition-only is insignificantly different from what occurs in the standard public goods game. Our experiment also informs the way peers dispense approval. Holländer (1990) denotes the ratio of contributions to approval received as the “approval rate,” and shows in theory that when one values peer approval more, each unit of approval triggers greater contributions. Our data are consistent with this prediction as they reveal a clear inverse relationship between these variables in our treatments that include competition.Footnote 7
 Our study provides important policy insights on how to promote pro-social behaviors. In particular, it might hold significant value for organizations with profit-sharing schemes, where employees have strong incentives to evaluate each other’s efforts (e.g., Avis Corporation; see also, Kandel and Lazear 1992; Mas and Moretti 2009). Our results also suggest that competition under peer evaluation should be adopted with caution. In particular, the nature of rewards may interact with the peer approval to jointly influence the level of contributions. The remainder of this paper is structured as follows: Sect. 2 presents the literature review. Section 3 describes our theory. Section 4 details the experiment design and our hypotheses. Section 5 reports our results. Finally, Sect. 6 concludes and discusses implications.",7
20.0,2.0,Experimental Economics,30 June 2016,https://link.springer.com/article/10.1007/s10683-016-9486-z,The impact of award uncertainty on settlement negotiations,June 2017,Eric Cardella,Carl Kitchens,,Male,Male,Unknown,Male,"Settlement negotiations between disputing parties are often carried out under the backdrop of an adjudicated award if the parties fail to settle. Examples of such disputes include: punitive damages, patent infringements, breaches of contract, antitrust, labor arbitration, and eminent domain. Litigation dispute models of this type abound, and while these models differ in their informational structures and underlying assumptions, a common feature is costly litigation when settlement negotiations fail; consequently, it is often beneficial for both parties to negotiate a settlement and avoid litigation.Footnote 1 While settlements are common in practice, they are not ubiquitous.Footnote 2 Given the (possible) inefficiency associated with excessive and costly litigation, it is important to understand the potential sources of settlement failure (as discussed by Babcock and Loewenstein 1997). In legal disputes, there is likely to be substantial variability and unpredictability in the adjudicated award, especially those handed down by juries. As an epitomizing example, in 1994 Stella Liebeck sued McDonald’s after accidentally spilling hot coffee on herself. After failing to reach a settlement, a New Mexico, USA jury awarded Ms Liebeck over $2.86 million to cover medical expenses and punitive damages.Footnote 3 Empirical evidence of substantial variation and positive skewness across court awards has been documented in several studies (e.g., Kahneman et al. 1998; Black et al. 2005; Kaplan et al. 2008; Mazzeo et al. 2013).Footnote 4 Sunstein et al. (2002) highlight the likely presence of variability in adjudicated awards in their concluding remarks where they state: “the result (of the award process) is a decision that is unreliable, erratic, and unpredictable” (p. 241). We posit that the degree of uncertainty in adjudicated awards, either real or perceived, may impact settlement negotiation behavior and, consequently, the likelihood that a settlement is reached. In this paper, we develop a laboratory experiment that enables us to empirically investigate how increases in variance and skewness of the adjudicated award distribution impact settlement negotiation behavior, settlement rates, and the degree of inefficient litigation. Changes in the distribution of awards (assuming the mean is unchanged) would not be expected to impact negotiation behavior and settlement rates under the assumption that the involved agents are risk-neutral (e.g., P’ng 1983; Bebchuk 1984; Nalebuff 1987; Schweizer 1989). However, over the past several decades, a plethora of research has documented decision-making inconsistent with risk-neutrality.Footnote 5 Specifically, the role of risk aversion has been explored in various bargaining environments.Footnote 6 More recently, several studies have experimentally documented evidence that agents exhibit prudent behavior (Deck and Schlesinger 2010, 2014; Ebert and Wiesen 2011, 2014; Maier and Rüger 2012; Noussair et al. 2014). As originally termed by Kimball (1990), prudence refers to a convex marginal utility function or an aversion to increases in downside risk (Menezes et al. 1980); prudent behavior is relevant in our context because prudence implies skewness seeking (Ebert and Wiesen 2011). That is, prudent agents have a preference for more positively skewed distributions. If disputing parties exhibit non risk-neutral behavior, then changes in the variance or skewness of the court award are likely to affect the disputing parties’ settlement offers, which can then impact the likelihood of settlement (Posner 1973). Ideally, one would want to explore the impact of changes in variance and skewness of adjudicated awards on settlement negotiations using empirical case data. However, this poses some obvious challenges, the most significant of which is the inability to observe the degree of uncertainty in the underlying court award distribution. Second, we may not observe rejected settlements, which would make it difficult to infer welfare implications due to selection. Third, it is often difficult to observe offers in the settlement negotiation process, as well as the associated reservation values of disputing parties. Given these challenges, a controlled experiment is both a suitable and necessary approach to rigorously examine how award uncertainty impacts settlement negotiations. We develop an experimental design that allows us to systematically manipulate the degree of uncertainty in the underlying award distribution while holding other factors constant. Moreover, we also observe the negotiation stage and settlement rates, which enables us to analyze the welfare effects of changes in award uncertainty. Lastly, we are able to elicit individual risk preferences and correlate these measures with the propensity to litigate. As such, our study joins a growing body of literature using a controlled experimental environment to better understand legal disputes.Footnote 7
 In our experiment, we consider a stylized, bilateral settlement negotiation game where the two involved parties are given an opportunity to negotiate a settlement under the backdrop of an adjudicated award. If negotiations fail and a settlement is not reached, then one of the negotiating parties receives the adjudicated court award, which consists of a random draw from a known but uncertain award distribution. We then systematically increase the variance (or skewness) of the award distribution across experimental treatments, while holding the mean and skewness (and variance) constant. By comparing across treatments, we can identify how increases in variance and skewness impact the negotiation behavior of each party (i.e., offers and propensities to accept offers) and, ultimately, settlement rates. Additionally, we elicit individual measures of risk aversion and prudence (a proxy for skewness seeking) using the binary choice lottery method developed by Eeckhoudt and Schlesinger (2006) (ES henceforth). This element of the design allows us to associate behavior in the negotiation task with relative measures of risk aversion and prudence, and provide a more robust analysis of possible differential treatment effects based on individual risk preferences. Overall, we find that increases in the variance of the court award result in decreased settlement rates. The effect of increases in skewness of the court award is less conclusive, increased skewness (from zero skewness to positive skewness) is generally associated with increased settlement rates, although this effect is not strictly monotonically increasing. Perhaps most importantly, we find that even after controlling for interactions when litigation would be efficient, relatively high levels of variance in the adjudicated award leads to excessive, inefficient litigation, while positive skewness leads to lower levels of inefficient litigation. Moreover, our main results are generally robust across both a contextually framed settlement negotiation and an abstractly framed setting, which, in our view, increases the robustness of our key findings. To reduce the burden of excess litigation, several states have enacted tort reforms that cap punitive and/or non-economic damages, or have changed liability laws that may alter the incentives of plaintiffs, defendants, and insurers.Footnote 8 Closely related to our work is the prior research that has investigated the effect of damage caps on litigation. Such studies include Browne and Puelz (1999) who show that damage caps tend to reduce both the value of claims and the frequency of frivolous suits. Similarly, Avraham (2007) uses medical malpractice suits and finds that award caps on pain and suffering lead to reduced settlement payments and fewer litigated cases. However, Donohue and Ho (2007) and Durrance (2010) find no evidence that damage caps result in fewer medical malpractice claims. Experimentally, Babcock and Pogarsky (1999) find that a “binding” damage cap tends to increase settlement rates; yet, in a follow-up study, Pogarsky and Babcock (2001) find that a very large “non-binding” cap actually tends to decrease settlement rates. While these prior studies suggest that the degree of award uncertainty can impact settlement negotiations, it is not possible to identify the effects resulting from changes in uncertainty from changes in the expected value of the award, since the imposition of award caps simultaneously decreases the mean, variance, and skewness of the award distribution. However, in our design, we hold constant the mean and variance (skewness), which enables us to separately identify the effect of increased skewness (variance) on settlement negotiations; we view this as an important complement to this extant body of research related to damage caps. Previous research has suggested that self-serving bias may be one channel that contributes to settlement failure (e.g., Loewenstein et al. 1993; Babcock et al. 1995; Babcock and Loewenstein 1997; Babcock and Pogarsky 1999). The general idea behind the theory is that under the presence of an uncertain and unknown award, plaintiffs hold more optimistic beliefs than the defendant regarding the likely outcome; consequently, this discrepancy in beliefs can lead to settlement failure. In our design, we eliminate this self-serving bias channel as a source of settlement failure by making the award distribution known to both parties. Yet, even with a commonly known award distribution, we document a substantial degree of settlement failure, and find that changes in the variance and skewness of the award distribution significantly impact inefficient settlement failure. Thus, we identify another possible channel through which award uncertainty can influence settlement negotiations. More broadly, we believe this paper contributes to several areas of existing literature. Regarding the literature on legal disputes, much of the prior work has focused on the role of information asymmetries, credibility, and court cost allocations in contributing to settlement failures. This paper suggests, as an alternative contributing explanation, that the degree of uncertainty in the adjudicated award can impact settlement rates and the use of inefficient litigation. Furthermore, our study contributes to the small existing literature on ultimatum bargaining with an outside option (see Croson et al. 2003; Anbarci and Feltovich 2013 for reviews). These papers have examined cases where the size of the pie is random and/or the outside option is fixed, while we study ultimatum bargaining with an uncertain outside option. Lastly, we join a recent series of papers that explore how prudence can affect economic behavior (see Noussair et al. 2014; Ebert and Wiesen 2014 for reviews); specifically, our study provides additional experimental evidence that subjects exhibit prudent behavior, which can influence negotiation behavior.",4
20.0,2.0,Experimental Economics,28 June 2016,https://link.springer.com/article/10.1007/s10683-016-9487-y,Pre-play communication with forgone costly messages: experimental evidence on forward induction,June 2017,Andreas Blume,Peter H. Kriss,Roberto A. Weber,Male,Male,Male,Male,"We study pre-play communication in Stag-Hunt coordination games in which subjects decide whether or not to send messages and, in some cases, incur a cost for sending a message. In this environment one may be concerned that agents refrain from communicating and attempt to free ride on the communication of others. In that case, outcomes might prevail that would result if communication were impossible. This would cast doubt on the relevance and external validity of the numerous demonstrations that cheap talk can significantly improve the outcomes of strategic interactions (Cooper et al. 1992a, b; Charness 2000; Blume and Ortmann 2007; Camerer 2003; Devetag and Ortmann 2007). A priori, it is also possible, however, that for sufficiently small costs communication would proceed unimpeded with similar effects as those from cheap talk. Our experimental findings support a third possibility: moderate communication costs do dramatically reduce message use, but without noticeable effects on efficiency relative to when communication is widespread. This suggests that the option to communicate can be as valuable as communication itself. Our paper makes three contributions. First, it introduces a novel experimental design; prior to playing a Stag-Hunt game all players are given access to an optional costly communication opportunity. Second, it demonstrates that the modal behavior with optional costly communication is efficient play without message use. Third, the paper uses these experimental findings to differentiate among formalizations of forward induction, the notion that players rationalize their opponents’ past behavior; the central tendency in our data can be accounted for by formalizations that take Nash equilibrium as a reference point (such as Kohlberg and Mertens 1986; Govindan and Wilson 2009), while formalizations that only appeal to (higher-order) knowledge of rationality remain silent in our game. Next, we discuss these three contributions in greater detail and relate them to the literature. As a benchmark, we replicate the relevant no-communication results from the experimental literature on risk-efficiency tradeoffs in games. This literature has primarily focused on 2-player, 2-action Stag-Hunt games and their multi-player multi-action relatives, Weak-Link games.Footnote 1 In Weak-Link games there is a strong tendency for efficient play to unravel toward players choosing secure and inefficient strategies (Van Huyck et al. 1990). For the Stag-Hunt game, a stylized fact that emerges from the literature is that the risk-dominant outcome (Harsanyi and Selten 1988) tends to be selected (Cooper et al. 1990, 1992a); our replication provides further support. A number of theoretical papers have made the case for cheap-talk messages playing a role in equilibrium selection, including Farrell (1988), Kim and Sobel (1995), Farrell and Rabin (1996), Blume (1998) and Demichelis and Weibull (2008).Footnote 2 Cooper et al. (1992a) and Charness (2000) experimentally explore the role of mandatory cheap talk in the Stag-Hunt game. Consistent with their findings, one of our experimental conditions shows that universal cheap talk continues to promote efficient play in Stag-Hunt games even if message exchange is voluntary—when voluntary messages are costless, they are regularly used. The design in the present paper differs from the experimental literature studying cheap talk in coordination games (Cooper et al. 1992a; Charness 2000; Blume and Ortmann 2007) in two dimensions: sending a message is optional and requires bearing a small cost.Footnote 3 These features make our design similar to experimental tests of the effects of outside options and money burning on equilibrium selection. For instance, Cooper et al. (1992b) study a Stag-Hunt game in which one player has access to an outside option. They find that conditional on a relevant outside option not being exercised, the predominant outcome is the Pareto-efficient equilibrium in the Stag Hunt, consistent with forward induction. However, unlike in Cooper et al. (1992b), in our study the game is always played in the post-communication stage and we replace the one-sided outside option with a two-sided option to burn money prior to the game. Our design is most closely related to Huck and Mueller’s (2005) experimental study of the role of a money-burning option in the Battle-of-the-Sexes game. The principal differences are that their base game is the Battle of the Sexes whereas ours is the Stag Hunt and that in our case all players have the option to burn money before playing the base game. Moreover, as we describe in more detail below, the forward induction logic in their paper can be captured through iterative deletion of dominated strategies alone, whereas in our setting it requires a reference to Nash equilibrium.Footnote 4
 A key attraction of our experimental design is that it lets us differentiate among alternative formalizations of the forward induction intuition. Kohlberg (1981) first proposed forward induction as a property that Nash equilibria have to satisfy in order to be self-enforcing. He illustrated the principle using a game in which one player has a choice between an outside option and entering a Battle-of-the-Sexes game. If the value of the outside option is (strictly) between that player’s pure-strategy equilibrium payoffs, then by forgoing the outside option he effectively communicates that he will take the action consistent with his preferred equilibrium payoff in the Battle of the Sexes. If the other player understands this message, this will undermine the equilibrium in which the outside option is chosen. The general principle at work here is that for an equilibrium to be self-enforcing it has to be “consistent with … deductions based on the opponents’ rational behavior in the past” (Kohlberg 1991). In the Battle of the Sexes with an outside option, the forward induction outcome is the only outcome that remains after iterative deletion of dominated strategies. While at least some aspects of forward induction can be captured by (iterative) deletion of dominated strategies, Kohlberg and Mertens (1986) noted that their concept of strategic stability has stronger forward induction implications: a stable solution is not only robust to the elimination of dominated strategies but also to the elimination of strategies that are not best replies in any equilibrium supporting the solution.Footnote 5 This latter property, of robustness against deletion of never weak best replies, plays a key role in our design and in understanding our results. The Stag-Hunt game with optional costly communication belongs to the class of “money burning” games that were studied in an influential paper by Ben-Porath and Dekel (1992). They showed that as long as only one of the players has the option to send a costly message, that player achieves his preferred equilibrium in the subgame without sending a message as the unique outcome that survives iterated admissibility (IA), the deletion of all weakly dominated strategies at every step. This is no longer true if, as in our experiment, both players can burn money. Thus, unlike in one-sided money-burning games, in two-sided money burning games the aspect of forward induction that is captured by iterated admissibility has no predictive power.Footnote 6
 Forward induction, however, regains some of its predictive power in two-sided money-burning games once we require that solutions are not only robust to deletion of dominated strategies but also to deletion of never weak best replies. This is in line with Kohlberg and Mertens’ conceptualization of FI. In the next section, we adopt this approach, using Govindan and Wilson’s (2009) recent definition of forward induction (GW-FI). GW-FI applied to the pure-strategy equilibrium outcomes of the Stag-Hunt game with a two-sided money burning option predicts that the money burning option is not exercised and that the efficient equilibrium is played in the subgame. The key to understanding the difference in conclusions from applying IA and GW-FI to our game is that GW-FI takes an equilibrium outcome as a starting point. This reference to an equilibrium outcome gives GW-FI extra leverage. Our experimental results suggest that this extra leverage adds explanatory power. The prior experimental evidence on forward induction is mixed. Huck and Mueller (2005) find support for the Ben-Porath and Dekel prediction when games are represented in extensive form, but not for the corresponding representation in strategic form. Brandts and Holt (1995) find support for forward induction predictions only in simple games, where forward induction is equivalent to elimination of dominated strategies. Cooper, et al. (1994) find partial support for the forward-induction hypothesis in the Battle-of-the-Sexes game with an outside option. Players benefit from having an outside option, but the outside option is frequently chosen when forward induction says that it should not be. While Van Huyck et al. (1993) find support for forward induction in experiments on auctioning the right to play median-effort games, Cachon and Camerer’s (1996) results suggest that some of this may be due instead to loss avoidance. Blume and Gneezy (2010) find evidence for “cognitive forward induction”—players who find it difficult to coordinate on a non-obvious focal point achieve better outcomes when they are conscripted into playing the game by someone with an attractive outside option. On the other hand, like Cooper et al. (1994), they find excessive use of the outside option. To the best of our knowledge, we are the first to look at forward induction in two-sided money burning games. We also explore several potential alternative explanations of our findings. Neither level-k reasoning (Ellingsen and Östling 2010) nor Quantal Response Equilibrium (McKelvey and Palfrey 1995; Anderson et al. 2001) can account for our results.Footnote 7 We can also rule out that our subject population simply exhibits a tendency to select the efficient equilibrium, as there is no tendency toward efficient play when either messages are unavailable or are available but unreasonably costly. Importantly, the behavior we observe is not the result of learning. The modal behavior of efficient play without message use is established right from the beginning and is robust throughout. We can also rule out that efficient play in the Stag-Hunt subgame results from the mere availability of messages (which might create a focal point analogous to an irrelevant outside option (Cooper et al. 1991)). When messages are too costly to be relevant they are not used and in the Stag-Hunt subgame the inefficient equilibrium prevails. This suggests that experimental subjects are sensitive to potential uses of relevant—but not of irrelevant—messages, consistent with forward induction logic. The remainder of the paper is organized as follows. Section 2 presents our theoretical analysis and hypotheses. In Sect. 3 we describe our experimental design and in Sect. 4 we report our experimental data. Section 5 discusses our findings.",10
20.0,2.0,Experimental Economics,08 July 2016,https://link.springer.com/article/10.1007/s10683-016-9488-x,Promises and lies: can observers detect deception in written messages,June 2017,Jingnan Chen,Daniel Houser,,Unknown,Male,Unknown,Male,"Economic and social relationships often involve deception (e.g., Gneezy 2005; Mazar and Ariely 2006). Such relationships are generally governed by informal contracts that require trust (Berg et al. 1995). While trust is essential to an economy, the knowledge of who and when to trust, i.e. deception or trustworthiness detection, is equally critical (see, e.g., Belot et al. 2012). In particular, trust is critically important in cases where an exchange can lead to gains, but there are also incentives for one side to defect and appropriate the surplus. In these situations, people may send informal “promises” of future behavior. These messages must be interpreted to gauge the extent to which they can be trusted. Substantial research has focused on deception in economics (see, for example, Hao and Houser 2013; Erat and Gneezy 2011; Rosaz and Villeval 2011; Kartik 2009; Sutter 2009; Dreber and Johannesson 2008; Charness and Dufwenberg 2006; Ellingsen and Johannesson 2004). Recent research has devoted increasing attention to the question of whether it is possible to detect deception or trustworthinessFootnote 1;(see, e.g., Belot et al. 2012; Darai and Grätz 2010; Konrad et al. 2014). While there have been important advances, previous studies have focused largely on face-to-face communication. To our knowledge, no studies in economics have focused on detecting deception in informal written communication.Footnote 2 This is unfortunate, as informal written communication (e.g., via email, texting, tweeting, or facebooking) plays an increasingly important role in social and economic exchange outcomes. One example is Internet dating,Footnote 3 where interactions often begin with initial informal written message exchanges. The purpose of these exchanges is to build a foundation of mutual trust upon which a real (as compared to virtual) relationship can developFootnote 4 (Lawson and Leck 2006). Evidently, during this process of written exchanges, each party must make decisions regarding the trustworthiness of the other. Consequently, it is an increasingly important skill for users to be able to write trustworthy-sounding messages, as well as to be able to detect insincere messages. There is a wide body of literature studying informal communication within the context of “cheap talk”Footnote 5 (see, e.g., Farrell and Rabin 1996; Crawford 1998). Nonetheless, the literature has focused heavily on how cheap talk affects senders,Footnote 6 and very little on how it affects receivers (see, for example, Farrell and Rabin 1996; Croson et al. 2003; Charness and Dufwenberg 2006). If cheap talk messages work by changing receivers’ beliefs about senders’ actions (as suggested by Charness and Dufwenberg 2006), then many important questions remain open. Such questions include: (i) the precise nature of messages to which people are most likely to respond positively; and (ii) the extent to which people are able to distinguish truthful messages from deceptive ones (and correctly update their beliefs). This paper takes a step toward answering these questions. In particular, we investigate whether there are cues that can predict whether a written communication is dishonest, and if so, whether the person reading the message can detect and correctly use those cues. Our study introduces a novel variant of the trust game (building on the hidden action game of Charness and Dufwenberg 2006). Our game captures an environment with misaligned incentives and opportunities to defect, but also includes potential gains from cooperation. In this context, we offer participants the opportunity to communicate with one another using hand-written messages. We use this design to accomplish three research goals: (i) to determine the characteristics of cheap talk messages that promote receivers’ trust; (ii) to discover objectively quantifiable cues for differentiating promises writers are likely keep from those they are likely to break; and (iii) to assess whether message receivers recognize and respond correctly to those cues. We find that receivers are significantly more likely to consider longer messages to be promises, as compared to shorter messages. In this sense, there is a payoff to a message sender’s effort. Second, we find that promises mentioning money are significantly more likely to be broken. Yet receivers fail to respond to this cue. Instead, they place more trust in longer promises, despite the fact that senders are just as likely to break such promises as they are to break shorter promises. Finally, people perform, on average, slightly better than random guessing at judging whether a sender will honor a message. The reason is that readers are able to distinguish promises from empty talk, and they correctly place more trust in promises. However, within kept and broken promises, readers cannot reliably determine which promises a sender will or will not honor. These findings help to explain features of our natural environment. For example, advertisements often provide extensive details regarding the benefits of offered products. Presumably, the reason is that companies have learned that longer promises are more likely to be believed. This paper proceeds as follows. In Sect. 2, we discuss related literature. Section 3 explains the context from which we obtain the message data, as well as the experimental design. In Sect. 4, we report our analysis and results. Section 5 summarizes and concludes.",15
20.0,2.0,Experimental Economics,29 July 2016,https://link.springer.com/article/10.1007/s10683-016-9490-3,"An experimental study of democracy breakdown, income and inequality",June 2017,Dmitry Ryvkin,Anastasia Semykina,,Male,Female,Unknown,Mix,,
20.0,2.0,Experimental Economics,28 July 2016,https://link.springer.com/article/10.1007/s10683-016-9491-2,Who never tells a lie?,June 2017,Christoph Vanberg,,,Male,Unknown,Unknown,Male,"One of the most basic moral rules in most cultures is that one should not lie. The importance of this moral principle is reflected in stories told to children, such as the one about the boy who cried wolf. Some moral philosophers follow Kant (1785) in suggesting that rules of this kind should be followed in all instances, irrespective of the consequences to be expected in any particular case. According to this position, it would be wrong to lie even in a case where doing so would lead to consequences that would be generally regarded as desirable. Given the importance of this and other basic moral principles, it is interesting to ask whether (or to what extent) common moral attitudes and behaviors coincide with this Kantian position. That is, to what extent do people regard lying as ‘wrong’ and therefore avoid lying, irrespective of the consequences to be expected from doing so? A number of authors have experimentally investigated this question using variants of a sender-receiver game originally introduced by Gneezy (2005) (e.g. Dreber and Johannesson 2008; Cohen et al. 2009; Sutter 2009; Erat and Gneezy 2012; Gneezy et al. 2013). In general, the message emerging from this literature has been that most experimental subjects take expected consequences into account when deciding whether to lie. In particular, it appears that people are more likely to lie if the personal benefit to be gained from doing so is larger, and if the harm imposed on others is smaller.Footnote 1 With one exception, these experiments have investigated lying in settings where the person telling a lie can benefit at a cost to someone else.Footnote 2
 In a recent contribution, (Erat and Gneezy 2012 henceforth EG) aimed to investigate cases where lying can be materially beneficial to another person (‘white lies’), and possibly to both the liar and another person (‘Pareto white lies’). Testing for an aversion to the latter type of lie is particularly interesting. Since the material consequences of telling such a lie are unambiguously desirable, such an aversion could be attributed only to a “pure” cost of lying. Therefore this could provide a good test for the existence of ‘Kantian’ moral attitudes. As the authors explain, [P]eople who are reluctant to tell Pareto white lies demonstrate lie aversion independent of social preferences for outcomes. Such people refrain from lying not (merely) because of the consequences, but because they simply view lying as a bad act in itself. This provides the best test of a pure cost of lying in line with a moral stand. (ibid. p. 724) To test for such a ‘pure’ aversion to lying, EG conduct an experimental game involving 2 players, labeled ‘sender’ and ‘receiver’. The sender is informed that a die has been rolled and the outcome was ‘2’. He is also told that the receiver does not know the outcome of the die roll. The sender is asked to send a message of the form ‘The number rolled was X’. This message is transmitted to the receiver, who must then choose a number between 1 and 6. (I will refer to this number as the receiver’s ‘guess,’ though EG do not use this terminology.) The sender is told that the players’ payoffs are determined as follows. If the receiver chooses 2 (‘guesses correctly’), both receive 20 dollars. Otherwise (i.e. if she ‘guesses wrong’), both receive 30 dollars. Finally, the sender is told that the receiver does not know the payoff consequences of guessing correctly or not. The result of the experiment is that \(35\,\%\) of senders tell the truth in this situation (choose the message “The number rolled was 2”). The authors interpret this result as showing that “A significant fraction of senders do not lie even when lying results in a Pareto improvement.” (ibid. p. 726, emphasis added). This interpretation appears to be widely accepted, and many authors cite EG’s experiment as evidence for a ‘pure’ aversion to lying (e.g. Cappelen et al. 2013; Chen and Houser 2013; Rosaz and Villeval 2012). Upon reflection, it becomes apparent that this interpretation relies on the implicit assumption that those senders who told the truth in the experiment did so believing that receivers will ‘follow’ their messages.Footnote 3 In order to investigate this assumption, I replicate EG’s experiment with the addition of a (non-incentivized) post-experimental questionnaire. I find that 32 out of 106 subjects (30 %) told the truth, closely replicating EG’s original result. The non-incentivized questionnaire answers suggest, however, that the vast majority of these subjects believed that the receiver would not follow their messages, and expected that telling the truth, not lying, would yield better material consequences for both subjects.Footnote 4
 Since the questionnaire evidence was not incentivized, the replication study delivers only suggestive evidence that the received interpretation of EG’s finding may be mistaken. I therefore conduct two new experiments explicitly designed to test the hypothesis that a significant part of the population avoids lying, even when doing so would result in a Pareto improvement. These experiments differ from EG’s in several respects. However, subjects in both experiments ultimately face essentially the same choice as in EG. Both experiments are designed such that senders are likely to be confident that receivers will follow their messages. This is confirmed using a (non-incentivized) questionnaire. As we will see, the results obtained differ substantially from those of EG, and cast doubt on the existence of a ‘pure’ aversion to lying. More generally, they suggest that subjects’ moral attitudes rarely, if ever, coincide with the Kantian position which demands that ethical rules be followed in all instances and irrespective of consequences. The remainder of the paper is organized as follows. Section 2 presents the replication study, including the results of the non-incentivized questionnaire. Section 3 discusses the new experiments and results. Section 4 concludes. Experimental instructions are provided in the online supplemental material.",10
20.0,2.0,Experimental Economics,09 August 2016,https://link.springer.com/article/10.1007/s10683-016-9492-1,Identity and social exclusion: an experiment with Hispanic immigrants in the U.S.,June 2017,Natalia Candelo,Rachel T. A. Croson,Sherry Xin Li,Female,Female,Female,Female,"As the world becomes increasingly integrated, many communities have become more diverse, facing a rapid growth of ethnic minority groups and an influx of immigrants. On the one hand, diversity boosts innovation, stimulates local economic growth, and leads to culturally more vibrant communities. On the other hand, many communities experiencing increasing diversity face serious challenges, for example, in supporting a high-quality public sector (Alesina et al. 1999; Habyarimana et al. 2007). Two factors—identity and social exclusion—play major roles in the social integration process of ethnic minority groups and immigrants into their new communities (Byrne 2005). This paper demonstrates how these factors influence voluntary contributions toward public goods. Identity and social exclusion are widely considered in social science disciplines such as sociology, politics, anthropology, psychology and economics. Identity is a person’s sense of self derived from perceived membership in social groups, e.g., race, gender, or religion (Tajfel et al. 1971; Tajfel and Turner 1979, 1986). Social exclusion refers to disadvantages that individuals or groups experience when systematically blocked from accessing opportunities and resources which are normally available to members of the mainstream society (Stewart et al. 2006). While each of these factors can impede the social assimilation of an individual into a new society, they are conceptually distinct. Take the Hispanic sample in our study as an example. Some participants were born in the U.S., and thus were very likely to have high identity and a low degree of social exclusion. But it is possible for some of them, despite the low degree of social exclusion, to have low identity, especially if they live in an ethnically heterogeneous community or a community dominated by another ethnic group. On the other hand, in a traditional Hispanic enclave, it is possible that Hispanic immigrants may have a high identity but a strong feeling of being excluded from the mainstream American society. We believe that the constructs of social identity and exclusion are not simply mirror opposites, and thus that the empirical relationship between them is of interest. Our dataset is unique in capturing these dimensions independently, and our results indeed indicate that they are not well-correlated, at least among this sample. Social assimilation of a person into a new society is a multi-dimensional process that can be impeded by higher levels of social exclusion and lower levels of identification. Despite their importance in explaining such phenomena as ethnic conflicts, racial conflicts and discrimination, there are few studies in economics that address the impact of identity and social exclusion in decision-making. One exception is Akerlof and Kranton (2003). They provide a theoretical framework to capture the impact of identity and social exclusion on economic decisions by a dominant group and a non-dominant group in a society. Their model and its solution not only show a major effect of identity, but also demonstrate how the non-dominant group’s feeling of being socially excluded and their choices to seek psychological fulfillment in opposing the mainstream culture may also influence economic outcomes. For non-dominant groups of a society, Akerlof and Kranton (2003) suggest that identity and social exclusion are two crucial components of the social integration process which profoundly affect engagement in economic activities considered to be the norms of mainstream society. In this paper we experimentally investigate the impact of identity and social exclusion on economic decisions. We focus on one important dimension of economic decision making—individual’s contribution to support public goods, which is a norm of mainstream society in many cultures (e.g., Ostrom 2000). Specifically, we conduct a lab-in-the-field experiment with low-income Hispanic residents in three neighborhoods in a large city in the U.S. to investigate the impact of identity and social exclusion on contributions to public goods. In the experiment participants are given opportunities to contribute to a linear lab public good and to three organizations which provide local public goods. We find that identity and social exclusion, elicited through a post-experimental questionnaire, are both important determinants of contributions to public goods. While contributions to the lab public good and local organizations increase with the strength of identity, contributions to the local organizations decrease with perceptions of social exclusion. As discussed in the literature review, much of the previous economics literature has manipulated identity (via priming). This is one of the few papers which elicit natural identities, and the only one examining their impact on public goods provision. This methodology thus complements the previous work by showing the importance of identity, both manipulated and measured, on economic decisions. In addition, to the best of our knowledge, this is the first study to experimentally investigate the impact of social exclusion on economic choices of any type. We focus on a unique subject pool—Hispanics—one of the largest and fastest-growing ethnic minority groups in the U.S. Hispanics constitute 17 % of the nation’s total population. This proportion is projected to be 31 % by 2060 (U.S. Census  2010). However, despite their growing importance in the economy and society, the racial phenotypes and strong accents make it difficult for many Hispanics to “pass” as an American. Massey and Sánchez (2009) show that 60 % of Hispanic immigrants reject the identity of being an American, while 82 % report a Latino identity. The unique dataset we collect from this population provides naturally-occurring variation in individual-level measures of identity and social exclusion. Since the rapid growth of the Hispanic population is a primary driving force behind the increase in ethnic heterogeneity in American communities, our dataset allows us to study closely the possible challenges faced by these communities in public goods provision, with an eye to actionable policy interventions.Footnote 1
 The remainder of the paper is organized as follows. Section 2 reviews the related literature and introduces hypotheses. Section 3 describes the experimental design. The empirical analysis and results are presented in Sect. 4. Section 5 concludes and discusses the policy implications.",17
20.0,2.0,Experimental Economics,11 August 2016,https://link.springer.com/article/10.1007/s10683-016-9493-0,"Compensation schemes, liquidity provision, and asset prices: an experimental analysis",June 2017,Sascha Baghestanian,Paul Gortner,Baptiste Massenot,,Male,Male,Mix,,
20.0,2.0,Experimental Economics,11 November 2016,https://link.springer.com/article/10.1007/s10683-016-9496-x,The impact of stress on tournament entry,June 2017,Thomas Buser,Anna Dreber,Johanna Mollerstrom,Male,Female,Female,Mix,,
20.0,3.0,Experimental Economics,15 October 2016,https://link.springer.com/article/10.1007/s10683-016-9498-8,Subliminal influence on generosity,September 2017,Ola Andersson,Topi Miettinen,Ute Stephan,Male,Male,Female,Mix,,
20.0,3.0,Experimental Economics,01 October 2016,https://link.springer.com/article/10.1007/s10683-016-9499-7,Illusion of control and the pursuit of authority,September 2017,Randolph Sloof,Ferdinand A. von Siemens,,Male,Male,Unknown,Male,"The allocation of authority—the right to make decisions—is an important determinant for organizational success.Footnote 1 Organizations can determine which hierarchical positions are endowed with authority. But how this authority is used is difficult to control and thus depends on the personal characteristics of the organizational members holding authority. To understand how the allocation of authority affects organizational success, it is therefore crucial to understand the characteristics of the people who actively pursue and thus are likely to ultimately hold authority. Authority often has clear economic value for those who possess it, since it allows individuals to choose the option that best serves their interests. Some people might also pursue authority because they value it beyond their purely instrumental value. Recent studies by Fehr et al. (2013), Bartling et al. (2014) and Owens et al. (2014) indeed suggest that a significant fraction of their experimental subjects intrinsically value authority. In this paper we want to highlight another motivation to seek authority: people overvalue its objective instrumental value. The psychology literature in “illusion of control” documents that many overestimate the extent to which keeping control increases their chances for success, even if outcomes are clearly governed purely by chance.Footnote 2 In this paper we identify individuals’ illusion of control and evaluate whether those subjects with stronger illusion of control indeed have a stronger desire to pursue authority. In our controlled laboratory environment, subjects first gain experience with two activities: adding numbers and counting letters. Adding numbers is easier and thus more profitable than counting letters, and 95 % of our subjects indeed prefer adding numbers over counting letters. One of the two activities is performed again in the third part of the experiment. Which activity depends on the choice between two neutrally labeled and initially unspecified tasks: Task Green and Task Blue. Participants know that Task Green and Task Blue are connected to adding numbers or counting letters. However, they do not know whether Task Green corresponds to adding numbers and Task Blue corresponds to counting letters, or whether it is the other way around. It is common knowledge that nobody knows which task is connected to which activity, and that the connection between tasks and activities was initially randomly determined with both possibilities being equally likely. All of this is made very clear in the instructions and with the use of control questions. The choice between Task Green and Task Blue—a transparently random choice between adding numbers and counting letters—is either made by the subjects themselves or made on their behalf by a randomly chosen other subject. We measure subjects’ beliefs concerning the likelihood of getting their preferred activity if they make the choice for themselves and if some other participant makes the choice for them. We also measure their willingness to pay for choosing themselves. Even though we explicitly tell subjects that Task Green and Task Blue are equally likely to correspond to the activities adding numbers and counting letters, only 43.79 % of our subjects report that the probability of picking the preferred activity equals 50.00 % no matter who chooses. Although these subjects report that they assign authority no instrumental value, they are on average willing to pay 25.58 points to make the task choice themselves. Consistent with the existing literature, we thus observe intrinsic preferences for authority among our subjects with fully rational beliefs. However, we also find that many of our subjects entertain an illusion of control: 35.29 % believe that the probability to get what they want exceeds 50 % if they decide themselves, and 25.49 % of our subjects believe that they are more likely to get their preferred activity if they rather than another participant choose the task. Most importantly, illusion of control and pursuit of authority are connected. We find that the stronger the illusion of control of individuals, the more they are willing to pay to obtain authority. The Spearman rank correlations between our two measures of illusion of control and willingness to pay for authority are positive and statistically highly significant (p-values of less than 0.001). Finally, a simple calibration suggests that at least 24.82 % of the willingness to pay for objectively useless authority can be explained by illusion of control. We therefore obtain tentative individual-level evidence that illusion of control does exist and could be an important driver of peoples’ pursuit of authority.Footnote 3
",23
20.0,3.0,Experimental Economics,11 November 2016,https://link.springer.com/article/10.1007/s10683-016-9500-5,"Asymmetric firms, technology sharing and R&D investment",September 2017,Matthew R. Roelofs,Stein E. Østbye,Eirik E. Heen,Male,Male,Male,Male,"In this paper, we use both theoretical and experimental methodology to analyze how firms’ initial positions affect the probability that they cooperate in innovation when facing an uncertain R&D environment. We term possible cooperation in the R&D process sharing of knowledge, and ask how much R&D investment takes place conditional on whether a sharing agreement is reached. The paper explores these issues using a simple duopoly stage model where firms not only compete in some dimension (e.g. the product market), but are also allowed to cooperate in other dimensions (e.g. sharing of R&D knowledge), what Brandenburger and Nalebuff (1995) have called ‘coopetition.’ The analysis is carried out in three treatments, one with symmetric firms and two involving asymmetric firms (leaders and laggards). In one of the asymmetric scenarios the lagging firm has an incentive to share but the leading firm does not, even though the cooperative equilibrium is a Pareto improvement over the non-cooperative equilibrium. In the other asymmetric scenario, a side payment is introduced to generate individual incentives for both the leader and the laggard to share to reach the Pareto optimal outcome. The side payment scenario can be interpreted as a reduced form that captures what might be the outcome in a dynamic setting in which firms are able to take advantage of arbitrage possibilities. These three treatments yield clear predictions about cooperation based on expected profitability. Sharing is the equilibrium in both the case of symmetric firms as well as the case of asymmetric firms with side payments, while no sharing is the equilibrium with asymmetric firms without side payments. From a behavioral point of view, however, there are a number of reasons why firms may fail to cooperate. Leading firms may find it difficult to give up the idea of being in front and may decline to share, even when the incentives exist to do so. Alternatively, since sharing means both firms benefit from each others investment, leaders may seek to solidify their lead in absolute terms by sharing. Lagging firms may feel that they are too far behind and give up, leading to a choice not to share or, alternatively, they may forego sharing in order to implement a ‘go-for-broke’ strategy in which the lagging firm devotes a large portion of its resources in an attempt to overtake the leader. If this strategy fails then the lagging firm often fails as well. The bottom line is that while the model has clear theoretical predictions for when firms should agree to share and when they should not, the actual response of experimental subjects, particularly in the leader/ laggard scenario, could be quite different.Footnote 1 In effect, our paper presents an integrated model of R&D behavior with uncertainty and firm asymmetry and then uses a laboratory experiment to test the predictions of the model. There are two types of firm heterogeneity in our model. The first is asymmetry that exists before the investment decision is made, and is known, ex ante, to be related to innovation success. We include this type of asymmetry by considering industries where one firm starts out with an advantage in the R&D process. This advantage could come from a history of past success, simple good fortune, or the implementation of a non-neutral innovation policy. The second type of heterogeneity comes from uncertainty about the likelihood of success even after the investment decision is made. Variability could arise here as part of the firm’s underlying production technology or strategic choices on the part of the firm. Examples include differences in research platforms across firms (Wiethaus 2005) or differences in absorptive capacities (Cohen and Levinthal 1989). The model includes this type of heterogeneity by using a stochastic process that independently determines success or failure for each firm in the industry. Taken together, our model allows for a priori favorable circumstances to increase the probability of, but not guarantee, a successful R&D project. Because success or failure of the project is determined individually, it is possible for a firm that started (and finished) with a lead in R&D to fail while a firm that did less (or finished with less) R&D to be successful. The paper proceeds as follows. A brief literature review is presented in Sect. 2 followed by the theoretical model in Sect. 3. Section 4 presents the experimental design. Section 5 covers the results and Sect. 6 provides a concluding discussion.",5
20.0,3.0,Experimental Economics,16 December 2016,https://link.springer.com/article/10.1007/s10683-016-9501-4,Intolerable nuisances: some laboratory evidence on survivor curve shapes,September 2017,Ciril Bosch-Rosa,Christina Aperjis,Bernardo A. Huberman,Male,Female,Male,Mix,,
20.0,3.0,Experimental Economics,08 November 2016,https://link.springer.com/article/10.1007/s10683-016-9502-3,Saving face and group identity,September 2017,Tor Eriksson,Lei Mao,Marie Claire Villeval,Male,,Female,Mix,,
20.0,3.0,Experimental Economics,07 December 2016,https://link.springer.com/article/10.1007/s10683-016-9504-1,Simultaneous versus sequential all-pay auctions: an experimental study,September 2017,Lian Jian,Zheng Li,Tracy Xiao Liu,,,,Mix,,
20.0,3.0,Experimental Economics,09 December 2016,https://link.springer.com/article/10.1007/s10683-016-9505-0,"Not quite the best response: truth-telling, strategy-proof matching, and the manipulation of others",September 2017,Pablo Guillen,Rustamdjan Hakimov,,Male,Unknown,Unknown,Male,"Matching theory has been extremely successful in providing the mechanisms used for the design of markets in the real world. Matching mechanisms are currently used for markets such as medical schools for graduates (Roth 1984; Roth and Peranson 1999), housing for students (Chen and Sönmez 2002; Abdulkadiroglu and Sönmez 2003), school choice (Abdulkadiroglu et al. 2005), and kidney exchange (Roth 1984). When it comes to its practical application, one of the most important advantages of any mechanism is its strategy-proofness. That is, if participants could be convinced of the impossibility to manipulate, they would then devote their energy to discovering their own preferences. For instance, investigating which schools are best suited for them, rather than devising strategies to game the system. There are two leading strategy-proof matching mechanisms recommended by market designers for school choice: top trading cycles (TTC), see Abdulkadiroglu and Sönmez (2003) and deferred acceptance (DA), see Gale and Shapley (1962). TTC and DA have competing properties. That is, other than being strategy-proof TTC is Pareto optimal, but not envy-free. DA is envy-free but not Pareto optimal. Both mechanisms have been adopted by school boards. The Boston Public School system chose to use DA although market designers recommended TTC, see Abdulkadiroglu et al. (2005). The New Orleans Recovery District adopted TTC in 2012 (Vanacore 2012). There is an ongoing debate on whether strategy-proofness can be safely assumed for the real-life implementation of a matching algorithm. Early matching experiments (i.e., Chen and Sönmez 2002, 2006)Footnote 1 suggest truth-telling rates are higher for strategy-proof mechanisms than for non-strategy-proof mechanisms. However, this result might be driven by the fact that the non-strategy-proof mechanism used for comparison (immediate acceptance) is easy to manipulate, in the sense that it is easy to find a seemingly good or satisfactory way to manipulate them. Conversely, the low manipulation rates found for strategy-proof mechanisms may not be caused by the participants’ understanding of strategy-proofness, but by them being unable to find a satisfactory manipulation strategy, thus leading them to report a default option—the induced preference order. Guillen and Hing (2014) give some support to these ideas by showing how manipulation becomes modal when wrong advice is introduced. In a similar vein, Pais and Pintér (2008) and Pais et al. (2011) find that manipulation rates increase when more information about the underlying preferences of other participants is introduced. To sum up, truth-telling rates vary as a response to theoretically irrelevant changes in the environment. Thus, among the previously safely assumed strong connection of observed truth-telling rates in the lab theoretical strategy-proofness is very much in doubt. One of the main aims of the current study is to crash-test this connection. Strategy-proofness implies that truth-telling is the best response to any strategy chosen by other players. Some evidence indicates manipulation attempts in the real-life implementation of theoretically strategy-proof mechanisms. For instance, Guillen and Hing (2014) cite popular blogs that encourage manipulation in the Boston Public School (BPS) deferred-acceptance-based system. Fisher (2009) elaborates on the general dysfunctionality of the quasi strategy-proof National Resident Matching Program (NRMP) system, Nagarkar and Janis (2012) point out the fact that Advisors occasionally tell (NRMP) applicants to realistically consider their chances of matching at a program when determining its position on their rank lists. In the same vein a survey-based study, Rees-Jones (2015), presents evidence of attempts at misrepresentation in the NRMP match. Another recent study, Hassidim et al. (2015), reaches similar conclusions when studying the market for graduate psychologists in Israel. Given the evidence, it is reasonable to hypothesize that the actions of participants may be influenced by the likely manipulation of other participants.Footnote 2 We use this conjecture as a motivation to test the connection between truth-telling and the understanding of strategy-proofness by varying, within subjects, the information available on the strategies chosen by other players. We make use of an individual decision-making set-up to precisely control the amount of information on the preferences, underlying and/or submitted, by computer-simulated players (computer players) to human participants. All the subjects in our experiment played two treatments simultaneouslyFootnote 3: a full information deterministic baseline and one out of four treatments with different amounts of information on the strategies of other participants. The latter four treatments are: uncertain misrepresentation (UMT), in which participants know the underlying preferences of the computer players and they know that at least one of them will not report truthfully; certain blocking misrepresentation (CBMT), in which the underlying preferences are known and human players are informed that a computer player is misrepresenting its preferences in a particular manner such that the human’s first preference is blocked under the assumption of the truth-telling of other computer players. However, we do not point this out to subjects but show only that one computer player misrepresents her preferences; certain unblocking misrepresentation (CUMT), in which one of the computer players misrepresents her underlying preferences in a way that does not affect the human player’s chances of getting the top choice; and underlying preferences (UPT), in which subjects know the underlying preferences of computer players, but nothing about how they are reported, other than that the computer players will maximize their profit. The baseline allows participants to use the TTC algorithm to find the best response to the perfectly-known behavior of computer-simulated agents. Over 62% of the subjects truthfully report the full preference list and 78% report their true top choice (sufficient to maximize the payoff) in the baseline. Truthful preference revelation decreases greatly and significantly in each of the misrepresentation treatments. Nevertheless, there is no significant difference within subjects between truth-telling rates in the Baseline and UPT. This leads us to draw the conclusion that the high truth-telling rates in Baseline or UPT cannot be attributed to subjects’ understanding of strategy-proofness: information about the misrepresentation by computer players leads human subjects to misrepresent more often. We observe how the majority of subjects, 69%, behave as if they try and fail to best-respond to changes in the environment. We cannot reject the understanding of the dominant strategy property of TTC for 31% of the subjects as they submitted their true preference orders in the two treatments they played. The percentage grows to 34% among subjects who solved the allocation task correctly (note that this difference is not significant). Additional tests allowed us to conclude that these subjects are more likely to achieve a higher score in the Cognitive Reflection Test (Frederick 2005), the Wonderlic IQ test and be more successful in answering multiple-choice questions about the mechanism’s properties. In contrast to Klijn et al. (2013) we find no significant difference between the risk-aversion of subjects who played optimal and defensive strategies. The rest of the paper is structured as follows: in Sect. 2 we justify the experimental design and treatments, while we explicitly formulate our hypotheses in Sect. 3. Section 4 presents the results. It is followed by the concluding remarks in Sect. 5.",23
20.0,3.0,Experimental Economics,06 January 2017,https://link.springer.com/article/10.1007/s10683-016-9506-z,External and internal consistency of choices made in convex time budgets,September 2017,Anujit Chakraborty,Evan M. Calford,Yoram Halevy,Unknown,Male,Male,Male,"Elicitation of time preferences in the discounted utility (DU) model requires simultaneous estimation of the felicity and discount functions. To demonstrate, consider a subject whose preferences over consumption streams are represented by a time-invariantFootnote 1 DU model, and decides at time 0 on her consumption in periods \(t\) and \(t+k\). Her utility is given by \(U\left( c_{t},c_{t+k}\right) =D\left( t\right) u\left( c_{t}\right) +D\left( t+k\right) u\left( c_{t+k}\right)\), where \(D\left( \cdot \right)\) is the subject’s discount function, and \(u\left( \cdot \right)\) is her felicity function. Estimation of a discount function that is based on indifference between consumption of \(c^{1}\) at time \(t\) and \(c^{2}\left(>c^{1}\right)\) at time \(t+k\) (and nothing in the other period), e.g. through multiple price list (MPL), implies that \(D\left( t+k\right) /D\left( t\right) =u\left( c^{1}\right) /u\left( c^{2}\right)\). It is well known that if the researcher assumes linear \(u\left( \cdot \right)\) while the true felicity function is concave, it will bias the estimated \(D\left( t+k\right) /D\left( t\right)\) downwards.Footnote 2
 To cope with this difficulty Andersen et al. (2008) used the fact that under the standard Discounted Expected Utility model risk and time preferences are intimately linked: a concave utility function exhibits both atemporal risk aversion and a desire for intertemporal smoothing of consumption. Their double multiple price list (MPL) procedure uses one atemporal multiple price list to estimate risk preferences and a second intertemporal multiple price list to estimate time preferences. They use the curvature of the atemporal utility function in order to adjust the estimation of the discount function. 
Andreoni and Sprenger (2012a, abbreviated exchangeably as AS in the following) proposed an interesting alternative according to which a single instrument can be used to jointly estimate the felicity and discount functions, without explicitly relying on the subject’s risk preferences. Andreoni and Sprenger’s convex time budgets (CTB) are a convexification of pairwise choices made on lines in the intertemporal MPL and allow the economist to directly measure intertemporal substitution. In their design the subject faces linear experimental budgets, which allow her to choose interior allocations between payments at two time periods (\(t\) and \(t+k\)). One can rationalize such interior allocations if the subject’s preferences between \(c_{t}\) and \(c_{t+k}\) are (weakly) convex. It thus provides a way to directly adjust the measurement of the subject’s discount function for intertemporal substitution without the need to explicitly invoke expected utility.Footnote 3 
Andreoni and Sprenger (2012a), and their closely related study (Andreoni and Sprenger 2012b), have been followed by a large number of applications and comments.Footnote 4
 The current paper provides commentary and guidance for economists who wish to use CTB to measure time preferences. Specifically, we discuss a methodology for measuring the consistency of subject-level choices with a very general model of intertemporal choice (more general than the DU model). A key element of this methodology requires the inclusion of two convex budgets that differ only in their income level in the CTB design, which makes a direct test of wealth monotonicity possible.Footnote 5 We illustrate our approach using the data set of AS (on time allocation of money) and on the most influential application of CTB to date—the work of Augenblick et al. (2015), which investigates allocation of effort over time. In the AS study, we find surprisingly high rates of violations of the general model of intertemporal choice that we consider. For the subjects who did not exhibit any curvature in their CTB choices, we directly estimate their discount factor based on the three choice lists and the corresponding CTBs assuming linearity of the felicity function, for the sake of comparison. We find WARP violations between choices made on CTB and choice lists for these subjects, and most of these violations are in the direction of exhibiting lower impatience in CTB than in choice lists. This could be an explanation for why Andreoni and Sprenger (2012a) obtain reasonably high CTB discount factors for these subjects even though their discount factors are not adjusted upward (as there is no evidence that their felicity function is concave). In the Augenblick et al. (2015) paper we find substantial rates of demand monotonicity violations, especially in their replication study. The latter violations are accompanied by violations of classical monotonicity, which in turn are empirically associated with time inconsistent behavior. Choices that violate classical monotonicity cannot be rationalized by a monotone utility function, a fact that relates this finding to the literature on “decision-making quality” (Choi et al. 2014)—if rationalizability of choices by a utility function is a marker of choice quality, then there is definitely some relation between the decision making quality and adherence to the normative standard of time consistency. We believe these surprising findings highlight the importance of implementing our suggested methodology before and after using CTB data for estimation. In what follows, we suggest possible behavioral mechanisms (for example, magnitude effect, reference dependence, subject confusion, experimental design) that may generate the observed inconsistencies. We hope that our work encourages future research to further address these questions. We identify three basic properties that allocations in a CTB design should satisfy in order to be rationalizable by a very general model of intertemporal choice: allocations should satisfy wealth monotonicity (normality) implying that \(c_{t}\) and \(c_{t+k}\) should be weakly increasing in wealth, holding interest rate constant; \(c_{t}\) should be weakly decreasing in interest rate (demand monotonicity), holding the dates \(t\) and \(t+k\) and wealth normalized to the later date constant, with \(c_{t}\) strictly decreasing whenever \((c_{t},c_{t+k})\) is interior; allocations should be consistent with impatience implying that as the later (earlier) date is shifted away from the present, \(c_{t}\) should weakly increase (decrease), holding the earlier (later) date, price ratio and wealth constant. Additionally, we use the fact that AS also included some multiple price lists in their design to test for violations of the weak axiom of revealed preferences (WARP). The various monotonicity criteria for which we evaluate the empirical demand should not be confused with monotonicity of the utility function with respect to \(\left( c_{t},c_{t+k}\right) .\) In particular, wealth and demand monotonicity are consequences of the very weak assumption that \(c_{t}\) and \(c_{t+k}\) are normal goods. When choices are inconsistent with monotonicity of the utility function we say that they violate “classical monotonicity.” We document the level of adherence of choices (at the individual level) to the above very mild external and internal consistency requirements. We find a very high level of WARP violations among the many subjects who made corner choices in Andreoni and Sprenger (2012a). Violations of all three internal measures of monotonicity are concentrated in subjects who make interior choices and thereby take advantage of the novel feature of Andreoni and Sprenger’s CTB experimental design. Wealth monotonicity violations are more prevalent and pronounced than either demand or impatience monotonicity violations (except when all choices are interior). We then investigate the consistency of choices in the Augenblick et al. (2015) study. This is the most significant application of CTB to date, as it tries to distinguish discounting of primary rewards (or costs—implemented through an effort task) from discounting of monetary rewards (as in the majority of experimental studies of intertemporal preferences). One of the important findings of Augenblick et al. (2015) is that subjects tend to make interior choices much more often when deciding on allocation of effort than of money, and that there is significantly more time inconsistency (in the form of present bias) in effort. Augenblick et al. (2015) includes two experiments: the design of the first experiment may confound present bias with other sources of time inconsistency,Footnote 6 and the second experiment was designed in order to eliminate some of these potential confounds. Although Augenblick et al. (2015) did not include some of the important comparative static treatments that were part of Andreoni and Sprenger (2012a), we are still able to test for demand monotonicity. In the first experiment we find levels of demand non-monotonicity in effort that are comparable to interior choices (on the allocation of money) made in AS. However, in the replication study we find higher levels of demand monotonicity violations, that we could not account for even after taking into account rounding effects that allowed subjects to make choices that are inconsistent with monotone preferences and a higher number of interest rates faced by subjects. Additionally, we find that non-adherence to classical monotonicity is significantly associated with time inconsistent choices. We believe that the findings reported here motivate the following fundamental question: are choices made in CTB reflective of deep and stable preferences? We urge researchers to study the source of the documented inconsistent behavior in order to decide if it could be attributed to the implementation of CTB in the two studies we cover or if it reflects some behavior that the standard discounted utility models (and hence the structural estimation methods used in the mentioned studies) are not equipped to handle. We are of the opinion that inclusion of the wealth shifter in Andreoni and Sprenger (2012a) was a crucial design innovation, and we recommend that future CTB papers include a similar ‘wealth shifter’ to facilitate analysis. At a minimum, we would encourage future researchers to test their CTB data for consistency with the internal measures of monotonicity before applying the data in new settings or using it for the purpose of structural estimation. We proceed as follows. Section 2 provides a brief overview of the extremely active literature on measuring time preferences. Section 3 discusses Andreoni and Sprenger (2012a) in detail; we first describe how to identify wealth monotonicity, demand monotonicity, impatience and WARP violations in the AS dataset, and then present the results of our investigation. Section 4 provides a similar analysis of Augenblick et al. (2015). Section 5 concludes.",14
20.0,3.0,Experimental Economics,06 January 2017,https://link.springer.com/article/10.1007/s10683-016-9507-y,Distributing scarce jobs and output: experimental evidence on the dynamic effects of rationing,September 2017,Guidon Fenig,Luba Petersen,,Unknown,Female,Unknown,Female,"To a person living in a developed economy today, the term ‘rationing’ likely evokes images of bread lines and reinforces the failures of central planning. Such a view is grounded in truth, but it obscures the fact that rationing of many forms still arises when the gears of the economy do not turn as smoothly as hoped. Simply put, markets do not always clear in a timely and efficient manner, and when that happens it falls to market participants or policymakers to decide how to do with what is available. Rationing has been approached using many different allocation schemes. Food rationing occurred throughout North America and Europe during the two World Wars. Rationing was undertaken in such a way that every person would receive an equal portion of food. Victory or war gardens were planted at private residences and in public parks in many countries to alleviate demand on rationed food supplies. The gardens provided households an opportunity to suppleextremely stylized assumptions which make upment their weekly rations with their private food production. Those who put forth more effort tending to their gardens were able to eat more, leading to millions of tons of household food production.Footnote 1 Other rationing system have been associated with panic and instability. Before the advent of deposit insurance in the United States in 1933, banks allowed depositors to withdraw their money on a first-come, first-served basis until they ran out of funds. Depositors’ expectations that others would withdraw their deposits would caused panic and a run on the bank, leading to a fragile banking system. Large price cuts on Black Friday in the US or on Boxing Day in Canada often result in consumers’ waiting long hours in line and in buying frenzies for the newest tech gadgets. The rationing of inputs into the production process, namely labor hours, has been employed in dynamic macroeconomic models to generate cyclical fluctuations of involuntary unemployment observed in the United States and Europe (Michaillat 2012). Labor rationing has been modeled in partial equilibrium settings as the result of efficiency wages (Stiglitz 1976; Solow 1980; Shapiro and Stiglitz 1984), gift exchange (Akerlof 1984), and search costs and turnover costs (Salop 1979; Akerlof 1984), or in general equilibrium environments as a consequence of matching frictions in labor and product markets (Michaillat and Saez 2015). Unemployment risk associated with labor rationing can lead to precautionary saving, endogenous underemployment, and potentially deep recessions (Ravn and Sterk 2013; Kreamer 2014). The need for rationing can become even more pronounced when dealing with goods and services that are not typically sold on the open market. To deal with a very low supply of organ donations, the Israeli government implemented a policy in 2008 to give priority on organ waiting lists to those willing to sign an organ donation card. By 2011, the policy had led to a dramatic increase in the number of deceased and living donors relative to previous years (Lavee et al. 2013). These examples demonstrate the pervasiveness of rationing across time and in all manner of countries, as well as the wide-ranging consequences that can result. They lead us to ask challenging questions about the efficiency and fairness of different rationing schemes. Standard macroeconomic models with rational agents, however, fall silent on this subject; such an agent optimally demands the amount of output associated with its labor supply, and thus no rationing is needed. This paper seeks to step into the gap between macroeconomic methodology and the answers we seek, by way of macroeconomic laboratory experiments. It is through such experiments that we have attempted to understand how varying approaches to rationing affect welfare not only directly, but also by affecting agents’ choices in other markets. We make use of a macroeconomic setting where households supply costly labor that produces utility-yielding output. In our framework, rationing occurs either when households are unwilling to purchase all the output they wish to produce, or when they prefer to consume more than they are willing to work to produce. Rationing occurs as a consequence of aggregate household decisions, an inability for prices to adjust fully, and a lack of inventories. When agents anticipate output rationing the optimal consumption–leisure tradeoff condition predicts that their willingness to supply labor also decreases, and vice versa. We investigate whether alternative allocation schemes lead to a greater reaction to and incidence of rationing, increased spillover effects into other markets, and overall greater volatility in production. Decision-making and aggregate outcomes are compared under three non-manipulable rationing schemes: a random queue where the rationed market is distributed on a first-come, first-served basis, an equitable allocation scheme, and a priority scheme where those willing to buy what they produce (or produce what they demand) receive priority for scarce labor hours (output). Our contribution is to provide causal evidence of the implications of rationing rules on the availability of scarce labor opportunities and output. First, unlike the predictions of standard equilibrium models, we allow for and observe rationing of jobs and output in all of our sessions, with most instances involving output rationing. We also observe that the mechanism by which the short side of the market is rationed does matter for welfare and macroeconomic stability. Participants are willing to supply high levels of costly labor if they are given priority to purchase the output they produce. Under an equitable allocation scheme, the willingness to supply labor decreases in the presence of rationing, and output volatility is considerably higher. Occasionally—and sometimes permanently—aggregate labor supply will collapse to low levels due to output rationing under random and equitable distribution schemes. Allocating scarce output and jobs according to a priority scheme, by contrast, results in significantly more stable production by reducing subjects’ exposure and reaction to rationing. The confident far-sightedness and market clearing assumptions upon which mainstream macro models rely is at odds with the strategic uncertainty agents face when the actions of others could affect the future availability of resources. The typical response to output rationing observed in our experiments is to increase output demands and decrease labor supplies. This propagates further rationing of output. To rationalize these results we develop a model of myopic decision-making in which agents focus only on maximizing current utility and pessimistically expect others in the market to possess extremely high demands. Consistent with our experimental findings, the model predicts suboptimal equilibria under equitable and random allocation schemes that involve high aggregate demand for output and low labor supply. Our findings also provide important political economy insights into redistribution policies. In an environment where individuals are equally skilled, redistributive policies that allocate equitably or on first-come-first-serve basis and that fail to appropriately compensate individuals for their costly labor can decrease willingness to work and lead to periods of economic turmoil. Conversely, policies that minimize redistribution and enforce property rights to the fruits of one’s labor creates a sufficient incentive to consistently supply labor and can foster greater macroeconomic stability.",2
20.0,3.0,Experimental Economics,31 December 2016,https://link.springer.com/article/10.1007/s10683-016-9509-9,Do casinos pay their customers to become risk-averse? Revising the house money effect in a field experiment,September 2017,Maximilian Rüdisser,Raphael Flepp,Egon Franck,Male,Male,Male,Male,"Suppose you enter a casino, and before you have the chance to gamble, a casino representative gives you free chips worth $50. Would this windfall “house money” alter your subsequent risk-taking behavior during your stay at the casino? For one-stage gambles, the prospect theory developed by Kahneman and Tversky (1979), in which an individual’s utility depends on his or her current level of wealth as reference point, predicts risk-averse behavior in the domain of gains and risk-seeking behavior in the domain of losses. However, in multiple-stage settings such as in the introductory example, prospect theory does not predict a clear behavioral pattern, as the single decision events could be either segregated or integrated with prior outcomes (Tversky and Kahneman 1981). Thaler and Johnson (1990) were the first to investigate how prior gains and losses influence subsequent risk-taking in a laboratory experiment. They found that prior gains induced risk-seeking behavior, which they referred to as the “house money effect”. The authors explained this effect with a quasi-hedonic editing rule that individuals adopt. According to this editing rule, after a prior gain, subsequent gains are segregated but subsequent losses are integrated. Thus, subsequent losses are coded as reductions in a gain, which mitigates the influence of loss aversion and facilitates risk-seeking behavior (Thaler and Johnson 1990). The following example illustrates the intuition behind Thaler and Johnson’s (1990) house money effect: consider an initial windfall gain of $50 and the subsequent option to enter a fair coin toss to either win or lose $20. This situation is edited as “a 50% chance to win $50 and to win an additional $20 and a 50% chance to end up with a combined win of $30.” Thus, instead of considering winning or losing $20 and being loss averse, the potential loss of $20 is integrated with the prior gain of $50 and evaluated as a reduction in gains. Because the potential reduction in gains is overcompensated by the potential win of $20, which is segregated from the prior gain of $50, this evaluation leads to the acceptance of the gambling option and thus to risk-seeking behavior. Many studies confirm the house money effect in laboratory experiments that include tasks, such as capital expenditure decisions (Keasey and Moon 1996), investments in risky assets (e.g., Ackert et al. 2006), choosing between lotteries (e.g., Battalio et al. 1990; Gärling and Romanus 1997; Weber and Zuchel 2005), and investments in trust games (e.g., Houser and Xiao 2015). Thus, the broad experimental evidence seems unambiguous. However, a major challenge when studying the house money effect is that the risk-taking behavior using house money has to be compared to the risk-taking behavior using one’s own money. Because the latter is associated with the risk of real losses for subjects, such a comparison is difficult to implement in laboratory experiments due to ethical reasons (Etchart-Vincent and l’Haridon 2011). Several experimental studies have addressed this issue by inducing real loss perceptions in different ways when studying the house money effect. Clark (2002) required subjects to bring their own money to the experiment, and he found no evidence of the house money effect in the case of voluntary contribution to a public good. Interestingly, when Harrison (2007) critically reevaluated the data of Clark (2002), he found that the provision of house money led to more free-riding in public goods. This could be interpreted as an increase in risk aversion, which contradicts the house money effect. Cherry et al. (2005) also failed to find a house money effect when they compared the levels of contribution to a public good between subjects who had earned their endowment in a task and subjects who had been directly endowed with house money. In an alternative approach, Cárdenas et al. (2014) distributed an endowment amount to a treatment group well before their experiment took place to induce an own-money perception. Employing different lotteries involving losses and gains, they found that “own-money subjects” were slightly more risk-averse, which they interpreted as evidence of a small and indirect house money effect. Finally, Etchart-Vincent and l’Haridon (2011) allowed their subjects to experience real losses in one of three sessions held on different days. They compared a “losses-from-an-initial-endowment” treatment to a real loss treatment but failed to replicate the house money effect. Consequently, the evidence on the house money effect from studies simulating real losses in the laboratory is unconvincing. Our paper adds to the literature by studying the house money effect in a randomized field experiment with real gamblers facing real potential losses.Footnote 1 A Swiss casino is an ideal setting for our experiment. Upon entering the casino, subjects were randomly selected to play a wheel of fortune. The treatment group consisted of 579 subjects who received a free play coupon worth between 5 and 50 Swiss Francs (CHF) after playing the wheel of fortune.Footnote 2 There were also two control groups. The first control group consisted of 186 subjects who received no free play coupon after playing the wheel of fortune; the second consisted of 152 randomly selected subjects who entered the casino without playing the wheel of fortune. Subsequently, the gambling behavior of all subjects was monitored using records from both slot machines and table gambling. Our results showed that subjects who were endowed with house money became more risk-averse than subjects who only played with their own money. More specifically, treated subjects reduced their average wager—our measure of attitude towards risk—by 15–30%, depending on the baseline control group. Furthermore, the size of the initial endowment moderated the increase in risk aversion; higher endowment amounts led to even more pronounced risk-averse behavior. These differences in risk-taking also translated into significant economic impacts. Considering only their own money they brought to the casino, treated subjects left the casino with more money (smaller losses) in their pockets than the control groups. Thus, rather than confirming the house money effect, our results point towards a “reverse house money effect”. More generally, this paper adds findings to the literature on sequential decision making under risk. Our data from the field reveal that prior gains indeed have a strong impact on subsequent decision making. However, our results suggest that a prior gain triggers an increase in risk-aversion, which challenges the existence of a house money effect and its underlying quasi-hedonic editing mechanism. The remainder of this paper is structured as follows. In Sect. 2, we present the experimental design and the data. In Sect. 3, we show the results. Finally, in Sect. 4, we discuss our findings and conclude.",15
20.0,4.0,Experimental Economics,24 February 2017,https://link.springer.com/article/10.1007/s10683-017-9517-4,Transparency and cooperation in repeated dilemma games: a meta study,December 2017,Lenka Fiala,Sigrid Suetens,,Female,Female,Unknown,Female,"Part of the social sciences is concerned with identifying determinants of voluntary cooperation in repeated group interactions, with the ultimate goal of obtaining knowledge about how cooperation can be influenced. How can individuals in a group be stimulated to contribute to a public good? But also, how can it be avoided that firms with market power collude? Clearly, the two environments in these examples—public good settings and oligopoly markets—are very different in many ways (for example, in terms of decision-makers and social-welfare effects of cooperation). However, the two settings also have a crucial feature in common; both have a ‘dilemma’ structure. In both settings, short-sighted selfishness of the players in the group leads to suboptimal outcomes for the group.Footnote 1 The point is that the basic strategic nature of the decision problem of firms in a cartel is the same as that of the ‘free-rider’ problem. Therefore, when it comes to the very basics of behavior, we believe there is scope for cross-fertilization between the two fields. An important determinant of behavior in finitely repeated dilemma games with fixed matching that has received considerable attention in both fields, is transparency about past outcomes or choices. Transparency can potentially be influenced by those who are in power to ‘govern groups’, and can therefore be an attractive tool to stimulate or push back cooperation.Footnote 2 This tool can find use both on a macro scale (e.g., regulation in the form of forced disclosure), micro scale (e.g., incentives within companies), and in laboratory experiments, as an inconspicuous mechanism to induce certain behaviors. To apply transparency effectively, however, we must understand how different forms of it affect behavior, and whether the effects are general, or, instead, can be overridden by context or group size. This is exactly what we study. Several ways have been identified through which transparency may increase cooperation in a group. First, giving players information about past choices made by others in the group makes it easier to spot deviations, and knowing whether a deviation has occurred is a necessary condition for stable long-run cooperation to be sustainable, at least in theory (Friedman 1971; Fudenberg and Maskin 1986; Benoit and Krishna 1984). Second, transparency allows cooperative players to signal their cooperative intentions, which may spur them to induce cooperation, and conditional cooperators to follow or imitate the induced cooperation (Fischbacher et al. 2001; Davis et al. 2010). Transparency may also impede cooperation, however. For one, if players receive information about the details of past choices and earnings, this may lead to an ‘imitation spiral’ where players imitate the most successful player, that is, the player who earned the highest amount, up until cooperation is completely destroyed (Vega-Redondo 1997; Huck et al. 2000). Also, receiving detailed information about earnings may put players in a relatively selfish state of mind, leading them to focus on the payoff consequences of their choices (Nikiforakis 2010). Finally, if detailed information about choices is provided, players may become aware of the potentially very unequal distribution of outcomes associated with it, and be less motivated to cooperate (Cheung 2014). The current paper is set out to study similarities and differences across the two fields in how transparency about past choices and payoffs affects choices (contributions, prices, or quantities). The focus is on games with finitely repeated interaction, where the same group of (more than two) players repeatedly plays a dilemma game.Footnote 3 These games can be taken to be representative for fundamentals of behavior of symmetric or close to symmetric agents, making decisions in relatively small groups (e.g., firms of similar size competing in oligopolistic industries, individuals working in teams, pollution problems in similar-sized countries). To do so, we assembled data from laboratory experiments on linear public goods games and oligopoly games of price- and quantity setting. The data set covers data from 71 studies on public goods, covering 122 different treatments that have 1205 independent observations and 5565 participants in total, and from 18 studies on decision-making in oligopoly, covering another 50 different treatments with 387 independent observations and 1339 participants. Related meta-studies are those of Zelmer (2003) on linear public goods experiments and Engel (2007) on oligopoly experiments.Footnote 4 Our study differs from these studies in that we focus on a specific topic—the effect of transparency about choices and payoffs on cooperation—and we study it across disciplines. Also, none of these studies track the details we track about the nature of feedback provided to participants in the experiments. Our main finding is that transparency about past individual payoffs of group members is destructive for cooperation in both dilemma settings. In public goods experiments, this type of transparency leads to a significant reduction in contributions, whereas in oligopoly experiments, it leads to a significantly lower degree of collusion.",27
20.0,4.0,Experimental Economics,28 January 2017,https://link.springer.com/article/10.1007/s10683-017-9510-y,Optimistic irrationality and overbidding in private value auctions,December 2017,Sotiris Georganas,Dan Levin,Peter McGee,Unknown,Male,Male,Male,"In a sealed-bid second-price auction (SPA) with private valuations, where the highest bidder wins and pays the second highest bid, bidding one’s value is a weakly dominant strategy (WDS, Vickrey 1961). This strategy requires only that each bidder behave rationally and it is unaffected by the number of rivals or their valuations, a bidder’s risk preferences, or beliefs regarding rationality of rivals. Repeated experimental studies have found that subjects deviate from the WDS by overbidding much more than underbidding, resulting in overbidding on average (e.g., Kagel et al. 1987; Kagel and Levin 1993). By contrast, experimental evidence from the strategically equivalent ascending English auction demonstrates almost immediate convergence to the dominant strategy (e.g. Kagel et al. 1987; Kagel and Levin 1993; Kagel 1995).Footnote 1 While overbidding relative to the risk-neutral, Nash equilibrium (RNNE) has also been frequently found in first-price auctions (FPA, e.g., Kagel 1995; Kagel and Levin 1993), the “usual suspects,”—risk aversion, beliefs about others’ play, biases in perceptions of probabilities—that may explain overbidding in FPAs are of no avail in SPAs. The contrast between SPAs and English auctions suggests that subjects discover the WDS in the English auction but not in the SPA; why is this the case? The cognitive process that leads to the discovery of the WDS in an SPA is far from trivial and an experimental subject may be unable to recognize it without experience or training. In an English auction, on the other hand, a subject needs to answer a simple question for herself: “Am I ‘in’ or ‘out’?” Answering this question leads a bidder to drop out at his value. Subjects who do not bid their value in SPAs are nevertheless still motivated by common sense economic incentives, such as expected payoffs, though imperfectly. Kagel et al. (1987) conjectured that subjects are aware that higher bidding increases the probability of winning the auctions but underestimate the additional cost associated with it. Instead of looking for dominant strategies, we suggest that optimistically irrational bidders are guided by a desire to maximize their profits combined with an inability to fully grasp the intricacies of the auction environment that allows them to view the consequences of their actions more favorably. We do this by modeling reasonable bidders who recognize (i) a higher bid increases the probability of winning, and (ii) the bidder may understate negative payoffs to higher bids. These behaviorally plausible assumptions about bidders are the building blocks of our simple model of how out-of-equilibrium incentives might affect behavior in SPAs. We test our model in SPAs in which we introduce a parameter that changes the expected payoff as a function of one’s bid but does not affect the WDS. The parameter multiplies realized losses by some amount \(\beta \), where \(\beta =1\) is the standard case. Consistent with previous results we find that when \(0<\beta \le 1\), overbidding is pervasive. In contrast, when we change \(\beta \) to 20, overbidding is significantly reduced and underbidding is more prevalent. Overbidding when \(0<\beta \le 1\) results in very few and fairly small losses (5.8% of auctions; median loss of $0.10). This is a product of our design: the domain of bidders’ private values is quite large relative to the number of bidders, so the second highest bid is almost always below the highest value, even with overbidding. This allows us to rule out a “hot stove” type of learning whereby losses reduce overbidding in subsequent auctions.Footnote 2 Instead, it appears that the dramatic reduction in overbidding occurs when \(\beta \) is exogenously and publicly increased and can be attributed to changes in expected out-of-equilibrium payoffs. While explanations for overbidding in various auction formats abound, and we compare the fit of our model to several of them in Sect. 5, the contribution of our model lies in its focus on the dominant strategy, adding to the recent theoretical interest in how dominant strategies influence decision calculus in games (e.g., obvious strategy-proofness, Li 2016). Our strong findings suggest that incentives outside equilibrium affect behavior in predictable ways in the laboratory, and probably in the field as well, even when equilibrium analysis predicts otherwise. Goeree et al. (2002) show a similar result in a FPAs, but in FPAs, as in many other games where Nash equilibrium is the solution concept, best responding requires “cardinal” computations. Since such computations often involve a high degree of complexity and a heavy mental cost, we do not expect that the outcome in FPAs will exactly reflect the point prediction of Nash equilibrium. It is much less surprising to find that the subjects’ calculations, possibly involving heuristics, approximations and simplification rules, will be affected by a change in the incentives, even if these ought to have no effect on Nash equilibrium. This complexity motivates many models that predict overbidding by allowing bidders to make, and learn from, mistakes (e.g., QRE). In an SPA with private values, however, the dominant strategy can be reached with just “ordinal logic” of dominance, without even a need for common knowledge of rationality.Footnote 3 Thus, one would expect the solution norm—bid your value—to have its best chance for success in this environment. Our study shows that behavior is still guided by some degree of conscious profit maximization, but subjects’ decision processes fail to recognize a characteristic that is very seldom present outside the lab: the dominant strategy. Errors in recognizing a dominant strategy require a new perspective on the cognitive processes underlying bidding behavior of the sort provided by optimistic irrationality to try to explain “errors” made by bidders that are as much a function of the simplicity of a dominant strategy as the complexity of the environment.",18
20.0,4.0,Experimental Economics,10 February 2017,https://link.springer.com/article/10.1007/s10683-017-9511-x,Externalities in appropriation: responses to probabilistic losses,December 2017,Esther Blanco,Tobias Haller,James M. Walker,Female,Male,Male,Mix,,
20.0,4.0,Experimental Economics,28 February 2017,https://link.springer.com/article/10.1007/s10683-017-9512-9,Do people who care about others cooperate more? Experimental evidence from relative incentive pay,December 2017,Pablo Hernandez-Lagos,Dylan Minor,Dana Sisak,Male,,Female,Mix,,
20.0,4.0,Experimental Economics,07 February 2017,https://link.springer.com/article/10.1007/s10683-017-9513-8,Leaving the market or reducing the coverage? A model-based experimental analysis of the demand for insurance,December 2017,Anne Corcos,François Pannequin,Claude Montmarquette,Female,Male,,Mix,,
20.0,4.0,Experimental Economics,11 March 2017,https://link.springer.com/article/10.1007/s10683-017-9514-7,A trust game in loss domain,December 2017,Ola Kvaløy,Miguel Luzuriaga,Trond E. Olsen,Male,Male,Male,Male,"People’s willingness to trust and reciprocate trust is of great importance for economic prosperity. It enforces incomplete contracts and reduces transaction costs. A large economic literature has thus evolved investigating the origins of trust, when and why people are willing to trust, when and why people act trustworthily, and what consequences trust (and the lack thereof) has for economic welfare. The experimental literature on trust and reciprocity has been an important contributor in this respect. It is now well established that people have a tendency to reward kind actions, and are thus willing to trust others since they expect to be reciprocated, see Fehr et al. (1993), Berg et al. (1995) and the large body of evidence thereafter. Bohnet and Croson (2004) and Bohnet (2008) provide comprehensive reviews of the literature. Except for a paper by Bohnet and Meier (2012), which we will turn to below, a common feature with experimental papers on trust is that they are variations of investment games where trust provides a potential gain and where no trust yields no loss. Trust is an active decision, an act of commission, while no trust is a passive one, an act of omission. Moreover, trustworthiness typically involves actively returning money (or effort) to the trustor, while no trustworthiness is a more passive decision, choosing not to return anything. In many situations, however, trust often involves taking no action, and the return from trust is the absence of loss. We trust others when we do not lock our door, do not take all necessary precautions, and do not spend resources in writing detailed contracts that cover every possible contingency. Additionally, we are trustworthy when we do not steal or do not abuse those who dare make transactions under loose contracts. In other words, full trust is often the default. This distinction between trust in the loss domain and the gain domain is potentially important. From Kahneman and Tversky (1979) we know that people act differently in the loss domain than in the gain domain, and importantly, that losses loom larger than gains. A consequence is that defaults or reference points matter, as particularly demonstrated by Thaler (1980) and Samuelson and Zeckhauser (1988). Reference points and defaults determine whether a given prospect or value is perceived as being in the loss domain or the gain domain. Related to this, it has been shown that perceptions of an outcome differ if the outcome resulted from an act of omission rather than an act of commission [see Kahneman and Tversky (1982), and Baron and Ritov (1994)]. Given these insights, the absence of trust expectedly feels more severe in the loss domain when full trust is the default than in the gain domain when no trust is the default. Furthermore, a given intermediate level of trust may be perceived as less kind in the loss domain than in the gain domain. Hence, if trustees care about the kindness of the trustors’ actions [as has been demonstrated in a number of experiments, most notably McCabe et al. (2003), and Charness and Levine (2007)], we will expect less trustworthiness in the loss domain when full trust is default than in the gain domain when no trust is default. Moreover, since losses loom larger than gains, we will expect that subjects respond more positively to higher trust in the loss domain than in the gain domain. However, the effect on trust is more subtle. The pure default effect calls for higher trust in the loss domain, but since the expected trustworthiness level is lower, one may consequently also expect lower trust in the loss domain. In this paper we formalize the hypotheses above, based on Cox et al.’s (2007) “Tractable model of fairness and reciprocity”. We then investigate the hypotheses by running a rephrased version of Berg et al.’s (1995) well known trust game (which they call the investment game). In a controlled laboratory experiment, subjects were randomly paired as players A and B, which we here will refer to as trustor and responder, respectively. Both the trustors and the responders received an endowment of 200 Norwegian kroner (about $34). The responder was then given the opportunity to take an amount from the trustor’s endowment. But before the responder made this decision, the trustor could choose to insure an amount that the responder could not take. The money not insured was then equivalent to money trusted. For each krone that the trustor insured, the total surplus in the relationship was reduced by two kroner. Hence, full trust was the default, while the lack of trust generated a loss. In economic terms, this game is identical to Berg et al., where the trustor could instead send money, and for each krone sent, total surplus increased by two (i.e. money was tripled). The only difference is that Berg et al. is framed in a gain domain, while our new trust game is framed in a loss domain. Indeed, subjects did behave differently in the gain domain than in the loss domain. We find significantly lower levels of both trust and trustworthiness in the loss domain when full trust was default, than in the baseline trust game. As a consequence, trust was on average profitable for the trustor when no trust was default, but not when full trust was default. The treatment differences are quite large, also in economic terms. The trust and trustworthiness levels were, respectively, 51 and 32% higher in Baseline than in the loss domain. Moreover, we find that responders were more responsive to a marginal change of money trusted in the loss domain than in the gain domain. In Baseline, higher trust did not lead to higher levels of trustworthiness. In the loss domain, however, the responders were more trustworthy the more they were trusted. 
Related literature There is now a large literature on reference-dependent preferences, but the experimental literature on how reference points and defaults affect pro-social behavior is limited. The return from pro-social behavior is typically in terms of gains, not the absence of loss.Footnote 1 There are, however, some exceptions: Andreoni (1995) and Willinger and Ziegelmeyer (1999) study framing in public good games and find that the average contribution to public goods is higher when the game is framed positively (giving to the public) than negatively (taking from the public).Footnote 2 Lopez and Nelson (2005) study how endowment effects influence public good provisions, and find that subjects contribute more to a public account when the money is said to start in their private account than when the money is said to start in their public account. In contrast, Messer et al. (2007) find that a status quo of giving increases public good contributions. It has also been shown that framing can effect coordination, see in particular Ispano and Peter (2013) who find that subjects coordinate better in the loss domain than in the gain domain. Framing effects have also been demonstrated in the classic generosity games. Bardsley (2008) and List (2007) find that reference points affect generosity in dictator games. In particular, it is shown that subjects are less willing to transfer money when the action set includes taking. Correspondingly, Abbink et al. (2011) find more antisocial behavior in the loss domain of money burning tasks. However, Baquero et al. (2013) find a huge generosity effect in the loss domain of ultimatum games, implying that strategic considerations, which are absent in dictator games, alter the framing effects. Closest to our paper is Bohnet and Meier (2012). They introduce a reframed version of Berg et al. (1995), which they call the distrust game. As in our paper, full trust is default and no trust yields a loss. Yet in contrast to our paper (and Berg et al.), the parties do not start out with the same endowment. The trustor starts with nothing, while the trustee (responder) has the whole endowment. Hence, the trustor is never in the loss domain. Moreover, there are two reference points that are different from the baseline trust game, namely the trust level and the inequality level, leading to a “double default effect”. In Sect. 3, we argue that this double default effect can lead the trustor to make decisions that are closer to the default of full trust/high inequality. In other words, the trustor’s default effects are more likely to dominate the strategic response to an expectedly lower trustworthiness level. Indeed, Bohnet and Meier find lower trustworthiness in their distrust game than in Baseline, but in contrast to us (and in contrast to their main trust hypothesis), they find higher trust. The rest of the paper is organized as follows. In Sect. 2 we present the experimental design and procedure, while in Sect. 3 we present behavioral predictions. In Sect. 4 we present the experimental results, while Sect. 5 concludes.",3
20.0,4.0,Experimental Economics,07 February 2017,https://link.springer.com/article/10.1007/s10683-017-9515-6,The effect of acute pain on risky and intertemporal choice,December 2017,Lina Koppel,David Andersson,Gustav Tinghög,Female,Male,Male,Mix,,
20.0,4.0,Experimental Economics,09 February 2017,https://link.springer.com/article/10.1007/s10683-017-9516-5,Redistributive choices and increasing income inequality: experimental evidence for income as a signal of deservingness,December 2017,Laura K. Gee,Marco Migueis,Sahar Parsa,Female,Male,Female,Mix,,
20.0,4.0,Experimental Economics,21 February 2017,https://link.springer.com/article/10.1007/s10683-017-9518-3,The emergence of language differences in artificial codes,December 2017,Fuhai Hong,Xiaojian Zhao,,Unknown,Unknown,Unknown,Unknown,,
20.0,4.0,Experimental Economics,02 March 2017,https://link.springer.com/article/10.1007/s10683-017-9519-2,Asymmetric and endogenous within-group communication in competitive coordination games,December 2017,Timothy N. Cason,Roman M. Sheremeta,Jingjing Zhang,Male,Male,Unknown,Male,"Cheap talk can facilitate coordination on the efficient equilibrium in experimental games with Pareto-ranked equilibria (Cooper et al. 1992; Charness 2000; Charness and Grosskopf 2004; Duffy and Feltovich 2002, 2006; Brandts and Cooper 2007). For example, Van Huyck et al. (1993) demonstrate that pre-play communication is efficiency-enhancing in coordination games. Blume and Ortmann (2007) find that costless nonbinding messages, even when they have minimal information content, can facilitate quick convergence to the Pareto-efficient equilibrium. Since many economic interactions can be modeled as coordination games, this finding may have a very important general implication: improving communication in coordination games can increase efficiency and social welfare. However, this broad conclusion can be misleading. Indeed, Cason et al. (2012) show that allowing within-group communication in competitive coordination games, such as rent-seeking contests, may lead to more aggressive competition between groups. Therefore, the introduction of within-group communication in such environments may actually cause inefficiency and decrease social welfare. This study further explores potentially harmful effects of within-group communication in competitive coordination games, by addressing two questions. The first question concerns with the effects of asymmetric communication: If only one of the two competing groups can communicate, does such asymmetric communication harm efficiency by increasing competition between groups? The second question concerns with the endogenous emergence of communication: Given that communication may potentially harm efficiency, do groups still choose to establish the “harmful” communication channel? To answer these questions, we re-analyze some existing data from and add two new treatments to the Cason et al. (2012) experiment that employs a weakest-link contest between two groups. The weakest-link contest combines features of a cooperative weakest-link game (Van Huyck et al. 1990) and a competitive rent-seeking contest (Tullock 1980). One key characteristic of this type of contest is that coordination on higher efforts increases the probability of winning the prize, thus receiving potentially higher payoffs. Efforts are aggregated within each group with a weakest-link production technology, so the effective group effort equals the lowest effort expended by an individual in the group. The weakest-link feature of this contest resembles many real life competitions where the performance of the entire group depends on the worst performer within a group (Hirshleifer 1983). For example, in many teamwork competitions each member of the team is responsible for a specific task. If any of the members performs his/her task poorly then the team loses the competition. Certain R&D competitions have such characteristics. Also, in terrorist attacks and in some military battles, the attacker’s objective is often to successfully attack one target, rather than a subset of targets (Clark and Konrad 2007; Deck and Sheremeta 2012). In a group contest coordination on higher efforts increases the probability of winning the prize but decreases the competitor’s payoff. Therefore, higher efforts may lead to lower efficiency due to the negative externality imposed on the competing group. This unique feature of the group contest has been used by researchers to examine questions about punishment and retaliation (Abbink et al. 2010), rent-seeking (Ahn et al. 2011), group structure (Sheremeta 2011), and leadership (Eisenkopf 2014).Footnote 1 Previous studies have shown that when there is no within-group communication, group members are able to achieve a substantial level of coordination within each group (Sutter and Strassmair 2009). Allowing within-group communication leads to even better coordination, but as a result of more aggressive competition it also leads to lower efficiency (Cason et al. 2012; Brookins et al. 2015).Footnote 2
 Our experiment employs a weakest-link contest to further explore the potentially harmful effects of within-group communication in competitive coordination games. The weakest-link feature gives contestants the ability to lower unilaterally their own group’s effort, thereby decreasing excessive effort expenditures and improving efficiency. Regarding the first question of asymmetric communication, we find that when only one group can communicate, the communicating group coordinates better and expends higher efforts than the non-communicating group. As a result, the communicating group earns payoffs similar to the baseline contest without any communication while the non-communicating group earns lower payoff. Allowing within-group communication in both groups leads to even more aggressive competition and the lowest average payoffs in both groups. We use content analysis to analyze why communication is harmful and find that subjects often send messages expressing their desire to compete and win (significantly more so than messages about cooperation). Moreover, such messages are positively and significantly correlated with effort expenditures in the contest, which could partially explain overly aggressive competition in the presence of communication. Regarding the second question of endogenous communication, we find that groups routinely choose to establish communication channels. As in the exogenous case, endogenously selected communication enhances coordination, but it also leads to more aggressive competition and lower efficiency. Choosing to communicate or not resembles a Prisoner’s Dilemma game. By jointly choosing to restrict within-group communication both groups can earn higher payoffs, but incentives are such that choosing to communicate is a weakly dominant strategy. Even though communication is only a weakly dominant strategy, almost all groups choose to communicate. Such strong adoption of communication is unlikely due to only strategic reasons. Therefore, we provide several other explanations for this result, such as natural preferences for communication, non-monetary incentives, social preferences and social group identity. We present the theoretical model and derive the predictions in Sect. 2. Section 3 describes the experimental design and procedures, while Sect. 4 presents the results. Finally, we discuss implications of our results in Sect. 5.",30
20.0,4.0,Experimental Economics,04 March 2017,https://link.springer.com/article/10.1007/s10683-017-9520-9,An experiment on auctions with endogenous budget constraints,December 2017,Lawrence M. Ausubel,Justin E. Burkett,Emel Filiz-Ozbay,Male,Male,Female,Mix,,
21.0,1.0,Experimental Economics,16 March 2017,https://link.springer.com/article/10.1007/s10683-017-9522-7,Why did he do that? Using counterfactuals to study the effect of intentions in extensive form games,March 2018,Yola Engler,Rudolf Kerschbamer,Lionel Page,Unknown,Male,Male,Male,"Other-regarding preferences capture people’s valuation not only for their own material resources but also for the material payoffs of other individuals as well as the perceived kindness of others’ behavior. The theoretical literature on such preferences can be divided into two broad classes: models with distributional (unconditional) other-regarding preferences and models with intention- or action-based (conditional) other-regarding preferences. The distributional (or “social”) preference approach focuses on preferences over allocations of resources which are driven by distributional properties of the allocations. The altruism models by Andreoni and Miller (2002) and by Cox and Sadiraj (2007) fall into this category, as well as the models of inequality-aversion by Fehr and Schmidt (1999) and Bolton and Ockenfels (2000), and the model of altruism and spite by Levine (1998).Footnote 1
 The conditional other-regarding preference approach, on the other hand, tries to explain findings neither consistent with self-regarding preferences nor in line with existing models of distributional concerns by agents’ desire to react to others’ intentions or actions. In this strand, a second mover’s preferences in a two-person two-stage game typically become more or less benevolent depending on the perceived “kindness” of the first mover, and kindness is typically interpreted as generosity. Two approaches have been proposed to investigate conditional other-regarding preferences theoretically. First, in psychological game theory, a player evaluates another person’s kindness by forming beliefs on what the other person believes the consequences of his choice are (see Rabin 1993; Dufwenberg and Kirchsteiger 2004, for instance). This necessarily involves second-order beliefs entering the picture. Models incorporating second-order beliefs provide quite sophisticated theories of conditional other-regarding preferences. Unfortunately, they often yield multiple equilibria even in quite simple games and finding these is often not trivial. Also, empirical tests of models in this class must deal with the important and non-trivial question on how to elicit or induce second-order beliefs in a clean way. To avoid these problems, a second approach of modeling conditional other-regarding preferences—the “revealed intentions” approach—has been proposed by Cox et al. (2007, 2008b). In this approach a second mover’s benevolence in a two-player two-stage game is a function of the relative kindness or unkindness of the first mover as revealed by the objective characteristics of his (observed) choices. The first mover’s kindness, in turn, is determined by the relative generosity of the opportunity set implied by his choice relative to alternative opportunity sets he could have chosen instead. The present paper contributes to the revealed intentions approach of conditional other-regarding preferences by exposing subjects in the lab to a large number of two-player two-stage games and by studying how second movers react to the opportunities of gains and losses for each player generated by the choice of the first mover. Specifically, we expose subjects in the lab to graphical representations of two-player two-stage games in which (1) the first mover has to choose between two budget sets, one containing a single allocation, the other containing several possible payoff allocations; and (2) the second mover has to choose one of the available payoff allocations in the non-trivial budget set—provided the first mover has chosen it. By systematically varying the two budget sets available to the first mover, we investigate how opportunities of gains and losses for each player influence the second mover’s benevolence towards the first mover. We find that the possibility of gains for the second mover (generosity) and the risk of losses for the first mover (vulnerability) are important drivers for second mover behavior. On the other hand, efficiency concerns and an aversion against violating trust seem to be far less important motivators. We also find that second movers compare the actual choice of the first mover and the alternative choices that would have been available to him to allocations that involve equal material payoffs. Compared to the existing literature on conditional other-regarding preferences the present paper makes three critical contributions: The first contribution is the introduction and implementation of an experimental design in which subjects are exposed to geometric representations of choice sets. This allows for the collection of a large number of observations per subject which facilitates statistical analysis at the level of the individual decision maker. Regarding this contribution the paper closest to ours is Fisman et al. (2007). Those authors are interested in unconditional other-regarding concerns. As a consequence, in their experiments there is only one player role—that of a dictator—and each dictator is exposed to 50 different decision problems, each graphically represented as a linear budget set from which the subject can choose.Footnote 2 Since our main research focus is on conditional other-regarding preferences we extend this approach by having two player roles—the role of a first mover and the role of a second mover; the first mover chooses among graphical representations of opportunity sets while the second mover makes a dictator decision within a given opportunity set similar to the one subjects are asked to make in Fisman et al. (2007). By varying the set of budget sets available to the first mover we are able to investigate how the second mover’s choice varies with the budget set actually chosen by the first mover and with the counterfactual alternative opportunity set the first mover could have chosen instead. Our second innovation is the experimental investigation of the relative importance of different motives for behavior of players in extensive-form games. In this respect the papers closest to ours are Cox (2004) and Cox et al. (2007, 2008b, 2016). While Cox (2004) employs a triadic experimental design to disentangle the relative importance of conditional and unconditional other-regarding preferences for behavior of second movers in the investment game, the present paper’s main aim is to disentangle the relative importance of different basic motives for the conditional part of players’ other-regarding preferences. Similar to Cox et al. (2007, 2008b) we suppose that the second mover in a two-player two-stage game cares about how the opportunity set chosen by the first mover compares to alternative opportunity sets the first mover could have chosen instead. However, while these papers compare opportunity sets in terms of generosity by the first mover towards the second mover and focus on reciprocity as possible motivation for the second mover, we look not only at the possible gains for both players but also at possible losses and look at a broader array of possible motivations. In this latter respect our paper is similar to Cox et al. (2016). However, in contrast to that work we look not only on trust game constellations and we also collect many observations per individual.Footnote 3 The latter feature of our experimental design allows us to estimate utility functions at the individual level in a within-subjects design while Cox et al. (2016) derive their results from comparisons of aggregate data across treatments in a between-subjects design. Our third innovation is the introduction of a silent social norm—the equal-split norm—into the revealed intentions approach. In this respect our paper is related to previous work on the importance of the equality norm for economic behavior—see Fehr and Schmidt (1999), Bolton and Ockenfels (2000) and Andreoni and Bernheim (2009), for instance. While Fehr and Schmidt (1999) and Bolton and Ockenfels (2000) stress the importance of the equal-split norm for unconditional other-regarding preferences, we show that this norm is also crucial for our understanding of conditional other-regarding preferences. Conditional other-regarding preferences might also be relevant for behavior in the experiments reported by Andreoni and Bernheim (2009). However, while Andreoni and Bernheim are interested in the impact of “audience effects” on behavior, we are interested in situations where audience effects are unlikely to play a role. The remainder of the paper is organized as follows: Sect. 2 presents our experimental design. It is followed by our conceptual framework in Sect. 3, which consists of a classification of choice characteristics, our model of social preferences, and predictions derived from the model. In Sect. 4, we report our data and estimate the parameters of our model. Section 5 discusses our findings and concludes.",3
21.0,1.0,Experimental Economics,16 March 2017,https://link.springer.com/article/10.1007/s10683-017-9523-6,Third-party manipulation of conflict: an experiment,March 2018,Piotr Evdokimov,Umberto Garfagnini,,Male,Male,Unknown,Male,"Many conflicts in human history involved acts of third-party provocation. In the midst of World War I, Germany sent what is now known as the “Zimmermann Telegram” to Mexico. This telegram proposed “an understanding on [Germany’s] part that Mexico is to reconquer the lost territory in Texas, New Mexico, and Arizona,” and its purpose was to provoke Mexico into conflict with the U.S., thereby delaying the U.S. from going to war with Germany in Europe. The authenticity of the intercepted telegram was publicly confirmed by the German Foreign Secretary, which made the provocation public, if ultimately unsuccessful at engaging Mexico in the war effort. Other examples of third-party provocation include the promotion of the Tutsi minority to positions of power over the Hutu by the Germans, and later the Belgians, in colonial Rwanda,Footnote 1 and the instigation of conflict between the Muslims and the Hindu by the British in India.Footnote 2
 History is also rife with examples of third-party peacemaking. The promotion of universal peace through nonviolent means such as communication was a central principle of Tolstoyism. The same principle inspired the movements led by Mahatma Gandhi, who communicated with Tolstoy, and Martin Luther King, Jr.Footnote 3 The anti-nuclear movement, which gained prominence following the 1945 bombings of Hiroshima and Nagasaki, and protests against the Vietnam War provide other prominent examples of nonviolent and successful calls for peace. The present paper uses a controlled experiment to study whether and how conflict between two rival players can be manipulated by an interested third party through public communication. Can a third-party provocateur increase the likelihood of conflict by making a strategic provocation? Can a third-party peacemaker reduce its likelihood by making public calls for peace? Although manipulation of conflict can have far-reaching economic and political consequences, surprisingly little empirical work has investigated it in economics. Our first and primary research question is the following: 
Can an interested third party manipulate the likelihood of conflict through public announcements?
 In theory, communication by a third party can be effective even if this third party cannot influence the payoffs of the conflicting parties directly. When the third party has private information about one of the conflicting players’ incentives and the conflicting players’ actions are strategic complements, as they are in many conflict situations, strategic communication can provoke one of the players into being hawkish, which in turn triggers an hawkish response from the player’s opponent (e.g., Baliga and Sjöström 2012). We call this the strategic communication hypothesis. If the conflicting parties care about the payoffs of the third party, its mere presence could make their behavior more or less hawkish independent of message content (e.g., Bland and Nikiforakis 2015). We refer to this as the third party social preferences hypothesis.Footnote 4 Uninformative public announcements could also be influential simply through their suggestive power (e.g., Schelling 1980; Charness 2000). This is the focal point hypothesis. The three possibilities outlined above motivate our second research question: 
What channels underlie the effects of provocation and peacemaking on the likelihood of conflict?
 The baseline condition in our experiment is a \(2 \times 2\) conflict game in which each player has private information about his cost of being hawkish. In the first treatment, we introduce a third party peacemaker that is commonly known to strictly prefer all players to be dovish. In the second treatment, we introduce a third party who is commonly known to prefer Player 1 to be hawkish and Player 2 to be dovish. That is, the third party is a provocateur who benefits from conflict. In both treatments, the third party has private information about one of the players and is allowed to make public cheap talk announcements before players choose their actions. In the absence of focal point effects and social preferences toward the third party, the peacemaker’s messages have no effect on behavior and his presence does not affect the likelihood of conflict in equilibrium. The game with the provocateur, in contrast, admits a unique informative communication equilibrium in which strategic communication leads to a higher likelihood of conflict. We find that public cheap talk announcements by an interested third party have a statistically significant effect on behavior in both of our treatments. I.e., we answer Question 1 in the affirmative. To study the underlying channels (Question 2), we explore the message senders’ communication strategies as well as the message receivers’ responses to the messages. Irrespective of the message sender’s identity, we find that communication in the experiment does not convey private information. This allows us to rule out the strategic communication hypothesis. All message-sending strategies, including those we identify in the data, are consistent with uninformative equilibrium behavior on the part of the message-senders. While uninformative messages are ignored in equilibrium, we find receivers to be more hawkish following hawkish messages and more dovish following dovish messages. While these findings are inconsistent with equilibrium behavior on the part of message receivers, they are consistent with the focal point hypothesis. In particular, we suggest that third party messages focus the attention of the conflicting parties on specific courses of action. To the extent that a player is neither a dominant hawk nor a dominant dove, he is a coordination type that plays a hawkish (resp., dovish) action when the probability of his opponent playing a hawkish (resp., dovish) action is sufficiently high. When it is likely that both players are coordination types, as it is in our experiment, the messages could induce a coordinated response. To investigate the effect of social preferences toward the third party, we study how the message receivers are affected by the identity of the message sender. Conditional on a dovish message being sent, we find that the receivers are no less likely to be hawkish when the message is sent by a peacemaker than when it is sent by a provocateur. This provides evidence against the third party social preferences hypothesis.Footnote 5
 Our results shed light on the channels through which manipulation of conflict can occur and highlight several behavioral regularities. In particular, they complement the theoretical work of Baliga and Sjöström (2012) by showing that third-party cheap talk communication can be influential even when it conveys no private information. The rest of our paper is structured as follows. Section 2 reviews the relevant theoretical and experimental literature. In Sect. 3, we discuss our experimental design. In Sect. 4, we present our results. Section 5 discusses the results and possible directions for future work.",2
21.0,1.0,Experimental Economics,29 March 2017,https://link.springer.com/article/10.1007/s10683-017-9524-5,What if women earned more than their spouses? An experimental investigation of work-division in couples,March 2018,François Cochard,Hélène Couprie,Astrid Hopfensitz,Male,Female,Female,Mix,,
21.0,1.0,Experimental Economics,13 May 2017,https://link.springer.com/article/10.1007/s10683-017-9526-3,Self-confidence and strategic behavior,March 2018,Gary Charness,Aldo Rustichini,Jeroen van de Ven,Male,Male,Male,Male,"Belief about one’s abilities is an important ingredient in many decisions, including making career choices, undertaking enterprises, and taking risks. There is considerable evidence that statements people make about their abilities often don’t accurately reflect their real abilities. Well-known studies in psychology and economics claim that people are overconfident in their ability (e.g., Svenson 1981; Dunning et al. 1989).Footnote 1 However, the roots of such apparent overconfidence in relative ability and the corresponding benefits that might explain the persistence of the phenomenon are not yet clear. One personal benefit is the consumption value of the belief that one is talented (or “ego utility,” Koszegi 2006). In this view, people feel better with a favorable self-perception, even at the cost of being overconfident and thus making wrong choices. People also derive benefit from overconfidence in relation to social image concerns, as they desire that others see them as being skilled. Recent studies support this view (Burks et al. 2013; Ewers and Zimmerman 2015). We consider here an additional explanation, postulating that overconfidence may also have a strategic foundation. Statements about one’s beliefs are often made to affect the decisions of others. This occurs in strategic situations, which are common in social life. For instance, appearing more confident is likely to increase one’s chances to be hired or to receive a promotion, and may discourage others from competing for that same position or promotion. It may elicit cooperation by others if they seek talented colleagues to start a joint project. However, in other situations it may pay to appear less skillful than one actually is, as in the case of a pool hustler. In the workplace, this strategy can be employed to elicit help by others. We emphasize that the notion that people may inflate expressed overconfidence for strategic purposes does not exclude other motivations also playing a role, and the different motivations may be complementary to each other. One main contribution of our paper is to provide a strategic foundation for expressed overconfidence (and under-confidence) by demonstrating that participants inflate (or deflate) their stated confidence when it is strategically beneficial for them to do so. We present experimental data showing that people are responsive to confidence statements by others in strategic decisions, and that these confidence statements are adjusted in ways that are consistent with strategic considerations. We also propose a model that can explain these results. In our experiment, we elicit confidence in one’s relative ability on a cognitive task. This is followed by a tournament stage, where scores on that task are compared. In the baseline treatment, all subjects automatically enter the tournament and there are no strategic reasons to distort the reported confidence relative to their true beliefs. In other treatments, we divide each pair of subjects into a sender and a receiver. In those treatments, receivers can opt out of the tournament after observing the sender’s reported confidence, giving senders incentives to distort their reported confidence. We implemented two such strategic treatments. In Deter, the sender prefers that the receiver does not enter the tournament. We show that in equilibrium, senders over-report to discourage receivers from entering the tournament. In Lure, the sender always prefers that the receiver enter the tournament, and senders under-report in equilibrium. Our main findings are as follows. In the baseline treatment, in which there are no strategic considerations to distort reported confidence, the mean reported confidence about being in the top 50% is well above 50 (63.4), consistent with overconfidence, or over-placement (Moore and Healy 2008).Footnote 2 In both strategic treatments, receivers are very responsive to the reported confidence of the senders. Senders exploit this fact. In Deter, we find evidence that male (although not female) senders inflate reported confidence. In Lure, we find evidence of under-reporting by both male and female senders, again consistent with the equilibrium of the game. Surprisingly, in Deter we also find an increase in reported confidence by male receivers, even though receivers have no possible strategic advantage from over-reporting; senders automatically enter into the tournament and are not even informed of the receiver’s confidence statement. By contrast, receivers in the Lure treatment do not adjust their reported confidence in comparison to the baseline treatment. Finally, we find that females are less likely than males to enter the tournament in Deter, despite very similar performance levels. This effect is driven by stated confidence levels in Deter, as there is no significant difference in entry rates when we control for these. In Lure, both the stated confidence levels and entry rates for males and females were almost the same (slightly higher for females). While overconfidence may have large and negative economic consequences on individuals, we find that individuals adjust their reported confidence for strategic reasons to adapt to the situation they are facing, perhaps mitigating the negative impact of overconfidence. Being (or at least appearing) overconfident (or under-confident) can have benefits in strategic interactions, and individuals understand this to a fair degree. That said, our participants do not always adjust their confidence optimally, and the degree to which they adjust their reported confidence is somewhat gender specific. The paper is structured as follows. In Sect. 2, we provide a literature review. We describe our hypotheses and our experimental design in Sect. 3. We present our results in Sect. 4, and we discuss the motivation of biased confidence in Sect. 5. We conclude in Sect. 6.",51
21.0,1.0,Experimental Economics,09 May 2017,https://link.springer.com/article/10.1007/s10683-017-9527-2,Conducting interactive experiments online,March 2018,Antonio A. Arechar,Simon Gächter,Lucas Molleman,Male,Male,Male,Male,"Online labor markets such as Amazon Mechanical Turk (MTurk) are increasingly popular tools for behavioral scientists. With their large and diverse pools of people ready to promptly perform tasks for pay, these markets present researchers with new opportunities to recruit participants for experiments.Footnote 1 Studies from across the social sciences have systematically compared data collected online with data from the physical laboratory. Their conclusions are promising: classic results from psychology and economics have been replicated using online samples, and the data obtained online is deemed as reliable as that obtained via traditional methods.Footnote 2
 Despite its great potential, behavioral research online has so far remained largely limited to non-interactive decision-making tasks or one-shot games with simultaneous decisions. Current online studies of social behavior often use survey software such as Qualtrics or SurveyMonkey to document decision making in tasks that participants complete individually, and emulate interactions through post hoc matching. Although this approach can be powerful, it does not permit the study of repeated, ‘hot’ interactions where live feedback between participants is essential. Experimental designs with live interaction are rarely implemented online, partly because there is not yet a widely-used web-based equivalent of z-Tree (Fischbacher 2007).Footnote 3
 In this paper, we assess the potential for interactive online experiments, where a set of participants interacts for more than one repetition. Interactive experiments raise novel challenges throughout the whole life cycle of an experiment. Our approach is to discuss these challenges, that is, methodological differences and similarities between interactive experiments in physical and online laboratories. We discuss these step-by-step, from recruitment to dismissal of participants after the experiment. A particularly important challenge of interactive online experiments relates to participant dropout. While in the physical laboratory participants rarely leave a session, online experiments are more prone to dropouts which affect both the participant who is dropping out and their interaction partners (who still have to be paid for their participation). If dropouts happen for reasons exogenous to the experiment—e.g. due to network problems, frozen screens, or random distractions—they are just a (costly) nuisance to the experimentalist. Much more problematic are dropouts that happen endogenously, that is, people quitting because of what has happened in the experiment. Such dropouts could jeopardize the internal validity of experiments (Zhou and Fishbach 2016). As a case study we replicate a repeated public goods game with and without peer punishment used in cross-cultural research (Herrmann et al. 2008), employing a sample of US participants recruited via MTurk.Footnote 4 We chose to replicate this experiment because it is fairly long and logistically complex. It is a within-subjects design with two experimental conditions of ten periods each, where, after the first set of ten periods, participants receive new instructions. Moreover, this experiment has often been replicated, and its design allows us to evaluate whether dropouts depend on the experimental conditions (that is, the presence or absence of punishment). We report data from participants recruited via MTurk (62 groups) and participants from the physical laboratory (18 groups). We used our own software LIONESS (Sect. 2.5), developed for conducting interactive online experiments. We observe that basic patterns of behavior online are similar to those in the laboratory. In the absence of punishment, aggregate levels of cooperation are higher on MTurk than in the laboratory, but show similar rates of decay over time. Moreover, our econometric analysis reveals that in both of our samples the group contributions strongly determine the level of cooperation. The introduction of punishment promotes the emergence and maintenance of cooperation in both samples. Punishment is mainly prosocial in nature in both samples (cooperators punish non-cooperators) but occurs less frequently online. Our most important result is that, in our implementation, dropouts are most likely due to reasons that are exogenous to the experiment. Together with the replication of findings from the laboratory, our results suggest that online interactive experiments can be a reliable tool for collecting internally-valid data and hence are a potentially valuable complement to the physical laboratory. Our paper contributes to a recently-emerged literature on the reliability of data gathered on online labor markets such as MTurk (see references in footnotes 1–3). The most important predecessor of our paper is Anderhub et al. (2001), who compared online and laboratory experiments in the very early days of experimentation on the Internet. They also provide a methodological discussion that, however, could not consider the specific properties of modern online labor markets where the bulk of present-day online experimentation is happening.Footnote 5
 The remainder of this paper is structured as follows. In Sect. 2, we introduce the experimental design. In Sect. 3, we discuss the conceptual and logistical differences between conducting interactive experiments in the laboratory and online and lay out our approach for dealing with them, highlighting important aspects of the data-collection process relating to attention and attrition. Section 4 shows the results of our experiment, systematically comparing cooperation and punishment behavior in our two samples. In Sect. 5 we present a detailed analysis of attrition in our online experiment. Finally, in Sect. 6 we make concluding remarks.",181
21.0,1.0,Experimental Economics,29 May 2017,https://link.springer.com/article/10.1007/s10683-017-9529-0,Cognitive bubbles,March 2018,Ciril Bosch-Rosa,Thomas Meissner,Antoni Bosch-Domènech,Male,Male,Male,Male,"In 1988, Vernon Smith, Gerry Suchanek and Arlington Williams (SSW) (Smith et al. 1988) published a seminal paper reporting the results of experiments on the efficiency of asset markets. In their experiment, subjects are first endowed with assets and experimental currency, and then are allowed to trade assets for currency in a multi-period double auction market. At the end of each period, assets pay a stochastic dividend whose distribution is common knowledge. At the end of the experiment, assets have no buyback value and subjects are paid in cash according to the amount of experimental currency they have accumulated. The asset’s fundamental value (FV) at any period can be calculated as the number of periods left times the expected dividend per period. The advantage of such experimental asset markets is that, contrary to real world financial markets, the asset’s fundamental value is known to all participants of the market and also to any observer attempting to assess the efficiency of these markets. SSW observed large positive price deviations from the FV (also called bubbles) followed by dramatic crashes towards the end of the experiment. To the surprise of most, these bubbles turned out to be extremely resilient to replications under different treatments.Footnote 1
,
Footnote 2
 Thus, the results became canonical to the extent that seldom a paper in the economic experimental literature has spawned such a large industry of replications and follow-ups. Stefan Palan in a recent survey (Palan 2013) documents the main findings based on the results from 41 published papers, 3 book chapters and 20 working papers. Palan concludes with an optimistic appraisal: “Hundreds of SSW markets have been run, yielding valuable insights into the behavior of economic actors and the factors governing bubbles”(p. 570). We are not so sure about that. We show below that the bubbles and crashes observed in experimental asset markets disappear when the participants have a sufficient level of cognitive sophistication. This would suggest that bubbles and crashes are not intrinsic to SSW experimental asset markets, but contingent on the cognitive profile of the experimental subjects. The idea that the observed bubbles and crashes in the SSW-type experiments may be due to some lack of understanding by the participants of the experiments is not entirely new. Huber and Kirchler (2012) and Kirchler et al. (2012) have managed to reduce bubbles in their experiments by either offering a more thorough rendering of the market or describing the asset as a “stock from a depletable gold mine”. According to them, an easier understanding of the market diminishes the bubbles. However, this interpretation has been challenged. Baghestanian and Walker (2015) argue that particular features of the experimental design by Kirchler et al. (2012) generate asset prices equal to the fundamental value through increased focalism or anchoring, and not because agents are less confused. More recently, while studying the effects of gender composition in SSW markets, Cueva and Rustichini (2015) report that subjects with higher cognitive ability both earn higher profits and trade at prices closer to fundamental. Also related is Hanaki et al. (2015) which shows that mispricing is larger when subjects are aware of the heterogeneity of cognitive ability among participants.Footnote 3
 In this paper, we test whether the occurrence of bubbles in SSW-type experiments depends on the subjects’ cognitive sophistication. Building on previous evidence relating some degree of misunderstanding with the appearance of asset price bubbles, it is not unreasonable to expect markets populated only by high sophistication subjects to generate fewer bubbles compared to markets populated by less sophisticated ones. To test this hypothesis we design a two-part experiment: In the first part we invite subjects to participate in a battery of tasks that allow us to approximate their “cognitive sophistication”. In part two, which is scheduled for a later date, we invite subjects who score low (high) in our tasks of cognitive sophistication to participate in an asset market experiment populated only by low (high) sophistication subjects. The results of the experiment verify our expectations. Bubbles and crashes persist when the experimental subjects are selected because of their lower cognitive scores. Interestingly, bubbles vanish completely when we run the experiment with the more sophisticated subjects.",53
21.0,1.0,Experimental Economics,21 June 2017,https://link.springer.com/article/10.1007/s10683-017-9530-7,On the provision of incentives in finance experiments,March 2018,Daniel Kleinlercher,Thomas Stöckl,,Male,Male,Unknown,Male,"The provision of monetary incentives is an important procedural pillar in experimental economics and crucial to the interpretation of experimental results (Smith 1976; Wilde 1980; Smith 1982). Studies applying experimental methods to issues in finance use a wide variety of incentive schemes with distinct characteristics. For the experimental profession, this variety is worrying, indicating a potential loss of control over subjects’ behavior in the experiment and thus a potential factor influencing reported results, their interpretation, and external validity. The issue of appropriately designed incentives, therefore, is of utmost importance to the profession. In this paper, we tackle this issue by investigating two research questions related to the provision of incentives in finance experiments. Our goal is to discuss the use of incentive schemes in experimental finance and to provide guidance for experimental design choices.Footnote 1
 In research question 1 (RQ 1) we focus on one specific aspect of incentive schemes, namely their level of salience. An incentive scheme is salient if individuals’ payouts depend on their actions, actions by other agents and the characteristics of the institution in an understandable fashion (Friedman and Sunder 1994). Most schemes implemented in the literature certainly fulfil these criteria. Our focus, however, lies on the level of salience subjects face within the experiment. The level of salience defines the sensitivity of subjects’ payouts to their actions in the experiment. According to this understanding of the term, subjects’ actions in an experiment would, ceteris paribus, result in more (less) extreme payout realizations the higher (lower) the incentive scheme’s level of salience. Based on these considerations we formulate RQ 1: 
RQ 1. How do salience manipulations in the incentive schemes affect results in finance experiments? We tackle RQ 1 by analyzing how four distinct monetary incentive schemes differing in their level of salience affect results in three finance experiments. We find a modest relationship between the level of salience and mispricing in an experiment resembling the (Smith et al. 1988)-asset market design. More specifically, markets populated by subjects experiencing low levels of salience exhibit higher levels of mispricing. This relationship, however, does not translate to other market characteristics. In an experiment testing a market’s ability to aggregate diverse information (Plott and Sunder 1988) we find that markets with non-salient incentive schemes show a marginal tendency toward higher spreads and volatility, but we cannot detect any other effects across incentive schemes. In an investment experiment we find that, facing a non-salient incentive scheme, subjects invest a significantly higher proportion into the risky asset. In research question 2 (RQ 2), we focus on another important aspect: subjects’ perception of the incentive scheme. Studying this aspect is important because incentives exert an influence on the subject pool the experimental economist recruits from. Usually, experimental economists try to avoid any circumstances likely to cause severe damage to this pool. Among other causes, one such cause is subjects being frustrated about their payout. Subjects may retain bad memories from a session, which is likely to negatively affect their likelihood to return for future experiments. We formulate RQ 2: 
RQ 2. How do subjects experience their incentive scheme? Within RQ 2 we elicit different aspects of how subjects perceive the incentive schemes. First, we analyze how subjects perceive the different incentive schemes with respect to salience and fairness. Our motivation is that non- or insufficiently-salient as well as unfair monetary compensation schemes could have an influence on subjects’ behavior during the experiment. Second, we focus on the elicitation of subjects’ basic motives for participation in experiments. Here, we test whether the monetary component is a dominant factor, but also whether other motives play an important role and whether motives change with subjects’ experience. Third, we analyze subjects’ understanding of the incentives schemes applied. It is obvious that only subjects who have a sound understanding of how the underlying incentive scheme influences their payout will adjust their behavior accordingly and will provide robust and reliable results. Fourth, we investigate how subjects’ motivation levels interact with the incentive schemes applied. Both in the short and in the long run the experimenter wants to avoid demotivated subjects as they have the potential to negatively affect current research projects as well as severely damage the subject pool. To the best of our knowledge, we are the first to investigate these issues. Our findings will help the experimenter avoid implementing incentive schemes which induce negative sentiments among subjects. We find that the majority of subjects (52.7%) participate primarily because of monetary rewards. However, for subjects participating for the first time the monetary reward motive is no more important than other motives like joy from the task or increasing their knowledge. With increasing experience though, the monetary reward motive increases significantly. Only a minor share of subjects experiences the implemented incentive schemes as unfair (14.0% across schemes) while the majority considers them as fair or are neutral in their evaluation. In the gain domain subjects favor more salient over less- or non-salient schemes while they negatively evaluate high salience in the loss domain. Non-salient incentive schemes lower subjects’ motivation but do not influence experimental outcomes to any larger degree. More complex incentive schemes lead to higher levels of motivation, but are more difficult to understand for subjects. Moreover, the number of previous instances of participation in an economic experiment positively contributes to subjects’ understanding of the incentive scheme but has no influence on subjects’ motivation within the experiment.",7
21.0,1.0,Experimental Economics,26 July 2017,https://link.springer.com/article/10.1007/s10683-017-9535-2,On the performance of rule-based contribution schemes under endowment heterogeneity,March 2018,Martin Kesternich,Andreas Lange,Bodo Sturm,Male,Male,Male,Male,"Many problems in public economics require appropriate institutional mechanisms to achieve a burden sharing that is perceived to be “fair” by the relevant stakeholders. Important examples span from current debates on the taxation of labor and inheritances to discussions on the progressivity of taxation (for providing public goods). A common element to these debates are controversies about what constitutes fair burden sharing schemes given agents’ heterogeneous income positions as well as the procedures through which these positions were determined. A normative basis for such procedural concerns is given by the accountability principle which requires “that a person’s entitlement or fair allocation (e.g., of income) varies in proportion to the relevant variables which he can influence (e.g., work effort), but not according to those which he cannot reasonably influence (e.g., a physical handicap)” (Konow 1996, p. 14). Moderated through this concept, the acceptance of policy measures and the “tolerance of inequality” may therefore not only depend on the degree of (income) heterogeneity, but also on whether a person perceives its current individual position within a society being determined mainly by its own decisions and achievements or rather as determined by chance. As much of public policy is geared towards the provision of public goods, it is particularly important to better understand how individual views on redistribution are shaped and how they affect the willingness to contribute to public goods within given burden sharing schemes, i.e. the effectiveness of the respective burden sharing institution. In this paper, we experimentally investigate the impact of procedural fairness concerns on (voluntary) public goods provision. For this, we compare a random (“windfall”) allocation of initial endowments to an allocation of earnings based on individual efforts. We explore the impact of the procedures that cause endowment heterogeneity on the provision of public goods within two different institutional settings: first, we consider a voluntary contribution mechanism (VCM) as a baseline. Second, we investigate the extent to which heterogeneous agents support the provision of public goods under different burden sharing rules. For this, we consider a “rule based contribution mechanism” that first allows agents to make proposals for the level of public goods provision, before the minimum of the proposals is implemented and individual contributions are enforced under the respective predetermined burden sharing rule (e.g., Kesternich et al. 2014; Orzen 2008). We consider two variants of these minimum contribution schemes being inspired by different notions of distributional fairness: equality in contributions versus equality in payoffs. While the former may serve as a simple focal point and disregards existing heterogeneities, the latter leads to a more differentiated burden regime but may be attractive if individuals were inequality-averse (e.g., Bolton and Ockenfels 2000; Charness and Rabin 2002; Fehr and Schmidt 1999). To guide our investigation, we formulate a theoretical model that combines inequality and efficiency concerns with considerations of procedural fairness that depend on the way initial endowments are allocated: randomly or based on effort. For the VCM, we predict average contributions to be lower under effort than under random allocation as high-type players are expected to reduce their contributions as they become less averse to advantageous inequality. Similar considerations hold for the two rule based-contribution schemes. In contrast to a random allocation of endowments, high-type agents can typically be expected to reduce their minimum proposals because they may feel to deserve a higher income relative to the mean income. In an equal-payoff scheme they may reduce their proposals for the group provision level if they feel entitled to remain with higher earnings than poor players. Nonetheless, we predict rule-based contribution schemes to increase average contributions and payoff levels relative to the VCM. Our experimental results indicate that the efficiency gains from rule-based contribution mechanisms relative to VCM are limited under a random allocation of endowments. In contrast to our theoretical predictions, neither an equal-payoff nor an equal-contribution scheme lead to significantly larger provision levels of the public good than pure voluntary contributions, while they clearly affect the distribution. This finding stands in stark contrast to previous findings for agents with identical endowments but heterogeneous benefits (Kesternich et al. 2014): for homogeneous endowments, the equal-payoff rule was found to outperform all other distribution rules as well as the voluntary contribution mechanism in a public goods game even when marginal benefits from the public good differ between agents. We therefore conclude that the specific dimension of heterogeneity, i.e. randomly allocated endowments versus marginal benefits, is crucial in influencing fairness perceptions and therewith the performance of specific distributional rule-based mechanisms. Interestingly, this picture changes under effort-based allocation of endowments: here, in line with our predictions, both rule-based contribution schemes generate larger average payoffs than VCM. The difference between effort-based and random allocation thereby is driven by the different behavior under the VCM: we observe substantially smaller voluntary contributions under effort-based allocation than under random allocation. This effect originates from lower contributions by agents with high endowments who apparently feel entitled to the higher income. Under effort, rule-based contribution schemes therefore successfully counteract coordination failures and typical downward trends in contributions in VCM. The allocation procedure also affects the behavior in the different burden sharing mechanisms. When the equal-contribution rule is applied, agents with low endowment make lower minimum proposals under effort-based than under random allocation, thereby “accepting” disadvantageous inequality. Different from our predictions, agents with high endowment tend to increase their minimum proposal under the equal-payoff rule when comparing effort-based with random allocation. Overall, however, the equal-payoff rule under endowment heterogeneity lifts less than half of the possible efficiency gains across players. The paper is structured as follows: we summarize related literature in Sect. 2, before laying out the theoretical predictions for our treatments in Sect. 3. Section 4 explains the experimental procedure, before the experimental results are presented in Sect. 5. Section 6 concludes the paper.",14
21.0,1.0,Experimental Economics,02 August 2017,https://link.springer.com/article/10.1007/s10683-017-9536-1,Immaterial and monetary gifts in economic transactions: evidence from the field,March 2018,Michael Kirchler,Stefan Palan,,Male,Male,Unknown,Male,"It is indisputable that employees appreciate monetary rewards. They expend additional effort when they receive wages exceeding a theoretical minimum wage. Akerlof (1982), for instance, argues that higher wages serve as “gifts” for employees, who reciprocate with higher effort. Laboratory evidence is broadly consonant, showing that higher wages lead to higher effort by employees (Fehr et al. 1993; Fehr and Falk 1999; Fehr and Gächter 2000; Gächter and Falk 2002; Charness 2004). Evidence from field experiments is mainly supportive as well. Most studies report that employees expend more effort following monetary gifts (Falk 2007; Maréchal and Thöni 2007; Kube et al. 2012; Currie et al. 2013; Cohn et al. 2015), while there is some evidence showing that the effects can be temporary (Gneezy and List 2006). Surprisingly, the role of purely immaterial gifts—e.g., private compliments and individual expressions of appreciation and respect—in economic contexts is less clear. This is remarkable, as the desire for approval and being esteemed is deeply rooted in human behavior, likely because esteem is associated with material and reproductive benefits (Fershtman and Weiss 1998; Fessler 2004). Moreover, reciprocating positive immaterial stimuli may have evolved as an evolutionarily stable strategy, because it signaled potential for future cooperation (Gintis et al. 2003). Theoretical studies postulate that expressions of esteem may increase the recipients utility and that the recipient will then reciprocate with additional effort (Brennan and Pettit 2004; Ellingsen and Johannesson 2007, 2008, 2011). However, evidence from the field is rather scarce. In the behavioral management literature the distinct concept of public social recognition is conjectured to be an important performance reinforcer beside money (Haynes et al. 1982; Bandura 1986; Markham et al. 2002; Stajkovic and Luthans 2003). The behavioral labor market literature also reveals some evidence that public recognition programs and awards positively influence work effort (Kosfeld and Neckermann 2011; Bradler et al. 2014).Footnote 1 However, such programs and awards include an extrinsic component in that they are awarded publicly, and thus provide the recipient with status through publicity (Frey 2007). Immaterial gifts as defined in our study—i.e., private compliments and individual expressions of appreciation and respect—do not contain a public component. The effects of such compliments or expressions of esteem are barely investigated even though this form of gift exchange is fundamental to everyday economic interactions. It occurs constantly between employees and employers, between employees at different hierarchy levels, and between salespersons and customers. To the best of our knowledge, Bradler and Neckermann (2016) is among the closest studies to ours as they investigate the role of singular nonfinancial gifts (Thank You cards) and gifts that combine financial and nonfinancial elements. They find that Thank You cards that signal worker appreciation induce reciprocity and they report interaction effects between money and appreciation when combined with a “personal touch” (i.e., a handmade element like money folded as a bow tie). In another study related to ours, Kube et al. (2012) show that non-monetary gifts have a more pronounced impact on work effort than monetary gifts of equivalent value. However, the non-monetary gifts in both studies are still material (i.e., a thermos bottle and a Thank You card) and thereby different from our concept of immaterial gifts (i.e., private compliments).Footnote 2
 Moreover, research on the influence of reciprocal behavior when gifts are given repeatedly over time is rather scarce. Gneezy and List (2006) show that reciprocal effects following one monetary gift can be temporary and fade out over time. Ockenfels et al. (2015) extend their analyses and show that work performance is higher for the same total wage when wage is increased in two steps as opposed to a single increase. Importantly, the impact of the repeated provision of immaterial gifts is unknown. In addition, material and particularly immaterial gift exchange situations have nearly exclusively been investigated in classical employer–employee relationships, raising the importance of robustness in other domains. We identify customer–salesperson interactions as ideal for analyzing the effects of repeated immaterial and material gift exchange. From a consumer perspective, interactions with salespersons are highly relevant as they occur frequently (i.e., in some cases multiple times a day) and extra effort/kindness from the salesperson is valued greatly. In this paper we narrow these research gaps by reporting results from two natural field experiments (Harrison and List 2004), both of which involve a salesperson preparing a food item following a customer (experimenter) order. We study whether immaterial gifts in the form of a private compliment (treatment COMPLIMENT) and monetary gifts in the form of a tip (treatment TIP)—both provided prior to the product’s preparation—trigger reciprocal behavior from the salesperson. The third treatment, NORMAL, serves as a benchmark. In the first experiment, we collected one-shot data for purchases of ice cream from McDonald’s restaurants, investigating salespersons’ reciprocation induced by our experimental treatments. We quantify the level of salespersons’ reciprocal behavior by measuring the food items’ weights. We further study the impact of repeated provision of immaterial and of monetary gifts by collecting data for repeated purchases of doner durum from independent vendors. Here, the experimenters visited the same salesperson on five consecutive days, exploring how the observed effects develop over time.Footnote 3
 Translating the idea of Ellingsen and Johannesson (2007) on social esteem to our setting, a salesperson’s utility depends both on her income and on her pride from being esteemed by the consumer. We hypothesize that making compliments leads to increased salesperson utility and therefore to increased reciprocal behavior. The salesperson is made to feel proud of what she is doing and exchanges kindness (measured by product weight) for given esteem, yielding our first research question.Footnote 4
 RQ1: Does an immaterial gift in the form of a compliment provided by the consumer trigger increased salesperson kindness compared to “normal” consumer–salesperson interactions? Based on the literature outlined above, we expect a tip in advance to also increase a salesperson’s utility, which she reciprocates with increased product weight. This exchange of greater salesperson kindness for extra money underlies our second research question. RQ2: Does a monetary gift by the consumer trigger increased salesperson kindness compared to “normal” transactions in consumer–salesperson interactions? As mentioned, Gneezy and List (2006) and Ockenfels et al. (2015) show the importance of investigating how reciprocity develops over time. While the former find that in a labor market setting reciprocal behavior triggered by one monetary gifts is temporary, the latter report that work performance is higher for the same total wage when wage is increased in two steps rather than once. We address this issue and extend it by providing five immaterial or five monetary gifts in five consecutive transactions over the same number of working days. This design allows us to answer our third research question. RQ3: Do the effects of repeated immaterial and monetary gifts change over time? We find that both immaterial gifts (compliments) and material gifts (tips)—given in advance—induce positive reciprocity, i.e., salespersons provide more product weight. While monetary gifts trigger a larger level effect, only the effect of immaterial gifts increases significantly over repeated interactions. Finally, we show that the increase in product weight does not necessarily suffice to compensate the customer for the increased cost of the tip, which means that immaterial gifts are more effective when accounting for transaction costs (i.e., the purchase price and the tip) in our settings. With our approach we extend the literature along four dimensions. First, we explore reactions to material gifts and particularly to immaterial gifts in the same settings (ice cream and doner), making them comparable. Importantly, our approach of defining immaterial gifts differs from those of Kosfeld and Neckermann (2011), Bradler et al. (2014), Bradler and Neckermann (2016) and the literature on social recognition programs. The “Thank You” cards and award certificates employees receive in these studies have both a material and an (immaterial) award component. A card or a certificate can be preserved and may provide utility at a later time. It can also be displayed to serve as a public signal to others. A private compliment like in our study is entirely immaterial, cannot be “stored”, cannot be used as physical evidence for impressing others, and thereby expresses respect directly (and usually only) to the recipient. Second, we investigate both types of gifts in natural consumer–salesperson interactions in everyday life situations. We consider this an important aspect of our study as we bring monetary and immaterial gift exchange situations to customer–salesperson interactions, testing for robustness of the results from the literature, which largely derive from employer–employee interactions. We believe that consumer–salesperson interactions are of high importance, because they occur very frequently (i.e., in some cases multiple times a day) and monetary and immaterial incentives (i.e., compliments) play a major role. Third, we use a repeated setting to analyze how monetary and immaterial gifts work over time. We outline the importance of repeated gift exchange situations by showing that reciprocal behavior following immaterial gifts gets stronger over time. Fourth, we analyze the robustness of our findings by investigating whether results replicate from the first to the second setting, i.e., from the ice cream to the doner setting.",11
21.0,1.0,Experimental Economics,11 April 2017,https://link.springer.com/article/10.1007/s10683-017-9525-4,Erratum to: Third-party manipulation of conflict: an experiment,March 2018,Piotr Evdokimov,Umberto Garfagnini,,Male,Male,Unknown,Male,"This erratum has been published as various errors introduced by vendor during proofing process, especially in tables and figures. The original article was corrected.",
21.0,1.0,Experimental Economics,06 July 2017,https://link.springer.com/article/10.1007/s10683-017-9534-3,Erratum to: The emergence of language differences in artificial codes,March 2018,Fuhai Hong,Xiaojian Zhao,,Unknown,Unknown,Unknown,Unknown,,
21.0,2.0,Experimental Economics,31 August 2017,https://link.springer.com/article/10.1007/s10683-017-9542-3,Measuring higher order ambiguity preferences,June 2018,Aurélien Baillon,Harris Schlesinger,Gijs van de Kuilen,Male,Male,Male,Male,"Risk aversion governs many parts of economic behavior. However, it has to be complemented by the higher-order risk preferences prudence and temperance if one wants to explain important economic phenomena such as precautionary saving or prevention efforts. Consider a decision maker (DM) whose income is the same now and at a future point in time, and who just learned that she will face a zero-mean risk at the later point in time. According to expected utility (EU) theory, the DM should reduce current consumption and increase savings (i.e., save precautionary) if his future expected marginal utility is higher than his present marginal utility, i.e. if he has a convex marginal utility (u′′′ > 0), a property typically called (risk) prudence (Leland 1968; Kimball 1990). This result illustrates the importance of risk attitudes in economic decision making beyond risk aversion. For long, the literature on higher order risk preferences remained purely theoretical (Leland 1968; Sandmo 1970; Kimball 1990, 1993). Eeckhoudt and Schlesinger (2006) were the first to propose behavioral, testable conditions of higher order risk preferences that made experimental investigation of these attitudes possible. For instance, one can test whether the DM is risk prudent if he prefers adding a zero-mean risk to a high income state rather than to a low income state. The DM is risk temperate (fourth order, u
(4) < 0 under EU) if he prefers to disaggregate independent zero-mean risks across equiprobable states of nature. The conditions proposed by Eeckhoudt and Schlesinger (2006) led to many measurements in the laboratory and in the field, generally finding strong evidence for risk prudence and, to a lesser extent, for risk temperance (Tarazona-Gomez 2004; Deck and Schlesinger 2010, 2014; Ebert and Wiesen 2011, 2014; Noussair et al. 2014; Maier and Rüger 2012). The literature mentioned above only concerns objectively known probabilities. However, such probabilities are unavailable in many important economic situations (Knight 1921). People have been found to dislike the absence of objective probabilities, exhibiting ambiguity aversion. Since Ellsberg (1961) proposed his famous paradox, empirical studies of ambiguity preferences have virtually all been restricted to studying the causes and prevalence of ambiguity aversion (e.g., see Trautmann and van de Kuilen 2015). This paper explores attitudes toward ambiguity beyond aversion. Ambiguity prudence is a preference for combining ambiguity with states of the world yielding a high chance of a good outcome, rather than with states yielding a low chance of a good outcome. An ambiguity temperate DM dislikes facing two sources of ambiguity at the same time, and prefers to disaggregate them (for details, see Sect. 2). Experimental evidence about higher order ambiguity attitudes is necessary to complement the recent theoretical literature that highlighted the importance of these concepts. For example, ambiguity prudence has been found to play a key role in models of survival of ambiguity averse agents on a market (Guerdjikova and Sciubba 2015) and of prevention behavior (Baillon 2017). Ambiguity prudence is necessary and sufficient for future ambiguity to trigger an increase in prevention efforts, e.g., an ambiguity prudent DM would rather get a vaccine against a new (“ambiguous”) disease than against a well-known disease with the same average severity and incidence (Baillon 2017).Footnote 1 Moreover, specifications of popular ambiguity models (Gilboa and Schmeidler 1989; Schmeidler 1989; Hansen and Sargent 2001; Ghirardato et al. 2004; Klibanoff et al. 2005) that are widely used in applications imply ambiguity prudence. A policy maker who makes robust decisions in the sense of Hansen and Sargent (2007) is ambiguity prudent and ambiguity temperate. Despite the theoretical importance of higher order ambiguity attitudes for economic decision making, there is no evidence regarding the prevalence of higher order ambiguity attitudes in the population. This paper is the first to investigate higher-order ambiguity preferences empirically. First, we develop an experimental methodology to implement the preference conditions proposed by Baillon (2017). Second, we embed our tests in a more general experiment, also measuring ambiguity aversion and higher order risk preferences (risk aversion, risk prudence and risk temperance). This allows us to investigate the relationship between these attitudes, and to test whether both these preferences stem from combining good outcomes with bad outcomes, as recently conjectured by Crainich et al. (2013) and observed by Deck and Schlesinger (2014). Our results show clear support for ambiguity prudence, thereby confirming the predictions of several ambiguity models. Our findings further support, to a lesser extent, ambiguity temperance. The remainder of this paper is organized as follows. In the next section, we present the theoretical foundation of our measurements of higher order ambiguity attitudes. Section 3 describes the design of our experiment. The results are presented in Sect. 4 and are discussed in Sect. 5. Section 6 concludes.",23
21.0,2.0,Experimental Economics,13 October 2017,https://link.springer.com/article/10.1007/s10683-017-9545-0,Experimental study of cursed equilibrium in a signaling game,June 2018,Nichole Szembrot,,,Female,Unknown,Unknown,Female,"Before the law required publication of nutrition facts, labels on many salad dressings did not contain information on fat content. This was true even for dressings that were not among the highest in fat (Mathios 2000). Film-makers can increase their revenues at the box office by selectively not releasing low quality movies to critics before premiering to the public (Brown et al. 2012). While Hillary Clinton was seeking the Democratic nomination for President for 2016, a voter asked her a direct question about whether she supported the Keystone XL pipeline. Clinton responded by continuing her policy of not taking a position on the issue, telling the voter that “‘if it is undecided when I become President, I will answer your question.”’ (Merica and Zeleny 2015) What do all of these examples have in common? All of these environments can be modeled as signaling games. Makers of salad dressings choose whether to include information about fat content, and consumers must infer the fat content of non-labeled dressings before making a purchase decision. Movie-goers must deduce the quality of a movie that was intentionally not released to critics before deciding whether to see it. Voters must form beliefs about the likelihood that the Keystone XL pipeline will be built if Hillary Clinton is elected, then decide whether or not to vote for her. These examples have more in common than this basic structure. In all three, the theoretical prediction under the assumption that people use Bayes’ Rule to update their beliefs when given new information is that all information will be revealed in equilibrium.Footnote 1 If consumers care about fat content in salad dressings (and evidence from choices made after the advent of nutrition fact labels presented by Mathios (2000) shows that they do), then Bayesian consumers would infer that only the highest fat dressings would not list their fat content. Dressing makers would optimally respond by listing the fat content of all dressings, or all but those with the highest fat content. Bayesian film aficionados would infer that a producer that chooses a cold opening must be releasing a dud, and film-makers would always release their films to critics. Potential voters would infer that Hillary Clinton’s position on the Keystone XL pipeline must be unpopular if she does not reveal it, giving her an incentive to be candid with voters. In all three examples, the theoretical prediction is not consistent with the empirical reality. An alternative solution concept for games of incomplete information that can explain these empirical findings is cursed equilibrium (Eyster and Rabin 2005). This concept is Bayes-Nash equilibrium with a modification of the requirement that all players use Bayes’ Rule when updating their beliefs. In cursed equilibrium, beliefs are modeled as a linear combination that puts weight \(1-\chi\) on the correct Bayesian posteriors and weight \(\chi\) on the prior beliefs. \(\chi \in \left[ 0,1\right]\) parameterizes a player’s degree of cursedness; a player who is more cursed (higher \(\chi\)) has less understanding of the link between type (e.g., preferred policy) and strategy chosen (e.g., decision to take no position). If \(\chi =0\), the player understands that other players use type-contingent strategies, and the concept reduces to standard Bayes-Nash equilibrium. If \(\chi =1\), the player is referred to as fully cursed, and she does not perceive a difference between the strategies chosen by different types. A player with \(\chi \in \left( 0,1\right)\) is called partially cursed; she updates from her priors toward the correct Bayesian posterior, but does not fully incorporate the fact that some types may be more likely than others to take a given action. The cursed equilibrium model has been tested in other types of games, as will be discussed in Sect. 2. However, there is no existing work that explicitly tests whether the cursed equilibrium model provides a good explanation of play in sender-receiver games. This paper provides an experimental test of cursed equilibrium in an election environment that mimics the Hillary Clinton example above. We choose to study this game for a few reasons. First, voting is a decision that most people (even students!) make or at least contemplate making even if they choose not to vote. People may have developed heuristics to help them make the right decisions in this domain that we miss when studying more abstract games or auctions with unfamiliar rules. Second, the game studied here is carefully designed to ensure that equilibrium predictions do not depend on beliefs about what happens off the equilibrium path—a rarity in games of incomplete information. Szembrot (2017) analyzes the effects of cursedness in electoral competition theoretically, and the interested reader can find a deeper discussion of theoretical issues in that paper. Section 3 explains the election game played by subjects in detail. Briefly, candidates are either extremists, mainstream, or centrists; they choose whether or not to announce their positions to the centrist voter. There are two possible equilibria, and which exists depends on how cursed the voter is. If the voter is Bayesian, then only extremists take no position, and the voter would choose to vote for a mainstream or centrist candidate over one who took no position. However, if the voter is sufficiently cursed, then both extremists and mainstream candidates take no position, responding to a voter who chooses a candidate who took no position over one who announced a mainstream position. The experimental design exploits the fact that the degree of cursedness that leads voters to switch from choosing a mainstream candidate to choosing a candidate who took no position depends on the distribution of candidate preferences. If parties contain a larger share of candidates who agree with the voter, then the voter does not have to be as cursed to be willing to take a chance on a candidate who took no position. The experiment also introduces an additional source of variation by adding a chance that a candidate will randomly be prevented from making an announcement. When the probability that a candidate will not be able to make an announcement is higher, then less-cursed voters will prefer candidates who took no position, since the action of taking no position is less informative. Subjects play seven rounds of this game, with the distribution of candidate preferred policies and the probability that candidates will be prevented from announcing changing each round. This changes the threshold degree of cursedness that determines whether voters prefer a mainstream candidate to a candidate who took no position, which in turn determines whether mainstream candidates choose to announce their preferred policies or take no position. Analysis of these choices as values of the parameters change puts bounds on the degree of cursedness that is consistent with subjects’ behavior. As explained in Sect. 4, sessions are divided among four treatments designed to shed light on the mechanism behind cursed equilibrium. Two treatments employ computerized candidates that are programmed to play the Bayes-Nash equilibrium of the game (only extremists take no position). A baseline treatment with programmed candidates provides a measure of subjects’ ability to best-respond given the candidates’ strategies. In this treatment, the candidates’ type-contingent strategies were explicitly given to the subjects, who each played the role of a voter. Subjects do not receive this information in the other treatment with programmed candidates; comparing these treatments allows for a test of whether subjects understand the candidates’ strategic incentives. A third treatment allows subjects to play both roles. Comparing play in treatments with subject-candidates and programmed candidates allows us to determine how uncertainty about other players’ degree of strategic sophistication affects behavior. These two treatments include a belief elicitation task to provide further insight into subjects’ beliefs about other players’ actions. Since the evidence on whether belief elicitation affects behavior is mixed (Schotter and Trevino 2014), a fourth treatment with subject-candidates does not include the belief elicitation task. To preview the results described in Sect. 5, the evidence is consistent with many subjects generally having difficulty best-responding to others’ strategies. Even when subjects were told the programmed candidates’ strategies, approximately one-third of votes between a candidate who took no position and a mainstream candidate who announced went to the candidate who took no position. However, play was further away from the Bayes-Nash equilibrium when subjects were not told the candidates’ strategies, and reported beliefs reveal that subjects were very unsure about the strategies chosen by mainstream candidates. Estimates of the distribution of the cursedness parameter come from a structural model of quantal response equilibrium that allows for cursedness. Choices made in the treatment with programmed candidates when subjects were not given the candidates’ strategies imply that 32% of subjects are Bayesians, 32% are fully cursed, and 36% are partially cursed. However, as will be discussed in Sect. 6, these estimates of the proportions of subjects who are cursed are likely biased upward because the model estimates attribute to cursedness choices made due to mistakes translating beliefs about strategies to posterior beliefs or other mistakes in best-responding to others’ strategies. We also estimate the level-k model, but it does not perform well in this environment. Finally, we show that a model of response error that depends only on payoff differences, even allowing for nonequilibrium beliefs, has difficulty explaining why subjects frequently make mistakes in some choices but rarely do so in others.",2
21.0,2.0,Experimental Economics,08 September 2017,https://link.springer.com/article/10.1007/s10683-017-9544-1,Loss aversion and the quantity–quality tradeoff,June 2018,Jared Rubin,Anya Samek,Roman M. Sheremeta,Male,Female,Male,Mix,,
21.0,2.0,Experimental Economics,19 September 2017,https://link.springer.com/article/10.1007/s10683-017-9543-2,Heterogeneous guilt sensitivities and incentive effects,June 2018,Charles Bellemare,Alexander Sebald,Sigrid Suetens,Male,Male,Female,Mix,,
21.0,2.0,Experimental Economics,01 September 2017,https://link.springer.com/article/10.1007/s10683-017-9539-y,Does the paradox of plenty exist? Experimental evidence on the curse of resource abundance,June 2018,Andreas Leibbrandt,John Lynham,,Male,Male,Unknown,Male,"Are societies with abundant resources cursed? The “paradox of plenty” refers to the observation that many societies with abundant natural resources have worse economic outcomes than those that lack natural resources. Typically, this paradox is attributed to abundant resources crowding out activities that improve economic outcomes. Explanations following the crowding-out/in logic are that resource wealth crowds-out positive externalities like entrepreneurial activity (Torvik 2002) and human capital development (Gylfason et al. 1999) or crowds-in anti-growth activities such as conflict (Collier and Hoeffler 2005), rent-seeking (Auty 2001), and corruption (Vicente 2010). There is considerable disagreement about the empirical relevance of resource abundance for economic outcomes: some studies conclude that there is indeed a causal link (Ross 2001; Sachs and Warner 1995; Sachs and Warner 2001) whereas others question its existence altogether (Brunnschweiler and Bulte 2008; Alexeev and Conrad 2009). Moreover, there are studies that suggest the institutional environment crucially determines whether large resource pools are a blessing or a curse (Mehlum et al. 2006; Robinson et al. 2006; Boschini et al. 2007) and it also seems possible that the level of resource endowment may determine the institutional environment (Ross 2001). One main unsettled question is whether (1) the abundance of resources itself, (2) other variables such as the institutional environment, or (3) the interaction between resource wealth and other variables cause inferior economic outcomes and conflict (Norman 2009). To provide a behavioral test of one form of the paradox of plenty and whether it can be prevented, this study uses randomized experimental methods to shed insights related to more complicated settings. The main advantage of this approach is the possibility to observe how a single exogenous change in the level of resource abundance affects economic behavior at both the individual and group level. While the decision environment is obviously extremely simplified, it still captures the crowding-out potential of resource abundance and the important trade-offs between individual and group benefits that often characterize the inefficient exploitation of many natural resources in the field. In our experiment, individuals are randomly assigned to groups of three and simultaneously decide about the extent to which they want to exploit a non-renewable resource (a common pool of money). The experiment lasts until the resource pool of the group is depleted, but at most for five time periods. If, at the end of each period, the group’s total extraction does not exceed the resource then a fraction of the resource that has not been claimed is invested in a public good account, which produces positive externalities for all group members. There are four treatments in our experiment in which we vary resource wealth ($20 or $100) and whether individuals have the institutional capacity to limit access to the resource pool (no regulations vs. voting to protect the resource pool). We find sharp treatment differences and a significant resource wealth × institution interaction. If resource wealth is high, individuals exploit on average 82% more at the start of the experiment than when resource wealth is low. However, if individuals have the option to establish an institution that limits exploitation, individuals exploit on average 50% less at the start of the experiment if the resource wealth is high than when it is low. Moreover, individuals in the low resource wealth treatment are 3.2 times more likely to vote against any resource protection as compared to individuals in the high resource wealth treatment. These treatment dependent behaviors lead to pronounced differences in economic outcomes. For example, giving subjects the option to establish an institution that limits exploitation increases growth of the group’s endowment by a factor of 26.8 if resource wealth is high but only by a factor of 4.4 if resource wealth is low. Our study contributes to the experimental literature investigating public goods/common pool resource exploitation (Ostrom et al. 1992; Cason and Kahn 1999; Sefton et al. 2007; Nikiforakis 2008), the endogenous formation of institutions (Kosfeld et al. 2009; Andreoni and Gee 2012) and voting (Walker et al. 2001; Tyran and Feld 2006). We also contribute to the experimental literature on whether cooperation decreases when stakes are large; this is particularly important since cooperation to conserve non-renewable resources in the field frequently involves very high stakes. The evidence on the role of stakes for cooperation comes mainly from ultimatum games and the findings are often mixed. Slonim and Roth (1998), Andersen et al. (2011), and Leibbrandt et al. (2015) find less cooperation when stakes are very large, whereas other studies report no or only minor stake effects (Forsythe et al. 1994; Hoffman et al. 1996; Camerer and Hogarth 1999; Cameron 1999; Clark and Sefton 2001; Cherry et al. 2002; Parco et al. 2002; Rapoport et al. 2003; Carpenter et al. 2005; Johansson-Stenman et al. 2005). An important difference between these studies and our own is that we investigate cooperation under different stakes when actors make simultaneous decisions in groups (N > 2). To the best of our knowledge, the only studies investigating cooperation in groups for different stake sizes are Marwell and Ames (1980) and Kocher et al. (2008) who study behavior in public goods games. These studies do not find a significant stake size effect. In contrast to our study where resources can be completely exploited, the resource in their games is renewable as it resets in each period. Another important difference between their and our decision setting is that subjects can take money away from a group pool in our setting, which closely mirrors the real-world resource curse and employs a frame that suppresses warm-glow (Andreoni 1995). Probably the closest related study to our own is Al-Ubaydli et al. (2014), which investigates how resource shocks affect resource exploitation in a continuous time laboratory experiment conducted in a virtual online world. They show that unexpected positive resource shocks during their experiment cause more exploitation and that communication can mitigate the extent of exploitation. Our study differs from theirs in that we investigate cooperation under different stakes when actors make simultaneous decisions about the extent to which they exhaust a non-renewable resource. Perhaps most importantly, our study presents some of the first evidence on the interaction between stake size and institutional choice and demonstrates that institutions can turn resource abundance into a blessing instead of a curse.Footnote 1
 Our experimental set-up provides a complementary approach to existing empirical studies on the paradox of plenty, which typically rely on cross-country comparisons, case studies or panels (for recent overviews see Wick and Bulte 2009; van der Ploeg 2011). Results based on non-experimental data are difficult to interpret because their units of observation differ on many (possibly unobservable) dimensions, have unique histories, and all or some of these differences may crowd-in unproductive activities. Finally, our study may also be of interest for the rich theoretical work exploring mechanisms through which resource abundance influences growth. In line with the design of our resource depletion game, a number of political economy theories emphasize the way that resource booms can encourage rent seeking (Tornell and Lane 1999; Torvik 2002; Mehlum et al. 2006; Hodler 2006; van der Ploeg and Rohner 2012). For example, in Torvik (2002), a greater amount of natural resources increases the number of entrepreneurs engaged in rent seeking and reduces the number of entrepreneurs running productive firms: more natural resources can thus lead to lower welfare. In Mehlum et al. (2006), entrepreneurs can either “grab” rents from natural resources or they can invest them in production. If institutions are weak, all resources are grabbed but if resources are strong then all the resources are invested in production and the spoils are divided equally among all entrepreneurs. We hope that our study fills an important gap between the abstract but causal approach of this theory literature and the real-world but correlative approach of the empirical literature.",5
21.0,2.0,Experimental Economics,23 August 2017,https://link.springer.com/article/10.1007/s10683-017-9538-z,Peer effects in computer assisted learning: evidence from a randomized experiment,June 2018,Marcel Fafchamps,Di Mo,,Male,Female,Unknown,Mix,,
21.0,2.0,Experimental Economics,24 May 2017,https://link.springer.com/article/10.1007/s10683-017-9528-1,The BCD of response time analysis in experimental economics,June 2018,Leonidas Spiliopoulos,Andreas Ortmann,,Male,Male,Unknown,Male,"It seems widely agreed that decisions “in the wild” (Camerer 2000, p. 148) are often afflicted by time pressure, typically to the decision maker’s detriment. Addressing these effects of time pressure, the common adage “to sleep on it”, for example, implies that delaying a decision can improve its quality by allowing more time to reflect on it cognitively and emotionally. In fact, legislators have acknowledged the influence of the interaction of time and emotions on decisions: Mandatory “cooling-off periods” are used to temper the effects of sales tactics such as time pressure on consumer purchases by allowing consumers to renege on impulse purchases (Rekaiti and Van den Bergh 2000). Similarly, “cooling-off periods” between the filing and the issuance of a divorce decree have been found to reduce the divorce rate (Lee 2013). When time is scarce, decision makers have less time to process information pertaining to the specific case at hand, and instead may rely on their priors, which may be driven by stereotypes. Under time pressure, stereotypes about defendants are more likely to be activated and can affect judgments of guilt and proposed punishment (van Knippenberg et al. 1999). Similarly, judgments under time pressure about a suspect holding a weapon or not are more likely to exhibit racial biases (Payne 2006). Assessments of whether acute medical attention is required can also be shaped by time pressure (Thompson et al. 2008). Other examples of environments that operate under time pressure include auctions, bargaining and negotiations, urgent medical care, law enforcement, social settings with coordination issues, and human conflict; moreover, all decisions have an implicit non-zero opportunity cost of time. Beyond the time taken to deliberate, collecting and processing information efficiently is also time-consuming. Yet, the temporal dimension of decision making has not featured prominently in economists’ analyses of behavior. We argue below that it often matters, both for individual and strategic decision making (henceforth, individual and strategic DM). We will argue that the analysis of (possibly forced) response time (RT) data can significantly complement standard behavioral analyses of decision making; of course, it is no panacea and we will highlight challenges and pitfalls along the way. The scientific measurement of the timing of mental processes (mental chronometry), starting with Donders (1868), has a long tradition in the cognitive psychology literature—see Jensen (2006), Luce (2004) and Svenson and Maule (1993) for contemporary discussions. While psychologists have long acknowledged the benefits of jointly analyzing choice and possibly forced RT data, even behavioral economists have until recently paid little attention to RT. Many of the most prominent behavioral models remain agnostic about RT, e.g., Prospect Theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992), models of fairness (Bolton and Ockenfels 2000; Charness and Rabin 2002; Fehr and Schmidt 1999), and temporal discounting models (Laibson 1997). Early work in economics can be classified into two types of RT applications. The first type of application emphasizes the usefulness of RT analysis for DM without any time constraints (Rubinstein 2004, 2006, 2007, 2008, 2013, 2016), which we refer to as endogenous RT. Decision makers are free to decide how long to deliberate on a problem; RT is shaped by the opportunity cost of time and the magnitude of the task incentives. Consequently, rational decision makers must choose a point on a speed–performance efficiency frontier. For economists, performance will typically be measured as utility. This is consistent with an unconstrained utility maximization problem only when the opportunity cost of time is very low relative to the incentives, thereby excluding a significant proportion of real-world decision environments. Researchers working with endogenous RT typically measure the time taken for a subject to reach a final (committed) decision—we refer to this as single RT. However, subjects’ provisional choices may be elicited throughout the deliberation period at various times (Agranov et al. 2015; Caplin et al. 2011)—we refer to this as multiple RT. Multiple RT captures the evolution of within-subject decision processes over time, yielding more useful information about the dynamic underpinnings of decision making. In most experiments, payoffs are typically independent of RT (non-incentivized). Another possibility is to use incentivized tasks that introduce a benefit to answering more quickly, for example, by having a time-based payoff reward or penalty (e.g., Kocher and Sutter 2006).Footnote 1
 The second type of application emphasizes the examination of DM under  time constraints (Kocher et al. 2013; Kocher and Sutter 2006; Sutter et al. 2003), which we refer to as exogenous RT. The most common type of time constraint is time pressure, i.e., limited time to make a decision. Time delay, i.e., the imposition of a minimum amount of time, can be also found in some studies, usually those interested in the effects of emotions on decision making (e.g., Grimm and Mengel 2011). Decision makers are increasingly being called upon to multi-task, i.e., handle many tasks and decisions almost simultaneously or handle a fixed number of tasks within a time constraint. Measuring the time allocated to individual tasks is crucial to understanding how decision makers prioritize and allocate their attention. One technique of implementing time pressure in the lab is to impose a time limit per set of tasks, instead of per task, as is typically done. This route is taken by Gabaix et al. (2006), who find qualitative evidence of efficient time allocation, i.e., subjects allocated more time to tasks that were more difficult. In the majority of studies, treatments within an experiment typically compare an endogenous RT treatment with other exogenous RT constraints, i.e., RT is the only variable that is manipulated across treatments. However, if other variables are also simultaneously manipulated across treatments, it is possible that the RT manipulations will interact to different degrees with the other variables. Furthermore, knowledge that an opponent is also under time pressure could induce a change in beliefs about how the opponent will behave. These two examples highlight the importance of a thorough understanding of RT constraints and a well-designed experiment that minimizes the impact of such issues—we return to the issue of identification later in Sect. 5.1. Endogenous and exogenous RT analyses differ in the benefits that they offer. The former’s usefulness lies primarily in revealing additional information about a decision maker’s underlying cognitive processes or preferences (aiding in the classification of decision-maker types) and the effects of deliberation costs on behavior. The latter’s usefulness lies primarily in exploring the robustness of existing models to different time constraints, i.e., verifying the external validity of models and the degree to which they generalize effectively to different temporal environments. We will present evidence that behavior on balance is strongly conditional on the time available to make a decision. In fact even the perception of time pressure, when none exists, can significantly affect behavior (Benson 1993; DeDonno and Demaree 2008; Maule et al. 2000). Experimental designs manipulating realistic time constraints in the lab are a useful tool to advance our understanding of behavior and adaptation to time constraints. Exogenous RT analysis has already led to important insights within the context of two different approaches to modeling decision making. The first approach examines how decision processes change under time pressure. Historically, this has been the focus of research in cognitive psychology that was driven by the belief that cognition and decision making rules are shaped by the environment (Gigerenzer 1988; Gigerenzer et al. 1999, 2011; Gigerenzer and Selten 2002; Hogarth and Karelaia 2005, 2006, 2007; Karelaia and Hogarth 2008; Payne et al. 1988, 1993). By exploiting statistical characteristics of the environment, such ecologically rational heuristics (or decision rules) are particularly robust, even outperforming more complex decision rules in niche environments. This raises the following question. How are the appropriate heuristics chosen for environments with different temporal characteristics? A consensus has emerged from this literature that time pressure leads to systematic shifts in information search and, ultimately, selected decision rules (Payne et al. 1988, 1996; Rieskamp and Hoffrage 2008). Subjects adapt to time pressure by: (a) acquiring less information, (b) accelerating information acquisition, and (c) shifting from alternative—towards attribute-based processing, i.e., towards a selective evaluation of a subset of the choice characteristics. These insights from cognitive psychology emerged from individual DM; in Spiliopoulos et al. (2015) we present evidence that similar insights can be had for strategic DM. Imposing time pressure in one-shot \(3\times 3\) normal-form games led to changes in information search (both acceleration and more selective information acquisition) that also have been documented for individual DM as well as the increased use of simpler decision rules such as Level-1 reasoning (Costa-Gomes et al. 2001; Stahl and Wilson 1995).Footnote 2
 The second approach examines how preferences may depend on time constraints. This approach contributes to the discussion on the (in)stability or context-dependence of preferences by adding the temporal dimension to the debate (Friedman et al. 2014). Specifically, we will review evidence that a wide range of preferences are moderated by time constraints. For example, risk preferences are affected by time pressure. Risk seeking (or gain seeking relative to an aspiration level) can be stronger under time pressure in the gain or mixed domains, although this may depend on framing (e.g., Kocher et al. 2013; Saqib and Chan 2015; Young et al. 2012). Furthermore, RT analysis has led to a burgeoning inter-disciplinary literature and debate about the relationship between social preferences and RT (both endogenous and exogenous). A debate is in progress about whether pro-social behavior is intuitive, and whether people are more likely to behave more selfishly under time pressure (e.g., Piovesan and Wengström 2009; Rand et al. 2012, 2014; Tinghög et al. 2013; Verkoeijen and Bouwmeester 2014). This is one of the most exciting topics that RT analysis has motivated, as the nature of human cooperation is central to our understanding of the functioning of society—we will discuss this debate in detail in the next section. The analysis of endogenous RT–while not as common–has also produced some interesting findings in experimental economics. Recall, that endogenous RT analysis is primarily a methodological tool that allows researchers to learn more about individuals’ decision processes and preferences, which tend to be quite heterogeneous. Consequently, researchers are often interested in the classification of decision-makers into a set of types based, say, on social preferences and risk preferences. Classification is typically accomplished solely on the basis of choices (through revealed preferences), but response time can also be used for this purpose. Numerous studies have determined that RT can be used to predict behavior out-of-sample or to classify subjects into types, often more efficiently than using other classical variables such as the level of risk aversion (Rubinstein 2013) and even normative solutions (Schotter and Trevino 2014b). Chabris et al. (2008, 2009) found that intertemporal discount parameters estimated using only RT data were almost as predictive as those estimated traditionally from choice data. Rubinstein (2016) proposes classifying players within a spectrum called the contemplative index. The degree of contemplation or deliberation that a person exhibits seems to be a relatively stable personality trait, which can be used to predict behavior even across games. While experimental economists have begun tapping into the potential of exogenous RT analysis, they have not embraced endogenous RT analysis to the same degree. It is our belief that there still exists significant potential for the latter; however, similarly to the endogenous RT work in cognitive psychology, unleashing the full potential is aided by the use of procedural (process-based), rather than substantive (outcome-based), models of behavior. In contrast to substantive models, procedural models stipulate how decisions are made (specifying the mechanisms and processes) in addition to the resulting decisions. Procedural models that jointly predict choice and RT are crucial for predicting how adaptation occurs in response to RT constraints—the class of sequential-sampling models discussed in Sect. 3.6 is one example. In mathematical psychology, model comparisons of procedural models have a tradition of using RT predictions (not just choices) to falsify models—see for example Marewski and Melhorn (2011). Our literature reviewFootnote 3 revealed that the existing RT studies in economics exhibit a lack of formal procedural modeling and are most often viewed through the lens of dual-system models (Kahneman 2011). These models contrast a faster, more instinctive and emotional System 1 with a slower, more deliberative System 2—under time pressure System 1 is more likely to be activated. Many studies on social preferences are devoted to reverse inference based on these two systems, i.e., types of decisions that are made faster are categorized as intuitive. This may be a problematic identification strategy. We have briefly presented what we see as examples of how RT analysis has already led to important insights in experimental economics. The case for collecting RT data in economic experiments seems strong, as RT is an additional variable available at virtually zero cost for all computerized experiments. If no time constraints are imposed, the collection of RT data is not noticeable by experimental subjects and neither primes nor otherwise affects their behavior. While there is little cost associated with collecting the data, the benefits depend on the type of study. Response time analysis seems particularly useful in the cases that we have outlined above where time constraints may mediate decision-makers’ preferences (e.g., risk or social preferences) or processes. Also, in information-rich environments where information search or retrieval may be costly, the imposition of a time constraint or high opportunity cost of time is likely to have an amplified effect on behavior. In empirical model comparison studies,Footnote 4 where it is practically difficult to collect enough choice data on a large enough set of tasks, RT can be used to more effectively discriminate between procedural models by increasing the falsifiability of models (they may be rejected either for poor choice predictions or poor RT predictions). Finally, even basic response time analysis can be useful in virtually any experimental study. Extremely quick responses or very slow responses are often symptomatic of subjects that are not engaging with the experiment seriously. The influence of such outliers on the conclusions drawn from experiments can be extremely problematic as we will show later on. Our manuscript is meant to assess the state of the art, to stimulate the discussion on RT analysis, and to bring about a critical re-evaluation of the relevance of the temporal dimension in decision making. In complex strategic decision making, adaptive behavior that makes efficient use of less information, less complex calculations (e.g., such as higher-order belief formation about an opponent’s play), and emergent social norms, seems even more important than for individual DM (Hertwig et al. 2013). Inspired by the results in cognitive psychology, we envision a research agenda for strategic DM that parallels that of individual DM. Whilst we emphasize the potential contribution of RT to strategic DM, we note that most of our arguments are relevant to individual DM. We envision this manuscript as a critical review of RT analysis that is accessible to readers with little prior knowledge of the topic, for instance an advanced graduate student who wants to jump-start her/his understanding of the issue. Since the paper is quite long, we have used a modular structure so that readers with prior experience may selectively choose the sub-topics they are more interested in. An extended version of the paper including some more technical arguments can be found in our working paper (Spiliopoulos and Ortmann 2016), which we first posted in 2013 and have revised contemporaneously. The present manuscript is organized as follows. We summarize the benefits, challenges, and desiderata (the BCD) of both the experimental implementation of RT experiments and the joint statistical modeling of choice and RT data in Table 1. A literature review of RT studies and summary of the most important findings follows in Sect. 2. In the following section we delve into the multitude of ways to model RT and choices (Sect. 3). We then devote the next three sections to pull together the benefits, challenges, and desiderata of RT analysis in experimental economics. We encourage the reader to preview our summary arguments in Table 1—keeping these arguments in mind before delving into detailed arguments will likely be beneficial. Section 7 concludes our manuscript. A detailed literature review of RT in strategic DM is presented in “Appendix 1”, including Table 3 taxonomizing all the studies we have found. A framework for relating behavior and decision processes to time constraints for strategic DM is presented in “Appendix 2”.",66
21.0,2.0,Experimental Economics,08 September 2017,https://link.springer.com/article/10.1007/s10683-017-9541-4,Higher-order risk preferences in social settings,June 2018,Timo Heinrich,Thomas Mayrhofer,,Male,Male,Unknown,Male,"Much of the economic research on behavior under uncertainty has focused on risk aversion. However, over the course of the last decades it turned out that higher-order risk preferences like prudence and temperance also impact many decisions. Their importance has first been pointed out with respect to saving decisions (e.g. Leland 1968; Sandmo 1970). Subsequently it has been shown that higher-order risk preferences also affect behavior in various fields, such as auctions (e.g. Esö and White 2004; Kocher et al. 2015), bargaining (e.g. White 2008; Embrey et al. 2016), public good provision (e.g. Bramoullé and Treich 2009), or medical decision making (e.g. Eeckhoudt 2002; Felder and Mayrhofer 2014, 2017). This study makes the first attempt to assess higher-order risk preferences in social settings under controlled conditions. In particular, we focus on two aspects that have been found to influence behavior in various experiments (often contrary to standard economic theory): communication and concerns about the payoff of others. In a laboratory experiment we elicit higher-order risk preferences of individuals and systematically vary how an individual’s decision is made (alone or while communicating anonymously with a partner) and who is affected by the decision (only the individual or the partner as well). In the framework of expected utility theory risk aversion is captured by a negative second derivative of the utility function. Prudence refers to a positive third derivative (Kimball 1990) and temperance to a negative fourth derivative of the utility function (Kimball 1992). Most of the commonly used utility functions imply mixed risk aversion, which means that the derivatives of the utility functions exhibit alternating signs (see Brocket and Golden 1987; as well as Caballé and Pomansky 1996). These utility functions imply risk averse as well as prudent and temperate behavior. In typical life-cycle models of consumption, prudence implies that an individual saves more if the risk of future income increases (also called “precautionary saving” which leads to more precautionary wealth) while temperance is a necessary condition that introducing an independent unfair background risk reduces the investment in risky assets (a necessary and sufficient condition is a combination of higher-order risk preferences called risk vulnerability, see Gollier and Pratt 1996).Footnote 1 However, empirical estimates for the fraction of saving that is precautionary are extremely diverse, ranging from close to zero to greater than 50% (see Geyer 2011, for an overview). In their critical review of the topic, Carroll and Kimball (2008) conclude that estimates of precautionary wealth are sensitive to elicitation procedures and subject to problems from unobserved heterogeneity. Therefore, recent studies have analyzed the prevalence of higher-order risk preferences via laboratory experiments in which individuals decide privately over lotteries that only affect their own payoffs (see Tarazona-Gómez 2004; Deck and Schlesinger 2010, 2014, 2016; Ebert and Wiesen 2011, 2014; Maier and Rüger 2012; Krieger and Mayrhofer 2012, 2017; Noussair et al. 2014; Haering et al. 2017). Most of these experiments are based on the model-independent and non-parametric definition of higher-order risk preferences introduced by Eeckhoudt and Schlesinger (2006). They find that a majority of choices are risk averse, prudent, and temperate—the only exception is the study by Deck and Schlesinger (2010) which finds more intemperate than temperate behavior. In a recent study, Noussair et al. (2014) experimentally elicit higher-order risk preferences from a representative sample of the Dutch population and observe most choices to be risk averse, prudent, and temperate. They also correlate individual lottery choices with individual field behavior. Interestingly, Noussair et al. (2014) do not observe any correlation of risk aversion with financial decisions in the field. However, in line with theoretical results, they observe more prudent individuals to have less credit card debt and more temperate individuals to have less risky investment portfolios. These real world decisions, however, are most often not made individually but in the social settings of households or organizations. That means, they affect more than one person or are made after consulting with others. In this respect, the settings in which they are made differ from the settings that have been studied experimentally thus far.Footnote 2
 In this paper, we study the social dimension of higher-order risk preferences. To limit confounding gender effects we focus on the behavior of male subjects only. We find that the majority of subjects are risk averse, prudent, and temperate individually and across social settings. Moreover, other-regarding concerns and the ability to communicate with a partner do not impact (higher-order) risk preferences per se. However, we observe a significant influence of the partner on the individual’s decisions with respect to risk aversion and prudence when subjects are able to communicate and the decision affects both. The rest of the paper is organized as follows. Section 2 discusses related papers on second-order risk aversion in social settings and presents our hypotheses. Section 3 describes the experimental design and procedure. Section 4 presents the results and Sect. 5 concludes the paper.",27
21.0,2.0,Experimental Economics,16 August 2017,https://link.springer.com/article/10.1007/s10683-017-9537-0,Focusing on volatility information instead of portfolio weights as an aid to investor decisions,June 2018,Christian Ehm,Christine Laudenbach,Martin Weber,Male,Female,Male,Mix,,
21.0,3.0,Experimental Economics,28 April 2018,https://link.springer.com/article/10.1007/s10683-018-9573-4,Introduction to the Symposium in Experimental Economics in memory of John Van Huyck,September 2018,Yan Chen,Catherine Eckel,,Male,Female,Unknown,Mix,,
21.0,3.0,Experimental Economics,28 February 2017,https://link.springer.com/article/10.1007/s10683-017-9521-8,Coordination and transfer,September 2018,David J. Cooper,John Van Huyck,,Male,Male,Unknown,Male,"Nash equilibrium (Nash 1950) is a fundamental concept in game theory. As the name implies, it is an equilibrium concept and does not specify how play arrives at an equilibrium. This is a critical issue in applying the theory to field settings, especially when more than one Nash equilibrium exists. Learning models, which conceptualize equilibrium as the steady state of a dynamic process, offer a good explanation for the emergence of equilibrium. Theorists have enjoyed great success showing conditions under which learning processes will converge to equilibrium.Footnote 3 Experimenters have also studied learning in games extensively. They have shown the existence of strong dynamics in game play, characterized the learning rules being used, and established conditions under which play converges to an equilibrium.Footnote 4
 The experimental literature on learning in games typically focuses on repeated play of the same game. This begs the question raised by Kreps in the opening quote. Relatively little work has been done on the issue of transfer, the ability to take information learned in one game and apply it in another related game.Footnote 5 The purpose of our experiments is to study transfer between games, particularly in an experimental design that forces subjects to rely on general principles rather structural similarity between games. Examples of structural similarity between games include having the same number of actions, the same location for Nash equilibria, or the same locations of secure and Pareto efficient outcomes.Footnote 6
 Our experiments employ two different types of coordination games, stag hunt games and order statistic games. Coordination games are a good environment to study transfer. It is simple to construct games that have similar structures but generate different behavior when played in isolation (e.g. minimum and median games). There are general principles (payoff dominance, risk dominance, and security) that apply to multiple classes of coordination games with distinctly different structures (i.e. different number of actions, payoff structures, and/or locations of equilibria). This makes it possible to study whether subjects are transferring general principles between games rather than merely learning an action and using it in an obviously related game. Our experiment contains three treatments. The main treatment grew directly out of the bet between John and me. Subjects initially play the series of 75 similar stag hunt games used by Rankin et al. in fixed cohorts of eight subjects. They then play a series of 14 “random order statistic” games with their cohort. These are eight player coordination games where subjects simultaneously choose a number between 1 and 7. A subject’s payoff is maximized if he matches the OSth lowest number chosen by the other seven members of his group (OS stands for “order statistic”). The value of OS ranges between 1 and 7 and is randomly drawn for each game. If OS = 1, this is almost the same as the well-known minimum game.Footnote 7 Likewise, OS = 4 is (almost) a median game. The structures of the random order statistic games are similar to each other but quite different from the stag hunt games. General principles like payoff dominance and risk dominance apply to both the stag hunt and random order statistic games. The other two treatments serve as controls. In the first, subjects play a sequence of random order statistic games without previous experience in stag hunt games. The second control treatment has subjects playing a sequence of order statistic games where OS is fixed rather than randomly changing. In other words, this treatment varies OS between groups but keep it fixed within groups. A simple model of learning with transfer predicts that the response of subjects’ choices to OS will be flatter in the control treatment where the value of OS changes randomly than the control treatment where OS is fixed within groups. Given the previous results of Rankin et al., we expect subjects in the stag hunt phase of the main treatment to converge to the payoff dominant equilibrium (subject to some convention emerging). Assuming that the concept of payoff dominance transfers between games, we predict that choices in the random sequence of order statistic games will be shifted upward for the main treatment where subjects have previous experience with stag hunt games relative to the control treatment where subjects have no previous experience in similar games. The results are in line with our hypotheses. The most important result is the presence of positive transfer between the stag hunt games and the random order statistic games. Relative to the control treatment, choices in the random sequence of order statistic games are higher when subjects have previous experience in the stag hunt games. Choices are highest in cohorts that had previously converged to the payoff dominant equilibrium in the stag hunt games. A closer look at the data indicates that initial play in the stag hunt games has little predictive power for the random order statistic games. Play in the random order statistic games depends on what subjects learn in the stag hunt games rather than their initial behavior. In other words, the effect does not appear to be driven by subjects’ types. Subjects’ play is consistent with them learning a principle (payoff dominance) and then transferring it to a different game. Comparing behavior in the two control treatments, the response of subjects’ choices to the value of OS is flatter when the value of OS changes randomly between rounds. To our surprise, subjects’ use of feedback is not sensitive to how close previous values of OS are to the current value. Subjects treat past experience as being more relevant to the current game than is warranted. With experience, play is shifted upwards for all values of OS when the value of OS is fixed relative to when OS changes randomly. This shift is extremely large for high values of OS. There is strong transfer between games when subjects play randomly changing order statistic games, but this transfer does not lead to more efficient outcomes. Our experimental results make a valuable contribution to the literature on transfer in games. Two strands of the literature are particularly relevant for our work. The first concerns the role of precedence in coordination games. Van Huyck et al. (1991) study whether a precedent established in one version of a median game would carry over to another version of the median game. They find little evidence of transfer. Other studies have yielded more positive results. Devetag (2005) studies transfer from critical mass games to minimum games. The critical mass game typically yields efficient coordination while play in the minimum games usually collapses to the secure equilibrium. Devetag finds that a precedent of efficient play in the critical mass game carries over to the minimum games with play shifting toward the payoff dominant equilibrium. Brandts and Cooper (2006) study a design where subjects initially play a minimum game with payoffs very unfavorable to coordination above the minimum. Players are then switched to a version of the game with favorable payoffs for coordination at higher effort levels. If the payoff table is returned to the original harsh environment, play does not collapse back to the minimum effort level. Instead, there is a persistent positive effect from having experienced coordination at high effort levels. Weber (2006) studies the precedent established in moving from minimum games with a small number of players (where efficient coordination is common) to games with larger numbers of players. He finds that efficient coordination is maintained as the group size grows. Cason et al. (2012) study transfer between median and minimum games. They find strong positive transfer from the median games to subsequent play in minimum games. Interestingly, no such transfer is found from minimum games to subsequent median games and there is little spillover when the games are played simultaneously. Overall, the bulk of the evidence points to positive transfer between coordination games with a precedent of efficient play carrying over to environments where efficient play is more difficult. A second strand of the literature looks at whether subjects can transfer general principles. The evidence on this count is mixed. Ho et al. (1998) find positive transfer, but of an odd variety. Subjects are switched between different p-beauty contest games, with iterated removal of dominated strategies being the relevant strategic concept in all cases. Subjects switched to a new game initially play no differently from inexperienced subjects but learn faster. Cooper and Kagel (2005, 2008) study the ability of subjects to take principles learned in one signaling game and apply them in another. The action space and general principles are the same across games, but the form of the equilibrium (pooling vs. separating) and the actions used in equilibrium differ. The results are mixed, with negative transfer reported in the 2005 paper and positive transfer reported in the 2008 paper. Subsequent work (Cooper and Kagel 2009) shows that the differing results can be attributed to the use of abstract versus meaningful context to frame the games. Haruvy and Stahl (2012) study play in a series of dissimilar 4 × 4 normal form games. They find that subjects learn rules and transfer these rules between games.Footnote 8
 The papers studying transfer between games share several common features: (1) With the exception of Haruvy and Stahl, the games being played are structurally similar. There are obvious cues that the games are related and obvious mappings of actions from one game into the other. (2) For papers studying transfer of general principles, the principles involved are strategic principles such as not playing dominated strategies, rather than principles of equilibrium selection such as choosing a payoff dominant equilibrium. (3) Little attention is paid to determining what features make subjects treat games as more or less similar to each other. Our work makes several contributions to the existing literature. We find that precedents transfer between coordination games, as in previous papers, but with two important differences. First, transfer does not necessarily lead to more efficient outcomes. In the control treatment with a random sequence of order statistic games there is strong transfer between games which harms payoffs relative to the treatment where OS is held fixed. Second, the random order statistic games have a different number of actions, a different payoff structure, and different locations for equilibria than the stag hunt games. Like Rankin et al., we have deliberately reduced the ability of subjects to rely on structural similarity rather than general principles. The positive transfer between stag hunt and random order statistic games shows that structural similarity is not a necessary condition for precedents to influence play in coordination games. We also contribute to the literature on the transfer of general principles. Our data is consistent with subjects learning and applying a general principle. Transfer of general principles has been observed before, but our example is unusual because the stag hunt and random order statistic games have little structural similarity. This should increase our optimism about the ability of subjects to learn and transfer general principles. Our experiments are also novel because the principle being transferred is not a principle of strategic play but rather an equilibrium selection concept.Footnote 9 Finally, our subjects are surprisingly insensitive to differences between games. This suggests that a central feature of the ABEE concept (Jehiel 2005), the treatment of broad classes of games as equivalent, may be realistic.",4
21.0,3.0,Experimental Economics,05 July 2018,https://link.springer.com/article/10.1007/s10683-017-9533-4,Conditional behavior and learning in similar stag hunt games,September 2018,John Van Huyck,Dale O. Stahl,,Male,,Unknown,Mix,,
21.0,3.0,Experimental Economics,23 May 2018,https://link.springer.com/article/10.1007/s10683-018-9577-0,When less information is good enough: experiments with global stag hunt games,September 2018,John Van Huyck,Ajalavat Viriyavipart,Alexander L. Brown,Male,Unknown,Male,Male,"The stag hunt game provides a tradeoff between two pure-strategy equilibria: an efficient, risky equilibrium and an inefficient, safe equilibrium. The risks involved are strategic. The strategy associated with the safe equilibrium ensures a constant payoff. The risky strategy yields reduced payoffs if others do not play the same strategy. Thus, the choice between equilibrium depends on players’ beliefs (and higher order beliefs) about how individuals naturally coordinate. While philosophical implications of this choice are quite profound, frustratingly, methods in basic game theory cannot tell us which of these two pure-strategy equilibria is the “right” one. A great deal of work in game theory and experimental economics has addressed the issue of equilibrium selection in this specific game under complete information. Early experiments find repeated, randomly-rematched, play converges to the inefficient equilibrium (Cooper et al. 1990, 1992). Later research has less absolute findings; structural differences across experiments appear to determine which equilibrium is dominant (see Devetag and Ortmann 2007). The most glaring exception, Rankin et al. (2000), achieves near universal coordination on the efficient equilibrium by slightly varying (i.e., perturbing) payoffs each round, in contrast to the convention of keeping payoffs constant for several rounds. Perturbed payoffs are also utilized in theoretical models, but generally to justify coordination away from the efficient equilibrium (Carlsson and van Damme 1993a, b) introduce private signals about payoffs in the stag hunt game. Agents best respond by playing the safe strategy up until a sufficiently high signal after which they play a risky strategy. Their “global games” approach determines a unique threshold that is the sole equilibrium that survives iterated removal of dominated strategies (provided noise is sufficiently small). This threshold is equivalent to the risk dominance solution (Harsanyi and Selten 1988) in this particular game. How can perturbed payoffs have such starkly different implications in theory and experiments? The distinction between identical signals that are public or private—while structurally very similar—may determine radically different equilibrium outcomes. In such case, the design of Rankin et al. (2000) is perfectly suited to test the predictions of global games in the stag hunt. We can greatly alter game theoretic predictions without changing much else. Further, the Rankin et al. (2000) design provides a case known to produce coordination on the efficient equilibrium, so any movement to the global-games prediction under incomplete information is easily observable. To that end, we conduct an experiment where subjects play a sequence of perturbed stag hunt games either with complete or incomplete information. Under the complete information treatment—a setting nearly identical to Rankin et al. (2000)—subjects coordinate on the efficient equilibrium. Under the incomplete information—a setting designed to match the motivating example of the theory of global games—subjects achieve the efficient equilibrium almost as often, inconsistent with the global games prediction. Our paper, Rankin et al. (2000), and (Carlsson and van Damme 1993a, b) all rely on the notion of a general class of stag hunt game forms, depicted in Table 1.Footnote 1 When \(Q\in [0,1]\), the game has two strict equilibria: either both players choose A or both players choose B. Both equilibrium selection criteria and human subject behavior can be generally classified by threshold strategies, that is, choosing A if \(Q<Q^{*}\) and B if \(Q\ge Q^{*}\) for some \(Q^{*}\in [0,1]\). Any strategy profile where both players use threshold strategies with the same \(Q^{*}\) will constitute an equilibrium because they will always end up playing (A, A) when \(Q<Q^{*}\) and (B, B) when \(Q\ge Q^{*}\). In fact, risk dominance (Harsanyi and Selten 1988) selects (A, A) when \(Q<0.5\) and (B, B) when \(Q>0.5\). Our complete information treatment replicates the findings of Rankin et al. (2000). Subjects utilize thresholds only on the top quartile of signals (i.e., \(Q^{*}\in (0.75,1]\)), far higher than the predictions of risk dominance. This results in subjects playing choice A with high probability. Our incomplete information treatment matches the environment of the motivating example of Carlsson and van Damme (1993a, b). They consider an environment where each agent observes a private and noisy signal about Q, the global games parameter, with the true value of Q remaining unknown. In this global stag hunt game, their theory uses iterated elimination of dominated strategies to pin down a unique dominance solvable equilibrium. The unique threshold equilibrium is also \(Q^{*}=0.5\). In contrast, we find most subjects utilize thresholds that are far higher than the predicted global games threshold. The cohort-level differences between treatments are not statistically meaningful. In sum, there is little evidence in our experiment to support the predictions of global games or risk dominance as criteria for equilibrium selection. Subjects appear to coordinate on the efficient equilibrium immediately and do not deviate from it. Further, the effect of introducing private signals—in the spirit of Carlsson and van Damme (1993b)—to the experimental setup of Rankin et al. (2000) appears to be nil. We caution how far we can extend this conclusion. The level of coordination in stag hunt games under complete information is sensitive to structural changes (Devetag and Ortmann 2007). The results under Rankin et al. (2000)’s design (our complete information treatment) are quite exceptional for their level of efficient outcomes achieved. Our results may only show that changes between incomplete and complete information treatments in this context are minimal, a result consistent with many global games experiments e.g., Heinemann et al. (2004). That being said, our findings are still quite relevant. Another experiment that compares stag hunt games under complete vs. incomplete information, Cabrales et al. (2007) concludes the criterion of risk-dominance is predictive in global stag hunt games. That paper uses an alternative study for their complete information baseline i.e., Battalio et al. (2001) that does not provide the high efficiency results of Rankin et al. (2000). We provide a necessary counterpoint. The remainder of the paper is organized as follows. The next subsection provides an extensive literature review of stag-hunt experiments, global games models and experiments. Section 2 describes our specific analytical framework. Section 3 lays out the experimental design. Section 4 presents results from the experiments, including debriefing questionnaires. We discuss our results in Sect. 5. There is a long and storied history of experimental stag hunt games under complete information. Beginning in the early 1990s, experiments featured a large number of periods to examine which of the pure-strategy equilibria would survive many rounds of repeated play. Under matching protocols designed to ensure no subject ever encountered the same subject again, Cooper et al. (1990, 1992) find subject play converged to the inefficient pure-strategy equilibrium. Combined with the results of similar coordination games with a greater number of equilibria (Van Huyck et al. 1990, 1991), the idea that strategic uncertainty doomed players to coordination only on low-payoff equilibria became prevalent. Robustness tests show that this result, while common, was not absolute (see Devetag and Ortmann 2007 for a survey). Altering payoffs to make the risky option relatively more attractive (Battalio et al. 2001; Clark et al. 2001; Dubois et al. 2012; Schmidt et al. 2003; Stahl and Van Huyck 2002), utilizing fixed rather than stranger matching (Clark and Sefton 2001), including preplay communication (Duffy and Feltovich 2002, 2006) and informing players about their opponent’s relatively high risk tolerance (Büyükboyacı 2014), all appear to increase coordination on the efficient equilibrium. Pairs who survey as being more patient are more likely to coordinate on the efficient equilibrium (Al-Ubaydli et al. 2013). The strongest evidence of subjects coordinating on the efficient equilibrium comes from Rankin et al. (2000), the basis for our complete information treatment. They report an experiment in which subjects play a sequence of 75 perturbed stag hunt games under complete information where payoffs, action labels, and game forms are changed in each period. They show that payoff dominance emerges as an equilibrium selection principle. Almost all subjects in the last 15 periods use threshold strategies that are close to 1, a payoff-dominant threshold; even when the value of Q is as large as 0.97, slightly more than half of the subjects select a risky choice. Our findings replicate these results. The \(2\times 2\) stag hunt game is also the basis for the initial theory of global games (Carlsson and van Damme 1993a). The same authors generalize this framework to a generic class of \(2\times 2\) games (Carlsson and van Damme 1993b). Building upon this work, Morris and Shin (1998, 2003) develop a more universal theory and the most popular application of global games theory, the currency attack game. In this game, usually modeled with more than two players, an individual has two choices: “attack” and “not attack.” Not attacking produces a sure payoff of 0. Attacking involves a sure cost of T, but may produce a benefit of Y, the global game parameter, provided enough other players also attack. The hurdle for a successful attack is determined by the non-increasing function h(Y). It is important to realize this hurdle function is the model’s major departure from global stag hunt games, as reaching the efficient or inefficient equilibrium—holding player choices constant—is independent from the global games parameter in the stag hunt game. Nonetheless, there are many similarities to the global stag hunt game. Both models feature two pure-strategy equilibria in complete information and a single global games threshold under incomplete information.Footnote 2
 Beginning with Heinemann et al. (2004), nearly all experiments on global games focus on some form of the currency attack model, rather than the stag hunt game. Their framework features a game of 15 subjects under two information conditions: complete and incomplete information about the value of Y. Subjects in their study play 10 independent situations in each of the 10 periods for a total of 100 situations. They present three main findings. First, estimated mean thresholds follow the comparative statics of the global games solution. Second, subjects select “attack” more often in sessions with complete information than in sessions with incomplete information and they coordinate more often than the theoretical predictions for both information conditions. The qualitative results of our data follow this second finding even though we use different games with different settings. Third, the variations in estimated thresholds among cohorts are large but are about the same for both information conditions despite the different theoretical predictions: equilibrium multiplicity for complete information and uniqueness for incomplete information. Our results are different from both Heinemann et al. (2004) and the theoretical predictions; we observe a larger standard deviation across cohorts with incomplete information than across cohorts with complete information. One interesting extension to the currency attack game is Szkup and Trevino (2017) who experimentally allow subjects choose the precision level at a cost.Footnote 3 In their experiment, subjects can select from six precision levels, where more precise signals are more costly. The results show that more than one-third of subjects select the equilibrium precision level, which yields the highest expected payoffs. For coordination, subjects who select signals with more precision coordinate more often than those who select less precise signals. We might view this costly information acquisition as a signal of coordination. In addition, when the precision level is exogenously given to them at no cost, subjects in their study behave similarly to subjects in our study. That is, subjects coordinate more often when observing more precise information. Schotter and Trevino (2017) examine subject response time in this framework. Other papers model and experimentally test the effects of receiving an additional public signal to a currency attack framework (Cornand 2006). Several make the game dynamic (Shurchkov 2013), including endogenous entry (Duffy and Ochs 2012), and exogenous public signals (Shurchkov 2016). Kawagoe and Ui (2010) examines how ambiguity differentially affects payoff and risk dominance predictions. It should be noted that these extensions to the currency attack game are generally done to understand and predict empirical macroeconomic phenomena rather than more basic questions on whether individuals naturally coordinate.Footnote 4 Nonetheless, several find little difference between incomplete and complete information treatments as we do. Some find coordination on the equilibrium predicted by payoff dominance (rather than risk dominance) as we do. However, given that very minor differences in the complete information stag hunt game can produce disparate results with regards to coordination on equilibria, it is unlikely one would be able to confidently infer results about the global stag hunt game from any of these studies using the currency attack game. To our knowledge, only two experiments examine the global stag hunt game. Brindisi et al. (2014) test differences between static and dynamic global stag hunt games, finding significant differences as their theory predicts. Their static treatment uses a variation of the global stag hunt where the parameter gives information about the risky payoff, rather than the safe as in ours. While it was by no means the point of their paper, similar to our study, they find subjects in sessions with incomplete information choose to coordinate more often than theory would predict. In a design, in many ways, very similar to ours, Cabrales et al. (2007) compare complete information stag hunt games with incomplete information stag hunt games. In sharp contrast to our results, they find that in both treatments subject play converges to the inefficient equilibrium, which under their parameters is the global games solution and risk-dominant threshold solution. (Specifically, their design only uses five discrete states, all of which, under complete information the risk-dominant criterion would select the secure strategy. So the risk-dominant threshold and the global games solution is a boundary condition.) Their complete information treatment does not perturb payoffs like Rankin et al. (2000). This creates two issues which make testing global games predictions more difficult: (i) the change between treatments involves both a change of payoff perturbations and information structures; (ii) the game under complete information was already likely to produce coordination on the inefficient equilibrium. Contrary to our results, the authors find support of the risk-dominant and global games solutions. However, the authors conclude that learning through payoff differences, rather than deducing iterated removal of dominated strategies is responsible for the movement to the global games and risk dominant solution. We find the differences between our results fascinating, and speculate on them in our final section.",6
21.0,3.0,Experimental Economics,11 June 2018,https://link.springer.com/article/10.1007/s10683-017-9550-3,A minimum effort coordination game experiment in continuous time,September 2018,Ailin Leng,Lana Friesen,Priscilla Man,Female,Female,Female,Female,"The majority of game theoretic models are in discrete time, where a single action is taken in each round. Accordingly, when these games are tested in experiments, a subject can make only one decision before the end of each period. In reality, though, decisions can often be made and changed asynchronously and swiftly—that is, in continuous time. Are discrete time models innocuous simplifications of these situations? Are the theoretical discrepancies between discrete and continuous time models (see Simon and Stinchcombe 1989) significant in experiments? This paper aims to answer these questions for a specific game: the minimum effort coordination game. Subjects in previous laboratory experiments of this game typically fail to coordinate at the efficient level, particularly when the group size gets large (Van Huyck et al. 1990; Devetag and Ortmann 2007; Camerer 2003). Is the failure to coordinate an artifact of the discrete time setting? Would allowing subjects to interact in continuous time help achieve the efficient coordination level? Is the difference (or the lack thereof) in coordination between discrete and continuous time settings related to the information structure? These questions are important for two reasons. First, it helps us to understand the minimum effort coordination game in its own right. This game is often used to model teamwork (in which a member’s extra effort cannot completely make up for her teammate’s shirking), a pervasive and important form of economic organization. Exploring the implications of discrete and continuous time modeling allows us to assess the applicability of this game to many real-life situations. Second, and more broadly, the minimum effort game is a stylized coordination game. It possesses features that are of interest for coordination games in general: multiple Pareto-ranked equilibria, the selection between the payoff-dominant (i.e. efficient) equilibrium and the risk-dominant equilibria, and the existence of a potential function (see Goeree and Holt 2005). Therefore, experimental results regarding the minimum effort game in continuous time will be informative for a wide range of coordination games as well. Previous laboratory experiments have shown differences in subjects’ behavior between continuous time and discrete time. Friedman and Oprea (2012) conduct an experiment on the continuous time prisoner’s dilemma and find median cooperation rates above 80% in their continuous time treatment, as opposed to nearly zero in their discrete time treatment. Recently, Bigoni et al. (2015) confirm these high cooperation rates in the continuous time prisoner’s dilemma, along with finding other behavioral differences (such as the response to stochastic termination rules) in discrete and continuous time. However, the ability to sustain cooperation in continuous time does not extend across games. In a four-player public goods experiment (Oprea et al. 2014), group contributions are not higher in continuous time unless free-form chat is available. Likewise, in a twelve-player Hawk–Dove game (Oprea et al. 2011), cooperation is not improved in the continuous time treatment. In light of these results, one may wonder how a continuous time set up would affect behavior in the minimum effort game, which, unlike these previously studied games, is a common interest game. On that note, Deck and Nikiforakis (2012) conduct an experiment on the minimum effort game in continuous time, but the payoffs in their experiment are determined only by the choices at the end of a period. As such, under their full information feedback treatment (“real-time monitoring” in their terminology), switching to a higher effort level before the end of the period is costless. In this regard, how individuals would behave when actions are consequential in continuous time remains an open question. To this end, we conduct an experiment on a minimum effort coordination game in (quasi-) continuous time, in which subjects are able to change their effort levels every 0.3 s.Footnote 1 We expect that subjects will reach a higher cooperation level in our continuous time treatment compared with our discrete time treatments. This is because, when the group is stuck in an inefficient low-effort equilibrium, one or two subjects may increase their effort temporarily. If other group members do not follow, the ability to switch effort quickly in continuous time means that these subjects can switch back to the group minimum with only a loss of a few cents. Thus the cost of unilaterally increasing effort is relatively small in continuous time. If other players follow, the group can reach a more efficient equilibrium and enjoy a higher payoff in the rest of the period (or go even further up). Moreover, previous experiments have shown that the payoff-dominant equilibrium can be reached when a minimum effort game is repeated for sufficiently many times (Berninghaus and Ehrhart 1998). Since a continuous time (coordination) game can sometimes be theoretically approximated by a sequence of discrete time games with the interval length going to zero (as modeled in Simon and Stinchcombe 1989), one may expect that the repetition will foster cooperation. Apart from the timing protocol, we also investigate the effect of the information structure on equilibrium selection. In previous discrete time experiments on the minimum effort game, it was standard to provide subjects only with information about the group minimum effort level (e.g., Van Huyck et al. 1990; Brandts and Cooper 2006a; Devetag and Ortmann 2007).Footnote 2 When more information is provided in discrete time, the effect on coordination is mixed (e.g., Van Huyck et al. 1990; Berninghaus and Ehrhart 1998; Devetag 2005; Brandts and Cooper 2006b; Engelmann and Normann 2010). However, in most continuous time settings (such as collaboration on a project using Dropbox), information can be gathered instantly. It is plausible that players can observe not only the coordination outcome but also the behavior of their team members. Can this information help? In our continuous time experiment, we include the full information treatment to achieve realism and the limited information treatment to compare with previous standard studies. Intuitively, one would expect that full information feedback should enable subjects to coordinate on simultaneous increases in effort levels. Alternatively they can signal their intention to move to a higher effort equilibrium by setting their own effort above the group minimum. Either way, it should be easier to reach the payoff-dominant equilibrium. The findings from the experiment, however, contradict our expectations. The timing protocol has no effect on the cooperation levels. Regardless of the type of information feedback, there are no statistically significant differences in the cooperation levels (either measured by the group minimum effort or by the group average effort) between the continuous time and discrete time treatments. The impact of the type of information feedback is mixed. In continuous time, the group average effort is significantly higher when full information feedback is provided, but the increase in the group minimum effort is statistically insignificant. Meanwhile, providing more information makes no difference in the discrete time setting. We probe the reasons behind our findings by comparing the within-period performance of the two continuous time treatments. Although the difference in the group minimum effort level is statistically insignificant between the two continuous time treatments, there are more upward switches of the group minimum in the full information treatment. This result indicates that with full information feedback, subjects signal by setting their effort level above the group minimum, which can induce increases in the group minimum effort level. Unfortunately, the average size of these increases is small. With limited information feedback, it is difficult to coordinate on a simultaneous increase in effort levels. As a result, the upward switches of the group minimum are more likely to occur at the start of a period than within a period, as the former provides a natural focal point for coordination. Meanwhile, subjects under limited information feedback have to increase their efforts more often for each upward switch of the group minimum. One interpretation of these observations is that continuous time with full information feedback is a form of costly and limited communication. When this channel is blocked, increasing individual effort is ineffective in increasing the group minimum. Our findings have several important implications. First, unlike the findings in Friedman and Oprea (2012), continuous time games are not always behaviorally similar to infinitely repeated games. Friedman and Oprea (2012) show that the cooperation rate in the prisoner’s dilemma experiment keeps increasing and approaches the continuous time level as the number of repetition increases in discrete time. The discrete time minimum effort coordination game has been shown to reach the payoff-dominant equilibrium as long as the number of repetitions is sufficiently large (Berninghaus and Ehrhart 1998), but we find no difference between our continuous time and discrete time treatments. In addition, the focal point provided by the start of each period can lead to a significant “restart effect”. Second, how payoffs are determined in a continuous time set up affects the coordination outcome. When payoffs are accumulated throughout each period, we find no significant difference between our continuous time and discrete time full information treatments. On the other hand, when Deck and Nikiforakis (2012) determine payoffs only by the last choice in each period, allowing subjects to constantly monitor the effort levels of the other group members does improve the cooperation level. Third, a finer action space appears to be more effective than a finer time interval in overcoming coordination failure. In our study, a continuous time frame fails to significantly change the coordination level. But in Van Huyck et al. (2007), a continuous action space significantly increases the number of groups converging to the efficient equilibrium. Fourth, performance in continuous time depends on the complexity of the game. While a continuous time setting can improve cooperation levels in a two-player prisoner’s dilemma (Friedman and Oprea 2012; Bigoni et al. 2015), its effect is much weaker in our six-person minimum effort game, as it has been in several other more complicated games (e.g. Oprea et al. 2011, 2014).",14
21.0,3.0,Experimental Economics,10 July 2017,https://link.springer.com/article/10.1007/s10683-017-9531-6,Equilibrium selection in similar repeated games: experimental evidence on the role of precedents,September 2018,John Duffy,Dietmar Fehr,,Male,Male,Unknown,Male,"One of the most important uses of the experimental methodology is to address questions of equilibrium selection in environments with multiple equilibria. As Van Huyck and Battalio (2008, pp. 454–455) observed: When these equilibria can be Pareto ranked it is possible for historical accident and dynamic process to lead to inefficient equilibria, that is coordination failure. Consequently, understanding the origin of mutually consistent behavior is an essential complement to the theory of equilibrium points. The experimental method provides a tractable and constructive approach to the equilibrium selection problem. Numerous experimental studies have explored questions of equilibrium selection in both static and repeated (dynamic) games (for surveys see, e.g., Ochs 1995 or Camerer 2003, Chapter 7). Typically, such questions of equilibrium selection have been studied using a single game played just once or repeatedly, though there are some papers exploring equilibrium selection across different games as discussed below in the related literature section. The novel question that we explore in this paper is whether equilibrium selection in one indefinitely repeated game with a multiplicity of equilibria serves as a precedent for equilibrium selection in a similar, indefinitely repeated game also having a multiplicity of equilibria.Footnote 1 That is, we are interested in whether there is a transfer of precedent to new, but similar indefinitely repeated game settings. This question is of real-world importance as it is likely that the payoffs that players face in indefinitely repeated strategic settings change over time and thus it is of interest to understand whether and how players adapt their behavior to such changes. It is also of interest to understand mechanisms that might support the play of efficient equilibria in such repeated game environments where many other equilibria are possible, and precedent (or history) is one mechanism that is relatively under-explored in the literature.Footnote 2 Our paper takes a first step toward addressing this question by combining the theory of repeated games with experimental economics methods. In particular, we consider indefinitely repeated play of versions of the \(2\times 2\) game \(\Gamma [T]\) shown in Table 1. In this game, the variable T denotes the “temptation” payoff. Our treatments consist of different values for T and the size of these differences provides us with a similarity measure across games. Note that if \(T>20\), \(\Gamma [T]\) is a Prisoner’s Dilemma game as strategy Y (equivalent to defection) is a dominant strategy for both players in the one-shot game. When \(10\le T <20\), \(\Gamma [T]\) is a Stag Hunt coordination game with two Pareto rankable pure strategy equilibria: the cooperative, efficient equilibrium where both players play X and the inefficient equilibrium where both players play Y. Under this same Stag Hunt game parameterization there also exists a mixed strategy equilibrium where X is played with probability \(\frac{10}{30-T}\). Our experiment will employ these two types of stage games, Prisoner’s Dilemma and Stag Hunt. All payoff values other than T in \(\Gamma [T]\) will be held constant across all of our experimental treatments; that is, treatments consist only of variations in T or in the order of the two types of stage games played, Prisoner’s Dilemma (PD) or Stag Hunt (SH). We study play of \(\Gamma [T]\) for various values of T under anonymous random matching and indefinite repetition. This is a stark though empirically plausible environment that has been the subject of much study with regard to mechanisms for sustaining social norms of cooperation among strangers. Here the mechanism we consider is historical precedent. Indefinitely repeated play of \(\Gamma [T]\) means that there can be multiple equilibria in the Prisoner’s Dilemma version of the stage game even under the random anonymous matching protocol for certain parameterizations of the environment. In particular, given the parameterizations of the PD versions of the game that we study and the continuation probability for indefinite play, it is possible to support play of the most efficient equilibrium where all players play the cooperative action X via the social norm, contagious strategy of Kandori (1992); the specific details are provided in Appendix A of the supplementary material. Prior research, for example, by Duffy and Ochs (2009) suggests that coordination on play of this efficient equilibrium when the stage game is a PD is not easily achieved under anonymous random matching in large populations of size 6 or 14, though it may have some success in smaller populations of size 4, as in Camera and Casari (2009). Here, we consider a population of size 10 and ask whether prior coordination on play of a pure strategy equilibrium in the repeated Stag Hunt game, either the inefficient equilibrium where all play Y or the cooperative, efficient equilibrium where all play X, serves as precedent for equilibrium selection in a subsequent repeated Prisoner’s Dilemma game involving the same population of 10 players holding the anonymous random matching protocol fixed across the two repeated games. We also explore this same question in the reverse order, i.e., whether a precedent for equilibrium selection in the repeated Prisoner’s Dilemma game carries over to the subsequent repeated Stag Hunt game again among the same population of anonymous, randomly matched players. Our main finding is that the role of historical precedents for equilibrium selection in the indefinitely repeated games that we study appears to be surprisingly limited. For small changes in the value of the temptation payoff parameter T, we observe large swings in the frequency with which players play the two actions available to them. More precisely, the frequency of play of the cooperative action, X, is stochastically larger in repeated play of the stage game with a Stag Hunt parameterization than in repeated play of the game with a Prisoner’s Dilemma parameterization, regardless of the order in which these two types of games are played. These swings in the frequency of cooperative play are associated with significant changes in subjects’ beliefs about the cooperative play of others. These beliefs are largely influenced by players’ initial propensities to cooperate as well as by the frequency of cooperative encounters in the early rounds of play. There is, however, some variation in the frequency of play of action X and beliefs about cooperation as the temptation parameter is varied, and there are some instances where a precedent of coordination on the inefficient, all play Y equilibrium in the repeated Prisoner’s Dilemma game carries over to the subsequent Stag Hunt game or the reverse (a precedent of the inefficient all Y play equilibrium in the repeated Stag Hunt game carries over to the subsequent Prisoner’s Dilemma game) though such transfer of precedent is not the norm. Importantly, we never observe that a precedent of efficient play, where all play X (as is often achieved in the repeated Stag hunt game), carries over to the subsequent Prisoner’s Dilemma game. We conclude that equilibrium selection results for indefinitely repeated games may be rather limited in scope to the particulars of the stage game being played and that there appears to be limited use of equilibrium selection precedents between similar indefinitely repeated games.",11
21.0,3.0,Experimental Economics,04 September 2017,https://link.springer.com/article/10.1007/s10683-017-9540-5,Fairness versus efficiency: how procedural fairness concerns affect coordination,September 2018,Verena Kurz,Andreas Orland,Kinga Posadzy,Female,Male,Female,Mix,,
21.0,3.0,Experimental Economics,10 February 2017,https://link.springer.com/article/10.1007/s10683-016-9508-x,Coordination with communication under oath,September 2018,Nicolas Jacquemet,Stéphane Luchini,Adam Zylbersztejn,Male,,Male,Mix,,
21.0,3.0,Experimental Economics,07 November 2016,https://link.springer.com/article/10.1007/s10683-016-9503-2,The role of cognitive ability and personality traits for men and women in gift exchange outcomes,September 2018,Emel Filiz-Ozbay,John C. Ham,Erkut Y. Ozbay,Female,Male,Male,Mix,,
21.0,3.0,Experimental Economics,04 July 2017,https://link.springer.com/article/10.1007/s10683-017-9532-5,Monitoring institutions in indefinitely repeated games,September 2018,Gabriele Camera,Marco Casari,,Female,Male,Unknown,Mix,,
21.0,3.0,Experimental Economics,12 April 2018,https://link.springer.com/article/10.1007/s10683-018-9568-1,Learning to alternate,September 2018,Jasmina Arifovic,John Ledyard,,Female,Male,Unknown,Mix,,
21.0,4.0,Experimental Economics,25 October 2017,https://link.springer.com/article/10.1007/s10683-017-9546-z,Context matters,December 2018,Wenting Zhou,John Hey,,Unknown,Male,Unknown,Male,"Risk attitude is a crucial factor influencing economic behaviour. As a consequence, experimenters are interested in eliciting the risk-attitude of their subjects. This can be done in two ways: either directly, using the context of a particular experiment to estimate the risk-aversion that best explains behaviour; or indirectly, eliciting risk aversion in a separate part of the experiment, and using the elicited value to explain behaviour in the main experiment. It is to those experimenters that use the latter approach to whom this paper is addressed. Economic theory posits that decisions under risk depend on how people evaluate, and hence decide between, risky lotteries. By these we mean lotteries where the outcomes are risky, and where the probabilities are known. Clearly how people evaluate lotteries depends not only on the lotteries, but also on the preference functionals of the decision-maker (DM). In the literature there are a number of proposed preference functionals, the best-known of which is the Expected Utility functional. All of these embody the idea of an underlying utility function u(.); it is the degree of concavity of this when it is defined over money that indicates the degree of risk-aversion. It is this that we are trying to elicit. There are a number of methods that are used in the literature to elicit risk aversion. Possibly the most popular is that known as the Holt–Laury Price List, introduced by Holt and Laury (2002), and which we will refer to as HL. While the detail may vary from application to application, the basic idea is simple: subjects are presented with an ordered list of pairwise choices and have to choose one of each pair. The list is ordered in that one of the two choices is steadily getting better or steadily getting worse as one goes through the list. There are many variants on the basic theme: sometimes one of the two choices is a certainty, and that is getting better or worse through the list; sometimes either just one or both of the choices are risky choices and one of them is getting better or worse through the list. Because of the ordered nature of the list, subjects should choose the option on one side up to a certain point thereafter choosing the option on the other side. Some experimenters force subjects to have a unique switch point; others leave it up to subjects.Footnote 1
 A second method is to give a set of Pairwise Choices, but separately (not in a list) and not ordered. We will refer to this as PC. Typically the pairwise choices are presented in a random order. This has been used by Hey and Orme (1994), amongst many others. Some argue that this method, whilst being similar to that of Price Lists, avoids some potential biases associated with ordered lists. Usually the pairwise choices are chosen such that they are distributed over one or more Marschak-Machina triangles.Footnote 2
 A method which is elegant from a theoretical point of the view is the Becker–DeGroot–Marschak mechanism proposed by Becker et al. (1964). This we will later denote by Lottery Choice (LC) because of the way that we implement it. The method centres on eliciting the value to a subject of a lottery—if we know the value that a subject places on a lottery with monetary outcomes, we can deduce the individual’s attitude to risk over money. There are two variants of this mechanism that are used in the literature: one where the DM is told that they own the lottery, and hence have the right to play it out or to sell it; and one where the DM is offered the chance to buy the lottery, and, if so, to then play out the lottery. The subject’s valuation of the lottery as a potential seller is the minimum price for which they would be willing to sell it, while the subject’s valuation of the lottery as potential buyer is the maximum price for which they would be willing to buy it. Here we describe the mechanism as it relates to a potential buyer—the mechanism is the same, mutatis mutandis, if it relates to a potential seller. The subject is asked to state a number; then a random device is activated, which produces a random number between the lowest amount in the lottery and the highest amount. If the random number is less than the stated number, then the subject buys the lottery at a price equal to the random number (and then plays out the lottery); if the random number is greater, then nothing happens and the subject stays as he or she was. IfFootnote 3 the subject’s preference functional is the expected utility functional then it can be shown that this mechanism is incentive compatible and reveals the subject’s true evaluation of the lottery. Analytically it is equivalent to a Second Price Auction. The problem is that subjects do seem to have difficulty in understanding this mechanism, and a frequent criticism is that subjects understate their evaluation when acting as potential buyers and overstate it when acting as potential sellers. So in our experiment we implement this mechanism in a new way, which, we hope, is easier for subjects to understand, and which we will explain later. The Allocation method, which we shall denote by AL, was originally pioneered by Loomes (1991). It was then revived by Andreoni and Miller (2002) in a social choice context, and later by Choi et al. (2007) in a risky choice context. This method involves giving the subject some experimental money to allocate between various states of the world, with specified probabilities for the various states, and, in some implementations, with given exchange rates between experimental money and real money for each of the states. This method seems easier for subjects to understand than BDM. This paper is a follow-up, and complement to, the previous literature and, in particular, the paper by Loomes and Pogrebna (2014), in which the authors compare three of the elicitation methods described above—specifically Holt–Laury price lists, Ranking and Allocations. Our paper complements theirs, not only in the elicitation methods we consider, but also in that our experimental design (and crucially the numbers of questions asked for each method), as well as the data analysis, are completely different. We also consider a slightly different set of elicitation methods. The purpose of this paper is to report on the results of an experiment in which subjects were asked to perform each of the four methods described in Sect. 1 above. The paper is organised as follows. We start with a literature review of the various elicitation methods in Sect. 2, and a survey of previous experimental results in Sect. 3. In Sect. 4 we describe how our experiment was organised and how the various methods were implemented in it, giving more detail about each of the methods. As we adopt an econometric methodology of fitting preference functionals to the data, we specify, in Sect. 5, the preference functionals that we fit to the data and describe the functional forms that we assume, and the parameters in them that we estimate. In Sect. 6, we describe how we analysed the data, detailing the stochastic assumptions that we make. Section 7 contains the results. In Sect. 8 we discuss what else we might have done; and Sect. 9 concludes.",23
21.0,4.0,Experimental Economics,01 November 2017,https://link.springer.com/article/10.1007/s10683-017-9547-y,On the cultural basis of gender differences in negotiation,December 2018,Steffen Andersen,Seda Ertac,Sandra Maximiano,Male,Female,Female,Mix,,
21.0,4.0,Experimental Economics,02 November 2017,https://link.springer.com/article/10.1007/s10683-017-9551-2,Reciprocity under moral wiggle room: Is it a preference or a constraint?,December 2018,Tobias Regner,,,Male,Unknown,Unknown,Male,"A series of studies, started by Dana et al. (2007), established that the pro-social behavior we observe in dictator games is inconsistent with distributional preferences. Instead, it seems that giving is to a substantial part the consequence of a transparent relationship between the dictator’s choice and the recipient’s outcome. If situational excuses for selfish behavior exist, people seem to make use of that ‘moral wiggle room’ and giving tends to be smaller. However, the dictator game is a setting in which giving has been found to be easily affected by variations of the context or framing.Footnote 1 What if ties between giver and taker are stronger? Would the effect of moral wiggle room prevail, if the relationship is not merely between a dictator and recipient but embedded in a richer environment, say, with social interaction between the two? For instance, having been trusted in the first place may introduce a motivation (reciprocity) strong enough to overcome the trustee’s tendency to exploit moral wiggle room. Two recent studies explore whether moral excuses affect behavior also in the context of reciprocity. van der Weele et al. (2014) use a between-subjects design and apply the ‘plausible deniability’ treatment from Dana et al. (2007) to second-mover behavior in a trust/moonlighting game. Compared to a baseline they find no behavioral differences and conclude that moral wiggle room has no effect on the incidence of reciprocal behavior. Regner and Matthey (2017) employ a within-subjects trust game design in which uncertainty about the back transfer implementation is varied. They identify reciprocators and find that 40% of them exploit moral wiggle room. With the two existing pieces of evidence pointing in different directions, it remains unclear whether reciprocal behavior is indeed partly due to people feeling obliged to do so or whether it is entirely the result of people liking to reciprocate. Both studies deviate from the moral wiggle room design of Dana et al. (2007). Thus, in order to test to what extent reciprocity results from a preference or a constraint, we decide to closely follow the original design of Dana et al. (2007). Essentially, we turn their binary dictator game into a binary trust game by adding a preceding stage in which the trustor chooses between a safe outside option and entering the game. We compare this baseline trust game to three trust treatments with moral wiggle room manipulations. We also conduct a dictator game condition (baseline and one treatment) in order to compare our results to the original setting without reciprocity. Behavior in the trust baseline (6.25%) is significantly less selfish than in the dictator one (33.33%), indicating that trustees tend to reciprocate trust, while we find significantly more selfish choices in the trust treatments (between 37.5 and 45%) than in the trust baseline. Our results suggest that the insight from the series of moral wiggle room studies in the dictator game setting—observed giving is not only due to a preference to give but also due to some subjects feeling obliged to do so—can be generalized to broader contexts. This is important, because many real life interactions are probably enriched with some degree of morally relevant information about the counterpart(s). While looking at dictator games made sense to identify the effect of moral wiggle room in an abstract setting, our findings suggest that the impact of moral wiggle room on our everyday choices extends beyond the real life equivalent of a dictator game. Thus, our study is a first step to a better understanding of moral wiggle room effects in real life. The paper is organized as follows. The next section reviews the related literature. In Sect. 3 we describe the experiment and present behavioral predictions. Results are reported in Sect. 4 and discussed in Sect. 5. Section 6 concludes.",15
21.0,4.0,Experimental Economics,09 November 2017,https://link.springer.com/article/10.1007/s10683-017-9552-1,Liking what others “Like”: using Facebook to identify determinants of conformity,December 2018,Johan Egebark,Mathias Ekström,,Male,Male,Unknown,Male,"The observation that people often follow others’ behavior—whether it is about what car to buy, which candidate to vote for in public elections or how much tip to give in a restaurant—has captivated social scientists for centuries. Sherif (1936) was first to establish the existence of conformity; a causal link between group behavior and the behavior of individuals. In this paper, we seek to understand what makes conformity take-off in the first place. In particular, we set up a field experiment on the world’s leading social networking service, Facebook, to examine how small changes in the size of the influencing group, and the introduction of social ties between the source and the target, affect behavior. Facebook enables users to communicate with their network by writing short messages, so called status updates. Immediately after a status update has been posted, anyone may react to it, either by writing their own comment or by pressing a Like button to show their appreciation. Using existing Facebook accounts, we post authentic status updates during a 7-month period. For every update, we randomly assign subjects (i.e., the members of a specific Facebook network) to either a treatment or a control group. Both groups are exposed to identical status updates. However, the treated individuals observe the update after it has been Liked by either: (1) one stranger, (2) three strangers or (3) one friend. By systematically varying the treatment intensity, we are able to identify two potential determinants of conformity. Comparing conditions (1) and (2) determines the importance of the number of predecessors, whereas comparing conditions (1) and (3) tests whether social ties matter. The results show that observing one Like from a single stranger had no impact on the number of subsequent Likes. However, increasing the size of the influencing group to three strangers doubled the probability that a subject Liked the update. Friendship was also decisive: people were four times more likely to press the Like button if a friend, rather than a stranger, did so before them. The results are robust to different specifications and apply to both men and women. Moreover, the treatment effects are both content and sender independent, which suggests they apply more broadly. Why do people conform in this setting, and, more importantly, why is group size and social ties crucial? To understand this better, we turn to existing theories of conformity. One line of research assumes that individuals imitate those who are believed to be better informed about quality to promote their own, monetary, self-interest—so-called rational herding (Bikhchandani et al. 1992, 1998). Another strand recognizes that people are motivated by “social” factors, such as the desire for prestige, esteem, popularity, or acceptance, and that these social factors also can produce conformity (Akerlof 1980; Jones 1984; Bernheim 1994). At the face of it, rational herding seems far-fetched in our context. The obvious reason being that choices in our experiment—whether or not to press the Like button—are made after subjects have read the status update, and been able to evaluate it against comparable alternatives. In essence, uncertainty about quality is, arguably, negligible. That said, by incorporating uncertainty about norms into the rational herding models (as a complement to quality) we think both lines of research can add insights to the current findings. First, with respect to rational herding, the model by Banerjee (1992) outlines why both larger groups, and trusted individuals (friends in our case), can provide more accurate information about the appropriate behavior in a given situation. Second, with respect to social status, there is a psychological literature suggesting that larger groups, and in-group members (friends in our case), can put more pressure on individuals (Asch 1952; Tajfel 1981). While Becker (1974) revitalized the study of social interactions within the economics discipline, empirical research is still lagging behind. As put forward by Manski (2000), this is mostly because identification has been too challenging. Manski’s main conclusion is that randomized experiments are needed to understand these issues better. A growing experimental literature on peer effects has started to fill this gap (see, e.g., Duflo and Saez 2003; Falk and Ichino 2006; Goldstein et al. 2008; Cai et al. 2009; Chen et al. 2010; Ayres et al. 2013). We add to this literature by offering a precise definition of what we mean by a peer. Rather than using vague definitions (e.g., someone employed at the same workplace) we have an objective measure of mutual friendship. What is truly novel is that we carefully compare the influence of friends to the influence of random strangers. The experiment also adds to the current debate regarding the effect of group size on conformity (Bond 2005). Social psychologists agree that the size of the influencing group should have an impact on the individual, but whether the exact functional form is linear, concave, or S-shaped with tipping points is still up for discussion. That we find no effect of one stranger but a substantial effect of three strangers gives further support for an S-shaped relationship. Another important contribution is the fact that subjects in our experiment act in their natural environment, thus being completely unaware of the fact that they are part of an experiment (Al-Ubaydli and List 2015). The large degree of conformity in the present study thus allows us to conclude that decades of social influence research from lab settings, including the influential studies by Asch (1956) and more recent contributions such as Goeree and Yariv (2015), cannot be dismissed as an artifact shaped by suspicious subjects, strange environments, or influential experimenters. Finally, the threshold effects found in our experiment may contribute to a broader understanding of the dynamics of conformity. Importantly, we show that single individuals are indeed influential, but only among friends within a network. This implies that contagion can take off from a single node and evolve endogenously from peer to peer. We also demonstrate that once a sufficient number of peers have adopted the behavior, they will affect people outside the network. Hence, size and source may serve as complements in the proliferation of behavior—source increases penetration whereas size generates dispersion.",16
21.0,4.0,Experimental Economics,17 November 2017,https://link.springer.com/article/10.1007/s10683-017-9553-0,Risk breeds risk aversion,December 2018,Tai-Sen He,Fuhai Hong,,Unknown,Unknown,Unknown,Unknown,,
21.0,4.0,Experimental Economics,13 November 2017,https://link.springer.com/article/10.1007/s10683-017-9554-z,Disapproval aversion or inflated inequity acceptance? The impact of expressing emotions in ultimatum bargaining,December 2018,Josie I. Chen,Kenju Kamei,,Female,Unknown,Unknown,Female,"In recent decades, economists have devoted considerable efforts to studying the impact of expressing emotions on people’s behavior when there is complete information and have found that emotional expression may affect both the senders and the recipients of the expression. On the one hand, it has been documented that people have preferences against receiving disapproval from others. Consequently, they behave pro-socially to ensure that they do not receive negative feedback.Footnote 1 On the other hand, expressing emotions has also been known to affect the behavior of the senders of those emotions. For example, in a one-shot ultimatum game, buyers (responders) are more likely to accept unfair offers when given opportunities to express emotions (e.g., Xiao and Houser 2005; Güth and Levati 2007). This finding suggests that expressing negative emotions is a substitute for punishing matched sellers (proposers), which thus increases buyers’ inequity acceptance. However, which effect is more dominant when there is a rating system present in ultimatum bargaining: buyers’ inflated inequity acceptance or sellers’ disapproval aversion? Does the relative strength of these two effects differ by information condition? This paper is the first to study how the impact of expressing emotions differs according to whether the size of the pie is common knowledge to all players (complete information) or is only known to sellers (incomplete information). Although past studies have used complete information setups to study the effects of expressing emotions, understanding such effects under the incomplete information setting is equally important for two particular reasons. First, the incomplete information setup is more realistic under some circumstances, in which sellers are better informed than buyers about the products they sell. On the one hand, such price settings as foods in grocery stores and standard items such as pens, university textbooks and music CDs in physical stores or on the online marketplace (e.g., Amazon.com) can be described as buyers having complete information. On the other hand, some transactions can be best described by containing incomplete information on the buyers’ side. Examples include used products, medical services, and education services such as higher education. Users usually become aware of the quality of the goods and services only after they have purchased or consumed them. A rating system is available in some cases (e.g., standard items or used products on the online marketplace, lectures at universities), but not for other cases (e.g., goods in grocery stores, used items in classified ads such as Craigslist). Second, asymmetry of information between sellers and buyers is known to change the picture of the bargaining between them. Many experiments with complete information have demonstrated that people prefer fair outcomes in ultimatum games (for a survey, see Roth 1995). At the same time, however, past studies have shown that in incomplete information setups (a) sellers can become greedier and their offering prices can be close to what standard theory predicts and (b) buyers are more likely to accept unfair offers.Footnote 2 These results may extend to an environment with a rating system. Moreover, the presence of a rating system may make buyers open to even more unfair offers with incomplete information. Buyers behave conservatively to avoid the disappointment that they may experience when the realized size of the pie is lower than their expectation. However, with a rating system present, buyers can release such negative emotions by using ratings; as such, buyers do not need to lower their acceptance level of inequity due to disappointment. Our experiment is based on a finitely repeated ultimatum game. We design four treatments by varying two dimensions. The first dimension is constituted by whether buyers are given the opportunity to rate sellers or not. Sellers are informed of their own ratings after the transactions have been completed. The ratings are not disclosed to other group members and are not carried over from period to period.Footnote 3 The second dimension is constituted by whether buyers are informed of the size of the pie or not (i.e., complete versus incomplete information). In each treatment, subjects are randomly assigned the role of either seller or buyer. Seller j has one commodity, the value of which is randomly drawn from integers between 0 and 40, and is then randomly matched with a buyer i. Seller j submits an offering price (p

sj
) and buyer i submits a purchase threshold (p

bi
). If p

sj
 ≤ p

bi
, then the transaction between i and j is closed. We first theoretically describe how bargaining between seller j and buyer i could result in more unequal divisions under incomplete information than under complete information conditions. We then describe how a fairer or a less fair situation could hold as an equilibrium outcome with a rating system if players are inequality averse and sellers exhibit disapproval aversion. We then show that with a rating system present, buyers’ inflated inequity acceptance could dominate sellers’ disapproval aversion when the buyers are not aware of the size of the pie (incomplete information), because buyers dislike disappointment resulting from possibly a lower-than-expected size of the pie. Our experiment results largely confirm the theoretical analyses regarding social disapproval aversion and disappointment aversion. First, the divisions of the pies were much more unequal with incomplete than with complete information. Second, sellers exhibited disapproval aversion with complete information, which is consistent with past research. Specifically, the sellers attempted to keep smaller shares of the pies when the rating system was available, compared with when it was not available. Third, and in sharp contrast, with incomplete information, sellers’ disapproval aversion did not affect their bargaining behavior. Whether or not the rating system was present, sellers aggressively attempted to take more from their buyers. Instead, buyers displayed greater acceptance of inequity when the rating system was present than otherwise. The enhanced buyers’ acceptance of unfair offers increased the inequality in the divisions of the pies. In short, our paper suggests that a rating system may have opposite effects depending on the information conditions. The rest of the paper proceeds as follows: Sect. 2 describes our experimental design. Section 3 provides theoretical considerations. Section 4 reports the experiment results, while Sect. 5 concludes.",1
21.0,4.0,Experimental Economics,14 November 2017,https://link.springer.com/article/10.1007/s10683-017-9555-y,Indirect higher order beliefs and cooperation,December 2018,Jiabin Wu,,,Unknown,Unknown,Unknown,Unknown,,
21.0,4.0,Experimental Economics,15 November 2017,https://link.springer.com/article/10.1007/s10683-017-9556-x,Status and the demand for visible goods: experimental evidence on conspicuous consumption,December 2018,David Clingingsmith,Roman M. Sheremeta,,Male,Male,Unknown,Male,"The idea that human beings are motivated to attain social status and to display evidence of status to others has a long history in economics (e.g. Smith 1776; Veblen 1899; Duesenberry 1949). Status is a form of socially-recognized and individually desired esteem, honor, or dignity. In their review of recent empirical work on social status, Heffetz and Frank (2011) identify two additional features of status: It is positional, meaning that what matters is how an individual is ranked relative to others, and it is non-tradable, meaning that it cannot be purchased in a market. Social status is related to an individual’s attributes, such as intelligence, creativity, beauty, affiliation, or family of origin. Veblen argued that social status has a profound influence on a person’s consumption decisions. His book The Theory of the Leisure Class contends that status concerns affect the consumption choices of anyone whose income places them above the level of subsistence (1899). Social conventions specify a minimum standard of clothing, food, and living conditions that are acceptable for each status level. Since social status and income are positively correlated, the acceptable standard of consumption for those of higher status includes more and better goods than for those of lower status. According to Veblen, an important function of consumption is to signal high status to others. Consumption choices can only do this to the extent they are both visible to others and associated with high status. Part of the motivation for wearing a fine suit or driving a luxury car, both of which are visible to others, is to convey the message to others that one has high status. Consumer goods vary in the degree to which the act of consuming them is visible to the public, and thus in their suitability for serving as public markers of status. Many people see us when we are in our cars, when we wear our work clothes, or when we eat at a restaurant. Far fewer see our sleepwear, what we have for breakfast at home, or the brand of toilet paper we buy.Footnote 1 Veblen termed status signaling via the acquisition and display of visible goods conspicuous consumption.Footnote 2
 A number of studies have presented evidence about the relationship between status and the consumption of visible goods (Ravina 2007; Grinblatt et al. 2008; Charles et al. 2009; Heffetz 2011; Kuhn et al. 2011). However, there are several difficulties in identifying conspicuous consumption as a motivation for consuming visible goods. First, visibility is only one of many properties possessed by any given good that contribute to the observed demand for it. It is difficult to disentangle demand for visibility from demand for these other properties. While we may conjecture that a person buys a Mercedes rather than a Toyota to signal high social status, a Mercedes is a superior car in many ways besides the signal it may send about the status of its owner. A second, subtly related problem is the link between income and social status. Income has effects on consumption that are independent of any status motivation, implying that observed correlations between status and consumption could be pure income effects. Income also tends to be correlated with various characteristics that confer social status through popular esteem, such as intelligence, education level, family background, profession, and political clout. Lastly, it is difficult to disentangle conspicuous consumption from social learning as factors that drive individuals with similar social status to make similar consumption decisions (Grinblatt et al. 2008). Some studies have examined experimentally the question of status signaling in the context of charitable giving and provision of public good (Andreoni and Petrie 2004; Soetevent 2005; Samek and Sheremeta 2017, 2017). Most of these studies find that recognizing donors by revealing their identities increases donations to charities and contributions to public good. However, it is not clear whether such change of behavior is due to social status (Glazer and Konrad 1996; Hopkins and Kornienko 2004), or due to other factors, such as a desire to be seen as generous (Ariely et al. 2009; Benabou and Tirole 2006; Andreoni and Bernheim 2009) or to avoid being seen as stingy (Bracha and Vesterlund 2017; Samek and Sheremeta 2014), or perhaps a purely altruistic desire to set an example for others to follow (Karlan and McConnell 2014). We tackle the challenge of identifying conspicuous consumption by conducting a controlled experiment in the context of physical product consumption. In our experiment, individuals have an opportunity to purchase a desirable physical good: gourmet chocolate truffles. After accruing income, each participant indicates their desired quantity of chocolate truffles for each of several potential prices. A common price at which sales are actually made is randomly selected from the potential prices at the end of the experiment. Since buying chocolate provides only private benefit to the person who purchased it, we are able to study conspicuous consumption without confounding factors of generosity and altruism, which are present in other studies (Andreoni and Petrie 2004; Samek and Sheremeta 2014, 2017).Footnote 3
 We manipulate the purchase of chocolate truffles in two ways, using a two-by-two design. In the first manipulation, income is either assigned based on a participant’s rank or randomly. A participant’s rank is determined how well they do relative to the eleven other participants in their experimental session on a 30-min cognitive test. Our participants are students at Case Western Reserve University (CWRU). As at other elite universities, cognitive ability confers social status at CWRU.Footnote 4 When income is assigned by rank on the test, it is correlated with this socially esteemed capability. When income is assigned randomly, it is not correlated with any attributes. However, to recognize the possibility that even randomly assigned income may confer some status, we introduce a distinction between the endogenous social status, which is what scholars typically mean when they refer to social status, and which in this experiment is based on cognitive test performance, and exogenous social status, which results from random assignment. In the second manipulation, communication about the quantity of truffles purchased is either private, so that only the participant and experimenter know, or public, so that all participants in the experimental session can see how much each purchased. Participants know how their choices will be communicated before making them. We refer to the four treatments as rank-private, rank-public, random-private, and random-public. Our design allows us to isolate the effect of visibility on demand since all other properties of the chocolate truffles are identical across the public and private treatments. We can also isolate the effect of the linkage between income and status. We can rule out social learning as a driver of consumption decisions within the experiment because our participants do not interact and have no information about the choices of others when they make their decisions. Veblen’s theory of conspicuous consumption predicts that demand will be higher in rank-public than rank-private because when status is linked to income, visibility leads people to consume more to signal status. To the extent that exogenous status is not valued by participants, Veblen’s theory also predicts that demand will be the same in random-public and random-private. Consistent with these predictions, we find that making consumption choices publicly visible leads to a large increase in demand when income is linked to endogenous status, but not when income is assigned randomly. In other words, we find that the necessary conditions for conspicuous consumption are (1) for income to be correlated with status and (2) for consumption choices to be publicly visible to others. The effect is quite large: mean quantity demanded is 1.94 truffles in rank-private and 4.98 truffles in rank-public, an increase of 257%. When income is unrelated to status, visibility does not induce conspicuous consumption: mean quantity demanded is 1.74 in random-private and 1.75 in random-public. Although our data provide support for a hypothesis that status is a significant factor motivating consumption of visible goods, the relationship between status level and conspicuous consumption is non-monotonic.Footnote 5 Therefore, a person’s income (and thus the social status) cannot be easily inferred from their actual chocolate choice in rank-public. We find that gender and cognitive reflection are important mediators of conspicuous consumption. Men engaged in conspicuous consumption much more than women. Quantity demanded by men is 477% higher in rank-public than rank-private. Individuals who scored high on a measure of cognitive reflection (Frederick 2005), which is the propensity to engage in conscious deliberation when a situation requires it, also engage more in conspicuous consumption. We find no impact of risk aversion or competitive social preferences on conspicuous consumption. Publicly visible choice causes participants to buy chocolate truffles at higher prices than they would have otherwise. By comparing the demand curves in rank-private and rank-public, we can estimate both the rank-private consumer surplus and the average welfare loss from making consumption public. We find that the average welfare loss is as large as the consumer surplus when the price of chocolate is $0.40, which is approximately the retail price. The negative effect of conspicuous consumption on welfare loss comes primarily from men, who account for most of the conspicuous consumption. For women, the loss is much smaller and insignificant. The non-monotonic relationship between consumption and income means that public consumption does not convey a credible signal about status, which means that the welfare loss from conspicuous consumption is not compensated by signaling value of such consumption. To investigate whether participants derive utility from conspicuous consumption unrelated to signaling, we asked them to rate their mood at the end of the experiment. For men, we find no difference in self-reported mood between rank-public and rank-private. To the extent that our simple mood measure captures the non-consumption externalities of status signaling, it provides suggestive evidence that the net welfare effect of conspicuous consumption is negative for men. Women’s moods are low in rank-private, so the net welfare effect of conspicuous consumption for them is likely positive. However, it is possible that our measure does not fully capture the non-consumption externalities of status signaling. We discuss some possibilities in the conclusion. We describe the experimental design and procedures in Sect. 2. Our main results are presented in Sect. 3, along with analyses of how conspicuous consumption is related to the level of status, the characteristics of those who engage in conspicuous consumption, and the welfare effects of conspicuous consumption. We discuss connections to the literature and implications of our results in Sect. 4.",18
21.0,4.0,Experimental Economics,11 December 2017,https://link.springer.com/article/10.1007/s10683-017-9557-9,Transmission of information within transnational social networks: a field experiment,December 2018,Natalia Candelo,Rachel T. A. Croson,Catherine Eckel,Female,Female,Female,Female,"Poverty and violence in developing countries have generated rapid growth in migration to more developed countries over the past 15 years.Footnote 1 Several studies have shown that country-level measures of the size of immigrants’ social networks impact country-level investment decisions in the home country.Footnote 2 Immigration scholars have speculated about the causal mechanism for these findings, but a lack of microdata at the individual level makes conclusions difficult. Scholars have suggested (but not demonstrated) that information transmitted via transnational social networks can affect investment decisions in the home country via social influence (Bauer et al. 2002). This paper experimentally supports this suggestion, providing direct evidence on how information transmitted via social networks across national boundaries influences financial decisions of individuals in the home country. We limit our attention to a 2 × 2 experimental design in which we manipulate the type of information transmitted (advice/decision) and the source of the information (family/friends) of a transnational social network. An individual’s social network, the set of people she knows, has been shown to predict a wide variety of economic and social behavior (Granovetter 1973, 1975; Rosenblat and Mobius 2004; Alcott et al. 2007; Karlan et al. 2009; Leider et al. 2009). Previous research has demonstrated that individuals use social networks to collect information and to help them make decisions that affect current consumption (Mobius et al. 2005), contribute to the alleviation of poverty (Bertrand et al. 2000), and impact their financial decisions (Bursztyn et al. 2014). We argue that the transmission of information within transnational social networks impacts investments in the native country. Immigrants in the destination country communicate information to individuals in the native country, and individuals in the native country use this information when facing investment decisions at home. We begin by modeling the decisions of individuals at home as decisions under risk (Sjaastad 1962). We use the social influence literature to generate predictions about the types of information that will be shared in the social network regarding these decisions. We then test these predictions using a lab-in-the field experiment with current Mexican immigrants and their home-based social network members. We examine the supply of information that current immigrants send to those at varying levels of social distance from themselves. We also elicit the demand for information, which reflects preferences from those at home about the information they could potentially receive from current immigrants in the destination country. We further examine the use of information for subsequent risky decision-making in the home country. Previous literature on social influence on financial decision making has identified two types of information that may be relevant for risky decisions (Bertrand et al. 2000; Bursztyn et al. 2014) and which we use in our experiment. The first is decision information. In our setting, this would involve observing current immigrants’ actions before making one’s own choice at home. The second is advice. In our setting, this would involve advice given from the current immigrant to the individual at home about the investment decision they were about to make. In our study, we will examine the effect of these two types of information as they are shared within the social network. This paper makes several important contributions. First, while previous papers have examined the impact of social networks on a variety of behaviors (e.g. consumption, Mobius et al. 2005; welfare program take-up rates, Bertrand et al. 2000; engagement in financial decisions, Bursztyn et al. 2014; likelihood of emigrating, Munshi 2003; obtaining a job, and getting higher wages, Amuedo-Dorantes and Mundra 2007; Massey and Espinosa 1997), we are the first to examine the impact of transnational social networks on risky behavior in the context of immigration. Second, while previous papers have examined the impact of information from peers or strangers on risky decisions (Cooper and Rege 2011; Bougheas et al. 2013), this study is the first to examine how information from one’s social network affects decisions under risk. Third, while previous papers investigate only others’ decisions or advice as the information transmitted to peers or strangers in investment decisions (Cooper and Rege 2011; Bougheas et al. 2013), we are the first to examine both mechanisms through which social networks might affect investment decisions. The results from this study contribute to our understanding of information transmission via social networks more generally, and have the potential to help us explain and predict when individuals will choose to invest in risky settings given the information they acquire from their social networks, even beyond the immigration domain. This paper continues as follows. Section 2 discusses relevant literature on transmission of information within social networks, social influence and risky investments, and presents the predictions. Section 3 describes the experimental design and procedures using participants in the US and Mexico. Section 4 presents the experimental results. Section 5 concludes.",2
21.0,4.0,Experimental Economics,14 December 2017,https://link.springer.com/article/10.1007/s10683-017-9558-8,Auctions with endogenous participation and an uncertain number of bidders: experimental evidence,December 2018,Diego Aycinena,Lucas Rentschler,,Male,Male,Unknown,Male,"Consider a government official that seeks to privatize an asset via an auction. If the aim is to maximize the government’s revenue, there are many practical issues to consider when choosing the rules of the auction (i.e., mechanism, information to be disclosed, etc.).Footnote 1 This is especially true if it is costly for bidders to participate and consequently the seller faces uncertainly about the number of bidders that a given set of rules can attract. As noted in Klemperer (2002), “[A] major area of concern of practical auction design is to attract bidders, since an auction with too few bidders risks being unprofitable for the auctioneer.” The revenue generated in an auction depends on entry decisions and bidding behavior in the chosen mechanism. A robust finding from the experimental literature is that, all else constant, first-price auctions generate more revenue than English clock auctions, largely due to persistent overbidding in the former.Footnote 2 This implies that bidder payoffs tend to be higher in English clock auctions. If potential bidders are able to anticipate this, one might expect that when given the choice, they would be more likely to enter an English clock auction, thus increasing its relative revenue. Will such entry decisions equalize the expected revenue of these two formats, or even cause the English clock auction to revenue dominate? The answer to this question is of practical concern to an auction designer who must consider whether or not a particular mechanism will appeal to potential bidders. Furthermore, revenue may also depend on whether or not bidders know the actual number of entrants when formulating their bids. When potential bidders know their value of the good prior to deciding on entry, they may use this information to select into the auction. Thus, disclosing the number of entrants reveals information about the distribution of values of other participants. With this information, bidders can adjust their strategy accordingly. Given these considerations, which mechanism and information revelation policy is optimal for the auction designer? We experimentally investigate these questions in independent private value environments where potential bidders know their value and a common opportunity cost of participating when they make their entry decisions. We vary the mechanism within subjects between first-price and English clock auctions. We vary whether or not bidders are informed of the number of entrants on a between subject basis. In this paper we focus on the case of a small number of potential bidders because this is when the effect of attracting an additional bidder will have the largest effect on revenue. That is, the importance of the auction rules in determining revenue via entry decisions is highest when the number of potential bidders is low.Footnote 3,Footnote 4
 Theory predicts revenue equivalence between these four environments. However, we find that, regardless of whether or not bidders are informed, first-price auctions generate more revenue than English clock auctions. Further, the effect of informing bidders differs across auction format. In English clock auctions, revenue is increased by informing bidders, and the opposite is true in first-price auctions. As such, our results suggest that an auction designer should opt for a first-price auction with uninformed bidders. This failure of revenue equivalence is not driven by differences in entry. In fact, entry does not differ across auction format or information structure, although it exceeds risk-neutral predictions.Footnote 5 Rather, more aggressive bidding in first-price auctions raises it’s revenue relative to that of English clock auctions, regardless of information structure. In the first-price auction environments we study, homogeneously risk averse potential bidders will, in equilibrium, reduce entry and increase their bids relative to the risk-neutral equilibrium (Menezes and Monteiro 2000). Since we observe entry that exceeds the risk-neutral predictions, one of the contributions of this paper is to show that homogeneous risk aversion is not able to explain behavior.Footnote 6
 The fact that not revealing the number of bidders can increase revenue in first-price auctions has also been observed in Dyer et al. (1989). In this study, the number of bidders is uncertain, but not endogenous, and the result is explained by a model with risk-averse bidders.Footnote 7
 Previous papers have analyzed auctions with endogenous entry. However, most of the relevant theoretical literature focuses on the case where signals regarding value are revealed only after entry. In such an environment potential bidders are unable to use their signals to self-select into the auction.Footnote 8 The empirical literature has also focused on the case in which bidders only learn their value after entry.Footnote 9
 In contrast, we study auctions in which potential bidders know their value before making their entry decisions. To the best of our knowledge, the only other paper to experimentally examine such an environment is Ivanova-Stenzel and Salmon (2011). However, their setup involves multiple auction formats which offer homogeneous goods, and are competing for a fixed pool of bidders. The environments we study mirrors closely the theoretical analysis in Menezes and Monteiro (2000).Footnote 10 They provide symmetric equilibria in first and second-price auctions, with a homogeneous and commonly known entry cost.Footnote 11 Potential bidders are predicted to enter if their private value (weakly) exceeds a threshold. In equilibrium, this threshold does not vary across auction format. Nor does it depend on whether or not the number of bidders is announced prior to bids being placed. Interestingly, theory predicts that neither the choice of auction format nor the information structure will affect expected revenue. Since the English clock auction is strategically equivalent to the second-price auction, there predictions carry over to the environments we study. Our results are relevant to environments in which the seller need not compete for bidders against alternative sellers, or the seller is auctioning a relatively unique good. It is important to note that our motivation is not e-Bay and other online auction websites that allow sellers of homogeneous goods to compete for bidders by choice of auction format.Footnote 12 Examples of such relevant environments abound, tend to be large in scale, and it is common to observe a small number of bidders participating in these auctions. For example, there is a wide variety of government auctions to which our results would apply: timber auctions, infrastructure procurement auctions, auctions for pollution permits, and auctions of state owned assets. Examples from the private sector include real estate auctions and art auctions. The remainder of the paper is organized as follows. Section 2 contains the theoretical predictions. Section 3 explains our experimental design. Section 4 contains the results, and Sect. 5 concludes.",6
22.0,1.0,Experimental Economics,12 July 2018,https://link.springer.com/article/10.1007/s10683-018-9583-2,Temporal dynamics of pro-social behavior: an experimental analysis,March 2019,Jan Schmitz,,,Male,Unknown,Unknown,Male,"Many individuals perform good deeds on a regular basis. They engage in voluntary activities to help the needy within the society by actively performing good deeds or by giving to charity. It is therefore not surprising that pro-social behavior has increasingly gained attention in the field of economics, as volunteering and charitable giving are important for societies at large [giving to charities exceeded 2% of the GDP in the US in 2011 (List 2011)]. An extensive theoretical and empirical literature has already brought forward several explanations for why individuals show other-regarding or social preferences (for reviews, see, e.g., Cooper and Kagel 2016; Fehr and Schmidt 2006). Yet, although time is an important factor in economic decisions (Frederick et al. 2002), the dynamic structure of pro-social behavior is largely unexplored. Little is known about how pro-social behavior in the past affects decisions to behave pro-socially in the present. This study aims to close this research gap and examine the temporal dynamics of pro-social behavior by investigating how good deeds in the near versus the more distant past impact giving behavior in the present. Recent findings from social psychology and behavioral economics already document three distinct effects of initial pro-social decision making on immediately subsequent pro-social behavior: First, individuals may behave in a morally consistent way and thus in accordance with their prior decisions (Bem 1972; Cialdini et al. 1995), leading to positive behavioral spillovers from an initial good deed to a subsequent one. Second, an initial good deed may lead to negative behavioral spillovers. That is, after an initial pro-social act, individuals may license themselves into more selfish behavior in subsequent decisions (Gneezy et al. 2012; Khan and Dhar 2006; Mazar and Zhong 2010; Sachdeva et al. 2009; Tiefenbeck et al. 2013). Third, pro-social acts may be seen as independent from each other, and subjects make their decisions in isolation, leading to the absence of behavioral spillover effects between pro-social decisions (Blanken et al. 2015). The abovementioned studies, however, do not vary the time horizon between the decisions. Nevertheless, studying how the time passed between good deeds affects pro-social behavior is important, as the time span between possibilities to act pro-socially varies substantially at the individual level. Moreover, related evidence from learning in ultimatum and public good games indicates that individuals indeed adjust their behavior over time (Cooper and Kagel 2016). In some cases, other-regarding preferences increase over time, while in other cases, they seem to decrease. For example, Cooper and Stockman (2002) show that over time, subjects tend to adjust their pro-social behavior to a medium level in public good games. Moderate pro-social behavior seems to become acceptable, while extreme unfair behavior is seen as anti-social. In studies with varying time horizons between decisions, subjects typically make all their decisions within one experimental session, and the time span between their decisions to behave pro-socially or selfishly varies only minimally. Gneezy et al. (2014), for example, study how prior unethical behavior affects subsequent pro-social behavior, finding that unethical behavior increases subsequent pro-social behavior. Expanding on the existing literature, I provide first empirical evidence regarding the dynamic structure of doing good when successive pro-social decisions are similar and the time span between pro-social acts varies significantly. To empirically test whether a pro-social act in one time period affects behavior in a subsequent similar pro-social decision, I employ a multi-session laboratory experiment using charitable giving as a proxy for pro-sociality.Footnote 1 In the experiment, subjects have the possibility to undertake a good deed by giving to one of three real public goods (charities) in two temporally separate sessions. One group of subjects runs through the two sessions within one day (same day treatment [SDT]). The other group is recalled for the second session one week later (one week treatment [OWT]). When pro-social acts are similar (such as the decision to give to charity in this experiment), the literature points out that pro-social behavior may be short lived, as the amount transferred in multiple anonymous dictator games is reduced with every repetition (Achtziger et al. 2015). Sass and Weiman (2012) find similar results of declining social preferences in repeated public good experiments. Relatedly, Jacobsen et al. (2012) and Tiefenbeck et al. (2013) find that pro-environmental acts in the field can lead to adverse behavior in related pro-environmental domains. Because of this empirical evidence on behavioral spillover effects of pro-social behavior (see also Mullen and Monin 2016, for a review), it is likely to detect moral licensing between the two decisions to act pro-socially. It is however, ex-ante ambiguous whether the behavioral spillover effects are increasing or decreasing over time. As pointed out by Cooper and Kagel (2016), other-regarding preferences may be more or less pronounced when individuals learn the consequences of their pro-social behaviors over time. I find that giving behavior in the SDT and OWT is similar in the first session. However, the behavior substantially differs in the second session of the experiment. In line with existing literature, subjects give more in the first decision compared with the second decision in both treatments (see, e.g., Achtziger et al. 2015). Giving is, however, significantly more reduced in the SDT compared with the OWT. Thus, two identical pro-social acts initially lead to negative behavioral spillovers, as giving in the second donation decision is reduced. Time, however, moderates this moral licensing effect of an initial good deed, and subjects give more to charity if the time span between possibilities to give increases. These findings contribute to the literature on charitable giving. While repeated soliciting is a common fundraising strategy (Meer and Rosen 2011), fundraisers struggle with a significant revenue gap. This revenue gap comprises the difference between the actual amount raised in a given time period and what would have been raised if not too many donors drop out in the future (Burk 2003). My findings indicate that repeated soliciting may be more effective if the previous donation was made further back in time. The findings carry further important implications for policymakers who are worried about adverse effects due to moral licensing. Although I find evidence of moral licensing for two consecutive and similar pro-social decisions, the results indicate that the behavioral spillover effect is limited by time. As time passes, subjects are more likely to revert to their initial pro-social behavior. Hence, in spite of studies reporting immediate net negative effects of behavioral spillovers (see, e.g., Tiefenbeck et al. 2013, for negative behavioral spillover effects in the environmental domain), moral licensing is likely to disappear over time. The remainder of this paper is organized as follows: Sect. 2 describes the experimental design and procedure. Sect. 3 presents the experimental results, and Sect. 4 concludes.",15
22.0,1.0,Experimental Economics,22 March 2018,https://link.springer.com/article/10.1007/s10683-018-9566-3,Is fairness intuitive? An experiment accounting for subjective utility differences under time pressure,March 2019,Anna Louisa Merkel,Johannes Lohse,,Female,Male,Unknown,Mix,,
22.0,1.0,Experimental Economics,05 July 2018,https://link.springer.com/article/10.1007/s10683-018-9582-3,Trust as a decision under ambiguity,March 2019,Chen Li,Uyanga Turmunkh,Peter P. Wakker,,Unknown,Male,Mix,,
22.0,1.0,Experimental Economics,01 December 2018,https://link.springer.com/article/10.1007/s10683-018-09598-4,"Scale matters: risk perception, return expectations, and investment propensity under different scalings",March 2019,Christoph Huber,Jürgen Huber,,Male,Male,Unknown,Male,"When Gulliver traveled to Lilliput he was a giant. On his next journey to Brobdingnag he was a dwarf. While he had not changed, the scale of everything around him had. It seems that the scale we see something in plays a major role in how we perceive it. In financial practice, the scaling of price and return charts, e.g. in documents given to customers, is an important issue—recognized by practitioners, but mostly ignored by regulators and research so far. We mention regulators as, for example, the European Union sets rules for the presentation of a security’s past performance in a Key Investor Information Document (KIID; see Commission Regulation (EU) No 583/2010). According to that regulation, returns have to be shown in the form of bar graphs with a linear vertical axis. Additionally, the scale has to be adapted appropriately and shall not compress the bars so as to make fluctuations in returns harder to distinguish (p. 15). While the European Commission sees the potential problems of highly compressed bars, it remains unclear what consequences arise regarding the risk and profit expectations to-be-identified by investors, and hence, regarding investment decisions. Maximizing the return bars on the available space makes yearly fluctuations more distinguishable, but also brings the danger of misinterpretation of the returns as highly volatile and therefore highly risky, even when they are not. Compressing the bars, however, could lead to risk being perceived as too low, possibly exposing consumers to unexpectedly high losses. The fast emergence of robo-advisers, online brokers, and new products like e.g. cryptocurrencies add to the importance of gaining a better understanding of how people’s risk perception and investment propensity are influenced by different graphical representations of past returns. As individuals focus on graphical and salient pieces of information in their information processing strategies (Jarvenpaa 1989), there is a wide range of literature on graphical representations of financial time series. One strand of research tackles the question of which presentation formats (e.g. returns, prices, or distributions) increase potential investors’ forecasting abilities and accuracy. Return charts are associated with lower expected returns (Glaser et al. 2018) but also with higher perceived uncertainty (Diacon and Hasseldine 2007), compared to price charts. Stössel and Meier (2015) also discuss framing effects of different presentation formats on risk perception, but restrict themselves to different forms of graphical representations in the narrow domain of the KIID.Footnote 1 While Weber et al. (2005) find no significant improvement in perceived risk with continuous density distributions, Kaufmann et al. (2013) and Ehm et al. (2014) develop possibilities to better calibrate people’s risk perception by experience sampling from return distributions. However, these efforts require and imply a known stochastic process underlying the financial instrument to be assessed. In real-world applications, however, we have to rely on historical data, which may or may not give a good estimate for future returns and volatilities. A number of studies has investigated graphical distortions in information processing, most notably regarding corporate reports (see e.g. Beattie and Jones 1992). They show that a disproportionate representation of the underlying data can be misleading (Tufte 1983) and is often purposely used to create a more favorable view.Footnote 2 However, only few studies have investigated potential effects of varying a graph’s vertical axis scale without violating proportionality principles:Footnote 3 Cleveland et al. (1988) examine the ‘shape parameter’ of graphs—that is, the ratio of the horizontal and vertical distances spanned by the data, while holding the scale’s range constant. Lawrence and O’Connor (1992, 1993) examine scale effects with regard to people’s forecasting ability in financial time series. They find that large scales or high variability in the presented time series leads to overly narrow confidence intervals. To our knowledge, however, the vertical axis scale’s relevance towards risk communication and investment decisions has not yet been investigated. Our aim with this paper is to fill this gap by providing a systematic and rich analysis of the scale effect in graphical representations of financial time series. The research question we address is whether the presentation scale—narrow or wide—affects people’s risk perception, return expectations, and propensity to invest. We define a chart as having a narrow scale when the time series depicted extends close to the upper or lower borders of the chart, while a wide scale leaves ample space above and below. To explore our research question we conduct a laboratory experiment with a 2 × 2 design where we vary the presentation scale (narrow or wide) and the presentation format: assets are presented either as return bar charts or as price line charts. In a within-subjects design we ask participants to assess the riskiness, expected return, and attractiveness as investment of the assets. In a second task subjects make pairwise comparisons between these assets along the same three dimensions. We find that varying the scale strongly affects people’s risk perception, namely, that a narrower scale of the vertical axis leads to significantly higher perceived riskiness of an asset across price and return charts, even if the underlying volatility is the same. We demonstrate that adapting the scale to the span of the bars is reasonable with regard to recognizing yearly return variations within a single security, but at the same time makes it harder to identify differences between dissimilar securities. This result is robust for different historical return trends. We further find that past returns predict future return expectations almost perfectly irrespective of the scale. Risk perception is highly correlated with losses which in turn drive investment behavior. Concerning investment choices, subjects tend to invest in the asset they regard as more profitable, even if they think it bears higher risk. This study extends the existing literature in several important ways: We analyze previously unexplored scale effects in a systematic and clean experimental setup; we embed these issues directly into the context of information presentation in financial markets; and we explore different aspects of financial decision-making relating to the scale, presentation format, and underlying asset fundamentals in individual assessments as well as in pairwise comparisons. We think our findings are also informative for regulators: As we show, adapting the scale of a chart makes it easier to recognize yearly return variations within a single security, but at the same time makes it harder to identify differences between dissimilar securities. Regulators should be aware of—and attentive to—the potentially distorting effects of different axis scales in performance charts. While return bar charts are appropriate, allowing issuers to adapt the axis scale arbitrarily leaves room for deliberate action aimed at distorting investors’ perceptions about risk. Keeping the presentation scale constant across different securities enables better identification of risk and therefore easier comparisons.",18
22.0,1.0,Experimental Economics,08 June 2018,https://link.springer.com/article/10.1007/s10683-018-9578-z,The interaction between competition and unethical behaviour,March 2019,Nick Feltovich,,,Male,Unknown,Unknown,Male,"Within economics, the phenomenon of unethical behaviour is not well understood. As an example, consider lying. Only a few moments’ observation or reflection is sufficient to conclude that few people are completely honest, but neither do we see the full extent of lying implied by a comparison of economic costs and benefits, and we doubt any economist would argue strongly for either of these polar cases.Footnote 1 Recent theories of unethical behaviour (and lying in particular), developed to account for evidence from the lab and the field, typically take a middle ground by treating individuals as having some kind of psychic distaste for such behaviour, but not an infinite one (see Sect. 2). Such a treatment often implies an interior solution for dishonesty – some, but not as much as monetary incentives alone would predict – and raises the question of what factors impact its extent. One factor that has received some attention is the degree to which competitive pressures are present. This can matter for at least two reasons. First, competition can intensify monetary incentives for unethical behaviour. For example, for a monopolist, paying workers the legally mandated wage instead of a lower wage “off the books” might merely mean a smaller yacht, but if she faces competition from other businesses without such ethical principles, staying ethical herself might lose her all of her customers and drive her out of business. Second, even if monetary incentives are unaffected, competition can change perceptions of the morality of lying. As discussed in the next section, evidence from previous lab experiments suggests that the effect of dishonesty that explicitly affects others depends on the nature of the relationship between the protagonist and those who are affected. To greatly simplify, dishonesty appears to be viewed as more distasteful when it harms others who are perceived as friends, accomplices or innocent bystanders, but less distasteful when they are viewed as rivals, compared to dishonesty “in a vacuum” that either affects no-one at all, or is at the expense of the experimenter only. A natural question that has received less attention in the literature is the converse: how does unethical behaviour affect the severity of competition?Footnote 2 There is a substantial experimental literature suggesting that individuals do not compete with one another as severely as standard game theory would imply (e.g., cooperation in the Prisoners’ Dilemma and in voluntary contributions to public goods, Bertrand oligopoly).Footnote 3 If this reduced competition is due to positive other-regarding preferences (wanting others to have higher payoffs, other things held equal), and if these preferences are conditional on ethical behaviour by others, then evidence of dishonesty by others may lead to increased competition by weakening these “pro-social” preferences, possibly even turning them into “anti-social” preferences. Evidence from other settings (see Sect. 2) suggests this may indeed be the case. Even some other potential causes of reduced competition—such as beliefs that others will behave cooperatively – have a similar implication. This study is an investigation of both sides of this interaction: (a) whether unethical behaviour is more prevalent when competition is more severe, and (b) whether competition is more severe when unethical behaviour is more prevalent. We use a lab experiment in which subjects play the role of price-setting firms. We have two treatment variables. First, we vary the severity of structural competition: monopoly, Bertrand–Edgeworth duopoly, or Bertrand duopoly. We also vary how firms’ costs are determined: by die rolls made by the subject who inputs the result into the computer (“self-roll” treatment) or by computer-simulated die rolls (“computer-roll” treatment). In the computer-roll treatment, manipulation of costs is impossible. In the self-roll treatment, under-reporting the die rolls can be done with impunity (die rolls are unobservable by the experimenter and by other subjects). Importantly, the firm’s costs are determined by the reported die roll, not the actual roll. Of course, there are few real-life settings where this literally happens: where a firm can mis-report its costs and have the lie become its true costs. But there are many examples where the effect of some form of unethical behaviour is to lower a firm’s cost, and where this can be done largely out of sight: e.g., mislabelling (free range, organic, dolphin-safe, country-of-origin, etc.); rigging emissions-reporting software; adulteration of food, medicines, or cosmetics; use of sub-standard construction materials; tax avoidance/evasion; underpaying workers; and so on. So, while we will typically use “lying” as a label for the kind of unethical behaviour possible in our experimental setting, our study speaks to a broader class of misbehaviour than just lying. Like most previous studies, we find little evidence in our experiment for either complete honesty or completely opportunistic behaviour. However, our results suggest that some apparent honesty may actually be imperfect understanding of the incentives to lie. We also find that within either of the duopoly settings, prices are lower in the self-roll treatment than in the computer-roll treatment. That is, behavioural competition is indeed more severe when unethical behaviour is possible. Importantly, this is not simply a consequence of lower reported costs in the self-roll treatment, but holds even when we control for costs econometrically. Finally, we find that in our self-roll treatment, reported costs are lower as structural competition increases from monopoly to Bertrand-Edgeworth duopoly and thence to Bertrand duopoly. Although these differences are not always significant, they do provide some evidence that lying is more prevalent the more severe competitive pressures are.",12
22.0,1.0,Experimental Economics,04 August 2018,https://link.springer.com/article/10.1007/s10683-018-9586-z,Why choice lists increase risk taking,March 2019,David J. Freeman,Guy Mayraz,,Male,Male,Unknown,Male,"A single binary choice reveals only a single preference, so experiments typically include not one, but many choice problems. Unfortunately, this complicates the link between preferences and choices. Experimentalists therefore seek designs in which subjects choose the same option in each choice problem as they would have in an experiment that consisted solely of that particular choice problem—a feature known as “isolation”. A standard approach is to present choices in a list, such as the one in Fig. 1, and to incentivise them using the Random Incentive Scheme (RIS). Subjects make their selection in all the choices in the list, and one of their choices is then randomly selected for payment. Choices are interpreted on the assumption that subjects isolate. A choice list in descending order. This particular list was used in two conditions: R-list Descending and K-list Descending. These two conditions differed only in incentives Unfortunately, several recent studies have foundssignificant violations of isolation (Cox et al. 2014, 2015; Harrison and Swarthout 2014; Freeman et al. (in press); Brown and Healy 2018). But while we now know that isolation can fail, we do not know why. The leading hypothesis puts the blame on the randomness that is inherent to the RIS, which creates a compound lottery over the options that subjects choose. Subjects who opt for a ‘certain’ $1 over some chance of $1.40, only receive the $1 if that choice happens to be selected for payment. This makes no difference for subjects whose preferences satisfy the Independence Axiom (Holt 1986; Karni and Safra 1987), but violations of Independence are well-documented. A preference for certainty in violation of the Independence Axiom thus provides a possible explanation of isolation failure. This argument, however, assumes that subjects think through the implications of the incentive scheme, and that they optimise accordingly. But there is evidence that people engage in “narrow bracketing”—making each decision separately from others (Tversky and Kahneman 1981; Read et al. 1999; Rabin and Weizsäcker 2009).Footnote 1 If subjects engage in “narrow bracketing” they are unlikely to think through the implications of the compound lottery that RIS creates. Alternatively, it may be the difference in presentation between choice lists and single choices that causes isolation to fail. Previous work suggests that subjects tend to switch around the middle of a choice list (Andersen et al. 2006; Beauchamp et al. 2015), and that they treat the fixed side of a choice list as a reference point (Castillo and Eil 2014; Sprenger 2015). These or other presentation effects offer an alternative explanation of isolation failure. We seek to disentangle the role played by incentives on the one hand and presentation on the other. We thus conduct a between-subjects experiment with conditions that vary both. Following previous research (Freeman et al. in press), we focus on choices between a certain monetary prize and the chance of a larger one. One example is choosing between $1.00 with 100% chance and $1.40 with 85% chance—a choice we refer to as Q85. Three conditions, known as R-lists, consist of choice lists with RIS incentives. Figure 1 shows one; the other two include the same choices, but in different order. Each single choice (SC) condition includes only one of the choices from the list. Finally, in conditions we call K-lists, subjects complete the same questions as in R-list conditions, but they know in advance that they would be paid on their choice in Q85. Each K-list condition is thus identical in presentation to the corresponding R-list condition, but has the same incentives as the SC condition in which Q85 is the only question. We find a statistically significant and quantitatively large violation of isolation. In Q85, the proportion of risky choices is 41% in R-list and only 23% in SC. The figure for K-list is 35%—considerably closer to R-list than to SC. Presentation thus appears to be the primary driver of isolation failure. Further insight is obtained by breaking down the results for K-list subjects by the question that they answered first. Most subjects in both R-list and K-list conditions start at the top of the list, and the K-list subjects who do so are about as risk seeking in their answer to Q85 as R-list subjects. Nearly a quarter of K-list subjects jump straight to Q85, however, and those subjects are about as risk averse in their answer to Q85 as SC subjects. Thus, even the relatively small difference in choices we observe between the K-list and R-list conditions appears to be the result of the attention drawn to the incentivised question, rather than of the difference in incentives per se. Figure 2 illustrates these comparisons. Risk taking by condition. Bars show the proportion of subjects who chose the risky option. Results for K-list are presented both in aggregate and broken down by whether subjects answered Q85 first. Errors bars denote the standard error of the mean While we focus primarily on choices between a certain and a risky option, we also include Allais variants of R-list and SC, in which winning chances are multiplied by a common factor of 0.4. Subjects in these two conditions are more likely to choose the riskier option than subjects in the corresponding main conditions, but the difference is much greater in SC than in R-list. As a result, subjects in the SCAllais condition are no more risk averse than subjects in the R-list Allais condition. Judging by this finding, isolation failure is specific to choices between a safe and a risky option. To explain our results we need a presentation effect that generates violations of isolation between a safe and a risky option, but not between two risky options. These and other findings do not fit any of the theories we considered ex-ante (Sect. 3). We thus suggest a new model of presentation-dependence in the spirit of the Discovered Preferences Hypothesis (Plott 1996). Decision makers have well-defined preferences, which they gradually discover by considering the choices that they have to make. Choices are made using a cautious decision criterion that evaluates the desirability of a lottery by the lowest certainty equivalent that is consistent with the preferences that are considered ‘reasonable’ at the time of making the choice. Since list conditions include many related choices, they induce subjects to narrow down their set of ‘reasonable’ preferences, causing the certainty equivalent of risky lotteries to go up. And since the certainty equivalent of riskless prizes is fixed, this process results in an increased tendency towards risk taking in choices between a riskless prize and an uncertain lottery. If RIS incentives were the cause of isolation failure, it would be right to see choice lists as providing a biased measure of preferences relative to the gold standard of single choice experiments. Our results suggest a more complicated and more interesting picture: there is no true gold standard for measuring preferences, but perhaps the preferences inferred from choice lists come closer to what we may think of as informed preferences. Related Literature Ever since the discovery by Lichtenstein and Slovic (1971, without incentives) and Grether and Plott (1979, with RIS incentives) of preference reversals between matching tasks incentivised using the Becker-DeGroot-Marschack and pairwise choice tasks, economists have been broadly aware of the potential for presentation (Tversky et al. 1990) and RIS incentives (Holt 1986; Karni and Safra 1987) to affect choices in experiments. There are many existing tests of the isolation hypothesis where subjects make a small number of pairwise choices. Earlier such studies have been interpreted as support for the isolation hypothesis when subjects make pairwise choices among simple lotteries (Starmer and Sugden 1991; Beattie and Loomes 1997; Cubitt et al. 1998) and against the hypothesis that subjects integrate their portfolio (Starmer and Sugden 1991; Hey 2005).Footnote 2 More recent studies provide evidence against isolation. These studies cover both the standard case where all choices in the relevant RIS condition are displayed on one screen (Cox et al. 2014, 2015, PORpi condition), and a variation in which they are displayed on separate screens with no prior information about forthcoming choices (Harrison and Swarthout 2014; Cox et al. 2015, PORnp condition). Cox et al. (2015) also establish violations of isolation for a variety of non-RIS incentive schemes, including versions of the “pay all questions” scheme. These recent papers interpret their results as due to incentives created by the RIS and related incentive schemes. As regards widely-used choice lists, Freeman et al. (in press) show that in choices between a safe and a risky option, subjects who face an ordered list of choices incentivised by the RIS exhibit significantly less risk aversion than subjects who make only a single choice. They find less pronounced effects away from certainty. They too interpret their findings as driven by the RIS incentives. Other studies document an effect of varying the content and position of other choices in a list on inferred preferences, and interpret their findings as driven by presentation (Andersen et al. 2006; Lévy-Garboua et al. 2012; Sprenger 2015; Castillo and Eil 2014; Beauchamp et al. 2015), but do not compare choices made in lists to the behaviour of subjects who make a single choice on its own. Our study builds on previous work by separately controlling for presenting many choices on one screen, the RIS incentives, and the ordered list structure (via our scrambled condition). Another novelty in our study is our use of click data—recording not only the choices subjects make in each line of the list, but also when they make them. This additional information sheds light on the process by which subjects make their decisions. Our use of the K-list condition to study the effect of list presentation while controlling for incentives is analogous to conditions used in previous research that does not study choice lists. In particular, the ordinal payoff scheme (Tversky et al. 1990; Cubitt et al. 2004) has been used to study differences between matching vs. choice tasks while controlling for incentives. Experiments by both Starmer and Sugden (1991) and Cubitt et al. (1998) also have all subjects make all possible choices, and thus their tests of isolation are most comparable to our comparison between between K-list and R-list conditions. The comparisons between the OT, impure OT, and PORpi conditions in Cox et al. (2015) are analogous to the comparisons between our SC, K-list, and R-list conditions. Concurrent work by Brown and Healy (2018) does study choice lists, and also uses SC, K-list, and R-list conditions, but does not vary question order nor consider a scrambled list, and does not consider choices with a certain option. Prior work by Lévy-Garboua et al. (2012) compares R-list conditions (based around the Holt and Laury (2002) design) with varying question orders (ascending, descending, and random), to conditions where each list is presented as a sequence of pairwise choices; however, they do not consider choices with a certain option. Our results contribute more broadly to the experimental literature on choice under risk. Early experimental work documented substantial violations of the Independence Axiom and some of its generalisations (Colin 1989; Battalio et al. 1990; Wu 1994). Our results suggest that the certainty effect, and perhaps other violations of Independence, are substantially stronger in single choices than in choice lists.",7
22.0,1.0,Experimental Economics,03 July 2018,https://link.springer.com/article/10.1007/s10683-018-9584-1,"Cheating, incentives, and money manipulation",March 2019,Gary Charness,Celia Blanco-Jimenez,Ismael Rodriguez-Lara,Male,Female,Male,Mix,,
22.0,1.0,Experimental Economics,20 September 2018,https://link.springer.com/article/10.1007/s10683-018-9592-1,Centrality and cooperation in networks,March 2019,Boris van Leeuwen,Abhijit Ramalingam,Arthur Schram,Male,Unknown,Male,Male,"Many organizations and groups have some form of network structure defined by who can interact with whom. Links between members are often formal. For example, a hierarchical chain of command typically creates a well-defined network structure. Network links might, however, also be of a more informal nature. Some individuals, for instance, might benefit from information provided by others even if these others are not from the same formal organization. Think of an employer A looking for suggestions from other employers about suitable candidates to recruit (Gërxhani et al. 2013). Even if she is not involved in any formal organization connecting her to other employers, A might receive suggestions from employer B who happens to be her golf partner. In fact, if B knows employer C, A might even receive information from C without knowing C. In this way, some individuals might connect others who are otherwise separated. Separation can occur, for instance, by location or lack of direct social connections. Individuals who connect others in this way are in a sense more ‘central’ in the network than those who do not connect strangers. Networks with central players might occur in formal organizations (for example, connecting two departments of the same firm), but intuitively, they seem more likely to be observed in informal organizations. This is because formal organizations typically try to avoid specific individuals becoming too central. Indeed, if distinct subgroups are only connected via a central player, then—in both formal and informal networks—her presence is essential for these groups to mutually benefit from each other’s actions. In their study of 50 large organizations, Cross and Prusak (2002) find that informal networks are ubiquitous, and that such networks often have central players who play a crucial role by connecting other individuals and subgroups in the organization. At the same time, many organizations have mechanisms allowing members to be expelled if they somehow fail in their obligations towards other members. An extreme example would be how, historically, military personnel would face serious consequences (possibly execution) if they shirked on the job. Less dramatic examples of exclusion are observed in firms, political parties, supporters groups, and clubs. Managers can be fired, politicians can be expelled, hooligans can be banned from visiting games, and club membership can be revoked. In some cases, the impact of this exclusion can also be substantial to the group as a whole (e.g. a complete government can fall due to the acts of a single politician). A common theme is that exclusion from these groups is costly to the person involved and possibly also to the group as a whole. When the member’s position is characterized by centrality, however, her exclusion may be even more costly to other members of the organization, who are now no longer connected. The powerful position that centrality thus brings might then lead central players to exploit their pivotal position with little fear of retaliation. Cross and Prusak (2002) find that ‘central connectors’ sometimes “use their roles for political or financial gain” (p. 8). The notion of centrality is closely related to the idea of structural holes in networks. In our framework a central player bridges a structural hole, which enables her to extract the rents from this position (Burt 1992; Goyal and Vega-Redondo 2007). This rent seeking is acceptable to those being connected because the connections she creates are valuable. It is a price they might well be willing to pay, as “it is not easy for the other members of the network to supplant an ineffective central connector” (Cross and Prusak 2002, p. 8). In this paper, we study how network positions affect cooperative behavior. We design an experiment that allows us to study the effects of centrality and how this relates to the possibility of excluding players from a network. Our setup creates a network where centrality is valuable while cooperation can be enforced by the threat of exclusion. In particular, we address the following research questions: (1) How is cooperative behavior affected by being central in a network? (2) How do other players respond to the use or (potential) abuse of a central position? In the experiment, subjects play a voluntary contribution mechanism (VCM) game where group members are connected on a network. Payoffs from individual contributions accrue to all, without rivalry. As is usual in these games, full contribution by all members is efficient but selfish individuals have a dominant strategy to free ride by contributing nothing. In one of our treatments, contribution constitutes a public good because no group member can be excluded from benefitting. An example is the collection of voluntary contributions amongst neighbors, to install safety cameras at the neighborhood’s periphery. In another treatment exclusion from group benefits is possible and contributions are best seen as constituting a ‘quasi’ or ‘impure’ public good (Cornes and Sandler 1996). Here, one can think of contributions amongst members of a club to build a new clubhouse, where membership of non-contributors can be revoked. The networks we design provide a formal structure on who can contribute and who enjoys the benefits of any individual’s contribution. Without centrality the network is complete; there is a connection between each pair of players. This means that a player’s departure from the network has no consequences for the possibilities of the remaining players to interact and benefit from each other’s contributions. With centrality, one player connects the other group members. Note, however, that centrality only creates a pivotal role if the player concerned could somehow leave or be removed from the network. This would break (some of) the connections between other group members. To study the effects of network structures with centrality, we therefore include a treatment with exclusion. Here, every player may vote to exclude specific group members (as in Cinyabuguma et al. 2005 and Charness and Yang 2014). Excluding the central player causes the group to fall apart, which is costly for all involved.Footnote 1 Several other experimental studies investigate public good provision in a network. In these studies, the network determines which contributions can be accessed (Rosenkranz and Weitzel 2012; Charness et al. 2014), who can monitor whom (Eckel et al. 2010; Fatas et al. 2010), who can punish whom (Leibbrandt et al. 2015) or a combination of these (Carpenter et al. 2012). However, none of these studies involves centrality. To the best of our knowledge, we are the first to study the effects of network centrality in social dilemmas. Our results show that centrality is often used as a license to free ride; central players contribute less than others. Other players tolerate such behavior: they contribute more than the central player and they tend not to vote to exclude central players. As a result, players with centrality earn higher payoffs than others. In other words, we find that central players take advantage of their position and manage to get away with this. The introduction of players with centrality creates heterogeneity across players. This is important, because homogeneity within organizations is unlikely to be found outside the laboratory. People differ along many dimensions, including their position in the network. A large body of previous work has found that heterogeneity in endowments, productivity and/or returns frequently reduces cooperation in VCM games (e.g. Cherry et al. 2005; Anderson et al. 2008; Tan 2008; Nikiforakis et al. 2012; Fischbacher et al. 2014; Hargreaves Heap et al. 2016; Gangadharan et al. 2017).Footnote 2 This is primarily due to a multiplicity of norms that underlie the behavior of players with different characteristics. The introduction of additional features such as a punishment mechanism or the activation of group identity allows groups to overcome this decrease in cooperation (Reuben and Riedl 2013; Weng and Carlsson 2015). However, not all additional mechanisms are equally effective. For example, Gangadharan et al. (2017) find that communication increases cooperation in both homogeneous and heterogeneous groups, but the positive effect of communication is stronger in homogeneous groups. Finally, not all studies find that heterogeneity is detrimental to efficiency, and not under all circumstances. Fisher et al. (1995) find that heterogeneity in MPCRs does not affect efficiency and Chan et al. (1999) find that heterogeneity in endowments and/or returns from the public good increase cooperation in a nonlinear environment. Our study tests the effects of a different kind of heterogeneity—that is, in network position—on overall efficiency. Our results show that heterogeneity due to centrality does not affect efficiency. In our setting, periphery (that is, non-central) players understand the positive value of being connected via central players and appear to be willing to pay a price for these connections. Central players seem to restrict their free riding to a level that periphery players find acceptable and thereby avoid being excluded. The remainder of this paper is organized as follows. The next section presents our experimental design and Sect. 3 presents our testable hypotheses. We describe our results in Sects. 4 and 5 concludes.",8
22.0,1.0,Experimental Economics,11 August 2018,https://link.springer.com/article/10.1007/s10683-018-9588-x,Higher order risk attitudes and prevention under different timings of loss,March 2019,Takehito Masuda,Eungik Lee,,Male,Unknown,Unknown,Male,"Intertemporal choice under risk dates back to Leland’s (1968) analysis of precautionary saving. Kimball (1990) found that willingness to save more under a greater background risk, called prudence, is equivalent to a positive third derivative of the utility function under the expected utility framework. Subsequent to this seminal work, the concept of higher order risk attitudes, including prudence, has also been widely applied to non-financial contexts. Gollier et al. (2000) showed that the intensity of prudence is an important factor in the optimal investment in new technology, such as genetically-modified food, when potential damage may be revealed in the future as scientific knowledge advances. White (2008) studied the role of prudence in a bargaining situation and showed that prudence plays a similar role as increasing a player’s patience and, hence, improves the bargaining outcome.Footnote 1 Recently, experimental investigation of higher order risk attitudes has emerged. Eeckhoudt and Schlesinger (2006) developed a model-free method of eliciting higher order risk attitudes called risk apportionment tasks. Since then, a lot of research has been done to find empirical evidence of higher order risk attitudes (Deck and Schlesinger 2010, 2014; Ebert and Wiesen 2011, 2014; Noussair et al. 2014; Haering et al. 2017; Bleichrodt and van Bruggen 2018; Heinrich and Mayrhofer 2018). These studies commonly find that subjects are highly prudent, and the findings are robust to various experimental designs, such as stake size, asymmetric zero-mean risks, and reduced lottery structures (Trautmann and van de Kuilen, 2018). Among many important findings, there are two from previous literature that are noteworthy. First, Noussair et al. (2014) find that prudent people are more likely to have a greater amount of savings. This result shows that prudence is important in explaining financial decision making. Second, Ebert and Wiesen (2014) find that measured downside risk premia are beyond the prediction of the expected utility model. This result implies that the expected utility model is not sufficient to empirically explain the high prudence of agents. This study is, to the best of our knowledge, the first attempt to experimentally test comparative statics between prudence and prevention behavior using different timings of loss in a self-protection context.Footnote 2 The outcomes of prevention come at different points in time. For example, having safe driver education reduces the risk of future accidents, while paying attention to driving reduces the immediate risk of an accident. Theoretically, prudence is known to have different effects depending on the timing of loss. In particular, Menegatti (2009) (Eeckhoudt and Gollier 2005) find that prudence increases (decreases) prevention efforts when the loss may occur in the future (same) period, compared to a risk neutral scenario. We test these different effects of prudence on prevention behavior using a lab experiment. Our test is parameter-free in the sense that predictions depend only on the assumption that a player is prudent. In this experiment, the subjects participated in a higher order risk attitude task, prevention game, and time preference elicitation. We employed a between-subjects design, where each subject was assigned to one of two variants of the prevention game, depending on the timing of the loss (current loss or future loss). The data systematically violate the expected utility predictions. We find that more than 95% of the subjects are prudent, in line with previous literature. Based on the predictions of Eeckhoudt and Gollier (2005) and Menegatti (2009), we expect that the risk neutral level of effort lies between the efforts in current loss treatment and future loss treatment. The results, however, show that subjects exert less effort than a risk neutral player regardless of the timing of loss. This result is in line with Eeckhoudt and Gollier (2005), but contradictory to Menegatti (2009). Ebert and Wiesen (2014), Eeckhoudt et al. (2017), and Trautmann and van de Kuilen (2018) argue that expected utility models could be too narrow to describe higher order risk attitudes. We use probability weighting (Kahneman and Tversky 1979; Tversky and Kahneman 1992; Wakker 2010) as an alternative explanation for this conflicting observation. We establish that, in our design, players with Prelec (1998) probability weighting exert less effort than a risk neutral player, regardless of the timing of loss. Moreover, returning to a choice-based definition of prudence, we also show that probability weighting can result in prudent choices in our higher order risk attitude elicitation task. The remainder of this paper is organized as follows. Section 2 presents the predictions of the prevention game, based on the expected utility model. Section 3 describes the experimental design and Sect. 4 discusses the main results. Section 5 examines how probability weighting can explain the data. Section 6 provides our concluding remarks.",10
22.0,1.0,Experimental Economics,16 May 2018,https://link.springer.com/article/10.1007/s10683-018-9576-1,"Risk, time pressure, and selection effects",March 2019,Martin G. Kocher,David Schindler,Yilong Xu,Male,Male,Unknown,Male,"Many, if not most, decisions in the workplace are made under time pressure today (Reid and Ramarajan 2016; Wheatley 2000). Consequently, research in decision making has recently started to investigate how a time constraint influences individuals’ preferences and choices (for an overview, Spiliopoulos and Ortmann 2017). By randomly allocating subjects into time pressure conditions, the literature has successfully identified its causal effects in a variety of domains, including risky, social, and strategic behavior (e.g., Sutter et al. 2003, on bargaining; Kocher and Sutter 2006, on beauty contests; Baillon et al. 2013, on decisions under ambiguity; Kirchler et al. 2017, on risky decisions; Buckert et al. 2017, on imitation in strategic games; Haji et al. 2016, on bidding in auctions). However, these causal effects should be interpreted with caution, because of two potentially serious problems, both relating to issues of self-selection. First, because people differ in their ability to cope with time pressure (e.g., Claessens et al. 2017; Eisenhardt 1989; Maruping et al. 2015, and references therein), we expect people self-select into decision environments with a different degree of time pressure. That is, outside the experimental laboratory, candidates self-select into activities and occupations and thus into job-related decision-making environments. In contrast, participants in experiments are exogenously assigned to treatment conditions that may not fit well with their tastes and skills (Omar and List 2015). External validity of the observed experimental behavior thus cannot be taken for granted. Despite similarity of the experimental and the natural decision environments (in term of time pressure), the decision makers may systematically differ across the two settings in a self-selected way. Importantly, while external validity is an issue in any empirical study, it is a more central aspect in laboratory experiments that explicitly aim to mimic natural decision environments.Footnote 1 Understanding the personality traits associated with the ability to perform under time pressure in the lab would allow an assessment of such selection in the field, and thus of the external validity of (lab-measured) average causal effects based on the distribution of traits in actual decision environments. Identifying such traits is one aim of the current paper. Second, if time pressure is substantial and relevant in an experiment, some people will violate the time constraint. This leads to problems of the internal, rather than the external validity of the time pressure effects, because the sample of decisions observed in the data set is self-selected.Footnote 2 Failure to take these selection effects into account may therefore result in a false interpretation of the observed behavior in terms of population averages. For example, Tinghög et al. (2013) argue that failures to replicate time pressure effects on cooperation in ethical dilemmas (Rand et al. 2012) may be due to the original studies excluding about half of the participants because of a failure to meet the time constraint (Casari et al. (2007) make a similar observation in the context of auction bidding; Ambuehl et al. (2018) show how high incentives disproportionally lure cognitively disadvantaged agents to self-select into transactions of unclear consequences). The current study aims to shed light on such heterogeneity and the resulting selection effects in the presence of exogenously imposed time pressure. To this end, we separately analyze subsamples of subjects who fail, respectively succeed, to react in time, in terms of their behavior and traits. We demonstrate that overlooking the selection issue in this context would result in a very different assessment of the performance under time pressure. We focus on the domain of risky decisions. In “Risky behavior under time pressure: summary of results” in “Appendix”, we review the findings of the experimental literature in this domain. We indicate for each study whether there is a potential threat to internal validity because of substantial violations of the time constraints, or whether there is low time pressure, questioning relevance. Table 8 in “Appendix” suggests that selection problems should be taken seriously in the interpretation of causal time pressure effects on risk taking. Observing that selection issues are at the heart of experiments with time pressure and other adverse conditions, we are the first to (1) directly measure the empirical relevance of selection effects and (2) test whether there are individual-level correlates based on observable background variables that can be used as predictors for the ability of a decision maker to cope with time pressure, and thus the propensity to self-select into time-pressure environments. The fact that decision makers differ in their ability to deal with time pressure requires us to predict this ability if we aim to ensure that decision makers are allocated to environments in an efficient way.Footnote 3 We use the term time pressure resistance for such ability. It relates to differences in the decision process, including the decision maker’s time management. Our study aims to provide insights into these processes, and how they differ between decision makers. To this end, we collect data on risky decisions under time pressure, augmenting a design used in Kocher et al. (2013) to allow for both between-subject and within-subject analyses of behavior across time-constrained conditions. That is, we observe each decision maker’s risky choice behavior both in the presence and in the absence of time pressure for a similar set of risky alternatives. We choose to study risky decisions under time pressure because of the ubiquity of uncertainty in financial and managerial decision making. This, however, does not render other domains of deciding under time pressure such as decisions in social contexts less relevant (e.g., Cappelen et al. 2016; Rand et al. 2012). To test whether individual differences predict decision quality under time pressure, we assess participants’ scores on a measure of cognitive ability, on a measure of intellectual efficiency, and a set of personality traits. Importantly, while performance under time pressure can be measured in many ways, a risky decision task requires complex reasoning and has no obvious solution from the perspective of the decision maker (because optimal choices depend on preferences). Consequently, the decision maker has to choose a decision strategy, and this strategy may be affected by time pressure (Ordóñez and Benson 1997). Our performance measures aim to detect such shifts in strategy. The details of the experimental design, including our measures of cognitive ability and intellectual efficiency are described in the next section. Employing this design to study person-environment interactions in the context of time pressure, we observe the following results. First, we observe clear differences in decision styles across people in the absence of time pressure, which are then associated with the success in mastering the time constraint when it is present. That is, selection is highly relevant for internal validity. Second, in attempting to then predict the ability to perform under time pressure, we find that those who score high in cognitive measures and have high self-efficacy perform better and are less likely to miss the deadline, although cognitive measures only possess predictive power jointly. Moreover, individuals’ decision style (defined below) in the absence of time pressure correlates with performance under time pressure. Yet, we note that there is still an important role of unobserved factors. We discuss the implications of our findings for the external validity of time pressure effects in decision making in the wild.",22
22.0,1.0,Experimental Economics,07 August 2018,https://link.springer.com/article/10.1007/s10683-018-9585-0,"Which performs better under trader settings, double auction or uniform price auction?",March 2019,Koji Kotani,Kenta Tanaka,Shunsuke Managi,Male,Male,Male,Male,"There have been numerous debates about the effectiveness of a marketable permit system (MPS) for environmental problems. Economists have long sought to address the advantages and disadvantages (Hahn 1989; Tietenberg 2006; Hahn and Stavins 2011; Goeree et al. 2010), and they appear to have reached a consensus on the following advantages provided by MPSs: (1) efficiency or least cost property, (2) incentive to innovate and (3) information requirements for efficiency (Field and Field 2006; Kolstad 2010). Previous studies have examined which trading rules and institutions work best for MPSs in controlled laboratory experiments (see e.g., Muller and Mestelman 1998; Cason 2010, for a review). The literature demonstrates that there are two important factors in an experimental design to determine market outcomes: (1) auction mechanisms and (2) trader or non-trader settings.Footnote 1 This paper addresses the performances of different auction mechanisms, focusing on double auctions (DAs) and uniform price auctions (UPAs) under trader settings in MPSs. The DA mechanism is known to perform well under general settings and has been extensively applied in economic experiments (see e.g., Boening and Wilcox 1996; Cason 2010). The DA is a real-time trading institution in which agents can submit bids to buy and offers to sell for permits; the agents can accept the best bid or offer made by other agents at any time during a trading period of several minutes under trader settings (Davis and Holt 1992). Therefore, a DA gives flexibility for agents to trade. In contrast, a UPA does not give agents flexibility in trading because all of the permit trades are made with a uniform price. First, a buyer is asked to submit “bids to buy” for each unit of additional permits, and a seller is asked to submit “offers to sell” for each unit of permits that she has. After all agents submit bids to buy and offers to sell, a central authority collects and ranks all of the bids to buy from high to low (i.e., a demand curve) and all of the offers to sell from low to high (i.e., a supply curve), and the authority finally determines the intersection of the demand and supply curves (Davis and Holt 1992). Most previous studies have used DAs for the experimental study of MPSs. In particular, Plott (1983), Kilkenny (2000) and Cason et al. (2003) use DAs under non-trader settings and report that the average efficiency observed in the experiments is approximately \({98}{\%}\). DAs under non-trader settings promise further simplicity in the decision making processes for agents in experiments and relief from administrative burdens compared to DAs under trader settings. These MPS results of DAs under non-trader settings are consistent with the high efficiency achieved under DAs in general auction studies, such as those by Williams (1980) and Plott and Gray (1990). Another group of studies, including those by Ledyard and Szakaly-Moore (1994), Godby et al. (1997), Muller et al. (2002) and Cason and Gangadharan (2006), have used DAs under trader settings. These experiments demonstrate that the observed efficiencies, which range between \({60}{\%}\) and \({98}{\%}\), can exhibit higher variation and be lower on average than those obtained in DA experiments under non-trader settings. Furthermore, these studies report that the observed prices of permits could be unstable. Cason and Plott (1996) and Cason and Gangadharan (2005) conduct experiments with UPAs under non-trader settings as a possible alternative. Their studies confirm that UPAs are efficient in an MPS and find that the price dynamics are stable and responsive to changes in the underlying values, especially with respect to marginal abatement costs during the experiments. Smith et al. (1982) compare DAs and UPAs under non-trader settings, demonstrating that UPAs exhibit high efficiencies and price stability but that DAs perform slightly better than UPAs do. In summary, the literature on MPSs employs mostly DAs and establishes that an institution achieves high efficiency under non-trader settings. However, the efficiencies and prices in DAs under trader settings could be lower and less stable than those under non-trader settings (Muller and Mestelman 1998; Cason 2010).Footnote 2 Few studies have evaluated and compared DAs and UPAs under trader settings in MPSs and their performances with those under non-trader settings within a single framework.Footnote 3 This is critical for exploring the possible better market institutions for MPSs than DAs under trader settings because players in MPSs participate as traders in reality and because DAs under trader settings allow agents to make speculative or erroneous trades, which may lead to a deviation from equilibrium prices and to efficiency losses (Cason and Friedman 1999). Given this state of affairs, we design and implement UPA experiments under trader settings as a possible alternative. To compare the two auctions, UPA and DA experiments are carried out in both trader and non-trader settings, employing the same structures of trading environments and controls, except for the auction rules.Footnote 4 Our study’s novelty lies in the design of the UPA experiments under trader settings in which each subject is asked to simultaneously submit “bids to buy” for each additional unit she may purchase and “offers to sell” for each unit of permits she has in each trading period. In this manner, the UPA can be considered a trader setting because no subject knows in advance whether she will be a buyer or a seller; indeed, the subject could be both, depending on the bidding and offering strategy and the announced uniform price.",8
22.0,1.0,Experimental Economics,22 November 2018,https://link.springer.com/article/10.1007/s10683-018-9596-x,Elicitation of expectations using Colonel Blotto,March 2019,Ronald Peeters,Leonard Wolk,,Male,Male,Unknown,Male,"Eliciting expectations about the future and constructing forecasts of future events are integral to successful business planning. In cases where for example employees possess information about the future that are not (yet) reflected in official performance metrics, it is integral that we are able to elicit and aggregate such information accurately. Recently, several market based mechanisms have been proposed; for instance, Baillon (2017) introduces Bayesian markets to elicit subjective beliefs and Gillen et al. (2017) show that forecasts constructed using a competitive forecasting mechanism outperform official sales forecasts at Intel. In this study, we propose a strategic mechanism that resembles a betting pool for a single event and test its elicitation performance using data gathered in a laboratory experiment. The strategic component offers an environment with simple rules and where payoffs are determined transparently. Further, it relies on a small number of forecasters, which offers the possibility for the mechanism to be implemented in small groups, or teams, akin to what is typically observed in a business setting where relevant information may not be widely dispersed throughout an organization. More precisely, the elicitation mechanism we propose concerns a variation of the Colonel Blotto game, where two players divide a given amount of resources over a set of possible future events and where final payoffs are determined by the resources allocated to the realized event. We implement two different payment rules, a winner-takes-all rule (Win), where the prize is awarded to the player(s) with the most resources on the realized event, and a proportional-prize rule (Prop), where the prize is shared in proportion to the resources allocated to the realized event. We show that, when probabilities by which events realize are common knowledge to the players, the games designed are Bayesian–Nash incentive compatible. Under the proportional-prize rule, there is a unique equilibrium in which both players allocate their resources in proportion to these realization probabilities. Under the winner-takes-all rule, in equilibrium, both players randomize their allocations in a way that the expected allocation of resources is proportional to the true realization probabilities. Hence, both variations of the Blotto game produce equilibrium properties that are appealing for elicitation and prediction practices. While in practical applications the strategic uncertainty present in our games may negatively affect the mechanisms’ performance, there are other factors embodied by strategic environments that have the potential to improve the performance of the mechanism. First, the game is easy to explain and to implement, which is important given that, in scope of endogenous participation, individuals are known to be attracted to simpler mechanisms (Carpenter et al. 2008). Second, the joy of winning in a game may trigger more cognitive effort, relative to a situation where forecasters are individually incentivized (for instance, by a proper scoring rule) or via a market mechanism (as in prediction markets).Footnote 1 Third, competition may incentivize players to allocate resources to extreme events, in accordance with realization probabilities, that are otherwise easily underestimated. Other major advantages of this game, which are not unique to our proposed game, are that few individuals are required to produce accurate forecasts and that individual expectations are elicited as a density over the entire distribution of possible future events, rather than just as a mean or a median. The latter property is particularly interesting when there are a small number of participants, or when the density is not symmetric or unimodal. Using data gathered in a laboratory experiment, we test the performance of the mechanisms using a third non-strategic mechanism (Ind) as a benchmark. In this treatment individuals are incentivized via a quadratic scoring rule (Brier 1950). Next to the three mechanisms, as one treatment dimension, we have a second informational treatment dimension. In the first informational variation, following theoretical predictions, the true realization probabilities are common knowledge to the players (Baseline). In the second variation, more relevant in view of practical applications, players gradually learn these probabilities via observations (Predict). We find that the Blotto game augmented with the proportional-prize payment rule outperforms both other mechanisms in the Baseline information variation and its superiority relative to the Blotto game with winner-takes-all payoffs is not driven solely by randomizing behavior. In the Predict information variation, we find that the performances of the different mechanisms are not statistically distinguishable. This is striking, since strategic uncertainty about opponent behavior in the game potentially could distort the expectations that we elicit. When eliciting expectations using a proper scoring rule there is no such distortion and comparing proportional incentives to the proper scoring rule, neither of the two mechanisms appear to be worse than the other from any perspective. Our paper closely relates to several streams of the literature. First, it relates to the elicitation of subjective information using incentive compatible mechanisms such as scoring rules (Brier 1950; Prelec 2004) as well as prediction markets (Forsythe et al. 1992). Scoring rules have been applied in a wide variety of fields and have shown their success in extracting subjective information (for an overview see Carvalho 2016). One advantage of a scoring rule is that it elicits beliefs on an individual level and it is thus free from any strategic concerns. We use such a mechanism as a benchmark for the strategic mechanisms in our experiments. Yet, market based mechanisms such as prediction markets have also shown to be successful in aggregating dispersed beliefs about a future event, and have been applied in a wide variety of business environments such as Google (Cowgill et al. 2009) and Hewlett-Packard (Chen and Plott 2002) and for scientific reproducibility (Dreber et al. 2015). Second, our study relates closely to the literature on parimutuel betting (Figlewski 1979; Thaler and Ziemba 1988). In a parimutuel betting market, a bookmaker offers prices for future events that are set by the relative demand and/or odds of these events taking place (Plott et al. 2003). Within this stream of the literature, our paper most closely links to that of Gillen et al. (2017), who design a distributional forecasting mechanism and conduct a field test at Intel, a large semiconductor firm. The authors’ mechanism closely resembles a parimutuel betting market where forecasters purchase tickets that can be spent on possible future outcomes. An interesting feature of their implementation is that the price of a ticket is not fixed but instead depends on the timing of the purchase. By inducing a cost of delay, this mechanism helps mitigate strategic behavior where betting takes place close to the end of the market. This is different from our study, which does not involve a time-dimension within each round and hence, in our experiment, there are no timing issues related to the placing of a ‘bet’. The authors report strong results and show that the mechanism consistently outperforms official Intel forecasts, especially at short horizons. However, since it is implemented in the field, the source of the performance improvement is not fully clear. It could either be that the mechanism is able to aggregate information more efficiently or that it is able to collect superior information compared to official Intel forecasts. Our study complements that of Gillen et al. (2017), as we focus on the incentive structure that affects the revelation of expectations and, in turn, also the forecasting performance. Hence, our interest lies primarily in the design and performance of payment rules and not in the actual information gathering process, which takes place outside the game in our study. Third, there is a large existing stream of literature on Blotto experiments, that includes four contributions that closely relate to ours.Footnote 2 Avrahami and Kareev (2009) conduct Blotto experiments investigating the role of asymmetries in players’ strengths on their allocation decisions. Their symmetric benchmark treatments are in essence identical to our baseline treatment with a winner-takes-all payment rule, but with uniform realization probabilities. They find that players’ behavior approximates the game-theoretic solution quite well (in particular for the benchmark treatment with equal player strengths). The experiments by Avrahami et al. (2014) also included treatments with non-uniform realization probabilities, and produced the finding that players’ resource allocations correlate with the realization probabilities. Chowdhury et al. (2013) also consider a setting with asymmetries in players’ strengths, both for the winner-takes-all payment rule and the lottery payment rule that is theoretically equivalent to our proportional-prize rule. In their set-up, players can collect a reward for each battlefield, which is theoretically equivalent to a setting like ours with uniform realization probabilities. They find that players’ allocations of resources are in accordance with the theoretical predictions for both payment rules. Finally, the study by Duffy and Matros (2017) include treatments with symmetric players and a lottery payment rule that is theoretically equivalent to our proportional-prize rule. Similar to the study of Chowdhury et al. (2013), players can collect rewards for each battlefield, but the valuations are unequal across battlefields. They find mean allocations to be close to equilibrium predictions. Moreover, like in Avrahami et al. (2014), players’ resource allocations correlate with the battlefield valuations. The remainder of the paper is organized as follows. In Sect. 2, we present the two game variations and their equilibrium properties. Next, we present our experimental design and results in Sects. 3 and 4, relegating a further discussion on these results to Sect. 5. Section 6 concludes.",1
22.0,2.0,Experimental Economics,25 April 2019,https://link.springer.com/article/10.1007/s10683-019-09611-4,Call for submissions: symposium of pre-results review in experimental economics,June 2019,,,,Unknown,Unknown,Unknown,Unknown,,
22.0,2.0,Experimental Economics,21 December 2017,https://link.springer.com/article/10.1007/s10683-017-9560-1,"Individualism, collectivism, and trade",June 2019,Aidin Hajikhameneh,Erik O. Kimbrough,,Unknown,Male,Unknown,Male,"Economists have long emphasized the crucial role of effective formal institutions in facilitating trade (e.g. North 1990). In particular, the transition from small-scale personal exchange to large-scale impersonal exchange is believed to rely on the development of contract enforcement institutions that facilitate trade by reducing the incentive to cheat. Starting with Greif (1994) economists have also explored how cultural factors may influence the development of impersonal trade. One line of research has focused on how cultural dispositions to individualism and collectivism influence the means by which parties solve the “fundamental problem of exchange”, that of contract enforcement (Greif 2000). Here we study the relationship between individualism/collectivism and the willingness to seek trade opportunities in the first place. As Triandis (2001) notes, “the individualism/collectivism cultural syndrome appears to be the most significant cultural difference [between societies]” (p. 907). One crucial distinguishing feature of individualists and collectivists is how they view their relationships with others. Individualists are believed to value relationships instrumentally, “[balancing] relationships’ costs and benefits, leaving relationships and groups when the costs of participation exceed the benefits and creating new relationships as personal goals shift” (Oyserman et al. 2002, p. 5). Collectivists are believed to value their relationships intrinsically, implying “that (a) important group memberships are ascribed and fixed, viewed as ‘facts of life’ to which people must accommodate; (b) boundaries between in-groups and out-groups are stable, relatively impermeable, and important; and (c) in-group exchanges are based on equality or even generosity principles” (Oyserman et al. 2002, p. 5). These differences may have implications for individuals’ willingness to initiate a new trading relationship. Personal exchange, characterized by repeat interaction, is a fact of human life, but the transition to impersonal exchange, characterized by one-off interactions with strangers and mediated by formal institutions, often involves severing (or weakening) ties to local trade partners in order to form new, potentially more lucrative, ties with an unknown party. In light of the differences between individualism and collectivism highlighted above, even if prospective traders have access to effective institutions, there may be differences in the willingness of individualists and collectivists to embrace new trade opportunities if doing so imposes an externality on existing relationships. While individualism and collectivism are often conceived at the societal level (e.g. Hofstede 1980), psychologists recognize these dispositions as personal characteristics, such that each person lies somewhere on a continuum from purely individualistic to purely collectivistic (Triandis 1995).Footnote 1 Understanding individualism and collectivism as individual traits means that we can measure subjects’ dispositions to individualism/collectivism and correlate them with decisions in an experiment. Thus, to test whether individuals’ cultural dispositions are associated with trade behavior, we report an experiment in which subjects may break off a pre-existing, mutually beneficial exchange relationship with one subject in order to seek a potentially more lucrative exchange with a stranger, and we compare the behavior of individualistic and collectivistic people who are faced with this choice under different institutional arrangements that alter monetary incentives to seek out trade. In our experiments, subjects initially face the (trivial) choice between autarky and risk-free, mutually beneficial exchange with another subject, whose role is purely passive. This choice is repeated for four rounds in fixed pairs to induce an existing relationship between these individuals. Starting in the fifth round, the game tree expands to include a third option: a one-shot, extensive form trust game with a perfect stranger. Trading with the stranger produces a much larger surplus, which the stranger then decides either to keep for himself or divide equally between himself and the first mover. Crucially, the decision to trade with the stranger imposes a negative externality on the passive player: if the first-mover attempts to trade with the stranger, he must forgo the opportunity to trade with the passive player, leaving him in autarky. This expanded game is repeated 4 times with a different subject in the role of the stranger in each round. Given the incentive structure of the game, which involves no repeat interaction in the trust games, the subgame perfect Nash equilibrium (SPNE) for risk-neutral, payoff-maximizing agents involves only trade with the passive player. In three experimental treatments, we vary the availability of an exogenous, formal contract enforcement mechanism in the trust game, under which subjects in the role of the stranger who do not share the surplus are punished probabilistically, potentially altering the incentives for the first mover to choose trade with the stranger. In the baseline no enforcement (NE) treatment, this probability is zero. In the weak enforcement (WE) treatment, the mechanism punishes cheaters with a low probability such that entering the trust game is still not incentive compatible in the SPNE. In the strong enforcement (SE) treatment, punishment is sufficiently likely that the first-mover attempts to trade with the stranger in equilibrium, even though the stranger still cheats. In each of these treatments, we aim to compare the behavior of individualistic and collectivistic subjects in the role of first movers. We follow Talhelm et al. (2014) who employed a measure of individualism and collectivism (hereafter, I/C score) that relies on simple choice problems (a “Triad Task”) to reveal differences in individual cognition that are known to be correlated with cultural dispositions to collectivism and individualism. We describe our reasons for using this task in more detail in Sect. 2.1 below, but here we note that Triad Tasks have been used in psychology to quickly and efficiently measure cognitive differences for many years. Following the definitions of individualism and collectivism introduced above (and explained in more detail below), we hypothesize that collectivists will be less likely to seek out new opportunities for trade with strangers than individualists, when doing so means abandoning an existing trading relationship. Moreover, by increasing the effectiveness of formal enforcement in the WE and SE treatments, we can also test whether improving the quality of institutions can overcome cultural differences in willingness to trade. We find evidence that individualists are more willing to trade than collectivists in all three treatments. Since the contract enforcement mechanism is probabilistic, one concern is that this difference can be explained by different attitudes toward risk that are correlated with dispositions to individualism and collectivism. To address this concern, we also collect an incentivized measure of individual risk attitudes, and we show that this is not significantly correlated with I/C scores, or with behavior in our main treatments. Then we report additional diagnostic experiments and analyses that attempt to facilitate interpretation of the observed behavioral difference between individualists and collectivists in the context of economic theory. In light of our initial findings, we consider two possible ways of incorporating the distinction between individualists and collectivists into economic models. The first approach models the notion that collectivists intrinsically value existing relationships as altruism toward in-group members. In the context of our experiment, then, collectivists may effectively incur an additional cost when abandoning the existing relationship with the passive player to seek trade with the stranger. To test this hypothesis, we design a diagnostic treatment in which the passive player is a robot rather than a human, thus eliminating the negative payoff externality of seeking the new trade opportunity. Indeed in this treatment, we no longer observe significant differences in the trade decisions of individualistic and collectivistic subjects, suggesting that this mechanism may be at work. The second approach considers the possibility that the types react differently to their histories of interaction. In particular, the notion of collectivism also contains the implication that collectivists are more prone to focus on group-level, rather than individual-level characteristics in forming beliefs. As noted above, collectivists tend to perceive individuals from a group as interdependent. If collectivistic subjects understand the strangers to be members of the same group, then an act of cheating by one stranger may be perceived as reflecting a group characteristic. Thus, when cheated once, collectivists may be more likely to be deterred from trade in the future. To explore this second possibility, we conduct additional empirical analyses comparing the dynamic effects of having been cheated in the past on the likelihood of future trade for both types. Consistent with this possibility, we observe a strong negative relationship between having been cheated in the past and the likelihood of future trade for collectivists, but not for individualists. Intriguingly, these differences seem to be mitigated somewhat in the WE and SE treatments as the enforcement institution strengthens, suggesting that institutions may be able to partially supersede cultural dispositions in promoting trade. Finally, in two further exploratory experimental tasks, we probe whether normative and strategic beliefs vary with cultural tendencies in ways that are consistent with our observations. In particular, we ask whether individualists and collectivists differ in the degree to which they believe abandoning trade with the passive player to seek trade with the stranger is normatively acceptable and whether they differ in their beliefs about the likelihood that strangers will trade without cheating. The results here are mixed, suggesting limited differences in normative beliefs across the types but more optimism among individualists about the probability of reciprocation by strangers. Given the small sample size in this portion of the design, we suggest future research explore the relationship between cultural tendencies and beliefs more thoroughly. While economists recognize the important role of formal institutions in shaping economic behavior, there is increasing agreement that institutions are typically endogenous to culture, making it difficult to disentangle their separate effects. By using a laboratory experiment that assigns institutions exogenously and by measuring and controlling individual cultural characteristics we are able to better disentangle the relative contributions of institutions and cultural traits. We focus on cultural tendencies toward individualism/collectivism and their relationship to willingness to seek out trade opportunities, but we believe our approach of using responses to survey measures to assign subjects to various roles in an experiment has broader applicability along these lines. Like Fehr (2009) in his analysis of trust games, we attempt to interpret differences in the willingness to engage in trade in our experiment in terms of both heterogeneous preferences and beliefs, but we tie this heterogeneity to underlying cultural dispositions toward collectivism and individualism. In the two approaches we consider, collectivists and individualists differ in their preferences over the external cost imposed on the passive player by the decision to trade with the stranger, and/or in how they update their beliefs after being cheated. While these interpretations of individualism and collectivism through the lens of economics help us to organize our thinking and partially account for our observations, they do not capture all of our observations perfectly, and we cannot rule out that alternative approaches to modeling these cultural distinctions would prove even more successful. Nevertheless, we believe an attempt to translate the important social psychological distinction between individualism and collectivism into the language of economics is useful even if incomplete. Many previous experiments have attempted to measure differences in preferences and beliefs related to trust and trade (e.g. McCabe et al. 2003; Cox 2004; Cox and Deck 2005). In previous experiments, the belief channel has been explored mainly through implementation of either an individual or a group reputation system, which allows subjects to learn about the past play of others before making their trade decisions. Studies have shown that individual reputation systems induce cooperative behavior in repeated trust games (Bernhard et al. 2006; Bohnet et al. 2005; Bohnet and Huck 2004; Charness et al. 2011). In the same vein, Cassar et al. (2010) used a continuous double auction setup to compare trade behavior in a known-identity “local” versus an anonymous “distant” market. They find that sharing individual reputation information significantly dampens cheating. Healy (2007) shows how the logic of sustaining cooperation via individual reputation (e.g. Kreps et al. 1982) can be extended to group reputation, so that in the presence of Pareto-improving cooperative outcomes and an inferior equilibrium, cooperation can be sustained until the penultimate period of a finitely repeated game. Recent experiments suggest that group reputation information either encourages cooperative behavior (Huck and Lünser 2010) or creates path-dependence (Kimbrough and Rubin 2015). There is no reputation system in our design per se; however, the notion that collectivists conceive of strangers as members of a group, such that past play by one such stranger is informative about future play by another, may be interpreted as collectivist farmers assigning strangers a group reputation. Experiments exploring the link between risk preferences and trust (or trade) are also related to our work. Most research on this subject has shown that trust and risk preferences are independent (Eckel and Wilson 2004; Houser et al. 2010); however, there is some evidence that risk preferences can partially explain trust (Schechter 2005; Karlan 2005). In a cross country study, Bohnet et al. (2008) compared the minimum acceptable probability of reciprocation by player 2 required for player 1 to enter into trade in trust games in which player 2 was either a human or a lottery. The minimum acceptable probability in the lottery treatment offered a measure of risk preferences, and the treatment with a human player 2 combined both risk and trust. They find that in all countries, the minimum acceptable probability is significantly higher in the human treatment, suggesting trust cannot be completely explained by risk preferences. As noted above, our experiments also included incentivized risk preference elicitations, and our data show that risk-preferences are not significant determinants of trade in the NE, WE, and SE treatments; nor are risk preferences correlated with individualism or collectivism. This suggests that observed cultural differences in trade behavior are not driven by differences in risk attitudes. In fact, only in our NoLM treatment, which is designed to eliminate the cultural difference we observe in the other treatments, do we observe a relationship between risk preferences and willingness to trade. Finally, previous studies of the trust game reveal considerable heterogeneity across subjects within a given treatment (see e.g. Johnson and Mislin 2011, for a summary), and our design may contribute to the explanation of some of that variation via heterogeneous cultural dispositions to individualism and collectivism across individuals.",7
22.0,2.0,Experimental Economics,03 April 2018,https://link.springer.com/article/10.1007/s10683-018-9569-0,New Hampshire Effect: behavior in sequential and simultaneous multi-battle contests,June 2019,Shakun D. Mago,Roman M. Sheremeta,,Unknown,Male,Unknown,Male,"The nomination process for the U.S. presidential election consists of a series of nationwide primary elections, beginning with the New Hampshire primary. The significance of this small New England state became entrenched in the quadrennial election politics in 1952, when Estes Kefauver defeated the incumbent President Harry Truman in the primary, leading Truman to abandon his campaign. In 1988, all but one of George Bush’s Republican opponents withdrew soon after the primary, and in 1992, number of Democratic Party candidates dwindled from five to two after the primary (Busch and Mayer 2004).Footnote 1 Controlling for other factors, Mayer (2004) finds that a win in the New Hampshire primary increases a candidate’s expected share of total primary votes by a remarkable 26.6%.Footnote 2 The perception that New Hampshire plays a pivotal and perhaps a disproportionately large role in the presidential election (and thereby derives a wide array of political and economic benefits from that position) led many states to move up the date of their primaries.Footnote 3 ‘Frontloading’ is the name given to a recent trend in the presidential nomination process in which more and more states schedule their primaries near the beginning of the delegate selection process. Clustering of primaries took a huge leap forward in 1988 with the formation of ‘Super Tuesday’ when 16 states held their primaries on a single day in March. By 2008, 24 states held their primary on Super Tuesday held in the first week of February. In 2004, James Roosevelt, co-chair of the Democratic Party Rules Committee proclaimed: “We are moving towards a de facto national primary.” For obvious reasons, with naturally-occurring data, it is difficult to examine the exact impact of the two alternative electoral structures on both election outcomes and their economic efficiency. For this reason, we use a controlled laboratory experiment to compare a sequential contest, such as the current presidential primaries, to a simultaneous contest, as reflected in a counterfactual national primary.Footnote 4 Our theoretical framework is based on Klumpp and Polborn (2006). In this political contest model, candidates have to win the majority of electoral districts in order to obtain a prize—the party nomination. As in Tullock (1980) and Snyder (1989), candidates can influence the probability of winning an electoral district by their choice of campaign expenditure in that district. In case of a sequential contest, theory predicts that candidates should spend disproportionately larger amounts in the earlier districts than in the later districts. This difference in expenditure is attributed to the “New Hampshire Effect.” That is, the outcome of the first election creates asymmetry between ex-ante symmetric candidates in terms of their incentive to spend resources in the next district, which in turn, endogenously increases the probability that the winner of the first district will win in subsequent districts and attain the final prize. For example, in a sequential contest with three districts (battles), the winner of the first battle wins the overall contest with probability of 0.875. Furthermore, the intense concentration of expenditure in the initial battles entails that there is a 0.75 probability that the contest will end in only two battles. In contrast, in case of a simultaneous contest, candidates are predicted to spend equal amounts of resources in all three battles. An important consequence of this temporal difference in contest design is that the sequential contest is predicted to induce lower expenditure than the simultaneous contest, which potentially could explain why political parties may choose the sequential electoral structure of the primaries in order to minimize wasteful campaign expenditure.Footnote 5 Our laboratory study of the three-battle contests provides evidence for the New Hampshire Effect. We find that the strategic advantage created by the winner of the first battle makes it more likely for him to win the entire contest. However, contrary to the theoretical predictions, we find that sequential contests generate significantly higher (not lower) expenditure than simultaneous contests. This is mainly because in sequential contests, there is significant over-expenditure in all three battles. We suggest sunk cost fallacy and utility of winning as two complementary explanations for this behavior and provide supporting evidence. Analogies between our laboratory environment and the naturally-occurring political contests are imperfect. Our design choices were made to facilitate analytical tractability and sharp experimental investigation, and do not capture all the details of the real-world elections. For instance, we assume that both contestants are symmetric and do not account for factors such as name recognition, time of announcing candidacy, information aggregation, or the nature of campaigns. We also ignore the carryover effect of winning (Schmitt et al. 2004), bandwagon effect (Callander 2007) and the conditional promise of additional funding upon winning the primary (Feigenbaum and Shelton 2013). However, at least some of these factors do not detract from our findings. For instance, theory predicts that the New Hampshire Effect holds even if players are asymmetric in the sense that one is a better campaigner or has assured win in certain districts (Klumpp and Polborn 2006). In fact, the exogenous ex-ante asymmetry is further strengthened in sequential contests by the endogenous ex-post asymmetry. Similarly, bandwagon theory and carryover effect provide additional rationale for momentum to shift forward to earlier battles, thereby reinforcing our results. Finally, sequential and simultaneous contests are not just restricted to political contests, and can in fact be employed to study resource allocation problems in military and systems defense (Clark and Konrad 2007), research and development portfolio selection (Clark and Konrad 2008), and advertising (Friedman 1958). By contrasting sequential and simultaneous multi-battle contests in the simplest possible framework using laboratory data, which is untainted from the various complicating factors that plague naturally-occurring data, we provide a direct empirical test of the theoretical model of primary elections by Klumpp and Polborn (2006).
 The rest of the paper is organized as follows. In Sect. 2, we provide a brief review of the multi-battle contest literature, both theoretical and experimental. Section 3 presents our theoretical framework and Sect. 4 describes the experimental design, procedures and hypotheses. Section 5 reports the results of our experiment and Sect. 6 concludes.",12
22.0,2.0,Experimental Economics,21 April 2018,https://link.springer.com/article/10.1007/s10683-018-9571-6,Communication in bargaining games with unanimity,June 2019,Marina Agranov,Chloe Tergiman,,Female,Female,Unknown,Female,"Communication is an integral part of bargaining processes. Formal bargaining is typically preceded by informal conversations between involved parties who attempt to influence bargaining results. For instance, in legislative policy-making, legislators spend a considerable amount of time and resources communicating with each other before bringing bills to the floor for a vote. Similarly, in arbitration cases and trade agreements, negotiations between conflicting parties are a vital part of the bargaining process. In many of these examples, bargainers communicate with each other despite the fact that they have commonly known unaligned preferences regarding bargaining outcomes and there is no incomplete information embedded in the environment. Great strides have been made in the bargaining literature to better understand the effects of communication on bargaining outcomes and on the bargaining processes underlying these outcomes. Much of the work that analyzes the effects of communication in complete-information multilateral bargaining setups comes from experimental studies. Agranov and Tergiman (2014) were the first to introduce communication in multilateral bargaining and showed that this alone allows proposers to increase their shares to nearly theoretical levels. Work by Baranski and Kagel (2015) further confirmed these findings. Both of these studies focused on bargaining games that use a majority voting rule to pass proposed allocations.Footnote 1 A majority voting rule means that a proposer allocates positive shares only to a subset of committee members (the minimum winning coalition, i.e. those who are expected to support the proposed allocation) and appropriates the remaining funds. The introduction of communication under a majority voting rule allows the proposer to create an auction for a place in the coalition among non-proposers, which ultimately drives the shares of coalition partners down via a “competition effect”. By exploiting this competition effect, the proposers are able to appropriate a higher share of resources in committees that allow negotiations between its members compared to those that restrict such negotiations. In this paper we investigate whether communication in bargaining has a role that goes beyond allowing competition to flourish. We do this by studying multilateral bargaining with a different voting rule often used in real-life bargaining situations: the unanimity rule, which requires support of all committee members. By changing the voting rule from majority to unanimity, we completely remove the competition effect discussed above. Specifically, we run a series of laboratory experiments in which a five-member committee is charged with allocating a unit of resources between its members using a standard bargaining protocol à la Baron and Ferejohn (1989). At the beginning of the bargaining stage, one member of the committee is chosen at random to propose a budget allocation. All committee members observe this allocation and vote either to support or reject the proposed budget. If all members support the allocation, then the budget passes, the game ends and the committee dissolves. If the budget is rejected, then the committee goes into the next bargaining stage with the same bargaining protocol. Our treatments vary whether committee members can communicate with each other before the proposed allocation is submitted.Footnote 2 Given that the “competition effect” is absent under a unanimity rule, one might expect that communication under such a voting rule might have no impact on proposer power. However, our results show that this is far from the case. In fact, adding communication under a unanimity rule has two strong effects. First, contrary to the competition effect observed in majority settings, proposers lose power and appropriate a smaller share of resources when members can communicate with each other. Specifically, without communication, in the vast majority of passed proposals (more than 85%), the proposer appropriates a higher share than any other committee member, while more than 90% of all passed allocations in the treatment with communication are exact five-way equal splits. Second, communication improves efficiency both in terms of the likelihood of delays occurring and, consequently, the overall resources appropriated by bargainers. Committees that allow communication rarely experience delays in reaching decisions (less than 7% of the cases) and appropriate 99% of available resources. Without communication, delays are very common and occur in 44% of the cases and committees lose about 15% of available resources. This last result speaks to the debate regarding the disadvantages of the unanimity rule (See the discussion in Buchanan and Tullock 1962; Miller and Vanberg 2013). To investigate communication patterns that arise in committees that use the unanimity rule, we analyze communication logs. We have three main results in this respect. First, when unanimous support is required to pass proposals, subjects choose to communicate publicly by sending messages that are delivered to the entire committee. This is strikingly different from the overwhelmingly private chatting behavior observed under a majority rule setting, as documented by Agranov and Tergiman (2014). Second, public and private statements serve opposite purposes. When subjects choose to communicate privately via back-room channels, they lobby for themselves, in particular their place in the coalition and their share. When subjects choose to make public statements, these statements serve to promote equality and a more egalitarian distribution of resources between all members. The prevalence of public messages coupled with the content of these messages is the driving force behind the egalitarian allocations observed in committees that use a unanimity rule and permit communication between its members. Third, there is a strong correlation between the actions of proposers and the content of the deliberation process despite the cheap-talk nature of such communication. The proposed allocations are more likely to feature an equal split of resources between all committee members and a smaller proposer share when members discuss fairness. The opposite is true when members discuss self-interest. Overall, the results documented in the current study and in Agranov and Tergiman (2014) show that the impact of communication on bargaining greatly depends on the voting rule in place. Under a majority rule, communication leads to higher proposer power via the competition effect. However, when the competition effect is removed, as is the case under unanimity rule, communication is detrimental to the proposer and completely removes proposer power.Footnote 3 Absent the competition effect, communication is used as a tool to promote and implement social norms via pressure for egalitarian allocation of resources. In committees that use unanimity rules and allow communication, these social norms become the dominant force determining bargaining outcomes, which are for the most part fully egalitarian and feature the highest efficiency levels. Given the presence of unanimity rules in real-life committees, these effects are important elements to consider when thinking about the optimal amount of interaction between bargainers and, more generally, the transparency of the bargaining process.Footnote 4 Our paper contributes to a large experimental body of literature that studies the complete information multilateral divide-a-dollar game. Much of this literature has adopted the stylized bargaining protocol of Baron and Ferejohn (1989) and has focused on analyzing the effects of various political institutions on the distribution of resources and bargaining efficiency. This literature is reviewed in Morton (2012) and Palfrey (2016). Among other things, studies have analyzed the comparative static predictions of the Baron–Ferejohn model (McKelvey 1991; Diermeier and Morton 2004), the effects of amendment rules (Frechette et al. 2003), of bargaining protocols (Frechette et al. 2005a), of voting rules (Miller and Vanberg 2013), of veto power (Kagel et al. 2010), and the more general determinants of voting behavior (Frechette and Vespa 2017). The present paper is, to the best of our knowledge, the first to study multilateral bargaining in committees that use a unanimity rule and allow communication between group members. The rest of the paper is structured as follows. The bargaining game and theoretical predictions are described in Sect. 2. The experimental design is presented in Sect. 3. Our experimental results are reported in Sect. 4. Finally, in Sect. 5 we offer some conclusions.",15
22.0,2.0,Experimental Economics,18 April 2018,https://link.springer.com/article/10.1007/s10683-018-9572-5,Good news and bad news are still news: experimental evidence on belief updating,June 2019,Alexander Coutts,,,Male,Unknown,Unknown,Male,"The ability to process new information in forming and updating beliefs is critical for a wide range of important life decisions. Students receiving grades adjust beliefs about their ability to succeed in different majors before declaring, entrepreneurs may be awarded or denied funding for their projects and must update beliefs about the viability of these projects, smokers who are informed of new health statistics on the dangers of smoking must update about these risks in deciding whether to quit. In modeling such situations, it is typically assumed that individuals use Bayes’ rule to update their beliefs. Individuals who receive partially informative signals about states of the world are assumed to incorporate this information in an unbiased, calculated way. While Bayesian updating is the current paradigm theoretically, it is also accepted that it has a strong normative basis. Given the importance of updating beliefs for decision making in economic contexts, experimental evidence on updating has been studied for some time (e.g., Kahneman and Tversky 1973; Grether 1980, 1992; Camerer 1987, 1995; Holt and Smith 2009). These studies greatly contributed to our understanding of how individuals update their beliefs, and highlighted the existence of cognitive biases, as updating deviated from Bayes’ rule in systematic ways. Even so, the nature of the updating tasks in these studies differed considerably from the real updating decisions that motivated them. Unlike updating decisions that are economically relevant to individuals, updating in lab experiments typically involved events such as drawing balls from urns, where subjects hold no personal or financial stake in the outcome, beyond an incentive payment for accuracy. Henceforth, I refer to such events as value neutral; information about them is just news, neither good nor bad. In contrast, value relevant or valenced events are those in which an individual strictly prefers one outcome to another, and news about the outcome can be categorized as good or bad. This distinction may be critically important, as there is now a small but growing body of theory and empirical evidence suggesting that there exist further psychological biases in how information is processed in value relevant contexts, depending on whether it is perceived as good or bad news (e.g., Eil and Rao 2011; Sharot et al. 2011; Ertac 2011; Mobius et al. 2014; Kuhnen 2014). Drawing in part on this evidence, Sharot et al. (2012) claim: “Humans form beliefs asymmetrically; we tend to discount bad news but embrace good news.” In this paper I examine whether updating differs across value relevant and neutral contexts. My primary focus is on understanding whether there exist additional psychological biases which lead to asymmetric updating when news is good or bad, beyond the cognitive biases which have been previously found for value neutral contexts. I examine binary events that are either (1) ego relevant, (2) financially relevant, or (3) value neutral. These consist of two uncertain events that are objective in nature, involving the rolling of virtual dice; one subjective, involving estimation of historical temperatures; and one that pertains to ego, involving relative performance on a math and verbal skills quiz. Financial relevance is introduced randomly at the subject-event level, with the endowment of additional financial prizes of $80 in the outcomes of interest. As this experiment utilizes a financially incentivized belief elicitation procedure, this introduces a different type of financial stake. To minimize confusion, I refer to this type of financial stake as an accuracy payment. Information comes in the form of partially informative binary signals regarding the outcomes of the events. I elicit beliefs utilizing the incentive compatible elicitation procedure of Grether (1992), Holt and Smith (2009), and Karni (2009). The primary analysis focuses on between subject variation in updating patterns and follows Mobius et al. (2014) in estimating an empirical model of belief updating that nests Bayesian updating as a special case, but allows for differential response to affirmative versus negative signals. The elicitation procedure improves on previous work that utilizes other elicitation procedures, such as the quadratic scoring rule (QSR), that are not invariant to subjects’ risk preferences.Footnote 1 The results show that, common to previous studies, updating behavior deviates from the strict mechanics of Bayesian updating. Updating is conservative, with many non-updates, and asymmetric, with negative signals receiving more weight than affirmative signals. I find evidence that observed asymmetry is affected by the sequence of signals received, with more negative sequences showing more negative asymmetry, a new type of confirmation bias. Yet critically, these deviations do not differ across value relevant and value neutral contexts, i.e. regardless of whether signals contain good or bad news, or are simply conveying neutral information. While I am able to reject the Bayesian benchmark with statistical precision, posteriors are well approximated by those calculated using Bayes’ rule. These results are consistent with Holt and Smith (2009) who found important deviations from Bayes’ rule, yet also that average posteriors appear to approximate their Bayesian counterparts well, particularly for intermediate priors. Overall the analysis indicates the importance of observing a broad set of counterfactual belief updates, as results of this paper demonstrate how narrowly comparing two events can lead to conclusions that don’t hold up to broader comparisons with other events. Specifically, updating patterns appear more asymmetric when updating about one’s own performance on the ego relevant quiz rather than another’s performance, yet similar asymmetry is present when subjects update about objective dice events. These results thus caution against attributing biased updating patterns to contexts where such bias is psychologically plausible, as updating patterns are similar across settings where such bias is clearly implausible. The remainder of the paper is as follows. The following section discusses recent theoretical and empirical work investigating belief updating. Next, I outline the experimental design, followed by a description of the results, and concluding with a brief discussion.",77
22.0,2.0,Experimental Economics,16 February 2018,https://link.springer.com/article/10.1007/s10683-018-9563-6,Social-status ranking: a hidden channel to gender inequality under competition,June 2019,Arthur Schram,Jordi Brandts,Klarita Gërxhani,Male,Male,Female,Mix,,
22.0,2.0,Experimental Economics,27 June 2018,https://link.springer.com/article/10.1007/s10683-018-9579-y,Bargaining under time pressure from deadlines,June 2019,Emin Karagözoğlu,Martin G. Kocher,,Male,Male,Unknown,Male,"Whether it is wage negotiations, climate negotiations, political negotiations on disarmament deals or contract negotiations in general—a common feature is (often severe) time pressure toward the deadline for striking a deal in bargaining. Last-minute deals in international negotiations or in wage bargaining happen frequently. Debt ceilings such as the ones for the U.S. government or similar inherent deadlines create time pressure for negotiators that are involved in political deal making. In sports, registration deadlines for competitions such as the UEFA Champions League require teams to strike a deal on player sign-ups before a specified deadline, with very severe consequences if the deadline is missed. Despite the obvious relevance of deadlines and time pressure in bargaining, the theoretical investigation of time as a variable does often not lead very far, as equilibria in most frequently used bargaining models are implemented instantaneously. This is, for instance, true for the Nash bargaining game (Nash 1953), alternating offers bargaining game (Rubinstein 1982), and many versions of war of attrition games (Bulow and Klemperer 1999).Footnote 1 However, for practical bargaining problems, the timing of offers and deadlines play a central role in bargaining strategies and outcomes. As a consequence of the instantaneous nature of the equilibrium prediction, or the intricacies of full-fledged theoretical models taking timing into account, economists have mostly neglected issues of time pressure and deadlines in empirical assessments of bargaining.Footnote 2 This paper provides empirical insights based on an experiment in a rich bargaining context (Gächter and Riedl 2005; Karagözoğlu and Riedl 2015; Bolton and Karagözoğlu 2016; Camerer et al. 2017) that yet has enough structure to rigorously control for important aspects in bargaining. It extends the scarce existing evidence on the effects of deadlines from simple and highly structured bargaining games such as the ultimatum game (Sutter et al. 2003; Cappelletti et al. 2011) to a more realistic environment that allows for taking strategic timing decisions of offers and other bargaining parameters explicitly into account. We implement time pressure resulting from a binding and known deadline.Footnote 3 Failure to complete tasks or reach agreements on time and the mistakes/bad decisions made under deadline pressure can have important economic consequences. Thus, for economists, it is relevant to understand the dynamics of bargaining with deadlines and time pressure. It may ultimately lead to richer models of bargaining considering time, timing, and psychological aspects of time as explicit variables. Our setup builds on the setup of Roth et al. (1988) and more recent experiments by Gächter and Riedl (2005) as well as, for instance, Karagözoğlu and Riedl (2015). The bargaining task is to allocate a salary budget of size X ∈ {x, \(\bar{x}\) } with x < \(\bar{x}\). A real effort task before the negotiations accompanied by a story frame in the experimental instructions creates reference point outcomes yi and yj for the two negotiators i and j. We implement yi  > yj, and x < yi + yj < \(\bar{x}\). The latter condition mplies that implementing the reference levels is feasible with the larger budget but infeasible with the smaller budget. There is a possible second, implicit reference point in such environments: the equal split. The negotiation outcome is an allocation tuple {zi, zj} with zi + zj = X, or disagreement, resulting in the allocation {0, 0} if the two bargainers do not agree. Our experiment uses an unstructured bargaining protocol, which allows the sequence and the timing of offers to be endogenously determined. As summarized by Camerer et al. (2017), (1) the unstructured bargaining protocol offers the researcher much richer data – especially on the negotiation process – than the structured alternatives, (2) it is more realistic than those, and (3) structured theoretical predictions can still be obtained. Our main treatment variable in this setup is the time allotted to bargainers for reaching an agreement. In our low time pressure treatment (LTP), the bargainers are given a 10-min deadline, and in our high time pressure treatment (HTP), they are given a 90-s deadline. Further, we collected data for a 45-s deadline (SHTP: severely high time pressure) as a robustness check. Bargaining takes place anonymously in pairs through a real-time chat on the screen. Subjects bargain only once. We observe a significant and sizeable increase in disagreement rates with high time pressure, and even more so with severely high time pressure. The disagreement rate rises from 4.5% under low time pressure to 31.4% under high time pressure (and to 38.1% under severely high time pressure), implying a very high level of inefficiency. In contrast to the results in many existing studies on structured bargaining, for which disagreement under time pressure is rare, the rich context with subjective entitlements and competing reference points seems to contribute to problems in reaching an agreement. The number of offers and counter-offers in our experiment indicate that it is not the physical limitations of time pressure that lead to the high numbers of disagreement. Even under severe time pressure, bargainers make several offers and counter-offers, but agreements often take place much closer to the deadline. Our data indicate that the time pressure has more explanatory power when explaining disagreements for those pairs with a larger difference in their first proposals. This observation can—at least partially—explain the much higher disagreement rate. It seems as if more distant initial proposals, and thus a higher level of initial conflict, can still be compensated for in negotiations with a looser deadline; but with a tight deadline and thus time pressure, the situation may lead to an impasse more easily. Interestingly, there are only small differences in the nature of the agreements (if reached) across the time pressure conditions. We observe a lower likelihood of implementing the implicit reference point (i.e., the 50–50 split) in agreements under high time pressure than under low time pressure. All other variables (e.g., first proposals, concessions) are identical in the two conditions.",18
22.0,2.0,Experimental Economics,03 February 2018,https://link.springer.com/article/10.1007/s10683-018-9562-7,The aggregate impacts of tournament incentives in experimental asset markets,June 2019,Debapriya Jojo Paul,Julia Henker,Sian Owen,Unknown,Female,Female,Female,"Professionals in the intensely competitive world of finance routinely vie for ‘prizes’ such as bonuses, fund flows, and promotions that are tied to their performance relative to others. Compensation of this nature, dependent on an employee’s relative rather than absolute performance, is referred to as a tournament. The sizeable upside provided by these compensation structures, often not matched by an offsetting downside, creates ‘convex’ incentives that can encourage some ‘contestants’, particularly those who trail the leader, to take more risk in an attempt to ‘win’. In light of recent upheavals in financial markets, concerns have been raised that such incentives may precipitate market instability by encouraging excessive risk-taking and short-termism (Rajan 2006; Bebchuk and Spamann 2009; Wagner 2013).Footnote 1 Yet despite the obvious importance of these concerns, the aggregate effects of tournament incentives have only been studied in a handful of experimental studies and are not well understood. In this study, we extend the nascent experimental literature by examining how the composition of tournament incentives affects price behaviour in markets. Specifically, we attempt to disentangle the role that rewards and penalties embedded into tournament contracts play in propagating price bubbles. By investigating the impact of adding an underperformance-induced penalty to a tournament contract, we examine an issue which the experimental literature has paid scant attention to despite theoretical and empirical evidence showing that disincentives or ‘sticks’ in tournament contracts curtail risk-taking by contestants (e.g. Gilpatric 2009; Qiu 2003; Kempf et al. 2009; Hu et al. 2011). Hence, it is possible that the behaviour observed under tournament incentives in existing studies—larger price bubbles relative to ‘normal’ incentives that worsen with trading experience (James and Isaac 2000; Cheung and Coleman 2014)—is driven by excessive speculation arising from a lack of consequences attached to poor performance. Conversely, since the fear of underperformance potentially encourages traders to herd (Rajan 2006; Dass et al. 2008), the addition of a penalty may result in even higher prices. The impact of penalties on market prices is thus an open empirical issue. The impact of rewards in tournament contracts is similarly unclear. The most well-known mechanism through which bonuses can affect prices is by inducing convexity in payoffs, which is known to be associated with risk-taking and bubble propagation (e.g. Allen and Gorton 1993; Allen and Gale 2000). However, in the presence of herding, the payment of bonuses can incentivise traders to break away from the herd in an attempt to win the bonus, thus acting against the propagation of the bubble (Dass et al. 2008). Hence, the price impact of rewards in tournament will depend on the net of these two countervailing forces. To examine these issues, we implemented a between-subjects experimental design featuring four treatments—an absolute-performance based or ‘normal’ incentive treatment called Linear and three tournament treatments, Carrot, CarrStick, and Stick. The compensation contracts in the Carrot, Stick, and CarrStick treatments reward, penalise, and both reward and penalise traders, respectively, on the basis of their performance relative to the ‘average’ trader. Participants in all treatments traded in a Smith et al. (1988)-type experimental asset market featuring two risky assets—a low-risk asset called X, which paid a modestly sized dividend in each period, and a high-risk asset called Y, which paid a much larger dividend but with lower probability. We find that in markets containing inexperienced traders, embedding a penalty into a tournament contract (CarrStick) reduces the amount of trading activity compared to the corresponding reward-only contract (Carrot). However, consistent with the herding hypothesis, the trading activity that occurs in CarrStick markets is characterised by significantly longer periods of overvaluation in both risky assets, in addition to significantly higher prices in the high-risk asset, as gauged by the relative size and duration of mispricing/bubbles in the markets of the respective treatments. We also find some evidence for the presence of greater herding in the CarrStick treatment at the individual level, where the correlation between traders’ bid/offer behaviour is stronger in CarrStick than in the other treatments. On the impact of rewards, we find that convexity effects predominate in markets with inexperienced subjects. The removal of rewards (Stick) is associated with lower prices and shorter periods of overvaluation, but more trading activity, compared to reward-and-penalty contracts (CarrStick). As a result, price behaviour in the Stick treatment is not significantly different to that in the Carrot treatment. The effects of penalties and rewards largely dissipate and potentially even reverse with experience. However, we interpret the in relation to rewards cautiously because the demographics of the Stick subject pool appear to differ from those of the other treatments. In secondary results, we do not find any compelling evidence that tournament incentives distort prices more than absolute-performance based incentives (Linear) in our two-asset market. Moreover, we find that, with the exception of the Stick treatment, bubbles in markets with tournament incentives do moderate in size and duration as traders gain experience of the experimental design. In our single asset markets, price behaviour generally more closely resembles behaviour seen in the existing single-asset literature, which raises the possibility that the results of existing studies are influenced by the single-asset nature of their markets. The remainder of this paper is structured as follows. In Sect. 2, we review the related literature and develop testable hypotheses. Section 3 details the experimental design while Sect. 4 describes the results. We present conclusions in Sect. 5.",3
22.0,2.0,Experimental Economics,28 February 2018,https://link.springer.com/article/10.1007/s10683-018-9564-5,Entry by successful speculators in auctions with resale,June 2019,Marco Pagnozzi,Krista J. Saral,,Male,Female,Unknown,Mix,,
22.0,2.0,Experimental Economics,30 January 2018,https://link.springer.com/article/10.1007/s10683-018-9561-8,"Nice to you, nicer to me: Does self-serving generosity diminish the reciprocal response?",June 2019,Daniel Woods,Maroš Servátka,,Male,Male,Unknown,Male,"Do you care whether a person is genuinely (selflessly) kind to you or whether he just pretends to be so he could reap future benefits? Does your response to his kind action depend on whether his behavior is potentially strategic (self-serving)? There are many everyday situations where the distinction between genuine or strategic intent plays a crucial role in determining the intensity of behavioral response. Consider the following scenario. You are at a restaurant in a country with a tipping culture and the waiter is providing an extraordinary service. You realize that his kindness might be disingenuous and that he might be pretending to be nice in order to extract a higher tip. How do you tip him? Do you care about the possible intentions behind his action, which while being beneficial to you, was potentially more beneficial to him? Do you then elect to not reward him with a higher tip or do you tip well, in excess of what you normally tip, because you had a pleasant dining experience thanks to his service? Previous research provides vast evidence that many economic transactions are governed by reciprocity (see Fehr and Gächter 2000; Camerer 2003; Sobel 2005; Fehr and Schmidt 2006; Chaudhuri 2008 for surveys). Kind and unkind intentions behind actions have been identified as an important driving factor of positively (Cox 2004; Cox et al. 2008b; Falk et al. 2008) and negatively reciprocal behavior (Blount 1995; Offerman 2002). Acts of commission, which actively impose kindness or harm, are found to reveal intent to a greater degree and therefore lead to a stronger reciprocal response than acts of omission, which represent failures to act kindly or to prevent harm (Cox et al. 2017). However, not much is known about the intensity of reciprocal responses to the precise nature of those intentions, for example when the level of kindness (or conversely unkindness) increases from ‘kind’ to ‘kinder’. From this perspective, most previous studies could be described as having a ‘binary format’ in that they consider reciprocal responses to kind actions versus actions with neutral or no intentions, or pitch kind actions against unkind ones. Our research takes a step towards the ‘continuous format’ as we keep the underlying actions kind, but vary intentions in a way suggested by the above example with the waiter who is being strategically kind. As reciprocity is particularly sensitive to perceived intentions, distinguishing between genuine and strategic intentions is central for understanding of the origins of reciprocal behavior. In this paper we therefore formally develop a conjecture that a self-serving generous action (a specific type of kind behavior that is strategic or fortuitously beneficial to the decision-maker) leads to a weaker positively reciprocal response than a selfless generous action. To pin down the terminology, in the spirit of the Revealed Altruism theory by Cox et al. (2008a) we define self-serving generosity as a giver’s (henceforth, First Mover or FM) action that directly benefits the recipient (henceforth, Second Mover or SM) by increasing her maximum payoff, while also benefiting the FM by increasing his own maximum payoff by more than that of the SM. Similarly, if the action results in a smaller increase (or a decrease) in the FM’s maximum payoff, we classify this as selfless generosity. To investigate reciprocal preferences, we focus on how the SM reacts after a FM chooses either a self-serving or selfless action. Importantly, we keep the underlying FM’s action otherwise equally generous in both cases. This gives us a clean test of the strength of reciprocal responses to actions that are selfless and self-serving. We experimentally test our conjecture in two novel designs that allow us to vary the precise nature of intentions (self-serving or selfless) in a way pinned down by the underlying theory to study the ‘primitive’ of reciprocity (i.e. the kinder you are to me, the more I am inclined to be kind back). We find some evidence that subjects perceive self-serving generous actions as being less generous than selfless generous actions, which would imply a different reciprocal response from the above basic (naïve) interpretation of reciprocity. Despite that, we find no support for our conjecture on the diminished reciprocal response. Our results have important theoretical implications as they suggest that considerations for self-serving generosity can largely be ignored in models of reciprocal behavior.",5
22.0,2.0,Experimental Economics,31 March 2018,https://link.springer.com/article/10.1007/s10683-018-9567-2,Why do people keep their promises? A further investigation,June 2019,Steven Schwartz,Eric Spires,Rick Young,Male,Male,Male,Male,"Considerable evidence suggests that individuals keep promises even when it is contrary to their financial self-interest. The natural question that arises is why individuals keep such promises. Two suggestions have emerged in the literature. The first is that, in the event a promise affects another individual’s expectations, promisors do not wish to disappoint the promisee, which we refer to as the expectations rationale. The second is that individuals have a preference for promise keeping per se, referred to as the commitment rationale. The expectations rationale for promise keeping was first suggested by Charness and Dufwenberg (2006) (CD), who build on an earlier study of Dufwenberg and Gneezy (2000) that examines expectations but does not include promises. CD posit that promisors wish to avoid the guilt associated with breaking their promises, or more literally, promisors wish to avoid violating what they believed to be the expectations of the promisee. Using a trust game and measurements of promisors’ beliefs regarding promisees’ beliefs (second-order beliefs), CD find evidence consistent with the expectations rationale. However, there are other explanations consistent with CD’s results, in particular, false consensus bias, which is the tendency for individuals to think others are more like them than they actually are. Several studies follow up on CD. Kawagoe and Narita (2014) elicit the trustor’s first-order beliefs and communicate them to the trustees but find little support for the expectations rationale.Footnote 1 In contrast, Ederer and Stremitzer (2016) manipulate second-order beliefs by administering games that differ in the likelihood that promisors can keep their promise. They find that first- and second-order beliefs and promisors’ willingness to keep their promise (conditional on the ability to do so) are all higher if the probability of being able to keep one’s promise is higher. They interpret this as evidence of the relevance of others’ expectations in promise keeping. Vanberg (2008) also conducts an experiment similar to CD, but designed to eliminate false consensus bias, and concludes that second-order beliefs are not sufficient to explain promise keeping.Footnote 2 Vanberg introduces an alternative explanation for promise keeping stating that “people have a preference for promise keeping per se” (p. 1467), which he refers to as a commitment rationale for promise keeping. In his experiment half of the promisors were switched to different promisees. It was found that promises made by former partners do not affect current partners’ behavior. Also a promisor who made a promise to a former partner does not appear to adhere to the promise when dealing with a new partner. However, questions on the role of expectations remain. As Bicchieri (2006, p. 25) notes when discussing social norms, “Guilt… presupposes the violation of expectations we consider legitimate” [emphasis in the original]. We suggest there is no particular reason why an individual would necessarily consider as legitimate the expectations of another if those expectations were not conditioned on that same individual’s promise. We propose a new, clearer dichotomy that consists of an internal consistency rationale and a communication rationale. The former, first proposed by Ismayilov and Potters (2016) (I&P), is a narrower version of the commitment rationale. The substance of this rationale is as follows: the mere fact that an individual explicitly states his or her intentions carries weight, even if the promisee is unaware of the promise. There is no role for expectations, as undelivered or unheard promises cannot affect first- or second-order beliefs. The latter element of the dichotomy, the communication rationale, is the non-overlapping alternative to the internal consistency rationale that comprises all aspects of the promisee learning of the promise, including, but not limited to the effect of the promise on the expectations of the promisee. In a similar vein, I&P conduct an experiment designed to test the validity of the internal consistency rationale. Their experiment is similar to CD’s except that 50% of the messages by trustees are never delivered. Trustees know the delivery status of their message when making their decision. I&P find evidence for the internal consistency explanation for promise keeping: promises were associated with more trustworthy behavior than non-promises and delivery status did not affect trustworthiness differently for promises and non-promises. They found no support for the social obligation rationale, which is their alternative to the internal consistency rationale.Footnote 3 However, using data from a second experiment that does not include promises, I&P conclude that people who make promises are inherently more trustworthy, and importantly, that promises themselves play no role. We examine the role of both the internal consistency and the communication rationales for promise keeping with an experiment that differs from I&P’s in several ways. In particular, our experiment employs (1) a wider range of trustworthy behavior (as opposed to a binary return trust/do not return trust choice),Footnote 4 (2) a within-participant rather than between-participant measurement of the effect of message delivery, and (3) a post-experimental questionnaire. The results of our experiment also differ from I&P. Whereas they find that the delivery of any sort of message is relevant to the behavior of the trustee, regardless of whether it contains a promise, we find that the delivery of the message is relevant only if it contains a promise. Our results show the amount returned by the trustee is 60% higher when the promise is delivered than when it is not delivered. Further, messages that do not contain a promise (delivered or not delivered), as well as undelivered messages that do contain a promise, are characterized by similar returned amounts. The rates are roughly equivalent to those found in similar experiments without promises. Thus, our results support the communication rationale. We find no support for the internal consistency rationale. After analyzing the results of the experiment and observing that simply writing an undelivered promise led to approximately the same amount of trustworthiness as similar experiments without promises, we designed a second experiment to look more closely at the factors that enhance promise keeping. Our second experiment is identical to the first, except the trustee’s message is either delivered before or after the trustor makes her decision. The experiment is intended to help identify which aspects of the promisee learning of the promise create legitimate expectations that the promise will be kept. Specifically, is the legitimizing aspect of the promise that it induces an action by the promisee? If so, late delivery would not be enough to legitimize the promise. Or is the legitimizing aspect simply that the promisee becomes aware of the promise, in which case the timing of the delivery is irrelevant? The question is particularly important, given the suggested relevance of promise keeping to contracts. The results of the second experiment reveal that promisors condition their response on late delivery versus on-time delivery, but significantly less so than on never delivered versus on-time delivery. Specifically, the difference in mean levels of trustworthiness between the two delivery protocols for promises is only about 40% as large in the second experiment as it is in the first experiment. Put differently, timely promises matter more than late promises, but late promises matter more than never delivered promises. The implication is that while implicit contracting may be part of the reason individuals keep their promises it is not the whole story. Had it been so, late arriving promises would be treated the same as non-delivered promises (as neither can be part of an implicit contract), and only on-time delivered promises would be relevant.",8
22.0,2.0,Experimental Economics,06 June 2018,https://link.springer.com/article/10.1007/s10683-018-9575-2,Designing feedback in voluntary contribution games: the role of transparency,June 2019,Bernd Irlenbusch,Rainer Michael Rilke,Gari Walkowitz,Male,Male,Male,Male,"How should feedback provision be designed to foster cooperation in repeated public goods settings? One of the most established observations in experimental research on repeated public goods games is that the behavior of a majority of individuals can be described as ‘conditionally’ cooperative, i.e., individuals cooperate when others do likewise (Fischbacher et al. 2001). Furthermore, Fischbacher and Gächter (2010), Neugebauer et al. (2009) and Muller et al. (2008) find that subjects do not perfectly match the other players’ behavior. More precisely, they tend to contribute a bit less compared to what they think others do. As a result, contributions go down as the game is repeated. Such behavior is called ‘imperfect conditional cooperation’.Footnote 1 To understand how this behavioral tendency unfolds, it is important to learn more about how information on others’ contributions influences the decision to contribute.Footnote 2 In this paper, we systematically investigate how selectively providing feedback on others’ behavior influences contribution behavior. Disclosing behavioral regularities under different feedback designs is of high practical relevance for environments such as teamwork in firms, the maintenance of community goods, or donations to charity. Think, for example, of teamwork. The individual efforts of the team-members might not be easily observable when the team-members are working in different places or at different times. Or, imagine the attempt of a city community to preserve the cleanliness of its public parks or train stations. In such locations, it is typically hardly observable what other community members contribute to sustain the common goods. Likewise, in many charity- or nonprofit-crowdfunding initiatives, donors might not be aware of what others donated. A common feature of these examples is a typical lack of transparency regarding others’ behavior. Workers, citizens, or donors often do not receive complete informative feedback on how others behave. Thus, they are not able to perfectly condition their behavior on others’ contributions. Furthermore, if they learn about single instances of behavior, they typically cannot unambiguously assess how representative these examples are. Firms provide employees with feedback on their peers’ efforts via public performance postings or public recognition programs like ‘Employee of the month’ (e.g., Johnson and Dickinson 2010; Luthans 2000; Nordstrom et al. 1991). Communities initiate campaigns that selectively highlight appropriate behaviors or exemplary citizens to facilitate cooperation.Footnote 3 Charity and crowdfunding organizations post exemplary donations on their websites. For their readers, however, it is not transparent how these examples have been selected.Footnote 4 Despite the fact that the feedback systems are in place, their impact on contributions is not well understood. In particular, what subjects (can) exactly learn about their peers’ behavior, and subjects’ beliefs—presumably shaped by the feedback on their peers’ behavior—might play a crucial role in such dynamics. So far, however, little systematic evidence is available on how (selected) feedback about others’ behavior affects cooperation in repeated settings.Footnote 5 To narrow this gap, we report on two repeated public goods experiments with feedback. In the first experiment, feedback is provided exogenously (i.e, by the experimenters) to the group of contributors. Subjects receive feedback about a single contribution in the group. We vary the way in which this particular contribution is selected as feedback and focus on two simple feedback selection rules: good and bad examples, i.e., subjects receive feedback either about the maximum contribution (MAX) or the minimum contribution (MIN) provided in the previous period in the group. As a control, subjects receive feedback about a randomly selected contribution (RAND). Additional to the feedback selection rule, we alter the subjects’ knowledge of which rule is applied, i.e., the awareness of the feedback selection rule (transparent or non-transparent). In our first experiment, we study how (1) good and bad examples of behavior influence agents’ inclination to cooperate and (2) the transparency about how the feedback was selected shapes the agents’ inclination to contribute. In our second experiment, feedback is selected endogenously, i.e., by a participant of the experiment. To this end, we add a group leader to the group who benefits from the group members’ contributions. The group leader has an incentive to elicit high total contributions. In addition, we make the available feedback rules salient to the group members.Footnote 6 In the second experiment, we additionally study (1) which feedback rules group leaders choose, (2) whether they make the feedback rule transparent or not, and (3) how the group members respond to endogenously implemented feedback regimes. In both experiments, we analyze the dynamics of feedback, agents’ beliefs and contributions over time. The results are as follows: in the first experiment, where the feedback selection rule is exogenously determined, we observe that good examples have a lasting positive effect on average contributions when the feedback rule is non-transparent compared to the control. The positive effect of good examples is not observed when the feedback selection rule is transparent. For bad examples, we find the opposite effect: when bad examples are provided as feedback and the feedback selection rule is non-transparent, cooperation is lower compared to the control. The former is also lower compared to a situation where it is transparent that the provided feedback is the minimum contribution. In the second experiment, we observe that most group leaders choose to provide the maximum contribution as feedback. About half of them make their choice of this feedback rule transparent. If the group leaders decide to keep the feedback selection rule non-transparent, they more often display the maximum contribution as compared to when they decide to make the feedback rule transparent. Most importantly, when the group leaders choose the maximum contribution as feedback, no difference in contributions can be observed between the cases when the leaders make this feedback rule transparent or not. Our analysis on the dynamics of feedback, beliefs, and contributions reveals that most individuals are imperfect conditional cooperators, who contribute a little less than what they believe others do. Feedback shapes subjects’ beliefs and beliefs and contributions are highly correlated, independent of the feedback rule. When the type of feedback is exogenously selected beliefs in the transparent case are lower (higher) in MAX (MIN) compared to the non-transparent case. The same difference between transparent and non-transparent is not observed when the feedback is endogenously chosen. At least this is true for the MAX case. For the other cases we have too small numbers of observations in the endogenous case since those who choose the feedback predominantly select MAX. Presumably this is because they expect providing good examples to be effective in enhancing cooperation. Recipients of feedback, however, seem to expect that good examples are chosen as feedback in the endogenous cases. As a result providing good examples as feedback is only effective when the recipients of feedback are not aware of the available feedback rules and that the feedback actually stems from good examples. The paper is organized as follows: Sect. 2 describes the design of our two experiments. The results are presented in Sect. 3. In Sect. 4, we offer an interpretation of our results, link them to the existing literature, and present potential implications for the design of feedback institutions.",6
22.0,3.0,Experimental Economics,27 August 2019,https://link.springer.com/article/10.1007/s10683-019-09622-1,Introduction to the special issue in honor of Professor Charles R. Plott,September 2019,Lata Gangadharan,Charles N. Noussair,Marie-Claire Villeval,Female,Male,Unknown,Mix,,
22.0,3.0,Experimental Economics,28 September 2018,https://link.springer.com/article/10.1007/s10683-018-9593-0,A new experimental mechanism to investigate polarized demands for public goods: the effects of censoring,September 2019,R. Mark Isaac,Douglas A. Norton,Svetlana Pevnitskaya,Unknown,Male,Female,Mix,,
22.0,3.0,Experimental Economics,02 November 2017,https://link.springer.com/article/10.1007/s10683-017-9549-9,On the scope of externalities in experimental markets,September 2019,Björn Bartling,Vanessa Valero,Roberto Weber,Male,Female,Male,Mix,,
22.0,3.0,Experimental Economics,27 October 2017,https://link.springer.com/article/10.1007/s10683-017-9548-x,Information aggregation in Arrow–Debreu markets: an experiment,September 2019,Lawrence Choo,Todd R. Kaplan,Ro’i Zultan,Male,Male,Unknown,Male,"The idea that markets can aggregate dispersed private information into prices is central to much of economic thinking.Footnote 1 When some traders are fully informed about the true state of the world, competitive bidding will push prices to reflect the true state, thus revealing the information to uninformed traders. A more challenging case is when each trader only holds partial information about the true state. If traders understand how the different pieces of information affect behaviour, the movement of prices eventually reveals the information held by other traders. Furthermore, if traders continuously condition their market demands on their own information, then market prices should continuously move until no trader can learn any more from the prices.Footnote 2 Naturally, whether equilibrium market prices fully reveal the true state depends on the information structure. Radner (1979) showed that prices converge to an equilibrium as if each trader openly and truthfully communicates his own private information. This result, however, requires that traders hold diverse or heterogeneous partial private information and that the true state is in the unique element within the intersect of at least two different private information.Footnote 3 This analysis equates the states of the world with beliefs over the traded assets’ possible valuations. It remains an open question, therefore, whether market prices are able to fully reveal the true state even when traders hold identical or homogeneous partial private information, in the sense that it induces the same beliefs over asset valuations. This paper studies the information aggregation properties of markets in a novel design that allows us to clearly distinguish between situations where traders hold homogeneous and heterogeneous partial private information about the true state of the world. Empirical studies suggest that the intricate process of information aggregation (e.g., learning about the information of others through price movements) postulated by theory often places too many demands on the abilities of the traders [see De-Bondt and Thaler (1985), Thaler (1988), Hirshleifer (2001) and Shleifer (2000)]. However, identifying ‘who knows what’ is often challenging in empirical market research. Controlled experimental environments complete the picture by studying market behaviour in a highly structured environment, where the experiment designer controls the information structure. Laboratory experiments study the general principles underpinning economic behavior with extraordinary control over independent variables, and generally generalise well to various field environments (Camerer 2015). Market experiments, in particular, yield similar results with student and businessmen populations (e.g., Porter and Smith 1994). In a study of information dissemination in experimental single-period markets, Plott and Sunder (1982) found that asset prices converge to fully reveal the true state even when only a subset of the traders had full information about the true state.Footnote 4 Markets where traders have partial information pose a greater challenge to successful convergence, as market prices have a double role of information aggregation and dissemination (e.g., Plott and Sunder 1988; Forsythe and Lundholm 1990; Copeland and Friedman 1987; Camerer and Weigelt 1991; O’Brien and Srivastava 1991).Footnote 5 Plott and Sunder (1988) pioneered the laboratory study of information aggregation in single-period markets structured as event-contingent assets. In their experiments, the true state of the world can either be X, Y or Z with some known probabilities. For each state other than the true state, half of the traders are informed of the false state. For example, let the true state be Y. In this case, half of the traders know the true state to be either X or Y (i.e., not Z), and the other traders know the true state to be either Y or Z (i.e., not X). Thus, traders have heterogeneous information that, if aggregated efficiently, is enough to fully reveal the true state. In the treatments closest to our design, the market consists of three types of assets x, y and z, which pay a positive dividend for the states X, Y, and Z, respectively (e.g., asset x pays a positive dividend if and only if the true state is X).Footnote 6 The results of Plott and Sunder (1988) show that market prices converge to the true value of the asset—thereby revealing the true state—even when there is no immediate incentive for any trader to reveal their private information. However, the information structure used in the experiments greatly facilitates information dissemination and aggregation. For example, if the true event is Y, then the distribution of private information implies that demand for asset y will easily outstrip those for assets x and z.Footnote 7 As such, the market demand for asset y relative to those of assets x and z will immediately reveal the true event to be Y. In this paper, we study a more complex and asymmetric information structure. This structure allows us to test an environment where information aggregation requires counterfactual reasoning, and to test gradual aggregation and dissemination of information. The information structure we employ differs from that of Plott and Sunder (1988) in two important ways. First, it relaxes the one-to-one relation of states of the world to profitable assets (Arrow–Debreu securities) by allowing different states to map onto the same asset. Second, while the states of the world are symmetric and equiprobable, the different possible asset evaluations are not. This asymmetry in information translates to asymmetry with regards to the information held by different types of traders. These two design innovations raise two new issues, respectively. First, as different states of the world map to the same asset value, we generate a situation where, in some states, all of the traders hold exactly the same beliefs over the assets’ values despite having diverse information regarding the underlying state of the world. In previous studies, such as Plott and Sunder (1988), traders always potentially observe some market activity (asks, bids, or transactions) that is inconsistent with some state of the world that they consider possible. Such market activity reveals that there exist other traders who hold different information, and thus conveys new information about the true state. In our setting, in contrast, the different information held by different traders translates to the same beliefs and the same market activity, hence the observed market activity does not provide any new information regarding the true state. The diverse information held by different traders, if aggregated successfully, is nonetheless sufficient to fully reveal the true state. To do so requires counterfactual reasoning: traders should realize that all observed market activity is consistent with their own beliefs about the assets’ values, and consequently eliminate the states of the world in which some of the other traders hold differing beliefs. Our experiment tests whether this feature of the market impedes, or even prevents, successful aggregation of information. Second, in states of the world where different traders do hold different beliefs, we can identify a minority of traders who hold different beliefs to those held by the majority traders. Since the majority traders dominate the market and the price setting, the minority traders stand to better learn new information from the initial movement of prices. Accordingly, we test whether these minority traders become better informed about the true state of the world and about the true value of the assets, and whether they can leverage this knowledge for profit.Footnote 8 Finally, previous studies have focused on aggregate outcomes in the market. Our experimental design introduces yet another new feature. To test individual belief updating and information dissemination, we elicit traders’ post-market beliefs about the true asset-relevant event. This allows us to test whether individual revelation of the true state is a necessary condition for information revelation in market prices. We present a simple fully revealing rational expectations equilibrium (FRE) model, which shows that information aggregation and full revelation of the true state is possible in our setup both when traders’ beliefs over the assets’ values are heterogeneous and homogeneous. Furthermore, when traders hold heterogeneous private information, the model predicts that a minority subset of traders (the minority traders) will learn about the true state before the others and will be instrumental in driving prices towards the FRE. Against this backdrop, three questions are investigated:  Can Arrow–Debreu markets be successful at aggregating private information about the true event into prices, even when traders are endowed with homogeneous beliefs, such that information aggregation is challenging and requires traders to employ sophisticated counterfactual reasoning? Are minority traders better informed about the true event, can they leverage this knowledge for profit, and are they instrumental in driving prices to reveal the true state? Can individuals in the market learn the true event from the movement of prices, and is individual learning a necessary condition for information aggregation in the prices? To measure success at information aggregation in the experimental setting, we benchmark the FRE model against a prior-information equilibrium (PIE) model.Footnote 9 The PIE model assumes that the traders update their beliefs given their private information, but fail to take market activity into account. Thus, we test the ability of the market to aggregate dispersed information by comparing the observed final market prices to the FRE and PIE predictions. The post-market beliefs allow us to test the role of individual beliefs in market prices convergence to the true values and the dissemination of information throughout the market. By repeating the market game, we are able to test the role of market experience in information aggregation and dissemination.Footnote 10 Our findings can be summarised as follows. Markets are successful at aggregating dispersed information in the prices when traders are experienced, but not in early rounds. Although market prices reflect the true state, traders largely remain ignorant in the post-market stage. This finding is consistent with the marginal trader hypothesis (e.g., Forsythe et al. 1992), which posits that efficiency in market prices only require a small population of sophisticated traders, who are influential in driving prices to the competitive equilibrium. We observe successful price convergence even in the more challenging case of homogeneous beliefs. That is, we do not find any significant differences in the information aggregation properties of markets when traders are endowed with homogeneous or heterogeneous private information. Finally, we find some evidence suggesting that minority traders learn about the true event before others and are instrumental in driving prices towards the FRE. There is no evidence, however, that the better-informed minority traders are able to extract a higher share of the surplus.",14
22.0,3.0,Experimental Economics,15 March 2018,https://link.springer.com/article/10.1007/s10683-018-9565-4,Individual speculative behavior and overpricing in experimental asset markets,September 2019,Dirk-Jan Janssen,Sascha Füllbrunn,Utz Weitzel,Unknown,,Male,Mix,,
22.0,3.0,Experimental Economics,07 October 2019,https://link.springer.com/article/10.1007/s10683-018-09601-y,Behavioral sources of the demand for carbon offsets: an experimental study,September 2019,Kai-Uwe Kuhn,Neslihan Uler,,Unknown,Female,Unknown,Female,"In recent years, many organizations have been created to increase individuals’ and firms’ awareness of their carbon footprints (emissions resulting from everyday activities like driving, heating homes/workplaces and production activities) and present them with opportunities to offset/lessen resultant environmental damages. In particular, organizations like myclimate.org, carbonfootprint.com, carbonfund.org, and terrapass.com offer carbon offsets to finance emissions-reducing activities.Footnote 1 Alongside these developments, we have witnessed the establishment of centralized trading institutions, like the Chicago climate exchange, and of private companies serving as aggregators for small scale carbon offset supplies (typical in agriculture). The purchase and trading of voluntary carbon offsets differs from more traditional emissions-targeting schemes (like those commonly observed in the electricity industry), which place ex-ante caps on aggregate emissions, and allow pollution permits to be traded on formal exchanges. In these schemes, emissions reductions are mandated and trade occurs as a consequence of profit-maximizing incentives. Voluntary carbon offsets, on the other hand, do not mandate emissions reductions per se. Instead, they aim to influence individuals’ decisions by making them aware of the externalities they cause, and present them with a mechanism to reduce these externalities by either avoiding externality causing activities or financing carbon emissions reducing investments elsewhere (e.g., supporting energy efficiency projects and tree planting). In this paper we design and conduct a laboratory experiment to assess the determinants of carbon offset purchases, in addition to investigating the effect of the introduction of voluntary carbon markets on emission-causing activities.Footnote 2 Our experiment considers a two-stage environment. In the first stage, subjects are given a chance to trade in a competitive market environment. Each trade among a buyer and a seller generates positive individual returns to the two subjects, but also generates carbon emissions (negative externalities) that harm everyone in the experiment equally. In the second stage, agents are asked to decide how many carbon offsets to purchase to ameliorate all (or part of) these damages. This paper examines how individual emissions, total emissions, and individual surplus from trading affect the individual carbon offset purchase decisions; and whether behavior is homogenous. In addition, by varying whether there is a second stage or not, we investigate the effect of the introduction of carbon markets on the number of trades. Moreover, we vary the price of offsets in order to capture its effect on voluntary offset purchases and trading behavior. Existing experimental research on behavior in anonymous competitive markets appears to confirm the traditional economic view that, in order for individuals to internalize negative externalities, explicit monetary incentives (i.e., taxes) are necessary. Plott (1983) showed that, while in a competitive market (modeled as a double-auction), individuals ignore the negative externalities arising from their transactions, and trading achieves the inefficient competitive equilibrium, an optimal tax to internalize the externality led reliably to efficient solutions.Footnote 3 While Plott’s results suggest that negative externalities cannot be mitigated in a double-auction environment without monetary incentives, this does not necessarily imply that carbon offsets would be ineffective in diminishing externalities. First, making the external effect of trading more salient through an offset market may trigger norm activation for norms requiring the avoidance of causing damage to others (and may lead to ex-ante externality mitigation via a lower number of trades in the double-auction).Footnote 4 Second, the mere presence of a mechanism to make offset payments may mitigate negative externalities, ex-post. In a double-auction with external effects (without the possibility of carbon offsets as in Plott’s set-up), social preferences may not be observed because a given individual’s effect on others is not directly observable and, in addition, involves a complex tradeoff by requiring that fewer privately advantageous trades be completed. Cognitively, it may therefore be difficult to compare costs and benefits of other-regarding behavior in a double-auction setting (Sandel 2012; Kube et al. 2012; Falk and Szech 2013). Voluntary carbon offset markets may activate other-regarding preferences, and might lead to carbon offset purchases to reduce negative externalities.Footnote 5 Before we summarize the results, we highlight that the behavioral motivations for purchasing carbon offsets could be very different than behavioral motivations for voluntary giving in a typical public goods game set-up. First, in the set-up we consider, the degree to which a public goods problem exists depends on the degree of trading in the first stage. Second, and more importantly, subjects may treat damages caused by themselves and those caused by others very differently. Since personal contribution to the negative externalities is observable by subjects, this may affect their decisions regarding their purchases of carbon offsets. In fact, carbon offset schemes usually depend on the belief that carbon reduction should start with the personal responsibility of individuals for the emissions they cause themselves.Footnote 6 We show that, when the price is low, there is a moderate demand for carbon offsets and no decay over periods (in contrast to typical public goods experiments). More importantly, we find that many subjects act based on their personal contribution to the damage: they increase their offset purchases as their own damages increase—a behavior that we will call “personal responsibility” driven contributions. We construct a “personal responsibility index” derived from a series of survey questions eliciting information on subjects’ concern for their contribution to externalities—such as engaging in non-monetary, time consuming projects like volunteering and/or recycling. We show that personal responsibility driven contributions occur among the set of subjects who have a high personal-responsibility index. Interestingly, trading in the double-auction market does not seem to vary with the presence of the opportunity to purchase offsets, nor with the cost at which carbon offsets can be acquired. In other words, the presence of the second stage carbon offset market does not affect the level of negative externalities, ex-ante. There is a demand for carbon offsets, however, which leads to a decrease in the level of negative externalities, ex-post. This seems to indicate a strong tendency for mental accounting (where externalities are ignored in trading, but make a difference when considering carbon offsetting later on). Finally, we show that individuals’ final earnings increase up to about 21% with the introduction of carbon offsets. Surprisingly, improved earnings do not translate into higher efficiency. Carbon offsets provide an opportunity to increase earnings to a great extent, yet individuals fail to use this tool effectively. Due to free-riding incentives, individuals do not buy carbon offsets at the optimal levels and, hence, introduction of offsets decreases economic efficiency. We stress that our results are not only relevant for carbon offset markets, but have much broader implications for collective action problems with negative externalities, in general. Our experiment refrains from using carbon offset terminology. We use terms like “damages” and “damage offsets”, instead of “environmental damages” and “carbon offsets”. This neutral terminology mitigates framing and experimenter demand effects, in addition to making our results generalizable to other settings. Our results show that individuals’ reactions to collective action problems will partially depend on how they contributed to the problem arising in the first place. Specifically, our results shed light on the relationship between the personal contribution to a negative externality problem and the willingness to contribute to its solution.Footnote 7 The remainder of this paper is organized as follows: in Sect. 2, we briefly review the related literature. In Sect. 3, we introduce our novel two-stage trading environment. The experimental design and procedures are presented in detail in Sect. 4. We provide our experimental findings for the second stage of damage offset purchases in Sect. 5, while Sect. 6 explores the impact of anticipated damage offset purchases on trading in Stage 1. Section 7 provides further discussion. Section 8 concludes.",4
22.0,3.0,Experimental Economics,19 April 2018,https://link.springer.com/article/10.1007/s10683-018-9570-7,Directional behavioral spillover and cognitive load effects in multiple repeated games,September 2019,Tracy Xiao Liu,Jenna Bednar,Scott Page,,Female,Male,Mix,,
22.0,3.0,Experimental Economics,07 October 2019,https://link.springer.com/article/10.1007/s10683-017-9559-7,Reference point effects in legislative bargaining: experimental evidence,September 2019,Nels Christiansen,John H. Kagel,,Male,Male,Unknown,Male,"This paper contributes to the literature on voting in legislative bargaining games where a number of experiments have explored the distribution of “cash” benefits or “pork”.Footnote 1 More recently there are legislative bargaining experiments concerned with the distribution of pork along with a policy choice.Footnote 2 None of these experiments address whether bargaining outcomes differ when players bargain over losses rather than gains even though this is also a part of the bargaining process. This paper fills that void by comparing legislative bargaining over “taxes” as opposed to “pork”. Given Prospect Theory (Kahneman and Tversky 1979), bargaining over “taxes” (losses) involves reducing benefits, which might be expected to set off different responses compared to increasing benefits (gains). The experiment is designed to maintain a theoretical isomorphism, under expected utility theory, between gains and costs, so that any differences in outcomes can be attributed to moving from gains to losses. The experiment enables us to explore reference point effects (Kőszegi and Rabin 2006) in a previously unexplored area of interest to both economists and political scientists, and has potential implications for institutional design. Models of legislative bargaining essentially involve committee decision making. This coincides with an early cornerstone of Charles Plott’s distinguished career—his interest in committee decision making and political economy (Fiorina and Plott 1978; Kormendi and Plott 1982) and the path dependence of outcomes in sequential voting (Levine and Plott 1977). These earlier studies of committee decision making involved unstructured bargaining whereas the present research involves structured bargaining, namely the Jackson and Moselle (2002; JM) model where legislators, with heterogeneous preferences, bargain over a one-dimensional public policy issue, along with a distribution of private goods that benefit each legislator’s home district. In an earlier paper (Christiansen et al. 2014; CGK) we analyzed the JM model for the case of expanding benefits: A Baseline treatment in which there are no private goods available to “grease” the legislative bargaining “wheels”, and a Gains treatment in which there are private goods to distribute between potential coalition partners along with a decision on the public policy issue. This paper extends the previous one to a Costs treatment in which legislators must come up with reductions in private goods (aka “taxes”) to cover budget costs. The Costs treatment is structured so that it is theoretically isomorphic to the Gains treatment, resulting in the same stationary subgame perfect equilibrium (SSPE) outcome, the most common theoretical reference point for legislative bargaining experiments. Outcomes are also isomorphic to the efficient equal split (EES), a behavioral model that better organized the Gains treatment in CGK (and will be shown, to better organize behavior under the Costs treatment here as well). There are a number of common outcomes between the earlier Gains experiment and the Costs experiment reported on here: Both serve to “grease” the legislative bargaining process, as there is significantly less delay in passing proposals than when bargaining strictly over a public policy. However, as between Gains and Costs, there is a significantly lower frequency of proposals passing without delay with Costs. Most of this increase has to do with rejections of proposals from the voter who cares the most about the public policy—a 23 percentage point reduction in her proposals passing without delay. In turn, this results in reductions in efficiency by just under 10%, along with a 20% reduction in that voter’s payoff, had her proposals passed at the same rate as in the earlier Gains experiment. This difference is consistent with reference point effects (another of the many topics Professor Plott has written on) in which legislators respond differentially to gains and losses, in conjunction with Prospect Theory type preferences (Kahneman and Tversky 1979).Footnote 3 This paper is related to a growing literature on legislative bargaining experiments. As noted, most early papers focus on purely distributive games, such as the bargaining model in Baron and Ferejohn (1989). Several experiments involve bargaining over two or more dimensions. These include the CGK paper where legislators with heterogeneous preferences also bargain over a one dimensional “public” policy. Other papers include Fréchette et al. (2012) and Christiansen (2015), which investigate the model in Volden and Wiseman (2007) where bargaining occurs over private and public goods, and funds for both come from a common budget. Agranov et al. (2016) examine a dynamic bargaining game over private and public goods where investment in public goods is durable. It is important to note that in each of these experiments bargaining is in the “gains” frame, that is, proposers are offering coalition members a higher payoff than they had before bargaining. The paper is also related to how reference points affect bargaining outcomes because of differential responses to gains and losses. Camerer et al. (1993) study a shrinking-pie, multi-round, bilateral bargaining game and compare the results to an isomorphic treatment in which losses increase over time.Footnote 4 They find more dispersed offers, greater initial rejections and lower proposer payoffs with increasing losses as opposed to an equivalent shrinking of benefits. Other experiments focus on bargaining between buyers and sellers. Neale and Bazerman (1985) show that framing a collective bargaining game between union and management as a gain rather than a loss results in fewer negotiations being sent to arbitration (also see Bazerman et al. 1985).Footnote 5 A similar result is reported in Kristensen and Gärling (1997) where buyers and sellers negotiate over the sale price of a condominium. They show that when buyers perceive the seller’s first offer price as a gain, relative to their reference point, it results in higher counteroffers than if they perceive the first offer as a loss, thereby reducing the overall number of counteroffers and bargaining impasses. Like the bilateral bargaining papers, the experiment in this paper speaks to the question of whether different reference points affect bargaining outcomes. But it extends earlier results in two important ways. First, bargaining is multilateral not bilateral. Second, the bargaining game is more complex than in earlier experiments since individuals with different preferences must simultaneously trade-off private goods along with the location of a common policy. This allows us to explore the implications for efficiency. The paper proceeds as follows: Sect. 2 describes the underlying experimental design used to investigate the JM model. Section 3 briefly reviews results from CGK that provide the starting point for the present experiment. Section 4 describes the experimental procedures and the motivation for the parameter values chosen. Section 5 reports the results and Sect. 6 is a summary and conclusions section.",5
22.0,3.0,Experimental Economics,07 October 2019,https://link.springer.com/article/10.1007/s10683-018-9581-4,The welfare costs of price controls and rent seeking in a class experiment,September 2019,Grace Finley,Charles Holt,Emily Snow,Female,Male,Female,Mix,,
22.0,4.0,Experimental Economics,29 January 2019,https://link.springer.com/article/10.1007/s10683-018-09597-5,Multiple hypothesis testing in experimental economics,December 2019,John A. List,Azeem M. Shaikh,Yang Xu,Male,Unknown,,Mix,,
22.0,4.0,Experimental Economics,07 August 2018,https://link.springer.com/article/10.1007/s10683-018-9587-y,The impact of the level of responsibility on choices under risk: the role of blame,December 2019,Gilbert G. Eijkelenboom,Ingrid Rohde,Alexander Vostroknutov,Male,Female,Male,Mix,,
22.0,4.0,Experimental Economics,09 June 2018,https://link.springer.com/article/10.1007/s10683-018-9574-3,The effects of make and take fees in experimental markets,December 2019,Vincent Bourke,Mark DeSantis,David Porter,Male,Male,Male,Male,"Make and take fees are the prevailing trading fee structure employed by U.S. equity exchanges today. The fee structure is a relatively minor change to the standard double auction but it introduces market complexities and incentives that may alter trader behavior. In an exchange that uses make and take fees, the market participant who enters the market first to post a non-marketable limit order is referred to as the ‘market maker’ while the participant who places a market order to hit the market maker’s limit order is referred to as the ‘market taker’. Upon order execution, the market maker receives a ‘make rebate’ while the market taker is assessed a ‘take fee’. Generally, the absolute value of the make rebate is less than the absolute value of the take fee. This means the market taker effectively pays the market maker and the exchange for facilitating the trade. The exchange receives the balance of the take fee after the market maker receives the make rebate. This balance is referred to as the ‘net fee’. Equity exchanges, who compete for trading volume, offer this fee structure to incentivize market makers to provide opportunities for trade on a particular exchange. The first recorded exchange to use make and take fees, the Island ECN, introduced them in 1997. Their trading volume market share of NASDAQ-listed shares increased from three percent in 1997 to thirteen percent in 1999 (Cardella et al. 2014). In 2007, the SECs National Market System regulations (Reg. NMS) were implemented with the goal of a single market composed of multiple trading venues all linked together via rules over access and trade priority (O’Hara 2015). These regulations created competition between exchanges for trading volume which led to widespread adoption of the make and take fee structure. The fee structure has seen more widespread attention in the past few years—most notably when Michael Lewis questioned the practice of paying market makers to inject liquidity into a system in his book Flash Boys. The SEC has been noncommittal on make and take fees but they believe that elimination would widen spreads and undermine the ability of the exchanges to compete with off-exchange “dark pools” (private venues for trading securities). This paper is the first known attempt to examine the effects of make and take fees using market experiments. We consider the impact of imposing a make and take fee structure on three well-established indicators of market quality– efficiency, bid-ask spread, and book depth. Our primary hypotheses and results focus on the difference in these measures between our make and take fee treatments and a baseline treatment in which no fees (rebates) were assessed (awarded). To ensure our results are specific to the make and take fee structure and not trading fees in general, we execute a second set of experiments in which the same trading fee is assessed to both parties of the transaction. This type of fee mechanism was utilized prior to the introduction of make and take fees. Our main result is that buyers tend to competitively place limit orders in an attempt to gain the make rebate. This results in higher prices leading to sellers faring better than buyers in the presence of make and take fees. The double auction is the workhorse market used to examine behavior in experimental economics because of its operational simplicity, efficiency, and its capacity to respond quickly to changing market conditions (Gjerstad and Dickhaut 1998). The standard rules used in this auction mechanism have not changed significantly since its introduction (see Davis and Holt (1993) Chapter 3 and Friedman (1993) for a review) but significant work has been done on introducing changes to the institution to observe the resulting effects on trader behavior. In the first version of the electronic double auction participants only saw the current bid-ask spread and could only either improve on the standing orders with limit orders or take the standing order with a market order. The first addition to the electronic version was to add a rank queue for bids and asks (Smith and Williams 1983). This rule allows for bids and asks to privately queue up behind the standing bid and ask ranked by lowest asks and highest bids. When a market order is consummated the best offers in the queue are sent into be the next standing bid and/or ask. Participants only knew their place in the rank queue. Smith and Williams (1983) found that there was significant competition in the queue to get to the top rank. This resulted in a faster convergence to equilibrium and narrower spreads. Caginalp et al. (2001) examine the effects of a closed book against an open book, in which all bids and asks at every point in time are displayed publicly in a standard asset market. They find that an open book has a marginal effect in slowing bubbles. In early experiments Smith and Williams (1983) used a five cent commission for each side of a trade to induce marginal traders to be involved but this addition had little effect on outcomes. Similarly, King et al. (1993) assessed a small transaction fee on both sides of an asset market and found no effect on price bubble characteristics. Our paper is the first experimental study to examine the make and take fee structure.",2
22.0,4.0,Experimental Economics,04 July 2018,https://link.springer.com/article/10.1007/s10683-018-9580-5,Strategy revision opportunities and collusion,December 2019,Matthew Embrey,Friederike Mengel,Ronald Peeters,Male,Female,Male,Mix,,
22.0,4.0,Experimental Economics,25 August 2018,https://link.springer.com/article/10.1007/s10683-018-9591-2,Endogenous claims and collective production: an experimental study on the timing of profit-sharing negotiations and production,December 2019,Andrzej Baranski,,,Male,Unknown,Unknown,Male,"Whenever two or more persons, countries, or firms engage in a mutually beneficial activity it is quite common that they will negotiate how to divide the benefits resulting from their joint effort. For example, certain medical groups, law firms, and other partnerships have been reported to hold end-of-year meetings to redistribute profits (or at least a portion of them) while others assign profit shares at the beginning of the year.Footnote 1 In fact, multiple management consulting firms offer advice to partnerships on how to design an optimal partner compensation plan in order to induce effort and maximize profits. One particular question that partners must ask regarding their profit-sharing scheme is: “ Will the distribution be prospective (distribution percentages or units of participation determined in advance of the year) or retrospective (distribution percentages or units of participation determined when year-end results are known)?” (Rose 2011).Footnote 2 In this article we make use of a laboratory experiment to examine how the timing of profit-sharing negotiations with respect to productive decisions affects both the distribution of profits and the efficiency levels that partnerships can achieve. Several studies have been concerned with whether a firm should be jointly or individually owned (Hart and Moore 1990; for an experiment see Fehr et al. 2008), why partnerships and other joint ownership structures exist (Levin and Tadelis 2005; Alchian and Demsetz 1972), compensation systems in partnerships (Gilson and Mnookin 1985; Lang and Gordon 1995), and even on how to dissolve partnerships (Morgan 2004). Here, partnerships are broadly represented as a group of players who posses voting rights and must agree on how to share profits which are endogenously determined via individual voluntary efforts or investments. We focus on the interaction between profit-sharing agreements and rent-generating incentives in a setting in which (1) partners can propose and vote on profit-sharing agreements, (2) no partner has total control over the agenda of negotiations, and (3) all engage in production voluntarily. Our model attempts to depict a democratic and participatory system as opposed to a centrally directed mechanism. Generally speaking, our model is a game of alternating offers and voting in which the surplus to divide is endogenously determined; the latter is an important feature that the standard divide-the-dollar models have typically abstracted from.Footnote 3 We explore the efficiency implications by providing a theoretical benchmark and an experimental test of two different timings: ex post bargaining, which we call redistribution, and ex ante bargaining, which we call pre-distribution. Under pre-distributive bargaining, partners negotiate how to divide ownership shares among themselves and once an agreement is reached, they proceed to a simultaneous game of investments. With redistributive bargaining, investments take place prior to profit-sharing negotiations. Our theoretical framework can be interpreted as the merging of two influential models in economics: the Baron and Ferejohn (1989) model of multilateral bargaining and the linear public goods game. While the canonical linear public goods game exogenously establishes an equal share of the common fund to every player, both redistributive and pre-distributive bargaining allow to exclude some players from consumption of the joint surplus. In spite of the non-excludability violation, the redistribution game can be conceived of as public goods game with a priori undefined claims and the pre-distribution game as one in which claims are endogenously determined via bargaining. A wealth of theoretical and experimental studies exists in the fields of Public Economics, Political Science, and Political Economy that build on the aforementioned games. Thus, our model and experiments provide a bridge between these streams of literature which are discussed in Sect. 2. The theory predicts that in the redistribution game, investments are considered sunk and opportunistic bargaining behavior will dissuade players from investing in the joint project. In contrast, the equilibrium prediction under pre-distributive bargaining entails a distribution of shares that induces at least certain members to invest in the common fund because their return is guaranteed and the hold-up problem is absent.Footnote 4\(^{\text {,}}\)Footnote 5 Our experimental results contradict unequivocally the theoretical predictions. The treatments of pre-distribution entail very low levels of efficiency while redistribution gives rise to almost full investments. Subjects largely tend to condition the division of the common fund on each member’s investment in the redistribution treatment, a strategy that is not possible when determining shares ex ante (the correlation is of course not perfect and opportunistic behavior is also observed). Moreover, a player’s contribution also serves as a credible behavioral commitment to vote against low offers (those below her contribution) and to not allow for the proposer to extract excessively inequitable shares. The previous results echo what a growing literature on bargaining over an endogenous fund has been documenting: that initial efforts act as an objective claim over the surplus and that bargainers tend to follow redistributive norms that take those claims into account. Nonetheless, it should be stressed that even in simple settings with clearly objective claims as in the linear and symmetric production case studied here, behavior reflects a plurality of fairness ideals (Cappelen et al. 2007).Footnote 6 What has been understudied is the effect of bargaining behavior on efficiency. Our findings are particularly striking because partners are not fixed throughout the experiment and reputation concerns play a negligible role. In the pre-distribution treatments, subjects implement ownership agreements in which no one finds it profitable to invest and free-riding predominates. A generalized downward trend in investments is observed throughout the experimental sessions which is reminiscent of the stylized behavior reported in standard public goods games (Ledyard 1995). One of our initial conjectures was that subjects could feel compelled to invest whenever they voted in favor of a proposal and could perceive others’ favorable voting decisions as signals of good will. In turn, these signals would elicit a collective reciprocal behavior in the investment stage. A similar argument has been set forth to explain the findings in experiments with endogenous institutional choice in which subjects tend to act more cooperatively and attain efficient outcomes more often when they have a say, by choice or vote, in creating or modifying the environment (see Kosfeld et al. 2009; Dal Bó et al. 2010). However, we did not find strong support for the effectiveness of endogenous ownership agreements in fostering economic efficiency. Theories of inequality aversion (Bolton and Ockenfels 2000; Fehr and Schmidt 1999) are not useful in explaining our experimental results. Inequality of outcomes is quite prevalent in the game of redistribution since a member who invests a small amount would generally be offered a low share thus receiving smaller payoffs compared to a high investor. Instead, we find that the theory of inequity aversion (Adams 1963; Selten 1987) is more useful in describing our bargaining outcomes in both redistributive and pre-distributive bargaining. Inequity is broadly construed as a feeling perceived from a discrepancy between one’s proportion of rewards to costs relative to others in the comparison group. We extend this theory to apply to cases where rewards are assigned ex ante and costs bourne ex post as in pre-distribution. If all partners are expected to invest the same amount, inequity theory helps account for the even distribution of shares among partners in ex ante bargaining. We elucidate upon this issue in Sect. 6. The article proceeds as follows. In Sect. 2, we provide a brief literature review, mainly focused on bargaining games and efficiency-enhancing mechanisms in public goods games. In Sect. 3, we present the model and theoretical results. In Sect. 4 we describe the experimental design and in Sect. 5 we present the results from the main experiments. Section 6 discusses the results and Sect. 7 concludes the article.",9
22.0,4.0,Experimental Economics,25 August 2018,https://link.springer.com/article/10.1007/s10683-018-9589-9,Hunger and the gender gap,December 2019,Yan Chen,Ming Jiang,Erin L. Krupka,Male,,,Mix,,
22.0,4.0,Experimental Economics,13 October 2018,https://link.springer.com/article/10.1007/s10683-018-9594-z,The Secure Boston Mechanism: theory and experiments,December 2019,Umut Dur,Robert G. Hammond,Thayer Morrill,Male,Male,Unknown,Male,"What is the right way to assign students to schools? The school assignment problem has received a great deal of attention precisely because this question has no “correct” answer. The reason for this is that the three main design objectives of fairness, efficiency, and strategic simplicity are incompatible. There is no mechanism that is fair and efficient (Balinski and Sönmez 1999). Worse, even when a fair and efficient assignment exists, there is no strategyproof and efficient mechanism that always selects it (Kesten 2010). Therefore, choosing an algorithm necessarily involves trade-offs among these objectives. Since each school board has its own preferences regarding these trade-offs, the role of economists is to inform a board as to what objectives are achievable. We introduce a new matching mechanism that is a hybrid of the two most common mechanisms. The Boston Mechanism (BM) is the most commonly used mechanism in the field, while the Deferred Acceptance algorithm (DA) is typically favored by economists.Footnote 1 Our new mechanism, the Secure Boston Mechanism (sBM), is an intuitive modification of BM that secures any school s a student has one of the top \(q_s\) priority (where \(q_s\) is the capacity of school s), but otherwise prioritizes a student at a school based upon how she ranks it. To our knowledge, sBM has not been used for school choice in practice. The main advantage of BM is that it is Pareto efficient with respect to reported preferences, which implies that it will assign more students to their reported first choice than DA. In particular, it maximizes the first choice assignment. This feature of BM has played an important role in the policy discussions of student assignment and media reports of the outcomes of assignment procedures often focus on the fraction of students assigned to their top choice or one of their top three choices.Footnote 2 BM is also very easy for school districts to explain and very easy for parents and students to understand.Footnote 3 However, BM is neither strategyproof nor fair, and the types of manipulations that give students an advantage under BM are easy for strategic students to identify. We present a theoretical analysis that compares our new mechanism to BM and find that, relative to BM, sBM is less vulnerable to manipulation and more fair.Footnote 4 Our equilibrium analysis of sBM is in the spirit of the analysis provided by Ergin and Sönmez (2006) for BM. Further, neither BM nor sBM Pareto dominates the other. We test these theoretical predictions in a laboratory experiment. The design of the experiment was inspired by the application website used by the Wake County Public School System, which is the 15th largest school system in the United States (WCPSS 2015). In this field setting, students are shown the number of other students who ranked a given school ranked first (Dur et al. 2018). Borrowing this approach for transmitting information to students in the application process, we believe that our design is a highly appealing way to increase saliency in school-choice lab experiments. The experimental results suggest that, relative to BM, sBM is less vulnerable to manipulation, more fair, and no less Pareto efficient. The effect sizes strongly support the use of sBM over BM: truth-telling is 72.9% higher with sBM, fairness is 65.0% higher with sBM, and Pareto efficiency is 4.0% higher with sBM. Further, our novel experimental design offers several methodological advantages over the designs commonly used in the experimental matching literature. Despite the empirical prevalence of BM, economists frequently emphasize the advantages of DA, specifically that it is strategyproof, fair, and Pareto dominates any other fair assignment. Even though DA is not efficient, Kesten (2010) demonstrates that no strategyproof mechanism can Pareto dominate it.Footnote 5 Our question is whether a mechanism that is neither strategyproof nor fair can improve upon DA in terms of efficiency in equilibrium with weakly undominated strategies. Given the importance of making efficient assignments, asking whether sBM can improve efficiency relative to DA is a crucially important question with implications in school choice and beyond. We present a theoretical analysis that compares sBM to DA and find that sBM can Pareto dominate DA in equilibrium with weakly undominated strategies but the efficiency comparison of sBM and DA is theoretically ambiguous. We present simulation evidence that suggests that sBM often does Pareto dominate DA when DA is inefficient, while sBM and DA very often overlap when DA is efficient. The comparison to DA uses a simulation, and not an experiment, because we want to present the most favorable case for DA, and observe the performance of sBM under this most favorable case for DA. Our results suggest that sBM should be considered as a viable alternative to DA. Our approach to comparing sBM to the leading alternatives is as follows. Section 2 presents a theoretical analysis of sBM: (1) a comparison of sBM to BM in terms of strategyproofness, fairness, and efficiency and (2) a comparison of sBM to DA in terms of efficiency. Section 3 lays out the design of an experiment that we use to compare sBM to BM in a realistic empirical setting, where the experimental results are found in Sect. 4. Next, Sect. 5 presents simulations in an environment that closely matches our experimental design to compare sBM to DA in terms of efficiency. The comparison to DA uses a simulation in which students’ preferences are reported truthfully under DA; this is the situation in which DA will have the best efficiency performance when students play weakly undominated strategies. In contrast, if we present experimental results on DA, the experimental literature that has followed Chen and Sönmez (2006) has provided empirical evidence that truth-telling is the norm in DA experiments but truth-telling rates remain well below 100%. As a result, we compare sBM to DA in a simulation that ignores the potential behavioral biases one might observe with DA in the lab. Note that, sBM is a obtained by a simple, but intuitive, modification of BM. With this small modification, sBM improves with respect to BM’s central flaws in that sBM is less vulnerable to manipulations and more fair. Because it inherits BM’s simplicity, sBM can be implemented by explaining BM to students/parents, then additionally explaining the modification with respect to guaranteed schools. Before beginning our analysis, we briefly mention several related papers that present experimental evidence on the performance of several school-choice mechanisms. While there is a large theoretical literature on matching, many recent papers have used experiments to consider the relative advantages of alternative mechanisms. Many school-choice experiments provide evidence that BM has meaningful costs relative to DA or other strategyproof mechanisms. For example, the experiments of Chen and Sönmez (2006) helped convince the Boston Public Schools to abandon BM in its assignment process (Pathak 2011). Other papers with additional evidence against the use of BM include Pais and Pintér (2008) and Calsamiglia et al. (2010). Focusing on preference intensities, Klijn et al. (2013) find evidence that DA is more robust to changes in cardinal preferences, relative to BM. This is in contrast to results from the theoretical literature that BM might be preferable when eliciting preference intensities is important (Abdulkadiroğlu et al. 2011). However, several other papers find reasons to support the use of BM in certain settings. First, Lien et al. (2016) compare BM to the Serial Dictatorship (SD) mechanism, arguing that BM can dominate SD when students submit their rankings before learning their priorities. Their experimental results confirm this claim, finding that BM is more efficient than SD when preference reports precede priorities. Second, Featherstone and Niederle (2014) present experiment evidence that, in environments where truth-telling is an ordinal Bayesian Nash equilibrium under BM, subjects report truthfully with BM at rates that are high and quite similar to the truth-telling rates with DA (where truth-telling is a dominant strategy). The authors interpretation is that BM can work well in settings where truth-telling is a simple strategy for students to find and play. This is not the first paper aiming to improve well-known mechanisms with small modifications. For instance, Miralles (2008), Dur (2013) and Mennle and Seuken (2015) propose a simple modification to BM that allows students to skip schools without available seats. Although this modification aims to decrease the level of manipulation under BM, modified BM is not less manipulable than BM based on the comparison notion introduced by Pathak and Sönmez (2013). Kesten (2010) introduced the Efficiency Adjusted Deferred Acceptance Mechanism (EADAM) in order to improve the welfare of students. Under EADAM, students are asked to voluntarily consent to priority violations and EADAM can improve welfare only if certain students consent. Unlike DA, EADAM is manipulable. Moreover, Dur and Morrill (2016) show that students can be harmed when they consent in equilibrium, so the expected welfare gain may not be observed. We now present our theoretical model.",14
22.0,4.0,Experimental Economics,12 November 2018,https://link.springer.com/article/10.1007/s10683-018-9595-y,An experimental examination of interbank markets,December 2019,Douglas D. Davis,Oleg Korenok,John P. Lightle,Male,Male,Male,Male,"Interbank markets are among the most important institutions in a modern financial system. In these markets banks with a surplus of liquid assets loan them to banks with liquidity deficiencies in a way that helps managers efficiently promote a bank’s primary objective of realizing returns from long-term investments, while avoiding the costly task of liquidating long-term illiquid loans. Broadly viewed, this borrowing and lending across banks promotes a rationalization of liquidity in a way that increases a banking system’s capacity for liquidity transformation. The smooth functioning of interbank markets is essential for the stability of the financial system and by extension the economy.Footnote 1 Despite the importance of these markets, relatively little literature was devoted to their analysis prior to an explosion of attention following the financial crises of 2007–2008, when interbank markets ‘locked up.’Footnote 2 Important early contributions include the foundational works by Allen and Gale (2004a, b), who analyze from first principles the interrelationship between financial intermediaries (banks) and markets.Footnote 3 These authors model a situation in which depositors are both uncertain of their own future liquidity needs, and do not have access to the full range of investment alternatives available to banks. In such a scenario, financial intermediaries and markets play complementary roles. Financial intermediaries allow depositors to insure against liquidity risk, while markets allow intermediaries to coinsure against liquidity shocks. Allen and Gale (2004b) ‘AG’ focus more explicitly on the equilibrium price (interest rate) and investment (consumption) aspects of an interbank market. More specifically, the authors use a simplified version of the mechanism design analysis in Allen and Gale (2004a) to study interbank market ‘fragility.’ They find that interbank markets can indeed be quite fragile. Although the banking system supports efficient investment, investment returns are variable across banks whenever banks are impacted by a stochastic idiosyncratic shock. Further, if the banking system is subject to even a small probabilistically occurring aggregate shock, the only robust equilibrium in their analysis involves asset price volatility. The intuition driving this result is that the return to holding a liquid asset (e.g., cash) varies wildly with small changes in the need for liquidity in the banking system. Given even a small excess supply of liquidity, the return to banks for holding cash is zero, and any uncertainty—even ‘extrinsic’ uncertainty unrelated to market fundamentals, can create a mismatch between liquidity needs and cash availability. To induce banks to hold cash they must be compensated with a high return in the form of illiquid asset purchases at ‘fire sale’ prices in instances where liquidity is needed, which results in the equilibrium price variability. In tandem, the combination of idiosyncratic and aggregate shock components can yield both price variability and a variability in investment returns that results in bank insolvencies. AG, however, observe that their model is highly specialized. In particular, their predictions are derived under the assumption of a single homogeneous market price. Other institutional arrangements, they suggest, may yield more stable outcomes. Moreover, they focus on equilibrium outcomes in a banking system, rather than the properties of an interbank market viewed as an allocative mechanism. Given the importance of the foundational papers by Allen and Gale to the analysis of interbank markets, it is important to examine the performance features of an actual market operating in the simplified structure they analyze, but in an environment more like that natural context where trading prices are not necessarily homogeneous. This paper reports a market experiment conducted to evaluate the performance of an interbank market that includes critical elements of the AG development, but is adapted to allow a strategic analysis of bank decisions, and relaxes the assumption of a single price by allowing liquidity deficient banks to sell assets to banks with excess cash via a standard open book double auction. The well-documented tendency for double auction traders to use past contract prices as a reference for future trades induces the sort of inertia in the price adjustment that arguably improves stability.Footnote 4 We emphasize that our experiment evaluates a very simple interbank market, and that naturally occurring interbank markets are in many respects more complex than the simple structure examined here. Among other things, bank loans are heterogeneous in quality, banks do not have symmetric access to interbank loans and loans are negotiated bilaterally via over-the-counter trades. Moreover, central banks use interbank markets to implement monetary policy, and the way they intervene may affect market performance. Richer models that assess interbank market performance in light of various of these complicating elements have been recently proposed. Pertinent papers explore the effects of bilateral bargaining between stochastically-paired banks on banking system stability (Afonso and Lagos 2015a, b; Bech and Monnet 2016), variations in the network structure of banks on banking system stability (e.g., Allen and Gale 2000; Acemoglu and Tahbaz-Salehi 2015; Babus and Kondor 2017), as well as the role of a central bank in stabilizing market performance (Allen et al. 2009).Footnote 5 Particular attention has focused on identifying reasons for market failure induced by liquidity hoarding.Footnote 6 Possible motivations for such hoarding include concerns about counterparty risk (e.g., Heider et al. 2015), precautionary motives (e.g., Caballero and Krishnamurthy 2008; Pritsker 2013), as well as strategic behavior by firms with excess liquidity (e.g., Diamond and Rajan 2011; Acharya et al. 2012).Footnote 7 Evaluating behavior in these richer environments provides an important agenda for future research. The more limited project of the present paper is to develop a baseline against which performance in these more complete structures can be evaluated. Treatment variables in the experiment reported here include variations in the kind of shock that impacts banks, and the range of post-shock trading prices. Shock environments consist of either an idiosyncratic shock, where shocks affect individual banks stochastically, but the banking system deterministically, or a combination shock, in which an idiosyncratic shock is supplemented by an aggregate component that impacts the banking system stochastically. The range of post-shock trading prices are either wide, extending from a low liquidation value to the mature value of assets, or narrow, where both the upper and lower bounds of admissible trades are truncated. As elaborated below, changes in contract price range have some potential policy pertinence, as it represents a minimally invasive tool available to some central banks for controlling interest rate variability. More critically, however, in the ‘wide’ treatment multiple prices can be consistent with an equilibrium. Restricting the post-shock price range reduces the set of equilibrium prices to a single element, an alteration which may not only improve price convergence but may also improve the efficiency of investment decisions. Experiment results parallel the predictions of the AG analysis in important respects, despite the distinguishing features of our experiment design. First, in all environments we observe reasonably efficient aggregate initial investments. A persistent variability in individual portfolio decisions reduces the efficiency of mature investment outcomes, as banks collectively underinvest in some periods, and collectively overinvest in others. Nevertheless, on net we find that our simple laissez-faire interbank market substantially increases investment in a banking system relative to levels sustainable in an autarkic outcome where banks rely only on their private liquidity holdings to address shocks. Second, we observe persistent price variability. Rather than being driven primarily by a need to compensate banks for holding cash in low shock realizations (as in AG) the price variability observed here is driven by a persistent heterogeneity in investment decisions, which creates imbalances in liquidity supply and demand across periods in all treatments. Third, we observe a high variability in earnings across banks, particularly in the combination shock environment, as banks impacted by high aggregate shock realizations are forced to sell assets to banks with available cash or liquidate investments. We find that restricting the range of asset prices very sizably reduces the incidence of large losses among banks. The remainder of this paper is organized as follows. Section 2 below describes the experiment design and procedures. Section 3 presents experiment results, and a short fourth section concludes.",4
22.0,4.0,Experimental Economics,06 December 2018,https://link.springer.com/article/10.1007/s10683-018-09599-3,Deception and reciprocity,December 2019,Despoina Alempaki,Gönül Doğan,Silvia Saccardo,Female,Female,Female,Female,"Work environment substantially affects employee satisfaction and behavior. Numerous examples demonstrate that low worker satisfaction decreases productivity. Consider labor disputes: Krueger and Mas (2004) showed that the 1994–1996 conflict between a labor union and the management of Bridgestone/Firestone was a major contributing factor to the increase in the number of defective tires produced by the company in that period. Similarly, labor conflicts at the construction equipment manufacturer Caterpillar resulted in a significant decrease in output quality (Mas 2008), and compensation disputes in police departments in New Jersey coincided with a decline in the number of arrests by police officers and an increase in crime rates (Mas 2006). Changes in employee behavior as a reaction to the workplace environment might stem from different sources. Workers can alter not only the quality and quantity of effort provision, but also their engagement in unethical behavior. Unethical behavior is widespread in the corporate world and takes different forms, negatively affecting companies’ productivity (see Treviño et al. 2014, for a review). Reciprocal motives are an important driver of employees’ decision to behave unethically.Footnote 1 Unethical behavior, or the lack thereof, might serve as a reciprocity device any time a direct reaction (e.g., effort reduction) is not feasible, for example, due to lack of power or fear of retaliation. In many situations, workers might reciprocate negative encounters with deceptive behaviors that hurt the company, such as misreporting the quality of their work or faking sick days. Similarly, after positive experiences within the company, employees might abstain from carrying out unethical acts they would have otherwise engaged in; that is, they might reciprocate with honesty that benefits the company at a cost to themselves. Because deception, by its nature, is difficult to observe, empirical work offers limited guidance in investigating whether and to what extent (a lack of) deception is used as a (reward) punishment mechanism. The vast experimental work in economics on gift exchange has, thus far, mainly explored reciprocity within the domain of effort provision, looking at behaviors that do not involve deception (see e.g., the survey on reciprocal behavior by Fehr and Gächter 2000). In this paper, we use a simple two-player, two-stage game to experimentally study whether deception serves as a reciprocity device. In the first stage, subjects play a variation of the dictator game. In the second stage, they play a deception game (Erat and Gneezy 2012), in which the recipient in the dictator game has the opportunity to lie to her counterpart by sending her an untruthful message. Compared to truth-telling, lying reduces the payoff of one’s counterpart if the counterpart follows the message. We use the strategy method (Selten 1967) and have the sender choose a message for each possible payoff received in the first stage. Predictions concerning whether and to what extent deception could be used as a reciprocity device are conflicting. Reciprocity models (e.g., Dufwenberg and Kirchsteiger 2004; Rabin 1993) predict that previous encounters will affect subsequent behavior (see also Charness 2004; Charness and Haruvy 2002; Charness and Levine 2007; Offerman 2002). However, it is unclear whether previous encounters would affect subsequent behavior when reciprocity occurs via lying. A fast-growing experimental literature has documented that deception entails positive moral costs (Bucciol and Piovesan 2011; Fischbacher and Föllmi-Heusi 2013; Gneezy 2005; Mazar et al. 2008).Footnote 2 This literature predicts that, because lying is morally costly, less reciprocal behavior could occur when reciprocating requires individuals to lie than when it does not, regardless of the nature of a previous encounter. To test whether this prediction is true, we compare the results of our baseline game with the results of a game in which individuals can reciprocate without engaging in deception. That is, we modify the second stage of the baseline game such that an identical payoff allocation can be obtained through a direct choice rather than by telling a lie. If there were indeed a moral cost of lying, we would expect lying rates to be lower than direct choice rates. This design allows us to test Hurkens and Kartik’s (2009) conjecture that individuals have either a zero or infinite moral cost of lying—the former types always lie if they prefer the outcome from lying and the latter types never lie. In such a case, the conditional probability of lying (the ratio of lying to direct choice) would be constant regardless of the type of previous encounter.Footnote 3 On the other hand, moral costs might be malleable. The morality literature has suggested that moral costs decrease when one can blame the other party (Bandura et al. 1996; Bandura 1999, 2002, 2004; Cubitt et al. 2011; Ditto et al. 2009). In that case, we would expect that the more unfair the previous interaction, the lower the moral cost of lying. In other words, deceiving someone who was previously unkind would be easier than deceiving someone who was previously kind. Additionally, kind encounters could be rewarded with honesty from people who would otherwise lie. Thus, the conditional probability of lying would decrease with increasing kindness. Further, moral costs of lying might be sensitive to whether individuals have monetary incentives to lie. There are conflicting predictions. On the one hand, the moral cost of lying may increase with one’s payoff from lying. When reciprocating unkind behavior requires individuals to lie and lying leads to a financial benefit, people might worry about the signal they convey to themselves or to others about their own morality, as it can be unclear whether individuals lie to gain a personal material benefit or to punish preceding unkind behavior. This concern may be present when lying is beneficial, but it cannot be present when lying is costly in terms of payoffs. Indeed, work on perceptions of generosity in psychology has found that people morally discredit prosocial actions if those actions result in monetary benefits for the self but not if they result in a monetary loss or no benefit for the self (e.g., Carlson and Zaki 2018; Lin-Healy and Small 2012; Newman and Cain 2014). If the moral cost of lying increases with one’s own payoff from lying—when lying is used as a tool to reciprocate unkind behavior—the conditional probability of lying would be smaller if lying were beneficial to the self than if it were costly. On the other hand, work on excuses to behave selfishly (e.g., Babcock et al. 1995; Dana et al. 2007; Exley 2015; Gneezy et al. 2017; Konow 2000) suggests that when lying is beneficial, self-serving biases might decrease one’s moral cost of lying. When lying is costly for the self, however, a self-serving bias cannot affect the size of moral costs. Hence, this mechanism would predict that the conditional probability of lying is higher when lying is beneficial to the self than when it is costly. To investigate whether lying costs are malleable in the presence of reciprocal motives as well as different incentives for lying, we study two domains of lies: In the Selfish domain, lying increases one’s own payoff at a cost to the receiver, and in the Spiteful domain, lying is costly for the self but imposes a larger cost on the receiver (see the taxonomy of lies by Erat and Gneezy 2012). In organizations, selfish lies would be akin to over-reporting the quality of one’s work or taking credit for someone else’s work. An illustration of spiteful lies is misrepresenting one’s private information in order to sabotage a project that has a small value to oneself and a high value to one’s counterpart. We also measure the extent to which lying rates change in response to increasingly kinder previous interactions, and whether the effect is driven by intentions rather than initial payoff allocations. The issue is quite elusive, because even in settings where reciprocity doesn’t involve deception, evidence is mixed. Some studies (Blount 1995; Falk et al. 2008) point to intentions, while others (Bolton et al. 1998; Charness 2004) highlight distributional concerns as the main driver of reciprocity. In our baseline game, a response to both intentions and initial payoff allocations—including inequity aversion and an income effect—would predict lying and direct-choice rates of the same option to decrease with increasing payoffs from the first stage. To identify whether intentions drive the reciprocal response, we compare our baseline treatment with a treatment in which the experimenter rather than one’s counterpart implements the first-stage outcome. We find that individuals use lies in order to reciprocate (un)kind behavior in both the Selfish and Spiteful domains: lying rates are highest after an unkind encounter and decrease with increasing kindness of the encounter. For selfish lies, the proportion of individuals who lie is always lower than the proportion of individuals who directly choose the selfish payoff allocation, and the conditional probability of lying decreases as a function of the counterpart’s kindness. This finding is in line with the predictions of a positive moral cost of lying that increases with increasing kindness, refuting the hypothesis of Hurkens and Kartik (2009). Surprisingly, for spiteful lies, lying rates are similar to the percentage of direct choices. This result provides evidence against a moral cost of lying when lying is costly for the self and is used as a punishment device, demonstrating the malleability of the moral cost of lying. For selfish lies, both intentions and initial payoff allocations affect lying rates, but the latter effect is larger. We find evidence for positive reciprocity, whereas the evidence for negative reciprocity is only directional and not statistically significant. For spiteful lies, only intentions matter. When the experimenter implements the first-stage payoffs, lying rates are close to zero in all cases; however, individuals punish unkind intentions by lying. As expected, lying rates are close to zero after kind encounters. Our paper adds to the growing literature on deception by investigating the drivers of deception within an interacting pair after an initial encounter.Footnote 4 Previous work by Ellingsen et al. (2009) showed that honest revelation of private information in bilateral bargaining is affected by whether individuals previously cooperated or defected in a prisoner’s dilemma game. Several features distinguish our experiment from theirs. First, we systematically vary the levels of kindness of the initial encounter to show whether the moral cost of lying affects retaliation via deception. We provide the first evidence of reciprocal deception that can be attributed to intentions. More importantly, we compare whether reciprocating through lying is conceptually different from reciprocity that doesn’t involve deception, and whether the presence (absence) of financial costs from deception affects the retaliation behavior of the deceiver. Finally, we contribute to the recent stream of papers that investigate the structure of lying costs (e.g., Abeler et al. forthcoming; Dufwenberg and Dufwenberg 2018; Gneezy et al. 2018; Khalmetski and Sliwka 2017). Our results show that there is an interaction between reciprocity concerns, lying costs, and incentives from lying, and highlight the need to combine insights from both the reciprocity and the lying literature in order to fully understand the drivers of deception.",10
22.0,4.0,Experimental Economics,14 February 2019,https://link.springer.com/article/10.1007/s10683-018-09600-z,Using machine learning for communication classification,December 2019,Stefan P. Penczynski,,,Male,Unknown,Unknown,Male,"This study investigates the possible contribution of machine learning techniques to the coding of natural language transcripts from experiments. The aim is to evaluate whether simple tools from Natural Language Processing (NLP) and machine learning (ML) provide valid and economically viable assistance to the manual approach of coding even when complex concepts are coded. In recent years, the analysis of communication has been an increasingly important element of many studies in economics. Communication transcripts are being consulted to understand behavior beyond what can be inferred from choice data and to obtain insights into team deliberation processes (e. g. Cooper and Kagel 2005; Burchardi and Penczynski 2014; Goeree and Yariv 2011; Penczynski 2016a). Computerized experiments make the collection of communication data very easy. And communication data are potentially very informative about reasoning processes. This strength, however, comes with the natural disadvantage that the coding of text—which is usually done manually—is time-intensive and based entirely on human judgment.Footnote 1 Enabling the assistance of computers in the processing of natural language is the aim of the many different research fields of NLP, such as machine translation, question answering and speech recognition.Footnote 2 A basic judgment of texts can be made with the help of simple statistics, such as message counts, word counts and word ranks. Moellers et al. (2017) fruitfully use those concepts when they experimentally investigate communication in vertical markets. More automated approaches like the Linguistic Inquiry and Word Count program (LIWC) group words in semantic classes such as positive or negative emotions, money, past tense etc. Abatayo et al. (2017) analyse communication in cooperation experiments with the help of such software. This automation comes at the cost that “the semantic classes may or may not fit the theory being investigated” (Crowston et al. 2012, p. 526). A closer fit with a specific economic theory and a higher level of automation can be achieved when statistical techniques such as ML use manually coded examples to build models of linguistic phenomena, an approach that I follow here.Footnote 3 Machine learning—or statistical learning—is a way of obtaining statistical models for prediction in large datasets. Due to the increasing importance of Big Data and variable selection, ML is making its way into the toolbox of econometricians and applied economists (Varian 2014). For example, its strong out-of-sample prediction capabilities support causality studies by estimating policy implementation and counterfactuals (Mullainathan and Spiess 2017). The computational handling of text data leads to datasets with many variables and makes these techniques appropriate. Across the sciences, text analysis with the help of ML has become more popular in recent years. Physicians classify suicide notes and observe that the trained computer model outperforms experienced specialists in suicide predictions (Pestian et al. 2010). Linguists use ML to sift Twitter for useful information during mass emergencies (Verma et al. 2011). Based on large volumes of text such as party programs and speeches, political scientists use ML to locate politicians and parties in the political space, for example in the left-right spectrum (Benoit et al. 2009). Similarly, economists have used it to quantify the slant of media (Gentzkow and Shapiro 2010) or the consequences of transparency rules for central banks (Hansen et al. 2017). To my knowledge, this is the first study to investigate this technique’s usefulness for experimental text data. A facilitating feature of experimental data is that the topic of the chat conversation is usually known which simplifies the machine learning analysis.Footnote 4 The communication transcripts studied here are obtained from implementations of Burchardi and Penczynski’s (2014) intra-team communication design in beauty contest, hide and seek, social learning and asymmetric-payoff coordination games. Among the applications in experimental work, the classification of reasoning in terms of the level-k model is certainly one of the more ambitious tasks. Still, the results are clearly positive and show that the out-of-sample computer classification is able to replicate many results of the human classification. They suggest that in similar or easier classification tasks, computer classification can be a valid option to reduce the additional effort that comes with communication analyses, especially large ones. The following sections will introduce the data and the machine learning techniques that are used. Afterwards, results will be presented for three different applications. The technical appendix introduces the computational method based on an example code.",11
23.0,1.0,Experimental Economics,12 March 2019,https://link.springer.com/article/10.1007/s10683-019-09607-0,Incentives in experiments with objective lotteries,March 2020,Yaron Azrieli,Christopher P. Chambers,Paul J. Healy,Male,Male,Male,Male,"Consider an experiment in which subjects make two choices. The first is to choose from the set \(\{\)apple,left shoe\(\}\), and the second is to choose from the set \(\{\)banana,right shoe\(\}\). Most subjects would prefer the apple over the left shoe and the banana over the right shoe. But when both choices are paid then subjects may choose the shoes instead, because they prefer a pair of shoes over having both an apple and a banana. In other words, complementarities between choice objects may distort subjects’ choices when multiple decisions are given. An experimenter might infer incorrectly that the left shoe is preferred to the apple and that the right shoe is preferred to the banana. In this case we say that the payment mechanism is not incentive compatible (IC) because it did not incentivize subjects to reveal their true preference in each individual problem separately.Footnote 1 A proposed solution to the problem of complementarities (due to Allais 1953) is to pay for one randomly-selected decision. We call this the Random Problem Selection (RPS) mechanism.Footnote 2 With this mechanism subjects cannot receive both shoes, and therefore have no incentive to choose the shoe in either decision problem. Although this solves the complementarities problem, it introduces randomness. And there are examples of preferences over lotteries for which the RPS mechanism is not IC.Footnote 3 Thus, exact conditions under which this mechanism is incentive compatible were not well understood. Neither was it known whether other mechanisms can be used to guarantee truthful revelation of choices in experiments with multiple decisions. In our earlier work (Azrieli et al. 2018) we filled this gap by studying experiment incentives in a general framework in which subjects are permitted to have any subjective belief over random outcomes. Assuming state-wise monotonicity (which requires that subject’s preference respects dominance) and nothing else, we showed that the RPS mechanism is the only incentive compatible mechanism that can be applied to any experiment. There can be contrived examples of experiments for which other mechanisms are incentive compatible, but these are almost never seen in practice. But what if an experiment consists entirely of objective lotteries? For example, suppose the experimenter flips a fair coin to determine which problem is paid. In this case allowing subjects to have any belief distribution over random outcomes may be too permissive. But if we restrict beliefs to equal the objective probabilities then we restrict the model, and in doing so we may open the door for additional incentive compatible mechanisms. Thus, it is important to study whether the set of incentive compatible mechanisms grows when we assume objective lotteries, and whether any of the new mechanisms would have broad applicability. In this paper we assume objective probabilities and that all preferences which respect stochastic dominance are admissible. In this framework we show that the set of IC mechanisms is strictly larger than that characterized by Azrieli et al. (2018). But the newly-identified mechanisms are again only applicable in certain contrived experiments. In almost every real-world experiment the RPS mechanism is the unique incentive compatible mechanism under our assumptions. As in Azrieli et al. (2018), we model an experiment as a list of decision problems, i.e. a list of sets of choice objects from which the subject should choose. The subject announces a chosen object from each decision problem. The experimenter then maps that announced vector of choices into a payment, which may be random. For example, the RPS mechanism takes the announced vector of choices and randomly chooses one of them for payment. A crucial observation in this analysis is that the choice objects and the payment objects in an experiment are typically non-overlapping sets. In the example above the set of all choice objects would be \(\{\)apple,banana,left shoe,right shoe\(\}\). The experimenter is interested in learning the subject’s preferences over those choice objects, which we denote by \(\succ\).Footnote 4 But the subject actually is being paid lotteries over these choice objects. Thus, the set of payment objects is a set of lotteries over the choice objects.Footnote 5 Which choice objects the subject chooses in the experiment will therefore be driven by their preferences over payment objects (lotteries), not their preference over choice objects. We denote the preference over payment objects (lotteries) by \(\succeq ^*\). In the experiment the subject chooses (or “announces”) the choice objects that map into her most-preferred payment object (according to \(\succeq ^*\)). We say that the payment mechanism is incentive compatible if what she announces coincides with her most-preferred choice objects according to \(\succ\). In other words, incentive compatibility ensures that the subject will reveal truthfully her most-preferred choice in every problem. We refer to \(\succeq ^*\) as an extension of \(\succ\). For us to study incentive compatibility we must make some assumptions about how \(\succeq ^*\) relates to \(\succ\). If they are not related—meaning every extension is admissible—then no mechanism can be incentive compatible. This is Proposition 0 of Azrieli et al. (2018). A natural restriction on extensions is that they satisfy monotonicity with respect to first order stochastic dominance (FOSD), relative to the underlying preference \(\succ\). Formally, an extension is monotonic if lottery f is preferred to lottery g whenever f dominates g in the sense of FOSD. Monotonicity places no restrictions on lotteries that are not ranked by dominance. We show in Theorem 1 that, as long as all admissible extensions are monotonic, the RPS mechanism is IC. In other words, if a subject’s preferences are such that she never prefers a dominated gamble, then any RPS mechanism provides her the right incentives to truthfully reveal her favorite element in each decision problem. The logic is simple: any time a subject switches from telling the truth to lying on any decision problem, they shift probability away from their most-preferred object and onto a less-preferred item. The resulting lottery is therefore stochastically dominated by the lottery induced by truth-telling. Notice that expected utility is not needed for this argument; the RPS mechanism is incentive compatible as long as monotonicity is satisfied.Footnote 6 Monotonicity is satisfied by nearly every decision-theoretic model of choice under uncertainty. Indeed, it is often viewed as normative, and models that violate monotonicity are often dismissed as implausible; see Quiggin (1982), for example. Thus we view monotonicity as a minimal assumption on \(\succeq ^*\), though we discuss its limitations in the sequel and more extensively in Azrieli et al. (2018). Assuming all monotonic extensions are admissible (and that beliefs coincide with objective probabilities), we characterize the class of all IC mechanisms for any given experiment. The main result of this paper, Theorem 2, shows that, in a certain sense, any IC mechanism resembles the RPS mechanism, but that the class of IC mechanisms may extend beyond the RPS mechanism in certain contrived experiments. To understand how incentive compatibility could extend beyond the RPS mechanism in some experiments, consider the following example. Let \(D_1=\{x,y\}\), \(D_2=\{y,z\}\) and \(D_3=\{x,z\}\) be the three decision problems in some experiment. Now, for every (strict) preference over \(\{x,y,z\}\), if the subject truthfully announces her choices, then her favorite alternative from the set \(E=\{x,y,z\}\) will also be revealed. Below we will call sets with this property surely identified sets. We can imagine an RPS-like mechanism that not only pays for choices in the actual decision problems, but also might pay for the inferred choice from this surely identified set E. For instance, consider the distribution \(\lambda\) over subsets of \(\{x,y,z\}\) given by \(\lambda (D_1) = \lambda (D_2) = \lambda (D_3) = 0.3\) and \(\lambda (E)=0.1\). And suppose the subject has preferences \(x\succ y \succ z\). If she announces truthfully in each \(D_i\) then their message vector will be (x, y, x), and the experimenter can use that to infer that x is also her most-preferred element in E. The mechanism will therefore pay x with probability \(\lambda (D_1)+\lambda (D_3)+\lambda (E)=0.7\) and y with probability \(\lambda (D_2)=0.3\). If the subject misrepresents and instead announces (y, y, x), then y would be inferred to be the most-preferred in E, so the subject would instead receive x with probability 0.3 and y with probability 0.7. This is strictly dominated by the truth-telling lottery, so any subject who respects dominance will not choose it. Indeed, any non-truthful message will result in a dominated lottery, so the mechanism is incentive compatible under monotonicity.Footnote 7 Still, we can generalize even further by allowing \(\lambda\) to put negative weight on some of the sets. For instance, set \(\lambda (D_1) = \lambda (D_2) = \lambda (D_3) = 0.4\) and \(\lambda (E)=-\,0.2\). For our subject with \(x\succ y\succ z\) reporting truthfully in this mechanism pays x with probability 0.6 and y with probability 0.4. Misrepresenting by announcing (y, y, z) would again switch those probabilities, leading to a dominated lottery. However, if we choose the weights to be \(\lambda (D_1) = \lambda (D_2) = \lambda (D_3) = 0.6\) and \(\lambda (E)=-\,0.8\), then the resulting mechanism will not be incentive compatible, since the revealed second-best alternative (y) is now paid with a higher probability than the revealed first-best alternative (x). Thus, some restrictions must be placed on \(\lambda\) in order for incentive compatibility to hold in the resulting mechanism. Theorem 2 shows that, in any experiment, any IC mechanism can be represented by a particular \(\lambda\) as above, and precisely describes the restrictions on \(\lambda\) that guarantee incentive compatibility. It is illuminating to compare this characterization to the one obtained in our previous paper (Azrieli et al. 2018). In that work we characterize incentive compatibility of experiments under monotonicity, but when mechanisms map choices to acts instead of objective lotteries.Footnote 8 Monotonicity in that framework means that if f is preferred to g in every possible state of the world, then f is preferred to g; otherwise their ranking is not restricted. The acts framework allows for more general extensions of preferences: Subjects may have their own subjective beliefs about the likelihood of different outcomes of the randomization device, or they may even have preferences which are not probabilistically sophisticated (Machina and Schmeidler 1992); e.g., they may be uncertainty averse. This might apply when subjects view the experimenter’s randomization as ambiguous. The assumption of the current paper that subjects view payments as lotteries can be thought of as an additional restriction on the set of admissible extensions in the acts framework. Since the experimenter can use this additional knowledge about extensions to construct IC mechanisms, one would expect that the class of IC mechanisms will be larger in the case of lotteries. In Sect. 5 we show that this is indeed the case: If a mechanism is IC in the acts framework, and one puts some (full-support) distribution over the state space of the randomization device, then the resulting lottery mechanism is IC. However, there are IC mechanisms in the lotteries environment that cannot be generated by any IC acts mechanism; in fact, these are exactly the mechanisms whose distribution \(\lambda\) uses negative weights. Although the set of IC mechanisms grows when we restrict attention to objective lotteries, the new mechanisms all require the existence of surely identified sets, such as \(E=\{x,y,z\}\) in the example above. But most experiments do not have surely identified sets, because most experiments have no overlap between decision problems. In that case the only incentive compatible mechanism (assuming all preferences that respect stochastic dominance) is the RPS mechanism.Footnote 9 Thus, we view our result as confirming the conclusion of Azrieli et al. (2018): under our stochastic dominance assumption, in practice, the RPS mechanism is the only incentive compatible mechanism. Nothing is gained by assuming objective probabilities. Behaviorally, we speculate that mechanisms that pay based on surely-identified sets or that use negative weights are excessively complicated and may lead to more confusion and mistakes by subjects.Footnote 10 Thus, even if an experiment does have surely identified sets, we see no particular reason to use anything other than the simple RPS mechanism. Indeed, we believe the practical implication of our characterization is that the RPS mechanism is the only IC mechanism any experimenter would want to use, assuming monotonicity. Any other mechanism is either not incentive compatible or adds unnecessary complications. In Sect. 6 we consider the particular case of experiments in which the choice objects are themselves lotteries over money. In this set-up an RPS mechanism generates a compound lottery, where in the first ‘upper’ stage a decision problem is randomly chosen for payment, and in the second ‘lower’ stage a dollar amount is randomly chosen according to the lottery that the subject chose in the realized decision problem of the first stage. Examples in the literature (Holt 1986, e.g.) show that if the subject reduces compound lotteries according to the laws of probability and has Rank-Dependent Utility (RDU) preferences over lotteries over money, then the RPS may not be IC. Our framework and results make it easy to see the source of the failure: Reduction of compound lotteries together with monotonicity imply the independence axiom. Since RDU preferences typically violate independence, if one assumes reduction then it must be the case that monotonicity does not hold. Our Theorem 1 cannot be applied then, and the RPS may not be IC. In fact, we show that if subjects reduce compound lotteries and if all RDU preferences are admissible then no IC mechanism exists. Fortunately, empirical evidence suggests that it is rare for subjects to satisfy reduction but violate expected utility (Halevy 2007), so such violations of monotonicity may not be a large concern. The issue of complementarities (paying both the left shoe and the right shoe) was addressed in Azrieli et al. (2018). There we showed that an incentive compatible mechanism can never pay in ‘bundles’ unless the researcher is willing to assume that subjects’ preferences exhibit no complementarities. But that conclusion holds whether we allow for subjective beliefs or objective probabilities, so the result is exactly the same in the current framework of objective lotteries. If the experimenter is going to pay for multiple decision problems (thus forming a bundle) then complementarities must be assumed away. We therefore restrict attention to non-bundle payments in this paper.Footnote 11 We do find that experimenters have lacked a convention for which payment mechanism to use. In our survey of papers published in 2011, we found that only 25% use the RPS mechanism, while 56% pay for every decision. Almost all of the remainder pay for some number of randomly-selected decisions (13%) or use a mechanism that is not incentive compatible under any standard assumptions (6%). Our goal is to provide a theoretical framework in which experimenters can understand exactly what assumptions justify one payment mechanism over another, and to understand exactly those conditions under which the RPS mechanism is incentive compatible. We review empirical tests of monotonicity and the RPS mechanism in the concluding section. Monotonicity violations appear to occur most frequently when multiple decisions are shown on one screen, or when a single decision problem is repeated multiple times. In other settings monotonicity appears to be satisfied.Footnote 12 From a theoretical perspective, our work is probably closest to the classic work of Gibbard (1977), who characterizes strategy-proof random mechanisms (using objective lotteries) when only ordinal preferences can be elicited. He characterizes these mechanisms as a kind of random-dictatorship, whereby a ‘dictator’ is an agent that solely determines the outcome. Our paper is comparable to the special case in which there is only one agent present. Gibbard does not, however, uncover the special structure of these dictatorial mechanisms in the form that we uncover, presumably because his interest was in understanding the implications of strategy-proofness across agents. Another important difference between the papers is that in Gibbard’s framework agents report their entire ranking over alternatives, while we consider the more general case in which the favorite alternatives in several subsets are reported. Finally, Gibbard requires only weak incentive compatibility, while we require that truth-telling be the unique optimum.",14
23.0,1.0,Experimental Economics,19 March 2019,https://link.springer.com/article/10.1007/s10683-019-09608-z,Where to look for the morals in markets?,March 2020,Matthias Sutter,Jürgen Huber,Markus Walzl,Male,Male,Male,Male,"In the early history of economic thought, some of the most important founders of modern economics dealt extensively with the relationship between markets and morals. Depending on the analysis, some scholars arrived at rather opposite conclusions. For instance, while Adam Smith argued that markets would, in principle, have a civilizing effect on the behavior of market participants (Smith 1763), Karl Marx and Thorsten Veblen expected markets to be destructive and bring out the worst in human beings (Marx 1867; Veblen 1899). Given the ubiquity of markets in our daily life, the question of how they affect human, and in particular moral, behavior is an immensely important one. Yet, during the second half of the twentieth century, the question of how markets relate to morals was relegated to the background of the academic debate. Only during the past decade the academic community has rediscovered this topic, probably fueled by scandals like Enron (Healy and Palepu 2003), the revelation of massive child labor as a backbone of the global textile industry (Edmonds 2007), or more recent scandals in the finance industry (Cohn et al. 2014). For instance, Shleifer (2004) has argued that the competitive pressure in markets creates strong incentives for unethical practices (like child labor, tax evasion or corruption) to reduce costs and thus guarantee survival in a competitive environment. In addition, Sandel (2012) has claimed that markets—or more generally price mechanisms—might undermine moral values per se by crowding out norms such as respect for human life and dignity. Using experimental methods, Falk and Szech (2013) were the first to demonstrate under controlled laboratory conditions that, indeed, markets can undermine moral values. More precisely, they let subjects decide whether to take some money and let a mouse be killed or forgo money and let the mouse live. The focus of their work was on comparing behavior when subjects decided individually and when they traded on bilateral or multilateral double-auction markets. First, they found that subjects were more frequently willing to let a mouse be killed in a double auction market than when making an individual decision. Second, they reported a downward trend in prices in the multilateral markets, which they interpreted in the following way: “The downward trend provides a further indication of moral decay in the mouse market and is suggestive of social learning and endogenous social norm formation. Intuitively, observing low trading prices in the market may make it normatively acceptable to offer or accept low prices as well.” (Falk and Szech 2013, p. 709).Footnote 1 In this paper, we start from their interpretation and investigate how moral behavior in markets influences aggregate market prices and quantities traded. In order to do so, we keep the general simplicity of the design of Falk and Szech (2013)—by letting buyers and sellers trade in a multilateral double auction market where trading has a negative externality—and add a treatment variation that is completely identical, except that we remove the negative externality. This creates the simplest possible environment to assess how a negative externality affects aggregate market outcomes. Given the growing literature in this field, this question is relevant from a methodological viewpoint. Furthermore, since policy interventions take place in specific market settings, understanding the intricate relation between moral behavior and market outcomes is also relevant from a practical viewpoint. In our experiment, we let buyers and sellers trade in a double auction market in a sequence of ten periods to split a fixed sum of money between a buyer and a seller. We implement a 3 × 2-between-subject design: With the first treatment-variable we vary the number of buyers and sellers such that there are either more buyers than sellers, more sellers than buyers, or an equal number of sellers and buyers. With the second treatment-variable we vary whether concluding a trade triggers an externality or not. Thus, in half of the markets striking a deal has only the consequence of distributing money between the buyer and the seller. In the other half, a deal entails the additional negative externality of voiding donations for a potentially life-saving vaccine that is provided by UNICEF to reduce the death toll of about 90,000 people that die each year because of measles (see the statistics for 2016 in the World Health Organization’s factsheet at http://www.who.int/mediacentre/factsheets/fs286/en/). By systematically varying the number of buyers and sellers, we investigate how the competitive pressure on each market side influences aggregate market outcomes, in particular prices and trading volume. We compare the price developments in markets with and without an externality, holding the number of buyers and sellers constant. This feature lets the number of buyers and sellers who trade in the market be endogenously determined, and it allows investigating whether the externality creates a difference in trading volume or trading prices over time. In this way, we can disentangle the impact of competitive pressure and of moral concerns on market outcomes. As predicted by a simple model of price-taking behavior by agents with an aversion against generating a negative externality from trade, we find that the presence of an externality reduces the trading volume but that the effect on prices depends on the market structure. If there is an equal number of buyers and sellers, prices remain unaltered. In contrast, if there are more buyers than sellers, competitive pressure between buyers increases prices, but this effect is moderated in the presence of an externality. If there are more sellers than buyers, the effect is reversed. With our study we contribute to the emerging experimental literature focusing on the interplay of morals and markets. Following the seminal work of Falk and Szech (2013), several recent studies have tried to identify why markets might erode moral values. Among the most important explanations are diffusion of responsibility and lack of pivotality in markets, social information about the acceptability of a particular (unethical) behavior, or market framing that distracts attention from the moral dimension of the traded good (Bartling et al. 2015, 2017; Breyer and Weimann 2015; Cappelen et al. 2017; Falk and Szech 2015; Gneezy et al. 2014; Irlenbusch and Saxler 2015; Irlenbusch and Villeval 2015; Kirchler et al. 2016). Although many of these studies discuss certain aspects of morals in markets, none of them did examine the interplay of market structure, moral behavior and aggregate market outcomes in detail. We contribute to this line of literature by showing that morals in markets reveal itself in lower trading volume, while prices mostly depend on the market structure. Declining prices are not a straightforward indicator of declining morals in markets of the Falk and Szech (2013) paradigm, but rather have to be reviewed in light of the relative market power of buyers and sellers (as their markets had two more sellers than buyers). In the next section, we introduce our experimental design and the details of the moral externality as well as our hypotheses derived from a simple model. Section 3 presents the experimental results and examines trading volume and prices separately. Furthermore, in Sect. 3 we also discuss trader characteristics that are relevant for trading behavior in markets with an externality. Section 4 discusses our results and concludes the paper.",31
23.0,1.0,Experimental Economics,09 April 2019,https://link.springer.com/article/10.1007/s10683-019-09610-5,Do economic inequalities affect long-run cooperation and prosperity?,March 2020,Gabriele Camera,Cary Deck,David Porter,Female,,Male,Mix,,
23.0,1.0,Experimental Economics,01 April 2019,https://link.springer.com/article/10.1007/s10683-019-09609-y,Voting on the threat of exclusion in a public goods experiment,March 2020,Astrid Dannenberg,Corina Haita-Falah,Sonja Zitzelsberger,Female,Female,Female,Female,"Cooperation among nonrelatives occurs frequently, for example among employees or users of a common pool resource. Stable cooperation often relies on actual or potential punishment of defectors. Punishment can take various forms, ranging from soft measures like disapproval to material measures like fines to harsh punishment like ostracism. Punishment may be assigned and enforced by an external authority, for example by the government or the employer, or it may be initiated and enforced within the community. Numerous studies in the lab and in various field contexts have shown that people are willing to punish defectors even at a personal cost (Ostrom 1990; Chaudhuri 2011). In this paper, we investigate a particular form of punishment, namely ostracism, in an experimental setting. Ostracism refers to the general process of excluding individuals from a group. We know from previous experiments that the option to exclude other players increases cooperation similar to other forms of punishment (see the next section). Our main interest in this paper is on whether people choose ostracism as a punishment institution when they have the choice and how this decision affects cooperation as compared to an exogenously imposed institution. Ostracism has been practiced in virtually all societies throughout all recorded history, from ancient Rome and medieval European kingdoms, to traditionalist communities like the Amish or clans in Tribal Montenegro (Boehm 1986; Gruter 1986; Zippelius 1986; Gruter and Masters 1986). Imprisonment can be interpreted as modern version of ostracism executed by the government. Many groups that exist in modern democratic societies, like political parties, companies, or nonprofit associations, have rules that determine if and under what circumstances a member can be excluded. These exclusion rules may be implemented fully at the group’s own discretion or they may be restricted by superior regulations. For example, unions are typically not allowed to exclude individuals from the negotiated improvements of the working conditions (Traxler et al. 2002). Political parties and nonprofit associations usually have discretion in using and determining exclusion rules and they often allow for exclusion if members violate important principles (Bolleyer and Gauja 2015).Footnote 1 Users of common pool resources implement exclusion rules, among other things, to secure a sustainable use of the resource. For example, small villages in Switzerland and Japan have established rules for managing communal land as well as measures for violations of the rules including, as the ultimate punishment, banishment from the village (Ostrom 1990). Microfinance groups whose members borrow under joint liability often exclude individuals who fail to repay the loan from social activities (Baland et al. 2017; Putnam et al. 1994). Unlike the deprivation or impairment of property (monetary punishment), ostracism necessarily is a collective decision as it requires some form of coordinated response by the community members. This can explain why it has been predominantly used for crimes that affected the community as a whole, such as cultic violations, arson, or high treason (Zippelius 1986). The immediate consequence of excluding non-cooperative individuals is that the society becomes smaller. The indirect and longer-term consequence is that further decline of cooperation may be averted. Ostracism can be useful in supporting group cohesion but it can also hurt the community if too many or the wrong individuals are excluded (Gruter and Masters 1986). Even if ostracism is exclusively targeted at defectors, the unforgiving nature of the punishment may preclude potential rehabilitation and, together with the provisions that may be needed to separate the excluded members from the group, make the punishment overly expensive. Despite the widespread incidence of ostracism in human societies around the world, the economics literature has devoted only little attention to the phenomenon, especially when compared to the study of monetary punishment, which has received considerable attention (Ostrom et al. 1992; Fehr and Gächter 2000; for a review see Chaudhuri 2011). In this paper, we use a repeated linear public goods game to study whether groups choose to implement an exclusion institution when they have a choice.Footnote 2 Depending on their choice, groups either have the option to exclude members over the course of the game or not. Excluding a player necessarily means that the social optimum is no longer available as the group loses a potential contributor. We distinguish between a costless exclusion institution and a costly exclusion institution which, if chosen, reduces the endowment of all players. If there is an institutional cost, then implementing the institution forecloses achievement of the social optimum even if no group member is excluded. With this design, we want to test whether groups choose to implement the exclusion institution, how this choice affects cooperation, and how a fixed institutional cost affects the decisions and outcomes. We also compare an exclusion institution that is endogenously chosen by the groups to one that is exogenously imposed to understand the robustness of the results with respect to how the institution is implemented. The experimental design clearly represents a marked simplification of the institution formation process in the real world which usually is a slow process with gradual changes over time. In many of the above-mentioned examples of ostracism, especially those with a long history, it is impossible to say when exactly members agreed to use ostracism as a way to punish defectors. In some cases, exclusion might have started as an ad-hoc reaction by a few members of the society and then developed into a social norm or tradition without ever being openly discussed and democratically chosen. Nevertheless, at any given point in time, the preservation and functioning of an institution depend on an internal agreement of at least some of the society’s members. Curtailing the institution formation process into a limited number of decisions in a short period of time allows us to compare the performance of groups that implement the institution and groups that do not implement it, and to compare the behavior of the supporters and the opponents of the institution. We find that a significant share of the experimental groups choose to implement the exclusion institution, even when it comes at a cost. Contributions to the public good are significantly higher when the exclusion option is available, not only because low contributors are excluded but also because high contributors sustain a higher cooperation level under the exclusion institution. Supporters of the institution contribute more than its opponents, but only when the institution is implemented. With respect to how the institution is implemented, we find that groups that choose the institution endogenously contribute slightly more than groups that are forced to play under the same institution. The differences, however, are small and not statistically significant. These results are to a large extent inconsistent with the standard economics model based on purely selfish preferences which predicts that the threat of exclusion is not sufficient to support cooperation in a finitely repeated game. Given this inconsistency, we use two simple and well-established models to show that the results can be better explained by assuming social preferences. The inequality aversion model by Fehr and Schmidt (1999) assumes that individuals dislike income differences between themselves and others. The reciprocity model by Rabin (1993) assumes that individuals derive utility from repaying kindness with kindness and unkindness with unkindness. The two models make similar predictions for the choice of the institution and the experimental results closely resemble the predictions for heterogeneous groups in which the majority of players is social and the minority is selfish. The remainder of the paper is structured as follows. Section 2 provides an overview of the previous literature on cooperation in finitely repeated games, the effects of punishment opportunities, and endogenous institutional choice. Section 3 describes our experimental design and Sect. 4 discusses the institutional choice based on standard economic theory and two models of social preferences. Section 5 presents the main experimental results (less important results are presented in an Appendix of ESM) and Sect. 6 discusses the results and concludes.",10
23.0,1.0,Experimental Economics,27 February 2019,https://link.springer.com/article/10.1007/s10683-019-09603-4,Auctions in near-continuous time,March 2020,Cary Deck,Bart J. Wilson,,,Male,Unknown,Mix,,
23.0,1.0,Experimental Economics,19 February 2019,https://link.springer.com/article/10.1007/s10683-019-09602-5,Dynamic runs and circuit breakers: an experiment,March 2020,Jacopo Magnani,David Munro,,Male,Male,Unknown,Male,"Financial market volatility can have important welfare implications.Footnote 1 In recent decades, mandated trading halts, dubbed “circuit breakers,” have become official policy for most major financial markets, with the aim of curbing financial instability. In the US, market-wide circuit breakers became widespread after the stock market crash of October 1987 and the Brady report: Brady Commission (1988). They have been recently extended after the May 6, 2010 Flash Crash, as discussed in Angstadt (2011). Circuit breakers can be a desirable policy if they prevent “unnecessary” market volatility and the welfare costs associated with this volatility. Thus the key to understanding whether these policies are beneficial involves understanding how circuit breakers interact with trading decisions of investors and why trading behavior can potentially lead to excess price volatility when such policies are absent.Footnote 2 In this paper we attempt to shed light on these questions and explore the potentially beneficial role of circuit breakers during financial market runs. We formalize and test the popular argument that circuit breakers provide a “cooling-off” period for investors during market runs and thus provide a theoretical grounding to help understand why circuit breakers may prevent market volatility. To help build intuition for why this is, consider the following scenario. A set of investors face uncertainty about their need for liquidity in the future. For simplicity, assume that the liquidity needs of the investors are perfectly correlated with a common state.Footnote 3 In the bad state, investors will have to sell a traded asset to meet their liquidity needs. In the good state, investors will be able to hold on to the asset and earn its long-run return. News about the state arrives gradually over time. Initially investors do not know whether they will be forced to liquidate. On the other hand, investors know from time 0 that in the bad state forced sales (liquidation) will lower asset prices and it is advantageous to preempt the market and sell before that happens in order to obtain a better price. This selling generates a negative externality on the salvage values for the remaining investors. If the early news is sufficiently bad, there can be a “run” as investors attempt to lock in higher prices and the asset is sold by all investors, although more accurate news may subsequently reveal that liquidating the asset was unnecessary. Thus agents may act precisely when the reliability of their information is lowest because private incentives are insufficient to ensure that the market waits for an efficient amount of information about the value of the asset. In these circumstances, a temporary halt in trading may eliminate the inefficiency caused by early liquidations when the state is good by allowing agents more time to accumulate information about the true state of the world.Footnote 4 We test this hypothesis in a laboratory experiment that reproduces an illiquid market where stochastic information arrives over time. Our experiment compares laboratory financial markets with and without circuit breakers in two different information settings: a setting with highly informative signals where beliefs converge quickly over time and thus a temporary trading halt theoretically should succeed in restoring financial stability, and another where signals are less informative and circuit breakers are expected to fail. These predictions are drawn from a model based on rational agents and we study whether they are robust to the actual behavior of human subjects. We find that, consistent with the basic intuition behind our model, subjects choose to liquidate inefficiently early on average. We also find evidence that in some sessions circuit breakers induce subjects to liquidate their assets earlier than in markets where there are no circuit breakers. However, this is true only in markets with relatively more noisy signals. When the quality of signals is high, the timing of liquidation decisions is not affected by whether the market is endowed with a circuit breaker or not. Finally, we find that circuit breakers reduce subjects’ welfare in the low information environment, but increase welfare in the high information environment.Footnote 5 These findings are consistent with the notion that a market halt can be useful in that it allows more time for information to accrue when private incentives to wait for that information are insufficient. However, this will only be the case when the information revealed during the halt is of sufficiently high quality. Our paper contributes to the large literature trying to evaluate the effects of circuit breakers. Some of the early empirical findings, surveyed in Kim and Yang (2004), are rather ambiguous: on the one hand trading halts do not calm the halted securities (in terms of activity and volatility), but on the other hand they seem to promote information transmission (Kim and Yang 2004, p. 132). More recently, a number of empirical papers have suggested that trading halts improve order flow, liquidity, information dissemination and price discovery (see for example Corwin and Lipson 2000; Chakrabarty et al. 2011; Engelen and Kabir 2006; Hauser et al. 2006; Madura et al. 2006). Lab experiments can complement field data studies on circuit breakers in two ways. First, lab experiments allow the researchers to evaluate whether trading halts are desirable by comparing identical markets with and without circuit breakers. As noted by Corwin and Lipson (2000, p. 1773) the central problem when using field data is that “we cannot know what would have occurred in the absence of a halt or what equilibrium trading patterns would be in a market where halts are not permitted.” Second, lab experiments allow the researcher to quantify (and manipulate) the information available to the market participants, while quantitative measures of information cannot be directly observed in real-world markets (and are usually proxied by the endogenous response of market statistics such as trading volume). Previous lab experiments on circuit breakers (Ackert et al. 2001, 2005) have studied laboratory markets where an asset with a common (but uncertain) final dividend is traded and in certain periods a subset of traders are informed about the distribution of the dividends. These studies find that circuit breakers are not effective in reducing deviations of prices from fundamentals and may accelerate trading activity when an interruption is imminent. Our experiment differs from these existing experiments in several ways, but two substantial differences are that (1) the market is prone to runs induced by liquidity concerns and (2) information accrues over time, even when trades are halted. We view this setting as desirable as it can capture the interactions between market-runs, information acquisition, and the potential role circuit breakers may play in reducing financial market volatility. Indeed, in describing the benefits of circuit breakers, the Brady Report states that they “facilitate price discovery by providing a “time-out” to pause, evaluate, inhibit panic, and publicize order imbalances to attract value traders to cushion violent movements in the market,” (Brady Commission 1988, p. 65).Footnote 6 Our paper contributes to the literature studying financial runs in the lab. Most of this literature has tested the predictions of bank run models (see for example Schotter and Yorulmazer 2009; Madies 2006; Arifovic et al. 2011; Bosch-Rosa 2018) or speculative attacks models (Heinemann et al. 2004; Cheung and Friedman 2009). A strand of this literature has focused on testing the effectiveness of different kinds of regulations in curbing financial instability, see for example Lugovskyy et al. (2014) on the role of asset-holding restrictions and Kiss et al. (2012) on the effects of insurance and the availability of information about other investors. To our knowledge, we are the first to develop a laboratory test of the market runs model of Bernardo and Welch (2004), a framework that is particularly useful for analyzing the role of circuit breakers. Our work is also related to the experiments on clock games by Brunnermeier and Morgan (2010). In clock games, agents receive differently-timed private signals when an asset value is above its fundamental and the price crashes to the fundamental when a fixed number of agents have decided to sell. One important difference between this framework and our experiment is that while in clock games each player acts after receiving his private signal, in our market game players do not wait long enough for information and this creates scope for a welfare-improving trading halt. Finally, in concurrent work, Kendall (2015) also studies trading panics in a laboratory market where information is received over time. However, while in Kendall (2015) subjects receive private signals about the asset fundamental value, in our experiment subjects receive signals about an impending shock to market liquidity as in the Bernardo and Welch (2004) framework. The rest of the paper is organized as follows. In Sect. 2 we present the theoretical framework. In Sect. 3 we describe the experimental design and hypotheses. In Sect. 4 we present the results and Sect. 5 concludes.",5
23.0,1.0,Experimental Economics,25 February 2019,https://link.springer.com/article/10.1007/s10683-019-09604-3,Strategic decisions: behavioral differences between CEOs and others,March 2020,Håkan J. Holm,Victor Nee,Sonja Opper,Male,Male,Female,Mix,,
23.0,1.0,Experimental Economics,11 May 2019,https://link.springer.com/article/10.1007/s10683-019-09612-3,Efficiency versus gender roles and stereotypes: an experiment in domestic production,March 2020,Hélène Couprie,Elisabeth Cudeville,Catherine Sofer,Female,Female,Female,Female,"In most countries, the allocation of time-use remains highly differentiated by gender (OECD 2018). Gender inequalities in the sharing of domestic work, that is taken on primarily by women, remain persistent despite the growing number of women entering the labour market and the decline in the gender pay gap (Aguiar and Hurst 2007). This observation, in addition to not be satisfactorily explained, casts doubt on the standard efficiency assumption made in most economic household models. The aim of this paper is to investigate the efficiency of household production decisions through an incentivized experiment involving well-established couples, and to further explore the role of task preferences, social gender roles and stereotypes in explaining spouses’ task specialization choices. Using household survey data to test the efficiency of household production decisions is challenging, given that domestic production is generally not observed.Footnote 1 Although time inputs of both partners can be recovered through time-use data, neither other inputs nor outputs within the household can be observed. Thus, domestic production functions can never be estimated in a satisfactory way, and convincing micro-econometric tests of efficiency cannot be implemented. The experimental approach proposed here overcomes these difficulties by allowing direct measurement of individual domestic productivity and by controlling for production technology. Productivity differences between men and women in market activities could explain why their housework time-allocation decisions differ. However, empirical studies show that variables reflecting productivity in the labor market, such as wage rates and education, are far from being the only factors driving the gendered division of labor (Greenstein 2000; Kalenkoski et al. 2009; Schneider 2011). Using the French Time-Use Survey, Rizavi and Sofer (2010) show that when a woman earns a higher wage than her male partner, there is no role reversal in domestic production: she remains more involved in household duties. Sevilla-Sanz et al. (2010) find similar results in Spanish data. In the US and Australia, the gender gap in non-market work is even higher when a woman earns more than her male partner (Bittman et al. 2003; Bertrand et al. 2013). These observations are hard to reconcile with standard household models that assume Pareto-efficiency in intra-household decisions. Furthermore, a number of studies question and reject the validity of the efficiency assumption, especially on the household production side (Udry 1996; Duflo and Udry 2004). Another way to reconcile the empirical evidence described above and the efficiency assumption would be to assume some gender differences in the marginal productivity of labor in home activities. Women could be more productive than men at home, and their productivity advantage could more than compensate for the wage gap when they are better paid than their partner. This is the assumption made by Becker (1981), which he bases on the gendered training that boys and girls receive, as well as on the possibility that they have different biological abilities. However today, in developed countries at least, though toys and games are still highly gendered, it is difficult to argue that specific skills are needed to perform household chores given the widespread use of market substitutes for domestic goods and household appliances. Moreover, most fathers are now involved in caring for children of all ages, including newborns. Therefore, excluding the very limited time period of pregnancy, and possibly breastfeeding, systematic gender productivity differences in domestic tasks and the raising of children do not seem likely. Beside productivity differences, persistent gender differences in preferences could explain the aforementioned puzzle. Women, on average, may “like” to perform domestic tasks more than men, due to habit, education or socialization. Preferences would then appear on the production side of household decision-making. Non-monetary costs (or gains) would then be added to (or subtracted from) monetary costs in the household profit function. To be consistent with the observed division of labor, the difference between men and women in terms of these non-monetary costs would have to be high enough to compensate for any difference in wages when women’s wages are higher.Footnote 2 Social norms could also be an explanation of inefficiencies in household production and of the observed differentiation between male and female activities. Rules of behavior dictated by society such as “gender roles” could impose additional utility benefits or costs to direct preferences when an individual does or does not comply with these rules (Akerlof and Kranton 2000).Footnote 3 In this case, households might choose rationally to deviate from first-best efficiency. This assumption, related to the literature on separate spheres of influence initiated by Lundberg and Pollak (1993), has found empirical support in the context of developed countries (e.g. Lundberg et al. 1997) as well as in developing countries (e.g. Duflo and Udry 2004; Kusago and Barham 2001; Dasgupta and Mani 2015). Inefficiencies in domestic production could also stem from gender stereotypes. Gender stereotypes are the set of societal beliefs that link personal attributes to the social categories “women” and “men”. For example, people may believe that women are more talented than men in domestic tasks and child care. Since the pioneering work of Phelps (1972) and Arrow (1973) on statistical discrimination, it has been recognized that stereotypes may impact actual group characteristics and perpetuate inequalities through false beliefs, even in the absence of intrinsic group difference (Coate and Loury 1993; Mechtenberg 2009; Sofer and Thibout 2011; Bordalo et al. 2016) and many empirical papers and experiments mention stereotypes as an explanation of observed differences in behavior between men and women (Lavy 2008; Dasgupta and Mani 2015 for example). In laboratory experiments, gender stereotypes have been a central topic in the field of experimental psychology (see Schneider 2005 or Jussim et al. 2015). In experimental economics, we can mention Coffman (2014), Günther et al. (2010) and Grosse et al. (2014) who compare competitive attitudes of men and women on “male” and “female” tasks. All find strong support for the theory of stereotypes. The proposed experiment was designed to test these three different possible causes of inefficiency: (i) gendered preferences for tasks, (ii) gender roles dictated by social norms and (iii) gender-stereotyped beliefs. Heterosexual couples, who had been together for at least 1 year, were asked to complete everyday tasks within a maximum allotted time, and the couple was paid for the time saved, possibly at different rates, allowing for differences in opportunity cost of time between male and female partners. Couples therefore had an incentive to minimize the time spent on tasks as it represented a fixed cost that reduced their paid-time, and hence their monetary payoff. Individual productivity on tasks was measured in separate work sessions to calibrate the optimal time allocation. It was then possible to verify whether spouses allocated their time efficiently across tasks according to their comparative advantage (that is, maximized their joint monetary payoff). In order to investigate the role of gender norms and stereotypes in deviations from efficiency, the experiment contained a 2 \(\,\times\,\) 2 design: half of the couples faced two highly gender stereotyped tasks (folding socks, screwing on metal brackets) and the other half two gender-neutral tasks (stuffing envelopes, completing forms), while in each group, half of the couples were informed of both partners’ performance on tasks, and the other half were not.Footnote 4 Moreover, information about task preferences of subjects and beliefs about their partner’s productivity was gathered through individual questionnaires, completed privately and separately during the course of the experiment. A vast body of literature has now analyzed gender differences using an experimental framework (see the survey by Croson and Gneezy 2009). A number of experiments involved well-established couples in order to get inside the household and explore various aspects of the household decision-making, such as efficiency, bargaining, risk attitude and time preference (see Munro, 2018 for a comprehensive survey). The present paper belongs to this strand of literature but is new on three grounds. First, it explores an aspect of efficiency that, to our knowledge, has not been studied experimentally to date: the production side of household decision-making and the division of labor. Second, unlike most experimental studies on household efficiency which rely on experimental games, such as trust, voluntary contribution or modified dictator games (e.g. Peters et al. 2004; Chao and Kohler 2007; Munro et al. 2010; Munro et al. 2014; Beblo et al. 2015; Cochard et al. 2016), subjects had to work on concrete day-to-day tasks. Rather than featuring “intellectual” real effort tasks, such as solving mazes or mathematical problems, we implemented manual work tasks in order to mimic, as much as possible, real household production.Footnote 5 Lastly, while in a typical experiment couples’ members are asked to make decisions together, to share information, or to make decisions separately, the design proposed here does not force a couple to adopt a particular model of decision-making. The latter two points are methodological in nature but important in terms of external validity, as pointed by Munro (2015). Our main results are the following. Although comparative advantage partially drives spouses’ specialization choices, on average, couples fail to attain full maximization of their money payoff. Moreover, when partners face gendered tasks, productive inefficiencies are exacerbated in the direction consistent with gender roles. Jointly providing partners with information about their individual productivity on tasks partially reduces deviations from efficiency under the gendered condition, which confirms the influence of stereotypical beliefs, above gender roles, on task sharing within the couple. Differences in partners’ preferences for tasks, controlled for using data collected from individual questionnaires, have no significant effect on these results. The remainder of the paper is organized as follows. Section 2 describes the conceptual framework that guides the experimental design. Section 3 details the experimental design: the setting and treatments. Section 4 presents the experimental outcomes and empirical analysis of these results. Section 5 concludes the paper.",2
23.0,1.0,Experimental Economics,01 April 2019,https://link.springer.com/article/10.1007/s10683-019-09606-1,Decentralized matching markets with(out) frictions: a laboratory experiment,March 2020,Joana Pais,Ágnes Pintér,Róbert F. Veszteg,Female,Female,Male,Mix,,
23.0,1.0,Experimental Economics,10 May 2019,https://link.springer.com/article/10.1007/s10683-019-09615-0,The effects of conflict budget on the intensity of conflict: an experimental investigation,March 2020,Kyung Hwan Baik,Subhasish M. Chowdhury,Abhijit Ramalingam,,Unknown,Unknown,Mix,,
23.0,2.0,Experimental Economics,18 May 2019,https://link.springer.com/article/10.1007/s10683-019-09614-1,The strength of weak leaders: an experiment on social influence and social learning in teams,June 2020,Berno Buechel,Stefan Klößner,Heiko Rauhut,Male,Male,Male,Male,"In our rapidly changing world, most modern organizations are embedded in highly dynamic environments. For the management of an organization, the first essential step to successful decision-making is the basic task of obtaining an accurate view of the environment.Footnote 1 For instance, this can be the foundation for defining a mission statement, as argued, e.g., in Bolton et al. (2013). Recently, there have been a number of contributions showing that organizations can improve their decision-making by harnessing the wisdom of crowds instead of using the expertise of only a single individual (e.g., Surowiecki 2004; Mannes 2009; Keuschnigg and Ganser 2017). However, this literature has not analyzed whether a team’s ability to learn from each other depends on characteristics of the team leader. Given each team member’s initial level of information, the updated opinions’ accuracy depends on the social learning process within the team. Many teams are organized such that one person, the team leader, directly communicates with each team member, while the other members often communicate only indirectly with each other—via the team leader. In this paper, we address the question of how the selection of the team leader affects the performance of social learning in the team. Is it necessary that the central person is the one with the highest expertise? How does self-confidence affect the process of social learning? Should the selection criterion be declared or rather hidden? Answering these questions can be informative for the design of successful organizations. To address these research questions, we set up a laboratory experiment in which subjects are asked to answer incentivized estimation questions repeatedly. After each round, every team member observes the leader’s guesses, while only the leader observes the guesses of all members. We randomly allocate subjects into three treatments, which differ by whether the leader is selected at random, by confidence or by expertise. We use real questions, while previous experiments used highly stylized tasks such as guessing an average (or its sign) of randomly drawn numbers (Çelen et al. 2010; Corazzini et al. 2012) or finding an abstract true state (Choi et al. 2005; Brandts et al. 2015; Chandrasekhar et al. 2015; Grimm and Mengel 2018). Yet, studying real teams has severe endogeneity problems. For these reasons, we explore the middle ground between theory-testing experiments and field data. We import a method developed outside of economics (Lorenz et al. 2011; Rauhut and Lorenz 2011), which is increasingly used. Participants are asked to answer knowledge questions about vaguely known facts for which the true answer is known (and could in principle easily be looked up, e.g., on Wikipedia.com). Subjects are paid according to their answers’ accuracy and can communicate their confidence levels. The latter aspect is missing in most other experiments of social learning because it is simply not necessary to communicate confidence if signal quality is artificially made common knowledge.Footnote 2 From a Bayesian perspective, selection of the leader does not matter due to efficient social learning: As it will become clear below, Bayesian learners exchange their opinions such that a consensus is reached independent of who is at the center of the communication network.Footnote 3 In contrast, naïve social learning predicts consensus over time with a strong “bias” towards the center’s initial opinion.Footnote 4 Unless the leader is much better informed than the other team members, this is suboptimal, giving the leader’s opinion too much weight. Hence, any leader characteristic that further amplifies the weight of the leader’s opinion undermines performance. As such, we study the leader’s self-confidence, as well as the public declaration of why the leader was selected. We assess performance by the proximity of a guess to the correct answer. In particular, we measure the individual and the collective errors of the team’s guesses, and use a measure of the wisdom of the crowds. We show that selection of leaders by accuracy or confidence does not outperform random selection. Selection by confidence even undermines performance. Teams with random leaders have the advantage that the non-leaders’ guesses are taken into account more strongly when updating information, thereby improving the team’s performance. The underlying reason is that declaring the leader as somewhat superior, be it in terms of past performance or past confidence, induces team members to put more weight on the leader’s opinion, making the team vulnerable to be misled by a single person. For a deeper understanding of the opinion dynamics, we further develop rational and behavioral learning models which we compare to our data. Despite a long tradition of theoretical insights and a growing body of empirical research, social learning is still far from being fully understood. Our comparison between theoretical models and empirical data reveals that people adapt their opinions insufficiently—providing evidence for what is called conservatism. While conservatism is common in experiments on belief updating,Footnote 5 our extension of social learning models by conservatism is novel. Notice that it is entirely possible that subjects are conservative and at the same time pay too much attention to another subject’s opinion. For instance, the declaration of the leader selected by confidence induces our subjects to put too high weights on both, the leader and themselves, at the expense of the weight they can put on the other group members. Our paper entails three contributions. First, we provide empirical evidence for advantages of random leader selection (also called sortition, demarchy, allotment, or aleatory democracy). Despite a long tradition of discussion (e.g. Zeitoun et al. 2014; Frey and Osterloh 2016), empirical evidence is rare and mechanisms are unknown.Footnote 6 We demonstrate that declaration of non-random leader selection amplifies the weight of the leader’s opinion, which may result in a loss because the wisdom of the crowds in the group is not harnessed. Second, we show that overprecision (or judgemental overconfidence), which is the tendency to provide too narrow confidence intervals for one’s estimates (e.g., Soll and Klayman 2004; Moore and Healy 2008; Herz et al. 2014) is associated with lower team performance. This suggests that either overprecise leaders should be generally avoided or that the trade-offs between the positive effects of overprecise leaders (e.g., fostering coordination, Bolton et al. 2013; or motivating team members, Gervais and Goldstein 2007) and their negative impact on social learning should be carefully balanced. Third, our paper makes a methodological contribution. By combining experiments on factual questions with theories on social learning, we build a bridge between neat theoretical frameworks and experimental set-ups that are less stylized. This demonstrates that the assumption of common knowledge about signal precision is problematic. In reality, people do not know the signal precision of their interaction partners, form expectations about it and take into account with which confidence others’ opinions are communicated. Moreover, behavioral biases such as overprecision, anchoring effects, or selection bias in information acquisition can give rise to conservatism in updating. When incorporating this idea into both naïve and rational models of social learning, we find that each model’s fit to the data increases, although the distances to the true answers become larger.",1
23.0,2.0,Experimental Economics,29 April 2020,https://link.springer.com/article/10.1007/s10683-019-09617-y,How soon is now? Evidence of present bias from convex time budget experiments,June 2020,Uttara Balakrishnan,Johannes Haushofer,Pamela Jakiela,Unknown,Male,Female,Mix,,
23.0,2.0,Experimental Economics,08 March 2019,https://link.springer.com/article/10.1007/s10683-019-09605-2,Experiments in high-frequency trading: comparing two market institutions,June 2020,Eric M. Aldrich,Kristian López Vargas,,Male,Male,Unknown,Male,"Telecommunications technology has transformed financial markets in the last decade. Order submission and execution times at major exchanges have declined from seconds to microseconds (1 millionth of a second). As a consequence, traders are rewarded for reacting quickly to information, resulting in a new market participant: high-frequency trading (HFT) firms. HFT firms use computerized strategies to transact large volumes in fractions of a second and now account for more than half of transactions at major exchanges worldwide. Proponents claim that HFT has improved market liquidity (the ease with which trades can occur) and reduced transaction costs (Narang 2010). Opponents argue that the multi-billion-dollar cost of HFT infrastructure is ultimately borne by investors, its liquidity is illusory, and it is a destabilizing force in financial markets (Lewis 2015). Most academic papers studying HFT use proprietary data with trader identification, and are able to classify accounts as either aggressive (primarily liquidity consuming) or passive (primarily liquidity providing). Passive accounts are almost uniformly associated with improved market performance. Hagströmer and Nordén (2013), Malinova et al. (2014), Brogaard et al. (2015), Jovanovic and Menkveld (2015) and Menkveld and Zoican (2017) are examples of papers that find such positive effects. The effects of aggressive HFT, however, are mixed: it is generally associated with informed price impact over short horizons, increased adverse selection costs for other traders, increased short-term volatility, and higher trading costs for institutional and retail traders. Examples from this literature include Zhang and Riordan (2011), Bershova and Rakhlin (2012), Breckenfelder (2013), Brogaard et al. (2014), Hasbrouck and Saar (2013), Hendershott and Riordan (2013), Hirschey (2013), Baldauf and Mollner (2015b), Baldauf and Mollner (2015a), Brogaard and Garriott (2015) and Menkveld and Zoican (2017). Despite the uncertainty surrounding the effects of high-speed trading, policy makers worldwide are already taking actions intended to discourage HFT. For example, in 2016, the U.S. Securities and Exchange Commission approved the Investors Exchange (IEX) to operate as a public securities exchange. A primary goal of the IEX market, which was founded in 2012 to provide a non-public alternative trading system (ATS) with delayed messaging (see Aldrich and Friedman 2017), is to reduce potential advantages of HFT firms. The SEC decision to approve the IEX system as a public exchange had substantial network effects, primarily because of quote protection under Reg NMS Rule 611 (Hu 2018). In addition to IEX, several other market formats have been proposed as alternatives to the CDA. These include the frequent batch auction (Budish et al. 2015, 2014) and the fully continuous exchange (Kyle and Lee 2017). However, no scientific evidence exists on the relative performance of such market institutions. The objective of this paper is not to address the net costs or benefits of HFT, but instead to compare the effects of different trading environments on market quality in the presence of HFT. Specifically, we use laboratory experiments to compare two leading financial market formats in the presence of high-frequency trading: the continuous double auction (CDA) and the frequent batch auction (FBA). The CDA (also known as the continuous limit order book) organizes trade in the majority of equities, futures and currency exchanges around the world. In this format, traders make publicly committed offers to buy and sell assets and are able to accept others’ offers at any moment of time. In addition, traditional CDA markets employ a price-time priority system, which first ranks orders by price and then by the time they are received at the exchange (within price level). Because trading is continuous in time and orders are ranked by their submission time, communication speed is crucial in the CDA. Traders who can quickly react to new information have a substantial advantage over slower traders. This generates competition for speed technology, which is potentially socially inefficient. The FBA, on the other hand, does not allow trading to occur continuously. Instead, bids and asks are collected (batched) over discrete time intervals and call auctions are conducted at the end of each batching period. FBA therefore gives equal priority to orders received within a batching period, reducing the advantage of fast communication technology. Competition for speed becomes much less relevant than in the CDA and competition on price regains primacy. The environment for our laboratory experiments is taken from the theoretical model of Budish et al. (2015) (hereafter referred to as BCS), where a single asset is traded on a single exchange. Traders submit commitments to buy or sell the asset in the form of limit orders and market orders. Two exogenous processes generate incentives to trade in this environment: changes in the publicly-observed fundamental value of the asset and the arrival of market orders from noise traders (investors) at random times. Although the two market formats price transactions differently, both assume that purchases (sales) are simultaneously liquidated (purchased back) at the fundamental value, allowing traders to book profits or losses instantaneously. Human participants (acting as traders) choose among three broad strategies to earn real profits: to act as market makers, to act as snipers, or to not participate in the market. Their choices can be continuously revised throughout the experiment. Market makers and snipers may subscribe to a technology that reduces messaging latency to the exchange for a pre-specified flow cost. Additionally, market makers choose a spread around the asset value at which they post bids to buy (below the value) and offers to sell (above the value). Market makers earn profit when the exchange matches them with a counterparty. Snipers attempt to exploit temporarily mispriced maker orders (stale quotes) by transacting with them at the time of a jump in the asset value. To emulate modern financial markets, we develop an electronic architecture in which information and tradingFootnote 1 occur at millisecond time granularity. Specifically, for each format we deploy remote exchanges in an Amazon data center and utilize the Nasdaq OUCH protocol for messaging. Human subjects make high-level strategic decisions (outlined above) by interacting with a computer interface in the laboratory. Their decisions are encoded into algorithms which act on their behalf by communicating with the exchange as market conditions change. The arrival of exogenous information to the market via investor orders and changes in the fundamental value is designed to be representative of the time-scale in which similar information arrives in the market for a liquid asset, such as a the S&P 500 exchange traded fund. Thus, human subjects take the role of analysts at trading firms that design algorithmic strategies rather than the algorithms themselves. The BCS model makes sharp equilibrium predictions regarding the roles that subjects choose and their decisions to purchase fast communication technology. Under the CDA, market liquidity is minimal in equilibrium since the predatory strategy (sniping) is chosen by all but one trader. In contrast, under the FBA, all traders choose to provide liquidity (i.e. to be a market maker) in equilibrium. The BCS model also predicts that in the CDA every trading firm adopts the costly speed technology, while in the FBA no trader purchases that technology. Although traders make zero profits in the equilibria of both formats, social welfare is superior under the FBA because retail investors pay a positive market spread in the CDA while in the FBA the spread is zero. This welfare difference is associated with a prisioners dilemma that arises under CDA—the dominant strategy is for all traders to purchase fast communication technology, despite the fact that all would be better off without it—and which disappears in the FBA, as the gains of purchasing speed technology are minimal. To understand the relationship of observed behavior to changes in the underlying environment, and hence predicted equilibria, we consider three treatments that vary the rate at which the asset value changes, the rate at which investors arrive, and the cost of fast communication technology. Regardless of treatment parameters, we find that subjects in the FBA market display substantially more passive liquidity, engage in less predatory behavior, and are less likely to purchase fast communication technology. Further, all FBA markets exhibit lower transactions costs, greater price efficiency, and less volatility. These results are all highly statistically significant and directionally support the predicted equilibria, although their magnitudes are somewhat attenuated relative to the precise equilibrium predictions. Our work is as much a test of the BCS equilibrium model as a comparison of the CDA and FBA market formats. As such, our results are only relevant to real-world markets to the extent that the BCS model is a good characterization of actual trading behavior in those markets. However, testing the behavioral robustness of the BCS environment is interesting and useful in its own right. First, it is not clear, a priori, that human subjects can adequately learn, or that their behavior can converge in such a complicated and highly stochastic environment. Our results demonstrate that this is possible and can lead to useful insights. Additionally, the predicted equilibria are stylized and somewhat implausible due to their invariance over a wide range of the parameter space. Understanding behavioral divergences from these predictions will assist in the development of new theoretical models. This paper contributes to the market design literature in finance and experimental finance. Related prior research on financial market design includes Roth and Xing (1994), who study the timing of transactions, Roth and Xing (1997), who study serial versus batch processing, Foucault (1999) and Roth and Ockenfels (2002), who introduce the idea of bid sniping, Ariely et al. (2005) who study how Internet auctions’ ending rules shape the incentives for bid sniping, Du and Zhu (2017) and Fricke and Gerig (2015), who study the optimal frequency of double auctions, and Biais et al. (2014), who study “fast trading” and the externalities it generates. Haas and Zoican (2016) present a theoretical study of the impact of batch length on liquidity using an extension of the BCS model. Webb et al. (2007) empirically study the impact of the frequency of market clearing on volatility. More broadly, this paper contributes to the literature that contrasts the performance of market institutions which has a long tradition in experimental economics.Footnote 2 Many of these studies allow for comparisons between versions of continuous double auction and repeated call markets (FBA, in our paper). As reported in Friedman and Rust (1993), early laboratory experiments comparing the efficiency of the CDA to other formats (e.g., bilateral search, posted offer, and one-sided auctions) yielded results that were favorable to the CDA (Plott and Smith 1978; Williams 1980). Smith et al. (1982) compares the CDA with call markets and finds that CDA results in faster equilibrium convergence and more efficient outcomes.Footnote 3 In contrast, Friedman (1993) reports that a continuous-information version of a call market exhibits efficiency levels (informational and allocative) comparable to the CDA. Additionally, transaction prices in the call market are closer to the equilibrium price and market spreads are narrower. McCabe et al. (1992) also finds that the call auction exhibits smaller price variation than the CDA. Cason and Friedman (1996) compare several market formats including a repeated call market, CDA, a uniform-price CDA and a single call market. They find that FBA and CDA exhibit similar levels of trading and informational efficiency.Footnote 4 Van Boening et al. (1993) finds that bubbles and crashes are observed in both the CDA and call auctions and Schnitzlein (1996) reports that call auctions are equally efficient as CDA markets and outperform the CDA in market liquidity. Theissen (2000) finds that opening prices in a call market track the fundamental value of an asset better better than the those of the CDA. However, the call market also underreacts to the arrival of new information. Using a model with randomly drawn redemption values, Kagel (1998) finds that both CDA and call markets display high efficiency levels and non-important price deviations from equilibrium level, but the CDA mechanism generates slightly higher allocative efficiency. Huber et al. (2008) study whether individuals with better information about the fundamental value earn more than those with less information in CDA and call markets. They report that, in both formats, only traders with significantly more informatation than others can achieve better payoffs. Morone and Nuzzo (2016b) study a market in which traders are uncertain about the presence of insiders. They find that information mirages occur more frequently under CDA relative to the call auction. Without market insiders, the call auction exhibits faster convergence to equilibrium and more stable prices. In the presence of market insiders, both formats exhibit similar informational efficiency performance.Footnote 5 We highlight that existing research on financial auctions focuses on environments that are not specifically relevant to the study of HFTs. To our knowledge, this paper reports the first experimental study of the CDA and FBA in the presence of HFT. The rest of the paper is organized as follows. Section 2 presents the experimental design and its implementation, Sect. 3 reports results of the experiments and Sect. 4 concludes. We collect discussions and proofs related to model calibration, off-equilibrium behavior and equilibrium market statistics in Online Appendices A–D. Instructions for the experiments are included in Online Appendix E.",8
23.0,2.0,Experimental Economics,11 May 2019,https://link.springer.com/article/10.1007/s10683-019-09613-2,Gender differences in sabotage: the role of uncertainty and beliefs,June 2020,Simon Dato,Petra Nieken,,Male,Female,Unknown,Mix,,
23.0,2.0,Experimental Economics,31 May 2019,https://link.springer.com/article/10.1007/s10683-019-09616-z,‘Everybody’s doing it’: on the persistence of bad social norms,June 2020,David Smerdon,Theo Offerman,Uri Gneezy,Male,Male,Male,Male,"Social norms provide informal rules that govern our actions within different groups and societies and across all manner of situations. Many social norms develop in order to overcome market failure, mitigate negative externalities or promote positive ones so as to facilitate some collective goal (Arrow 1970; Hechter and Opp 2001). However, social norms that are inefficient from a welfare perspective also persist. A key feature of a social norm is the desire to conform to the majority in a group. We follow Bicchieri (2017, p. 35), who defines a social norm as a “rule of behavior such that individuals prefer to conform to it on condition that they believe that (a) most people in their relevant network conform to it (empirical expectation), and (b) most people in their relevant network believe they ought to conform to it (normative expectation) and may sanction deviations.” We define a ‘good’ social norm as a norm that is supported in an equilibrium that maximizes group welfare, and a ‘bad’ social norm as a norm that is supported in an equilibrium that does not. Sometimes good norms become bad norms when over time the payoff structure changes such that the norm ceases to be good for the group. One such example is provided by norms of revenge. In societies where there is no or minimal rule of law to enforce individual rights, tit-for-tat strategies are more likely to emerge as substitutes so that people can defend their property and honor (Elster 1990). In such settings where people cannot rely on the protection of an authority, norms involving revenge behavior can be welfare-enhancing (Elster 1989). The danger of such norms is that if someone transgresses—and, for instance, kills a neighbor in a fight—this may trigger a long, bloody encounter between families. A culture of these so-called ‘blood feuds’ can persist even after political or legal transitions have eroded the social benefits, or after a group has migrated to an area with an effective government and legal system (İçli 1994; Grutzpalk 2002). In either case, the persistence of the social norm no longer serves its purpose and becomes welfare-damaging. Another example is that of gender roles in the labor market. Alesina et al. (2013) find causal evidence that traditional agricultural practices explain differences in attitudes towards workforce participation today, and that this effect is driven by persistent cultural norms.Footnote 1 Specialization of production along gender lines may have been a good equilibrium under certain conditions, such as when particularly productive occupations were physically demanding, home production was less efficient, and/or women had a comparative advantage in mentally-intensive tasks (Galor and Weil 1996; Greenwood et al. 2005). The traditional gender roles and cultural beliefs stemming from these labor conditions are supported by social sanctions for deviations and can persist even when the economy moves away from such an environment, or when groups migrate to a more developed country (Fernandez and Fogli 2009; Jayachandran 2015). In this paper, we investigate conditions under which bad norms persist. We experimentally test situations in which a bad norm initially emerges as a good norm, but changes to the environment over time alter the payoff structure such that the norm ceases to solve negative externalities and actually begins to promote them. The most important contribution of our paper is that we investigate the extent to which a lack of information about others’ preferences or attitudes is important for the development of bad social norms. In particular, we provide evidence that compares two predominant but opposing views on how information about others’ preferences shapes bad norms. One perspective is that bad norms can thrive independent of whether or not people are informed of the preferences of others. This view is supported by the model of Brock and Durlauf (2001), whose approach we use to study the development of norms. They propose a stationary coordination game in which agents are driven by a taste for conformity. All other things equal, agents benefit when more people make the same choice as they do. In equilibrium, players either coordinate on the welfare-maximizing allocation or on a welfare-inefficient allocation. We consider different versions of their game by allowing players to be uncertain about the preferences of others, and by allowing preferences to change over time.Footnote 2 The alternative view is that bad norms are driven by pluralistic ignorance. Pluralistic ignorance refers to a situation in which most individuals have private attitudes and judgments that differ from the prevailing norm, and wrongly believe that the majority of group members have a private preference to keep to the status quo (Miller and McFarland 1987; Katz and Allport 1931). As a result, a bad norm may persist even though the majority of the group would like to change it.Footnote 3 Notice that in this approach, the uncertainty about other individuals’ preferences is a necessary condition (Sherif 1936). In combination with wrong beliefs about the preferences of others, it may lead to the emergence and persistence of bad social norms. We use the model of Brock and Durlauf (2001) to design an experiment that allows us to investigate the role that information about others’ preferences plays for the development of bad norms, thus incorporating the insights from the pluralistic ignorance viewpoint. In our experiment, we monetarize social payoffs.Footnote 4 In a setup with a relatively small group size and in which the benefits from coordinating are large (with a strong ‘social value’ component), a setup that we conjecture to be particularly conducive for bad social norms, we vary the information about others’ preferences in two ways. First, we compare a version of the game in which subjects are uncertain about others’ preferences with a version in which subjects are fully informed. Second, in the version in which they are uncertain about others’ preferences, we vary whether subjects can communicate about their intended actions. We use the version of the game with incomplete information about others’ preferences to investigate some other variables that may affect how likely it is that people adhere to norms after they have turned to bad norms: (i) the social value component and (ii) group size. In fact, a strong social value component is essential for the existence of bad social norms in the approach of Brock and Durlauf (2001). The experimental results show that the information about others’ preferences is crucial for the emergence and persistence of bad norms. In agreement with pluralistic ignorance, when subjects are fully informed about others’ preferences, groups move swiftly away from a good equilibrium after it has become bad. Allowing subjects to communicate also helps to escape norms that have become bad. Communication reduces the uncertainty about other subjects’ preferences and intended behavior. On the other hand, bad norms thrive when subjects are uncertain about the payoffs of others. While the main results support the pluralistic ignorance perspective, other experimental results within an incomplete information environment accord well with the Brock and Durlauf approach. In particular, the stronger the social value component, the more likely a bad norm is to persist. This result resonates with the finding in minimum effort games that it is more difficult to coordinate on the ‘good’ equilibrium when it is individually more costly to do so (for example, Devetag and Ortmann 2007). We find that bad norms are more likely to persist in larger groups in the short term, but this effect disappears in the long run. Our paper contributes to a literature on the emergence and persistence of bad norms. Devetag and Ortmann (2007) survey how bad outcomes can emerge in team production processes that are characterized by a minimum effort production function. In the minimum effort game, players simultaneously exert costly effort, and the minimum effort in the team determines its productivity. The stage game hosts a multitude of Pareto-ranked equilibria. In agreement with risk dominance, subjects in experiments usually quickly coordinate on a bad equilibrium that offers them a secure but low payoff, unless group size is very small (Van Huyck et al.1990; Knez and Camerer 1994).Footnote 5 A special feature of the minimum effort game is that if only one ‘rotten apple’ provides low effort, all other players want to choose the same low effort. In the framework of Bicchieri (2017, p. 35), the minimum effort game is not about norms, because it is not a game where players want to follow what the majority in the group does (except in the cases where the majority choice coincides with the minimum). In addition, in the game that we study in the experiment the good equilibrium always risk-dominates the bad equilibrium, so risk dominance by itself cannot explain the persistence of bad norms that we observe in some circumstances. In a similar vein, Lim and Neary’s (2016) experimental investigation of stochastic adjustment dynamics also uses a large binary-action population game, the language game, in which individuals’ choices are strategic complements. They find strong evidence that individuals behave consistently with a best-response learning rule based on the previous period’s outcomes, which, in a noisy environment, can lead to groups escaping coordination equilibria. While our results are broadly consistent with this literature, our game also yields different insights: in the game that we study incomplete information on others’ preferences is needed for the persistence of bad norms, while in the minimum effort and language games the bad outcome results even with complete information about preferences. More recently, Abbink et al. (2017) identify an alternative driver of bad norms. The central insight from their experiment is that punishment opportunities can, under certain circumstances, lead to socially destructive norms being enforced in public good games. Specifically, in a linear public good game where group members only marginally benefit from others’ contributions, such that the socially optimal act is to not contribute, they find that subjects support a bad social norm when they have the possibility to punish free-riders. The key difference between their approach and ours is that they study the emergence of bad social norms in inefficient public good provision, whereas we focus on pure coordination situations in which the question is whether groups can move from one equilibrium to a better one. Closely related to our paper in terms of experimental design are Andreoni et al. (2017) and Duffy and Lafky (2018). Andreoni et al.’s (2017) investigation of so-called ‘conformity traps’, conceived independently and concurrently, complements our approach. The most important differences in design are the information environment and the payoffs pertaining to individuals who deviate from a norm. Individuals in their experiment know the true distribution and evolution of group preferences, such that pluralistic ignorance cannot play a role. By comparison, in our setup the individuals who deviate first from the current norm incur disproportionately large costs for pioneering the change, creating stronger incentives to wait for others to deviate first.Footnote 6 A particularly relevant feature of their results is that bad norms can still persist with full information over group preferences, so long as the strength of social payoffs is sufficiently high. Other than that, their main results are consistent with our own: (1) The scale of social payoffs, relative to individualistic utility differences, is crucial for conformity to a bad equilibrium, (2) Smaller groups can break a conformity trap faster, and (3) Anonymous communication through polls can aid escaping a conformity trap. Duffy and Lafky (2018) independently study the role of the strength of conformity and uncertainty about others’ preferences to test under which circumstances people choose against their privately held preferences. There are a couple of differences with our study. First, their model looks at two types rather than a continuum of individual preferences. Second, in their full information condition, subjects know the progression of preferences. Third, in their incomplete information condition, subjects know that in each period with probability 0.75 one player’s type will switch. Despite these and other minor differences, their results agree qualitatively with our results. The remainder of the paper is organized as follows. Section 2 presents the game and some theoretical benchmarks. In Sect. 3, we detail the design and procedure used to transpose the model into the laboratory, and we present the equilibrium predictions for the experimental parameters. Section 4 discusses the experimental results, from which insights into the factors affecting bad norm persistence are presented, and Sect. 5 concludes with a discussion of the results and implications.",16
23.0,2.0,Experimental Economics,01 July 2019,https://link.springer.com/article/10.1007/s10683-019-09618-x,Cognitive processes underlying distributional preferences: a response time study,June 2020,Fadong Chen,Urs Fischbacher,,Unknown,Male,Unknown,Male,"A huge body of evidence indicates that people are willing to sacrifice their own material resources to benefit or hurt others. Empirical research has investigated the motives underlying this behavior and theoretical models have been developed to formalize these motives (Bolton and Ockenfels 2000; Charness and Rabin 2002; Dufwenberg and Kirchsteiger 2004; Falk and Fischbacher 2006; Fehr and Schmidt 1999; Rabin 1993). More recently, the cognitive processes which govern people’s social behavior have come into focus: How are social decisions actually made? Social decisions are particularly interesting because they can be considered as compound goods satisfying different motives. Cognitive processes are about how we deal with these conflicting motives. Identifying the cognitive processes underlying social decision making has major implications for understanding human nature, and from a practical point of view for modeling cognitive processes in order to predict human behavior or design institutions to promote prosocial behavior (Bear and Rand 2016; Cone and Rand 2014; Krajbich et al. 2014, 2015b). There has been considerable interest in exploring the cognitive processes of social decision making using dual-process approaches which assume the existence of two qualitatively distinct processes: One is relatively automatic and intuitive, and the other is relatively controlled and deliberative (Achtziger and Alós-Ferrer 2014; Alós-Ferrer and Strack 2014; Brocas and Carrillo 2014; Chaiken and Trope 1999; Frederick 2005; Fudenberg and Levine 2006; Hauge et al. 2016; Kahneman 2003, 2011; Sloman 1996; Strack and Deutsch 2004). Relative response times (RTs) are widely used to distinguish between intuitive and deliberative processes, since intuitive processes are executed more quickly than deliberative processes (Krajbich et al. 2015a). In the domain of social preferences, this raises the question of whether some motives are processed more automatically than others, in particular whether the selfish or the social motive is more intuitive. The evidence based on studies using RTs and the manipulation of cognitive processes is mixed so far (Chen and Krajbich 2018). Some studies find that social motives are more intuitive since prosocial decisions are quicker than selfish decisions and people tend to be more prosocial under time pressure or cognitive load (Cappelen et al. 2016; Cappelletti et al. 2011; Cornelissen et al. 2011; Lotito et al. 2013; Nielsen et al. 2014; Peysakhovich and Rand 2016; Rand et al. 2012; Rubinstein 2007; Schulz et al. 2014), while other studies find that the selfish motive is more intuitive (Duffy and Smith 2014; Lohse et al. 2017; Merkel and Lohse 2019; Piovesan and Wengström 2009; Tinghög et al. 2013; Verkoeijen and Bouwmeester 2014). Unlike dual-process theories, which assume that decisions are the interaction results of intuitive and deliberative processes, sequential sampling models assume that decisions are made by a single comparison process. Specifically, sequential sampling models assume that a noisy relative decision value is integrated at each moment in time and a choice is made when this accumulated decision value crosses a threshold. Sequential sampling models were developed for perceptual decision making (Ratcliff 1978; Ratcliff and Smith 2004) and recently adapted to the analysis of economic and in particular social decisions (Chen and Krajbich 2018; Dickhaut et al. 2013; Frydman and Nave 2016; Hutcherson et al. 2015; Krajbich et al. 2010, 2014, 2015a). These studies indicate that sequential sampling models can capture key patterns of choice, response time, attention, and neural response in the brain for social decision making. Intuitively, we can think of the accumulation process as an accumulation of arguments for the decision. While dual-process theories claim that decisions associated with intuitive processes are quicker than decisions associated with deliberative processes, sequential sampling models argue that the strength of preference which is based on the utility difference between choice options determines RTs, and stronger preference for one option results in shorter RTs. Particularly, the RT of selfish decisions is not significantly different from that of social decisions after controlling for the strength of preference (Krajbich et al. 2015a). On account of the current debate and the conflicting results, we conjecture that both the sequential sampling process and the intuition of selfishness or prosociality may be part of the cognitive processes underlying social decision making. In addition, it is crucial to take heterogeneity into account, both with respect to the preferences as well as in explaining the cognitive process of social decision making. This paper investigates these topics by experimentally analyzing RTs in distribution decisions. It is a well-established fact that people are heterogeneous in the relevant motives and in the strength of preferences (Andreoni and Miller 2002; Engelmann and Strobel 2004; Erlei 2008; Fisman et al. 2007; Kerschbamer 2015). For instance, some people care more about efficiency, while others care more about fairness (Murphy et al. 2011). We explicitly take individual heterogeneity into account, not only with respect to what kind of social motives people care about, but also with respect to the strength of selfishness. Our experiment includes two types of binary three-person dictator games: the third-party (TP) dictator game and the second-party (SP) dictator game. In TP games, the dictator’s payoffs are the same in the two allocations, while in SP games, the dictator’s payoffs differ between the two allocations. Decisions in TP games allow us to identify subjects’ social motives. Based on this identification, we can study how people with different social motives react to various decision situations in SP games using RT analysis. In addition, we can study which kind of motive, selfish or social, can be considered more intuitive when controlling for the utility difference between choice options. We classify subjects into three norm types which differ with respect to the relevant social motives based on the decisions for TP games. Based on this identification, a within-subjects analysis on the SP games shows that RT increases with the number of conflicts between individually relevant motives and decreases with the utility difference between choice options. These results are in line with the predictions of sequential sampling models. A between-subjects analysis reveals the heterogeneity with respect to whether the selfish or the social motive is more intuitive. It turns out that the selfish motive is more intuitive for subjects who are more selfish. Our findings demonstrate that both the sequential sampling process and the intuition of selfishness or prosociality are involved in the cognitive processes underlying social decision making. Our findings also show that it is important to take into account the heterogeneity of the cognitive processes. Our study contributes to the emerging and conflicting literature on the cognitive underpinnings of social decision making. The experiment allows identifying both the heterogeneity in social motives and the strength of selfishness. Based on this identification, we are not only able to provide evidence for sequential sampling models, but we can also show the heterogeneity in whether the selfish or the social motive is related to intuitive processes. This provides a way to reconcile the mixed results on the correlations between RT and prosociality, and also provides an explanation to resolve the debate between the dual-process and a single comparison process underlying social decision making. We argue that conflicts between individually relevant motives, strength of preference, and the intuitiveness or deliberativeness of selfishness all contribute to variations in RT. Thus, it is crucial to take these factors and the heterogeneity of preferences into account when investigating the cognitive processes of social decision making.",7
23.0,2.0,Experimental Economics,19 July 2019,https://link.springer.com/article/10.1007/s10683-019-09619-w,Cooperation in stochastic games: a prisoner’s dilemma experiment,June 2020,Andrew Kloosterman,,,Male,Unknown,Unknown,Male,"One of the most celebrated results of the dynamic games literature is that repeated interaction provides a mechanism for agents to cooperate in the infinitely repeated prisoner’s dilemma. Infinitely repeated games, while a useful abstraction in many respects, are a drastic simplification of the real world. Rather, dynamic environments usually evolve over time.Footnote 1 This feature is captured by stochastic games, a generalization of repeated games in which the game played each period changes and is determined by an underlying stochastic process. This paper investigates cooperation in a laboratory experiment on a stochastic version of the infinitely repeated prisoner’s dilemma. Besides being more realistic for many applications, the reasons to go beyond the simple repeated game to better understand cooperation are two-fold. First, beliefs about future behavior are theoretically critical for supporting cooperation in both repeated and stochastic games, because it is the threat of punishment in the future that deters defection. In the stochastic game, however, the future is more complicated because the game in future periods may be a different game where behavior and payoffs may be quite different. Defection does not depend on beliefs and is thus an easy and safe option, so it is important to understand what role more complexity may play in depressing cooperation levels. Second, the richer stochastic environment allows investigation of new questions that are not possible to address with repeated games. Both environments are essentially coordination games among various cooperative equilibria and the non-cooperative equilibrium defect every period. In the infinitely repeated prisoner’s dilemma, cooperative equilibria allow constant play: cooperation every period over the infinite horizon (the players threaten to defect in future periods after deviations, but these threats are never realized). On the other hand, in the stochastic game, even the most efficient equilibria may require defection in some periods and cooperation in others. This presents two complementary questions: is there cooperation after defection and/or defection after cooperation? Cooperation after defection could be quite difficult to sustain given that defection is consistent with the cooperative equilibria that switch to cooperation, but also with the non-cooperative equilibrium defect every period. So when a player observes defection from their opponent, they will not be able to determine what this opponent intends to do in the future. Alternatively, defection after cooperation may also not emerge if the players have non-standard preferences such as a preference for conditional cooperation (Kartal and Müller 2018), reciprocity (e.g. Falk and Fischbacher 2006) or fairness (e.g. Fehr and Schmidt 1999; Bolton and Ockenfels 2000). The experimental game consists of playing one of two prisoner’s dilemmas, named A and B for convenience, each period over an indefinite horizon \(t=0,1,2,\ldots\). The first treatment variable is which of the two games is played in period 0. The second treatment variable is the probability that A is played in period 1, and takes the values \(p_A=.25\) and \(p_A=.75\). The game realized in period 1 is played in all future periods. This stochastic process implies equilibria with the following features that provide testable hypotheses. First, cooperation is possible in period 0 when B is more likely in future periods (\(p_A=.25\)), and the dilemma played in period 0 (the first treatment variable) is irrelevant. Second, cooperation is possible in periods \(t\ge 1\) when B is realized in period 1, but not when A is realized in period 1. The first observation means that beliefs about the future are all that matters for period 0 cooperation, and the second observation means that there may be cooperation in period 0 followed by defection in period 1 (if \(p_A=.25\) but A is realized in period 1) or defection in period 0 followed by cooperation in period 1 (if \(p_A=.75\) but B is realized is realized in period 1). While these implications are developed in the context of equilibrium, the theory section of this paper also shows that they are robust to risk-dominance based arguments that have been shown to better describe behavior in the infinitely repeated prisoner’s dilemma (Blonski et al. 2011; Dal Bó and Fréchette 2011). The main result for period 0 behavior confirms the theoretical effect of beliefs about the future on cooperation in period 0, although only after subjects gain experience with the game. That is, there is a similar amount of period 0 cooperation in the two treatments where B is more likely in future periods and a similar amount in the two treatments where A is more likely in future periods and the former is greater than the latter. Interestingly, before they become experienced, subjects first focus on the period 0 game and not the future, the exact opposite of the theoretical prediction. Hence the main finding is that the setting is indeed complicated, but learning trends behavior towards the theoretical predictions. The results also show some evidence for shifts from cooperation to defection and from defection to cooperation. There are small shifts from defection to cooperation and vice versa when the theory predicts them. However, the differences are much smaller than the differences between when there was mutual cooperation in period 0 and when there was defection in period 0. This includes always cooperating in period 1 when A is realized after mutual cooperation in period 0, which, as mentioned above, is not consistent with equilibrium behavior. The role for non-standard preferences is then discussed as a possible explanation. It is important to highlight this contribution as equilibrium and the predictions with non-standard preferences are not separately identified in the infinitely repeated prisoner’s dilemma (i.e. cooperation after a cooperative outcome is consistent with both equilibrium and reciprocity in the repeated game). The closest-related experiments are Rojas (2012) and Wilson and Vespa (2018). Both consider stochastic versions of the prisoner’s dilemma where the gains from defection vary each period but beliefs about the future are held constant (the opposite of the present setup). Rojas (2012) finds some evidence for subjects cooperating and defecting over the course of the game, although perhaps it is easier in his environment, because the consequences are immediate rather than through forward looking behavior. Alternatively, Wilson and Vespa (2018) find that cooperation decreases in one dilemma when it can no longer be supported in the other dilemma indicating less evidence for cooperation and defection over the course of the game. Neither paper addresses the issue of varying beliefs about the future that is the main treatment of the present experiment. The differences are not drawbacks of these two experiments in any way. They are focused on different and also very interesting questions. The main results of Rojas (2012) consider imperfect monitoring. The main results of Wilson and Vespa (2018) compare exogenous processes to endogenous processes where the game is determined also by the actions of the players in the previous period. A few other stochastic games have been investigated in the lab (e.g. Charness and Genicot 2009; Ruffle 2013; Cabral et al. 2014; Roy 2014; Vespa 2015; Saijo et al. 2015; Kloosterman 2019). Cooperation in infinitely repeated games, mostly prisoner’s dilemmas, has been studied extensively in the laboratory (see Dal Bó and Fréchette 2018 for a recent survey). One main implication of recent work is that, as noted briefly above, period 0 cooperation is best described by risk-dominance based arguments. This finding, and its relevance here, is discussed thoroughly below. Imperfect monitoring also induces equilibria with cooperation and defection. While there has been extensive experimental work on this topic as well (Holcomb and Nelson 1997; Feinberg and Snyder 2002; Aoyagi and Fréchette 2009; Fudenberg et al. 2012; Rojas 2012), it is difficult to compare results because the non-constant outcomes are driven by imperfect signals rather than the stochastic process. The rest of this paper is organized as follows. Section 2 presents the stochastic prisoner’s dilemma. Section 3 presents the experimental design. Section 4 presents results and Sect. 5 concludes.",3
23.0,2.0,Experimental Economics,12 August 2019,https://link.springer.com/article/10.1007/s10683-019-09620-3,Costly voting: a large-scale real effort experiment,June 2020,Marco Faravelli,Kenan Kalayci,Carlos Pimienta,Male,Male,Male,Male,"We present a large-scale real effort experiment (1200 subjects) testing the turnout predictions of the costly voting model in small (\(N=30\)) and large (\(N=300\)) elections. Costly voting theory dates back to Downs (1957), who formalized elections as a collection of participation decisions in which citizens choose, for purely instrumental reasons, whether to pay a cost to vote for their preferred alternative or to abstain. Since the publication of Downs’ seminal work, this conceptual framework has been a point of discussion among economists and political scientists alike. This is because the model gives rise to the well known paradox of (not) voting, i.e., turnout rate is predicted to quickly decline as the electorate grows and to be negligible in large elections, a feature which is clearly at odds with experience. Downs originally formulated the problem in decision theoretic terms, where the pivot probability is exogenous. Palfrey and Rosenthal (1983, 1985) recast the analysis in a game theoretic framework, but the result does not change. What causes the paradox is that the probability of being pivotal declines exponentially as the electorate increases, becoming rapidly insignificant. Despite this, over the past sixty years scholars have consistently turned to rational choice theory to model voting as a strategic behavior (see Blais 2000, for an overview). At the same time, several ways to overcome the paradox have been proposed by incorporating additional features into the original framework.Footnote 1 The reasons for the enduring interest in this theoretical model are essentially two. Primarily, although voting in elections is not only instrumental, there is ample evidence that strategic considerations certainly play a role (Blais 2000). Moreover, by casting the problem in the familiar framework of costs and expected benefits, costly voting theory generates a series of turnout predictions which, paradox aside, are at least partly supported by factual (see Shachar and Nalebuff 1999) as well as anecdotal evidence. These predictions can be summarized as follows. First, ceteris paribus, individuals with a higher opportunity cost are less likely to vote (cost effect). Second, the larger the electorate size the lower turnout rate is (size effect). Third, the closer the election is ex-ante the higher turnout rate is (competition effect). Finally, the minority turns out to vote at a higher rate than the majority group (underdog effect), although the majority wins the election in expectation. Alas, properly testing the validity of such predictions by analyzing real electoral data is not possible, as a plethora of confounding factors are at play.Footnote 2 A natural alternative is to turn to experiments. There exist few laboratory experiments on voters’ turnout (e.g. Levine and Palfrey 2007).Footnote 3 The suitability of the costly voting model to explain voting behavior in small electorates, such as committees, can be properly tested in the lab. However, as these models aspire to explain voting behavior in considerably larger electorates, we are interested in exploring the turnout predictions of costly voting theory in larger voting groups. Ledyard (1995) already points out the problems associated with testing public good games with large numbers due to the scaling limitations of laboratory experiments. Nonetheless, the validity of the costly voting model in larger electorates, such as public office elections, is more appropriately tested using a large pool of subjects with enough demographic and socioeconomic diversity. Nowadays, this can be implemented using online experiments, but one needs to be careful not to lose control over the experimental environment. We do this through an artefactual field experiment involving a total of 1200 participants recruited via Amazon’s Mechanical Turk (MTurk). MTurk is an internet based marketplace for work where employers (the “requesters”) contract workers (the “providers”) to accomplish a task requiring human intelligence (“HIT”, i.e., human intelligence task).Footnote 4\(^,\)Footnote 5 The rules of the participation game are explained in neutral language and no reference to voting or elections is made at any point. There are two groups, A and B, and each participant is assigned to A with probability \(\gamma\) and to B with \(1-\gamma\). While this probability and the total number of participants are common knowledge, each participant is the only one to know which group she belongs to. Participants are asked whether or not they are willing to complete a real effort task lasting on average 30 min that involves moving a fixed number of sliders on the screen (see Gill and Prowse 2012, for the first experimental use of the slider task). If more members of group A than members of group B complete the task then everyA member receives a bonus payment, while B members get nothing, and vice versa. Thus, performing this task serves as voting and, for every participant, the cost of voting is the opportunity cost of her time. We employ a \(2\times 2\) between subjects design, where we vary the electorate size (30 vs. 300) and the probability of being assigned to either group (close vs. lopsided election). Each treatment is comprised of 300 participants. All treatments were conducted at the same time (see Sect. 2 for more details). Our design has two main features. First, the use of MTurk, which allows us to recruit a large number of subjects and to feasibly manage the experiment. Second, the adoption of a real effort task. While real effort and induced effort cost are both valid alternatives, depending on the experimental setup (see, e.g., Gaechter et al. 2016, for a discussion), we opted for the former for a number of reasons. A real effort task induces a very salient cost and captures the very spirit of the costly voting model, in which the cost of voting is the opportunity cost of time. It also fits naturally with the subject population: MTurk workers are accustomed to performing tasks costing them time, to deciding which HIT to take on among competing ones and how to spend their time to maximize their profitFootnote 6; moreover, running all treatments at the same time guarantees that participants across treatments face the same alternative HIT’s. Finally, it helps avoid the house money effect, which is known to induce greater inequality aversion (Dannenberg et al. 2012), less selfish behavior in public goods (Harrison 2007) as well as overbidding in contests (Sheremeta and Price 2015), all relevant settings when considering that an election, being a group contest, combines elements of contests and of public goods.Footnote 7 Our results can be summarized as follows. First and foremost, we find strong evidence that participants with a higher opportunity cost are significantly less likely to vote. We document this in two ways. We find that voting decreases with income and that participants are 1% less likely to vote for every $10,000 increase in income. Moreover, we find that a one standard deviation increase in predicted time to complete the task is associated with a 3.2% drop in the probability of voting. We find partial evidence of size and competition effect. The size effect is statistically significant in lopsided elections, but not in close ones. Similarly, we observe the competition effect in large elections, but not in small ones. Both effects are strongly significant. Finally, we find no significant evidence of the underdog effect. In fact, we find that, in the large lopsided election, the majority’s turnout rate is higher than the minority’s. Following Morton and Ou (2015), we refer to this as bandwagon effect.Footnote 8 As noted by Morton and Ou, this is a common finding in laboratory experiments on voting behavior (Duffy and Tavits 2008; Grosser and Schram 2010; Kartal 2014; Agranov et al. 2018). In order to interpret our results, Sect. 4 compares observed behavior with numerical computations of the theoretical model. This exercise reveals that turnout is higher than the rational model’s predictions, even when we do observe a significant size or competition effect. Over-participation is a common finding in voting experiments (see Levine and Palfrey 2007) and we do know that individuals typically overestimate their probability of being pivotal (Duffy and Tavits 2008), leading to higher turnout rates. We consider a quantal response equilibrium (QRE) model and it predicts participation rates much closer to the subjects’ turnout levels. However, like the rational model, the QRE model also gives rise to the underdog effect. Thus, quantal response equilibrium alone cannot account for our results. Morton and Ou (2015, p. 229) point out two mechanisms that can be at work, either simultaneously or in isolation: “a voter gains some utility by voting for the winner” and “a voter loses some utility by voting for the loser”. These two effects can be induced by several factors, e.g. people are overly competitive, overly cooperative in social dilemmas and may exhibit other regarding preferences. Whatever the source, these mechanisms can be taken into account in an economic model with a suitable modification of the utility function. We include in the QRE model a small extra utility from voting and winning and show that predictions are qualitatively comparable with our subjects’ behavior. A similar alteration of the utility function, although in a different model, can be found in Callander (2007, 2008). Such a model induces the bandwagon effect in equilibrium in large lopsided elections, as we observe in our experiment. The intuition is that in a larger electorate, the majority is more likely to win in equilibrium and, therefore, the extra utility from voting and winning acquires more importance. Although the observed behavior is certainly complex and cannot be fully explained by the rational model, this paper is the first to present experimental evidence of the predictions of the costly voting framework (i.e., size and competition effect) outside the laboratory and when the electorate is large. Levine and Palfrey (2007) conduct a laboratory experiment, based on Palfrey and Rosenthal (1985)’s theoretical setting, testing the model’s turnout predictions. They consider electorate sizes varying between \(N=3\) and \(N=51\), confirming most of the model’s comparative statics at the aggregate level. There is, nonetheless, little evidence of instrumental voting when N grows beyond 27, as turnout rate only marginally decreases with size when the election is lopsided, but increases when it is close. Interestingly, we do not observe the size effect in close elections either. To the best of our knowledge, the only other attempt to find experimental evidence of instrumental voting in large elections is a recent paper by Morton and Tyran (2012). Using a representative sample of the Danish population, they conduct two online virtual laboratory experiments on small and large elections, 600 being their largest electorate size, focusing on the distinction between selfish and ethical motivations to vote. As in Levine and Palfrey (2007), the cost of voting is monetary. They do find that participants with higher voting costs are less likely to vote. In contrast, they find little evidence of selfish instrumental voting, as turnout rate does not diminish with size nor when elections go from close to lopsided. The remainder of the paper is structured as follows. Section 2 describes the experimental design and hypotheses. Section 3 reports our findings. Section 4 provides a discussion of the results and Sect. 5 concludes.",7
23.0,2.0,Experimental Economics,14 August 2019,https://link.springer.com/article/10.1007/s10683-019-09621-2,Eliciting utility curvature in time preference,June 2020,Stephen L. Cheung,,,Male,Unknown,Unknown,Male,"In both standard and behavioral theories of choice under risk and over time, the value of a risky or temporal prospect is typically modeled as a weighted sum of the utilities of its constituent elements. Thus, in the standard model of risk preference (von Neumann and Morgenstern 1944), the expected utility of a lottery is given by the probability-weighted sum of the utilities of its individual prizes, as evaluated by a Bernoulli utility function. Under expected utility, concavity of the Bernoulli function captures classical risk aversion, giving rise to a preference for more equally-distributed payoffs over states of nature. Analogously, in the standard model of time preference (Samuelson 1937), the discounted utility of a stream of payoffs is given by the (exponentially-) discounted sum of the utilities of its individual payoffs, as evaluated by an instantaneous utility function. Under discounted utility, concavity of instantaneous utility captures resistance to intertemporal substitution, giving rise to a preference to smooth payoffs over time. Leading behavioral alternatives, such as rank-dependent utility and cumulative prospect theory for risk (Quiggin 1982; Tversky and Kahneman 1992), and (quasi-) hyperbolic discounting for time (Laibson 1997; Loewenstein and Prelec 1992), retain this underlying additive structure while relaxing the assumptions of linear probability weighting and exponential discounting, respectively. In principle, risk aversion and intertemporal substitution describe conceptually distinct preferences. Nonetheless, in settings where both risk and time are present it is common—and perhaps even natural—to assume that Bernoulli utility for risk is one and the same as instantaneous utility for time. In standard theory, this gives rise to the model of discounted expected utility, which has been a workhorse model of economics dating back at least to Phelps (1962). Alternatives to the standard model take divergent approaches to the question of whether interchangeability of Bernoulli and instantaneous utilities is maintained. On one hand, the class of recursive preference models developed by Kreps and Porteus (1978) and Epstein and Zin (1989) set out precisely to break the nexus—described by Weil (1990, p. 29) as a “purely mechanical restriction ... devoid of any economic rationale”—between risk aversion and intertemporal substitution. On the other hand, prospect-theoretic models of time-dependent probability weighting (Halevy 2008; Epper et al. 2011; Epper and Fehr-Duda 2012) posit a relationship between probability weighting and hyperbolic discounting, under the assumption that a single function characterizes utility for both risk and time. The question of whether utility under risk is interchangeable with utility over time is also a core issue in the design of experiments to elicit time preference, even though such experiments need not of necessity entail any interaction between risk and time. The primary objective of such studies is usually to estimate the parameters of a discount function. However, since choices are a product of both the utility and discount functions, it is necessary to allow for the possibility of non-linear utility. Unfortunately, until quite recently there were essentially no known methods to elicit the curvature of utility outside the domain of risk. This resulted in the prevalence of two main approaches. First, Coller and Williams (1999) estimate discount rates under the maintained assumption that utility is linear. These estimates are potentially biased if utility is in fact concave (Frederick et al. 2002, pp. 381–382).Footnote 1 Second, Andersen et al. (2008) measure utility by eliciting subjects’ risk preferences, and combine risk and time preference data to jointly estimate a discount function adjusted for the curvature of utility. This assumes that utility under risk also represents utility over time; it is found that adjusting for this degree of curvature results in substantially lower discount rates than when utility is assumed to be linear. The objectives of this paper are twofold. First, I introduce a novel experiment design that allows a clean comparison of the curvature of utility elicited under risk (in the absence of delay) and over time (in the absence of risk). This design builds upon and extends the well-known Holt and Laury (2002, hereinafter HL) procedure for risk preference, by transposing that design from state-payoff space into time-dated payoffs. The HL task is popular in its own right as means of eliciting the curvature of utility under risk, and also forms the basis for the curvature adjustment in the joint estimation approach. Second, I examine the effect upon estimated discount rates of alternative measurements of utility—namely whether utility is assumed to be linear, inferred from risk preferences, or revealed through choices over time. Several related studies have likewise sought to measure the curvature of utility directly from choices over time.Footnote 2 In common with this paper, these studies share the key insight that to identify the curvature of instantaneous utility it is necessary to construct choices involving bundles of time-dated payoffs, as opposed to boundary choices between all-sooner versus all-later payoffs.Footnote 3 These studies find, again in common with this paper, that instantaneous utility is significantly concave yet close to linear. In the following paragraphs, I discuss these studies, and explain how this paper differs from each of them. Abdellaoui et al. (2013) compare the curvature of utilities elicited under risk and over time, however they are not concerned with implications for the estimation of discount rates. For risk, they elicit the certainty equivalent (CE) of a risky prospect that pays x with probability por otherwise y. For time, they elicit the present equivalent (PE) of a temporal prospect that pays x at time kandy today. Thus notice that these two procedures are not exactly comparable. For risk, the CE is an amount paid in both states. This implies, firstly, that the impact of diminishing marginal utility upon the marginal rate of substitution vanishes at the CE (see Eq. 2 in Sect. 2.1), and secondly that the CE lies between x and y. By contrast for time, the PE is an amount paid solely on a single date. The impact of diminishing marginal utility is thus maximized because the difference in payoffs between the two dates is also maximal (as the payoff on the second date, k, is implicitly zero), and the PE may be larger than both x and y. Thus Abdellaoui et al. measure the curvature of utility over different intervals of payoffs for risk and time, and in such a way that diminishing marginal utility has differing effects upon the trade-offs faced in the two domains. The design of the experiment in this paper seeks to avoid these confounds. Andreoni and Sprenger (2012a) and Andreoni et al. (2015) compare estimates of utility curvature and discounting elicited using the Convex Time Budget (CTB) procedure to measures derived using the binary choice methodology of Andersen et al. (2008). The CTB design of Andreoni and Sprenger (2012a) identifies instantaneous utility by allowing subjects to choose any convex combination between an all-sooner and an all-later extreme, while the modified CTB of Andreoni et al. (2015) simplifies this to a multinomial choice. In this environment, the preference to smooth payoffs over time is expressed through the choice of an interior allocation. In fact, when payments on both dates are sent with certainty, choices occur predominantly at the corners of the budget set, indicating that utility is close to linear.Footnote 4 Andreoni and Sprenger (2012a) and Andreoni et al. (2015) compare this finding to that of a binary choice risk task of the type used by Andersen et al. (2008). They find that the latter indicates substantially greater utility curvature, and that the two curvature measures are uncorrelated at an individual level. Andreoni et al. (2015) further show that the risk-elicited curvature measure overstates the preference for interior allocations in the modified CTB. Thus, Andreoni and Sprenger (2012a) and Andreoni et al. (2015) compare utilities for risk and time elicited using different experimental designs (binary choice for risk and CTB for time), with different associated estimation procedures. However, it is well-known that owing to violations of procedure invariance, risk and time preferences may not be stable across elicitation procedures (e.g., Tversky et al. 1990; Loomes and Pogrebna 2014; Freeman et al. 2016). Moreover, the bulk of previous research on time preference uses binary choices, and estimation techniques for such data are well established in both the risk and time preference literatures. Estimation methodology for continuous and multinomial CTB data is less settled (see discussions in Andreoni and Sprenger 2012a; Harrison et al. 2013; Andreoni et al. 2015). Thus, inferences from binary choice data for risk and CTB data for time may differ through any combination of: differences in experimental design, differences in estimation procedures,Footnote 5 or genuine differences in the curvatures of Bernoulli and instantaneous utility. I seek to avoid these confounds by comparing the curvatures of utility for risk and time within a unified design and estimation framework, using binary choices for both. Moreover, my binary choice task for time is derived from a transposition of the standard HL task for risk: rather than varying probabilities (holding payoffs fixed), it is a payment date that varies instead. This ensures that when comparing these results to the risk preference task (or a joint estimation procedure as in Andersen et al. 2008), the estimation apparatus remains unchanged and it is only the source of information on the curvature of utility that differs. The remainder of the paper proceeds as follows. Section 2 first interprets the HL design for risk in a state-preference framework before showing how it can be translated into time-dated payoffs and extended to identify both utility and discounting. The full experiment design consists of a series of choice lists that differ in whether the smaller-sooner option offers a more or less temporally-balanced combination of payoffs. If instantaneous utility is linear, a subject will have the same switch point in all lists, identifying the discount rate. However if utility is concave, this generates a preference for more temporally-balanced payoff bundles, resulting in systematic shifts in switching behavior across lists. Section 3 presents the results. The pattern implied by concave instantaneous utility is indeed observed, and is highly significant, but the magnitude is not large. The curvature of utility estimated from these choices is significantly concave, but less so than utility under risk, with the CRRA coefficient being an order of magnitude smaller. Adjusting for this degree of curvature has only a modest effect upon estimated discount rates compared to assuming linear utility, and a much smaller effect than when utility is inferred from risk preference using joint estimation. At an individual level, the curvatures of Bernoulli and instantaneous utility are uncorrelated. Joint estimates that constrain them to be the same predict time preference choices poorly because they overstate the preference for temporally-balanced payoff bundles. Section 4 concludes.",10
23.0,2.0,Experimental Economics,30 August 2019,https://link.springer.com/article/10.1007/s10683-019-09623-0,Trading while sleepy? Circadian mismatch and mispricing in a global experimental asset market,June 2020,David L. Dickinson,Ananish Chaudhuri,Ryan Greenaway-McGrevy,Male,Unknown,,Mix,,
23.0,2.0,Experimental Economics,11 September 2019,https://link.springer.com/article/10.1007/s10683-019-09624-z,Cortisol meets GARP: the effect of stress on economic rationality,June 2020,E. Cettolin,P. S. Dalton,W. Zhang,Unknown,Unknown,Unknown,Unknown,,
23.0,2.0,Experimental Economics,26 September 2019,https://link.springer.com/article/10.1007/s10683-019-09625-y,The economic effects of Facebook,June 2020,Roberto Mosquera,Mofioluwasademi Odunowo,Ragan Petrie,Male,Unknown,Unknown,Male,"Social media usage has increased dramatically over the past decade, and Facebook has dominated the market. Almost 2.2 billion individuals worldwide have an active Facebook account, and nearly 1.4 billion log on daily (Facebook 2017) for an average of 50 minutes per day (Facebook 2016). Facebook not only provides means to connect with friends and build social networks and capital (Bailey et al. 2018; Mayer and Puller 2008; Cramer and Inkster 2017), but it is also exposes users to a vast amount of information and news. Despite the potential influence of Facebook on an individual’s behavior via information and content provision, there is surprisingly little known about its direct and comprehensive effects on news exposure and awareness, subjective well-being and day-to-day activities. Facebook’s platform has several characteristics that lend well to investigating its effects on an individual’s exposure to news content as well as its impact on well-being. The platform consolidates information from many sources, making it an important and compelling place to go on the internet to keep up with news. People tap into Facebook for local, national and international news. Indeed, roughly two-thirds of Americans get at least some of their news from social media sources (Gottfried and Shearer 2016). While there is a concern that news transmitted through social media could be fake or skewed and affect political outcomes (Allcott and Gentzkow 2017), these type of platforms could also serve to uncover corruption (Enikolopov et al. 2016). As individuals rely more on social media and news aggregators as a primary source of information, segregation may increase (Gentzkow and Shapiro 2011) and voting behavior can be affected (DellaVigna and Kaplan 2007; Bond et al. 2012; Martin and Yurukoglu 2017). The consequences of this in terms of news awareness and biases—highlighted by political investigations regarding Facebook’s involvement in the 2016 U.S. Presidential Election—are largely unknown. More broadly, there is little consensus on Facebook’s impact on well-being, especially in the context of daily behaviors and activities. Facebook is often used to connect with friends and family, organize events and share information and photos (Laroche et al. 2012; De Vries et al. 2012; Ashley and Tuten 2015; Lee and Ma 2012; Bailey et al. 2017). Being able to seamlessly keep in touch with others might improve mood and happiness, but it might also induce negative emotions and habits from social comparison (Tromholt 2016; Deters and Mehl 2013). How Facebook directly affects well-being and mood in general and the correlation with daily activities is unclear. Facebook’s platform is provided for free to users and paid for by advertising, so the monetary value to users, as reflected in a market price, is untested. The platform facilitates building social networks and seamless access to relevant information. Usage rates, both in frequency and intensity, suggest this provides benefits to users. While the economic impact of Facebook on advertising has been estimated, the benefits to users and impact on behavior have been given more limited study.Footnote 1 Knowing the value of Facebook would inform an understanding of welfare effects and provide a monetary measure of the importance of Facebook to users. We ran a field experiment in the Spring of 2017 with a randomized, and validated, Facebook restriction to investigate how Facebook may affect daily activities and news exposure and quantify how much users value access. In total, 1769 individuals from a large U.S. university participated in the study. Using an incentive-compatible procedure (Becker et al. 1964), we asked participants how much they would need to be paid to not use Facebook for one week. Qualified participants were then randomly assigned to either a one-week Facebook restriction group or a control group that faced no restriction. Our design has several important and unique features worth noting. First, we can exploit the rich data collected on the distribution of Facebook’s value to check for possible selection effects in our results. Second, we enforced and validated the restriction by logging participants off Facebook and verified treatment compliance using an unobtrusive online monitoring procedure throughout the week. Our procedure was undetectable to the participant and did not involve direct contact which could potentially impact behavior. Finally, participants completed two surveys, the first prior to random assignment and a second survey one week later. These surveys were designed to provide a comprehensive view of behavior and measure the short-term effects of Facebook on news awareness and consumption, well-being, daily time allocation and daily activities. We have several key results. First, our study reveals that one week of Facebook is worth about $67 to users, with a median value of $40. This value is in line with other studies (Brynjolfsson et al. 2018; Corrigan et al. 2018; Allcott et al. 2019; Sunstein 2019; Herzog 2018) and represents a significant portion of a typical university student’s weekly budget and expenses [roughly 30% according to Flood et al. (2017)].Footnote 2 Individuals place a nontrivial value on Facebook usage, and the value increases 19.6% after not being able to use it for one week. This is consistent with addiction or the compounding loss of information, however, we note this is only suggestive as we are underpowered to detect a statistically significant effect. Second, our data document that Facebook is an important source of news exposure. Individuals restricted from Facebook are less aware of politically-skewed sources, and this is stronger for men than women. Consistent with this result, the Facebook restriction reduces news consumption and participants do not substitute towards other news sources or social media platforms when being off Facebook for a short period of time. There is no effect on news awareness from mainstream sources. The causal estimates show that Facebook is an important conduit for news from non-mainstream outlets, and this echoes the findings of Allcott and Gentzkow (2017) who show that social media is correlated with the distribution of “fake news.” Our results provide additional evidence that Facebook plays an important role in the acquisition of information by affecting what news is available to consume and thus an individual’s ability to assess its veracity. Third, our findings contribute to the literature that focuses on Facebook’s effect on happiness and well-being. Early studies found mostly positive effects of social media on subjective well-being, perhaps through enhanced engagement, in cross-sectional studies (Ellison et al. 2007; Valenzuela et al. 2009; Gonzales and Hancock 2011; Kim and Lee 2011) and laboratory experiments (Sagioglou and Greitemeyer 2014; Vogel et al. 2015; Verduyn et al. 2015). More recent studies have found mixed results using panel data (Shakya and Christakis 2017) and Facebook use limitations (Tromholt 2016).Footnote 3 Cross-sectional evidence on the effect of Facebook on depression is mixed. Feinstein et al. (2013) finds depressive feelings are driven by negative outcomes from social comparison, but other studies find no relationship between Facebook and depression (Steers et al. 2014; Jelenchick et al. 2013; Tandoc et al. 2015). We contribute to this literature by using a randomized and verified Facebook restriction and show no significant effect of using Facebook on overall life satisfaction. However, we do find a large short-term reduction in feelings of depression when restricted from Facebook, especially for men. Finally, we build on existing research by studying the effect of Facebook on behaviors largely found to be correlated with mood. We find suggestive evidence that individuals restricted from using Facebook engage in healthier activities. While our design does not allows us to recover the underlying mechanism, this finding is consistent with research in psychology (Salovey et al. 2000; Ostir et al. 2000; Fredrickson and Joiner 2002; Blake et al. 2009; Kettunen 2015; Newman et al. 2014; Sonnentag 2001) that better mood is positively correlated with engagement in healthier behaviors. Overall, the effects our study finds on news awareness, news consumption, feelings of depression and daily activities show that Facebook has significant effects on important aspects of life not directly related to building and supporting social networks. Furthermore, almost two years after our experiment, Allcott et al. (2019) find similar results for news awareness and subjective well-being for a different population, which supports our findings. The effects of Facebook are far reaching, and our results provide a more comprehensive documentation of these impacts on daily life. Users seem to understand this and place a substantial value on the experience that Facebook provides. The paper is organized as follows. Section 2 describes the study design and implementation. Section 3 reports results on the value of Facebook to users and the effect of the Facebook restriction on news awareness, subjective well-being and activities. Section 4 continues with robustness checks on our main findings. Section 5 concludes.",38
23.0,3.0,Experimental Economics,08 June 2020,https://link.springer.com/article/10.1007/s10683-020-09662-y,Symposia on behavioral economics of the COVID-19 pandemic,September 2020,,,,Unknown,Unknown,Unknown,Unknown,,
23.0,3.0,Experimental Economics,16 December 2019,https://link.springer.com/article/10.1007/s10683-019-09636-9,Strategically delusional,September 2020,Alice Soldà,Changxia Ke,William von Hippel,Female,Unknown,Male,Mix,,
23.0,3.0,Experimental Economics,04 October 2019,https://link.springer.com/article/10.1007/s10683-019-09626-x,Fairness considerations in joint venture formation,September 2020,Tanjim Hossain,Elizabeth Lyons,Aloysius Siow,Unknown,Female,Male,Mix,,
23.0,3.0,Experimental Economics,19 October 2019,https://link.springer.com/article/10.1007/s10683-019-09627-w,Silence is golden: team problem solving and communication costs,September 2020,Gary Charness,David J. Cooper,Zachary Grossman,Male,Male,Male,Male,"Suppose you are a manager facing a difficult problem. You can try to find a solution yourself or you can put together a team to help you. It would seem obvious that you would do better with the help of a team. While there are costs associated with having a team (salaries and opportunity costs), you gain the benefits of others’ insights. Interactions among teammates might even lead to synergies and further insights that would not occur to individuals, since diverse ideas can be complementary and build upon each other. Indeed, there is a great deal of research that supports the notion that teams are better than individuals at solving intellective problems. However, anyone who has ever worked on a group project may be less convinced about the effectiveness of teams or the value of communication from co-workers. Not all shared insights are good insights, and it takes time to separate good ideas from bad ones. We conduct experiments exploring whether the benefit of having a team for solving intellective problems varies with the type of problem and the cost of communication within a team. Specifically, we ask participants to solve logic problems and vary three elements between treatments: whether participants work alone or in a team of four, whether or not communication within a team is costly, and the type of logic problem. All treatments feature an initial stage where subjects, working alone, solve a series of relatively easy logic problems for low stakes. Stage 1 familiarizes subjects with the problems and provides a natural measure of ability. The second stage features more difficult problems and higher stakes. Subjects either work alone or in teams. When working in teams one subject is the “leader,” responsible for solving the puzzle, while the other three teammates are “followers” who assist the leader. Followers have the same information as leaders, and receive the same payoff as the leader. Team members can exchange chat messages, enabling followers to assist leaders, but only the leader can make a decision. Some team sessions have no cost for sending a message, while in others we impose a tiny cost for each message sent. While having a team potentially helps the leader solve logic problems, having a team can also have a negative effect on performance if the leader is led off-course by incorrect suggestions or slowed down by having to wade through large numbers of messages. It seems obvious that message costs ought to harm team performance by reducing helpful suggestions, but perhaps there are positive effects if the frequency of incorrect suggestions or distracting messages decline even more. The preceding observations lead to our central questions: Is having a team beneficial? Are teams more or less helpful when communication is costly? Our participants face two very different types of logic problems, either solving nonograms, a type of grid-based puzzle related to Sudoku, or answering questions similar to those in the Cognitive Reflection Test (Frederick 2005). Solving a nonogram requires completing a series of incremental steps rather than having a single central insight. This contrasts with our second type of logic problem, henceforth referred to as “CRT questions,” in which solving the problem hinges on having a single central insight. Because the tasks are so different in nature, the way in which followers’ suggestions to a leader affect a team’s success may also differ between the two tasks. Indeed, our evidence suggests that the two types of problems put differing weights on correct and incorrect suggestions. This leads to our final research question: Do the effects of the other treatments (individuals vs. teams; cost of communication) vary by the type of logic problem being solved? Our experiments provide clear but unexpected answers to these questions. For both types of logic problems, the increase in performance for teams without message costs is small relative to individuals working alone: the solution rate increases by four percentage points for nonograms and five percentage points for CRT questions. Performance by individuals and teams without message costs is statistically indistinguishable in either case. This contrasts strongly with the received wisdom from previous studies. For example, Shaw (1932), Cooper and Kagel (2005), and Charness et al. (2010) all compare the performance of individuals and teams solving logic problems with a “eureka” solution, similar to the CRT questions. In all three cases the improvement with teams is large and statistically significant. In our experiment, the improvement from individuals to teams without message costs is small and neither economically nor statistically significant. Teams with costly messages perform significantly better than individuals for both types of logic problems. This does not imply that teams with message costs do exceptionally well, since the increase in performance relative to individuals is still small in magnitude relative to what other studies report (eight percentage points for nonograms and 11 percentage points for CRT questions). To the best of our knowledge, our study is the first to find that imposing friction on communication leads to more effective performance in teams. To better understand the positive effect of message costs, we analyze the quantity and quality of messages sent by teams. Even though the cost of a message is minimal, imposing message costs reduces the volume of messages by more than half in both treatments. Critically, this effect is especially large for incorrect messages. As a simple way of seeing this point, the probability that a leader receives at least one correct suggestion falls for teams with message costs solving nonograms (97 vs. 82%), but this is less than half of the decrease for incorrect suggestions (72 vs. 40%). The difference is even larger for the CRT questions with message costs, with the probability that the leader receives a correct suggestion barely affected by message costs (46 vs. 45%) while the probability of receiving an incorrect suggestion falls dramatically (83 vs. 48%). We show that the relationship between message content and performance varies across the two types of logic problems, as expected, but the common thread is that message costs always improve the types of messages sent. We make this point more precisely by developing a measure of the overall quality of communication, and showing that the overall quality of communication is significantly higher with message costs for both sorts of logic problems. The positive effects of message costs are caused by these increases in the quality of communication, despite the drop in message quantity. Our findings suggest that employing a team to solve problems may not be valuable, especially once the costs are considered. To illustrate this point, we examine whether it is better to delegate the problems to the ablest available individual or to form a team.Footnote 1 Specifically, for each team we select the member who performed best in Stage 1, and then use their Stage 1 performance to predict Stage 2 performance. The predicted solution rate for nonograms is 24 percentage points higher than the solution rate in the individual treatment, a far larger increase than achieved by teams with or without message costs. The results are weaker for the CRT questions, largely because Stage 2 performance is harder to predict, but delegating to the ablest individual still does slightly better than teams with message costs (12 percentage-point increase relative to individuals) and would only involve paying one individual. These results imply that a manager facing a difficult problem should hire someone who is good at solving similar problems and delegating the task rather than hiring a team of helpers to work together. To summarize, our experiments yield clear but somewhat unexpected conclusions. Contrary to most of the existing literature, we find little evidence that freely interacting teams out-perform individuals at solving intellective problems. This is true for two very different types of logic problems, and reflects the negative effects of incorrect suggestions largely cancelling out the positive effects of correct suggestions. Matters improve when message costs are imposed, largely due to increased quality of communication, but the positive effect of teams remains small from an economic point of view. We find little evidence that using teams to solve intellective problems is worthwhile once the costs are considered. More generally, it is easy to feel bombarded by “free” information in our society. Our results sound a cautionary note regarding the notion that cheap-talk messages are always highly effective or at least harmless. There may be value in restricting the flow of information; for example, social welfare might be improved by imposing a very small cost on e-mail messages. The remainder of the paper is organized as follows. Section 2 is a literature review, and we present our experimental design and implementation in Sect. 3. Hypotheses are put forward in Sect. 4, and results and analysis follow in Sect. 5. We offer some discussion in Sect. 6, and Sect. 7 concludes.",2
23.0,3.0,Experimental Economics,07 November 2019,https://link.springer.com/article/10.1007/s10683-019-09628-9,Preference discovery,September 2020,Jason Delaney,Sarah Jacobson,Thorsten Moenig,Male,Female,Male,Mix,,
23.0,3.0,Experimental Economics,01 November 2019,https://link.springer.com/article/10.1007/s10683-019-09629-8,The choice of institutions to solve cooperation problems: a survey of experimental research,September 2020,Astrid Dannenberg,Carlo Gallier,,Female,Male,Unknown,Mix,,
23.0,3.0,Experimental Economics,19 November 2019,https://link.springer.com/article/10.1007/s10683-019-09630-1,Escalation in conflict games: on beliefs and selection,September 2020,Kai A. Konrad,Florian Morath,,Male,Male,Unknown,Male,"Substantial research has been devoted to the study of conflict, described as an adversarial interaction between players who expend efforts and try to achieve mutually exclusive goals.Footnote 1 We study dynamic properties of such conflict if the contestants take part in a sequence of pairwise contests, have incomplete information about their opponent’s type and learn from the interactions about the composition of the population of potential future opponents. Extensive experimental work on conflict confronts theory predictions with subjects’ behavior in the laboratory and has uncovered systematic behavioral departures from the complete information benchmark models. One main interpretation of the findings is that the individuals who interact in conflict games—in the laboratory and elsewhere—follow motives beyond the maximization of monetary payoffs and that these motives are not uniform across individuals. Whereas monetary incentives are typically common knowledge in experimental setups, these other (intrinsic) motivations are not; this turns the interaction into a game with incomplete information. Individuals may know their own preferences but have to form beliefs about the types of opponents they interact with. If players interact repeatedly with other players from the same population (experimental session), they may learn about other players’ types, update their beliefs about the type of future opponents and adjust their behavior accordingly, which may lead to an escalation or de-escalation of contest effort. The importance of unobserved heterogeneity in individual motivations for behavior in strategic interactions such as conflict games has three main implications, which establish the research agenda of this paper. First, a suitable benchmark model of conflict games needs to incorporate incomplete information about individuals’ ‘preference types’ such as their intrinsic motivation to win. To allow for learning about the population of opponents, the model needs to be dynamic. Second, observed adjustments of behavior in the experimental data should be contrasted with the equilibrium prediction of the dynamic game, which can be structurally different from the complete information benchmark. Third, individual heterogeneity and learning can cause self-selection if the likelihood of future interactions is not fully exogenous such as in models of dynamic conflict that consist of several “battles”. Our framework adds a simple dynamic structure to a generic model of distributional conflict.Footnote 2 In the multi-stage game considered, each contest stage takes the form of a standard two-player Tullock contest, with the only modification that the prize is awarded with some (exogenous) probability only. Otherwise the game moves to the next contest stage, the players are re-matched in pairs and choose new efforts.Footnote 3 The conflict game is set up such that effects of belief updating and self-selection based on unobserved preference heterogeneity can be isolated in the experimental data. Ignoring preference heterogeneity, a standard theory with complete information (without population uncertainty) would predict behavior to be identical across all stages of conflict. This is mainly due to the assumption of an exogenous continuation probability which, hence, plays a key role for the identification strategy by removing the strategic links between the stages, except possibly for belief updating.Footnote 4 Unobserved heterogeneity in characteristics such as the intrinsic motivation to win and uncertainty about the composition of the population of opponents introduce dynamics of conflict behavior caused by belief formation and updating. In line with a theory of social projection, we show that a player’s own preference type partially shapes her beliefs about other participants.Footnote 5 The different player types start with different beliefs about the population of opponents. Learning about the opponents’ propensity to exert effort has different strategic implications for players who are strongly or weakly intrinsically motivated. Intuitively, learning that a majority of opponents have a weak intrinsic motivation (choose low effort) reduces the incentive to exert effort for strongly motivated participants, as they can ensure a high win probability at lower costs. However, the same signal about the opponents increases the incentive to exert effort for weakly motivated participants, similar to an encouragement effect. This logic is closely related to strategic considerations in other contest applications and is a consequence of the usual non-monotonicity of best reply functions. In the closely corresponding baseline experimental treatment (BASE), a monetary prize is awarded based on a Tullock function with probability 1/3 in a given stage, which would end the game. In each stage reached, the players are randomly matched in pairs and, apart from choosing contest expenditures, have to state their beliefs about the opponent’s effort. If the prize has not been awarded after 5 stages, the game ends without prize allocation. We find considerable evidence for nonincentivized heterogeneity among players which—together with the implied updating of beliefs in later stages—also explains dynamic adjustments of contest efforts across the contest stages. Unobserved preference heterogeneity and belief formation about potential opponents are seemingly a driver of the observed effort escalation and de-escalation; not allowing for this type of information asymmetries would yield theory predictions that are structurally different from the experimental findings. As a main experimental variation (the EXIT treatment), we consider a variant of the game that allows for self-selection—an aspect which is crucial in most dynamic conflict games where participation at later stages is not fully exogenous.Footnote 6 To emphasize the implications of self-selection in the presence of unobserved preference heterogeneity, we extend the multi-stage contest by an explicit continuation decision after the first contest encounter (to be made in case the prize has not yet been awarded). The outside option is chosen such that equilibrium behavior based on monetary payoff maximization does not change, that is, it is lower than the equilibrium expected continuation payoff. The experimental results, however, confirm our theory prediction that unobserved preference heterogeneity and updating about the population of future opponents can cause self-selection of certain types into continuing conflict and result in an escalation of efforts in later contest stages. Our paper is the first to develop and test a theory of sequential contests in which conflict behavior may be driven by intrinsic (behavioral) motives, in which players cannot observe the intensity of the motives of their competitors, and in which they are uncertain about the environment described by the distribution of types of possible competitors. While the dynamics of conflict caused by population uncertainty and belief updating have not been studied, some elements of this theory relate to results that have been developed in the theory of all-pay contests with incomplete information in a purely static context. Malueg and Yates (2004) analyze the static contest between two players whose prize valuations are drawn from a commonly known binary distribution. Even though their information assumption differs from the one that emerges from social projection in our dynamic framework, their results are structurally similar to stage 1 of our game. Fey (2008), Ryvkin (2010), Wasser (2013a, b) and Einy et al. (2015) study existence of Bayesian equilibrium in the static incomplete information Tullock contest.Footnote 7 The results by Einy et al. (2015) are closest to our existence results, as they allow players to have private information about the state of nature. Broad empirical evidence on conflict behavior suggests an intrinsic motivation to win, leading to a mismatch between a complete information benchmark and the experimental results. Moreover, contest efforts often exhibit dynamics that do not square with the standard theory intuition.Footnote 8 Other findings suggest that self-selection based on unobserved heterogeneity of contestants may explain deviations from the complete information benchmark model. In line with this, Fu et al. (2013) show that players sometimes engage in costly messages prior to a lottery contest and explore the role of incomplete information as the rationale for this behavior. Herbst (2016) finds selection effects which she explains by players’ differences in a ‘joy of winning’. Herbst et al. (2015) consider unobserved behavioral heterogeneity of players in the context of free-riding in fighting alliances and the endogenous versus exogenous formation of such alliances. They also find that players make inference from past actions of their co-players, and weak players exploit strong players if both types enter into the same fighting alliance. Strong players understand this and tend to self-select: rather than joining the fighting alliance with a player who is likely to be weakly motivated, they prefer to become stand-alone fighters. In our paper, random re-matching after each interaction avoids that players can make inference on the behavioral type of their specific co-player. However, the players learn about the nature of the overall population. This population learning turns out to be sufficient for an adjustment of their behavior and for whether to continue to participate in the conflict game or quit. Another dimension of learning dynamics in experimental contests concerns the extent to which feedback is provided, with mixed evidence so far. In a setting with fixed matching of participants, Fallucchi et al. (2013) find that information about the opponent’s choice has opposite effects on effort levels in probabilistic and deterministic contests. Mago et al. (2016) consider four-player contests with fixed matching and find no effect of information about others’ effort on average efforts but dynamic adjustments of efforts which reduce effort heterogeneity (the latter is in line with our theory; the former may arise from the predicted countervailing adjustments of different player types). Keeping the set of choices observed at each stage constant and eliminating learning about the specific opponents and hence strategic signaling by design, our approach addresses the idea that different types of players may hold systematically different beliefs, which can lead to different adjustments of beliefs and efforts when learning about the population of potential opponents. Our paper also relates to a methodological discussion of the benchmark choice in laboratory experiments. If players understand that their co-players do not play the money-guided Nash equilibrium action, this should trigger a different optimal reply, even for strictly money-oriented players. Fudenberg and Levine (1997) find evidence in experimental contexts that actual co-players’ behavior may induce learning and may cause players to optimize against this observed behavior. Konrad et al. (2014) report similar findings in the context of monopoly pricing and consumer boycott. Camerer and Weigelt (1988) study experimental behavior in a finite lending game with reputation building. In their context, players have incomplete information about other players’ monetary incentives by experimental design of the game. Our approach combines elements of these approaches. We do not induce heterogeneity in incentives or incomplete information about these incentives. We rather draw on experimental evidence that finds players’ heterogeneity along an important (‘behavioral’) dimension and acknowledge that subjects have incomplete information about the non-monetary payoff components of their opponents.Footnote 9 The role of population uncertainty, self-projection and Bayesian learning may be important in various contexts beyond contest applications. Ample evidence has shown that many players who interact in a laboratory environment have motives in addition to the extrinsic monetary incentives provided.Footnote 10 Since players cannot really know the distribution of types in a subject pool when taking part in an experiment, a well-reasoned choice requires players who enter a laboratory session to form a belief about the composition of the population of subjects from which the co-players are drawn. It may be appealing early on to make Bayesian inference from one’s own type and update this belief from interaction to interaction. With an increasing number of observations of others’ choices the importance of a player’s own type for the beliefs about the opponents’ types may fade.",4
23.0,3.0,Experimental Economics,09 November 2019,https://link.springer.com/article/10.1007/s10683-019-09631-0,Aggregation mechanisms for crowd predictions,September 2020,Stefan Palan,Jürgen Huber,Larissa Senninger,Male,Male,Female,Mix,,
23.0,3.0,Experimental Economics,25 November 2019,https://link.springer.com/article/10.1007/s10683-019-09632-z,Concentration and variability of forecasts in artificial investment games: an online experiment on WeChat,September 2020,Xiu Chen,Fuhai Hong,Xiaojian Zhao,,Unknown,Unknown,Mix,,
23.0,3.0,Experimental Economics,16 November 2019,https://link.springer.com/article/10.1007/s10683-019-09633-y,The benefit of the doubt: willful ignorance and altruistic punishment,September 2020,Robert Stüber,,,Male,Unknown,Unknown,Male,"A large and influential strand of literature shows that individuals are willing to punish other individuals if they violate social norms, even if the punishment comes at a monetary cost and yields no material gain (e.g., Fehr and Gächter 2000; Fehr and Fischbacher 2004; Carpenter 2007; Carpenter and Matthews 2012). Some of these studies show that this altruistic punishment of norm violations is even conducted by third parties, whose own economic payoff is unaffected by the norm violation. The willingness to altruistically punish norm violations has been suggested as being one major enforcement mechanism of social norms. In turn, social norms that are enforced by social sanctions are seen as a key driver of cooperation between strangers, individuals’ willingness to be generous, and the existence of human societies more generally.Footnote 1 Social preferences are thought to be the reason for third-party punishment: Unaffected third parties punish subjects who violate norms, although it is costly and they receive no material benefit from it. They do so, presumably, because they expect a benefit for others.Footnote 2 However, more recent studies emphasize that people willfully ignore information and in turn exploit ambiguities about the consequences of their actions. In a seminal paper by Dana et al. (2007) and a plethora of follow-up studies (Larson and Capra 2009; Cain and Dana 2012; Grossman 2014; Feiler 2014; van der Weele 2014; Grossman and van der Weele 2017; Moradi and Nesterov 2017), it is shown that dictator game-giving declines when subjects can choose not to reveal how their actions affect a passive recipient’s payoff.Footnote 3 It is an open question whether people’s tendency to remain ignorant in order to avoid costly moral behavior might also lower their willingness to altruistically punish norm violations. At the same time, one can think of many real-world scenarios in which willful ignorance might impact altruistic punishment. A university professor supervising an exam might prefer to look away rather than check carefully whether a student brought a forbidden cheat sheet, knowing that if she found the cheat sheet she would need to engage in a nerve-wracking discussion with the cheating student and exclude him from writing the exam. A restaurant manager who suspects that one of his waitresses is not sharing her tips with her colleagues as agreed upon, might be reluctant to check, because finding out that the waitress was not sharing her tips would imply the need to confront her, which would have detrimental effects on the working atmosphere and, ultimately, his profits. And, when a firm asks to be paid for its services without providing an invoice, people might refrain from asking for the invoice and from checking whether it contains the total amount—even though they would find it unfair if the firm were to evade taxes—because asking might increase the amount one has to pay oneself. To study the effect of willful ignorance on third-party punishment, I run a laboratory experiment in which I modify the dictator game with third-party punishment (“third-party punishment game”; Fehr and Fischbacher 2004). The game consists of two stages, each played by a group of three players: a dictator, a recipient, and a third party. In the first stage, the dictator decides between a selfish option, giving him a high payoff and the recipient a low payoff, or a fair option that gives a lower payoff to the dictator but a higher payoff to the recipient. In the second stage, the third party whose payoff is unaffected by the dictator’s choice, has the opportunity to punish the dictator. I vary whether the third party always observes the dictator’s choice prior to making its decision (baseline treatment) or can choose to reveal it at no cost (hidden information treatment). I find that a substantial fraction (36%) of third parties avoid learning the choice of the dictator in the hidden information treatment. These third parties act as if the dictator has chosen the fair option. Dictators who choose the fair option are almost never punished. As a result, the fraction of subjects choosing to altruistically punish a selfish dictator is significantly lower when the information about the dictator’s behavior is initially unobserved compared to when it is exogenously provided: The frequency of altruistic punishment decreases by 50%. Hence, the possibility to avoid information diminishes third-party punishment. Surprisingly, although this drastic decrease in altruistic punishment significantly alters dictators’ payoffs and their payoff-maximizing choice, dictators do not choose the selfish option more often. There is no treatment difference in dictator choices. In a second step, I investigate the social norms related to third-party punishment using the incentivized norm elicitation method proposed by Krupka and Weber (2013) in a separate experiment. Eliciting social norms is important for two reasons. Firstly, the social norms related to third-party punishment under full information, i.e., without moral wiggle room, have not been investigated in the literature before. The most important question in this context is whether punishing a selfish dictator is seen as being prosocial. The results make clear that punishment is indeed seen as the moral action. Secondly, I provide evidence regarding the social norms that prevail in a situation of initial ignorance. I show that the norm prescribes revealing the information about the dictator’s choice. Hence, ignorance is not appropriate. I then explore two ways in which the social norms might still be in line with the choices observed. I do not find that it is more or less appropriate to punish a norm violation depending on whether the information about the norm violation was revealed or exogenously given. Contrarily, if a subject chooses to remain ignorant of the dictator’s choice, then the social norm prescribes not punishing the dictator. And, little can be gained in terms of appropriateness by revealing the norm violation and punishing. This finding deviates from the result for the dictator game with hidden information about the recipient’s payoff and indicates why the possibility to remain ignorant might have a particularly strong impact on altruistic punishment. In a third step, I explore why some subjects remain ignorant in order to avoid altruistic punishment. I follow two approaches. I first investigate whether the choices observed in the experiment can be predicted on an aggregate level based on monetary payoffs and the measured social norms (see Fig. 1). A third party who reveals the information might observe a fair dictator, in which case both monetary and normative incentives are aligned, or a selfish dictator, in which case she faces a trade-off between punishing the dictator (which is costly, but appropriate) or not (which saves on income, but is inappropriate). In contrast, for a third party who remains ignorant, not punishing is income-maximizing and not socially inappropriate. Depending on how strongly a third party weighs monetary incentives compared to adhering to the social norm, she hence might choose to remain ignorant and to not punish. Doing a similar exercise for all possible strategies of the third party, I find that monetary incentives and social norms can well predict the distribution of choices. I then allow for observable heterogeneity between subjects and examine whether different choices can be explained by the subjects being of varying social types. Considering a measure of prosociality, I find that the third parties who reveal a selfish dictator choice and choose to punish are the most prosocial. However, if I analyze punishment choices depending on whether third parties selected into receiving information on the norm violation or were exogenously informed of it, and if I consider a measure of self-image, I do not find evidence of a sorting of types. Predicting Behavior Based on Monetary Payoffs and Norms. Note: Figure 1 shows a stylized version of the game tree in the hidden information treatment including the income implied by and social appropriateness of the third party’s actions In a final step, I revisit the finding that dictator behavior does not vary across treatments. Whether dictator behavior can be affected by the treatment variation is crucial, because no treatment difference in dictator behavior implies a null-effect on norm compliance. In two additional treatments, the baseline informed treatment and the hidden information informed treatment, dictators are informed about the proportion of dictators that were punished conditional on their choice (selfish or fair) in the baseline and the hidden information treatment, respectively. I find that when dictators have perfect information on the consequences of willful ignorance for punishment, dictators hold adjusted beliefs and behave significantly more selfishly when their choice is initially hidden. The first contribution of this study is to show that third-party punishment significantly decreases if the third parties have the possibility to remain ignorant of a potential norm violation. This has important implications for future research and policy. Third-party punishment may not be as effective in enforcing social norms as previously thought, given that in a richer design that allows for avoiding information but leaves everything else unchanged, I no longer find it to be very common. I provide evidence on the social appropriateness of altruistically punishing norm violations supporting the narrative about a socially appropriate punishment, which has been used, but not analyzed, in the literature on third-party punishment so far. I also provide evidence on the social norms that prevail in a situation in which there is initial uncertainty over whether a norm violation has taken place. Based on this, I explain why the possibility to remain ignorant reduces altruistic punishment by explaining choices based on norms and monetary incentives. Moreover, I analyze whether behavior under initial ignorance can be explained by individual characteristics (such as a subject’s prosociality), which also allows testing theoretical predictions. Finally, I provide evidence showing that whether willful ignorance is likely to affect norm compliance depends on the salience of its effect to the dictators. The remainder of this paper is structured as follows. In Sect. 2, I briefly discuss the related literature. In Sect. 3, I describe the main experiment and its results. Section 4 analyzes the social norms in punishment behavior. Section 5 is dedicated to explaining choices. In Sect. 6 the effect of providing information about punishment rates on norm compliance is studied. Section 7 concludes.",6
23.0,3.0,Experimental Economics,12 February 2020,https://link.springer.com/article/10.1007/s10683-020-09646-y,Would depositors pay to show that they do not withdraw? Theory and experiment,September 2020,Markus Kinateder,Hubert János Kiss,Ágnes Pintér,Male,Male,Female,Mix,,
23.0,3.0,Experimental Economics,23 November 2019,https://link.springer.com/article/10.1007/s10683-019-09635-w,Room composition effects on risk taking by gender,September 2020,Marco Castillo,Greg Leo,Ragan Petrie,Male,Male,Unknown,Male,"Our decisions and behavior can be strongly influenced by who is in our presence. Choices and actions may be different in groups of the same gender than in those of mixed gender. But, can we be influenced by the group gender composition, even if others have no bearing on our choices either now or in the past, and decisions are made in absolute privacy? Understanding whether such effects exist is important because they imply that the mere presence of certain others can affect the choices of individuals. The existence of these effects means that the gender composition of teams or even the design of the built environment, which dictates who one is surrounded by, could impact behavior. Despite its relevance to understanding decision making, there is no direct evidence showing that who is in the room affects behavior, absent strategic interaction, information transfer, or payoff relevance of others’ decisions. We address this by randomly varying the gender composition of the group present when decisions are made in an economic experiment. Importantly, the presence and behavior of those individuals provide no information and are not strategically or payoff relevant. The results are striking. We find that the gender composition of the room alters individual behavior. Women are more risk taking as the proportion of men in the room increases, whereas men are unaffected. We explore several potential mechanisms for this change and conjecture it is driven by conformity and awareness of social context. In our research design, individuals are invited to participate in a laboratory experiment on decision making and are randomly assigned to a particular date and time. After all participants arrive, they are randomly assigned to one of two rooms and a seat within the room. Each room is identical with four computers arranged around a table in a way that guarantees privacy of decisions but allows participants to naturally observe the gender composition of the room. At no point is the composition of the room, or gender, explicitly mentioned. Participants are asked to make a series of eight decisions which consist of dividing $10 between a certain option and a risky option.Footnote 1 The properties of the risky option change over the eight decisions and include lotteries with an expected value less than, equal to, and greater than one dollar per dollar invested. In each session, participants are randomly split into two two rooms to make these decisions. The composition of the room, in our environment, can affect behavior only through some mechanism unrelated to payoff or behavioral information channels. By randomly assigning participants to experimental sessions, we minimize selection on unobservables and the chance that participants know one another or would interact with each other afterwards. By randomly assigning participants to one of two rooms, we generate different environments based on gender. By asking participants to perform a task that is individual in nature, we eliminate the effect of payoff dependence. By keeping decisions confidential and randomizing the presentation order of decisions, we make it difficult to infer any information on the decisions of others for a particular lottery. Finally, by randomizing the lottery used to calculate payoffs we reduce any meaningful earnings comparisons across participants, should they engage in such cheap talk after the experiment. Consistent with previous research (Croson and Gneezy 2009; Eckel and Grossman 2008b), we confirm that women are more risk averse than men. This gives us confidence that our data are not atypical. Room composition has a significant effect on behavior, but it is one-sided. Women become less risk averse in the company of men, but men are unaffected by who is in the room. The effect for women is large—a woman is 3.5 times more likely to invest a dollar in a risky lottery when surrounded by men compared to when she is surrounded by women. We consider several mechanisms and conjecture that this result is driven by women being more aware of the social context and adjusting their decisions to mimic the expected behavior of those surrounding them.Footnote 2 This is consistent with psychology studies which find that women are more likely to conform than men (Bond and Smith 1996), evidence that individuals guess correctly that women’s behavior is more risk averse than men’s (Eckel and Grossman 2008a) and social comparison theory (Levinger and Schneider 1969). Our paper is not the first to experimentally investigate the role of audience or room composition on individual behavior. However, in other studies, the actions of participants are linked through behavioral, informational, strategic, or other payoff channels. For example, in bargaining, social dilemma or tournament experiments the choices of the other participants are strategically payoff relevant. In dictator experiments, the gender composition of the room may affect expectations about the gender of the recipient and thus be behaviorally relevant.Footnote 3 The literature most closely related to our study examines gender composition of groups and risk attitudes. Adolescent girls are found to be more risk taking when in same-sex groups (Booth and Nolen 2012), and women become less risk averse over time in a same-sex class (Booth et al. 2014).Footnote 4 The results of these studies are consistent with research on gender differences in competitive attitudes (Gneezy et al. 2003; Niederle and Vesterlund 2007), where women are more likely to compete in same-sex groups, and suggest the importance of audience effects and signaling (Bohnet and Frey 1999; Charness et al. 2007; Andreoni and Bernheim 2009). By contrast, our results show that when competition, strategic interaction, payoff dependence, previous interactions, feedback and audience effects are not possible, women are less risk averse in the presence of men, not women. There are several implications of our results. Peer effects (Bertrand et al. 2000; Duflo and Saez 2003; Conley and Udry 2010) may be even more basic and fundamental, apart from those due to payoff relevant information transmission between group members.Footnote 5 If this is the case, some choices are a reflective reaction to who is in the room. This highlights the potential importance of the environment in which decisions are made. For example, if a woman makes more risky investments when in the presence of men, the surroundings in which women consider financial or retirement decisions could have an important impact on the ultimate financial health of women and their families. Our results also have organizational implications, since they suggest that aggregating individual preferences to predict group behavior would produce different outcomes than those produced by examining group behavior directly. Finally, our results show that gender differences in behavior are not immutable. A woman’s behavior in male-dominated activities could end up being similar to that of a man’s. However, this might not be costless. While the observed behavior of both men and women might be similar, women might enjoy the task less. This would be consistent with women’s preferences having a strong effect on job selection even in the absence of any other observable differences (e.g. Dohmen and Falk 2011; Buser et al. 2014). The paper is organized as follows. Section 2 describes our experimental design, Sect. 3 shows results, Sect. 4 discusses potential explanations for the results, and Sect. 5 concludes.",6
23.0,3.0,Experimental Economics,10 December 2019,https://link.springer.com/article/10.1007/s10683-019-09637-8,Testing for the emergence of spontaneous order,September 2020,Konstantinos Georgalos,John Hey,,Male,Male,Unknown,Male,"The idea of Spontaneous Order—that societies can co-ordinate without government intervention—is mainly attributed to Adam Smith, but some think that it goes back further, even as far back as the fourth century BC, and the Chinese philosopher Zhuang Zhou who argued that “good order results spontaneously when things are let alone”. The idea was further developed by the French philosopher (and anarchist) Proudhorn in the nineteenth century, and played a major role in the thinking of the Scottish Enlightenment, being immortalised in Adam Smith’s Invisible Hand. Smith developed the concept of the division of labour and argued that rational self-interest and competition can lead to economic prosperity. Michael Polanyi (1948) was the first to actually call this process Spontaneous Order, a notion that the Austrian School of Economics would later refine and make it the flagship of its social and economic thought, mainly expressed by Karl Menger, Ludwig von Mises, and Friedrich Hayek, with Menger wondering “How can it be that institutions which serve the common welfare and are extremely significant for its development come into being without a common will directed toward establishing them?” (Menger 1985, p.146). More recently, Sugden (1989) provided a thorough exposition of the importance of Spontaneous Order for studying economics, by discussing how many of the institutions in the market economy are conventions that no one has designed but they have simply evolved. Sugden concludes by stating “Thus the study of spontaneous order may help to explain why we have some of the moral beliefs that we do have, without in any way being able to show that we ought to have them”. We experimentally investigate whether and how Spontaneous Order emerges, building on Smith’s idea of the division of labour—as demonstrated in his Pin Factory example. In this, he showed that in a production process, if different workers specialise in different parts of the production process, then the workers can jointly produce a much greater volume of output than if they do not: when all workers specialise we have an efficient outcome. Moreover, he argued that this would happen ‘spontaneously’ and without external intervention. We investigate this hypothesis, first with just 2 workers, then with 6 and finally with 9—to explore whether the number of workers influences the speed of convergence to the efficient outcome. We also examine the crucial influence of communication on convergence. By this, we mean communication between the workers, and not outside involvement. This paper starts with a review of relevant background experimental research, providing the motivation for this study. We then describe the basic model, and then, in Sect. 4 we discuss extensions to, and variations on, this basic model. The experimental implementation is described in Sect. 5. Section 6 presents a description of behaviour in the experiment, while Sect. 7 analyses the text messages between subjects and sheds light on why they did what they did. Section 8 concludes.",3
23.0,4.0,Experimental Economics,13 August 2020,https://link.springer.com/article/10.1007/s10683-020-09666-8,Reactions to (the absence of) control and workplace arrangements: experimental evidence from the internet and the laboratory,December 2020,Katrin Schmelz,Anthony Ziegelmeyer,,Female,Male,Unknown,Mix,,
23.0,4.0,Experimental Economics,30 January 2020,https://link.springer.com/article/10.1007/s10683-020-09644-0,(Not) alone in the world: Cheating in the presence of a virtual observer,December 2020,Jantsje M. Mol,Eline C. M. van der Heijden,Jan J. M. Potters,Female,Female,Male,Mix,,
23.0,4.0,Experimental Economics,25 February 2020,https://link.springer.com/article/10.1007/s10683-020-09645-z,"Altruism, fast and slow? Evidence from a meta-analysis and a new experiment",December 2020,Hanna Fromell,Daniele Nosenzo,Trudy Owens,Female,Female,Female,Female,"A recent literature in economics and psychology argues that human behavior can be understood as the interaction between two different decision systems (Kahneman 2002, 2011): one that is fast, intuitive, automatic and largely effortless (“System 1”), and one that is slower, more deliberate, and requires some level of reflection and cognitive effort (“System 2”). For some decisions, the two systems may diverge in the choices they favor: while deliberation may pull the individual towards a certain choice (e.g., keeping a healthy diet), intuition may pull them towards a different choice (e.g., eating tasty but highly caloric food). In these cases, an individual must use willpower and spend cognitive resources to override the intuitive impulse and take the choice favored by the deliberative system. It has been suggested that altruistic behavior can also be rationalized by this dual-system framework (Loewenstein and O’Donoghue 2007; Moore and Loewenstein 2004; Zaki and Mitchell 2013; Deck and Jahedi 2015; Dreber et al. 2016). One of the most debated questions in this literature is whether altruism (and pro-social behavior more generally) is a spontaneous and intuitive response, where individuals must use willpower if they wish to act in their self-interest; or whether instead self-interest is intuitive, and individuals must use willpower to behave pro-socially. A number of experiments have been designed to address this question. These experiments rely on different types of manipulations, designed to inhibit one of the systems and promote the other. For example, subjects in an experiment may be asked to distribute an amount of money between themselves and another participant (e.g., in a dictator game), while at the same time performing another task that is more or less cognitively-taxing (e.g., holding a 7-digit or 3-digit number in their memory). Compared to subjects who perform the easier task, subjects who perform the hard task are under “cognitive load”: they have fewer cognitive resources to devote to the dictator game decision, and are therefore less able to use their deliberative system when deciding how much to give to the other participant. Other commonly used types of manipulations include: “ego depletion” where subjects participate in a sequence of two tasks, the first to deplete cognitive resources, and the second to measure how the consequent reduced ability to use deliberation affects behavior; “time pressure” where subjects are forced to make a decision either quickly or after having deliberated for some time; and “priming” where subjects are consciously or subconsciously encouraged to decide using either their intuitive or deliberative system. In this paper, we report a meta-analysis of the experimental studies that have used cognitive load, ego depletion, time pressure, and priming to study the effects of promoting intuition on altruistic behavior. Our meta-study covers 22 papers involving a total of 60 experiments and more than 12,000 subjects. We find that in 57% of the experiments, promoting intuition leads to more self-interested behavior, suggesting that self-interest is an intuitive response. In the other 43% of experiments promoting intuition encourages more altruistic behavior, suggesting that altruism, and not self-interest, is intuitive. These effects, however, are statistically significant only in a minority of studies. In the large majority of cases (78% of experiments), the effect of promoting intuition is insignificantly different from zero. The overall effect size estimated across all 60 experiments is only − 0.015, and we cannot reject the null hypothesis that this is actually zero. It is unclear how to interpret this evidence. On the one hand, taken at face value, the fact that the literature has found effects in either direction, and an average estimated effect close to zero, may suggest that the true underlying effect is actually very small or non-existent, and that the few significant effects reported in the literature are false positives. That is, altruism is neither fast nor slow: this type of behavior simply escapes the logic of the dual-system framework. On the other hand, some researchers have argued that whether intuition favors altruism or self-interest may depend on a variety of individual, social, and contextual factors. While for some subgroups of individuals in some specific situations intuition may favor altruism, for other subgroups or situations intuition may favor self-interest (e.g., Hauge et al. 2016; Rand et al. 2016; Grossman and Van der Weele 2017; Balafoutas et al. 2018). That is, the mixed results reported in the literature could simply reflect a genuine heterogeneity in the underlying decision processes that different individuals use in different situations—and since this heterogeneity is typically unanticipated and unaccounted for in most existing studies, this could explain why the reported overall effect is small and close to zero in most of the literature. To assess the role of heterogeneity in explaining the mixed results found in the literature, we conduct a mediator analysis to identify factors that may account for systematic differences across studies in the size and direction of the effect of intuition on altruism. We examine a variety of factors that have been proposed in the literature as possible mediators of the effect, including gender (Rand et al. 2016), the frame of the game (Hauge et al. 2016; Banker et al. 2017; Gärtner 2018), and the experimental stakes (Mrkva 2017; Andersen et al. 2018). With the possible exception of the frame of the game, for which we find mixed support, we find no evidence that any of the factors we consider can explain the variance in effect sizes across studies. For gender, a previous meta-analysis by Rand et al. (2016) found that promoting intuition has significantly different effects for women and men: the effect is positive and significant for women, but negative and insignificant for men. If anything, our analysis, which subsumes most of Rand et al.’s data and is based on a much larger number of studies, finds the opposite: the effect is negative for both women and men (significantly so only for the latter), and we cannot reject the null of no difference in the size of the effect between genders. These results lend little support to the argument that there may be a genuine heterogeneity in the size and direction of the effect of intuition on altruism across different subsamples and that this may explain the mixed evidence reported in the literature about the overall effect of intuition on altruism. To further probe this conclusion, in the last part of the paper we report a new experimental test of the hypothesis that altruistic behavior responds to the logic of the dual-system model. While existing studies have tested this hypothesis by studying whether promoting System 1 makes individuals more altruistic or more self-interested, we examine whether making choices that involve a trade-off between altruism and self-interest triggers a conflict between the two systems that is cognitively taxing and willpower-depleting. An advantage of our experimental test is that, unlike existing experiments, it is robust to the presence of heterogeneity in whether System 1 favors altruism or self-interest. Even if System 1 is altruistic for some individuals and self-interested for others, both types of subjects would have to spend willpower to regulate a conflict between self-interest and altruism, as long as System 1 and 2 diverge in the behavioral motive that they favor. We do not find evidence that facing a trade-off between self-interested and altruistic choices depletes willpower. Taken together the results from our meta-study and our new experiment offer little support for a dual-system theory of altruistic behavior. The remainder of this paper is structured as follows. In Sect. 2 we present the findings of our meta-analysis of the existing literature. Section 3 describes the design and results of the new experiment. Section 4 concludes.",20
23.0,4.0,Experimental Economics,26 December 2019,https://link.springer.com/article/10.1007/s10683-019-09638-7,Underpricing of initial public offerings in experimental asset markets,December 2020,Sascha Füllbrunn,Tibor Neugebauer,Andreas Nicklisch,,Male,Male,Mix,,
23.0,4.0,Experimental Economics,18 December 2019,https://link.springer.com/article/10.1007/s10683-019-09639-6,Delegation and coordination with multiple threshold public goods: experimental evidence,December 2020,Luca Corazzini,Christopher Cotton,Tommaso Reggiani,Male,Male,Male,Male,"In threshold public good games, players choose how much (money, time or effort) to contribute towards a public good that will provide benefits only if total contributions exceed a minimum level. Such games can be used to model strategic contribution decisions involving crowdfunding projects, social movements, and charitable giving. In the games, individuals choose how much to contribute to a common cause, while recognizing that their contribution may have a meaningful impact only if the cause also receives enough support from others to be viable. For example, philanthropists who wish to support the construction of a new community arts center recognize that their contribution will only have its intended effect if total contributions from all donors are high enough for the project to move forward. Except in the case of very large donors who can unilaterally ensure the success of a project, donors prefer to contribute to projects that receive enough funding from others to be viable, but not so much funding from others that the marginal impact of their own contribution is low. Further complicating the donor decision is the fact that, in many settings, multiple projects or opportunities simultaneously vie for donor funding. Donors must choose not only how much to contribute, but also to which projects or charities to contribute. In these settings, donors are exposed to the additional risk that they inefficiently spread contributions too thinly across projects. Corazzini et al. (2015) (henceforth CCV) extend the threshold public good environment to allow for multiple public goods simultaneously vying for funding, and show how increasing the number of public goods can discourage giving, decrease total contributions and increase the probability that all public goods fail. The present paper extends the multiple threshold public good environment from CCV to allow for strategic delegation.Footnote 1 Here, donors may provide their contributions to an intermediary, which then chooses how to allocate total contributions across potential uses. The presence of an intermediary has the potential to simplify donor strategies and reduce coordination problems. No longer must one choose how to allocate contributions across alternative projects. One must simply choose how much to contribute, leaving the decision about where to contribute in the hands of the intermediary. The use of delegation strategies may avoid situations where contributions are spread inefficiently across projects. The presence of an intermediary could therefore encourage contributions and increase the probability that public goods successfully reach their funding thresholds. Intermediaries are common in real world philanthropic environments (Chlass et al. 2015; Coffman 2017; Giving.USA.Foundation 2018). Americans gave more than 410 billion to charity in 2017 (2.1% of the GDP). Voluntary contributions represented 70% of total giving of which only 2% did not flow through organizations and intermediaries (Giving.USA.Foundation 2018). Many large non-profits are involved with a variety of projects and choose how to allocate contributions they receive across alternative uses. Furthermore, some prominent organizations exist with the specific mission of encouraging and coordinating philanthropic efforts for their causes or within their communities. At the cause level, for example, the Susan G. Komen Foundation raised approximately $400 million to reduce cancer deaths during the 2009–2010 fiscal year. The organization then choose how to allocate its funds across related activities such as funding various research projects (21%), education campaigns (39%), providing cancer screening and treatment (19%), and fundraising and administration (21%) (Susan G. Komen Foundation 2010). At the community level, community chest organizations such as the United Way operate in many locations to encourage and coordinate local donor efforts. In 2016, United Way pooled funds from more than 9 million individual donors and 60, 000 corporate partners for a total of \(\$4.7\) billion raised. It also managed a network of 1, 200 local offices in 40 different countries and coordinated the volunteer efforts of 2.9 million individuals. The funding activities and the volunteer efforts supported projects in 1800communities, serving more than 60 million people (UnitedWay 2016; Economist 2017). Typically, individuals choose how much to contribute to their preferred cause’s foundation or a local United Way. Then the organization decides how to allocate the sum of its contributions across many viable projects and organizations in order to maximize the social impact of the donations. By contributing through the United Way or related organizations, individuals do not need to strategize about whether their own donations are optimal given the allocation decisions of others. They simply choose how much to give and defer to the intermediary organization to allocate their contributions across projects in the optimal way. After incorporating an intermediary organization into a repeated multiple threshold public good game, we show experimentally that the effectiveness of such organizations depends heavily on the formal restrictions placed on its use of donor funds. First, we consider an environment in which the intermediary is under no obligation to allocate donor funds to a use that is valued by the donors. This need not be interpreted as illegal theft or embezzlement of funds, but, rather, it captures the possibility that an organization may be able to direct contributions towards increasing its staff size or salaries, or towards projects that are unrelated to the projects donors hoped to fund. However, just because an intermediary can expropriate funding doesn’t mean that it will do so. Intermediaries may feel obligated to use funding in accordance with donor expectations, especially in dynamic environments where fund use today may affect future donations. Second, we consider an alternative version of the environment in which a “destination rule” formally requires that the intermediary pass along the entire value of the donors’ delegated contributions to public goods. Redirecting all or part of the delegated contributions for private benefit is no longer possible. Such a destination rule is consistent with non-profit sector regulations or public commitments made by NGOs that generally improve transparency regarding the use of funds and limit the share of contributions that can be directed to overhead or administrative costs.Footnote 2 In both environments, the presence of an intermediary may reduce the risk of donors contributing to projects that do not receive enough support from others to be viable. In doing so, the presence of an intermediary may potentially encourage contributions and increase public good success and payoffs. In the unrestricted delegation environment, however, donors face a dimension of risk that is not present with a destination rule: they face uncertainty about whether the intermediary will work in the public interest, and risk that their contributions will not be passed along to a public good. Regardless of whether the intermediary intends to behave outside of the public interest, donors face the possibility that it might. The relative risk associated with contributing is lower in the delegation game with a destination rule than it is in either the game without delegation or the game with unrestricted delegation. We therefore expect delegation to increase contributions and public good success when intermediaries face restrictions on their use of funds. It is less clear, however, whether the presence of an intermediary will increase contributions and public good success when there is no destination rule. Unrestricted delegation reduces the risk of mis-coordination among donors, while simultaneously introducing the risk of intermediary expropriation. Which dimension of risk is more effective at discouraging contributions is an empirical question, which we consider experimentally. In the laboratory experiment, we show that the presence of an intermediary increases success of public goods and overall welfare only in the setting with a destination rule. With a destination rule, the ability to delegate donations has a significant positive impact on public good funding, including coordination, success rates and payoffs. When the intermediary does not face formal restrictions on the use of funds, however, donors contribute even less than in the case without an intermediary. Without a destination rule, the presence of an intermediary does not help increase contributions or improve public good success, leading to worse outcomes for groups and less success for the public goods they are trying to support. Together, these results suggest that an intermediary organization can help facilitate coordination among donors and successful funding of public goods. For this to be the case, however, donors must have reason to believe that the intermediary will use their donations effectively. Without this confidence, the presence of an intermediary can decrease contributions and public good success. Our findings highlight the potential for community chest organizations and other intermediaries, and the benefits from such organizations restricting how they are able to use donations ahead of any funding drive. They also illustrate a channel through which regulations and oversight of charitable organizations, such as rules governing the portion of donations that may be directed to administration, may facilitate donations and lead to more successful charitable giving.",11
23.0,4.0,Experimental Economics,23 December 2019,https://link.springer.com/article/10.1007/s10683-019-09640-z,Measuring and controlling for the compromise effect when estimating risk preference parameters,December 2020,Jonathan P. Beauchamp,Daniel J. Benjamin,Christopher F. Chabris,Male,Male,Male,Male,"The compromise effect arises when options in a choice set can be ordered on common dimensions or attributes (such as price, quantity, size, or intensity), and decision makers have a propensity to select the options in the “middle” of the choice set. In short, the compromise effect is a bias toward the middle option. For example, suppose a group of respondents were asked whether they wanted a free nature hike of either 1 mile or 4 miles. Now suppose that a different, otherwise identical group were asked whether they preferred a free nature hike of 1, 4, or 7 miles. A strong compromise effect would lead to a  greater fraction of respondents choosing 4 miles in the second choice set (see Simonson 1989 for a closely related empirical result and Kamenica 2008 for a discussion of microfoundations). The compromise effect poses conceptual and practical problems for economic research. By influencing choices, the compromise effect can bias researchers’ inferences about other economic parameters. In this paper, we propose and estimate an econometric model that disentangles and separately measures both the compromise effect and other parameters of interest. To demonstrate our approach, we conduct a laboratory experiment with 550 participants in which we elicit risk preferences using multiple price lists (MPLs). We study this context because, despite the limitations of the MPL procedure (e.g., Freeman et al. 2019), it is among the most commonly used methods to elicit preferences in the economics literature (e.g., Tversky and Kahneman 1992; Holt and Laury 2002; Harrison et al. 2007a; Andersen et al. 2008) and because the compromise effect has been carefully and robustly documented already in the context of inferring risk preferences using an MPL (Birnbaum 1992; Harrison et al. 2005b; Andersen et al. 2006; Harrison et al. 2007a). The screenshot shown in Fig. 1 is drawn from our own experiment and is typical of MPL experiments. In this example, a participant is asked to make seven binary choices. Each of the seven choices is between a gamble and a sure-thing alternative. The gamble doesn’t change across the seven rows, while the sure-thing alternative varies from high to low. Screenshot from the experiment A subject who displayed a very strong compromise effect would act as if she were indifferent between the gamble and the sure-thing in row (d), which is the middle row. Such indifference would imply that she is risk seeking because the gamble has a lower expected value than the sure thing in row (d). In this example, a strong compromise effect would lead a participant who may otherwise be risk averse to make risk-seeking choices.Footnote 1 Following prior work (Birnbaum 1992; Harrison et al. 2005b; Andersen et al. 2006; Harrison et al. 2007a, b), we experimentally vary the middle option using scale manipulations. Specifically, we hold the lowest and highest rows of the MPL fixed and manipulate the locations of the five intermediate outcomes within the scale. For example, compare the screenshot in Fig. 1 to the one in Fig. 2, which has new alternatives in rows (b) through (f), although rows (a) and (g) are the same. With respect to this second MPL, an agent who acts as if the middle option, row (d), is her indifference point would be judged to be risk averse. Screenshot from an alternative scale treatment condition of the experiment In our experiment, each participant is exposed to one of five different scale treatment conditions. To econometrically disentangle other economic parameters from the compromise effect, and to measure the strength of the compromise effect, we augment a discrete-choice model with additional parameters that represent a (rising) penalty for expressing an indifference point further from the middle of the ordered MPL. Our approach of incorporating the compromise effect into the econometric model is different from including treatment-condition indicators as controls. Simply controlling for the treatment condition would not identify risk preferences because the compromise effect influences choices in every treatment condition (i.e., there is no compromise-effect-free treatment condition). The risk preferences we study in the current paper are prospect-theoretic preferences over risky lotteries (e.g., Tversky and Kahneman 1992; Wakker 2010; Bruhin et al. 2010). Our ex ante hypotheses focus on two parameters: utility curvature \(\gamma\) (which captures concavity in the gain domain and convexity in the loss domain) and loss aversion \(\lambda\) (which captures the degree to which people dislike losses more than they like gains).Footnote 2 Our analysis yields three main findings. First, our estimates of the compromise-effect parameters replicate the findings from earlier work that participants have a bias toward expressing indifference closer to the middle rows of the MPL (e.g., Harrison et al. 2005b; see other references above). Moreover, our quantitative estimates indicate that the bias is economically significant; we estimate that the attractiveness of the middle rows relative to the extreme rows represents 17–23% of the prospects’ monetary value. Second, when we estimate the prospect-theory model without controls for the compromise effect, the scale manipulations have a very powerful effect on the (mis-) estimated preference parameters. In particular, the compromise effect is strong enough to cause us to estimate either loss-domain convexity of the utility function (as predicted by prospect theory) or loss-domain concavity (the opposite of what is predicted by prospect theory), depending on the scale manipulations. The compromise effect is also strong enough that, when manipulated, it can make behavior look as if there is essentially no loss aversion (see the results for the Pull 2 treatment below). Third, when we estimate the prospect-theory parameters while including additional parameters to capture the compromise effect, our estimates of \(\gamma\) and \(\lambda\) are robust across the five scale treatment conditions. The robustness of these preference-parameter estimates implies that they are not biased by the compromise effect. (When estimating the model pooling all of our experimental data, our estimates are \(\hat{\gamma } =0.24\) and \(\hat{\lambda }=1.31\), which fall within the range of estimates in the existing literature, albeit with \(\hat{\lambda }\) toward the lower end of the range.) In addition to the scale manipulations described above, we also study the effect of explicitly telling experimental participants the expected value of the risky prospects. We hypothesized that this manipulation would anchor the participants on the expected value, thereby nudging their preferences toward risk neutrality. However, we find that expected value information does not affect measured utility curvature nor measured loss aversion. This paper contributes to the literature on the compromise effect by estimating a model that explicitly accounts for the compromise effect and enables us to separately estimate it from risk preferences. Our sample is substantially larger than those used in earlier work, which allows us to precisely estimate the effects of the scale manipulations. Moreover, because we pose gambles involving losses as well as gambles involving gains, we can study the effect of scale manipulations not only on utility-function concavity in the gain domain, but also on curvature in the loss domain and on loss aversion. In addition, we provide estimates of the economic magnitude and importance of the compromise effect relative to the prospects’ monetary value, and we examine the demographic correlates of the parameters in our econometric model. A limitation of our experiment is that only one out of its four parts (which involves 28 of the 62 sets of choices we analyze) is incentivized. Reassuringly, all of our results still hold when we restrict attention to the incentivized data. The rest of the paper is organized as follows. In Sect. 2, we discuss our experimental design. In Sect. 3, we describe our econometric discrete-choice model, which incorporates the compromise effect. In Sect. 4, we list and discuss the five formal hypotheses that we test. In Sect. 5, we report the results of the estimation of our model with the compromise effect, and we test the robustness of the estimates to the scale manipulations. Section 6 parallels Sect. 5 but examines the model without controls for the compromise effect. Section 7 estimates the economic magnitude and importance of the compromise effect in our data. Section 8 briefly analyzes the demographic correlates of the main parameters of our econometric model (including \(\gamma\), \(\lambda\), and parameters that capture the compromise effect). Section 9 briefly discusses the results of our expected value manipulation. Section 10 compares our results to other findings in the literature on the estimation of risk preferences. Section 11 concludes by discussing possible directions for future work.",10
23.0,4.0,Experimental Economics,16 January 2020,https://link.springer.com/article/10.1007/s10683-019-09641-y,Experience and rationality under risk: re-examining the impact of sampling experience,December 2020,Ilke Aydogan,Yu Gao,,,,Unknown,Mix,,
23.0,4.0,Experimental Economics,11 February 2020,https://link.springer.com/article/10.1007/s10683-020-09642-2,"The one player guessing game: a diagnosis on the relationship between equilibrium play, beliefs, and best responses",December 2020,Ciril Bosch-Rosa,Thomas Meissner,,Male,Male,Unknown,Male,"Subjects in laboratory experiments consistently deviate from equilibrium behavior (Camerer 2003). Many models of bounded rationality try to explain these deviations through errors in belief formation (e.g., Nagel 1995; Ho et al. 1998; Weizsäcker 2003). Another explanation is that subjects fail to fully understand the structure of the game (Chou et al. 2009 refer to this as an absence of “game form recognition”). Generally, when analyzing deviations from equilibrium behavior, one would expect both of these effects to play a role. However it is typically hard (if not impossible) to distinguish between the two, as correct belief formation crucially depends on a correct understanding of the structure of the game. With the help of a novel one-player guessing game experiment, we are able to disentangle these two effects, thus improving the understanding of why subjects deviate from equilibrium behavior. An extensive literature has attempted to analyze both belief formation and understanding the structure of the game. Costa-Gomes and Crawford (2006) present subjects with a series of two-player dominance-solvable games and conclude that most subjects understand the games, but play non-equilibrium strategies due to their “simplified models of others’ decisions.” In Costa-Gomes and Weizsäcker (2008) the authors look at subject’s actions and their stated beliefs, and find that subjects rarely best respond to their stated beliefs. However, Rey-Biel (2009) observes that in simplified versions of the games studied in Costa-Gomes and Weizsäcker (2008), Nash Equilibrium is a better predictor of subject behavior than any other model based on level-K reasoning. Another strand of the literature focuses on whether subjects understand the structure of the game. Using two-player guessing games, Chou et al. (2009) find that subjects are surprisingly unable to understand the experimental setup they are participating in. By using different sets of instructions for the same game, and by introducing hints, they show that subjects do not deviate from equilibrium because of cognitive biases, but rather because of a lack of game form recognition, which they define as the relationships between possible choices, outcomes, and payoffs. Fragiadakis et al. (2016) let subjects play a two-player guessing game repeatedly against random opponents, and subsequently ask subjects to replicate or best respond to their previous choices. They find that while behavior of only 30% of subjects is consistent with a set of commonly used models (including equilibrium play and level-K), they also identify subjects who play strategically but are not identified by commonly used models. Finally, Agranov et al. (2015) develop an experimental protocol that allows them to track the decision-making process of subjects in a beauty contest game. The results show that around 45% of subjects consider playing weakly dominated strategies at some point in their decision-making process. In this experiment, we use a one-player guessing game which allows to measure how well subjects understand the structure of the two-player guessing game (Grosskopf and Nagel 2008). In this “game” subjects play the role of both players in a two-player guessing game. That is, they are asked to pick not one but two numbers between 0 and 100, and are paid according to the proximity of each of their choices to two thirds of the average of both choices. This setup switches off the belief channel, but still demands subjects to understand the structure of the game.Footnote 1 By comparing their actions in a two-player guessing game to the choices made in the one-player guessing game we can disentangle the effects of beliefs from “game form recognition,” and analyze to what extent understanding the structure of the game determines their belief formation and their best-responses.Footnote 2 Our experimental results show that a majority of subjects fails to fully solve the one-player guessing game, and that subjects with a better understanding of the structure of the one-player guessing game play values closer the Nash Equilibrium in the two-player guessing game. This implies that an important part of non-equilibrium play is likely due to the inability of subjects to fully understand the structure of the game. Additionally, we observe that subjects with a better understanding of the one-player guessing game form more accurate beliefs, are better at best-responding to their own beliefs, and tend to better adjust their beliefs according to the population they face. These results confirm the intuition that understanding the structure of the game is crucial for belief formation.",8
23.0,4.0,Experimental Economics,07 February 2020,https://link.springer.com/article/10.1007/s10683-020-09643-1,When does less information translate into more giving to public goods?,December 2020,Billur Aksoy,Silvana Krasteva,,Female,Female,Unknown,Female,"Private voluntary contributions have been increasingly viewed as a vital source of funding for public goods. For example, DonorsChoose, a fundraising platform for public school projects, has quickly gained popularity since its inception in 2000 and has raised close to $640 million up-to-date.Footnote 1,Footnote 2 Other crowdfunding platforms that fundraise for public projects include Public Good,Footnote 3 Razoo,Footnote 4 and Pledge Music.Footnote 5 Interestingly, while the non-profit sector is growing, with the number of non-profits surpassing 1.5 million, recent evidence suggests that individual donors are often poorly informed when making contributions. According to 2015 Camber Collective survey about private charitable giving in the U.S., “49% of donors don’t know how nonprofits use their money”.Footnote 6 Such lack of information may have a significant impact on giving since existing lab experiments suggest that donors care about the use of their money, with more valuable projects receiving higher contributions (see Ledyard 1995; Cooper and Kagel 2016). This paper uses a linear public good framework to theoretically and experimentally investigate the impact of more information about the marginal per-capita return (MPCR) of the public good on total contributions. In particular, it compares two information environments (treatments) corresponding to informed and uninformed donor populations (subject groups). An uninformed population (subject group) only observes the prior distribution of the MPCR and thus makes a contribution decision based on the expected MPCR. An informed population (subject group) observes the realized MPCR prior to contributing. By studying how donors respond to good news of higher than expected MPCR and bad news of lower than expected MPCR, we can determine whether information provision is on average beneficial or harmful for giving. Interestingly, our theoretical analysis and experimental findings reveal that the relative response to good and bad news crucially depends on the generosity level of the population, which reflects the strength of donors’ pro-social motivations for giving. We find that a less generous population increases contributions substantially in response to good news and reduces contributions relatively little in response to bad news. The opposite is true for a more generous population that exhibits a stronger response to bad news than good news. As a result, we find that information provision is beneficial for giving in a less generous donor population, but harmful in a more generous donor population. To glean more insight into this differential response to good and bad news, it is instructive to discuss how donors’ preferences impact the equilibrium contribution behavior in our theoretical model. We follow a growing literature that depicts agents as having other-regarding preferences (e.g. Rabin 1993; Fehr and Schmidt 1999; Bolton and Ockenfels 2000; Charness and Rabin 2002; Falk and Fischbacher 2006; Arifovic and Ledyard 2012). Such preferences are partially motivated by existing experimental findings revealing that, while it is payoff maximizing to contribute nothing to the public good and socially optimal to contribute the entire endowment, most contributions in the lab are in-between these two extremes (Ledyard 1995; Cooper and Kagel 2016). To capture this behavior, we follow Arifovic and Ledyard (2012) by modeling agents as having pro-social concerns, represented by agents’ preference for higher average payoff in the population, and also having fairness concerns, represented by agents’ dis-utility from obtaining lower than the average payoff. We refer to agents with stronger pro-social preferences as more generous since they have higher propensity to contribute. The above preference specification gives rise to an equilibrium giving behavior by each agent that can be classified into one of three types based on their own generosity level: (1) selfish giving of 0 for low generosity; (2) generous giving of the entire endowment for high generosity; (3) conditional cooperative giving, matching the average expected contributions in the population, for moderate generosity. The individual generosity level is private information to each agent and thus the expected equilibrium giving takes into account the distribution of the pro-social preferences in the population. Quite intuitively, for a fixed MPCR, the expected equilibrium giving increases with the likelihood of generous giving and decreases with the likelihood of selfish giving. Besides the distribution of the pro-social preferences, the equilibrium giving function is also affected by the MPCR. As the MPCR increases, the marginal cost of giving goes down while the social benefit goes up. This reduces the incentives for selfish giving in favor of cooperative and generous giving. As a result, the equilibrium giving function is increasing in the MPCR. Interestingly, the rate of increase is not constant, but instead it features increasing returns to MPCR when the MPCR is low (i.e. the expected giving function is convex in the MPCR for low values) and diminishing returns when the MPCR is high (i.e. the expected giving function is concave in the MPCR for high values). This is because at low MPCR, selfish giving dominates and an increase in the marginal return has a significant impact of incentivizing agents away from selfish to conditional and generous giving. This impact of increasing the MPCR eventually diminishes as the likelihood of selfish giving decreases substantially at high values of the MPCR. The shape of the equilibrium giving function is instrumental in evaluating the impact of information provision. While a convex giving function tends to generate a greater response to good news and a weaker response to bad news, the opposite is true for a concave giving function. Interestingly, we find that a more generous population reaches diminishing returns faster, and thus exhibits a wider concave region. This is because more generous agents are more easily induced to give and contribute significant amounts even at lower values of the MPCR. As a result, a more generous population is less responsive to good news and more responsive to bad news. The opposite is true for a less generous population, which features increasing returns for a wider range of the MPCR and thus is more responsive to good news than bad news. Consequently, the average informed giving is higher than the uninformed giving for a less generous population, while the opposite holds for a more generous population. This novel finding of our model gives rise to testable hypotheses, which we experimentally investigate in the lab. Since our theoretical model suggests that the generosity level of the population plays a vital role in how agents respond to information, a defining feature of our experimental design is controlling for the generosity level of the sessions. We accomplish this by running our experiment in two stages. First, we conduct an online experiment to elicit subjects’ generosity levels in the public good game prior to the lab experiment. Using this data, we create more and less generous groups in the lab, and inform the subjects about the generosity level of their session by using a neutral language. In the lab, subjects play a repeated one-shot linear public good game in groups of three with uncertain MPCR (either high (0.60) or low (0.40) with equal probability). There are two information treatments. In the Known MPCR treatment, subjects observe the randomly chosen MPCR before they make their contribution decisions. In the Unknown MPCR treatment, they are only informed about the distribution of the MPCR, and asked to make their contributions without observing which MPCR is chosen. The experimental findings are in line with the theoretical predictions. In the more generous sessions, the average contributions in the Unknown MPCR treatment are not statistically different than the average contributions in the Known MPCR treatment under good news (MPCR of 0.60), but are statistically higher than the average contributions under bad news (MPCR of 0.40). Thus, on average, information reduces contributions to the public good in the relatively more generous sessions. The opposite is true for the less generous sessions: the average contributions in the Unknown MPCR treatment are not statistically different than the average contributions under bad news, but are statistically lower than the average contributions under good news. Thus, information is on average good for giving in the less generous sessions. Our findings suggest that targeted information provision may be a successful strategy for increasing public good contributions. In particular, the model and the experimental results reveal that less generous donors are more responsive to good news about the returns from the public good. Thus, focusing on better informing these donors, who are often overlooked in fundraising campaigns, may be a more fruitful strategy than uniform information provision. In the next section, we briefly discuss how our findings contribute to the existing literature on public good provision.",2
23.0,4.0,Experimental Economics,20 February 2020,https://link.springer.com/article/10.1007/s10683-020-09647-x,Nash versus coarse correlation,December 2020,Konstantinos Georgalos,Indrajit Ray,Sonali SenGupta,Male,Unknown,Female,Mix,,
23.0,4.0,Experimental Economics,11 March 2020,https://link.springer.com/article/10.1007/s10683-020-09648-w,The relative income effect: an experiment,December 2020,John Ifcher,Homa Zarghamee,Lina Diaz,Male,Female,Female,Mix,,
23.0,4.0,Experimental Economics,27 February 2020,https://link.springer.com/article/10.1007/s10683-020-09649-9,The joy of lottery play: evidence from a field experiment,December 2020,Martijn J. Burger,Martijn Hendriks,Jan C. van Ours,Male,Male,Male,Male,"The average return on lottery tickets is typically just over 50%, which is considerably lower than the average return on other gambling games, such as horse racing, blackjack and roulette (Clotfelter and Cook 1990). Although buying lottery tickets is not a rational investment from a financial point of view, lottery play is the most popular form of gambling and the majority of the population participates at least once a year in a lottery (Kearney 2005; Garvía 2007). In 2015, the sales in the European lottery sector amounted to approximately €80 billion (The European Lotteries 2015), with European citizens spending on average €100 per person per year on lottery tickets. To increase our understanding of lottery participation, we conducted a field experiment randomly providing free lottery tickets to some participants of an existing household panel survey. In the economics literature, several explanations for widespread lottery play have been put forward. Already at the end of the 1940s, Friedman and Savage (1948) argued that lottery play offers an opportunity to win substantial amounts of money and improve one’s socio-economic status at a relatively low stake. Following the Friedman–Savage hypothesis and its later extensions and modifications (e.g, Pryor 1976; Brunk 1981; Hartley and Farrell 2002; Nyman et al. 2008), lottery play is considered rational when it offers the opportunity to improve one’s socioeconomic status or lifestyle when there are few or no other options to realize this otherwise. In prospect theory, widespread lottery play has been attributed to irrational beliefs that people uphold regarding their chances of winning a lottery since people tend to overweight the small chances of winning the lottery (Kahneman and Tversky 1979). Although there is some empirical support for both the Friedman–Savage theory and prospect theory, several scholars (e.g., Conlisk 1993; Clotfelter and Cook 1990; Scott and Gulley 1995) argue that these theories only partly explain people’s propensity to gamble. In particular, the Friedman–Savage theory suggests that lottery is predominantly played among the low and middle social classes. Poorer people indeed tend to spend a larger proportion of their income on lottery tickets (Beckert and Lutter 2013), but the theory cannot explain why people play the lottery also in those parts of the income distribution where additional wealth does not result in much additional expected utility (Walker 1998; Perez and Humphreys 2013). According to prospect theory, some people participate in lottery play because they largely overweight their small chances of winning. However, also this theory cannot explain widespread gambling because most lottery participants have quite rational expectations regarding the outcome of a draw (Forrest et al. 2000). A different explanation for widespread lottery play that has received less empirical attention in the economics literature is that lottery play itself has a utility value (Hirshleifer 1966; Eadington 1973; Loewenstein 1987; Conlisk 1993; Le Menestrel 2001).Footnote 1 In other words, there is a non-monetary or process utility of participating in a lottery. Lottery players may experience positive emotions before and after the draw. Positive emotions before the draw may result from one’s hope for a happier life, from the fun and excitement of the game as well as from social bonding activities when the lottery is played together with family or friends (Forrest et al. 2000; Guillén et al. 2011; Kocher et al. 2014). Positive emotions after the draw may originate from winning a prize, even when the prize is only very small and lower than the purchasing price of the lottery ticket. In addition to the monetary utility of winning a prize, there may also be a non-monetary utility of winning unrelated to the magnitude of the prize (Sheremeta 2010). Since most lotteries have many small prizes and the chance of winning a prize is high (in the lottery in our experiment the probability to win a prize is 50%), this could explain the widespread popularity of lottery play, including (seemingly) irrational behavior and the fact that lottery play takes place across the whole income distribution. While this utility of gambling model has considerable appeal, there is limited empirical evidence in support of this model (Perez and Humphreys 2013). This is probably due to the difficulty of identifying an appropriate observable proxy for the procedural utility generated by playing the lottery (see also, Nyman et al. (2008)). Happiness measures, as suitable indicators of procedural utility measures, have been suggested and applied in economic research (Frey and Stutzer 2002; Frey et al. 2004). Burger et al. (2016), using the British Gambling Prevalence Survey 2010, found a small positive effect of lottery participation on happiness for individuals who engage in lottery play for fun. Bruyneel et al. (2005) reported that the purchase of lottery tickets is associated with reducing negative mood. Along similar lines, in a lab experiment Kocher et al. (2014) identified hope and thrill as determinants of the popularity of Lotto tickets. Other studies showed a positive relationship between the hope of winning and lottery participation (Forrest et al. 2000; Clarke 2005; Ariyabuddhiphongs and Chanchalermporn 2007). These findings are also echoed in studies that examined the motives for lottery play: people do not only play for the money, but also for social bonding and fun (Miyazaki et al. 1999; Burger et al. 2016). In our study, we focus on understanding participation in lotteries. We investigate the utility of lottery play using a field experiment. Some randomly selected participants in a regular panel survey were provided with a free ticket of the Dutch State Lottery while others were not.Footnote 2 We assess the procedural utility of lottery play by comparing the change in momentary happiness (i.e. happiness experienced today) of lottery players and non-lottery players at three points in time: (1) before receiving a (free) lottery ticket, (2) after receiving a lottery ticket but before the draw, and (3) after the draw. We examine both the procedural utility of lottery play before and after the draw. We hypothesize that before the draw, players may gain procedural utility from the excitement of playing the game, the hope of winning a large prize, as well as social bonding, while after the draw players may gain procedural utility from winning a small prize (which was in almost all cases smaller than the original retail price of the ticket). To rule out the possibility that the utility effect we observe is related to receiving a free lottery ticket, and to account for the fact that some people purchased a lottery ticket themselves, we compare four groups of people: with a free lottery ticket, with a purchased lottery ticket, with both a free and purchased lottery ticket, and without a lottery ticket. Our paper contributes to the economics literature on lottery play and consumption in several ways. First, although many economic studies have addressed the utility gains of lottery wins, this is to the best of our knowledge the first paper to causally identify the procedural utility of lottery play using a large-scale field experiment. Second, in our study we take into account that lottery players may gain procedural utility before and after the draw. We find that lottery participation increases momentary happiness before the draw, but winning a small prize has no effect on momentary happiness. These results indicate that there is a procedural utility of gambling in the sense that people do not only care about winning prizes, but also enjoy the game. We conclude that lottery play has a utility value in itself. Third, and more generally, our article shows that consumption outcomes are not the only source of utility, but consumers also enjoy procedural utility, which is in turn a driving force behind consumer behavior (Frey and Stutzer 2002; Frey et al. 2004).",5
24.0,1.0,Experimental Economics,17 February 2021,https://link.springer.com/article/10.1007/s10683-021-09706-x,Special Issue in honor of John H. Kagel: Experimental Economics,March 2021,,,,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Experimental Economics,26 May 2020,https://link.springer.com/article/10.1007/s10683-020-09658-8,Best practices in replication: a case study of common information in coordination games,March 2021,Roy Chen,Yan Chen,Yohanes E. Riyanto,Male,Male,Unknown,Male,"Over the past decade, the issue of empirical replicability in the social sciences has received a great deal of attention (Open Science Collaboration 2015; Camerer et al. 2016). In particular, this discussion has focused on the degree of success in replicating laboratory experiments, the interpretation when a study fails to be replicated (Gilbert et al. 2016), and the development of recommendations on how to approach a replication study (Coffman et al. 2017; Shrout and Rodgers 2018). This paper contributes to our collective understanding of the best practices for replication studies. Among the large-scale replication studies, Camerer et al. (2016) examines the replicability of laboratory experiments in economics. They re-examined 18 experimental studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014, and successfully replicated 11 studies. Of the seven experiments which did not replicate, Chen and Chen (2011) study whether an ingroup identity can help subjects overcome coordination failure and achieve efficient coordination in the minimum-effort game (Van Huyck et al. 1990). In the laboratory, subjects are randomly assigned to minimal (random, anonymous, non-interacting) groups. They then use an online chat program to solve a problem together with their group members. This online chat is designed to enhance their group identity. The authors establish both theoretically and experimentally that an enhanced group identity can lead to more efficient coordination in environments that would naturally lead to coordination failure (Goeree and Holt 2005). As this experimental result did not replicate, we investigate the causes of this specific case of non-replication and use this case to propose a set of replication procedures designed to increase the quality of replication efforts. Examining potential reasons for this lack of replication, we first consider the possibility that cultural differences may play a role in the different results across the original and replication studies (Roth et al. 1991; Henrich et al. 2001). Specifically, the original study was conducted at the University of Michigan (UM), whereas the replication was conducted at the National University of Singapore (NUS), two universities with different ethnic and racial compositions (Online Appendix C). Indeed, the replication report notes that “Singapore subjects were less active in the communication stage than the American subjects” (Ho and Wu 2016).Footnote 1 To enhance ingroup identity in the study, subjects were asked to use a chat protocol to identify painters. Therefore, we began our investigation by comparing the effects of using (culturally appropriate) Chinese paintings versus the original study’s German expressionist paintings (Chen and Li 2009) on both communication volume and subsequent effort among NUS students. However, interestingly, when the first author conducted baseline experiments at NUS using the UM protocol, the original results replicated [see Fig. 1 and ingroup coefficient \(= 27.24\), \(p<0.001\), column (2) in Table 2]. The minimum and mean effort level across treatments at NUS We next examined the instructions provided to participants in the replication experiment. In the original study, subjects were given a paper copy of the full set of experimental instructions. However, this was not done in the replication. A recent experiment varies instruction delivery and reinforcement and shows that providing paper instructions is among the most effective methods to reduce potential subject misunderstandings during the experiment (Freeman et al. 2018). Finally, we examined the provision of written versus oral instructions to participants. Subjects in the original experiment received a paper copy of the full set of instructions, were able to read the instructions on a computer screen, and had the instructions read aloud to them as well. We discovered after communications with the replication author that participants in the NUS replication study did not receive oral instructions for the minimum-effort game.Footnote 2 This is an important protocol deviation, since having subjects read these instructions on their individual computer screen without hearing the instructions aloud removed the common information condition of the original study. As it is impossible to implement common knowledge in the laboratory, we use common information to refer to the information condition where every subject knows the game form as well as what other subjects know about the game (Smith 1994). As such, it is an approximation of the common knowledge condition required by the game theoretic framework. Removing the oral instruction protocol without replacing it with another method to achieve common information means that subjects are no longer certain about what other subjects know, which increases the uncertainty surrounding others’ strategies (Crawford 1995). Previous research in experimental economics has outlined the importance of providing experimental instructions that match the information condition required by the respective theory. For experiments that require common information, experimenters read the instructions aloud to ensure that everyone in the lab is certain about what everyone else knows. Indeed, in an experimental study of advice-giving in coordination games,  Chaudhuri et al. (2009) find that common information of advice leads to efficient coordination when advice from previous players is “not only distributed on paper but actually read out loud.” They further find that even small deviations from the common information protocol lead to suboptimal outcomes. This leads to our second conjecture that common information is critical to establishing efficient coordination with ingroup members. The rest of the paper is organized as follows. Section 2 reviews various experimental methods to approach common knowledge in the laboratory. In Sect. 3, we present our research design and summarize features of all four experiments used in our data analysis. In Sect. 4, we present our pre-registered hypotheses, our data analysis and results. Section 5 concludes by recommending a set of best practices for replication studies.",12
24.0,1.0,Experimental Economics,13 April 2020,https://link.springer.com/article/10.1007/s10683-020-09653-z,"Belief updating: does the ‘good-news, bad-news’ asymmetry extend to purely financial domains?",March 2021,Kai Barron,,,Male,Unknown,Unknown,Male,"Throughout our lives, we are constantly receiving new information about ourselves and our environment. The way that we filter, summarize and store this new information is of critical importance for the quality of our decision making. Theories of human behavior under dynamic uncertainty are therefore enriched by an understanding of how individuals process new information. Economists typically write down models where information is summarized in the form of probabilistic ‘beliefs’ over states of the world, and updated upon receipt of new information according to Bayes’ rule. However, the assumption that individuals process information in a statistically accurate way has increasingly been called into question, with many studies documenting systematic deviations.Footnote 1 One important strand of this literature examines whether belief formation and updating is influenced by the affective content of the new information, i.e. whether individuals update their beliefs symmetrically in response to ‘good-news’ and ‘bad-news’ (see, for example, Eil and Rao 2011; Ertac 2011; Möbius et al. 2014; Coutts 2019). This literature tests an implicit assumption of Bayesian updating, namely that the only object that is relevant for predicting an individual’s belief is her information set—her beliefs are completely unaffected by the prizes and punishments she would receive in different states of the world. This fundamental assumption—that people update their beliefs symmetrically—is important because it underpins the entire orthodox approach to modelling uncertainty. In this paper, we test whether individuals update their beliefs symmetrically in response to ‘good-news’ and ‘bad-news’ when states differ only in the financial rewards they offer. To do this, we conduct a laboratory experiment in which subjects complete a series of belief updating tasks. In each task, there are two possible states of the world. Subjects are told the prior probabilities of each state, and then receive a sequence of partially informative binary signals. We elicit subjects’ beliefs after each signal. The financial rewards associated with the two states are either identical (symmetric), or different (asymmetric). This experimental design allows us to compare posterior beliefs in situations where the entire information set is held constant, but the rewards associated with the states of the world are varied. For example, we can compare how two groups of individuals revise their beliefs when both groups share the same prior belief and receive an equally informative signal, but for one group of individuals the signal is ‘good news’ and for the other the signal constitutes ‘bad news’. We can also conduct a similar exercise for a single individual, comparing contexts where she has identical priors and signals, but the signal constitutes ‘good news’ in one context, and ‘bad news’ in the other. These comparisons provide a clean test of the asymmetric updating hypothesis. The experiment considers belief updating in two contexts. In the symmetric treatment, subjects have an equal stake in each of the underlying states. In the two asymmetric treatments, a larger bonus payment is paid in one of the two states of the world. This design permits two separate tests of the asymmetric updating hypothesis. First, we can compare how the same individual responds to ‘good-news’ and ‘bad-news’ within the asymmetric treatments. Second, we can conduct a between-subject comparison of belief updating in the symmetric treatment and asymmetric treatments. Each individual in our experiment faces only one incentive environment. However, since we exogenously endow participants with a prior over the states of the world, we are able to repeat the exercise several times for each individual and study how they update from each of five different priors, \(p_{0}\), chosen from the set \(\{\frac{1}{6},\frac{2}{6},\frac{3}{6},\frac{4}{6},\frac{5}{6}\}\). The experimental design and analysis aim to address several challenges that are present when studying belief updating in the presence of state-dependent stakes. First, we use exogenous variation in the priors to ensure that the estimates are robust to the econometric issues that arise when a right-hand side variable (i.e. the prior) is a lagged version of the dependent variable (i.e. the posterior). Second, we avoid a second type of endogeneity issue, which arises when the underlying states are defined as a function of some personal characteristic of the individual (e.g. her relative IQ) that might also be related to how she updates (see Online Appendix C for further details). Third, we measure the influence that hedging has on belief elicitation when there are state-dependent stakes. Furthermore, we conduct several exercises to correct our estimates for this hedging influence—both experimentally, and econometrically. Fourth, our experimental design allows us to study belief updating from priors spanning much of the unit interval. Importantly, averaging across all subjects, the design generates a balanced distribution of ‘good’ and ‘bad’ signals. Our results show no evidence in favor of asymmetric updating in response to ‘good-news’ in comparison to ‘bad-news’ in the domain of financial outcomes. Several robustness exercises are carried out in support of this conclusion. Furthermore, we find that average updating behavior is well approximated by Bayes’ rule.Footnote 2 This average behavior masks substantial heterogeneity in updating behavior at the individual level, but we find no evidence in support of a sizeable subgroup of asymmetric updators. The evidence reported here contributes to the recent literature studying the asymmetric updating hypothesis across different contexts (e.g. Eil and Rao 2011; Ertac 2011; Grossman and Owens 2012; Mayraz 2013; Möbius et al. 2014; Kuhnen 2015; Schwardmann and Van der Weele 2019; Gotthard-Real 2017; Charness and Dave 2017; Heger and Papageorge 2018; Buser et al. 2018; Coutts 2019). The results in this literature thus far are highly heterogeneous, with some papers finding a greater responsiveness to good-news, some a greater responsiveness to bad-news, and some no evidence of an asymmetry. The objective of the experiment discussed in this paper is not to isolate the contextual features that activate and deactivate asymmetric updating; rather the objective is to provide a clean test of the asymmetric updating hypothesis for contexts in which states differ only in their material rewards and have no direct ego-relevance. The reason for focusing on this context (where states yield different financial outcomes) is that it characterizes a large class of economically important decision problems under uncertainty—most economic models with uncertainty fit this description. This paper does not disentangle the reasons for why updating is asymmetric in some settings but not in others. Instead, by providing a clean test of the asymmetric updating hypothesis for one specific domain, the paper contributes to the growing collective body of evidence pertaining to asymmetric updating across a range of contexts. There are several candidate contextual and experimental design factors that could be generating the heterogeneous results observed in the literature as a whole. Section 7 below offers a discussion of some of these candidate explanations, and asks whether the existing body of evidence can help us to detect a systematic pattern that organizes the results. The remainder of the paper proceeds as follows. Section 2 outlines the theoretical framework, Sect. 3 details the experimental design, Sect. 4 provides some descriptive statistics, Sect. 5 presents the empirical specification, Sect. 6 discusses the related literature and Sect. 7 concludes.",38
24.0,1.0,Experimental Economics,20 March 2020,https://link.springer.com/article/10.1007/s10683-020-09650-2,Learning to accept welfare-enhancing policies: an experimental investigation of congestion pricing,March 2021,Nicholas Janusch,Stephan Kroll,Steffen Kallbekken,Male,Male,Male,Male,"Many problems surrounding externalities, such as excessive pollution, overfishing or traffic congestion, have relatively straightforward remedies that most economists embrace—price the externality-generating activity and people will change their behavior accordingly. Often, however, the public does not support policies based on these remedies as much as economists would predict (or like), even when these policies seem to be in the public’s interest. As a consequence, local and national governments have struggled to implement welfare-improving policies and wonder what determines the acceptability of them. This paper investigates how the effectiveness and personal experience of a policy trial as well as individuals’ worldviews and beliefs influence the acceptability of a welfare-enhancing policy in a laboratory experiment. While the problem of lack of acceptability applies to many incentive-based policies, from carbon pricing (e.g., Carattini et al. 2018) to economic instruments for agricultural groundwater use (e.g., Figureau et al. 2015), we focus here on the acceptability of congestion pricing. Traffic congestion and its impacts have been a growing problem in most urban centers around the world. In the United States, for example, 8.8 billion hours were lost in 2017 from the additional travel time due to congestion and 3.3 billion gallons of fuel were wasted, according to the 2019 Urban Mobility Report (Schrank et al. 2019). By optimizing road use and making people pay the true social cost of their traveling, congestion pricing is argued to be an efficient tool to tackle the congestion problem and lessen its societal costs. Moreover, most academic economists agree that revenue-neutral congestion charges “would make citizens on average better off.”Footnote 1 Theoretical models demonstrating the potential impact of congestion pricing on efficient road use date back to Pigou (1920), and since then the welfare impacts and incidence of pricing policies have been studied extensively (Small and Verhoef 2007).Footnote 2 While congestion pricing works in theory, there have been relatively few applications (Mahendra et al. 2012; Hall 2018). Urban congestion pricing has been successfully implemented and accepted in Stockholm, London, Singapore, Rome, and Milan (Börjesson et al. 2012), and as of 2019 is planned for Manhattan to start in 2021,Footnote 3 but implementation has failed in places such as Hong Kong, Edinburgh, Manchester, San Francisco, and previously in Manhattan (Ison and Rye 2005; Anas and Lindsey 2011).Footnote 4 The congestion pricing literature argues that the lack of public acceptability prevails as the main barrier to implementation. Two related factors seem to play a major role in undermining acceptability of congestion pricing and other incentive-based policies: First, while overall society gains from these policies, some segments of society might actually lose; based on a basic theoretical model and empirical studies, Fullerton (2011) provides an overview of who gains and loses from the introduction of an incentive-based policy (carbon pricing in his case) that results in a potential Pareto improvement overall. Second, even if all segments of society might gain from a policy, some groups might gain more than others, which could appeal to fairness considerations. For example, Light (2009) argues theoretically that congestion pricing helps those with high and low values of time more than those with intermediate values. Another factor impacting acceptability of a policy is the uncertainty about the effects a proposed policy will have. Fernandez and Rodrik (1991) and De Borger and Proost (2012) explain that the reluctance to implementing efficiency-improving policies that are advocated by economists may stem from a bias toward the status quo stemming from individuals’ uncertainties of the policy’s impacts. De Borger and Proost (2012) provide a model on how the presence of uncertainty is responsible for the evolution of public attitudes in places where congestion pricing was introduced like Stockholm and London. Using a simple majority voting model that employs two types of uncertainty (the idiosyncratic individual uncertainty about the exact cost of car use and the political uncertainty on the use of collected revenues), they demonstrate that because of individual uncertainty, a majority of drivers that are ex ante against road pricing may ex post be in favor after a policy trial removes individual uncertainty. This ex post majority favorability would suggest that policymakers might want to consider experimental trials against the political will of the majority of their constituents. But how individuals inform their beliefs and attitudes when assessing these two types of uncertainties may go beyond self-interest and fairness motives (even after a policy trial when uncertainty is removed). Peoples’ worldviews and beliefs, as well as psychological responses towards the introduction of congestion pricing, may also explain acceptability or the lack thereof (Schade and Baum 2007), and in the absence of more specific information on the specific costs and benefits of a proposed policy they may choose to rely on easily available heuristics. The cultural cognition thesis is particularly appropriate for our setting: The individualism-communitarianism dimension differentiates people with “attitudes toward social orderings that expect individuals to secure their own well-being without assistance or interference from society” from “those that assign society the obligation to secure collective welfare and the power to override competing individual interests” (Kahan et al. 2011) with clear implications for general attitudes towards Pigouvian taxation. The hierarchy-egalitarianism dimension holds equally clear implications for attitudes towards policies with (in-)equitable implications for payoffs. Cherry et al. (2017) find a consistent and strong impact of worldviews on support for efficiency-enhancing policies: “people with different worldviews exhibit substantially different levels of policy aversion—by more than 25% points in some cases.” To examine effectiveness and acceptability of congestion pricing, we pose three research questions: (1) Does congestion pricing work even with heterogeneity of users and potential losers from a toll? (2) Does experience and the resulting removal of the policy’s uncertainties from a policy trial influence acceptability? And (3) Do individual attributes impact the acceptability of tolls and does this acceptability evolve when an individual becomes accustomed to the problem and policy? We address these questions using a laboratory experiment. Understanding why congestion pricing was accepted in some places but not in others by observing both the performance and acceptability of congestion pricing at an individual level in the real world would be ideal. But such data collection would be too costly and almost impossible to implement. Alternatively, we turn to experimental economics. Falk and Heckman (2009) argue that laboratory experiments complement other empirical methods and data sources in the social sciences. Laboratory experiments allow for a low financial and political cost alternative. They provide a controlled environment in which researchers can test competing theories or evaluate the impacts of alternative policies on participant behavior. Previous laboratory experiments in the transportation economics literature have examined travel decisions (e.g., departure time, route choice, or mode choice) and how congestion pricing, information disclosure, and a new link in a transportation system affect user travel behavior.Footnote 5 To our knowledge, however, no previous laboratory congestion experiments incorporate voting or measures of public acceptability. The most relevant laboratory congestion experiments on congestion pricing use two-route networks to investigate the Pigou-Knight-Downs paradox, which states that improvements in a road network might not improve traffic congestion. Anderson et al. (2008) and Hartman (2012) investigate the effects of an efficient toll and information disclosure of past entrants and do find similar results regarding the effect of information and that the toll has its intended effects. Hartman (2006, 2007) also examines travel behavior when individuals have either real or assigned heterogeneous time preferences; Hartman (2007) compares the outcomes from the same toll when heterogeneous users have different assigned value-of-time distributions (no, low, or high heterogeneity). However, the outcomes were not compared to the behavior of the same assigned heterogeneous individuals for when no toll existed in the network. Our experiment is the first to compare route-choice behavior with and without a toll of heterogeneous individuals with assigned values of time. Recent experimental papers on public acceptability of Pigouvian policies have examined factors that contribute to the (un)acceptability of Pigouvian policies. Cherry et al. (2014) find that experience of a trial run of a Pigouvian tax increases the acceptability of the tax and that the positive experience can overcome misperception and biases.Footnote 6  Kallbekken et al. (2011) observe that a lack of understanding of the workings and effects of a Pigouvian tax instrument does not influence the opposition of such policies. The authors also find an aversion to Pigouvian taxes: a substantial subset of subjects oppose taxes that can increase individual and social welfare. By challenging the behavioral notion that people act solely on their monetary self-interest, this result reveals a barrier in implementing potentially efficient policies. Cherry et al. (2017) observe a strong correlation between individuals’ worldviews based on Kahan’s cultural cognition framework and their acceptance of different Pigouvian instruments. While in their paper all participants are materially equally impacted by the implementation of a policy, our research contributes by examining personal attributes that may affect acceptability as well as have a context where a policy either creates all ‘winners’ or both ‘winners’ and ‘losers’ with unequal outcomes. Worldviews might play a different role in our case, if people feel differently about government policies with distributional effects (this paper) or without distributional effects (Cherry et al. 2017).Footnote 7 Our experiment employs a congestion game, in which individuals with heterogeneous “time preferences,” induced by the experimenters, choose between two routes. One route is shorter but congestible, and the other route has a longer but constant travel time. Subjects vote three times—before they experience the game, after they experience it without tolls and again after they experience it with tolls—on whether the last stage of the experiment should have a toll. This novel design allows us to address the two main research questions 2 and 3 above: The votes provide a measure of the evolution of the acceptability of the toll by first obtaining an initial preference of tolls given exogenous characteristics (including individual cultural worldviews that are elicited in a post-experiment survey), and then any changes in attitudes from being accustomed to the congestion problem and the congestion pricing policy. To test additionally whether the answers to the two main research questions are sensitive to inequality and efficiency concerns and to information about relative positions, we vary two factors between different groups of subjects in a \(2\times 2\) design: (a) toll revenues are either recycled 100% (which makes every subject better off with the introduction of the toll, albeit at different levels) or 40% (which makes only some subjects better off with the toll, even though overall the sum of individual payoffs increases in equilibrium), and (b) after the policy trial subjects either know only how much the policy affected their costs in absolute and percentage terms or they additionally know their group-ranked position by viewing how much the policy affected the costs of the other group members. We find that, as expected, without a toll the congestible route is overused. Once a congestion toll is implemented it initially does not work well, but after a few periods the toll leads to improved coordination and higher efficiency—not only does the number of commuters converge to the efficient level but subjects also sort efficiently; the subjects with high values of time take the shorter, congestible route and pay the toll, while the subjects with a low value of time take the longer, free route but receive parts of the toll revenues. Approval of the toll is at its highest level, particularly among subjects that gain from the toll, when subjects have experienced the congestion problem and the toll. Interestingly, in the last vote monetary earnings have a stronger effect on votes than worldviews. Our findings offer two main contributions to the behavioral literature. First, we introduce an experimental design for voting experiments that controls for experience across multiple rounds of voting. Second, we provide evidence on the behavioral influences of cultural worldviews in voting experiments with heterogeneity in payoffs and equilibrium outcomes. The following sections provide an explanation of the theoretical two-route congestion model used in the experiment (Sect. 2) and the design of the experiment (Sect. 3). Section 4 discusses the empirical results, and the paper concludes with Sect. 5.",3
24.0,1.0,Experimental Economics,12 March 2020,https://link.springer.com/article/10.1007/s10683-020-09651-1,Is response time predictive of choice? An experimental study of threshold strategies,March 2021,Andrew Schotter,Isabel Trevino,,Male,Female,Unknown,Mix,,
24.0,1.0,Experimental Economics,10 April 2020,https://link.springer.com/article/10.1007/s10683-020-09652-0,"Information exchange in laboratory markets: competition, transfer costs, and the emergence of reputation",March 2021,Roman Hoffmann,Bernhard Kittel,Mattias Larsen,Male,Male,Male,Male,"Imperfect information and insufficient contractual enforcement can undermine trust and cause large inefficiencies in markets that suffer from moral hazard problems (Akerlof 1970). Reputation mechanisms, i.e. the availability of information about the past conduct of market participants, can serve as an effective remedy to overcome trust problems. Learning about the past behavior of (potential) transaction partners helps reduce uncertainties and creates incentives to cooperate in order to avoid exclusion from future transactions (Anderhub et al. 2002; Huck et al. 2012). While there is strong evidence that reputational information fosters trust and limits opportunistic behavior in markets, only a few studies have analyzed the conditions under which reputation and feedback mechanisms emerge (Abraham et al. 2016; Gërxhani et al. 2013; Jappelli and Pagano 1999). The automatic provision of feedback, for example, in online markets or through third party institutions such as credit bureaus and consumer protection agencies, can facilitate information exchange and help overcome barriers to information sharing among market participants. In other cases, the sharing of information does not occur automatically, but requires the strategic decisions of actors who weigh the benefits of information sharing against its costs (Pagano and Jappelli 1993; Frey 2017). This creates a second-order social dilemma (Fehr and Gächter 2002; Ostrom 1990) in which individuals may decide not to share their experiences, even though the transfer of information helps to establish market transparency, to bridge information asymmetries, and to overcome trust and moral hazard problems. Microfinance is an example of a market suffering from informational asymmetries for which—despite recent attempts to establish stronger external regulation—there are no institutional solutions to facilitate the sharing of information (Cason et al. 2012; de Janvry et al. 2010; Giné et al. 2012; Luoto et al. 2007). The resulting informational deficit and low information exchange, which are often due to fierce competition among the microfinance providers, can generate harmful effects for clients, who experience high interest rates or over-indebtedness due to lending from multiple sources. They can also be harmful for the microfinance organizations, who have to deal with adverse selection effects and hard to predict default risks. If information about borrowers is shared, this typically occurs in an informal way, for example, by word of mouth. Other examples include employers informally exchanging information about job candidates (Gërxhani et al. 2013) or purchasing companies about suppliers (Buskens and Raub 2002). Focusing on markets without institutionalized feedback mechanisms, our study (1) analyzes the conditions under which information is voluntarily shared among actors, (2) documents the consequences of a lack of information exchange and information asymmetries, and (3) highlights settings that could benefit from external regulation. We study the endogenous sharing of information in a laboratory experiment using repeated investment games to replicate markets suffering from informational asymmetries and moral hazard (Berg et al. 1995). The game is played between a first mover, the trustor, and a second mover, the trustee. Due to the sequential structure of the game, the trustor’s payoff after engaging in a transaction will depend on the cooperation of the trustee. In our design, trustors can decide whether or not to share information about previous transactions with others. The sharing of information may come at a cost. Conceptually, we distinguish between direct and indirect costs. Direct costs refer to costs that arise directly in the information transfer process, such as efforts to inform others, which are randomly varied in a within-subject treatment. Indirect costs, on the other hand, arise as a consequence of one-sided competition between market participants, the group of trustors in our case. With a rivalry in payoffs, information about the quality of trustees has a private value for trustors and concealing it may create a competitive advantage. Our study contributes to the literatures on strategic information sharing and the role of exchange barriers and costs for the functioning of reputation mechanisms (Brown and Zehnder 2010; Frey 2017; Gërxhani et al. 2013; Abraham et al. 2016). We consider the effects of voluntary information sharing and of exchange barriers on both trustors and trustees. Our results show that barriers to information exchange in the form of direct transfer costs and competition affect both types of players, leading to interesting dynamics. While free information exchange significantly improves trust and welfare, both types of costs diminish information sharing among trustors, making it less valuable for trustees to build a good reputation. We find evidence for a non-linear cost elasticity function, with even small direct costs leading to a disproportionally large drop in information sharing. The one-sided competition among trustors has a dual effect on the market: While it leads to a substantial decrease in information sharing and less trustworthiness, it encourages trustors to take more risks and to make higher investments to outperform competitors, which absorbs some of the negative welfare effects of reduced information sharing. Finally, there is strong evidence that the amount of information sharing by trustors is influenced by reciprocal motives towards fellow trustors and trustees alike. In our design, we abstract from many features that characterize markets suffering from moral hazard. Most notably, our design does not include a pricing mechanism and restricts free choice of trading partners. While these are important features of real markets, our stylized laboratory design focuses on the core issues of interest for our study, i.e. the role of voluntary information sharing and the consequences of different exchange barriers on trust and reputation building. The findings are relevant for markets where reputational information is important but no institutionalized information sharing or feedback mechanisms exists. Even if there is a common interest in exchanging information, transfer costs and competition may diminish information exchange, with major implications for the entire market. The remainder of the paper is structured as follows. Section 2 discusses the existing theoretical and empirical literature. Section 3 introduces the experimental design and the measurement of key variables. Section 4 presents the results and Sect. 5 discusses the findings and conclusions. The main text is accompanied by supplementary material offering a more detailed overview of the literature (S1), additional descriptive statistics (S2), model variations and sensitivity checks (S3), and more information on the experimental procedures and instructions (S4–S7).",1
24.0,1.0,Experimental Economics,04 May 2020,https://link.springer.com/article/10.1007/s10683-020-09654-y,Improving decisions with market information: an experiment on corporate prediction markets,March 2021,Ahrash Dianat,Christoph Siemroth,,Unknown,Male,Unknown,Male,"Many important corporate decisions depend on information about future conditions. Whether to fund the development of a new technology, whether to assign more resources to a project, or how many inputs to order for production: The answers to these questions depend on uncertain variables like the future demand of the technology, future problems with the project, and future sales. Thus, better information about these future conditions can help improve decision-making. Prediction markets are one forecasting mechanism that has received a lot of attention in predicting public events like election outcomes (e.g., Forsythe et al. 1992; Wolfers and Zitzewitz 2004). A typical market trades an asset that pays off 1 if event A occurs in the future, and 0 otherwise. Traders can either buy or sell these assets. If many traders believe that A will occur, their trading will drive up the price of the asset, which creates a “wisdom of crowds” prediction of event A’s probability. Since money is at stake, these markets provide incentives to acquire and reveal information (e.g., Page and Siemroth 2017). Moreover, the logic of the efficient market hypothesis suggests that these markets aggregate information that is dispersed among traders. Indeed, prediction markets have been shown to outperform both polls (e.g., Berg et al. 2008) and experts (e.g., Spann and Skiera 2009). Hence, prediction markets could be a valuable tool in corporations and organizations, and more generally, policy makers and decision makers might improve real (non-financial) decisions based on information inferred from asset prices. For example, central banks might glean information about economic fundamentals or inflation expectations from financial markets (e.g., Bernanke and Woodford 1997), and bond prices might reveal information about bank health to regulators and help them to audit or intervene (e.g., Prescott 2012; Sundaresan and Wang 2015). But theoretical models show that not every asset or every market is expected to be equally useful in providing pertinent information for decision-making: In some situations, the incentives of traders and decision makers are not aligned, whereas in others they are. This presents a market or asset design problem, and in this paper we test two market designs experimentally in a corporate prediction market setting to see which helps to improve a binary state-dependent decision. Some firms are already using prediction markets, and several software companies now offer corporate prediction market packages as a service to clients.Footnote 1 Still, research on corporate prediction markets is relatively scarce. Cowgill and Zitzewitz (2015) analyzed corporate prediction markets in three large corporations and found that they outperformed expert forecasts that had been previously used as a basis for decisions. In this project, rather than analyzing existing designs, we instead ask how such markets should be designed. To the best of our knowledge, we are the first to investigate how prediction markets inform decision making, whereas previous papers almost exclusively studied forecasting quality. For the more general problem of decision makers using information from financial market prices, the literature is almost exclusively theoretical, so we provide one of the first tests of the theory and how it can be used for market design. In our corporate framing, a manager needs information about whether a project deadline will be met in order to decide whether to invest additional resources into the project at a cost. Workers on the project are informed of the state of this project, but managers are not. One interpretation is that programmers on the project are intimately aware of the progress and potential problems, whereas the manager is not. If the state of the project is bad, then the project will miss the deadline unless it receives additional resources. If the state is good, then the deadline will be met even without additional resources. Since the optimal manager decision depends on the state of the project, we can test different prediction market designs—where workers trade and potentially reveal their information to managers—to see which leads to better decisions. The straightforward market design that is already in use (e.g., Cowgill and Zitzewitz 2015) trades an asset that pays off 1 if the deadline is met and 0 otherwise (the “deadline asset”). However, Siemroth (2020) showed that traders have poor incentives to reveal their information in this design. If workers bet on a missed deadline in the bad state, then this drives down prices and signals to the manager to assign additional resources, which in turn leads to the deadline being met and traders losing money. Thus, workers are punished for revealing their information. In technical terms, there is no equilibrium where workers reveal their information. Intuitively, workers predicting a missed deadline suffer from a self-defeating prophecy, as their prediction triggers a manager reaction that falsifies the prediction. An alternative market design addressing these incentive problems uses an asset that pays off 1 if the manager assigns additional resources, and 0 if not (the “action asset”, Siemroth 2020). Here, a revealing equilibrium exists: Workers trade at high prices in the bad state (suggesting to invest additional resources) and at low prices in the good state (suggesting not to invest), and the manager follows these “market recommendations.” Thus, this alternative market design aligns incentives between workers and managers. However, this design also features multiple equilibria and only one of them corresponds to the positive outcome where information is revealed. In a lab experiment, we test these two market designs against each other in terms of managerial decision quality. We also run supporting experiments that investigate whether a fictitious “CEO message” can improve information transmission and decision-making. The CEO message explains the purpose of the prediction markets and describes a revealing trading strategy. Hence, it can be viewed as offering advice to subjects on how to act. The inclusion of the advice treatments was motivated by a sense that the interaction between the prediction market and the manager’s decision is sufficiently subtle that subjects might benefit from a description of how these markets can plausibly operate. Moreover, the CEO message can help to facilitate coordination in the financial markets based on the action asset, which feature multiple equilibria. Based on our \(2\times 2\) experimental design (varying the market design and the presence of the CEO message), our first main finding is that the theoretically superior market design actually leads to worse manager decisions in the absence of the CEO message. This is because workers play non-equilibrium strategies in the markets with the deadline asset, which leads to better manager decisions than predicted by theory. Furthermore, workers play the non-revealing equilibrium in markets with the action asset, which leads to worse decisions than if play corresponded to the fully revealing equilibrium. Our explanation for worker/trader behavior in these two treatments is based on incorrect beliefs: Their behavior is consistent with the mistaken belief that managers do not react to the information contained in prediction market prices. In the language of Eyster et al. (2019), workers appear to incorrectly believe that managers are cursed, i.e., that managers are unable to infer the informational content of prices. Our behavioral explanation can account for all observed qualitative results and exactly predicts the average price levels in three out of four state-treatment cells. Our second main finding is that the CEO message—advice to use a revealing trading strategy—significantly improves both the information revealed by the prediction markets and managerial decisions by up to 25 percentage points. With the CEO message, managerial decisions between the two market designs are no longer significantly different, although the point estimate on correct managerial decisions is 9 percentage points higher with the action asset. A secondary criterion favoring the action asset design is that it exhibits significantly less mis-pricing, which should make it more stable. Thus, managerial decision quality being roughly equal, a practical recommendation would be to use the action asset design with top-down advice akin to our CEO message. Our findings have at least two important implications. First, absent advice, subjects are unable to realize the theoretically superior outcome of the action asset market design. This suggests that a theory based on rational expectations is not well-suited for mechanism design with inexperienced players in this complex setting. Even in the last few rounds of the experiment, after allowing for experience and potential learning, subjects do not converge towards the theoretical predictions. Our behavioral explanation suggests that incorrect beliefs among traders drive this result, which can potentially be corrected via communication by the market designer. Second, while the treatments without the CEO message provide valuable insights into strategic reasoning in interactive market settings, these conditions are unlikely to prevail in the field. If a firm implements a corporate prediction market to aid in managerial decisions, then it would also explicitly communicate the purpose of the market and explain how it is meant to be used. This is precisely what our CEO message does, and we show it improves outcomes significantly. In Sect. 5, we also discuss some methodological implications for experimental economists. We have a few remarks about our design choices. First, we study a setting in which the state of the project is common knowledge among workers, rather than a more general setting in which workers have asymmetric information. However, the ability of financial markets to at least partially aggregate private information has been extensively researched and is well-documented in the experimental literature. Moreover, it is not the focus of this paper. For these reasons, we chose the simplest possible information structure that still allows us to shed light on our main research question: whether asset prices can be used to improve decision making, when these decisions in turn feed back into asset values. Second, if we take our setting literally, it is reasonable to ask whether a prediction market is even necessary: In other words, can the manager learn the state of the project simply by asking any individual worker? In many relevant cases, we believe the answer is no. An individual worker might be reluctant to reveal the state of the project to the manager, for a variety of reasons outside of both the theoretical model and the experiment. For instance, workers might fear that the manager will “shoot the messenger” for delivering bad news. A prediction market mitigates this problem by offering anonymity to participants, with the additional benefit of providing financial incentives for information revelation (unlike the flat incentives that arise when the manager just asks an individual worker). Finally, our experiment controls for other forces that might also be at play in workplace environments. This allows for a cleaner and more direct test of the underlying theory. We further discuss this issue in Sect. 5. Overall, the potential of prediction markets to improve decisions is often mentioned but rarely investigated or explicitly allowed in experimental settings. Hence, our main contributions are to show how prediction markets can aid decision making and to demonstrate that the choice of market design very much affects information revelation and subsequent decision quality. To the best of our knowledge, we are the first to experimentally study prediction market design for the purpose of improving organizational decisions. In addition, our investigation of top-down advice in a complex market signaling game is novel and valuable for corporate applications. While the framing of a project deadline we consider here may appear somewhat specific, our setting gives insights into the more general problem of a decision-maker having to make a state-dependent decision, which is informed by traders in a financial market. This possibility has been considered in several theoretical papers in other contexts, for example managers learning from financial market prices to decide upon investment opportunities (e.g., Bond et al. 2010; Bond and Goldstein 2015; Edmans et al. 2015; Dow et al. 2017) or central banks learning from financial market prices to decide upon monetary policy (e.g., Bernanke and Woodford 1997). Indeed, though the framing differs, our setting is formally very close to Bernanke and Woodford (1997)’s static model when considering only two states and two decisions, so the problem we investigate is of interest beyond corporate prediction markets. In the prediction market literature, several studies have investigated the informational efficiency of these markets in non-corporate settings, such as Oliven and Rietz (2004), Berg et al. (2008), Page and Clemen (2013) based on field data or Page and Siemroth (2017, 2019), Corgnet et al. (2018, 2019), Choo et al. (2019) based on experimental data. The evidence on markets incorporating public information is positive, although the experimental work shows that there is typically under-reaction to private information (the latter paper being an exception). The experimental literature has also investigated potential problems of prediction markets, such as manipulation, where the evidence is mixed (Hanson et al. 2006; Deck et al. 2013). Moreover, thin markets with very few traders were found to hamper information aggregation (Healy et al. 2010). Ledyard et al. (2009) investigate prediction markets when the number of variables to be predicted is large, and find that performance suffers in this case. Studies with field data from corporate prediction markets are very rare. Cowgill and Zitzewitz (2015) provide analyses of markets from three different major companies and find that they improve upon the tools used before. However, they also identify an optimism bias where assets based on events that would be positive for the company are overpriced. Plott and Chen (2002) use corporate prediction markets to forecast sales. They find the market forecasts are an improvement over the existing forecasting methods. Gillen et al. (2017) use a parimutuel betting rather than a prediction market mechanism to forecast sales in a company, and find that the mechanism outperforms the existing forecasting techniques. There are very few studies of settings where markets can affect non-market decisions. Kogan et al. (2011) consider a setting where subjects in a group choose an effort level in a coordination game. A pre-play prediction market could potentially help to facilitate coordination by letting subjects bet on the effort levels in the subsequent coordination game. However, they find that prediction markets actually decrease effort levels in the coordination game and lead to worse outcomes. Our setting differs, because in our case the market can potentially reveal information about a state variable that the decision maker needs but does not have. Moreover, we consider a concrete corporate setting where markets are already used, whereas Kogan et al. (2011) consider an abstract setting. Davis et al. (2014) investigate whether financial markets can help a regulator make a conversion decision for contingent convertible bonds. They consider a prediction market as an additional tool to provide information and find that the treatment with the additional prediction market does best. In contrast to our setting, the prediction market is only a minor addition in one treatment that coexists with the market for contingent convertibles, which is the focus of their study.",2
24.0,1.0,Experimental Economics,04 May 2020,https://link.springer.com/article/10.1007/s10683-020-09655-x,Strategic gaze: an interactive eye-tracking study,March 2021,J. Hausfeld,K. von Hesler,S. Goldlücke,Unknown,Unknown,Unknown,Unknown,,
24.0,1.0,Experimental Economics,29 July 2020,https://link.springer.com/article/10.1007/s10683-020-09668-6,Learn or react? An experimental study of preventive health decision making,March 2021,Günther Fink,Margaret McConnell,Bich Diep Nguyen,Male,Female,,Mix,,
24.0,1.0,Experimental Economics,09 June 2020,https://link.springer.com/article/10.1007/s10683-020-09659-7,Learning while shopping: an experimental investigation into the effect of learning on consumer search,March 2021,Ben Casner,,,Male,Unknown,Unknown,Male,"Much of the literature on consumer search behavior assumes that searchers know the distribution of offers available in the market. In many markets the consumers who are most familiar with this distribution are the consumers who search the least (Moorthy et al. 1997). Therefore, it may be unreasonable to assume that consumers know the distribution in circumstances where they purchase infrequently or where a large percentage of consumers are new to the market. Uncertainty about the distribution can have many implications for search behavior depending on the form uncertainty takes. In this article I focus on circumstances where theory predicts that learning will result in declining reservation values similar to those frequently seen in field and experimental search behavior (e.g. Brown et al. 2011). Most papers in the theoretical literature agree that with well behaved priors, searchers who are uncertain about the distribution of values will use a “one step” reservation (or cutoff) strategy similar to those common in standard search models.Footnote 1 “One step” here means that the searcher is deciding between accepting the best offer currently available and one additional search (Burdett and Vishwanath 1988; Dana 1994; Bikhchandani and Sharma 1996; Dubra 2004; Mauring 2017; Kaya and Kim 2018).Footnote 2 Furthermore, the initial reservation value of a purely Bayesian learner should be above that which would be used for the expected distribution from the priors, and reservation values decline as a search spell continues (Burdett and Vishwanath 1988; Bikhchandani and Sharma 1996; Dubra 2004). The high initial reservation value comes not from a desire to gather information about the distribution, but from the fact that a high observation will lead to more optimistic posteriors and therefore a more persistent searcher. It takes a very high draw to cause the Bayesian learner to stop after just one search. Declining reservation values stem from the fact that a low observation is viewed not just as an unlucky draw, but as bad news about the distribution. Consequently this low draw will lead to posteriors which are more pessimistic than the priors. A high draw may be sufficient to make the searcher more optimistic, but any draw which would be high enough to raise the reservation utility above its previous value will also be above that reservation value, meaning that the search spell ends before the reservation value can increase.Footnote 3 I refer to this elimination of searchers who would raise their reservation values from the search process as the selection effect, whereby searchers who would increase their reservation values are filtered out of the data because they cease searching. This selection effect is important because it could serve as a potential explanation for real world behavior which is inconsistent with standard sequential search theory without learning. Most notably, the decline in reservation values means that a rational searcher will sometimes exercise recall or exit the market in an environment with learning where they would not if they had full information. In a policy context this has important implications for labor market search. In particular, the increasing pessimism of job searchers could exacerbate the standard supply/demand dynamics that lead to low wages in a loose labor market. For managers of firms, particularly firms which are acting as marketplaces, learning means that consumers need to be shown appealing products early in their search process. Otherwise they are likely to believe that the products on offer are low-quality or overpriced, which can lead to consumers exiting the market entirely (e.g. Nosko and Tadelis 2015). The selection effect has been thoroughly explored in the theoretical literature, but it has not previously been tested in an experimental environment. In this article I attempt to fill this gap in the experimental literature by presenting an experiment which compares subjects’ behavior in a simulated consumer search environment with a known distribution (the “full information treatment”) to their behavior when they are updating over a set of priors (the “learning treatment”). I elicit reservation values using a method similar to Jhunjhunwala (2018) and Brown et al. (2011). Subjects exhibit declining reservation utility in both treatments, consistent with the previous experimental literature, but the rate of decline is much stronger in the learning treatment, suggesting that learning is a plausible explanation for the recall seen in real world search data.Footnote 4 While the selection effect appears to work well in this experimental environment, the prediction that subjects will use a one-step reservation value does not. The change in the initial reservation values across treatments is inconsistent across subjects. Higher ability subjects—as measured by numeracy metrics and academic data—set higher reservation values than in the full information treatment and often maintained these values for several searches. Lower ability subjects tended to decrease their initial reservation values in the face of uncertainty. In a post-session questionnaire, those subjects who raised their initial reservation values expressed a desire to gain information about the distribution. This heuristic (gathering information and then deciding how to react) requires much less computation than the theoretically optimal strategy and performs quite well in terms of search payoffs. Additionally, it results in behavior which is quite similar to the “search funnel” described in the marketing literature, whereby consumers first engage in broad searches within a market to learn about the products on offer and their prices before narrowing their search once they have this information. Previous discussions of this search funnel have suggested that it may result from searchers’ imperfect knowledge of their own tastes (Blake et al. 2016). However, taste variation is not a factor in my experiment. The similarity of behavior between the subjects in this article and the description of the search funnel suggests that learning about the market may be sufficient explanation for this phenomenon. This information heuristic is likely to be of interest to researchers studying learning in complex environments. The general form of separating an information gathering “explore” stage from payoff relevant “exploit” behavior seems like it would apply to many economic contexts. For example, subjects in a market experiment who are setting prices against uncertain demand might prefer to test out several prices to get a sense of the market before they focus on maximizing profits. The heuristic performs well in this search environment, but the penalty for deviating from optimal behavior might be much larger in a different setting. Despite some subjects setting higher initial reservation values in the learning treatment, most subjects in this experiment drastically under-search relative to the theoretical optimum. I provide evidence that this is a result of the structure of the game failing to give them negative feedback when they set a reservation value which is too low. Subjects who draw a value close to their reservation value are much more likely to increase their reservation value in the subsequent search. This draw makes the possibility of settling for a low value more salient and encourages them to become more persistent. Stopping with a value high above the reservation value does not intuitively feel like negative feedback, but stopping with a value close to or below the elicited reservation value (the latter due to exercising recall) does. Therefore the only significant change between search spells is subjects who reduce their initial reservation values in the following spell after receiving this “negative” feedback. This pattern is suggestive of a situation similar to Charness and Levin (2005), where the obvious reinforcement heuristic leads to actions opposite of those which would be indicated by Bayesian rationality.",5
24.0,1.0,Experimental Economics,26 June 2020,https://link.springer.com/article/10.1007/s10683-020-09661-z,Dimensions of donation preferences: the structure of peer and income effects,March 2021,Michalis Drouvelis,Benjamin M. Marx,,Male,Male,Unknown,Male,"There are a variety of reasons that an individual might donate to charity. An understanding of donation preferences can inform the design of mechanisms that address the expected underprovision of donated goods. Much research in economics examines preferences over charitable giving through the framework of the impure altruism model of Andreoni (1989), in which donors may value the public benefits of their donations or the “warm glow” they feel from their own donation. Researchers have devised experiments that have identified motives for giving that include pure altruism (Ottoni-Wilhelm et al. 2017), income effects (Cherry et al. 2002; Cherry and Shogren 2008), and a variety of factors that can produce warm glow, such as reciprocity (Falk 2007; Croson 2008), signalling (Glazer and Konrad 1996; Andreoni and Bernheim 2009; Ariely et al. 2009; Bracha et al. 2011), social pressure (DellaVigna et al. 2012; Andreoni et al. 2017), and social norms (Krupka and Croson 2016), among others. Given the variety of identified motivations for charitable giving, can we describe preferences with a single utility function? To which motivation(s) should charities appeal? When multiple techniques can appeal to a similar motivation, which should the fundraiser use? Are there diminishing returns to each technique? Are the individuals who respond to gifts from the charity the same individuals who respond to announcements about others’ giving? How should charities target and sequence their appeals? In this study, we explore these questions with an approach that has not been commonly used in experimental studies to-date. Our goal was to estimate a flexible utility function for each subject, allowing each to exhibit different motives for giving, in order to evaluate a variety of potential fundraising mechanisms. Estimating such flexible utility functions requires multiple sources of variation. For example, DellaVigna et al. (2012) used three fundraising treatments and a variety of survey treatments to estimate a two-parameter distribution of utility function parameters and costs associated with giving. Ottoni-Wilhelm et al. (2017) estimated a two-parameter utility function for each subject using a within-subject design varying endowments and the contributions of others across six scenarios. Our study follows these examples and expands the design to incorporate a number of factors that have been found to affect charitable giving. The design of our experiment varied multiple treatments both between and within subjects to provide a uniquely comprehensive characterization of donation preferences. Subjects were paid a piece rate for real-effort tasks, one of which was randomized to induce predictable variation in earnings. When subjects were informed of their earnings, they were asked if they would like to donate to a local charity. Subjects were then informed that they would be shown several scenarios, that they could choose a different donation amount for each scenario, and that one of these scenarios would be selected at random for implementation. Across scenarios, we allowed subjects to condition their donations on many different inputs, including the levels of bonus income, donations of concurrent subjects, and a donation by an anonymous donor. We also included a between-subjects treatment that informed subjects of either a high or low level of average donations in recent laboratory sessions. We obtain a variety of results that paint a complex picture of motives for giving. Peer effects are positive, with subjects’ donations increasing in those of labmates and past subjects. However, subjects did not respond to labmate donations from bonus income or to gifts by an anonymous donor. Income effects exhibit similarly strong dependence on the source: a £1 increase in earned income had no effect on donations (estimate of £0.008 with standard error 0.067), yet paying subjects a £1 bonus increased the average donation by £0.38 (standard error 0.04). These results were robust across subject nationality, subject responses to explanation of the design, and two randomly assigned orders of the treatment scenarios. Though the average treatment effects were similar across groups of subjects, there was considerable heterogeneity in the responses. Comparing responses across what we label “social treatments” provides evidence of a split between individuals whose preferences depend on others’ choices and individuals whose preferences do not. The relative strength of responses to each social treatment suggests that individuals are most responsive to the choices of their nearest peers. There is also a strong correlation between a subject’s giving from own bonus income and giving from others’ bonus income, but these responses are not highly correlated with responses to social treatments. Thus, it appears that there are peer-effect and income-effect components to multi-dimensional donor types. We argue that our results are most consistent with a model of pure warm glow driven by a preference to comply with an uncertain social norm.Footnote 1 In this model, subjects learn about the norm from donations by others. We estimate this model for each individual subject using donation choices, beliefs about what others have donated (which we elicited with financial incentives), and the parameters of the scenarios they faced. We account for non-negativity of donations and allow subject perceptions and preferences to vary. Our model allows us to estimate donations under counterfactual fundraising campaigns. We find that the charity would not benefit from simply providing individuals with either bonuses or information about the average donation of others. There are solicitation strategies using bonuses targeted to particular types of donors that can increase net donations, but these require a degree of information that would be rare for a charity. A simpler strategy can increase donations by an estimated 30%, however, by first soliciting those who are not motivated by their peers’ donations and then announcing the average donation in this first round to the individuals who will respond positively to their peers’ donations. However, even this simpler strategy may be difficult to implement in that it would require charities to gauge donor types prior to the fundraiser. Our experiment follows several others that have estimated income effects and peer effects in charitable giving. Our estimates of the causal effects of income on donations employ a novel strategy of randomly assigning tasks, yet give results similar to those in the literature on “house money” or “windfall money” effects (Cherry et al. 2002, 2005; Harrison 2007; Kroll et al. 2007; Erkal et al. 2011; Reinstein and Riener 2012; Carlsson et al. 2013). We generally find positive peer effects, consistent with Drouvelis and Marx (2018) and with past work finding positive effects of the perceived donations of others (Hermalin 1998; List and Lucking-Reiley 2002; Vesterlund 2003; Andreoni 2006a; Potters et al. 2007; Shang and Croson 2009; Huck and Rasul 2011; Karlan and List 2012; Smith et al. 2013; Huck et al. 2015; Kessler 2017).Footnote 2\(^{,}\)Footnote 3 We build on these findings by examining heterogeneity across subjects, as in studies of conditional cooperation in public goods games (Fischbacher et al. 2001; Gächter 2007). We find heterogeneity in multiple dimensions of preferences, and we document correlations between them, which allows us to refine the model of donor preferences and provide guidance for increasing donations. These studies make up part of a larger literature on donation preferences. The broader literature has been reviewed by Andreoni (2006b), Schokkaert (2006), List (2011), Andreoni and Payne (2013), and Vesterlund (2016). In a prominent example of this literature, Crumpler and Grossman (2008) provide evidence for warm glow by showing that subjects donate even when they know their donations will be offset so as not to affect the allocation to the recipient. Gangadharan et al. (2018) use this method to identify warm glow givers and then identify altruism by giving the same subjects the opportunity to transfer more money to the recipient. Both Gangadharan et al. (2018) and Ottoni-Wilhelm et al. (2017) identify altruism by pairing givers with individual recipients in order to control the level of giving by others. Studies tend to find stronger evidence for warm glow when subjects are simply one of many contributors to a charitable organization (e.g., volunteering in Brown et al. 2019) and when donation amounts are small (DellaVigna et al. 2012; Karlan and Wood 2017). Consistent with the latter types of studies, we only find evidence of warm glow in our setting of giving to a charity following a transaction, as in “point-of-sale” campaigns in retail stores and restaurants.Footnote 4 While such campaigns raise small amounts from each donor, these amounts can add up, as shown by 73 campaigns that raised over $440 million in 2016 (engageforgood 2017). Our study offers several contributions to the body of research on charitable giving. We find that our subjects’ behavior is consistent with past work on income and peer effects but does not replicate the finding of positive effects from anonymous lead donations. We also find that subjects do not respond to our novel treatments that increase the giving of their labmates by providing them with bonuses. By varying giving by others in multiple ways, these treatments provide more robust evidence that subject choices are not motivated by the total amount donated, as they would be in the altruistic model. If subjects are indifferent to the amount received by the charity, then peer effects, which the literature has shown could arise through quality signaling or threshold effects in the level of the public good, instead appear to be best explained by more social factors such as norms. We show that subjects vary in their responsiveness to this social influence as well as their propensity to donate windfall income, dimensions of donation preferences that our design allows us to show are largely uncorrelated. We then estimate counterfactuals that demonstrate how identifying donor types could allow charities to improve the timing and targeting of their solicitations. The paper proceeds as follows. Section 2 describes the design of the experiment. Results of the experiment appear in Sect. 3. Section 4 proposes a model that synthesizes the results and presents estimates of the model and implied counterfactuals. Section 5 concludes.",12
24.0,1.0,Experimental Economics,03 July 2020,https://link.springer.com/article/10.1007/s10683-020-09663-x,Closing a mental account: the realization effect for gains and losses,March 2021,Christoph Merkle,Jan Müller-Dethard,Martin Weber,Male,Male,Male,Male,"Many risky endeavors, be it a night at the casino or an investment in a stock, involve instances in which individuals must decide whether to continue, to abandon, or to double down on a previous decision. They often view such episodes in isolation, even though normative theory suggests integrating them into a broader perspective of total wealth. They instead engage in mental accounting (Thaler 1985, 1999), which refers to a cognitive process to categorize outcomes by their source or purpose. Prior outcomes within a mental account, perceived as a gain or a loss, obtain special relevance for this account and affect subsequent risk-taking (Thaler and Johnson 1990). The direction of this influence has been subject to a long-standing debate. After losses, many studies find that individuals become more risk-seeking (Coval and Shumway 2005; Weber and Zuchel 2005; Langer and Weber 2008; Andrade and Iyer 2009), while others report they become more risk-averse (Massa and Simonov 2005; Shiv et al. 2005; Frino et al. 2008). Similarly, after gains, investors will either exhibit more risk-seeking behavior (Thaler and Johnson 1990; Weber and Zuchel 2005; Suhonen and Saastamoinen 2018) or more risk-averse behavior (Kahneman and Tversky 1979; Clark 2002; Coval and Shumway 2005). Existing theory can account for these different reactions by a variety of models or arguments. On the one hand, risk-seeking behavior after a prior loss and risk-averse behavior after a prior gain are often explained by prospect theory (Kahneman and Tversky 1979). After a loss, the relevant part of the prospect theory value function to evaluate further outcomes is convex, which implies risk-seeking behavior. In contrast, a prior gain will situate a person in the gain domain for which the value function is concave, which implies risk-averse behavior. On the other hand, more risk-seeking behavior after gains and more risk-averse behavior after losses can be motivated by the house money effect (Thaler and Johnson 1990) and the hedonic editing hypothesis (Thaler 1985). The house money effect describes a situation in which prior gains can be used to wager in subsequent gambles. People find it easier to part with money not coming from their own pocket. In addition, hedonic editing allows them to offset future losses against earlier gains. For losses, it is argued that they become more painful when they follow on the heels of prior losses (Barberis et al. 2001). A unifying framework to resolve the conflicting evidence has been recently proposed by Imas (2016). It builds on the distinction between realized and unrealized outcomes, whereby a realization is defined “as an event in which money or another medium of value is transferred between accounts” (Imas 2016, p. 2091). He argues that individuals behave differently depending on whether a loss is realized or whether it is still unrealized (a paper loss). Experimentally, he replicates prior findings that participants become more risk-averse after a realized loss, while they become more risk-seeking after a paper loss. He labels the difference in risk-taking between paper and realized losses the “realization effect” and explains its occurrence with cumulative prospect theory (Tversky and Kahneman 1992) and choice bracketing (Read et al. 1999; Rabin and Weizsäcker 2009), an idea directly related to mental accounting. The proposed framework sheds light on why both, risk-averse as well as risk-seeking behavior, can be observed after the same prior outcome. However, drawing general conclusions from realization for subsequent risk-taking requires some caution. First, Imas’s (2016) theoretical and experimental elaboration focuses exclusively on losses, and second, it tests the realization effect for an investment opportunity with a positively skewed distribution of outcomes. We argue that the literature is still in need of empirical and theoretical clarification about how prior outcomes—losses as well as gains—affect subsequent risk-taking, and in particular, under which conditions a distinction between paper and realized outcomes leads to differential risk-taking behavior. In this study, we contribute to this goal by examining two major research questions: (1) Does the realization effect exist for gains as well? (2) Does the realization effect depend on the skewness of the underlying investment opportunity? To this end, we derive theoretical predictions for risk-taking behavior after gains and investment opportunities with positive skewness, no skewness, and negative skewness. We model loss-averse investors who open a mental account at the beginning of an investment episode and close it upon realization. Paper gains and losses alter the balance of the mental account and can thereby affect risk-taking. Paper gains act as a cushion against future losses and thus invite higher risk-taking, which is absent after gains are realized. We thus predict a realization effect for gains. Skewness comes into play mainly via the size of potential gains and losses relative to the account balance. With non-positive skewness, losses become less probable but larger. They threaten to exceed the paper gain cushion, attenuating the realization effect after gains. Likewise, after paper losses, more probable but smaller gains take away the potential to break even, which is a major motivation for higher risk-taking after losses. We thus predict a smaller or absent realization effect for non-positively skewed lotteries. We conduct three well-powered experiments to test these predictions. In the first experiment, we replicate the main experiment by Imas (2016) using an identical design, which examines a series of positively skewed investment opportunities. The importance of replication for scientific progress in economics has been highlighted recently (Maniadis et al. 2014; Camerer et al. 2016; Christensen and Miguel 2018). At the same time, the experiment allows us to address the first research question about a realization effect for gains. Not only is risk-taking after gains arguably as important as after losses, but it shares a similar conflict in previous empirical results and theory. If there is evidence for a realization effect in the gain domain as we predict, the proposed framework would have broader implications than those already suggested for the loss domain. To answer the second research question, we analyze in two further experiments boundary conditions for the realization effect. In particular, we depart from positively skewed lotteries used so far and examine how symmetric or negatively skewed lotteries affect risk-taking behavior after paper and realized outcomes. Not only does positive skewness encourage risk-seeking behavior as it is often associated with gambling (e.g., lotteries or casinos), but the underlying distributions of most financial investment opportunities (e.g., stocks or funds investments) are less or not at all positively skewed. In order to establish the validity of the realization effect for these settings, it is essential to confirm whether the effect is indeed reduced as theory predicts. The first experiment, which replicates study one by Imas (2016), involves a sequence of four positively skewed lotteries, each of which represents the throw of a die. One lucky number (out of six) wins seven times the stake invested in the lottery, while the stake is lost for all other outcomes. Up to EUR 2.00 can be invested in each lottery. After the third lottery, previous earnings are either paid out to participants or remain unrealized, which defines the two treatments in the experiment (realization treatment and paper treatment). The relevant comparison then is what participants do in the fourth and final lottery depending on realization. We use a larger sample size (N = 203) than the original study to ensure sufficient statistical power and to be able to examine outcome histories that occur less frequently. We first confirm that participants take less risk after a realized loss compared to a paper loss. However, the difference of 16 cents in average invested amounts between treatments is smaller than in the original experiment (38 cents), and the realization effect is not statistically significant. While we confirm a decrease in risk-taking in the realization treatment, we cannot corroborate an increase in risk-taking in the paper treatment. Standard replication measures show that the replication is at least partially successful. Exploiting observations in which participants have obtained a gain at the time of realization, we find a similar investment pattern as for losses. Participants take significantly less risk after a realized gain than after a paper gain. The realization effect is larger for gains than for losses with a difference of 22 cents in average investment between treatments. In the paper treatment, participants seem to gamble with the house’s money, while in the realization treatment, they have closed the mental account and regard gains from the lottery as their personal money. Given the consistent direction of the realization effect for gains and losses, we test for the realization effect unconditional of a particular outcome history. The results show a positive and strongly significant realization effect (\(p<.01\)) in the full sample. In addition to our own experimental data, we analyze data from the original study by Imas (2016) with respect to gains.Footnote 1 Although limited in the number of observations, the realization effect for gains is strong and consistent with our results. Thus, we find evidence for a realization effect for gains in two independent samples. Moreover, pooling the data from both studies, we find a positive and strongly significant realization effect (\(p<0.001\)) for gains and losses. To test for the theoretical relation between the realization effect after gains and the house money effect, we examine the invested amounts after a paper gain. In almost all cases, participants do not invest more than what they have gained in the lotteries. This implies that they gamble with the house’s money, but do not touch their initial experimental endowment. In experiments two and three, we examine how other distributions of outcomes affect risk-taking behavior after paper and realized gains and losses. We keep the basic experimental setup but change the probability of gains. Instead of a positively skewed lottery, participants invest in a symmetric or negatively skewed lottery, respectively. By construction this also increases the heterogeneity of outcome histories prior to realization. We find neither in the symmetric lottery nor in the negatively skewed lottery a statistically significant realization effect for gains or losses (total sample size N = 304). In contrast to the positively skewed environment in the first study, participants tend to invest similarly after a paper outcome and a realized outcome. This finding is in line with theoretical work by Barberis (2012) and Imas (2016) in which individuals form contingent plans over a sequence of lotteries. The realization effect across all experiment. The figure displays average changes in risk-taking after paper and realized outcomes unconditional of the prior outcome history, and split by loss and gain for positively skewed, symmetric, and negatively skewed lotteries. The error bars show 90%-confidence intervals. (Color figure online) The results across all experiments suggest boundary conditions for the realization effect. Figure 1 depicts the magnitude of the realization effect we find, conditional on the outcome history as well as the skewness of the investment opportunity. Increased risk taking after paper gains and losses requires positive skewness, while decreased risk-taking after realized gains and losses does not. The absence of the realization effect for non-positively skewed lotteries is thus primarily driven by an absence of increased risk-taking after paper outcomes. This includes the absence of loss chasing, which seems to be limited to positive skewness environments. The remainder of the paper is organized as follows. In Sect. 2, we derive theoretical predictions for the experiments, in particular for risk-taking behavior after gains and lotteries with different skewness, and review the prior literature. Section 3 presents the experimental design and the main results. A final section concludes.",8
24.0,1.0,Experimental Economics,06 May 2020,https://link.springer.com/article/10.1007/s10683-020-09656-w,Does level-k behavior imply level-k thinking?,March 2021,Ye Jin,,,,Unknown,Unknown,Mix,,
24.0,1.0,Experimental Economics,13 January 2021,https://link.springer.com/article/10.1007/s10683-020-09699-z,Correction to: Does level-k behavior imply level-k thinking?,March 2021,Ye Jin,,,,Unknown,Unknown,Mix,,
24.0,2.0,Experimental Economics,12 August 2020,https://link.springer.com/article/10.1007/s10683-020-09673-9,The impact of taxes and wasteful government spending on giving,June 2021,Roman M. Sheremeta,Neslihan Uler,,Male,Female,Unknown,Mix,,
24.0,2.0,Experimental Economics,10 September 2020,https://link.springer.com/article/10.1007/s10683-020-09676-6,Integration and diversity,June 2021,Sanjeev Goyal,Penélope Hernández,Angel Sánchez,Male,Female,Male,Mix,,
24.0,2.0,Experimental Economics,28 June 2020,https://link.springer.com/article/10.1007/s10683-020-09664-w,"Bubbles, crashes and information contagion in large-group asset market experiments",June 2021,Cars Hommes,Anita Kopányi-Peuker,Joep Sonnemans,Unknown,Female,Male,Mix,,
24.0,2.0,Experimental Economics,17 July 2020,https://link.springer.com/article/10.1007/s10683-020-09667-7,Experiments on centralized school choice and college admissions: a survey,June 2021,Rustamdjan Hakimov,Dorothea Kübler,,Unknown,Female,Unknown,Female,"For a long time, economists have focused on markets where prices coordinate demand and supply. However, in many markets, prices do not determine who receives what. Examples include matching markets such as entry-level labor markets, school choice, university admission, social housing allocation, and kidney exchange. Over the past decades, the study of matching markets has become an active area of research. These markets have in common that agents have preferences over other agents or over objects they can be matched to. For instance, workers have preferences over firms, or students have preferences over universities. Many of these markets are centralized where a clearinghouse collects the preferences from the agents and uses a mechanism to determine the matching which satisfies the designer’s objectives, like efficiency or fairness. Economists have been involved in re-designing centralized matching markets, canonical examples being the National Resident Matching Program for young doctors in the US (Roth and Peranson 1999) and school choice in Boston (Abdulkadiroğlu et al. 2005). A growing interest in the topic as well as novel questions arising when analyzing existing matching procedures have fueled the rapid progress of research on the topic, and a considerable fraction of this research employs experiments. This survey provides a comprehensive overview of the experimental literature on centralized matching that is based on the school choice and the college admissions model. It thereby complements the chapter on experiments in market design by Roth (2015).Footnote 1 The two models speak to a number of practical applications such as school choice programs, centralized university admissions, the allocation of public housing, and entry-level labor markets, among others. Almost all of the experiments are lab experiments. Field experiments on matching are faced with the difficulty that the preferences of participants are not known, but we report on two papers that find a way around this limitation (Guillen and Hakimov 2018; He and Magnac 2017). The goal of the survey is not only to summarize the main experimental findings but also to identify what appear to be robust results across studies. To do so, we provide statistics across studies if possible, and also compare the results of related studies. Finally, by grouping the articles into a set of topics, we structure the current state of research. Experiments are useful tools in the domain of market design for a number of reasons: Experiments can be used to demonstrate problematic aspects of existing mechanisms, also vis-à-vis policymakers. For instance, the large number of participants misrepresenting their preferences in the mechanism that was employed in Boston to allocate school seats has been demonstrated with the help of experiments which helped to convince practitioners. Also, experiments have permitted researchers to identify the causes of market failure, which is often impossible with observational data alone. In many matching markets, the market participants are inexperienced. Thus, the equilibrium predictions that rely on fully rational players might not be in line with actual behavior. Understanding whether a new mechanism is behaviorally robust with the help of experiments is crucial. For example, one of the central themes of market design is whether participants have an incentive to reveal their preferences truthfully and whether they behave in line with this incentive. Experiments play a primary role in testing the theoretical predictions regarding preference reporting. The advantage of experiments is that preferences can be induced by the experimenter, e.g., by assigning monetary payoffs for being matched to different schools, and are therefore fully controlled for. If the properties of the outcomes of a mechanism are analyzed under the assumption that participants state their preferences truthfully or that they play the equilibrium strategy, this can lead to wrong conclusions regarding the desirability of the mechanism. The efficiency of the allocation has to be calculated based on the true preferences of participants, which are hard to know from field data. Thus, experiments are a handy tool for the comparison of mechanisms, since they allow for testing whether subjects understand the incentive properties of the mechanisms and for comparing allocations based on the true preferences of participants. Experiments enable researchers to identify the factors that influence the agents’ strategies, such as the information available to them about the preferences of other market participants, the size of the market, and so on. Experiments can be used to test a new mechanism before the mechanism is implemented on a large scale with real consequences. By creating counterfactual situations, experimenters can use the lab as a testbed for new mechanisms.Footnote 2 While experiments play an important role for the study of centralized matching markets, market experiments are complex and have many degrees of freedom regarding their design. For example, school choice problems are characterized not only by the preferences of participants, but also by the size of the market, by whether schools have preferences or follow rules when ranking students, the amount of information provided about own and others’ preferences, and the capacities of schools. For this reason and because the literature is still relatively young, there are fewer replications than in other areas of experimental economics. Nevertheless, we believe that there is a lot to be learned from relating the existing work to each other. Thus, we compare experiments that share a number of similarities even if they differ with respect to some other features of the markets. Overall, we find a great level of consistency in the findings with clear patterns of behavior emerging. At the end of each subsection, we provide a short summary of the main findings. Most of the results that we review concern individual behavior, namely the input of subjects into the mechanisms. The studies consider whether participants report truthfully in strategy-proof mechanisms and manipulate optimally in the ones where manipulations are part of an equilibrium. The rates of equilibrium reporting often have a direct effect on the properties of the resulting allocation. However, different subjects can have a different influence on the allocation with their preference reports. Moreover, subjects often have a weakly dominant strategy of reporting truthfully in strategy-proof mechanisms, and thus not every deviation from truthful reporting influences the resulting allocation. For this reason, some papers (typically papers that compare allocations reached by different mechanisms) study the stability and Pareto efficiency of the matching outcomes. Some papers emphasize efficiency, others stability, depending on the main interest and the mechanisms studied. For instance, if allocations reached under the student-proposing deferred acceptance (DA) mechanism are analyzed, the emphasis is on stability, as DA is predicted to produce stable allocations which do not have to be efficient. In the case of the top trading cycles mechanism (TTC), the emphasis is on efficiency, as TTC is predicted to produce Pareto-efficient allocations that are not necessarily stable.",24
24.0,2.0,Experimental Economics,01 August 2020,https://link.springer.com/article/10.1007/s10683-020-09669-5,Playing the field in all-pay auctions,June 2021,Daniel G. Stephenson,Alexander L. Brown,,Male,Male,Unknown,Male,"Nash equilibrium often provides an approximate characterization of long-run behavior, but the short-run behavior of inexperienced agents often bears little resemblance to equilibrium predictions (see Davis and Holt 1993). In all-pay auctions, Nash equilibrium can involve non-uniform mixed strategies over a continuum of actions. Considering the complexity of such equilibria, it seems unsurprising that inexperienced subjects would fail to play them. Previous studies find that subjects consistently overbid relative to Nash equilibrium and often play dominated strategies by overbidding beyond the actual value of the prize (Dechenaux et al. 2014; Gneezy and Smorodinsky 2006; Lugovskyy et al. 2010). After many periods of experience, overbidding often decreases, but is not eliminated (Davis and Reilly 1998; Lugovskyy et al. 2010). It remains unclear how much overbidding would persist in the long run as subjects continue to gain experience.Footnote 1 Experimental procedures designed to match the assumptions of long-run play, such as continuous-time experimental protocols, may help address this question. This paper reports the first experimental investigation of the all-pay auction in continuous time. Subjects could adjust their bids at will, receive almost instantaneous feedback, and earn mean-matching payoffs continuously over time. Continuous-time experimental protocols effectively reduce the length of discrete periods to near instants. Mean-matching protocols provide subjects with information about their expected payoff from being randomly matched against others. Together, these experimental protocols provide subjects with far more experience in a fraction of the time relative their conventional discrete-time counterparts, allowing boundedly rational subjects to more easily assess the expected profitability of their strategies. In contrast to previous experimental studies of the all-pay auction—all of which used discrete-time protocols—mean bids did not exceed the Nash predictions. On average, subjects slightly underbid relative to Nash and earned positive payoffs. Subjects rarely played dominated strategies; bids only exceeded the value of the item in roughly 3% of observations. These initial findings suggest that previous overbidding results may not apply to settings where individuals accumulate a large amount of experience, a result that may have bearing in the field. As Dechenaux et al. (2014) notes, empirically studying applications of the all-pay auction is often infeasible as the cost of effort is frequently unobservable. While experiments are imperfect substitutes for field settings, they provide an alternative method of investigation. If competitors in the field have extensive experience, we should cautiously interpret the overbidding observed in past discrete-time experimental studies where subjects received far less experience. The use of continuous time experimental protocols may provide a bridge from these conventional discrete-time laboratory settings to field settings where agents accumulate extensive experience. While average bids did not exceed Nash predictions, aggregate bidding behavior did exhibit strong cyclical patterns that run contrary to Nash predictions. Evolutionary models predict cyclical behavior in all-pay auctions because outbidding one’s opponent is optimal only if their bid is below the value of the prize. Once bids are at or above the value of the prize, the best response is to bid zero and the process repeats. This results in a cyclical pattern of gradual bid increases followed by sharp decreases. Consequently, all-pay auctions provide a particularly informative test of evolutionary models as their evolutionary predictions can be very different from their equilibrium predictions. The new application of continuous-time and mean-matching experimental protocols to this environment mirrors the structure of evolutionary models. We test evolutionary dynamics in two distinct auctions as between-subjects experimental treatments. In auction 1, two bidders compete for a single prize. In auction 2, three bidders compete for two prizes. Nash equilibrium predicts a fixed distribution of bids in each auction. In contrast, evolutionary models predict cyclical behavior in both auctions, convergence to a fixed point in auction 1, and persistent non-convergence in auction 2.Footnote 2 Aggregate behavior exhibited persistent cycles in both auctions and greater instability in auction 2 than 1. Because these results are predicted by evolutionary models but not by conventional fixed-point models,Footnote 3 they illustrate how evolutionary models can provide additional substantive information not found in conventional models. This contribution is important for two distinct reasons. First, the all-pay auction is applicable to a rich set of concrete field environments such as political lobbying (Baye et al. 1993), patent races (Marinucci and Vergote 2011), biological competition (Chatterjee et al. 2012), and international warfare (Hodler and Yektaş 2012). For practitioners who encounter such settings in the field, evolutionary models can identify which strategic environments are likely to exhibit long-run behavior that approaches equilibrium predictions. Second, while other studies observe cycles and instability in games with a relatively small number of actions (i.e., 2 or 3), the continuous-action space of the all-pay auction allows us to provide evidence for the applicability of evolutionary models to a much wider class of environments (see Sect. 2 for more detail). This includes a variety of concrete settings where evolutionary models are employed such as finance (Hens and Schenk-Hoppé 2005), bargaining (Abreu and Sethi 2003), and industrial organization (Alós-Ferrer et al. 2000). When evolutionary models predict convergence, our results suggest that equilibrium models can often provide a useful characterization of long-run aggregate behavior. Conversely, when evolutionary models predict non-convergence, our results suggest that equilibrium models often fail to characterize long-run aggregate behavior. Even when equilibrium models are unreliable, our results suggest that evolutionary models can provide practitioners in the field with a useful characterization of behavioral dynamics.Footnote 4 The remainder of this paper proceeds as follows: Sect. 2 discusses the related literature. Section 3 presents the theoretical predictions. Section 4 describes the experimental design and procedures. Section 5 presents the experimental hypotheses. Section 6 provides the main results. Section 7 discusses our main findings and provides context.",6
24.0,2.0,Experimental Economics,23 July 2020,https://link.springer.com/article/10.1007/s10683-020-09670-y,Reluctant donors and their reactions to social information,June 2021,David Klinowski,,,Male,Unknown,Unknown,Male,"Social information can influence giving behavior through a variety of mechanisms. Theoretical and experimental work (reviewed in the next section) has highlighted the role of motivations such as altruism, conformity, and reciprocity in shaping how individuals might change their donations in response to learning how much others donate. However, a growing body of research has shown that seemingly kind behavior such as giving to charity is often done reluctantly, out of image concerns. People often contribute because they do not want to appear selfish, either to themselves or to (imagined) others, but they would prefer to keep their money and would have kept it had they found an opportunity to do so without compromising their moral image. How will an individual with such motivation respond to social information? Answering this question is important because it can help us to better understand when and why social information is effective at encouraging donations. As the results in this paper show, taking account of image concerns and the tendency of individuals to exploit “moral wiggle room” in order to avoid giving is important also because it can lead us to recognize that the timing of the information—and not just its content, as is generally the focus of the literature—can play an important role in shaping the response to social information. In this paper, I investigate whether the response to social information stems from “reluctant” giving, and if so, how this motive affects the response to the information. To do so, I conduct an experiment that separates the effect of the information on the decision to give (the extensive margin) from its effect on the decision of how much to give (the intensive margin). Subjects in the experiment make the two decisions one after the other, and receive information on a small or a large previous donation either before or after the decision to give, which allows me to study the effect of the information on each margin of giving separately. If giving is motivated by altruism, conformity, or reciprocity, one would expect consistent reactions to the information independently of when the information is provided. But if giving is motivated by image concerns, we may see conflicting reactions to the same information at different decision stages: if information on a large donation makes opting out more socially acceptable, but conditional on opting in it makes giving a small amount less socially acceptable, this information will discourage giving when provided during the extensive-margin decision, but will encourage giving when provided during the intensive-margin decision. Results from the experiment match the image-concerns prediction. To provide additional evidence that the response to the information was motivated by image concerns, I also show that a preference for quietly exiting a subsequent dictator game—behavior that suggests sensitivity to social and self-image—is correlated with responding to the information in the direction of the image-concerns prediction, at least for the extensive-margin decision (examining the correlation with the intensive-margin decision was infeasible due to the small sample of positive donations). Similarly, scoring high on neuroticism, a personality trait associated with anxiety from social evaluation and thus potentially indicative of image concerns, was also predictive of the response to the social information at the extensive margin, while no other personality trait was. Thus, taken together, the results indicate that individuals can respond to social information from a combination of self-interest and an attempt to manage the self-image. As discussed in the concluding section, this has implications for fundraising mechanism design, and for the inferences we can draw about donor motives and welfare from observed changes in giving in response to social information. In the remaining of the paper, Sect. 2 reviews the literature, Sect. 3 describes the experimental design, Sect. 4 outlines the hypotheses behind the empirical tests, Sect. 5 presents the results, and Sect. 6 discusses some implications of the results.",7
24.0,2.0,Experimental Economics,02 August 2020,https://link.springer.com/article/10.1007/s10683-020-09671-x,Deterring collusion with a reserve price: an auction experiment,June 2021,Pacharasut Sujarittanonta,Ajalavat Viriyavipart,,Unknown,Unknown,Unknown,Unknown,,
24.0,2.0,Experimental Economics,26 August 2020,https://link.springer.com/article/10.1007/s10683-020-09672-w,Cognitive sophistication and deliberation times,June 2021,Carlos Alós-Ferrer,Johannes Buckenmaier,,Male,Male,Unknown,Male,"Economic agents form different expectations and react differently even when confronted with the same information, leading to substantial behavioral heterogeneity, which in turn has long been recognized as a fundamental aspect of economic interactions (e.g., Haltiwanger and Waldman 1985; Kirman 1992; Blundell and Stoker 2005; Von Gaudecker et al. 2011). A key source of heterogeneity is the fact that cognitive capacities differ among individuals, as does the motivation to exert cognitive effort. This observation has given rise to a rich theoretical literature on iterative or stepwise reasoning processes, including level-k models (Stahl 1993; Nagel 1995; Stahl and Wilson 1995; Ho et al. 1998) and models of cognitive hierarchies (Camerer et al. 2004). Such models endow individuals with differing degrees of strategic sophistication or reasoning capabilities, and might hold the key to describe heterogeneity in observed behavior (for a recent survey, see Crawford et al. 2013). In particular, they have proven invaluable to explain behavioral puzzles as overbidding in auctions (Crawford and Iriberri 2007), overcommunication in sender-receiver games (Cai and Wang 2006), coordination in market-entry games (Camerer et al. 2004), and why communication sometimes improves coordination and sometimes hampers it (Ellingsen and Östling 2010). More recently, a small but growing literature in macroeconomics has started to incorporate heterogeneity in cognitive depth and iterative thinking (Angeletos and Lian 2017), leading to promising insights on the effects of monetary policy (Farhi and Werning 2019) or low interest rates (García-Schmidt and Woodford 2019). Existing models of heterogeneity in cognitive depth, however, face a fundamental problem. Choices are classified into different cognitive categories assumed to require different levels of cognitive effort. So far, there is little direct evidence linking observed play to cognitive effort. Most of the experimental literature has used observed choices to infer an individual’s depth of reasoning from the associated cognitive categories. Hence, the observation of a given choice is used to infer cognitive effort taking the underlying path of reasoning or thought processes that led to the classification of choices as given, creating an essentially circular argument. One problem with this approach is that the same choice is always attributed to the same level of cognitive effort, although it might very well be the result of completely different decision rules. For example, an agent choosing an alternative after a complex cognitive process and another agent choosing the same alternative because of some payoff-irrelevant salient features cannot be distinguished on the basis of those choices alone. As a consequence, the level of cognitive effort associated with a choice becomes a non-testable assumption, and the sources of heterogeneity remain in the dark. A case in point is the work of Goeree et al. (2018), who identified a game where imputing cognitive depth from choices alone leads to clearly unreasonable conclusions, in the form of abnormally high imputed cognitive levels. To establish that the source of observed behavioral heterogeneity is actually heterogeneity in cognitive effort and capacities, what is needed are individually measurable correlates of cognitive effort beyond choice data. That is, instead of identifying particular choices with particular levels of cognitive effort, one needs to provide a direct measure of effort which allows to independently show that certain choices actually are the result of stronger cognitive effort. We argue that response times, or, more properly in our context, deliberation times can be fruitfully used for this purpose. We focus on deliberation times for two reasons. First, they are easy to collect in standard experimental laboratories, without any need for additional equipment. Second, within the context of iterative thinking, it is safe to assume that the total deliberation time for a decision reduces to the sum of deliberation times for the individual steps. In principle, other psychophysiological correlates of cognitive effort could be used in place of deliberation times.Footnote 1 A notable example is pupil dilation, because the eye’s pupil dilates with the amount of mental effort exerted in a task (Kahneman and Beatty 1966; Alós-Ferrer et al. 2019). However, it is unclear at this point how to disentangle the individual contributions of thinking steps to phasic pupil dilation for a single decision. In the present work, we test a simple model linking cognitive sophistication to choices and deliberation times, taking into account stylized facts from the psychophysiological literature on response times. We build on Alaoui and Penta’s (2016a) model of endogenous depth of reasoning, which postulates that players proceed iteratively, making an additional step of reasoning if the value of doing so exceeds its cognitive cost. Specifically, the value of reasoning, which depends on the payoffs of the game, essentially corresponds to the highest possible payoff improvement resulting from an additional step of thinking. This model delivers a first, straightforward prediction, namely that higher incentives will result in additional steps of reasoning and hence more cognitively sophisticated choices (Prediction 1). Alaoui and Penta (2016a) conducted an experiment confirming this prediction. We take this model as a starting step and enrich it by linking steps of reasoning to deliberation times. The total deliberation time of an observed choice is assumed to be the sum of the deliberation times for the chain of intermediate steps. That is, if arriving at a choice through iterative thinking requires seven steps, deliberation time is the sum of the times associated with the seven corresponding, intermediate steps. This natural structure suffices to derive a further prediction, namely that choices involving more steps of reasoning should be associated with longer deliberation times (Prediction 2). Further predictions depend on the properties of the function relating value of reasoning and the time required for each step of thinking. Suppose that the time required for a given step were independent of the associated value of reasoning. This would automatically imply that higher incentives lead to longer deliberation times, since the former would result in more steps of reasoning (Prediction 3).Footnote 2 However, this prediction might be implausible, because the assumption it rests on is at odds with received empirical evidence. This is due to a well-known phenomenon in psychology and neuroscience (going back to, at least, Cattell 1902 and Dashiell 1937), according to which easier choice problems (where alternatives’ evaluations show large differences) take less time to respond to than harder problems. Hence, deliberation times are longer for alternatives that are more similar, either in terms of preference or along a predefined scale. This so-called chronometric effect has also been shown to be present in various economic settings such as intertemporal choice (Chabris et al. 2009), risk (Alós-Ferrer and Garagnani 2018), consumer choice (Krajbich et al. 2010; Krajbich and Rangel 2011) as well as in dictator and ultimatum games (Krajbich et al. 2015). Alós-Ferrer et al. (2018) examine the consequences of the chronometric effect for revealed preference, and Alós-Ferrer et al. (2016) show that it helps explain and understand preference reversals in decisions under risk. This effect follows naturally when the choice process is captured by a drift diffusion model (DDM) (Ratcliff 1978), a class of models that has been applied extensively in cognitive psychology and neuroscience, and which is receiving increasing attention in economics (Chabris et al. 2009; Fudenberg et al. 2018; Baldassi et al. 2019; Webb 2019).Footnote 3 In accordance with this evidence, it should be expected that the deliberation time for a given step of thinking is larger the smaller the value of reasoning for that step. This leads to the prediction that, fixing the number of steps required for a choice, the associated total deliberation time should become shorter as the value of reasoning of the corresponding steps increases (Prediction 4). Thus, increasing the value of reasoning (e.g., by increasing incentives) has a twofold effect. On the one hand, it will lead to a larger number of steps of reasoning (Prediction 1), hence, in principle, resulting in longer deliberation times through Prediction 2. On the other hand, the deliberation times per step will be shorter (Prediction 4). As a result, the total effect on deliberation times is indeterminate. In particular, increasing the value of reasoning can result in shorter total deliberation times, in direct contradiction with Prediction 3. We tested Predictions 1–4 in a laboratory experiment.Footnote 4 Our design included two different games commonly used to study iterative thinking (and, in particular, level-k reasoning): the beauty contest game (or guessing game; Nagel 1995), which is the workhorse in that literature, and several variants of the 11–20 money request game, recently introduced by Arad and Rubinstein (2012), in the graphical version of Goeree et al. (2018). Given the standard level-0 behavior in the 11–20 game, these variants all share the same path of reasoning, usually assumed to result from iterated application of the best-reply operator, but we systematically manipulate the payoff structures to vary the value of reasoning and test our predictions. In the beauty contest game we find longer deliberation times for choices commonly associated with more steps of reasoning, confirming the basic prediction of our model that deliberation time is increasing in cognitive effort (Prediction 2). That is, the beauty contest game, a game where there is little doubt that level-k reasoning is prevalent, serves as a basic validation of the relationship between cognitive effort and deliberation times. This prediction is also confirmed in the 11–20 game, that is, again deliberation times are longer for higher-level choices. Thus, in both games our data verifies the assumed connection between observed level and cognitive effort in support of level-k reasoning. To test the remaining predictions, we take advantage of the fact that our implementations of the 11–20 game systematically vary incentive levels. In agreement with Prediction 1 and with the results in Alaoui and Penta (2016a), we find a systematic effect of incentives on the observed depth of reasoning as predicted by the model, that is, higher incentives are associated with an increase in higher-level choices. We also find shorter deliberation times when incentives are increased, even though observed depth of reasoning is increased. This result directly contradicts Prediction 3, implying that decision times per step are not independent of incentives. It is, however, fully compatible with Prediction 4 and the assumption that deliberation times per step are decreasing in the value of reasoning. A regression analysis then allows us to provide more direct evidence in favor of this latter property. In summary, we show that heterogeneity in behavior can be traced back to heterogeneity in cognitive effort by using deliberation times as a direct correlate of the latter rather than exogenously identifying choices with different levels of cognitive effort. More generally, our results show that deliberation times can be used as a tool to study cognitive sophistication. In particular, this provides a direct correlate of cognitive effort which avoids potentially-circular arguments where observing a choice is used to impute a higher cognitive effort because higher cognitive effort would have resulted in that choice. In the absence of this correlate, one might be led to draw wrong conclusions if models of iterative thinking are applied without an external way of testing for heterogeneity. In the Appendix, we provide an example of a variant of the 11–20 game (following Goeree et al. 2018) where deliberation times suggest that imputing higher levels of cognitive effort from certain choices might be unwarranted. The paper is structured as follows. Section 2 briefly relates our work to the literature. Section 3 introduces the model and derives the predictions. Section 4 describes the experimental design. Section 5 presents the results of the experiment for the beauty contest. Section 6 presents the results on depth of reasoning and deliberation times (Prediction 2) for the 11–20 games. Section 7 presents the results on the effect of incentives for those games (Predictions 1, 3, 4). Section 8 discusses and summarizes our findings. The Appendix contains two additional variants of the 11–20 game, and the Online Appendix discusses the robustness of our findings with respect to alternative level-0 specifications.",7
24.0,2.0,Experimental Economics,12 September 2020,https://link.springer.com/article/10.1007/s10683-020-09674-8,The risk elicitation puzzle revisited: Across-methods (in)consistency?,June 2021,Felix Holzmeister,Matthias Stefan,,Male,Male,Unknown,Male,"Risk is an integral part of many economic decisions and, thus, has been considered a key building block of economic theory (Arrow 1965). As a consequence, the question how to properly elicit and classify individuals’ risk preferences is of vital importance in academic research. In experimental economics and psychology, irrespective of differences in their approaches, incentivized risk preference elicitation tasks have evolved as widely accepted tools to measure and assess individual-level attitudes towards risk. While economists and psychologists have developed a variety of competing methodologies, a consensus on which of the elicitation procedures gives rise to the most accurate estimates of individual-level risk preferences has not been reached yet (Charness et al. 2013). Facing this pluralism of methods, pragmatism prevails among researchers when choosing among various competing risk preference elicitation tasks. The implicit assumption behind this common practice is the procedural invariance axiom, which states that normatively equivalent elicitation methods give rise to the same preference ordering (Tversky et al. 1988). Accordingly, the experimenter’s choice of which method to use should not systematically affect participants’ revealed risk preferences. However, experimental evidence, reviewed in detail in Sect. 2, suggests that participants’ attitudes towards risk may vary considerably when measured with different elicitation methods—a finding recently referred to as the “risk elicitation puzzle” (Pedroni et al. 2017). What is particularly challenging about the risk elicitation puzzle is not the heterogeneity in risk preferences across different methods per se, but rather the question how to properly interpret the observed variation in risk attitudes. In particular, how can we assess whether choices that can be described by varying risk preferences are indeed the result of unstable preferences, or, whether different elicitation methods rather stimulate distinct preference relations? While the former interpretation challenges the assumption of stable risk preferences, the latter challenges the procedural invariance axiom; and indeed, calling procedural invariance into question dates back to early systematic examinations of preference reversals (see, e.g., Tversky et al. 1988; Tversky and Thaler 1990). A third option is to adhere to both assumptions, i.e., preference stability and procedural invariance, but rather interpret subjects’ behavior as inconsistent—a term abundantly used in the literature with various meanings. However, it is not immediately obvious what the term inconsistent should refer to in terms of choice behavior. As argued by Sen (1993), “the basic difficulty arises from the implicit presumption underlying that approach that acts of choices are, on their own, like statements which can contradict, or be consistent with, each other.” Thus, to assess the consistency of behavior, eventually, one needs to invoke a theory upon which choices can be interpreted as contradictory (Sugden 1991). This essential insight illustrates that one can only assess the consistency of choices across different methods on the basis of some underlying theoretical framework. Part of this framework are the premises of preference stability and procedural invariance, which allow for evaluating participants’ behavior as inconsistent under the assumption that different methods elicit the same stable preference relation. If either of the two premises is waived, however, classifying heterogeneity in revealed risk preferences as inconsistent becomes questionable. While we can conceptually disentangle preference stability from procedural invariance, it is important to emphasize that the validity of either of the two premises cannot be tested in isolation. Any test of either concept involves the assumption of the other: Examining the stability of preferences requires the usage of different risk preference elicitation methods to compare the elicited preferences, which (implicitly) assumes procedural invariance—and vice versa.Footnote 1 To get a better understanding of variability of revealed preferences across methods, in this paper we take into account participants’ subjective point of view: In addition to incentivized risk preference elicitation tasks, our experimental protocol comprises survey items, which allow for examining participants’ subjective accounts of the different methods—in particular, their awareness of the risk they are willing to take in the different tasks. We use a within-subject design comprising four widely used risk preference elicitation methods: (1) the “bomb” risk elicitation task (Crosetto and Filippin 2013), (2) the certainty equivalent method (Cohen et al. 1987; Dohmen et al. 2010; Abdellaoui et al. 2011), (3) a multiple choice list between pairs of lotteries (Holt and Laury 2002, 2005), and (4) a single choice list (Binswanger 1980, 1981; Eckel and Grossman 2002, 2008). While previous studies typically assess the magnitude of across-methods variation based on correlations between risky choices in different tasks, we employ an individual-level measure of preference stability relying on the comparison of implied crra parameter intervals. For our sample, we observe that subjects’ revealed preferences are stable in less than 50% of pairwise comparisons of methods. Conducting simulation exercises to obtain benchmarks for participants’ behavior, we find that the observed heterogeneity of revealed risk preference across methods is qualitatively similar to the heterogeneity arising from independent random draws from choices in the experimental tasks. While this finding is indicative of substantial across-methods variation in risk-taking behavior, our main result is that subjects’ assessments of the riskiness of their choices is significantly related to the risk preference estimates across the different tasks. Thus, subjects seem to be well aware of their choices across methods. In the light of these results, we argue that the observed variation in revealed preferences cannot be straightforwardly interpreted as being inconsistent.",16
24.0,2.0,Experimental Economics,10 September 2020,https://link.springer.com/article/10.1007/s10683-020-09675-7,Managerial incentives and stock price dynamics: an experimental approach,June 2021,Te Bao,Edward Halim,Yohanes E. Riyanto,,Male,Unknown,Mix,,
24.0,2.0,Experimental Economics,15 February 2021,https://link.springer.com/article/10.1007/s10683-021-09704-z,Correction: Managerial incentives and stock price dynamics: an experimental approach,June 2021,Te Bao,Edward Halim,Yohanes E. Riyanto,,Male,Unknown,Mix,,
24.0,2.0,Experimental Economics,19 August 2020,https://link.springer.com/article/10.1007/s10683-020-09665-9,Strategy-proofness in experimental matching markets,June 2021,Pablo Guillen,Róbert F. Veszteg,,Male,Male,Unknown,Male,"Laboratory experiments have been instrumental for the success of market design.Footnote 1 This is true, in particular, for centralised markets based on matching mechanisms. Those experiments show how strategy-proof (i.e., dominant-strategy incentive-compatible direct) mechanisms such as Deferred Acceptance (DA) and Top Trading Cycles (TTC) outperform non-strategy-proof mechanisms like the so-called Boston Mechanism (BOS). That is, both DA and TTC induce higher truth-telling rates and efficiency than BOS. This experimental evidence has been used to convince stakeholders to adopt DA or TTC. The canonical reference for our claim is the famous Abdulkadiroğlu et al. (2006) paper which reports the efforts of a group of researchers to convince the Boston Public Schools planning team to adopt a strategy-proof matching mechanism. The researchers gathered theoretical and empirical, both historical and experimental, evidence about the vulnerability of the BOS mechanism to preference misrepresentation (Abdulkadiroğlu and Sönmez 2003; Chen and Sönmez 2006; Roth 1991). Among those, Chen and Sönmez (2006) offers the seminal comparison of the strategy-proof DA and TTC with the non-strategy-proof BOS. In that experiment, both DA and TTC generate more truth-telling than BOS and also outperform it in terms of efficiency. Given the modest truth-telling rates observed in DA and TTC, between 43–50% and 56–72% respectively, Chen and Sönmez (2006) are cautious in terms of promoting the virtues of theoretically strategy-proof mechanisms and recommend participants to be instructed so they would not hurt themselves by misrepresenting their preferences. The authors seem, however, to assume that truth-telling participants understand the incentives. Since Chen and Sönmez (2006), other articles have reported different and, generally higher, truth-telling rates (Calsamiglia et al. 2010: DA 57–58%, TTC 62–74%; Pais and Pintér 2008: DA 67–82%, TTC 87–96%; Pais et al. 2011: DA 58–76%, TTC 62–84%). This evidence did, perhaps, prompt overenthusiastic comments such as the following one in Pathak and Sönmez (2013): “Another factor [in favour of using strategy-proof mechanisms] was the potential to use unmanipulated preference data generated by the student assignment mechanism in various policy-related issues, including the evaluation of schools.”Footnote 2 More recently, a handful of articles have been trying to evaluate the performance of strategy-proof mechanisms in the field and in the laboratory. Rees-Jones and Skowronek (2018) report on the results of an online survey, run with recent participants in the National Residency Matching Program (NRMP), in which 23% of participants fail to play the dominant strategy. Note that, as compared to the typical undergraduate college students sampled for a laboratory experiment, student participants in NRMP have high cognitive ability, they have access to excellent resources in terms of high quality advice from the NRMP itself, and they face a game with extremely high stakes. Similar results are reported by Rees-Jones (2018), Hassidim et al. (2017, (2018) and Shorrer and Sóvágó (2018). Notwithstanding the significant proportion of participants deviating from truthful preference revelation in those studies, the results do not provide much indication on whether the remaining majority (which did send truthful messages) do indeed understand the incentives of the game. A more careful approach taken by Guillen and Hakimov (2017) uses a within-subject experimental design based on TTC in a tightly controlled environment in which decision-makers report their preferences both with and without being informed about the choices of others. Their data revealed that most participants (69%) failed to adapt to changing circumstances and to play the dominant strategy. Only 31% of them exhibit behaviour that is compatible with theory. We ran an experiment to investigate the robustness of the commonly reported high truth-telling rates of 50–80% in matching laboratory experiments. That is, our research goal is to determine whether the majority of participants tell the truth because they understand their incentives to do so or because they simply follow a default and choose a salient strategy. Indeed, the essential problem with matching experiments lies in the complexity of the games under study and that the induced preference order constitutes a strong focal point. These two are not independent, because the induced preference order can be understood as a default from which experimental subjects may or may not decide to depart. Experimental studies by Guillen and Hing (2014) and Guillen and Hakimov (2018) show that the standard experimental instructions explaining matching algorithms are too difficult to understand and the majority of experimental participants can be easily influenced by (correct or incorrect) advice. Simply put, an unsophisticated experimental subject who does not understand the strategic environment created by the experimenter and the incentives of the experiment may just submit the induced preference order and unwittingly play the dominant strategy of the game. Such behaviour then could be mistakenly interpreted as supporting evidence to the underlying theoretical model. To some extent, the literature shows awareness of the bias introduced by unsophisticated players to the desired empirical test of theoretical models and to the interpretation of the experimental results. Experimental studies targeting complex theories and mechanisms like centralised matching markets often rely on protocols that include both a solved numerical example and perhaps an incentive-based quiz (i.e, an example to be solved by participants) prior to the main part of the experiment. The purpose of this is to make sure that participants pay attention to the instructions and fully understand the rules of interaction before making decisions, which is a fundamental assumption behind all theoretical models in market design. This, however, does not guarantee that participants act in a consciously optimal manner, as such, incentive-based quizzes only test a cursory understanding of the instructions rather than the understanding of the incentives of the game and thus theoretical properties like strategy-proofness in DA or TTC.Footnote 3 Additionally, many matching experiments set a top priority for the second-best object (school) thus trying to tempt subjects to manipulate the mechanism by inducing a so-called district-school bias (DSB). They do so, because playing the dominant strategy under these circumstances may well be regarded as a good understanding of the incentives. Once again, we are skeptical about this claim. In this paper, we report results from a carefully designed experiment (with the above-mentioned preference structure) to distinguish between subjects who play the dominant strategy because it is a default (and a strong focal point) from subjects who play it for other, perhaps rational strategic, reasons. Our design includes two baseline treatments, one for DA and another for TTC, that follow the standards set by Chen and Sönmez (2006). Our experiments, which study one-shot interactions, are based on an induced priority order, use standard instructions explaining the workings on the corresponding algorithm, and incorporate a solved example and an incentive-based exercise, or quiz, to be solved by participants. Also, every participant in our experiments faces DSB. Not surprisingly, truth-telling rates in our two baseline treatments are high and in line with well-known results from the existing literature. Our treatments introduce two novel matching mechanisms, Reverse Deferred Acceptance (RDA) and Reverse Top Trading Cycles (RTTC), as benchmarks. Both are small variations of DA and TTC, respectively. The idea is that the central clearinghouse runs an algorithm that sends out proposals in the reverse order, starting from the lowest ranked object moving gradually towards the highest ranked. It took merely a few word changes in the instructions of DA and TTC to describe and induce the reversed mechanisms. Namely, “highest” got replaced by “lowest”.Footnote 4 This change results in RDA and RTTC to have a dominant-strategy equilibrium in which participants are to submit their preferences in exactly the opposite order they are induced. Our benchmark mechanisms are not strategy-proof, as participants do not have incentives to directly report their true preferences, but they create strategic environments which are otherwise identical in other desirable and celebrated theoretical properties to the ones induced by DA and TTC. Note that when comparing DA to RDA and TTC to RTTC, for isolating naïve truth-tellers from strategic decision-makers, our design is more parsimonious than previous experimental tests of matching mechanisms which simply compare the performance of very different mechanisms.Footnote 5 In a nutshell, we find that the majority of participants fall in one or another behavioural trap. Only 16% and 26% of them play the dominant strategy in RDA and RTTC, down from 68% and 46% in DA and TTC. The induced preference order is played by 31% and 22% in RDA and RTTC, while DSB and what we call a naïve district school bias (NDSB) accounts for the majority of other observed strategies. We observe that truth-telling is not only the modal strategy for DA and TTC, but it remains so even under RDA and ranks as the second most-frequently played strategy under RTTC. Many decision-makers seem to stick to the truth not because it constitutes an optimal strategy, but because it is a default in a very complex situation. Our results indicate that, by far, default play is the main driver of the usual experimental results. The complexity of matching mechanism obscures strategy-proofness to the extent that it becomes an irrelevant theoretical property in laboratory experiments. On a positive note, our results also suggest that truth-telling, being such a salient strategy, may be also salient in the field. That is, real-world participants who do not know how to behave strategically may just choose to tell the truth. However, this optimistic idea should be taken with some caution as it questions fundamental assumptions behind theoretical models and it undermines the very purpose of the theory behind market design. The design of our experiment and the procedures used are thoroughly explained in Sect. 2. Section 3 goes over the theoretical prediction, Sect. 4 presents the experimental results, and Sect. 5 discusses the implication of those results and concludes. The appendix describes the deferred-acceptance, the top-trading-cycles algorithms and their reverse versions (“Appendix A of ESM”), it contains further statistical results and discussion from our experiment (“Appendix B of ESM”), and it also includes the experimental instructions, examples and quizzes used (“Appendix C of ESM”).",6
24.0,2.0,Experimental Economics,10 December 2020,https://link.springer.com/article/10.1007/s10683-020-09677-5,Not all group members are created equal: heterogeneous abilities in inter-group contests,June 2021,Francesco Fallucchi,Enrique Fatas,Ori Weisel,Male,Male,Male,Male,"Many situations in social and economic life are characterized by rivalry and conflict between two or more competing parties. Warfare, socio-political conflicts, political elections, lobbying, R&D competitions, and promotion tournaments, are all examples of inter-group conflicts in which groups spend scarce and costly resources in order to compete with other groups. Within each competing group, group members may differ with respect to a variety of characteristics such as preferences, resources, wealth, productivity, or motivation, which, in turn, can affect their ability and willingness to compete. Acknowledging that such within-group heterogeneity is the rule rather than the exception, a straightforward implication is that competing groups are rarely identical, and contests are typically not symmetric. Examples abound. For instance, countries competing for access to natural resources or geopolitical influence will typically (if not always) differ regarding the degree of diversity in society, such as the distribution of income, education, or other sociodemographic characteristics. In the domain of organizations, firms often rely on interfirm alliances to compete with other firms or alliances (for example in the context of developing new products). Such alliances can be cross-function when partners contribute diverse and complementary resources, or same-function when firms have similar competencies (Amaldoss and Staelin 2010). The resulting competition can then be either symmetric (between two cross-function alliances or two same-function alliances) or asymmetric (between a cross-function and a same-function alliance). Similarly, within-firm contests between groups can occur in the context of performance-contingent payment schemes, such as paying bonuses to the best performing group(s) to increase productivity (Nalbantian and Schotter 1997; Bandiera et al. 2013). Within-group heterogeneity in such settings is only natural, as group members can have different skills or abilities. It follows that the competing groups themselves are also not necessarily similar, resulting in asymmetric competition.Footnote 1 Despite these rather natural applications, the bulk of previous literature on contests has focused on situations in which symmetrical agents or groups compete against each other (see Dechenaux et al. 2015; Sheremeta 2017, for overviews). There is, however, a respectable (and growing) number of studies that have investigated the effects of various types of asymmetries, including group size (Abbink et al. 2010; Ahn et al. 2011), wealth (Rapoport et al. 1989; Hargreaves Heap et al. 2015), sharing rules (Kurschilgen et al. 2017), and the availability of communication (Cason et al. 2017) and punishment opportunities (Sääksvuori et al. 2011). We contribute to this literature by experimentally testing how heterogeneity in players’ ability to contribute to the group’s success affects competition. In our systematic analysis, we study the effect of heterogeneity both within and between groups. For ease of exposition, we will use the terms homogeneous and heterogeneous to describe within-group structures (i.e., whether group members are similar or not), and symmetric and asymmetric to capture the relationship between the groups (i.e., whether competing groups are similar or not). In our setting, a high-ability person is more efficient in converting her effort to a contribution to the group than a low-ability person. This means that the marginal productivity, or the contribution/effort ratio, of the high-ability person is higher: for each unit of invested effort, a high ability player contributes more to the group than a low ability player. Heterogeneity in this respect is only natural, as some group members may be stronger, smarter, or have better task-specific capabilities than others. As an illustration, think of a group of salespersons, with one member who is more talented, experienced, or is endowed with a more densely populated sales territory, than the others. The high ability individual has a higher marginal productivity in the sense that even if all group members exert the same effort (in terms of, e.g., hours worked or energy expenditure), she will contribute more to the group’s success than her less able peers. Previous studies modeled ability, or related concepts, in different ways. Sheremeta (2011b) manipulated players’ valuation of the prize, i.e., some group members derive a higher payoff than others when the group wins the contest.Footnote 2 Ryvkin (2011), in a theoretical model, and Brookins et al. (2015), in an experiment, consider within-group heterogeneity based on players’ cost of effort; for some players, investing effort towards the group’s success is costlier than for others. From an individual, self-interested, perspective, high abilities (as operationalized by us), high prize valuations, and low effort costs are all expected to increases players’ inclination to contribute to their group. However, if players care not only about their own payoff, but also about the distribution of payoffs within the group (e.g., have a preference for equality) these different ways of introducing heterogeneity may differ in how they affect contributions. The reason is that when abilities are heterogeneous, equal efforts lead to equal payoffs; high-ability players’ inclination to contribute is at odds with equality. When valuations or effort costs are heterogeneous, equal efforts result in un-equal payoffs; group members without higher prize valuations, or with lower effort costs, must contribute more than others for payoffs to be equal. Whether these differences in the modeling of ability matter behaviorally is ultimately an empirical question, which the current study can help answer. We use a laboratory experiment to investigate the role of heterogeneity in abilities within and between groups. The major advantage of using a laboratory experiment is that it allows to tightly distinguish between a player’s ability and her effort choice. In the field this is almost impossible to achieve, since typically only performance—which is a function of both effort and ability (and noise)—can be observed. The laboratory setting further allows us to exogenously manipulate the composition of players within groups, which circumvents complicating factors such as self-selection that emerge in most field settings where groups form endogenously. As a workhorse for studying group contests, we follow previous literature by using an experimental version of Tullock’s contest game (Tullock 1980) in which two groups compete for a prize that is divided equally among all members of the winning party (Katz et al. 1990). We study this basic decision situation in three different treatments, in which we systematically vary the heterogeneity both within and between groups. In the first treatment, we study (commonly explored) symmetric contests between two homogeneous groups, in which all group members in both groups are equally able to compete. To study the pure effect of within-group heterogeneity, in the second treatment both competing groups are equally heterogeneous. Specifically, each group consists of one low-ability, one medium-ability, and one high-ability player. Importantly, we hold the average ability of group members constant compared to homogeneous groups, which consists of three medium-ability players. In the third and last treatment, we focus on the most interesting and natural situation in which the two competing groups differ from each other. To provide a clean comparison to the first two treatments, and to be able to investigate how conflict engagement depends on the opponent’s group type (while holding constant the own group type), we examine an asymmetric contest between a homogeneous group and a heterogeneous group. This set of treatments further distinguishes our study from the above-mentioned studies by Sheremeta (2011b) and Brookins et al. (2015). In both studies competing groups are always heterogeneous to a certain degree; in the latter, contests are always asymmetric. In contrast, our design includes the natural benchmark, featured in the bulk of the literature, of contests involving completely homogeneous groups as well as symmetric contests between heterogeneous groups. Including these situations allows to better identify and separate the role of within- and between-group heterogeneity.Footnote 3 We find that while heterogeneity per se has no discernable impact on the degree of competition, asymmetry between groups leads to an intensification of conflict, with homogeneous and heterogeneous groups winning the contest equally often. One reason for this result is that players in heterogeneous groups contribute to the success of the group much more equally than predicted by a number of theories (see Sect. 2), which agree in stating that only high-ability players should contribute, while low- and medium-ability players should free-ride.Footnote 4 Intra- and inter-group dynamics—the way individuals condition their behavior on the past behavior of others—help to further explain these results. We find that for homogeneous groups, asymmetric contests lead to increased conditional cooperation among group members, while no such effect is observed for heterogeneous groups. Furthermore, in heterogeneous groups players of different abilities condition their effort on that of their ‘relevant’ peer (see Croson et al. 2005, 2015; Kölle 2015). Low-ability players react to efforts by the medium-ability player but largely ignore the ones by the high-ability player. Medium-ability players, in contrast, react to both low- and high-ability players. The latter, in turn, react to the behavior of their medium-ability group member, but only in symmetric contests; when the contest is asymmetric they become “unconditional cooperators”, as their contribution is not driven by the effort levels of their group members. Finally, we show that asymmetric contests are not only more intense, but also more volatile, suggesting that facing a group that is different from your own can lead to an increase in strategic uncertainty. As a result of the increased intensity and volatility, asymmetric contests have two detrimental effects for the participants as they decrease individual earnings and increase payoff inequality within groups. Taken together, our results show that heterogeneity in abilities significantly affects contest behavior only in the most natural setting where heterogeneity exists both within and between groups. The rest of the paper is organized as follows. Section 2 presents the general setup of our group contests as well as theoretical predictions. In Sect. 3 we describe our experimental design and procedures in more detail. Section 4 summarizes our results. In Sect. 5 we discuss our findings and provide some behavioral rationale for the observed effects. Section 6 concludes.",7
24.0,3.0,Experimental Economics,20 October 2020,https://link.springer.com/article/10.1007/s10683-020-09679-3,40 years of tax evasion games: a meta-analysis,September 2021,James Alm,Antoine Malézieux,,Male,Male,Unknown,Male,"The year 2018 marked the 40th anniversary of the first article that used laboratory experiments to examine individual tax evasion behavior—what we term “Tax Evasion Games” (or TEGs). In this seminal contribution, Friedland et al. (1978) asked 15 Israeli psychology undergrads to take part in the following experiment: This research takes the form of an economic game. In general, each one of you will receive salary slips. You will be asked to report your income, and pay income tax on the income you reported. From time to time, audits will be conducted according to a random sample, and fines imposed on tax evaded. At the end of each “round” of 10 months, each person’s net income will be added up (gross income less income tax less fines). The objective of each person in the game is to accumulate the maximum amount of net income. Since 1978, this simple game has been the subject of well over 130 publications (Torgler 2016), with more papers added on a regular basis. This extensive use of TEGs has been driven by three main factors. First, there are compelling reasons to find ways to fight tax evasion, and laboratory experiments offer one way to examine these policy tools. Second, laboratory experiments generate observable and reliable data about individual tax evasion choices, information that is by nature very difficult to measure in the field. Third, laboratory experiments have a high degree of “internal validity”, allowing researchers to test hypotheses that isolate the cause-and-effect of the policy intervention. However, despite the wide use of TEGs, it is disappointing—and surprising—that the impacts of many variables examined in these TEGs remain unclear (Muehlbacher and Kirchler 2016; Malézieux 2018). It may seem obvious that increasing the audit probability and/or the penalty rate will increase compliance, but even so these impacts may be different on the intensive margin (i.e., the compliance from evaders) versus on the extensive margin (i.e., the probability of full compliance). The lessons are even more uncertain when we look at other policy levers like the tax rate, the audit rule, the type of tax system, or the use of tax amnesties. There is also uncertainty about the ways in which subject demographic characteristics affect compliance behavior, and the impacts of such experimental design features as the nature of experimental instructions, the presence or absence of an earning task, the ways in which subjects are directed to report income, or the redistribution of taxes to the participants or to a real life public good, have a significant impacts on tax compliance. As a result, we believe that it is worthwhile to determine what we have learned from these TEGs, as well as what we have still to learn from them. These are our goals here: To examine the impacts on tax evasion behavior in TEGs of traditional public policy variables (e.g., the audit rate, the fine rate, the tax rate, the audit rule, a tax amnesty, the tax system), experimental design variables (e.g., the framing of the instructions, the use of an earning task, the redistribution of taxes), and individual-level variables (e.g., age, gender, occupation, risk attitude), and to identify fruitful areas for future TEG research. We do this by collecting 70 TEG datasets and then using meta-analysis to estimate these impacts. As defined by Glass (1976), a meta-analysis is “...the statistical analysis of a large collection of [...] results from individual studies for the purpose of integrating the findings” (p. 3). The term “meta-analysis” refers to “the analysis of analyses”. This method has been extensively used in clinical research, and it has attracted increasing attention in experimental social sciences (Davis et al. 2014). The aim of a meta-analysis is to determine, if possible, the definitive impact of a variable, to guide the use of replication analyses, and to inform public policy (Maki et al. 2018). There are two types of meta-analysis, one based on aggregated data and another based on individual participant data (Riley et al. 2008). In an important earlier paper on tax evasion behavior in TEGs, Blackwell (2010) ran a meta-analysis on aggregated TEG data, studying the compliance impact of the audit probability, the fine rate, the tax rate, and the multiplier on the public good. However, Blackwell (2010) was only able to use 26 articles in his work, given the time at which he conducted the meta-analysis, and he was also unable to differentiate between the intensive and the extensive margins. The considerable expansion in TEGs over the last decade allows us to increase considerably the number of articles included in our meta-analysis, as well as to increase the number of variables that we can examine (including intensive and extensive margins). We are also able to utilize individual participant data, a method considered the “gold standard” of meta-analysis, with many advantages over meta-analysis of aggregated data (Riley et al. 2008). We collect an enormous dataset of 256,801 observations on 16,467 subjects, more than any previous study on tax evasion behavior. Our results for policy levers show that audits and fines interact differently on the extensive and intensive margins. Those variables have a positive impact on the extensive margin, and their interaction is positive; that is, the higher the audit probability, the greater is the impact of the fine rate on the decision to comply, and vice versa. However, their interaction is negative on the intensive margin, so that the higher the audit probability, the more negative is the impact of the fine rate for tax compliance coming from evaders, and vice versa. In addition, we find that a flat tax system, the tax rate, and a tax amnesty have an unambiguously negative impacts on tax compliance. We also find that two endogenous audit rules achieve higher compliance than a random audit rule, while other endogenous audit rules have no significant impact, suggesting a mixed performance of endogenous audits. As a comparison, the original Allingham and Sandmo (1972) model of tax compliance predicts that increases in the audit probability and the fine size both increase tax compliance. The present meta-analysis indeed finds that audits and fines both have a positive impact on the extensive margin. However, we also find that on the intensive margin audits and fines work against each other. Also, contrary to the predictions of Yitzhaki (1974), there is a negative impact of raising tax rates, at least on the extensive margin of compliance. As for design features, we find no impacts of framing the experiment with non-neutral terms, directing subjects to report income or of making participants earn an income rather than giving them income. We also find that redistributing the taxes that are collected to the participants or to an investment in a real life public good each has a positive impact on tax compliance. We also show that using a research fund as a real life public good is a representative type of public good to implement in a TEG. Finally, our estimation results indicate the importance of some individual-level characteristics. The typical tax evader in the lab is a male who is risk averse. On average, the higher his income in Euros, the higher is his probability of evading. Being a student increases his share of evasion. We do not find any impact from the subject’s age. The outline of the paper is as follows. Section 2 provides a literature review of all the variables under study here. Section 3 reports on our data and our methods for conducting a meta-analysis, and Sect. 4 presents the results of our meta-analysis. We conclude in Sect. 5.",30
24.0,3.0,Experimental Economics,27 September 2020,https://link.springer.com/article/10.1007/s10683-020-09678-4,Is the Allais paradox due to appeal of certainty or aversion to zero?,September 2021,Elif Incekara-Hafalir,Eungsik Kim,Jack D. Stecher,Female,Unknown,Male,Mix,,
24.0,3.0,Experimental Economics,28 September 2020,https://link.springer.com/article/10.1007/s10683-020-09680-w,Optimal design of experiments to identify latent behavioral types,September 2021,Stefano Balietti,Brennan Klein,Christoph Riedl,Male,Male,Male,Male,"Experimentation in the social sciences is a fundamental tool for understanding the mechanisms and heuristics that underlie human behavior. At the same time, running experiments is a costly process and requires careful design in order to test hypotheses while maximizing statistical power. This experimental design process is often guided by the intuition of the scientists conducting the research. While there are many benefits in relying on the intuition of experienced researchers, there is often a lack of principled guides when choosing which experiment to run (Fisher 1936; Hill 1995). As a result, experiments may have low power to distinguish between different models of behavior (Salmon 2001) and lead to increased costs for data collection. At worst, they lead to reduced effect sizes, and incorrect rejection or acceptance of a null hypothesis (Berman et al. 2018). A growing body of researchers including social scientists, computer scientists, and industry professionals have explored ways in which experiment selection can be optimized and how artificial intelligence (AI) can be used to select experiments (Rzhetsky et al. 2015). For example, researchers now optimize the experiments they run using statistical techniques (e.g., Thompson sampling) to preferentially assign participants to experimental treatments in order to optimize the treatment effect. This allows researchers to easily decide which treatment among many possible treatment arms is most effective in maximizing a certain outcome, such as the response rate in a marketing campaign (Eckles and Kaptein 2014; Letham et al. 2017; Schwartz et al. 2017; Zhou et al. 2018). Other techniques involve optimizing the length of the experiment, the number of participants needed, or the sequence of questions in behavioral batteries (Wang et al. 2010; Imai and Camerer 2018; Pooseh et al. 2018; Chapman et al. 2018). In each case, optimizing experimental design has proved fruitful. However, these techniques tend to focus on the optimization of individuals’ decisions (e.g., finding the best sequence of survey questions for estimating an individual’s risk preferences). These approaches are ill-suited for those situations involving strategic interactions, information asymmetries, or network effects (David 1985; Katz and Shapiro 1985; Bramoullé et al. 2020). In these contexts, agents with heterogeneous preferences derive different utilities for outcomes that depend on the actions of others agents (Camerer 2011; Fudenberg and Tirole 1991). For example, for goods with network effects, the utility of one consumer depends not only on that consumer’s preference for the product but also on how many others adopt the good (Tauber 1972; Gilchrist and Sands 2016). Attempting to optimize experiments with such strategic complements thus requires a different approach. We focus on a different set of optimal experimental design procedures that optimize interactive experiments mapping human decision-making to behavioral types, i.e., mathematical models that predict human behavior under specific sociological, economic, and psychological hypotheses (Harsanyi 1967). By building and testing various models about how individuals of a specific behavioral type make strategic decisions, modelers and experimenters can make fine-tuned predictions about the likely behavior of participants in experimental settings. As a result, we are better positioned to anticipate responses to societal, market, or technological changes (McIntyre and Chintakananda 2014). In this paper, we introduce an AI-based optimal design procedure that maximizes the information gained from running a behavioral experiment. Our procedure recommends which experiments to run (i.e., which parameter values to use for data collection) in order to maximally diverge the predictions of multiple competing models of human behavior. The output of this procedure takes the form of a coordinate—a point in the space of all possible experiments—that corresponds to optimal experimental parameters. By way of example, consider an experiment that measures gambling tendency and is parameterized by a maximum payout (M) and a payout multiplier (m). The experimenter’s question is which values to choose for M and m for data collection. Our protocol would output a particular experimental design in the form of a coordinate, (M, m), for example, M=$4.25 and m=1.2 (as opposed to M=$4.15 and m=1.3). While knowing the optimal experimental design is valuable for collecting informative data, finding the optimal design is a computationally-intensive task. The approach generally requires evaluating complex models such as whether a set of observations corresponds to a Nash equilibrium. We propose two methodological innovations that build on seminal methods in Bayesian optimal design (El-Gamal and Palfrey 1996). The first one addresses which parameter combinations are evaluated, the second addresses how the information content of a given parameter combination is evaluated. Together, both improvements reduce the computational costs of designing optimal experiments by several orders of magnitude while maintaining accuracy. We give a brief intuition behind our two improvements. First, we use an adaptive search algorithm based on a Gaussian Process to efficiently search the space of all possible experimental parameter combinations (Contal et al. 2013). Absent such an improvement, the optimal design procedure has to follow a brute-force approach that evaluates all possible experimental designs. The intuition behind this improvement is that the search algorithm adaptively explores those experimental designs that are likely very good (designs with high information content) more thoroughly, while reducing the exploration of parameter combinations that are similar to those already known to be bad (low information content). Second, we develop a sampling technique that evaluates the informativeness of a given experimental design on the basis of simulated likely datasets instead of all observable datasets. This allows us to reduce the size of datasets while simultaneously increasing the relevance of data points in the dataset and thus retaining accuracy despite the smaller size. There are six main parts in this article. First, we describe how to optimize experimental design for information gain, and we lay out our methodological improvements to this protocol. Second, we evaluate and quantify the extent of these computational improvements. Third, we use our improved procedure to find the optimal experimental design of a classic two-person imperfect information game (El-Gamal and Palfrey 1996). Fourth, we conduct an expert prediction experiment to see which experimental designs domain experts would recommend, and we compare these predictions to the optimal experiment suggested by our algorithmic approach. Fifth, we implement and run five experiments—each with different parameterizations—to illustrate the crucial role that information gain plays in distinguishing competing models of behavior and to showcase the adaptive nature of our approach. Finally, we expand the set of behavioral models (with Roth-Erev reinforcement learning) to illustrate how the optimal design procedure can be used iteratively to test behavioral hypotheses as they arise.",7
24.0,3.0,Experimental Economics,26 September 2020,https://link.springer.com/article/10.1007/s10683-020-09682-8,Last word not yet spoken: a reinvestigation of last place aversion with aversion to rank reversals,September 2021,Andrea F. M. Martinangeli,Lisa Windsteiger,,Female,Female,Unknown,Female,"It is well established that relative comparisons enter individuals’ utility function (Duesenberry 1949; Blanchflower and Oswald 2004; Ferrer-i Carbonell 2005; Luttmer 2005) and research over how social rankings and one’s own placement contribute to individuals’ utility has also been gaining momentum. For instance, the effect of rank-dependent remuneration and feedback has been found to depend on individuals’ rank (Card et al. 2012; Gill et al. 2018), and the impact of society-wide exogenous shocks on demand for redistribution to depend on individuals’ income bracket (Alesina et al. 2018; Martinangeli and Windsteiger 2019). However, why individuals’ preferences over social ranks (rank preferences, henceforth) affect their demand for social policies, and how, remains poorly understood.Footnote 1 The disutilities and behavioural consequences induced by changes in societal rankings, triggered by technological change, immigration or other exogenous shocks, but especially by policies explicitly addressing the shape of the income distribution, should be better understood and carefully accounted for in policy design. This paper aims at better understanding the nexus between rank preferences and demand for redistribution schemes in an environment removing the confounding endogeneities typical of natural settings. In particular, we ask whether society’s support for redistributive policies is affected by last place aversion (a dislike for occupying the last rank in a distribution (Kuziemko et al. 2014)), and/or rank reversal aversion (a dislike for disadvantageous rank reversals (Xie et al. 2017)), by disentangling the two. Answering this question would elucidate on whether rank preferences can account for a paradox robustly observed in the literature: Support for redistribution near the bottom of the income distribution is often lower than economic reasoning predicts (Fong 2001; Gilens 2009; Alesina et al. 2018). On the one hand, income support closing the gap between the poorest and the immediately higher income bracket might impose a utility loss on the latter by de facto turning their rank into the last (Kuziemko et al. 2014; KBRN henceforth). On the other, as income support is unlikely to invert income rankings, then if individuals are sensitive to rank reversals but not to occupying the last place, explanations for the paradox above should be searched for elsewhere. Specifically, KBRN show that individuals closest to the bottom of an income distribution are more likely to avoid actions resulting in their rank being disadvantageously inverted with that of others (at constant income). They argue that occupying the lowest rank might induce discontinuously larger disutilities than occupying the next higher rank, a regularity they interpret as last place aversion.Footnote 2 We believe this interpretation is premature: In their design, individuals’ choices confound preferences over the last rank with a dislike for disadvantageous rank reversals (Xie et al. 2017). We argue that KBRN’s finding might not stem from last place aversion but from rank-dependent rank reversal aversion: People might not be averse to occupying the last rank per se, but they might dislike disadvantageous rank reversals more the lower the rank they would acquire. The distinction between last place aversion and rank-dependent rank reversal aversion is consequential. The two concepts bear different implications on support for redistributive policies, on the disutilities originating from changes in the income distribution, and on the potential corrective measures to be enacted. Intervening on the income distribution to the point of inverting societal ranks is hardly, if ever, the explicit purpose or outcome of a redistributive policy. If rank reversal aversion but not last place aversion holds empirically, individuals should not be opposed to policy measures aimed at closing—but not inverting—income gaps at the bottom, such as minimum wages or negative income taxes. Low support for redistribution among the poor would thus remain unexplained in the absence of misconceptions about its distributional consequences and its impact on social rankings. Increasing support for redistribution would then require improved communication of the desired effects of such policies, strengthening trust in the government’s abilities to execute redistributive measures properly and reassuring voters that status quo rankings will be preserved. On the other hand, lower than predicted demand for redistribution at the bottom of the income distribution could be explained by last place aversion per se (absent rank reversals) should it be borne by the data. Income support policies are in fact arguably aimed at making the lower tail of the income distribution “catch up” with the rest, turning the next higher rank de facto into the last. In this case, increasing popular support for the policy might warrant potentially nontrivial adjustments of the intervention to ensure, for instance, that sufficient spacing between the lowest income brackets is preserved. Understanding which between last place and rank reversal aversion matters most is thus vital for understanding political opposition to, and the welfare consequences of, redistributive policies. Notice further that while last place aversion is idiosyincratic to the last rank, rank reversal aversion might bite across the whole distribution, thus making its potential impact on policy selection extremely large. We therefore re-investigate the modified dictator game adopted by KBRN, thus contributing to the literature replicating experimental results (Camerer et al. 2016), and extend the design to provide evidence for and to distinguish between pure preferences over the last rank and aversion to rank reversals.Footnote 3 The modified dictator game in KBRN captures the behavioural consequences of rank preferences on individuals’ support for redistributive actions. In their design, KBRN rank six subjects according to the size of their (randomly distributed) monetary endowment (from one to six Dollars in unit increments). Subjects were then asked to allocate two Dollars either to the person immediately above or below themselves in the distribution.Footnote 4 By allocating the two Dollars to the person immediately below themselves, a decision maker increases the income of the beneficiary by twice the monetary distance between them, causing their ranks to be “swapped”.Footnote 5 Their finding is that the probability with which the two Dollars are allocated downwards decreases the lower the allocator’s rank is, with a sharp drop for fifth (second-to-last) ranked subjects. As subjects ranked fifth-of-six transferring money to the sixth (last) would end up in the last place, KBRN interpret this result as evidence for an aversion to occupying the last place in the income distribution. In a recent paper, Xie et al. (2017) uncover however evidence for rank reversal aversion by observing external dictators avoiding monetary allocations resulting in the disruption of a pre-existing income distribution. They argue that—similar to what is observed elsewhere in the animal world—such regularity might stem from an evolutionary strategy aimed at reducing costly ingroup conflict (Smith 1982). It stands to reason that if third-party dictators are averse to changing pre-existing rankings among individuals they have no relation with, such aversion will be even more pronounced when individuals can, by virtue of their own actions, disadvantageously change their own rank. Dislike of disadvantageous rank reversals might be a function of the rank an individual is bound to acquire, the stronger the lower the new rank is. In this case, rank-dependent rank reversal aversion would be fully consistent with the evidence presented by KBRN. In other words, rather than from last place aversion, KBRN’s finding that the propensity to allocate money downwards is lowest in the second-to-last place compared to elsewhere in the distribution might originate from a stronger aversion to having one’s rank inverted with the last than with higher ranks. We modify KBRN’s design to disentangle the effect of last place aversion and that of aversion to (disadvantageous) rank reversals. To do so, we replicate KBRN’s design but ask subjects to allocate an amount of money exactly equal to the monetary distance between two consecutive ranks. This way, allocations in favour of the subject ranked immediately below oneself do not cause one’s rank to be inverted with that of the beneficiary. Instead, the latter’s income is increased just enough to catch up with the allocator, causing them to share a common rank. Rank reversal aversion cannot play a role as ranks are now not inverted. On the other hand, last place aversion still predicts individuals to shy from allocations causing them to end up in the last place, even though the last place would be shared with another subject.Footnote 6 As ranks cannot be inverted, we term this condition the No Rank Reversal condition (NoRR, henceforth). To allow for comparability of our results with those in KBRN, we include a replication of their original design to serve as a benchmark: As ranks can be inverted in this condition, we term our replication of KBRN the Rank Reversal condition (RR, henceforth).Footnote 7 As in Camerer et al. (2016), our investigation of condition RR fails to deliver the results presented in KBRN. Moreover, our modified condition NoRR does not provide evidence in favour of last place aversion either. The intuitive, anecdotal, and scientific appeal (Fong 2001; Gilens 2009; Gill et al. 2018; Alesina et al. 2018; Martinangeli and Windsteiger 2019) of the mechanisms here discussed warrant however a critical reappraisal of the conditions under which their measurement was attempted. We thus propose to further extend KBRN’s design to investigate the mechanisms and conditions conducive to last place and/or rank-dependent rank reversal aversion. Specifically, in our conditions NoRR and RR, closely following KBRN’s design, the subjects randomly rotate across ranks at every repetition of the stage game. This feature of the design has two disadvantages. First, when individuals rotate across ranks they will (potentially early on in the session) be assigned low ranks and experience the associated monetary and psychological disutilities. More importantly, they will anticipate, in early periods, the positive probability with which they will be (re)assigned a low rank in the future. They might hence feel compelled to allocate money to the next-lower-in-rank out of compassion. A second argument relies on subjects’ ex-ante evaluation of the whole repeated game: The subjects can anticipate the positive probability with which they will be assigned any of the ranks. In expectation, each individual will ex-post be assigned the average rank throughout the session, such that the monetary and psychological disutility of being assigned (or losing) a specific rank in any given period will be reduced.Footnote 8 We believe that as the (dis)utility of dwelling in a given rank is the object under investigation, randomly reassigning the subjects’ ranks across periods might undermine the likelihood of capturing the effects of interest. Our third and fourth conditions, fixed No Rank Reversal and fixed Rank Reversal (NoRRfixed and RRfixed, henceforth), address these concerns by keeping ranks strictly fixed across all repetitions of the game. As the subjects in these conditions know that they will keep the assigned rank throughout the whole session, the material and psychological disutility of being assigned a given rank (or of losing it) is expected to be stronger. We therefore predict a stronger aversion to allocating resources to the next-lower-in-rank under fixed ranks, particularly when it implies rank reversals. We implement these conditions using both the monetary allocations originally adopted by KBRN and the allocations we adopted in our first extension to their design, hence both allowing and not allowing for rank reversals to take place. Fixing ranks across periods strengthens moreover the external validity of the experiment. While social mobility or policy interventions allow individuals to move upwards or downwards in the income distribution (perhaps even covering, ultimately, a large portion of it), such movements are likely to be slow. An individual is unlikely to jump from the very bottom to the very top of the distribution, or vice-versa, in a very short time. More likely, they will progressively move from one rank to the next over their lifetime.Footnote 9 Even more unrealistic is for that individual to move erratically in the distribution over time, as random rank reassignment would mimic. Rather, losing even one rank can have a lasting impact with sizeable negative consequences. Individuals will plausibly take actions to prevent such an event from occurring and will hence strenuously oppose any policy perceived at threatening their status. We therefore believe that fixing ranks within our experiment better captures the mechanisms and the incentives at play in real life scenarios.Footnote 10 Our design can be summarised as a \(2 \times 2\) factorial design (Table 1), one cell of which is the original KBRN design. When ranks are randomly reassigned, our findings support neither last place nor rank reversal aversion. The results from KBRN thus fail to replicate, which is consistent with Camerer et al. (2016). When ranks are fixed we find instead support for both mechanisms and can distinguish between them. We find evidence for pure last place aversion in condition NoRRfixed. Subjects ranked second-to-last are less likely to allocate resources downwards, even though such action cannot cause rank reversals, than intermediate ranks which instead allocate downwards significantly more frequently. When ranks can be inverted (condition RRfixed), we find evidence for rank reversal aversion. However, our data suggests rank reversal aversion is not rank-dependent. In fact, subjects in all ranks are less likely to allocate resources downwards, save for the second ranked subject. These results allow us to conclude that both last place aversion and rank-independent rank reversal aversion impact subjects’ behaviour: Last place aversion makes subject ranked second-to-last less likely to allocate downwards even when rank reversals are not possible, whereas rank reversal aversion bites throughout the distribution. and makes all subjects less likely to allocate downwards to prevent disadvantageous rank reversals. Section 2 describes the experimental set-up, Sect. 3 lays out the hypotheses, Sect. 4 presents the results, Sect. 5 discusses and concludes.",2
24.0,3.0,Experimental Economics,01 December 2020,https://link.springer.com/article/10.1007/s10683-020-09681-9,"An experiment on deception, reputation and trust",September 2021,David Ettinger,Philippe Jehiel,,Male,Male,Unknown,Male,"During World War II, the Red Orchestra was the most important spying network of the Soviet Union. It was used to send information to Moscow through radio transmissions. The Germans managed to get control over the Red Orchestra network, and to convince some of its members to work for them. Then, began a new strategy: the Funkspiel. Rather than interrupting the Red Orchestra transmissions, the Germans kept on using it to send information to Moscow. Not only did they send information, but, at least initially, they even sent accurate and important pieces of information. One can guess that the idea of the Germans was to maintain a high level of trust in the mind of the Russians regarding the quality of the Red Orchestra information (because Moscow also knew that radio transmitters could be detected), and to use this communication network to intoxicate the Russian services at a key moment.Footnote 1 The Red Orchestra can be viewed as providing an illustration of the kind of deceptive tactics this paper is about. Cialdini (2006) in his best-seller book on influence provides another illustration. At the end of the chapter on authority, Cialdini reports the story of a waiter named Vincent who was particularly successful at increasing the bill and the tip that goes with it in large party dinners. After letting the first customer in the party make her choice of starter, Vincent would invariably suggest that the pick was not the best on that evening, and would redirect the customer on a cheaper starter. By gaining the trust of the customers, Vincent was able to put forward his recommendation of the most expensive wine afterwards, which would increase considerably the bill. The cases of the Red Orchestra and of the waiter Vincent are vivid examples of repeated information transmission situations in which the agent/organization sending the information and the agent/organization receiving the information may possibly have conflicting interests, and some pieces of information may be attached to higher stakes. We believe that there are many environments with similar characteristics. To suggest a very different context, consider the everyday life of politicians: they intervene frequently in the media and elsewhere (the communication aspect); they sometimes care more about being reelected than just telling the truth about the state of the economy (the potential difference of objective between the sender and the receiver), and as reelection time approaches, they become more anxious to convey the belief that they are highly competent and trustworthy (the high stake event).Footnote 2 The key strategic considerations in such multi-stage information transmission environments are: (1) How does the receiver use the information he gets to assess the likelihood of the current state of affairs but also to assess the type of sender he is facing (which may be useful to interpret subsequent messages)? (2) How does the sender understand and make use of the receiver’s inference process? (3) On the receiver side, it requires understanding how trust or credibility evolves. (4) On the sender side, it requires understanding the extent to which deception or other manipulative tactics are effective. This paper proposes an experimental approach to shed light on deception, reputation, credibility, and trust. Specifically, we summarize below results found in experiments on a repeated information transmission game (à la Sobel 1985) in which the stake of one period is much higher than that of other periods in agreement with the motivating examples discussed above. Senders and receivers are randomly matched at the start of the interaction. Each sender/receiver pair plays the same stage game during twenty periods. In each period, a new state is drawn at random, the sender is perfectly informed of the state of the world while the receiver is not. The sender sends a message regarding the state of the world, then the receiver chooses an action with the objective of matching the true state. The sender is either benevolent and always sends a truthful message (a truthtelling machine in the experiment) or she is malevolent in which case her objective is to induce actions of the receiver as far as possible from the true states of the world. The receiver does not know the type of the sender he is matched with, but he discovers at the end of each period whether the message received during this period was truthful or not. Furthermore, the 5th period is the high stake period, having much more impact than other periods on the overall payoff of both the sender and the receiver (in the Red Orchestra, this high stake could correspond to a key offensive). In our baseline treatment, the key period contributes five times as much as the other periods to the overall payoff (we say the weight of this period is five), and the initial share of benevolent senders represents 20% of all senders. In this treatment, receivers were not informed that human senders had preferences opposed to them, and they were only told that such senders lie on average half of the time.Footnote 3 We considered several variants of the baseline treatment either increasing the weight of the key period to 10 or reducing the share of benevolent senders to 10% or letting the receivers know about the preferences of human senders. We obtained the following results: In the baseline treatment, a large share of senders (roughly 28%), chooses the following deceptive tactic: they send truthful messages up to the period just before the key period and then send a false message in the key period. The share of deceptive tactics followed by malevolent senders is roughly the same whether the initial proportion of benevolent senders is 10% or 20% and whether the weight of the key period is 5 or 10. Receivers are (in aggregate) deceived by this strategy. In the key period, they trust too much a sender who has only sent truthful messages until the key period (i.e., they choose an action which is too close to the message they receive as compared to what would be optimal to do). The deceptive tactic is successful. The behaviors are roughly the same whether or not receivers are informed of senders’ preferences but, this is true when subjects play the game for the first two times, while some learning effect is observed after more plays of the game. Assuming subjects behave as in the sequential equilibrium (SE) of the game does not provide a good account of the observations for several reasons: (1) Senders follow the deceptive tactic too often. (2) The deceptive tactic is successful in the sense that, in our data, deceptive senders obtain higher payoffs than non-deceptive senders while sequential equilibrium would predict that all employed strategies should be equally good. (3) While the sequential equilibrium would predict that the share of senders following the deceptive tactic should increase if the weight of the key period increases and/or if the initial proportion of benevolent senders increases, we see no such comparative statics in our data. Faced with these observations, we suggest interpreting our findings by considering that at least some share of our subjects followed an inference process that is less sophisticated than the one involved in SE. Specifically, given that receivers knew that human senders lie overall half of the time, a simple (though naive) inference process for coarse receivers consists in believing that human senders lie half of the time in every period independently of the history (with a corresponding Bayesian updating process as no lies are observed). If a human sender knew she was facing such a coarse receiver and given her preferences, she would pick the deceptive tactic. Indeed, by telling the truth up to the period just before the key period, she would increase considerably the belief in the coarse receiver’s mind that she is a benevolent machine, which she could exploit at the key period by lying (similarly as in the waiter Vincent story). The kind of reasoning just proposed involving naive inference on the receiver side and deception on the sender side -while at odds with SE- can be captured in the framework of the analogy-based sequential equilibrium (ABSE) developed in Ettinger and Jehiel (2010) (see Jehiel (2005) for the exposition of the analogy-based expectation equilibrium in complete information settings on which EJ build and Jehiel and Samuelson (2012) for an application of ABSE). We observe within the ABSE framework that allowing subjects -senders or receivers- to be either coarseFootnote 4 with probability 3/4 or rational with probability 1/4 provides a good account of the (qualitative) observations made above. Our study relates to different strands of experimental literature. First, it relates to the experimental literature on reputation in games as initiated by Camerer and Weigelt (1988), Neral and Ochs (1992) and Jung et al. (1994) which considers reputation games such as the chain-store game or the borrower-lender game. A key difference with that literature is our focus on repeated sender/receiver communication games in which there is no value for a malevolent sender to being permanently confounded with a machine always telling the truth, but only a value to being temporarily confounded so as to take advantage of it in the key period.Footnote 5 Interestingly, previous studies on reputation games have suggested that the sequential equilibrium may be a good tool to organize the data,Footnote 6 which contrasts with our finding that theories beyond the sequential equilibrium are needed to give a reasonable account of the data in our experiment. Our study is also related to a lesser extent to the experimental literature on non-repeated strategic information transmission games à la Crawford and Sobel (1982) that was initiated by Dickhaut et al. (1995) and Blume et al. (1998) (see also Blume et al. (2001), Cai and Wang (2006), Kawagoe and Takizawa (2009) or Wang et al. (2010)). That literature has noted that senders have a tendency to transmit more information than theory predicts suggesting that (at least some) senders may be averse to lying.Footnote 7 It has also suggested that receivers may be more credulous than theory predicts. Our study is complementary to that strand of literature to the extent that our main interest is focused on the timing of the lies and the dynamic inference process which cannot be studied in non-repeated communication games.",1
24.0,3.0,Experimental Economics,12 October 2020,https://link.springer.com/article/10.1007/s10683-020-09683-7,Friend or foe? Social ties in bribery and corruption,September 2021,Jin Di Zheng,Arthur Schram,Gönül Doğan,Female,Male,Female,Mix,,
24.0,3.0,Experimental Economics,24 November 2020,https://link.springer.com/article/10.1007/s10683-020-09684-6,Coordinating expectations through central bank projections,September 2021,Fatemeh Mokhtarzadeh,Luba Petersen,,Female,Female,Unknown,Female,"The economy is highly complex with many moving parts. It can be very challenging for the average person, with limited cognitive capacity and attention, to accurately forecast how it will evolve. To ease this cognitive burden, guide expectations, and improve monetary policy efficacy, which operates largely through an expectations channel, central banks have become increasingly transparent about their objectives, future policies, and their outlook on the future. Many central banks publish a combination of projections about future GDP, GDP growth, CPI, and their policy rates. The Reserve Banks of Australia, New Zealand, and Norges Bank were pioneers in the publication of their inflation projections during the early 1990s. Likewise, the Reserve Bank of New Zealand has communicated their projections for the 90-day bank bill rate via Monetary Policy Statements since 1997. Other central banks followed suit with Norway (2005), Sweden (2007), and the FOMC (2012) publishing their projections of key policy rates. Central banks face two critical decisions when constructing and communicating their projections. First, they must make numerous assumptions about how the economy evolves, including how people think about the future. Many central banks construct their projections under the assumption that households and firms form rational expectations. While projections based on the assumption of non-rational expectations may be more accurate and may enhance central bank credibility, it is not clear which of many models of non-rational expectations to use. Assumptions about the form of aggregate expectations can have important implications for predicted aggregate dynamics and optimal monetary policy. Moreover, the information in projections has the potential to influence the heuristics individuals use to form their expectations.Footnote 1 Second, central banks must decide which of their many projections to communicate to the public. Too little information may insufficiently guide public expectations and the central bank to be perceived as “opaque”; too much information can result in cognitive overload and audiences not paying sufficient attention to any particular information. The contribution of this paper is to provide empirical insight into these two important policy decisions. We study individual and aggregate forecasts in 24 multi-period laboratory economies where we can systematically control the information that central banks communicate about their projections in otherwise identical underlying economies. In each period of our experiments, each subject reports incentivized forecasts of the following period’s rate of inflation and the output gap. Aggregate expectations and a random disturbance to aggregate demand endogenously determine the current state of the economy. We study the effects of four different types of central bank projections on individual forecasting heuristics and aggregate dynamics. In our baseline environment, participants observe current and historical information about the economy, as well as full information about the economy’s data-generating process. We compare our benchmark economies, where the central bank does not communicate its projections, to treatment economies operating under three alternative communication policies. In our Interest Rate Projection treatment, all subjects observe the central bank’s projection of future nominal interest rates, derived according to the economy’s rational expectations equilibrium (REE) solution. In the Dual Projections treatment, all subjects are instead informed about the central bank’s projection of future inflation and the output gap, also derived using the REE solution. For a rational subject, the communications in either of these two projection treatments are redundant and should not influence expectations. For boundedly rational subjects, however, such projections provide potentially useful focal information. While both of these REE projections convey the same overall information about the economy, we hypothesized that Dual Projections would be cognitively less demanding for subjects to apply to their private output gap and inflation forecasts. Finally, the Adaptive Dual Projections treatment mirrors the Dual Projections treatment except that the central bank’s projections follow an adaptive model of expectations that, based on previous work, we expect would better predict aggregate dynamics, and thus, reduce credibility concerns. The purpose of the Adaptive Dual Projections treatment is to address discussions in policy circles as to whether to implement boundedly rational agents into central banks’ forecasting models. We find that certain central bank projections can significantly stabilize expectations and the aggregate experimental economy by nudging naïve forecasters towards fundamentally-driven rational expectations. Rational projections of future output gap and inflation result in consistently less dispersion in forecasts and significantly smaller forecast errors. By contrast, projections of nominal interest rates lead to mixed results. For relatively low variability in aggregate demand shocks, subjects are willing and able to employ the projections, resulting in significantly more rational forecasts. However, as the variability of shocks increases, the ease and value of using the projection decreases, and subjects instead rely on adaptive forecasting heuristics. These results suggest that policymakers cannot take for granted that private agents can infer the implied path of inflation and the output gap from an interest rate projection. Rather, central banks concerned about anchoring a specific type of expectation should directly communicate about that variable of interest. Adaptive dual projections generate significantly larger inflation forecast errors, greater forecast dispersion, and increased inflation variability. These effects are a consequence of a large fraction of subjects directly employing the central bank’s adaptive projections as their own while other participants respond to their counterparts’ adaptive behavior by forecasting even higher inflation. Our paper provides original empirical evidence that inflation-targeting central banks may prefer to avoid communicating a projection based on adaptive expectations. Loss of credibility is a crucial concern central banks face when deciding whether to communicate their projections. We find that this concern is valid when the central bank communicates either a nominal interest rate projection or an adaptive dual projection. Under both types of projections, the likelihood a subject employs the central bank projection decreases as the central bank makes larger forecast errors in the recent past. Effective usage of the interest rate projections is very low as it is more challenging to infer what the projection implies about future output and inflation. In the Adaptive Dual Projections treatment, subjects’ credibility in the projection is very high but sensitive to the central bank’s forecast errors. As the central bank’s implied forecast of future output and inflation becomes increasingly incorrect, the likelihood that subjects utilize the projections significantly decreases. By contrast, the central bank’s credibility appears to be impervious to its forecast errors when it publishes rational dual projections. Our paper complements the existing empirical and theoretical work on the role of central bank communication and projections in shaping expectations. The empirical literature has found mixed evidence on the effectiveness of forward guidance in influencing expectations (Kool and Thornton 2015; McCaw and Ranchhod 2002; Goodhart and Lim 2011; Brubakk et al. 2017; Turner 2006) while macroeconomic projections appear to more consistently manage inflation expectations. Hubert (2014) finds a significant positive relationship between inflation projections and forecasters’ expectations of inflation in Sweden, the UK, Canada, Japan, and Switzerland. In a closely related paper to ours, Jain and Sutherland (2018) construct an original panel data set of 23 countries to estimate the effects of numerous central bank projections and forward guidance on private-sector forecast dispersion and accuracy. They find that macroeconomic central bank projections are generally ineffective at reducing private-sector forecast dispersion and forecast errors about inflation and output growth. They do, however, reduce forecast errors and dispersion about short-term interest rates. Interest rate projections do not have a large or consistent effect on inflation expectations. Goy et al. (2020) computationally examine agents’ expectations near and at the zero lower bound (ZLB). They find that forward guidance in the form of output and inflation projections significantly reduce the likelihood of deflationary spirals when the economy is at the ZLB. Likewise, theoretical work by Ferrero and Secchi (2010) highlights that macroeconomic projections are more effective than interest rate projections at stabilizing expectations of recursively learning agents. Our paper provides experimental validation of these results and additional insight into the consequences of modifying projections in response to the public’s backward-looking behavior. Moreover, our findings provide original empirical support for the policy recommendation that strict inflation-targeting central banks disregard the public’s adaptive forecasting heuristics when designing their communication strategy. Learning-to-forecast experiments (LtFEs) are used to study how macroeconomic expectations respond to information, policy, and structural features of the economy. In LtFEs, subjects play the roles of forecasters and are tasked with forming accurate forecasts for the following period(s) over a long multi-period horizon. Each period, aggregated forecasts are used by computerized households, firms, and banks to make decisions according to a pre-specified data-generating process. In other words, subject-provided aggregate expectations about the future have a direct effect on the current macroeconomy. The assumption that expectations influence economic decision making is supported by recent experimental evidence. In a field experiment involving participants in the University of Michigan Survey of Consumers and RAND’s American Life Panel, Armantier et al. (2015) observe, on average, a correlation between participants’ expectations and decisions in a manner consistent with economic theory. Cornand and Hubert (2020) investigate the external validity of expectations elicited in LtFEs. They find that expectations elicited from undergraduate participants in LtFEs are consistent with those formed by households, firms, professional forecasters, financial market participants, and central banks in that forecast errors are large, serially correlated, and predictable indicative of information frictions.Footnote 2 There are a handful of LtFEs that investigate the effect of central bank communication on expectation formation.Footnote 3 Kryvtsov and Petersen (2013, 2020) study, among many things, the effects of central bank projections of nominal interest rates. They find that focal five-period ahead interest rate projections have an inconsistent effect on forecasting behavior. Many inexperienced subjects incorporate the projections into their forecast, leading to improved inflation stability in some sessions. However, if only a few subjects initially employ the projections in their forecasts, the announcement creates confusion, and expectations become increasingly destabilized. Like Kryvtsov and Petersen (2013), we find that nominal interest rate projections lead to heterogeneous heuristics. Our paper extends their findings by providing a more robust study of different types of projections. We additionally consider rationally- and adaptively-formed inflation and output gap projections to gain insight into the ability of central bank projections to influence expectations and maintain central bank credibility. The communication of inflation targets has also been shown to have mixed effects on the management of expectations. Under a dual mandate to stabilize both inflation and output gap, Cornand and M’baye (2018) find that expectations and inflation are better anchored, while Mirdamadi and Petersen (2018) observe increased heterogeneity in heuristics as forecasters have more information to coordinate on. Arifovic and Petersen (2017) find that communicating a time-varying, history-dependent inflation target can make expectations even more pessimistic at the ZLB when the central bank fails to achieve its explicit targets. Ahrens et al. (2016) have extended our paper and Arifovic and Petersen (2017) to study the effects of one-period ahead inflation projections in the presence of both demand and supply shock in the normal times or at the zero lower bound. Similar to our findings, they observe that central bank inflation projections significantly alter how subjects forecast and reduce economic instability at the zero lower bound. The paper is organized as follows. Section 2 lays out our theoretical framework and experimental design. Results are presented in Sects. 3 and 4 discusses our findings in the context of the learning and inattention literature.",10
24.0,3.0,Experimental Economics,15 October 2020,https://link.springer.com/article/10.1007/s10683-020-09685-5,"Property, redistribution, and the status quo: a laboratory study",September 2021,Konstantin Chatziathanasiou,Svenja Hippel,Michael Kurschilgen,Male,Female,Male,Mix,,
24.0,3.0,Experimental Economics,04 November 2020,https://link.springer.com/article/10.1007/s10683-020-09686-4,In absence of money: a field experiment on volunteer work motivation,September 2021,Vanessa Mertins,Christian Walter,,Female,Male,Unknown,Mix,,
24.0,3.0,Experimental Economics,11 November 2020,https://link.springer.com/article/10.1007/s10683-020-09687-3,The relevance of irrelevant information,September 2021,Ian Chadd,Emel Filiz-Ozbay,Erkut Y. Ozbay,Male,Female,Male,Mix,,
24.0,3.0,Experimental Economics,03 November 2020,https://link.springer.com/article/10.1007/s10683-020-09688-2,The ABC mechanism: an incentive compatible payoff mechanism for elicitation of outcome and probability transformations,September 2021,Yi Li,,,,Unknown,Unknown,Mix,,
24.0,3.0,Experimental Economics,21 November 2020,https://link.springer.com/article/10.1007/s10683-020-09689-1,Returns to effort: experimental evidence from an online language platform,September 2021,Fulya Ersoy,,,Female,Unknown,Unknown,Female,"There has been a growing interest in the use of educational technology (Escueta et al. forthcoming) and a steady increase in the number of students who engage in distance learning in recent years (Seaman et al. 2018).Footnote 1 Although technology-assisted learning has the potential to be a transformative force in education due to its flexibility, accessibility (Goodman et al. 2019), and affordability (Deming et al. 2015), we do not know much about the effectiveness of effort in these learning environments. High attrition rates in distance learning environmentsFootnote 2 combined with endogenous effort choices of students render measuring the causal returns to effort in these settings quite difficult. In this paper, I design an experiment in which study effort is exogenously manipulated and cleanly observed to measure the returns to effort for one of the most popular language learning platforms, Duolingo. The aim of this paper is not to evaluate the effectiveness of Duolingo, it is rather to understand whether and how study effort affects performance in a remote setting with technology-assisted learning. To measure causal returns to effort in a technology-assisted learning environment, I design an experiment. First, I recruit college students who want to learn Spanish. I assess their initial Spanish knowledge with two tests. Students take an internal test which is based on Duolingo lessons and an external test, WebCAPE. Then, students sign up for Duolingo. I randomly assign them to one of the five groups (32-lessons, 48-lessons, 64-lessons, 80-lessons, and 96-lessons) in which they are asked to complete a varying number of Duolingo lessons for four weeks. To increase compliance with assignments, I require students to commit to complete the number of lessons assigned, I send them email reminders twice a week about their progress, and I tie their compensation to their compliance with the lesson assignments. I observe students’ effort within Duolingo through administrative data. After four weeks, students once again take the internal and external tests which assess their final Spanish knowledge. Both of these tests are relevant metrics of performance since the internal test is similar to performance measures in classroom settings (i.e. lecture-based assessment) and the external test is used by many post-secondary institutions to place students into the appropriate levels of language courses. The experimental design is successful at manipulating Duolingo effort without causing differential attrition. Random assignment to different lesson groups generates the necessary exogenous variation in Duolingo effort. Most of the students complete their assigned number of lessons. The difference between the distributions of the number of completed lessons for any two assignment groups is statistically significant. Furthermore, there is no evidence of differential attrition across different assignment groups. I find that effort within Duolingo positively impacts test performance. It is often debated what test scores measure. Test scores are likely to be influenced by different factors such as innate ability, effort invested in studying, effort on the test, and random factors unrelated to the test. In this paper, I am able to show that effort invested in studying improves test scores. In particular, I find that completing 9 lessons, which corresponds to approximately 60 minutes of Duolingo, results in a 0.095 standard deviation (sd) increase in internal test scores (statistically significant at 1% level) and a 0.057 sd increase in external test scores (statistically significant at 5% level) using an instrumental variables approach.Footnote 3 Furthermore, I find that the effectiveness of Duolingo positively correlates with college GPA. The estimated effects are for intrinsically motivated college students for an introductory level course. Hence, the generalizability of the results to other populations or contexts should be explored. The results suggest that effort in Duolingo can be as effective as effort in in-person courses. In particular, I find that a one sd increase in the number of lessons completed leads to a 0.19–0.32 sd increase in test scores. These effects are comparable to the effects reported in the literature for in-person (non-language) courses [0.20 sd in Eren and Henderson (2011), 0.25 sd in Bonesrønning and Opstad (2012), and 0.14 sd in Bonesrønning and Opstad (2015)]. Moreover, using WebCAPE’s benchmark scores for placement into college-level language courses, a back-of-the-envelope calculation suggests that the effectiveness of Duolingo effort is comparable to the effectiveness of effort in in-person language courses. In particular, an individual needs to complete, on average, 38 hours with Duolingo to be placed in the second semester of college level Spanish course which is comparable to the time spent in a semester of in-person Spanish course. However, we should take this conclusion as suggestive due to the linear extrapolation assumption used for the back-of-the envelope calculations and the noisiness of the results for WebCAPE test scores. This paper contributes to three areas in the literature. First, it contributes to the literature on returns to effort. A limited number of studies explore the causal relationship between study effort and educational outcomes due to the fact that effort is multi-dimensional, difficult to observe, and hard to manipulate in most settings. To overcome the difficulty of unobservability, researchers frequently use students’ self-reports of effort (Krohn and O’Connor 2005; De Fraja 2010; Kuehn and Landeras 2012). However, self-reports can contain substantive reporting error (Stinebrickner and Stinebrickner 2004) and can suffer from social desirability bias (Nederhof 1985; Furnham 1986). In this study, I measure effort using administrative data on the number of lessons completed, which provides a quantitative measure of effort while controlling for quality due to the structure of Duolingo. To deal with the endogeneity of effort, researchers use potentially exogenous variation in the value of leisure (Stinebrickner and Stinebrickner 2008; Metcalfe et al. 2019), change financial incentives associated with effort (Angrist and Lavy 2009; Fryer 2011; Hirshleifer 2016), modify non-financial incentives associated with effort (Bonesrønning and Opstad 2015; Chevalier 2018), control for student and teacher fixed effects (Eren and Henderson 2011), use natural variation in the amount of homework assigned or manipulate whether homework submission is compulsory (Betts 1996; Eren and Henderson 2008; Grodner and Rupp 2013), modify course materials covered in lectures (Chen and Lin 2008), and vary whether attendance is mandatory (Dobkin et al. 2010). In this study, I create exogenous variation in effort by randomly assigning different numbers of lessons to students in a remote setting. To my knowledge, this is the first study that estimates the causal returns to effort for a remote setting with technology-assisted learning. Second, this paper contributes to the small but growing literature on technology-assisted learning. The literature on technology-aided instruction has mixed findings with widely varying impacts, from significantly negative effects to significantly positive effects (Glewwe and Muralidharan 2016). The highly effective interventions seem to be the ones that uses technology to personalize the learning to the level of the student (Banerjee et al. 2007; Barrow et al. 2009; Muralidharan et al. 2019) and the ineffective interventions are the ones that use technology as a substitute rather than as a complement to traditional instruction methods (Lai et al. 2015; Linden 2008). Most of this literature focuses on using technology in a school or after school context where the teachers and peers are available. Duolingo utilizes technology-aided instruction methods effectively as a remote learning platform. In particular, it personalizes and tailors the learning experience of its users through technology, it provides real time feedback, and it uses gamification features to improve attention. I contribute to this literature by showing that well-designed technology aided online instruction can be as effective as in-person learning as long as students exert effort. Third, this paper contributes to the distance learning literature that explores the causes and remedies of low student engagement and high attrition rates (see Bawa 2016, for a literature review). In a survey of 1,698 learners in 20 different online courses, Kizilcec et al. (2015) identify time management as the main reason for attrition. As a remedy to these problems, researchers try using small nudges in online settings to improve course outcomes (Kizilcec and Brooks 2017).Footnote 4 My paper contributes to this literature by showing that lesson assignments coupled with modest monetary incentives and reminders increase student effort successfully in a remote environment.Footnote 5",3
24.0,3.0,Experimental Economics,24 March 2021,https://link.springer.com/article/10.1007/s10683-021-09711-0,Correction to: Improving decisions with market information: an experiment on corporate prediction markets,September 2021,Ahrash Dianat,Christoph Siemroth,,Unknown,Male,Unknown,Male,Co-author Zultan name missing in citation of article Information aggregation in Arrow–Debreu markets: an experiment Original article has been updated.,
24.0,4.0,Experimental Economics,12 December 2020,https://link.springer.com/article/10.1007/s10683-020-09690-8,Sharing or gambling? On risk attitudes in social contexts,December 2021,Stefan Grimm,Martin G. Kocher,Fabrice Le Lec,Male,Male,Male,Male,"Most decisions under risk take place in a social context. Other individuals than the decision-makers themselves are present or involved when decision-makers have to make choices under risk. In these situations, other individuals or their behavior may, directly or indirectly, influence the decision-maker and her decisions (Trautmann and Vieider 2012).Footnote 1 One channel through which social context may affect decision under risk is the possibility of social comparison. Thus, the question of whether elicited risk attitudes are affected by the relative social position of the decision-maker arises. The existing empirical literature on this question is inconclusive, despite the ubiquity of day-to-day decisions under risk for which social comparisons are possible. Our paper analyzes the influence of the relative social position of a decision-maker on her elicited risk attitudes, using a new experimental protocol that allows for additional insights. More specifically, we implement a laboratory experiment in which the decision-maker has to choose either to share a prize with another player according to a given distribution, or to gamble for the entire prize. The two options (sharing or gambling) are set to be equivalent in terms of expected payoff, and hence the choice between them provides a measure of risk attitude in a social context. By systematically varying the distribution of the prize as well as the probability to obtain it, we can systematically change the relative social position of the decision-maker vis-à-vis the other player and directly observe effects of the relative social position on risk-taking. Our experiment reproduces, in a simplified manner, important aspects of situations that involve risk in a social context, especially those for which the total amount, endowment or resource distributed is constant. This corresponds to circumstances in which an individual can either favor a given distribution of financial resources and power, or take the chances of obtaining the entire prize herself. For instance, an executive can accept the proposed split of available funding between her and another manager’s project or argue that the company should focus on just one of them, with the risk of losing out. A political leader may have to choose between accommodating the current division of power between herself and a party rival or go for a shootout that will leave just one of the two standing. A poker player in a cash game can leave the table with current possessions or continue playing until all is lost or won. In any of these examples, not only the relative position (being ahead or behind) may affect the decision-maker’s inclination to take risk, but the mere fact of being in a social context may lead her to take more (or less) risk. To sum up, our setup involves the comparison of decisions on implementing a given distribution in a social context, when either being ahead or behind (favorable inequity or unfavorable inequity), and a control setting in which equivalent decisions are taken in isolation (i.e. without social context). At least three effects established in the relevant literature may influence risk attitude through social comparisons. First, positional concerns (e.g., Charness et al. 2013; Kuziemko et al. 2014) may provide additional utility to winners or reduce utility of losers. They may lead decisions makers to take more risk than they would do otherwise if there is a possibility to be ahead of another individual. Vice versa, they may lead decision-makers to be more cautious if gambling involves the possibility of falling behind. Second, decision-makers may feel gloating and envy with different intensity. That is, decision-makers subjectively value an additional dollar differently when being ahead than when catching up from behind. Bault et al. (2008) provide evidence for such an effect that may lead decision-makers to seek risky options that provide a chance of being ahead of others. Third, utility curvature may be different in the socially unfavorable domain (behind others) than in the socially favorable domain (ahead of others). Linde and Sonnemans (2012) propose the idea of a social reference point applied to prospect theory (Kahneman and Tversky 1979; Tversky and Kahneman 1992) that would make decision-makers risk-averse in the favorable domain and risk-seeking in the unfavorable domain. With regard to the general question of whether social comparisons affect risk-taking and, in particular, if so, in which direction, the existing experimental economics literature is quite inconclusive hitherto: Linde and Sonnemans (2012) find, in contrast to their theoretical predictions, that decision-makers are more risk-seeking when being ahead of another individual. Bault et al. (2008) observe that decision-makers are slightly more risk-seeking for mixed lotteries in the social domain, i.e. with consequences in the socially favorable and the socially unfavorable domain. Bolton and Ockenfels (2010) as well as Gamba et al. (2017) in a real-effort task find, in contrast, that decision-makers are more risk-seeking when being behind another individual.Footnote 2 In light of the existing mixed evidence, our experimental setting provides the following benefits: the design is parsimonious, it clearly identifies the effect of social comparisons, it varies systematically the relative positions of the decision-maker, and it compares (otherwise identical) decisions in a social environment and in isolation. Furthermore, it avoids confounding factors—sometimes present in other studies—such as (ex post) social preferences or specific peer effects (conformism). Our main finding is that the number of risky choices is strongly affected by the social context. Subjects in our experiment are more risk-seeking when the deterministic option involves unfavorable inequity than in the same decision in isolation. In contrast, a favorable social context (when the deterministic option corresponds to favorable inequity) does not increase the willingness to take risks compared to an identical decision without social context. The analysis of individual behavior suggests that most of this asymmetry is driven by about two-thirds of experimental participants who very strongly exhibit this choice pattern of becoming more risk-seeking in what appears to be the social loss domain. The pattern is robust to various controls and sensitivity checks. A deeper inspection of our data suggests that neither a difference in the intensities of gloating and envy, on the one hand, nor a mere concern for being ahead in an ordinal ranking, on the other hand, can explain all aspects of the data pattern. Among the documented effects found in the literature, the difference in curvature in the favorable and unfavorable social domain seems the only explanation compatible with our data. In particular, the hypothesis that the other participant’s payoff plays the role of a (social) reference point, below which the decision-maker is risk-seeking (social loss domain) and above which she is risk-averse (social gain domain) seems to be consistent with the observed pattern. The remainder of the paper is organized as follows: the next section presents the experimental design and procedures; Sect. 3 shows our results, and Sect. 4 discusses the results in light of the existing literature and existing theories of decision-making under risk. Section 5 concludes the paper.",1
24.0,4.0,Experimental Economics,11 January 2021,https://link.springer.com/article/10.1007/s10683-020-09691-7,Pecunia non olet: on the self-selection into (dis)honest earning opportunities,December 2021,Kai A. Konrad,Tim Lohse,Sven A. Simon,Male,Male,Male,Male,"“Pecunia non olet” (money doesn’t stink), as the Roman emperor Vespasian said to alleviate concerns that a presumably impure source of money would lower its value. One such impure source is dishonestly earned money. While some people are incorruptible and will suffer from lying, others follow Vespasian’s adage and will cheat for personal gain without hesitation. This trait allows them to extract additional rents, and may make professions that allow for misreporting opportunities particularly attractive to them. Indeed, survey evidence indicates large differences in trust in different professions. Typically, those professions with cheating opportunities, such as insurance agents, advertising executives but also politicians are the least trusted ones. In contrast, professions with little potential for manipulation, such as firefighters or teachers, are among the most trustworthy professions globally.Footnote 1 These opposites in the trustworthiness of different professions involve a deeper problem of causality. On the one hand, it is conceivable that the mere existence of a cheating opportunity in a certain profession might induce dishonesty. On the other hand, it is also conceivable that individuals self-select deliberately ex ante into different professions with different degrees of cheating opportunities. Such a self-selection could occur according to an individual’s attitude toward honesty because money might stink for some, but not for others. These considerations lead to empirical questions. Are individuals willing to pay to earn in an honest fashion? How badly do they suffer as a side effect of dishonest earnings? How is this lying cost related to their choice between an honest earning opportunity and one that allows them to make more money, but only if they are prepared to cheat? Finally, is dishonest behavior the outcome of the misreporting opportunity per se, or is it rather the outcome of deliberate self-selection into such opportunities? In other words, do certain professions attract dishonest people, or is it the cheating opportunity that makes people dishonest? We provide causal evidence of a self-selection effect, and rule out that the mere likelihood of facing the cheating opportunity induces dishonest behavior. Moreover, we show that the individual’s lying costs determine the self-selection. Hence, abstracting from self-selection may lead to an underestimation of actual cheating behavior. We generate these insights from a theory-guided experiment in the laboratory in which individuals choose between two binary lotteries in two different choice conditions. One lottery, let us call it the ‘Bad Lottery,’ has a zero outcome with high probability and a win outcome with a low probability. The other lottery yields the zero outcome with a low probability and the win outcome with a high probability. Let us refer to it as the ‘Good Lottery.’ The Bad Lottery is costless. To obtain the Good Lottery instead, individuals have to pay a price. We elicit individuals’ willingness to pay (WTP) for the Good Lottery which has higher odds of having an opportunity to earn honestly instead of earning money through a misreporting opportunity. This WTP can reflect how badly different individuals suffer from making dishonest earnings. Therefore, this WTP can be seen as a subtle way to measure the heterogeneity in lying costs. Their lottery choice is recorded for two different conditions. One condition is truthful by design, and the lottery outcome is identical to the individual’s final payout. Here, an individual’s attitude toward truth-telling is irrelevant. The other condition allows for misreporting. In particular, the individual may claim the win outcome even if the true lottery outcome is zero. Observing individuals’ actual reporting behavior when the misreporting opportunity emerges allows us to study how the WTP and the reporting behavior are related to each other. These findings give a good indication of whether individuals self-select into honest or dishonest earnings based on their lying costs. Our results indicate that some, but not all individuals have a smaller WTP for the Good Lottery in the condition with the misreporting opportunity. Some individuals are not willing to spend money to obtain the Good Lottery. They also tend to misreport whenever the opportunity arises. Other individuals expend considerable resources to obtain the Good Lottery. These individuals with a higher WTP are also less likely to misreport if the misreporting opportunity emerges. Importantly, the finding does not depend on whether they are assigned to the preferred Good Lottery or whether they are exogenously assigned to the Bad Lottery instead. Their consistently honest behavior clearly indicates that (dis)honesty is determined by self-selection and not by the probability of facing the cheating opportunity. Our work is related to the literature on cheating in general (as recently surveyed by Abeler et al. 2019 and Gerlach et al. 2019), and to two strands of this literature in particular. One literature strand touches on the issue of the self-selection of dishonest subjects into, or out of, lying opportunities. Several field experiments analyze the relationship between the occupational choice to work in corrupt sectors and the individual propensity to cheat. Departing from high corruption in the public sector of India, Banerjee et al. (2015) show that aspirants for public sector jobs engage in more corruptive behavior compared to aspirants for the private sector. Similarly, Hanna and Wang (2017) find a positive correlation between dishonest behavior in the laboratory and fraudulent behavior in the field for government nurses. A further dimension is the role of cheating opportunities for self-selection into competitive settings as documented by Faravelli et al. (2015) and Gino et al. (2015). However, clean and theory-guided evidence on whether subjects deliberately self-select into cheating opportunities based on their lying costs is limited so far. In Shalvi et al. (2011) the individuals are offered a fixed payoff as an exit option over participating in the dice-rolling game. In their framework the take-up of the exit option is not much affected by the possibility to lie, and lying behavior seems to be almost unrelated to the value of the exit option. We advance this literature since our design allows us to show that it is self-selection rather than the mere existence of a cheating opportunity which drives dishonest behavior. Additionally, by relating the WTP for honest earnings to reporting behavior, we identify an individual’s lying costs as the determinant of self-selection. A potential implication is that screening procedures for applicants might be particularly promising for fighting dishonest behavior in certain professions. Second, we contribute to the literature on the role of monetary incentives for lying and the measurement of lying costs. One approach is to vary the incentive size between treatments/experiments. Fischbacher and Föllmi-Heusi (2013) do not find a significant increase in lying when monetary incentives are tripled. Kajackaite and Gneezy (2017) come to a similar conclusion and show that the lying rate is largely insensitive to the multiplication of stakes. In meta-studies, Abeler et al. (2019) and Gerlach et al. (2019) confirm this pattern and document only little variation in dishonest behavior with respect to the reward size. Hilbig and Thielmann (2017) use an alternative approach and vary the incentive size within-subjects for repeated cheating opportunities. They find evidence of ‘brazen liars’ and ‘incorruptible subjects,’ but also of subjects whose reporting decision is dependent on the stake size. However, the repeated setting might have an effect on cheating in itself. Finally, Gibson et al. (2013) and Vranceanu and Dubart (2019) elicit subjects’ reservation price to engage in fraudulent reporting and deceptive communication. Both find considerable heterogeneity in reservation prices, which suggests that misreporting is sensitive to monetary incentives. In contrast to most of the literature on dishonest decision-making, this approach implies that cheating is not a binary decision but rather there is a price for being dishonest by design. We propose a more subtle procedure to measure lying costs at the individual level. Our setting allows us to pin down a subject’s lying costs in a two-step procedure, but avoids implying that there is a price for misreporting by design. Hence, we complement the findings of the previous literature, and resolve some of the conflicting results on the heterogeneity of lying costs. We proceed as follows. First, we sketch the backbones of the decision-theoretic framework which is closely mapped in our experimental design. We then derive predictions and provide a benchmark for the interpretation of the behavior in our laboratory experiment. We describe and discuss our experimental results before we finally conclude.",2
24.0,4.0,Experimental Economics,15 January 2021,https://link.springer.com/article/10.1007/s10683-020-09692-6,Do people really want to be informed? Ex-ante evaluations of information-campaign effectiveness,December 2021,Romain Espinosa,Jan Stoop,,Male,Male,Unknown,Male,"Public policies usually aim to modify deeply-anchored behaviors in the population. Laws and taxes are the government’s most direct way of changing behavior, but they are not always very popular in the eyes of the general public. The Yellow Vests movements in France, Belgium and the UK are examples of how the public opinion has turned against the government (Anderson 2019). On the other hand, information campaigns are a softer measure that the government can use. The tool is easier to implement, but is by no means cheap. For example, the UK government plans to run 140 information campaigns in 2018–2019 (Griggs 2018), with annual expenditures being over £300 million (Reynolds 2014). The impact of information campaigns is obviously limited when individuals do not want to be informed about the topic. This may be the case when cognitive dissonance is strong (Festinger 1962; Brehm and Cohen 1962; Bem 1967; Elliot and Devine 1994; Cooper 2007, e.g.). We propose a method to quantify the relative weight of information resistance, of which cognitive dissonance is a large part, in a particular knowledge domain. More specifically, we propose a question chart that distinguishes various dimensions of the knowledge spectrum. We separate seven types of beliefs, varying from pretending not to have knowledge to wrongfully thinking one has accurate knowledge (see Fig. 1, and Appendix A in Electronic Supplemntary Material for a rationalization). After developing the knowledge spectrum, we present an experimental method to disentangle the seven types of beliefs in a sample of over 7,000 UK residents. They are presented with a series of statements and have to report whether each statement is true or false. We exogenously vary if subjects are incentivized for their answers (stated vs. revealed beliefs), and if we provide an information link to a website showing the correct answer. Based on the experimental outcomes, we construct a measure of how effective information campaigns will be, the information-campaign effectiveness index (ICEI). We then use these ICEI’s to predict the rank order of the impact of information campaigns using a new set of experiments. We apply our method to three topics for which individuals might engage in motivated reasoning, while the government has an interest in having a well-informed population. These topics are animal-based diets, alcohol consumption and immigration. The psychology literature extensively discussed the presence of cognitive dissonance for animal-based diets, and we suspected a potential for resistance to information for alcohol consumption and immigration as well. We summarize our results as follows. First, across topics, we find sharp differences in the presence of information resistance. It is most prevalent in animal-based diets, at roughly 12 percent of our subjects’ knowledge spectrum. This is in line with studies in psychology (Loughnan et al. 2014; Graça et al. 2016; Piazza and Loughnan 2016). For alcohol, information resistance is close to 0 percent, but rises to almost 3 percent for heavy drinkers. Immigration falls at 1.5 percent. Second, as a consequence of the first result, we find that the ICEI differs across the topics. Most room for an effect of an information campaign is expected for immigration, because subjects report low levels of knowledge and low levels of information resistance. These findings are consistent with recent works on information effects on the perception of immigration (Facchini et al. 2016; Grigorieff et al. 2018). Lower levels of improvement are expected for alcohol consumption, because of the combination of good prior knowledge and little information resistance. Low levels of improvement are expected for animal-based diets too. Subjects report a low level of knowledge, but face fierce information resistance. Third, we test these predictions by simulating information campaigns in an additional set of experiments. The results show that information campaigns do indeed have the greatest impact for immigration. The impact is smaller and similar for diet and alcohol consumption. Last, we present several robustness checks on our measures of information resistance and the ICEI. Among these checks, we show that our results are not driven by (i) respondents exerting extra cognitive effort in the incentivized treatments, (ii) respondents conducting side online searches, (iii) specific questions in our questionnaires, (iv) an experimenter demand effect, (v) an order effect of the control covariates we gather. The simplicity and effectiveness of our method make it an attractive tool for public authorities, and even private companies, that wish to launch information campaigns. This is the first method that can predict ex-ante the potential success of information campaigns. Given the budget constraints that governments face, our method can help to set priorities over competing information campaigns. Moreover, our method provides governments with an idea of the potential opposition that reforms may face: topics with higher information resistance scores are likely to receive more criticism. Our protocole brings therefore more information than what pilot information campaigns would: it does not solely measure the effectiveness of a specific information campaign, but assesses the maximum effectiveness an information campaign on a specific topic could reach. Finally, this method can be used as a selection device, helping governments choose appropriate arguments to back up policies. One main component of information resistance is cognitive dissonance. We are not the first to consider these topics in economics. However, we do move this literature in a new direction by using an incentivized method that allows us to measure its extent. Cognitive dissonance was first introduced in economics by (Akerlof and Dickens (1982)). The concept was used to enrich economic theory, providing ways in which cognitive dissonance at the personal level affects behavior at the aggregate level. Examples can be found regarding issues such as social change (Rabin 1994; Benabou and Tirole 2006; Oxoby 2004), duty-oriented consumption (Nyborg 2011) and the labor-market consequences of discrimination (Goldsmith et al. 2004). In empirical and experimental evidence, dissonance is induced exogenously without incentives. Differences in treatments confirm the existence of cognitive dissonance, but remain silent about its magnitude. Contributions in this spirit can be found in the fields of finance (Goetzmann and Peles 1997; Antoniou et al. 2013; Chang et al. 2016), other-regarding behavior (Konow 2000; Dickinson and Oxoby 2011; Matthey and Regner 2011), and social and political attitudes (Di Tella et al. 2007; Elinder 2012). Finally, our work is closely related to recent works in political literature that show that incentivized surveys substantially reduce the gap in stated beliefs between groups of partisans (Bullock et al. 2015; Prior et al. 2015). In the remainder of the paper, Sect. 2 presents our theoretical background. Section 3 explains the experimental method, and Sect. 4 shows the results. Section 5 tests the predictions of our ICEI estimates, Sect. 6 shows robustness checks and Sect. 7 concludes.",3
24.0,4.0,Experimental Economics,15 February 2021,https://link.springer.com/article/10.1007/s10683-020-09693-5,Experimental evidence of limited attention at the gym,December 2021,Wolfgang Habla,Paul Muller,,Male,Male,Unknown,Male,"The prevalence of insufficient physical exercise is 36.8% in high-income Western countries and has been increasing over the past two decades (Guthold et al. 2018). Given the overwhelming evidence of the long-term health benefits of regular exercise and the related short-term benefits of improved well-being, such low levels of physical activity are both surprising and worrisome. Research has pointed towards behavior that deviates from a standard rational-economic model to explain underinvestment in preventive activities. One explanation for the lack of exercising is ‘limited attention’ (e.g., Dean et al. 2017), which describes that individuals have limited cognitive ability and therefore may forget to exercise during busy day-to-day life.Footnote 1 In this paper, we provide new evidence on the importance of limited attention in the repeated decision to exercise, using detailed data from a large health club in Sweden. We evaluate a randomized experiment among almost 2500 gym members in which we sent weekly email reminders to half of the sample. We find a strong and robust positive effect of these reminders on the number of gym visits during and after the intervention, for nearly all types of individuals. This result is most naturally explained by the prevalence of limited attention among gym members. Our setup is related to the study by Calzolari and Nardotto (2017), and our contribution lies in (i) the use of a large sample size, allowing for more precise estimates, (ii) a sample of gym members who are arguably more representative of the general population than those used in previous studiesFootnote 2, and (iii) a gym that offers a wide variety of activities, ranging from regular weightlifting to ball sports, yoga, cycling, swimming and dancing classes.Footnote 3 All of these factors speak towards strong external validity of our analysis. Furthermore, our paper is the first to identify whether the reminder emails were actually opened, allowing us to investigate whether opening the email is related to a behavioral response. Finally, we observe the booking and canceling of gym classes, allowing investigation of the making of plans to exercise in the future. We sent weekly reminders during a three-month period to gym members who had recently purchased an annual membership.Footnote 4 While the content varied across weeks, the emails were always short and encouraged regular visits to the gym. We find that email reminders lead to a substantial and statistically significant increase in gym visits of 13%.Footnote 5 The effect appears to be somewhat larger for class training (19%) than for free training (11%), although the difference is not statistically significant. These estimates follow from a difference-in-differences (DiD) model in which we control for a slight pre-experiment difference in attendance between the control and treatment groups, but we find similar effects when using simple differences. These findings are robust to a number of different specifications. Furthermore, the positive impact of email reminders applies to nearly all types of individuals (students, nonstudents, men, women, etc.), except that low attenders (based on the pre-experiment period) seem to benefit substantially more than high attenders. This finding is not surprising, given that high attenders typically have a lower scope to increase their exercising frequency. When extending the observation period to three months after the last reminder was sent, we find evidence that the positive effects of reminders persist (visits remain 12% higher in the treated group). We interpret this finding as a sign of habit formation, although it should be noted that these results are less precise than those for the experimental period and slightly smaller in some robustness checks. Furthermore, due to lower baseline attendance in the postexperiment period, the same percentage increase implies a lower increase in absolute terms. We do not find any effects of the reminders on the duration or renewal of membership contracts, which initially lasted 12 months. Comparing our results to Calzolari and Nardotto (2017), we find a significant effect of the reminders for the whole sample and not only for low attenders. For the latter group, we find a 16% increase in weekly visits, which is 10%-points smaller than the effect in Calzolari and Nardotto (2017). In addition, our postexperiment effects are significant for the whole sample, while Calzolari and Nardotto (2017) find no significant postexperiment effects for the whole sample (although their effect size is similar). We also use novel data on class bookings and cancellations of bookings to document how individuals plan their future gym visits and how they change their plans over time. We find that the majority of individuals who attend classes plan their class attendance several days in advance of the class (by making a booking), but a significant share of class bookings (42%) are canceled, most of those (82%) on the day of the class. This suggests that many individuals change their plans at short notice. With respect to the effect of the email reminders, we find that class attendance is affected through an increase in the number of bookings and a lack of an increase (or even a decrease) in the share of bookings that are canceled (though these results are less precise). The positive impact of regular reminders on gym visits (and the significant share of bookings that are ‘forgotten’Footnote 6) can best be explained by the prevalence of limited attention, which is loosely defined as decision makers not taking their full set of choices or actions into consideration, due to cognitive limitations.Footnote 7 In the context of exercising, gym members may forget to visit the gym on a particular day, forget to book a gym class in advance or forget to attend a previously booked class. Reminders are a potential remedy for limited attention because they can direct people’s attention to a particular choice or action and thus make the execution of this choice or action more likely. In the conclusion, we argue that alternative explanations for the effectiveness of reminders are less plausible. The rest of this paper is structured as follows. Section 2 describes the setup for the reminder experiment. In Sect. 3, we analyze the effects of reminders on the number of gym visits, class bookings and cancellations as well as on contract renewal. Section 4 provides a discussion of plausible mechanisms and concludes.",6
24.0,4.0,Experimental Economics,15 January 2021,https://link.springer.com/article/10.1007/s10683-020-09694-4,Risky choices and solidarity: disentangling different behavioural channels,December 2021,Renate Strobl,Conny Wunsch,,Female,,Unknown,Mix,,
24.0,4.0,Experimental Economics,08 February 2021,https://link.springer.com/article/10.1007/s10683-020-09695-3,Salience and social choice,December 2021,Mark Schneider,Jonathan W. Leland,,Male,Male,Unknown,Male,"The neo-classical models of decision making for risk (expected utility theory) and time (discounted utility theory) have been carefully tested for over half a century. The axioms of neo-classical social choice models have received less systematic empirical study.Footnote 1 One reason for this difference is that while expected utility (EU) and discounted utility (DU) are viewed as both normative and descriptive models of individual behavior, social choice models are traditionally viewed as purely normative, in which case the validity of the axioms of cardinal social choice theory cannot be refuted experimentally. We argue, however, that there is at least some scope for empirical studies in the evaluation of social choice models. In settings like cost–benefit analysis and policy analysis, applied economists implicitly, and sometimes explicitly, assume appropriately parametrized social welfare functions describe society’s objective or the policy maker’s preferences. In such applications, it is useful to know which models best represent these preferences in order to accurately measure costs and benefits. Relatedly, in a volume dedicated to empirical studies of social choice, Gaertner and Schokkaert (2012, p. 8), comment, “In a political democracy it is nearly impossible to implement any theory of justice without sufficient support from the general public. This support will depend on the citizens’ own values and preferences. Empirical research on the acceptance of notions of justice by different social groups is therefore essential to understand the social environment in which policy decisions are taken”. Even a prominent social welfare theorist has noted the need to consider the public’s beliefs about welfare and justice when developing normative theories of justice. Sen (2009, p. 44) comments that “public reasoning is clearly an essential feature of objectivity in political and ethical beliefs … In seeking resolution by public reasoning, there is clearly a strong case for not leaving out the perspectives and reasonings presented by anyone whose assessments are relevant, either because their interests are involved, or because their ways of thinking about these issues throw light on particular judgments.” Empirical studies of how people prefer to allocate resources can provide insight into the distributional preferences of a society, and hence, of which type of social welfare function best reflects that society’s aggregate preferences. If one objective of normative social welfare theory is to be ultimately applied in resolving society’s problems of distributive justice, a social choice theorist might consider the feasible set of implementable social welfare functions in a democratic society, which likely includes the class of social welfare functions that approximate the preferences of society’s members. We do not emphasize this argument, but provide it to illustrate that individual preferences may have some role in determining which normative models can be practically applied. In this paper, we conduct an experimental study of five basic properties of additive social welfare functions to better understand how individuals prefer to allocate resources, and to investigate to what extent the observed preferences are consistent with cardinal social welfare theory. Although we test basic axioms, one might argue that an experiment cannot falsify the normative validity of these properties. One might view our test in light of the preceding arguments as providing evidence for or against the feasibility of cardinal social welfare theory in a society where policy is constrained to reflect individual preferences, rather than a test of the validity of cardinal social welfare theory on purely normative grounds. Of the five properties that we study, three apply generally to the standard model of cardinal social choice theory (Moulin 2004) which consists of the class of additive social welfare functions (cardinal social welfare functions that can be expressed as the sum of functions indexing the welfare of each member of society). The fourth property applies to the special case of power social welfare functions and the fifth property to the special case of concave power social welfare functions. To generate our test questions and predictions, we construct a simple and psychologically grounded model of a salience-based social planner. This model extends Salience Weighted Utility over Presentations (SWUP) (Leland et al. 2019) which provides a unified explanation for anomalies occurring in choices under risk and over time as arising from the same psychological properties of human salience perception. The proposed salience model for social choice was developed prior to the experiment (and indeed, an analogous model for decisions involving risk or time is introduced in Leland et al. (2019)). However, as the experiment was designed to test for anomalous behaviors which are analogous to those observed empirically for risk and time, the experiment was consequently designed to test for behaviors where the salience model is predicted to perform well. In the experiment, we placed subjects in the roles of social planners whose choices will be implemented with some probability to test the axiomatic foundations of traditional social choice models and the predictions of the salience model. In our experiment, we observe strong and systematic violations of traditional social choice theory for four of the five properties we test. Each of the modal responses is predicted by the salience model. Our results reveal anomalies for social choice theory that are analogous to the classical anomalies for choices under risk and over time.",1
24.0,4.0,Experimental Economics,24 February 2021,https://link.springer.com/article/10.1007/s10683-020-09696-2,State lottery in the lab: an experiment in external validity,December 2021,Raman Kachurka,Michał Krawczyk,Joanna Rachubik,Male,Male,Female,Mix,,
24.0,4.0,Experimental Economics,15 February 2021,https://link.springer.com/article/10.1007/s10683-020-09697-1,Worker-firm relational contracts in the time of shutdowns: experimental evidence,December 2021,Sera Linardi,Colin Camerer,,Unknown,Male,Unknown,Male,"Disruptions in labor demand often occur in everyday labor markets. This is a more common experience for workers in certain sectors with high failure rates or seasonality of consumer demand (e.g. hospitality, entertainment, service, and high-tech start-ups). However, the recent coronavirus pandemic has resulted in temporary unemployment for about 25 million workers—affecting workers across the economy, including those employed in typically stable sectors.Footnote 1 These disruptions are likely to continue into the future as many conditions for reopening require businesses to shut down when local public health conditions (e.g. infection and testing rates) do not remain below government requirements.Footnote 2 More generally, firm-worker relationships can be interrupted by many exogenous reasons such as business cycle effects, liquidity shocks, or changes in upper management’s priorities.Footnote 3 Employment relations are tricky to navigate even without exogenous disruptions. Typical employment contracts specify the duration and terms of employment only loosely, leaving many details implicit (Williamson et al. 1975; Chevalier and Ellison 1997).Footnote 4 In theory, since third parties cannot enforce implicit contracts, such contracts may be inefficient due to moral hazard. That is, after contracts are agreed upon, each party could take actions that undermine efficiency but which cannot be fully anticipated and contracted around. However, experimental evidence, such as from Brown et al. (2004) (BFF), has shown that high efficiency can be achieved under incomplete contracts through relational contracts—informal long-term relationships with no explicit guarantee of relationship continuation. These contracts are driven by reciprocity: firms offer high wages to specific workers, workers reward these firms with high effort, and the firms return the favor with immediate contract renewals. Mirroring naturally-occurring labor markets, this high-tier market often coexists with a low-tier market of “McJobs” in which firms make public (indiscriminate) low wage offers and workers who accept those jobs deliver low effort.Footnote 5 Intuitively, stochastic disruptions can undermine relational contracts in many ways. Workers who are uncertain about the stability of their jobs may not find it worthwhile to invest effort to maintain goodwill, and shirk. Firms may lower wages in anticipation of such defensive shirking, resulting in lower market efficiency.Footnote 6 But these shocks can conceivably improve market efficiency. Azariadis (1975) and Rute Cardoso and Portela (2009) argue that firms will provide workers with de facto unemployment insurance in the event of stochastic shocks (such as more generous wages), thus inducing positive reciprocity from the workers. Lazear et al. (2016) traced increased productivity during the recent 2008 recessions to incumbent workers working harder as their local unemployment rate rises, an effect they termed “making do with less.” Putting this in the context of relational contracts, stochastic shocks may increase workers’ anxiety over relationship continuation, driving workers to put in extra effort to keep their jobs or to improve their chances of being rehired in case of layoffs. Stochastic shutdowns may also improve market efficiency by accelerating productivity-enhancing reallocation (the “cleansing” effect of recession (Davis and Haltiwanger 1992)). However, Barlevy (2002) argues that as much as recession speeds the destruction of less efficient matches, it also slows the formation of the most efficient matches. Laboratory experiments provide a useful first step to investigate the impact of temporal labor demand shocks on relational contracts. Unlike in naturally-occurring labor markets, complicating factors (such as uncertainty about the length or probability of shutdown, informal promises to rehire workers, or benefits offered to laid-off workers) can be controlled in the lab. Additionally, shocks that are truly independent and evidently exogenous can be easily created in the lab, allowing us to study relationship shocks in the simplest setting.Footnote 7 Our experiment extends the BFF gift-exchange paradigm as follows: in every period, there is a probability that a firm experiences a publicly observable stochastic shock, which forces a three-period shutdown. The firm can observe the market during the shutdown, but is unable to contract with any workers. We employ three treatments: No Shutdowns, Infrequent (shutdowns), and Frequent (shutdowns).Footnote 8 We develop a theory that follows previous literature in assuming the existence of some known proportions of fair (non-shirking) workers and selfish workers (cf. BFF and Bernard et al. 2018). We show that there is a perfect Bayesian equilibrium in which stochastic disruptions introduce instability in relationships, but the high wage-effort equilibrium can persist despite frequent firm shutdowns. The intuitive idea is that both firms and workers prefer to contract with their incumbent partners,Footnote 9 but workers temporarily laid off by a shutdown will accept offers from non-incumbent firms, put in high effort, and treat the new employer as its incumbent firm if rehired. Because laid-off workers are seen as unemployed through no fault of their own,Footnote 10 they are desirable to unmatched firms who poach these workers during shutdowns, leaving previously matched firms in need of new workers.Footnote 11 This creates opportunities to build new relational contracts much later in the experimental periods than in markets without shutdowns.Footnote 12 Our experiment finds that exogenous shutdowns destabilize matches, but, remarkably, high efficiency is maintained as firms and workers adapt their strategies. In our benchmark No Shutdowns condition, our results replicate BFF’s findings: firms quickly become matched with workers and then rely on immediately renewing offers to the same workers as long as they did not shirk. However, as shutdowns increase in frequency, renewals between incumbent firm-worker pairs went down from 61% of market activity (No Shutdowns) to 47% (Frequent), while contracts between new pairs went up from 17 to 36%. In spite of this instability, market efficiency is maintained across treatments with the exception of the last few periods. We see evidence that one firm’s misfortune is another firm’s opportunity: a worker is more likely to receive a private job offer from a new firm when his incumbent firm is temporarily shut down.Footnote 13 Instead of treating these new offers as temporary side gigs, workers work just as hard (if not harder) for their wages indicating their desire to make a good impression. Aware of these fluid loyalties, firms returning after shutdowns are less likely to recall previous workers that have been actively working for others. This leads to relational instability without loss of market efficiency. Overall, we see that relational contracts are robust to stochastic disruptions. Underneath it is a remarkable adaptability of firms and workers, who, given enough freedom, appear to reliably invent new strategies and norms that allow business to proceed even in the most difficult environments.Footnote 14 Consistent with Davis and Haltiwanger’s (1992) ’cleansing’ effect, the shock destroyed many inefficient matches while leaving efficient matches mostly unharmed. While contracts between high-earning workers and high-earning firms were similar across treatments, contracts between low-earning pairs were replaced by interactions between high-earners and low-earners. Overall, these disruptions have significant welfare effects. Workers cede more of their share of the trade surplus to firms (from 56 to 47%), and within-worker inequality grew. This echoes Lazear et al.’s (2016) and Cajner et al.’s (2020) findings: workers, particularly those that can least afford to take a hit, are those who suffer most when shocks hit real-life labor markets. The rest of the paper proceeds as follows. In Sect. 2 we provide a theoretical model of the impact of stochastic shutdowns on contracting, with implications that are later tested by data from the experiments. Section 3 describes the experimental design. Section 4 discusses results. Section 5 is a short conclusion.",1
24.0,4.0,Experimental Economics,29 January 2021,https://link.springer.com/article/10.1007/s10683-020-09698-0,The impact of taxation and signposting on diet: an online field study with breakfast cereals and soft drinks,December 2021,Daniel John Zizzo,Melanie Parravano,Marc Suhrcke,Male,Female,Male,Mix,,
24.0,4.0,Experimental Economics,09 February 2021,https://link.springer.com/article/10.1007/s10683-021-09700-3,"Randomized double auctions: gains from trade, trader roles, and price discovery",December 2021,Katerina Sherstyuk,Krit Phankitnirundorn,Michael J. Roberts,Female,Unknown,Male,Mix,,
24.0,4.0,Experimental Economics,14 February 2021,https://link.springer.com/article/10.1007/s10683-021-09702-1,Strategic uncertainty and equilibrium selection in stable matching mechanisms: experimental evidence,December 2021,Marco Castillo,Ahrash Dianat,,Male,Unknown,Unknown,Male,"Strategic uncertainty plays a crucial role in games with multiple equilibria. The central insight is as follows: when there are multiple rationalizable actions, players often face a tension between profitability and safety. In \(2 \times 2\) coordination games, Harsanyi and Selten (1988) developed the concept of risk-dominance to capture the intuition that one equilibrium may appear more or less strategically risky than another equilibrium. Lab experiments have documented the predictive power of risk-dominance in simple settings (Cooper et al. 1990; Van Huyck et al. 1990). More recently, variations of risk-dominance have been fruitfully applied to other domains. In the infinitely repeated prisoners’ dilemma, for instance, there is experimental evidence that cooperation is more likely to be sustained when it is both an equilibrium action and a risk-dominant action (Dal Bó and Fréchette 2011). In the context of continuous-time games, Calford and Oprea (2017) show that a particular measure of risk-dominance does an impressive job of organizing their experimental data. In this paper, we apply a version of risk-dominance to stable matching mechanisms. In a stable matching mechanism, participants in a two-sided market report rank-order lists of their preferences over match partners to a central authority. The central authority then uses the reported preferences to calculate the final matching. Crucially, the final matching is stable with respect to the reported preferences.Footnote 1 This environment induces a preference-revelation game in which the strategy space is the set of all possible ordinal preference lists. In particular, we investigate how varying the “riskiness” of preference misrepresentation affects selection among stable matchings. By automating the side of the market that has a dominant strategy, we are able to model the strategic environment as a coordination game. For a very general class of preferences, this coordination game has at least two symmetric and Pareto-ranked equilibria in pure strategies: an equilibrium in “truncation” strategies (i.e., removing less preferred match partners from the tail end of a preference list) and an equilibrium in “permutation” strategies (i.e., switching the order of match partners in a preference list). Although the truncation equilibrium yields a higher payoff, the presence of strategic uncertainty makes truncation strategies less appealing. Intuitively, truncation generates a trade-off between the likelihood of matching and the quality of match partner (conditional on matching). Thus, an agent who plays a truncation strategy opens herself up to the possibility of remaining unmatched for some profile of other agents’ preference reports.Footnote 2 This insight allows us to apply a version of risk-dominance to compare the riskiness of truncation against other strategies that secure against remaining unmatched. We then study this coordination game in the lab, using the simplest possible setting in which this strategic tension arises. Each experimental market consists of four agents: two firms (implemented by computers) and two workers (human subjects). Subjects play 20 rounds of the preference-revelation game induced by the firm-optimal stable mechanism, with random and anonymous re-pairing across rounds.Footnote 3 We use an ordinal constellation of preferences such that each subject has two stable match partners.Footnote 4 However, these ordinal preferences can be represented by different cardinal utilities. Indeed, we have full freedom in choosing the payoff difference between two ordered alternatives. In our experiment, we choose cardinal representations such that our treatments vary whether the criterion of risk-dominance selects truth-telling or truncation. When remaining unmatched is particularly costly (to be precisely defined later), then truth-telling is risk-dominant. When an agent has a strong intensity of preference for her first choice partner (to be precisely defined later), then truncation is risk-dominant. We now preview our main results. Overall, we find that truth-telling is the modal strategy. However, we observe several notable patterns in the experimental data. First, both truth-telling and truncation are played more often when they are risk-dominant. This result is robust to whether the treatment effect is measured at the subject or the session level. Second, in both treatments, truncation strategies are played more often in later rounds of the experiment. Since truncation is a necessary component of all payoff-dominant equilibria, this suggests that the salience of payoff-dominance as a selection criterion increases with subject experience. In our setting, the set of Nash equilibrium outcomes of the preference-revelation game is identical to the set of stable outcomes of the underlying two-sided matching market. When attention is confined to stable outcomes, the interests of the two sides of the market are opposed in a fundamental sense: the best stable matching for one side of the market is the worst stable matching for the other side of the market.Footnote 5 This makes equilibrium selection an important and relevant consideration for policymakers, who may have reasons to favor the welfare of one side of the market over another when designing matching institutions. An example of this is provided by the history of the National Resident Matching Program (NRMP), the entry-level labor market for American physicians. In May 1997, the NRMP unanimously voted to alter the algorithm that was being used over concerns that the original design unduly favored hospitals at the expense of students.Footnote 6 Thus, our experimental results can be useful in predicting which stable matching is more likely to be implemented when there are multiple candidates for consideration. We discuss further implications for market design in more detail in Sect. 6 of the paper. The argument for using laboratory experiments in this context is compelling. While data on participants’ submitted rank-order lists may be available in field settings, participants’ true preferences are unobserved. Under the assumption of truthful preference reporting, matching clearinghouses used in practice implement an extremal stable outcome (i.e., the most preferred stable outcome for one side of the market). But if agents strategically misrepresent their preferences in the field, then it is less clear which stable matching is implemented or even whether the final matching is stable. By allowing us to control for subjects’ preferences, the laboratory setting is ideally suited for answering questions related to equilibrium selection. Our paper naturally bridges several different strands of literature. First, there is a large body of work on the performance of matching mechanisms in the lab.Footnote 7 The current paper is most closely related to Castillo and Dianat (2016), which is an experimental investigation of truncation behavior in what approximates a decision-theoretic setting. To that end, Castillo and Dianat (2016) employ a restricted strategy space that yields a unique equilibrium in dominant strategies. The current design, on the other hand, introduces multiple Pareto-ranked equilibria and allows us to better understand the conditions that favor the implementation of one equilibrium over another. There are also experimental studies of matching mechanisms that investigate whether intensity of preference has implications for strategic behavior. For instance, Echenique et al. (2016) report that the cardinal representation of subjects’ preferences has a significant effect on the stability of final outcomes, with instability more likely to arise in the presence of weak incentives. However, Klijn et al. (2013) find that subject behavior is fairly robust to changes in cardinal preferences in the Gale-Shapley mechanism but not the Boston mechanism. In addition, the prevalence of out-of-equilibrium truth-telling in our experiment mirrors earlier findings by Featherstone et al. (2019). Second, there is a large experimental literature on behavior in coordination games.Footnote 8 Several of these studies focus on stag hunt games in an attempt to evaluate the merits of competing equilibrium selection principles. Our results are largely consistent with this literature, suggesting that equilibrium selection arguments may have significant generality and explanatory power across environments. In particular, our finding that the salience of payoff-dominance increases with subject experience mirrors that of Rankin et al. (2000), who show that subjects learn to gravitate toward payoff-dominance in a sequence of stag hunt games where cosmetic details (e.g., action labels, player labels, payoffs) are randomly perturbed. Finally, there is a growing literature that applies risk-dominance or related concepts to other strategic environments. In particular, our work is methodologically related to a series of lab experiments that use strategic risk (as measured by the size of the basin of attraction) to predict behavior across a variety of settings: the centipede game (Healy 2017), the infinitely repeated prisoner’s dilemma (Dal Bó and Fréchette 2011; Kartal and Müller 2018), the finitely repeated prisoner’s dilemma (Embrey et al. 2017), continuous-time games (Calford and Oprea 2017), and dynamic games (Vespa and Wilson 2016). The rest of the paper is organized as follows. Section 2 introduces the necessary theoretical background, Section 3 presents our experimental design, Sect. 4 presents our experimental results, Sect. 5 evaluates the ability of related solution concepts to explain our results, Sect. 6 discusses broader implications for market design, and Sect. 7 concludes.",1
24.0,4.0,Experimental Economics,08 March 2021,https://link.springer.com/article/10.1007/s10683-021-09703-0,Indefinitely repeated contests: An experimental study,December 2021,Philip Brookins,Dmitry Ryvkin,Andrew Smyth,Male,Male,Male,Male,"Contests are frequently-observed strategic situations where players devote costly and irreversible resources (such as time, money, or effort) to increase their chances of winning a reward (a prize, rent, or patent). Research and development races, advertising wars, political campaigns, lobbying efforts, legal battles, sports tournaments, and employee-of-the-month challenges are all examples of contests. A defining characteristic of many contests is that they are repeated and have an indefinite time horizon. For example, Coca-Cola and Pepsi have targeted aggressive advertising campaigns at each other since the 1950s. Both firms continue to engage in a series of monthly, weekly, and even daily contests for soft drink market share, and their ongoing feud has no well-defined end time. Another example involves network neutrality lobbying. In recent years, internet service providers (Comcast, Verizon, AT&T) and content providers (Netflix, Google, Facebook, Amazon) have repeatedly contested the legality of termination fees—payments from content providers to service providers—over a changing sequence of regulatory regimes. This study examines repeated contests of both known and, especially, unknown length.Footnote 1 Our methodology is experimental, because the opportunity costs of contest expenditures are difficult—often impossible—to parse from field data. While one-shot and finitely repeated contests have received much experimental attention, to the best of our knowledge, there is no extant experimental research on indefinitely repeated contests.Footnote 2 Such contests are worthy of study as important economic phenomena, but they also deserve attention because they have quite large strategy spaces, and little is known about behavior in indefinitely repeated games with large strategy spaces. Our paper both extends the existing experimental indefinite supergame literature to contests and adds to our general understanding of behavior in indefinite supergames with many feasible actions. Beyond better knowledge of contest behavior vis-à-vis game theory, we are simply interested in the extent of cooperation in indefinite contests. Changes in the (expected) length of interaction can affect how firms collude on research and development, how politicians cooperate on legislation, or whether litigants sustain suits or settle them. Examining cooperation in indefinite contest experiments sheds light on cooperation in many real-world contests. In focusing on the extent of cooperation in indefinite contests, we track a number of existing studies of indefinitely repeated games. The great majority of this work examines the Prisoner’s Dilemma (PD). It typically focuses on testing two theoretical predictions: (i) Cooperation increases in the expected length of an indefinite supergame, and (ii) Cooperation in indefinite supergames should be at least as high as cooperation in comparable finite supergames. Many, though not all, studies confirm these predictions.Footnote 3 Like the PD, contests are social dilemmas. In both games, the stage game equilibrium is not socially optimal, but the socially optimal or “cooperative” outcome can be supported in an indefinite supergame when players are sufficiently patient. This being said, there are several important differences between contests and PDs. First, as already mentioned, relative to the two-strategy PD, there are many more feasible strategies in contests. Moreover, contests do not have a dominant strategy. In this respect they are not only more complex than PDs, but also more complex than linear public good games (PGGs) that can serve as extended strategy space analogs of PDs. Contests have a nonmonotone (typically, single-peaked) best response; that is, relatively low expenditure levels are best responses to low rival expenditure and to high rival expenditure. This contrasts with PDs and linear PGGs which have a dominant strategy, and to coordination games and supermodular games which have unidirectional best responses. Finally, contests are rife with “overbidding”—an almost ubiquitous experimental finding that average expenditure exceeds the risk-neutral Nash equilibrium expenditure and that a sizable fraction of participants choose strictly dominated expenditures (Sheremeta 2013; Dechenaux et al. 2015). Such overbidding increases one’s chances of winning and imposes negative externalities on other players, which is, by construction, impossible in PDs, PGGs, or supermodular games. It is thus an open empirical question as to whether the comparative statics of cooperation in indefinitely repeated contests are similar to those in indefinite PDs and other previously studied games. We conduct indefinitely repeated contest experiments using the well-established continuation probability approach.Footnote 4 Following the seminal setups by Engle-Warnick and Slonim (2004) (trust games) and by Dal Bó (2005) (PDs), our experimental design lets us compare cooperation across indefinite contests of different expected length and between finitely and indefinitely repeated contests of the same expected length. As do existing indefinite PD studies, we ask two main questions: (i) Does cooperation increase in the expected length of indefinite contest supergames?, and (ii) Is cooperation greater in indefinite contest supergames compared to finite contest supergames? We also consider whether contest outcomes depend on the allocation rule for distributing the contested prize. Specifically, we examine a winner-take-all allocation rule and a proportional-prize allocation rule. In a winner-take-all (WTA) setting (Tullock 1980), the entire contest prize is awarded stochastically, according to probabilities equal to each player’s share of total expenditure. As in a patent race, this setting is extreme because one player receives the prize, while all other players receive zero revenue. In a proportional-prize (PP) setting (Van Long and Vousden 1987), each player’s share of the contest prize is their share of total expenditure. As in our Cola Wars example, this “smooth” allocation rule implies that a firm’s market share is increasing in its own advertising expenditure, but decreasing in its rivals’ expenditures. For risk-neutral players, the equilibrium expenditure is the same across both settings, but empirically: Is cooperation the same across indefinitely repeated winner-take-all and proportional-prize contests? We find evidence of greater cooperation in indefinitely repeated contests of longer expected length. Both WTA and PP indefinite contests feature lower expenditures when the continuation probability is larger compared to when it is smaller. However, this difference is only statistically significant when participants have played relatively few contests; as participants experience more and more rounds of play, expenditure remains lower in high continuation probability treatments, but the difference loses its statistical significance. When we compare indefinitely repeated contests to finitely repeated contests of the same expected length, we find significantly more cooperation in both indefinitely repeated WTA and PP contests. Finally, we report evidence that expenditure is greater (less cooperative) in PP contests relative to WTA contests, but only in longer treatments with more contests played. Our results are particularly interesting vis-à-vis results from indefinitely repeated PD experiments. At least some support for repeated game theory is typically found in these experiments, but the sparse experimental evidence on cooperation in indefinitely repeated games with larger action spaces is mixed. In line with theory, Holt (1985) reports both collusive and non-cooperative behavior in indefinitely repeated Cournot duopolies and only non-cooperative behavior in one-shot Cournot duopolies.Footnote 5 However, Lugovskyy et al. (2017) find no systematic differences in cooperation across indefinitely and finitely repeated public goods games.Footnote 6 In a contest setting, we find qualified support for the predictions of the theory of repeated games. Our results are equivocal because in our data the effect of the shadow of the future appears to depend on participant experience. We think that much experimental work remains to be done on indefinite supergames with “large” strategy spaces, since the very early returns suggest that standard predictions for social dilemmas may have less bite in more complex indefinite supergames than in relatively simpler indefinite supergames such as the Prisoner’s Dilemma. Our paper is organized as follows: In Sect. 2 we briefly describe a model of indefinitely repeated contests. Section 3 details our experimental design and procedures. We outline our three testable hypotheses in Sect. 4. In Sect. 5 we analyze the results of experiments, and we discuss our results and conclude in Sect. 6.",3
25.0,1.0,Experimental Economics,12 May 2021,https://link.springer.com/article/10.1007/s10683-021-09717-8,The coordinating power of social norms,February 2022,Francesco Fallucchi,Daniele Nosenzo,,Male,Female,Unknown,Mix,,
25.0,1.0,Experimental Economics,28 April 2021,https://link.springer.com/article/10.1007/s10683-021-09701-2,Belief adjustment: a double hurdle model and experimental evidence,February 2022,Timo Henckel,Gordon D. Menzies,Daniel J. Zizzo,Male,Male,Male,Male,"Agents form and update their beliefs when they receive new information. In the presence of rational expectations, new information leads to belief updating every period according to Bayes rule. In reality many agents do not behave according to basic statistics, and task complexity and inattention may contribute to deviations from Bayesian predictions. Such violations of rational expectations have been studied in static settings, where all the information is presented at once to subjects who discount priors (Kahneman and Tversky 1973; Tversky and Kahneman 1982; El-Gamal and Grether 1995). In this paper we study a dynamic setting in which new information arrives sequentially and consider the frequency as well as the extent of belief adjustment, referring to sticky belief adjustment when it is insufficient in either domain. Relative to previous urn experiments where agents need to state guesses, such as Khaw et al. (2017), our study is innovative in a number of dimensions. First, it is simple, in that our experiment does not have an evolving state of nature. We are interested instead in the basic question of how beliefs are updated dynamically, and this basic question can be answered in a way that is easiest for experimental participants and most interpretable for researchers by having a simple dynamic environment with new information flowing in. Second, we attempt to control for risk aversion and therefore are able to measure beliefs more accurately than in previous research. We try to do so by using the Offerman et al.’s (2009) technique. Third, we do not use cumulative earnings, which may lead to uncontrolled factors such as income effects or portfolio diversification. Fourth, we develop a double hurdle econometric model to combine in a single framework different types of belief adjustment we may observe in the laboratory: time-dependent (random) belief adjustment and state-dependent (Bayesian, Quasi-Bayesian) belief adjustment.Footnote 1 Each type has been utilized in macro and microeconomic modelling, though the tendency has been to focus on only one type. Within macroeconomic research, sticky belief adjustment can be seen as a possible microfoundation of sticky price adjustment, for example as a result of inattention and observation costs (Alvarez et al. 2016), information costs (Abel et al. 2013), cognitive costs (Magnani et al. 2016) and the consultation of experts by inattentive agents (Carroll 2003), or some combination of these factors (Cohen et al. 2019). State-dependence in beliefs implies a dependence of belief adjustment on the economic state, which in turn may depend on new information flowing in. Time-dependence in beliefs is often viewed stochastically [as for example in Caballero (1989)] and therefore yields random belief adjustment, following some underlying data generating process. One useful way of conceptualizing state- and time-dependent beliefs is the inferential expectations (IE) model of Menzies and Zizzo (2009): that is, agents hold a belief until enough evidence has accumulated for a statistical test of a given test size \(\alpha\) to become significant, at which point beliefs switch. Furthermore, if agents’ stance towards evidence is modelled by a probabilistic draw on their \(\alpha\), then the probability that the test size is unity (which results in an update for any evidence, or noneFootnote 2) is the probability of a random belief adjustment. Within microeconomic research, Quasi-Bayesian (QB) belief adjustment has been the preferred route to think about bounded-rational belief adjustment. Rabin (2013) distinguishes between warped Bayesian models which encapsulate a false model of how signals are generated, for example ignoring the law of large numbers (Benjamin et al. 2015); and information-misreading Bayesian models that misinterpret signals as supporting agents’ hypotheses, thus giving rise to confirmation bias (Rabin and Schrag 1999), and therefore lead to underweighting of information (for early evidence, see Phillips and Edwards 1966). In static problems where priors and ‘new information’ were given, Kahneman and Tversky (1973) and Tversky and Kahneman (1982) made the contrasting finding of base rate neglect, with more weight being put on the new information; reviews of the literature on base rate neglect can be found in Koehler (1996), Barbey and Sloman (2007) and Benjamin (2019). One simple way of modelling QB adjustment, which we follow, is that the agent adjusts beliefs every period in response to new information, but this adjustment is either too big or too small (Massey and George 2005; Ambuehl and Li 2014). That is, if the posterior probability is the prior multiplied by \((likelihood)^{\beta }\), Quasi-Bayesian models are marked by departures of \(\beta\) from unity.Footnote 3 In this paper we use a double hurdle model to consider different perspectives about belief adjustment emphasized within macro- and microeconomics. Full rationality requires clearing two hurdles in a very specific way: fully rational agents must adjust every period as they clear hurdle 1, and they must use Bayes rule with \(\beta =1\) as they clear hurdle 2. Table 1 describes the first hurdle using different values of the IE test size (\(\alpha\)) and the columns describe the second hurdle using different values of the QB parameter \(\beta\). Fully rational agents are fully attentive (\(\alpha =1\)) and Bayesian (\(\beta =1\)). The double hurdle model is formalized in Sect. 4.2 and generates a distribution for \(\alpha\) and \(\beta\), parameterizing both the frequency and extent of adjustment. Fifth, we provide the first study that looks at how increased task complexity or scope for inattention affects belief updating. Task complexity and inattention are two factors that have been independently identified as playing a potentially substantial role in bounded-rational decision making. Among others, Simon (1979), Gigerenzer and Gaissmaier (2011) and Caplin et al. (2011) have identified complexity of decision settings as a key reason for ‘satisficing’ and heuristic-based decision making. This is neurobiologically plausible (Bossaerts and Murawski 2017) and leads to different ways information is processed (Payne 1979) and lotteries selected in binary choices (Wilcox 1993). Examples of practical applications where complexity can be important is consumer exploitation by firms to achieve greater profits (Carlin 2011; Huck et al. 2011; Sitzia and Zizzo 2011; Sitzia et al. 2015); decisions to engage in vertical integration or outsourcing (Tadelis 2002); climate change inaction as linked to the complexity of the relevant task environment (Slawinski et al. 2017); defaults becoming more attractive as an omission bias (e.g., Baron and Ritov 2004). Inattention has been independently identified as a key source of bounded-rational decision making (e.g., Alvarez et al. 2016; Magnani et al. 2016; Carroll 2003), in ways that may but do not necessarily reflect rational inattention trade-offs. (See Caplin et al. 2020, for a discussion of this point.) It has a wide ranging and growing set of applications. Examples of applications in macroeconomics include the New Keynesian Philips Curve (Mankiw and Reis 2002), business cycle dynamics (Mackowiak and Wiederholt 2015) and the failure of uncovered interest rate parity (Bacchetta and van Wincoop 2010). Examples of applications in microeconomics include strategic product pricing (Martin 2017), corporate strategy (Dessein et al. 2016) and portfolio selection (Huang and Liu 2007). Surprisingly, given the importance that both complexity and inattention have been stated to have in a wide range of settings with risk and imperfect information, we are not aware of papers that have looked at the effect of either on belief updating. A key contribution of this paper is to address this gap. Regarding task complexity, we expect it to potentially reduce the frequency of stated belief changes in the first hurdle (\(\alpha\) in Table 1) as well as the extent of the stated belief change when this takes place in the second hurdle (\(\beta\) in Table 1). This is because complexity makes subjects less likely to wish to make an ‘active’ choice and therefore more likely to stick to the default (see Gerasimou 2018); and because, if they do change their stated beliefs, as they perceive the task as more uncertain, they are likely to be more conservative in the degree to which they do so (see Brainard 1967). Regarding inattention, if it matters in the way that the literature has suggested, then a simple experimental manipulation increasing the likelihood of inattention will lead experimental subjects to be less likely to update their beliefs regarding the variable to which they are not paying attention. Importantly, while this would not be surprising if the alternative distracting task were incentivized, in our experiment (as in Sitzia et al. (2015)) it is not. There is therefore unlikely to be any preference-based reason why a rational agent should ignore the guessing task on the basis of which payments are wholly made, and deviations from Bayes can be more precisely identified as being due to cognitive costs in information processing. Inattention should reduce the likelihood of agents switching their belief, and therefore enter the first hurdle of the model. In brief, our results are as follows. Subjects change their beliefs about half the time, which is consistent with random belief adjustment, but they also consider the amount of evidence available, which is consistent with state-dependent belief adjustment. When subjects do change beliefs, they do so by around 80 per cent of the full Bayesian update, which is consistent with our version of Quasi-Bayesian belief adjustment. There is substantial heterogeneity in our results and the frequency and extent of belief adjustment are negatively correlated: agents who update with low frequency do so by more than 80 percent of the full Bayesian update. Furthermore, we find evidence that inattention reduces the propensity to update, as predicted, as well as the extent of update. Complexity is less important, as it only affects the propensity to update and does so by less than inattention. We do not find that task confusion explains belief stickiness to an important degree, nor is there any financial incentive to explain why beliefs are stickier if we add an alternative distracting task. Rather, inattention and cognitive costs are likely to explain the infrequent belief adjustment, to different degrees, by half of our subjects. Only a small fraction of agents have rational expectations where that is understood as full Bayesian updating each period. Our paper is structured as follows: in Sect. 2 we construct a balls-and-urn experiment with treatments for complexity and inattention. Section 3 describes the balls-and-urn environment, and predicts behavior, under different expectational assumptions. Section 4 analyzes the experimental results using: nonparametric (model free) statistics for the raw data; the double hurdle econometric model for the risk-adjusted data; and a subject-specific density of test sizes derived from the double hurdle model. Section 5 draws together the main results, and concludes.",1
25.0,1.0,Experimental Economics,10 March 2021,https://link.springer.com/article/10.1007/s10683-021-09705-y,"Inequality, role reversal and cooperation in multiple group membership settings",February 2022,Andreas Lange,Jan Schmitz,Claudia Schwirplies,Male,Male,Female,Mix,,
25.0,1.0,Experimental Economics,12 March 2021,https://link.springer.com/article/10.1007/s10683-021-09707-w,Putting relational contract theory to the test: experimental evidence,February 2022,Nisvan Erkal,Steven Y. Wu,Brian E. Roe,Unknown,Male,Male,Male,"Many business and social transactions are conducted using informal agreements that are self-enforced through repeat trading. Such relational contracts are often necessary because it may be prohibitively costly to specify formal contracts with sufficient detail to capture all relevant performance conditions. In some cases, the set of third-party observable performance outcomes that can be specified in a formal contract may be smaller than the set of all performance factors that the contracting parties care about. Levin (2003) points out that relational contracts are also useful in ensuring flexibility in the face of changing circumstances. Indeed, Bernheim and Whinston (1998) and Scott (2003) point out that there are circumstances in which contracting parties may omit even easily verifiable performance factors in order to strengthen informal incentives. In this study, we experimentally investigate a number of canonical predictions that arise from relational contract theory. Our experimental design is flexible enough to nest several well-known theoretical predictions which are consistent with foundational theories of relational contracts, such as Telser (1980), Klein and Leffler (1981), Baker et al. (1994), MacLeod and Malcomson (1989), Bernheim and Whinston (1998), Schmidt and Schnitzer (1995) and Levin (2003), among others. We consider a framework of relational contract theory where a rational, self-interested principal designs an optimal contract subject to individual rationality and self-enforcement constraints. Existing experimental studies have, in different contexts, established the existence of behavioral considerations that deviate from rational, purely self-interested behavior. In light of this evidence, it is important to re-visit standard relational contract theory to systematically identify its strengths and weaknesses. This will allow us to combine standard and behavioral theories in a targeted and complementary way to improve explanatory power. Our experimental design is guided closely by a canonical theoretical model of relational contracting. Our starting point is a first-best benchmark treatment where contracts can be made perfectly third-party enforceable. We then consider two variations from this benchmark treatment. First, we introduce imperfections in terms of how performance is measured. Specifically, we assume that the performance measurement technology can only detect crude performance outcomes, such as whether a product is defective, but not more refined outcomes. This imperfection creates inefficiencies when using formal contracts, which, in theory, should lead to the endogenous emergence of relational contracts. Hence, by incorporating partial enforcement, our experiment allows us to examine endogenous contract choice between formal and relational contracts in the spirit of classic papers by Baker et al. (1994) and Bernheim and Whinston (1998). As a second variation, we consider different discount factors. The impact of an exogenous variation in the discount factor on behavior has been the focus of many papers in the literature, both theoretical and experimental. In one treatment, we assume the discount factor is sufficiently high such that it is theoretically possible for the first-best performance level to be self-enforcing. In a second treatment, the discount factor is lowered so that the first-best performance is no longer self-enforcing. We are interested in studying whether contracting parties react to this change in the discount factor by changing the quality that they contract for. Infinitely-repeated trading is implemented using a random continuation rule, which is the approach adopted by a number of well-known laboratory experiments on infinitely repeated games (Murnighan and Roth 1983; Dal Bó 2005; Dal Bó and Fréchette 2011; Fudenberg et al. 2012; Dal Bó and Fréchette 2019). Our design allows subjects who have been assigned to be principals to choose from a large set of contract types, including formal versus relational contracts, along with several forms of relational contracts (e.g., fixed price contracts, discretionary bonus contracts, and pure bonus contracts). This feature allows us to analyze endogenously emerging contractual forms and test a wide range of theoretical predictions, including those emerging from the theory of strategic ambiguity (Bernheim and Whinston 1998). In recent years, there has been a growing interest in testing standard theories of relational contracts empirically (Gil and Zanarone 2018) (GZ). GZ point out that empirical work using observational data is often constrained by difficulties in measuring intertemporal discount factors, reservation payouts, one-shot deviation payouts from shirking on an informal agreement, formal contracting alternatives, and other variables needed to specify self-enforcement and individual rationality constraints. Experiments can complement studies based on observational data by allowing researchers to directly specify and parameterize a relational contracting model, and conduct comparative statics analysis of discount factors, reservation utilities, costs, etc. Thus, a major contribution of our work is that we provide a direct test of the empirical relevance of standard relational contracting predictions using an experimental design that allows us to specify key model parameters and constraints rather than rely on imperfect proxies from observational data. Another major contribution of our study is that our experimental design allows for the endogenous choice of contractual forms. Contract theory studies how principals faced with agency problems can optimally choose the contractual form best suited for the contracting environment in order to deliver efficient incentives, mitigate verifiability problems and/or induce self-selection to resolve adverse selection problems. Studies that impose ad hoc restrictions on contractual forms without providing a justification for the restrictions imposed often invite criticisms from theorists (Carmichael 1985; Tirole 1999; Schmitz 2001). Accordingly, in an experimental setting with exogenously imposed contractual forms, it is difficult to study principals’ choice of different contractual forms in response to different agency problems. As a result, by studying the endogenous choice of contractual forms, our study contributes to our understanding of the variety of contractual forms observed in real world settings. Our results reveal that the standard model of relational contracts is successful in many ways in predicting behavior observed in the lab. Specifically, our findings are consistent with the following predictions: (a) when total pay does not meet individual rationality conditions or the promised discretionary bonus does not satisfy either the principal’s or the agent’s self-enforcement conditions, there is an increase in contract rejection or shirking; (b) with only partial contract enforcement, subjects shift towards relational contracts; and (c) in the presence of imperfect enforcement, subjects show a preference for discretionary bonus contracts rather than efficiency wage contracts, which is consistent with the theoretical optimality of discretionary bonus contracts in our model and the theory of strategic ambiguity of Bernheim and Whinston (1998). Two prominent weaknesses we identify in standard relational contract theory are that subjects often apply inefficient punishments following a deviation and that the contractually specified level of performance does not appear to respond to an exogenous change in the discount factor. Instead, as the discount factor decreases, there appears to be an increase in opportunistic behavior on the part of sellers as reflected by a reduction in actual performance (i.e., delivered quality). These results are useful in guiding future theoretical developments. We conjecture that incorporating social preferences and semi-grim strategies (Breitmoser 2015) can potentially improve the explanatory power of theory.Footnote 1 The paper proceeds as follows. After reviewing the related literature in Sect. 2, we present a simple theoretical framework in Sect. 3. In Sect. 4, we discuss the empirical predictions and present the experimental design. Section 5 provides descriptive statistics and an overview of the results, and Sect. 6 reports the results of formal hypothesis tests. Section 7 concludes.",1
25.0,1.0,Experimental Economics,29 March 2021,https://link.springer.com/article/10.1007/s10683-021-09712-z,Correction to: Putting relational contract theory to the test: experimental evidence,February 2022,Nisvan Erkal,Steven Y. Wu,Brian E. Roe,Unknown,Male,Male,Male,Author corrections were partially completed and thus there was a need to include additional corrections which were missed. Original article has been updated.,
25.0,1.0,Experimental Economics,05 April 2021,https://link.springer.com/article/10.1007/s10683-021-09708-9,Short-term fluctuations in incidental happiness and economic decision-making: experimental evidence from a sports bar,February 2022,Judd B. Kessler,Andrew McClellan,Andrew Schotter,Male,Male,Male,Male,"Every day, we face myriad opportunities to be generous, buy things, take risks, and trust others. Between making these decisions, we face the outcomes of numerous unrelated events: whether it is a sunny day, whether the barista making our coffee is friendly, whether the episode of TV that we are watching has a happy ending, and whether our favorite sports team wins a game. While these events may temporarily change our mood, the incidental emotions they induce do not have a direct effect on our material well-being, and so standard economic theory dictates that they should not affect our decisions. If such subtle changes in happiness were to systematically affect the choices we make, we might want to be conscious of this relationship to avoid taking actions we might later regret.Footnote 1 In addition, those who might potentially profit from our choices would also want to understand the relationship between our emotions and behavior. In this paper, we introduce a new kind of experiment to explore the effects of emotions on behavior. We run a lab-in-the-field experiment that allows us to observe the impact of fluctuations in incidental happiness on behavior over the course of a few hours.Footnote 2 Our design leverages naturally occurring short-term variations in emotions from seemingly frivolous exogenous events while holding all other aspects of the subjects’ lives constant. In particular, we leverage the short-term fluctuations in mood induced when subjects watch a live National Football League (NFL) football game. Experimental subjects watch the football game in a sports bar (allowing them to engage with the game as they normally would). During selected commercial breaks, we repeatedly measure each subject’s self-reported happiness and present the subject with a set of four economic decision-making tasks: charitable giving, buying a consumer good, taking a risky gamble, and trusting or being trustworthy. Our main empirical approach leverages instrumental variables (IV) specifications in which we use game events as an exogenous instrument for incidental happiness. This empirical approach requires the reasonable assumption that events in the game affect happiness but do not have a direct effect on economic decisions.Footnote 3 The NFL games subjects watched generated variation in self-reported happiness for the vast majority (\(81\%\)) of subjects, suggesting the potential for a first stage of our IV approach. We capture the impact of game events using measures of the time-varying probability that a subject’s favored team will win the game. We run the analysis two ways, each with one version of this probability as the instrument in an IV specification. One version is an objective (i.e., external) prediction of a team’s probability of winning from a popular sports statistics website. The other version uses a subjective prediction of the team’s probability of winning provided by the subject.Footnote 4 Using these IV approaches, we find that changes in incidental happiness statistically significantly affect behavior in two of the four decision-making tasks: charitable giving and trust. In addition, we provide a simple theory of incidental happiness and decision-making that can rationalize our results.Footnote 5 Our modeling framework allows changes inincidental happiness to alter the decision maker’s marginal utility of income. We use this model to make predictions about the effect of mood on the four economic decision tasks in the experiment. We provide conditions on individuals’ utilities where increases in happiness decrease marginal utility of income and decreases in happiness increase it and show that agents in a better mood will donate more to charity, spend more on consumer goods, take more risks, and be more trustworthy. That subjects are more willing to donate money to charity and to trust more when they are relatively happier are consistent with the predictions of the model under this assumption. In addition, the non-statistically significant effects on the other outcomes are also in the direction predicted by the theory. A main contribution of our paper is introducing a new experimental paradigm to explore the effect of emotions on economic decision-making. Our design differs significantly from the approaches in earlier work exploring the effect of emotions on decisions. In those studies, an experimenter might induce one or two emotions (e.g., happiness or sadness) at one point in time by either: (a) showing a subject a happy or sad movie clip (see, e.g., Kirchsteiger et al. (2006); Ifcher and Zarghamee (2011); Oswald et al. (2015)) or (b) having them recollect in writing a sad or happy event in their life using the Autobiographical Emotional Memory Task (see, e.g., Strack et al. (1985); Myers and Tingley (2016)). Subjects primed with these emotions are then asked to engage in a decision task, and the experimenter compares the behavior of those primed with different emotions. Our approach, in contrast, is to let exogenous events in the football game induce changes in incidental happiness to explore the relationship between happiness and decision-making.Footnote 6 Relative to the more standard experimental designs, our paradigm has a number of important advantages (as well as some limitations, discussed below). First, our design allows us to observe subject behavior in a naturalistic setting outside of the lab, complementing the prior laboratory by work by demonstrating the robustness of the relationship between emotions and decisions to field settings. In addition, our field setting allows—and perhaps invites—subjects to engage in the type of natural emotional expression that might accompany emotional swings (e.g., such as cheering or groaning). Providing subjects with this opportunity heightens the realism of the emotional experience and thus the external validity of our findings.Footnote 7 Second, our design allows us to observe a time-series of emotional swings and decisions for each subject (i.e., we see subjects get happier and less happy over the course of the game). This data generating process gives us more statistical power per subject and helps to ensure that we are identifying our treatment effects off of emotional swings in both directions.Footnote 8 Third, by recruiting football fans to our study, we get to observe how subjects respond to emotional stimuli that they endogenously choose to experience, heightening the external validity of the results.Footnote 9 As also noted above, we see our paradigm, and its advantages, as a complement to the existing laboratory approaches exploring emotions. There are also a few disadvantages of our paradigm, which are all associated with sacrificing the control of the laboratory to move into field. First, our setting limits what we can measure. A rich set of papers use physiological measures, such as skin conductance and heart rate, to measure emotions (see, e.g., Buser et al. (2017) and Halko and Sääksvuori (2017)). It was impractical to attempt to measure these in the sports bar setting, and so we rely on self-reported emotions for our analysis (an issue we discuss in Sect. 2.5). Second, running the experiment in the sports bar gives subjects the opportunity to consume alcohol, which might alter their choices (an issue we also discuss in Sect. 2.5). Third, by leveraging the events of a sporting event, which was taking place in real time, to induce emotional swings, we risked recruiting subjects to watch a game that was boring or otherwise failed to induce emotional swings, which would have prevented us from being able to observe the effects of emotions on decisions. Recruiting sports fans helped to mitigate this concern. An additional contribution of our paper is adding to a rich existing literature, which spans multiple disciplines, on how emotions affect decision making. Emotions have been shown to affect: dictator game giving and prosocial behavior (see, e.g., Tan and Forgas (2010); Kandrack and Lundberg (2014); Drouvelis and Grosskopf (2016)); trust and trustworthiness (see, e.g., Capra (2004); Dunn and Schweitzer (2005); Kirchsteiger et al. (2006); Myers and Tingley (2016)); time preference (see Ifcher and Zarghamee (2011)); risk and risk perceptions (see, e.g., Wright and Bower (1992); Johnson and Tversky (1983); Nygren et al. (1996); Lerner and Keltner (2001); Loewenstein et al. (2001)); willingness to pay (see, e.g., Lerner et al. (2004)); productivity (Oswald et al. (2015); Isen (2008)); and overconfidence and helping behavior (see, e.g., Aderman (1972); Isen and Levin (1972); Rosenhan et al. (1974); Konow and Earley (2008)). Much of this prior literature relates specifically to the four decision-making tasks we chose for our experiment; we discuss our connection to this work in Sect. 2.3. Finally, while our paper mainly aims to contribute to the economics literature, theories on emotions from psychology makes predictions about what we might see in our experiment. The “Appraisal-Tendency Framework,” predicts that momentary variations in happiness that result from events in the game are associated with specific automatic functions that may not be in line with long-term judgment (Lerner and Keltner 2000). For example, an agent may be willing to pay more for a good than his perceived value of that good after an unrelated rise in happiness. The “Broaden and Build Model” predicts that incidental fluctuations in happiness from positive events in the game would affect information processing differently than negative events. Positive events are believed to broaden the decision-maker’s awareness, leading to benevolent behaviors, whereas negative ones narrow awareness and result in survival-oriented, non-cooperative behaviors (Fredrickson 2001). This would predict more charitable giving and perhaps more trusting and trustworthy behavior when subjects are in a better mood. The “Hedonic Contingency Model” asserts that those in positive moods are more likely to engage in activities for which they will be hedonically rewarded, which would predict higher willingness to pay for a good, more charitable giving, and more trusting and trustworthy behavior. This paper proceeds as follows. In Sect. 2 we present our experimental design. In Sect. 3 we describe the model. In Sect. 4 we present our results. In Sect. 5 we offer some conclusions.",5
25.0,1.0,Experimental Economics,27 March 2021,https://link.springer.com/article/10.1007/s10683-021-09709-8,Give me a challenge or give me a raise,February 2022,Aleksandr Alekseev,,,Male,Unknown,Unknown,Male,"Labor economics has long recognized the role of monetary incentives in determining workers’ effort (Lazear 2018). The effectiveness of monetary incentives that are tied to performance is well established. Behavioral economics added new insights to the effect of these conditional rewards by showing that their effectiveness is subject to psychological factors, such as crowding out of intrinsic motivation (Gneezy and Rustichini 2000) and choking-under-pressure (Ariely et al. 2009; Hickman and Metz 2015).Footnote 1 This literature also discovered alternative incentive mechanisms, often inspired by studies in psychology, that can boost workers’ effort at no extra monetary cost. Among such mechanisms are goal-setting (Gómez-Miñambres 2012; Corgnet et al. 2015, 2018), performance ranking (Blanes i Vidal and Nossol 2011), and framing (Hossain and List 2012). A recent World Bank Report highlights behavioral mechanisms as an important incentive tool in developing countries (World Bank Group 2015). Despite the convergence of economic and psychological insights on the determinants of effort, an important strand of psychological literature remains untapped by economists. A psychological theory of motivation called motivational intensity theory posits that the primary determinant of effort is task difficulty (Brehm and Self 1989; Wright 1996). Task difficulty can be defined as a characteristic of a task that, when increased, reduces the probability of success in the task for any given level of effort.Footnote 2 Task difficulty is conceptually different from the cost of effort, even though the two concepts are sometimes mixed in the literature (Bremzen et al. 2015).Footnote 3 I attempt to close this gap by proposing a theoretical mechanism behind the effect of task difficulty on effort and by causally testing this mechanism in an incentivized experiment. Motivational intensity theory assumes that effort investment in completing a task is determined by the motivation to avoid wasting energy. This implies that effort is determined by the minimum amount of work needed to complete a task, as long as success seems possible and beneficial (Wright 1996; Brehm and Self 1989).Footnote 4 The main prediction of the theory, which follows directly from its main assumption, is that effort increases with task difficulty up to a certain point. A further increase in difficulty causes effort to drop leading to an inverse-U pattern of effort response. Motivational intensity theory views monetary rewards as a mediating factor in effort provision that determines the point at which effort as a function of difficulty starts to drop. Empirical studies in psychology provide extensive evidence for the inverse-U relationship between task difficulty and effort in the context of a direct effect of difficulty on effort (Smith et al. 1990; Richter et al. 2008; Richter 2015), as well as the effect of difficulty on effort interacted with other factors, such as learning and ability (Brouwer et al. 2014; Latham et al. 2008), mood and fatigue (Brinkmann and Gendolla 2008; LaGory et al. 2011; Silvestrini and Gendolla 2009, 2011), motivation (Capa et al. 2008; Gendolla and Richter 2005; Gendolla et al. 2008), and anticipated difficulty (Wright 1984; Wright et al. 1986).Footnote 5 Task difficulty has also been proposed as an important factor in activating analytical reasoning (Alter et al. 2007) and achieving goals (Labroo and Kim 2009). The majority of these studies focus on physiological correlates of effort, such as cardiovascular activity, as originally suggested by Wright (1996). Subjects in these studies are given a task that can vary in difficulty, such as a memory task (Richter et al. 2008). Effort is measured by the difference in the cardiovascular activity of subjects before and during performing a task. A common finding in this line of research is that subjects’ effort initially increases with task difficulty but then drops. This effect appears to be quite universal in that it is observed regardless of the nature of a task (cognitive, physical, or social). A natural question arises of whether the inverse-U effect of difficulty on effort can be observed in an economic environment. Addressing this question is important for, at least, two reasons. First, understanding the role of difficulty and its interaction with other incentive elements, such as extrinsic monetary rewards, is relevant for principals who are seeking to design optimal incentive schemes for employees (Holmström 2017). Second, understanding the theoretical mechanisms behind the role of task difficulty in effort provision is relevant for researchers who are using models of effort provision to explain workers’ behavior in the labor market or to predict the potential effects of policy interventions. I seek to accomplish two goals: first, to empirically study the effect of difficulty on effort and its interaction with monetary rewards; and second, to use this evidence to improve our understanding of the mechanisms behind effort provision and, in particular, to clarify the modeling assumptions that are needed to generate different patterns of effort response to difficulty.Footnote 6 In the empirical part, I set up an incentivized experiment that follows a chosen effort framework (Fehr et al. 1997; Abeler et al. 2010; Charness et al. 2012). Subjects assume the roles of agents who choose how much effort to exert in a series of projects. The probability of a project’s success depends on a subject’s chosen effort level and on a project’s difficulty. Higher difficulty reduces the probability of success on a project for any given level of effort. Monetary rewards consist of an unconditional (wage) and conditional (bonus) parts. The cost of effort is monetary and is subtracted from a project’s monetary outcome. I vary difficulty, monetary rewards, and cost within subjects and observe how this exogenous variation affects subjects’ effort levels. The chosen effort framework allows me to precisely define and observe difficulty and effort, and to derive sharp testable hypotheses. To establish theoretical predictions, I use a very general model of effort choice under risk that allows for a utility function that is potentially non-separable in money and effort, as in Mirrlees (1971). Allowing for such a general utility function is necessary, since, as I show in the comparative statics analysis, the pattern of complementarity between effort and money in the utility function, together with the pattern of complementarity between effort and difficulty in the probability of success function, determines the overall effect of difficulty on effort. I consider two alternative models of agents’ preferences. First, I use a benchmark Expected Utility (EU) model in which an agent chooses effort to maximize the weighted average of outcomes’ utilities with the weights being the probabilities of each outcome. I show, in particular, that in the present experimental design a risk-averse agent would monotonically decrease her effort in response to higher difficulty. Interestingly, a model with an additively separable cost of effort, which is a workhorse model in the literature, predicts that higher difficulty would lead to no change in effort at all. Given that the inverse-U relationship between effort and difficulty in the present experiment cannot be generated by the standard EU-approach, I consider an alternative model that can deliver such a relationship. The alternative model allows for probability weighting, as in the Cumulative Prospect Theory (CPT) (Tversky and Kahneman 1992). In this model, the agent also chooses effort to maximize a weighted average of outcomes’ utilities, but now the weights are the probabilities that are transformed using a probability weighting function. I show that allowing for the probability weighting makes it possible to generate richer predictions about the potential effect of difficulty on effort. In particular, if the probability weighting function is inverse-S-shaped (respectively, S-shaped), the pattern of effort response to difficulty in the present experiment would be U-shaped (respectively, inverse-U-shaped). I argue that probability weighting is the only plausible channel that can generate a non-monotonic effect of difficulty on effort. I find that monetary rewards affect effort in the predicted direction. Conditional rewards, on average, have a strong positive effect on effort, while the effect of unconditional rewards is positive but weak. These results are consistent with the previous findings in the literature (Gneezy and List 2006; DellaVigna and Pope 2018). Despite strong statistical significance, the economic significance of conditional rewards is somewhat disappointing: doubling the conditional rewards increases effort only by 20%. Making the cost of effort steeper leads to a sharp decrease in effort, as predicted. This result is consistent with the findings in the contest literature (Dechenaux et al. 2015), as well as in the studies employing real-effort tasks (Goerg et al. 2019). I find that the effect of difficulty on effort is inverse-U-shaped, which supports the hypothesis of an S-shaped probability weighting. This result is new to the economics literature.Footnote 7 It does echo, however, the findings from the studies on the motivational intensity theory. Interestingly, the magnitude of the effect of difficulty is on par with the magnitudes of the effects of conditional rewards or costs. I find that difficulty mediates the effects of monetary rewards. Conditional rewards are most effective at the intermediate and high levels of difficulty. The inverse-U effect of difficulty on effort refutes the benchmark EU-based model of effort and supports the alternative model with an S-shaped probability weighting. I further confirm this in a structural analysis by estimating the parameters of the probability weighting function. While a typical finding in the literature on risk preferences is an inverse-S-shaped probability weighting (Wu and Gonzalez 1996; Bruhin et al. 2010; l’Haridon and Vieider 2019), the subjects in the DellaVigna and Pope (2018) study also tended to underweight small probabilities (implied by an S-shaped weighting) in a real-effort experiment.Footnote 8 The present findings have several important implications for the design of incentive schemes and modeling effort. The strong incentive effect of task difficulty suggests that managers should take this effect into account when assigning tasks to their subordinates. A manager can expect that it will take workers more effort to complete a moderately hard task than an easy task. However, workers might not devote as much effort to a very hard task as the manager would desire. This behavioral response to high difficulty can substantially lower a worker’s performance by amplifying the direct negative effect of difficulty on performance. A manager, however, can counter this detrimental behavioral effect of difficulty using conditional rewards, since they are most effective when difficulty is high. From a theoretical perspective, the workhorse model with an additively separable cost of effort might not be a good behavioral assumption in some cases. My results suggest that supplementing the workhorse model with probability weighting can better explain certain patterns of effort provision and yield higher predictive power. The implications of probability weighting on effort provision in other contexts, thus, deserve further investigation. The uncovered effect of difficulty on effort has general methodological implications for using the existing effort tasks and designing new ones (Gill and Prowse 2012; Gächter et al. 2016). Experimental designs should control for task difficulty since it can have a strong and non-monotonic effect on subjects’ effort. The interaction effect between task difficulty and incentives can be exploited to ensure that monetary incentive treatments do not suffer from a “ceiling-effect” problem. Setting task difficulty to an intermediate level should provide enough room for monetary incentives to affect subjects’ effort. I proceed as follows. Section 2 discusses the related literature. Section 3 describes the experimental procedures, design, and treatments. Section 4 presents the theoretical framework and derives testable hypothesis. Section 5 discusses the results of the experiment and estimates a structural model motivated by the observed behavioral patterns. Section 6 concludes.",
25.0,1.0,Experimental Economics,16 April 2021,https://link.springer.com/article/10.1007/s10683-021-09710-1,Do traders learn to select efficient market institutions?,February 2022,Carlos Alós-Ferrer,Johannes Buckenmaier,Georg Kirchsteiger,Male,Male,Male,Male,"When investigating the functioning of markets one is puzzled by the huge variety of market institutions used to trade similar or even the same good (see e.g. Madhavan 1992; Quan 1994; Zumpano et al. 1996). Extensive research has revealed that the institutional rules governing trade have a substantial impact on prices, quantities, and the resulting efficiency of markets (see, among many others, Lucking-Reiley 2000; Reynolds and Wooders 2009; Ariely et al. 2005; Ausubel et al. 2014; Genesove and Hansen 2016). This in turn led to substantial research on the evolution of market rules (see e.g. Rud and Rabanal 2018). Starting with the seminal contribution of Hayek (1967) it has been argued that traders, if left to their own devices, choose the most efficient trading institution that the good at hand allows for.Footnote 1 This article aims to test this conjecture experimentally. Specifically, we set out to investigate whether efficient institutions are indeed used in the long run, while inefficient ones fade as they are eventually avoided by the traders. Take a situation where different trading platforms are feasible for trading a particular good. Each trader has to choose which one he/she wants to use.Footnote 2 An individual buyer or seller will not choose a platform where he/she cannot find a trading partner, even if the institutional setup of the avoided platform would lead to very efficient trading outcomes. Hence, traders face a (partial) coordination game when choosing on which platform they want to trade.Footnote 3 After choosing the trading institution, i.e. for a given distribution of buyers and sellers over the feasible platforms, the institutional setups–together with demand and supply resulting from the platform choice–will lead to more or less efficient trading outcomes. Hence, the whole process is characterized by two steps: First, traders have to choose the trading platform (where to trade). Then, they conduct their trades with the partners available at their platform (how much and at which price to trade). In our experiment each subject is either a buyer or a seller. In the first stage of each experimental round, each subject had to choose individually between a posted offer (PO; with sellers posting prices) institution and an open double auction (DA, Plott and Gray 1990; “open” meaning that there was no auctioneer, thus multiple prices could be realized).Footnote 4 In the second stage of each round, subjects could trade multiple units of a homogeneous good according to the rules of the chosen institution with those potential trading partners who had opted for the same institution. At the end of each round, all the participants received aggregate information about what happened at both institutions, enabling them to make an informed decision on where to trade in the next round: number of buyers and sellers, distribution of trading prices (and their average), and number of trades per buyer and per seller at each institution. Then a new round started, where first all subjects had to choose between the DA and the PO. After that, they traded on the chosen platform anew, received information about the outcome of both institutions, and so on. For each trade conducted, the seller received the price paid by the buyer, but in turn she had to pay production costs that were determined by the experimental design. Hence, her net earnings from a trade were the difference between the price and production costs. For a buyer, the net earnings from a trade were the difference between the resale values, which were determined by the design of the experiment, and the price paid.Footnote 5 That is, in each given round, the actual, endogenously-induced supply and demand at a given platform was determined by the production costs and resale values of the sellers and buyers who opted for that platform. To keep the demand and supply functions constant, and since our focus is on which institutions survive, the induced values were the same for all buyers and the production costs were the same for all sellers. We chose DA and PO as institutional alternatives since it is very well documented that, when functioning in isolation, DAs are highly efficient, whereas POs tend to create inefficiencies and are biased towards sellers. Theoretically, for example, Rustichini et al. (1994) show that, in double auctions, possible inefficiencies due to traders’ strategic misreporting vanish quickly as the number of traders increase. Empirically, it has been repeatedly shown that DAs induce a very quick convergence of prices and quantities to market clearing levels (see, e.g., the seminal article of Plott and Smith 1978a). As a consequence, the DA has been found to be the most efficient market institution when efficiency is measured by the sum of all gains from trade reaped by buyers and sellers (as it is usually done in the context of market experiments). In fact, DA is routinely used in market experiments as a proxy for competitive markets (e.g., Plott 2000; Crockett et al. 2011; Gillen et al. 2020). On the other hand, in the PO prices and quantities typically show a much slower convergence to the market clearing level, with prices typically converging from above (see Plott and Smith 1978a). For given demand and supply, the efficiency level is considerably higher for a DA than for a PO. If efficiency drives the selection of market institutions, we should expect that traders coordinate on the DA, and that the PO eventually falls into disuse. The experimental results indicate that the validity of this prediction depends crucially on the properties of the distribution of production costs. If the production technology underlying the sellers’ supply function exhibits decreasing returns to scale, traders indeed learn to coordinate on the efficient DA, with PO becoming mostly inactive in finitely many rounds. In contrast, if production displays constant returns to scale, resulting in a flat induced supply function, coordination on DA happens more slowly and both institutions remain active until the last round.Footnote 6 This difference might be caused by the fact that a Pareto-dominance relation between the two institutions depends on the distribution of the production costs. For a production technology with constant returns to scale, sellers’ earnings are (nearly) zero at an institution like the DA that induces prices which are at (or very close to) the market clearing level. Hence, one market side, i.e. the sellers, has a strong incentive to try to coordinate traders on an institution like the PO, where prices are above the market clearing level. To put it differently, if an institution yields outcomes which Pareto-dominate those of competing ones, it should be expected that it will clearly and quickly drive the latter out of the market. However, switching from coordination on the less efficient PO to coordination on the more efficient DA does not constitute a Pareto-improvement if sellers produce under constant returns to scale. Our results thus suggest that efficiency alone, as measured by the sum of the gains from trade, might not be enough to guarantee that a market institution drives competing ones out of the field. The reason for this is that sellers face a tradeoff between a more efficient institution which brings their profits down and an inefficient, low-volume one which however typically yields higher prices. In the absence of Pareto-dominance, the distribution of gains from trade, and not only aggregate efficiency, becomes consequential for market selection and survival.Footnote 7 Our data allows us to look both at actual trading behavior within an institution and the previous decision of which institution to trade in. Regarding the former, we observe that aggregate behavior, as reflected by actual prices, does converge to the theoretical market-clearing benchmark both in PO and in DA (even though we use an open DA implementation without an auctioneer). Regarding the choice of market institution, we find that simple rules of thumb are sufficient to capture most behavior. Buyers seem to mostly chase after low (past) prices, switching to the institution where observed prices were better for them, along the lines of previously-postulated behavioral rules based on past performance only (e.g. Huck et al. 1999; Offerman et al. 2002; Bosch-Domènech and Vriend 2003). Sellers’ behavior appears to be more complex, but is well-explained by the combination of a similar rule which points toward high prices, and a complementary rule which focuses on favorable trader ratios. In our setting, where all buyers have the same resale values and all sellers have the same cost functions, equilibrium prices are a function of trader ratios, hence the latter rule might reflect forward-looking behavior. An extensive experimental literature, going back to Chamberlin (1948) and Smith (1962), has analyzed the empirical properties of market institutions for given demand and supply when viewed in isolation (for an overview see Plott and Smith 1978b, Part 1).Footnote 8 Surprisingly, however, very few contributions have combined the experimental investigation of actual trading behavior with the analysis of the choice of the trading institution for a given good. One notable exception is Campbell et al. (1991), which investigated the endogenous choice between a computerized double-auction market (with an auctioneer) and (illegal) off-floor trading in the context of stock markets, and its impact on the bid-ask spread. The latter was implemented as direct negotiations, specifically traders could submit direct offers for blocks of three units to their two neighboring traders of the other market side. Kugler et al. (2006) compare direct negotiations and centralized markets in an experimental setting where each trader can trade a single unit only. Trade can be conducted in two alternative institutions. The first captures centralized markets through a sealed-bid double auction with a single market clearing price (as in Campbell et al. 1991, computed by an auctioneer). The second captures direct negotiations through bilateral matching, where the matching was designed as to maximize trade. Inefficiency might appear due to asymmetric distribution of traders across the two institutions, but their focus was not on the comparison of institutions in terms of efficiency. Rather, they focus on heterogeneous traders with different values and find that different types of traders generally prefer different market mechanisms. Their results show an unraveling of direct negotiations, which is led by higher-value traders (buyers with high resale values or sellers with low costs), who learn faster to coordinate on the centralized market. Last, the theoretical work of Alós-Ferrer and Kirchsteiger (2015) included an experiment on platform selection but adopted a reduced-form payoff table for actual trade, that is, traders did not make actual trading decisions. Alós-Ferrer and Kirchsteiger (2015) focused on the selection of market-clearing institutions (vs. institutions with price biases) and showed that certain alternative institutions could survive in the long run when players followed simple behavioral rules of thumb.",2
25.0,1.0,Experimental Economics,04 May 2021,https://link.springer.com/article/10.1007/s10683-021-09713-y,If you could read my mind–an experimental beauty-contest game with children,February 2022,Henning Hermes,Daniel Schunk,,Male,Male,Unknown,Male,"An important skill for economic actors is the ability to anticipate the actions of others in strategic settings and to choose one’s own actions accordingly. There is a very wide range of situations in which these strategic interaction skills are crucial.Footnote 1 For example, in their well-known study on signaling games, Cooper and Kagel (2005) argue, based on recording and coding of the dialogues of experimental participants, that “a critical step in monopolists’ learning to play strategically is putting themselves in the entrant’s shoes, reasoning from the entrant’s point of view to infer likely responses to their choice as a monopolist.” Similarly, empirical evidence demonstrates that individual investors base their investment decisions on their beliefs about the return expectations of other investors (Rangvid et al., 2013; Egan et al., 2014). Another example are most forms of matching markets; specifically, Braun et al. (2014) show this in the context of a laboratory study that investigates students’ behavior in university admissions procedures. Finally, the dynamics and outcomes of weakest-link games, representing, for example, coordination problems in a firm, depend on the players’ ability to anticipate the actions of their peers (e.g., Brandts and Cooper, 2006). But why studying these skills in children? In the last two decades, a large number of studies (summarized in Kautz et al., 2014) has documented that preferences and skills are shaped in the early years, they form the basis for future investments, and fundamentally determine adult life outcomes. For this reason, the early development of preferences and skills has been studied in great detail in recent years in the economic literature (see Sutter et al., 2019, for a review). By contrast, children’s strategic interaction skills have been studied much less, despite being of similar importance in their own right as well as for the life cycle of skills. The study of strategic interaction skills among children is inherently difficult, reflecting Jean Piaget’s view that up to a certain age, children systematically lack the ability to adopt other’s perspectives (Piaget, 1962). Hence, most existing work on this topic has so far considered only settings, in which children interact either with a single peer child or with a computer (see the literature review below). However, this disregards the fact that most strategic interactions outside the laboratory involve several (and often large numbers of) peers, and thus require a more comprehensive notion of strategic sophistication. Here, we develop a novel design to study strategic interaction skills in children, which is based on the experimental beauty-contest game (henceforth, BCG) introduced by Nagel (1995, at that point called “guessing game”): N decision-makers simultaneously choose a number x between 0 and 100 and the person with the number closest to \(p*{{\bar{x}}}\) wins a fixed prize (mostly, \(p = 2/3\)). This economic game has been used to study strategic interaction in groups for more than two decades. The experimental BCG has a strong advantage compared with most other interactive games used in economics to study decision-making (e.g., ultimatum game, public goods game, etc.): Neither social, nor time or risk preferences should affect decision-making (cf. Kocher & Sutter, 2005).Footnote 2 A substantial body of knowledge has been developed concerning how various game parameters such as repetition, feedback, or time pressure affect performance in a BCG (Duffy & Nagel, 1997; Ho et al., 1998; Weber, 2003; Kocher & Sutter, 2006) and how individuals versus teams of different sizes behave (Kocher & Sutter, 2005; Sutter, 2005). Yet, there is no study using a BCG to measure strategic interaction skills in groups with children. Given the relevance of strategic interaction skills for economic decision-making (see above), having such a measure of strategic interaction skills for children would not only be interesting in itself but would also enable us to shed light on which other skills (cognitive or noncognitive) are the building blocks to developing strong strategic interaction skills over the life cycle. To achieve this goal, we simplify the experimental BCG into a board game—we make it less abstract, provide concrete and visually illustrated operations with a spatial interpretation corresponding to each step in the game, and use the median, integers (0–100), \(p=1/2\), and only five players per group. Applying this new design for the experimental BCG, we study the behavior of \(n = 114\) children aged 9–11 years in the game and demonstrate that they are capable of successfully playing an experimental BCG. We also validate the new design using an adult student sample with \(n = 120\). In a second step, aimed at understanding the building blocks of children’s strategic interaction skills, we analyze the link between children’s performance in this game and their fluid IQ, an important part of cognitive skills. Our results demonstrate that fluid IQ is predictive only of whether children choose weakly dominated strategies, but is not associated with lower choices in the first round or with successful performance in the subsequent BCG. Hence, we contribute to the literature on strategic interaction in two ways, namely by (1) developing a tool to study strategic interaction in groups with young children and adolescents, and by (2) analyzing the role of cognitive skills for strategic interaction in a less abstract game form. Our study is related to a literature on strategic interaction in children. Some early studies (starting with Murnighan & Saxon, 1998; Harbaugh & Krause, 2000; Harbaugh et al., 2003a, 2003b) have examined children’s behavior in simple interactive games, such as dictator, ultimatum, trust, and public goods games. However, there is not much research about more complex strategic interactions in children, i.e., settings in which the cognitive demand of the task is higher and in which it matters to what extent children are able to take the perspective of others and translate this into their own strategic decision-making.Footnote 3 Among the few notable exceptions are the following studies: First, there is a study on strategic interaction that involves observing child-experimenter interactions: Sher et al. (2014) have children play two games, a sticker game, and a sender-receiver game. They argue that for successful strategic interaction, children require the ability to undertake recursive thinking, i.e., “the ability to use the output of one step of a reasoning process as input to a following step” as well as the ability to put themselves into another person’s shoes, i.e., to understand another person’s motives and emotions (“Theory of Mind”) as well as their incentives (again, cf. Cooper & Kagel, 2005). In their study, many children possess these skills from about the age of 7 years onward. Second, there is a number of two-person games, i.e., involving the interactions between two children: Brocas and Carrillo (2018a) examine iterative reasoning among children from pre-kindergarten to first grade in the context of four two-person games. They show that both logical thinking and Theory of Mind are key ingredients to successful strategic reasoning, and trace children’s limitations in both these skills in their age groups. In another series of two-person games, Brocas and Carrillo (2018b) present further support for these findings and show that although preschoolers are able to think strategically in principle, this does not mean that they are capable of acting accordingly. Brocas and Carrillo (2020) play a two-person BCG with children and adolescents from 5 to 18 years. Their findings indicate that strategic sophistication is present early on and, while abstract reasoning is known to develop through adolescence, equilibrium play in their game improves through childhood and stabilizes already after age 10. The detected learning trajectory is mostly due to feedback-learning, i.e., observing the choice of the partner. Fe et al. (2019) play a competitive game designed to trigger level-k thinking with pairs of children aged between 5 and 12 and find that Theory of Mind, cognitive ability, and age are related to the children’s level-k-behavior. Furthermore, Czermak et al. (2016) present 10–17-year-old children with simple experimental normal-form games for two children and elicit their beliefs, providing evidence that children of this age are clearly able to strategize in their choices. Third, there is one study that involves the interaction between three children: Brocas and Carrillo (2019) use a graphical paradigm of a dominance solvable game to study the evolution of the ability to think strategically between 8 years and adulthood. They report significant performance increases between 8 and 12 years and a stabilization afterwards. While these above-mentioned studies are related to our approach, our experimental setting involves the interaction with a group of other children in the context of a normal multi-player BCG. That is, the game requires a substantive degree of logical thinking and perspective-taking. Research by Brosig-Koch et al. (2015) has demonstrated that children at the age of 6 years already have the ability to reason backwards and that this ability increases with age. Thus, we expect that children in our sample (aged 9–11 years) will possess the necessary cognitive skills.Footnote 4 Moreover, empirical research using Theory of Mind tasks confirms that, by the end of preschool, most children should be well able to take the perspective of others (Wellman et al., 2001; Brocas & Carrillo, 2018a); hence, we expect children to also possess sufficient perspective-taking abilities. Nevertheless, strategic interaction in the form of an experimental BCG is very challenging, even for adults. Grosskopf and Nagel (2008) conducted a two-player version of the BCG, which can be easily translated into a “whoever chooses the smaller number wins” contest. They found that only 10% of (adult) lab participants and 37% of professionals (i.e., participants at economic conferences) actually chose zero—in this form of the game, zero is the dominant strategy, irrespective of the other player’s choice (means were 35.6 and 21.7, respectively). Similarly, Bosch-Rosa and Meissner (2020) use one player guessing games with adults to show that even with this simple structure, many subjects fail to fully understand the structure of the game. The findings of our study suggest that—despite this challenging setting—children are able to understand the experimental BCG if it is presented in a visual manner, and we use this new design to draw conclusions on the determinants of strategic interaction behavior in children. The remainder of this paper is structured as follows: Sect. 2 describes the experimental procedures and the new design of the experimental BCG used in the present study, Sect. 3 presents the results and discussion, and Sect. 4 concludes.",
25.0,1.0,Experimental Economics,23 June 2021,https://link.springer.com/article/10.1007/s10683-021-09714-x,Network defense and behavioral biases: an experimental study,February 2022,Daniel Woods,Mustafa Abdallah,Timothy Cason,Male,Male,Male,Male,"Economic resources spent on securing critical infrastructure from malicious actors are substantial and increasing, with worldwide expenditure estimated to exceed $124 billion in 2019 (Gartner 2018). Cybersecurity defense is becoming increasingly difficult, as systems are frequently connected to the outside world through the Internet, and attackers innovate many new methods of attack. The interaction of computers, networks, and physical processes (termed ‘Cyber-Physical Systems’, or CPS) has a wide variety of applications, such as manufacturing, transportation, medical care, power generation and water management (Lee, 2015), and has both practical and theoretical importance. Proposed CPS such as the ‘Internet of Things’ promise vast benefits and efficiencies, but at the cost of increased attack vectors and targets (see Alaba et al., 2017; Humayed et al., 2017 for surveys). To realize the potential gains that these new technologies can provide, we must understand and maximize their security. To reduce interference with their systems, institutions allocate a security budget and hire managers responsible for minimizing the probability of successful attacks on important assets and other vital parts of the infrastructure. Such decision-makers, however, are subject to behavioral biases that can lead to sub-optimal security decisions (Abdallah et al., 2019a, b; Acquisti & Grossklags, 2007). Human decision-makers can exhibit many possible biases. The security decisions they face broadly involve probabilistic assessments across multiple assets and attack vectors, many featuring low individual likelihood. We therefore ex-ante focus on the possibility that people incorrectly weight the actual probability of attack and defense (Tversky & Kahneman, 1992). We ex-post find that people also exhibit locational and spreading biases in their defense resource allocations, due to the directional and compartmentalized nature of these systems. Given the immense size of global expenditures on cybersecurity, as well as successful attacks being potentially very damaging, it is important to understand the nature and magnitude of any biases that can lead to sub-optimal security decisions. Such insights on biases can then be applied by security professionals to reduce their impact. We focus on human biases as infrastructure security decisions have not yet been given over to algorithmic tools. They are still mostly made by human security managers (Paté-Cornell et al., 2018). Adoption of automated tools are stymied by legacy components in these interconnected systems, so instead managers use threat assessment tools which return the likely probability that individual components of the infrastructure will be breached (Jauhar et al., 2015). These probabilities must be interpreted by the human manager, which motivates our initial emphasis on non-linear probability weighting. Evidence also exists that security experts ignore more accurate algorithmic advice when available and instead rely more on their own expertise (Logg et al., 2019). We model a security manager’s problem as allocating his budget over edges in a directed attack graph with the nodes representing various subsystems or components of the overall CPS. An example of a directed attack graph is shown in Fig. 1. The manager’s goal is to prevent an attacker who starts at the red node on the left from reaching the critical green node on the far right. The inter-connectivity of different systems is represented by connections between nodes, and alternative paths to a given node represent different methods of attack. Allocating more of the security budget to a given edge increases the probability that an attack through that edge will be stopped. Such an ‘interdependency attack graph’ model is considered an appropriate abstraction of the decision environment a security professional faces in large-scale networked systems.Footnote 1 The probability of successful defense along an edge is weighted according to the manager’s probability weighting function. We use the common Prelec (1998) probability weighting function, but similar comparative statics can be obtained with any ‘inverse S-shaped’ weighting function. We assume the attacker is sophisticated and observes the manager’s allocation decision, and does not mis-weight probabilities. This reflects a ‘worst-case’ approach to security (discussed further in Sect. 2.1), and represents a necessary first step in investigating the impact of probability weighting and other biases on security expenditures. The manager’s mis-weighting of probabilities can cause investment decisions to substantially diverge from optimal decisions based on objectively correct true probabilities, depending on network structure and the security production function. The security production function maps defense resources allocated to an edge to the probability that an attack along that edge will be stopped. Empirical evidence has shown probability weighting to be relatively non-linear on the aggregate subject level (Bleichrodt & Pinto, 2000), so the impact on security decisions could be substantial. Probability weighting is also heterogeneous across individuals (Tanaka et al., 2010; Bruhin et al. 2010). Therefore, if probability weighting affects choices in this environment, individuals should exhibit heterogeneity in their sub-optimal security decisions. Example directed network attack graph We seek to address the following research questions: Question 1: What is the effect of probability weighting on security investments over a directed network graph? Question 2: Is probability weighting an empirically relevant factor in human security decision-making? Question 3: What other behavioral biases significantly affect decision-making in this environment? To address Question 1, we numerically solve the security manager’s problem described above. In practical situations the relationship between investment spending and reductions in the probability of an attack is far from explicit to an outside observer. Moreover, investigations of successful breaches are often not revealed until months or years later. Furthermore, information on security investments is highly confidential for obvious reasons, making it difficult or impossible to obtain directly from firms. We therefore conduct an incentivized laboratory experiment to address Questions 2 and 3. We employ networks that cleanly identify the impact of non-linear probability weighting on security investment decisions, and the generated data also reveal other behavioral biases that exist in this environment. Our experiment elicits separate measures of probability weighting outside the network defense problem to help address Question 2. One measure uses binary choices between lotteries which is relatively standard, and elicits probability weighting while controlling for the confound of utility curvature. The other measure is novel, and uses a similar network path framing to the network defense environment. This new measure reduces procedural variance relative to the main network defense task. It also exploits the irrelevance of utility curvature when there are only two outcomes to focus solely on probability weighting. We find that the network-framed measure of non-linear probability weighting is statistically significantly correlated with all the network defense allocations situations we consider. However, this correlation exists even in cases where probability weighting should have no impact. This suggests that subjects may exhibit limited sophistication beyond probability weighting alone. We therefore conduct a cluster analysis to identify heterogeneous patterns of behavior not predicted by probability weighting. This identifies additional behavioral biases. The first is a form of ‘naive diversification’ (Benartzi & Thaler, 2001), where subjects have a tendency towards allocating their security budget evenly across the edges. The second is a preference for stopping the attacker earlier or later along the attack path. Stopping an attack earlier can be seen as reducing the anticipatory emotion of ‘dread’ (Loewenstein, 1987) while stopping it later can be seen as delaying the revelation of potentially bad news (e.g., see Caplin & Leahy, 2004 for a strategic environment). Accounting for these additional biases, we continue to find some evidence that non-linear probability weighting influences subject behavior, as well as strong evidence for the additional biases. In our environment the additional biases seem especially naive, as edges are not different options with benefits beyond defending the critical node, and information on the attacker’s progress is not presented to the subjects sequentially. These inconsistencies possibly reflect a subject’s own mental model (e.g., of how an attack proceeds), but should be accounted for in future directed network decision environments. This paper contributes to the theoretical literature on attack and defense games over networks of targets, most of which can be related to computer network security in some fashion.Footnote 2 Our attack graph environment is rather flexible, and can represent some of the strategic tensions present in alternative network environments. Instead of focusing on attack graph representations of these other environments (which can be quite complex), we utilize more parsimonious networks in order to specifically parse out the effect of probability weighting. We have the ‘security manager’ play against a sophisticated computerized attacker who moves after observing the manager’s allocation. Playing against a computer dampens socially related behavioral preferences.Footnote 3 It also removes the need for defenders to form beliefs about the attacker’s probability weighting. This allows us to more cleanly identify the empirical relevance of non-linear probability weighting in this spatial network defense environment. If probability weighting is important empirically, then future research should incorporate it into models to better understand the decisions of real-world decision-makers. This paper also contributes to the experimental literature of attack and defense games in network environments.Footnote 4 One set of related experimental studies test ‘Network Disruption’ environments. McBride and Hewitt (2013) consider a problem where an attacker must select a node to remove from a partially obscured network, with the goal to remove as many edges as possible. Djawadi et al. (2019) consider an environment where the defender must both design the network structure as well as allocate defenses to nodes, with the goal of maintaining a network where all nodes are linked after an attack. Hoyer and Rosenkranz (2018) consider a similar but decentralized problem where each node is represented by a different player. Our environment differs from these Network Disruption games as we consider a directed attack graph network, i.e. the attacker must pass through the network to reach the critical node rather than remove a node to disrupt the network. Some other related experimental papers include ‘multi-battlefield’ attack and defense games, such as Deck and Sheremeta (2012), Chowdhury et al. (2013) and Kovenock et al. (2019). The most closely related of these types of papers is Chowdhury et al. (2016), who find experimental evidence for the bias of salience in a multi-battlefield contest, which induces sub-optimal allocations across battlefields. We are the first to investigate empirically the bias of probability weighting in networks and attack and defense games.",2
25.0,1.0,Experimental Economics,03 May 2021,https://link.springer.com/article/10.1007/s10683-021-09715-w,Smartphone bans and workplace performance,February 2022,Adrian Chadi,Mario Mechtel,Vanessa Mertins,Male,Male,Female,Mix,,
25.0,1.0,Experimental Economics,06 May 2021,https://link.springer.com/article/10.1007/s10683-021-09716-9,Individual discount rates: a meta-analysis of experimental evidence,February 2022,Jindrich Matousek,Tomas Havranek,Zuzana Irsova,Unknown,Male,Female,Mix,,
25.0,1.0,Experimental Economics,22 June 2021,https://link.springer.com/article/10.1007/s10683-021-09718-7,Learning to hesitate,February 2022,Ambroise Descamps,Sébastien Massoni,Lionel Page,Male,Male,Male,Male,"In the economic theory of decision under uncertainty (risk or ambiguity) the decision maker has to choose between alternatives while facing an irreducible uncertainty about their final payoffs. However, in a wide range of situations, the decision maker has the possibility to reduce the uncertainty she faces by looking for information before making a choice. Typical examples include the following situations: a consumer can search for additional reviews before making a purchasing decision, an agent can look for more information before choosing between different investment opportunities, a doctor can request new tests before making a diagnostic, an inspector can decide to investigate a case further before making a decision. In such situations, the decision maker is confronted with an “optimal sequential sampling” problem whereby she needs to decide when to stop acquiring information about the different alternatives and make a choice between them. As early as 1945, Wald developed a formal framework to find the solution to this question. For a Bayesian decision maker (henceforth DM), the optimal solution is to gather information until the marginal expected value of information is smaller than the cost of sampling information. Does this model offer a good representation of actual individual behaviour? The empirical evidence is scarce, which is somewhat surprising given the general nature of this framework and the widespread situations where it can be used. In the present study we experimentally investigate whether this theoretical framework helps us understand how people make decisions. We design a controlled situation where individuals have to choose between two alternatives with uncertain payoffs. Before making a choice, they have the opportunity to wait and collect additional (costly) pieces of information which help them get a better idea of the likely alternatives’ payoffs. The design of the experiment allows us to precisely identify the optimal sequential sampling strategy and to assess whether participants are able to approximate it. We find that participants deviate in systematic ways from the optimal strategy. They tend to hesitate too long and oversample information when it is relatively costly, and therefore when the optimal strategy is to collect only little information. On the contrary, they tend to undersample information when it is relatively cheap, and therefore when the optimal strategy is to collect a lot of information. We show that this pattern of oversampling and undersampling can be explained as the result of Fechner cognitive errors which introduce stochasticity in decisions about whether or not to stop. Cognitive errors create a risk to stop at any time by mistake. When the optimal level of information to acquire is high, DMs should continue to sample information for a long time. As a consequence, errors are likely to lead to stop too early, and therefore to undersampling. When the optimal level of evidence to acquire is low, DMs should stop sampling early. In that case, cognitive errors are more likely to lead to fail to stop early enough, and therefore to oversampling. The deviations we observe, lead participants to lose between 10 and 25% of their potential payoff. However, participants learn to get closer to the optimal strategy over time, as long as information is relatively costly. The present study contributes to two strands of literature. First, it contributes to the empirical study of the ability of optimal sequential sampling models to explain human behaviour. The optimal sampling framework has recently attracted a lot of interest to explain human decision making. In psychology, it is now seen as giving a foundation to a wide range of cognitive models (e.g. drift-diffusion models) where it has been found to appropriately describe the information accumulation process in the brain. While such models have mainly focused on perceptual choices (Gold and Shadlen 2002; Bogacz et al. 2006), recent studies have suggested extending these models to economic choices (Webb 2013; Fehr and Rangel 2011; Krajbich et al. 2014; Caplin and Martin 2016). Optimal sequential sampling models can explain several empirical patterns. For instance, they imply that choices should be partly random due to the random nature of new information. As a consequence, they offer a foundation to random utility models (Webb 2013; Woodford 2014). Such models can also explain apparently puzzling patterns such as the negative correlation between decision times and quality of decisions (Fudenberg et al. 2015). While the interest for the optimal sequential sampling framework is growing in economics we still know little about its ability to explain actual behaviour. Our study adds to this emerging literature. Second, this paper contributes more broadly to the study of the empirical relevance of real option theory for individual decisions. When having to decide between making a choice now and waiting to get more information, the DM has to compare the cost of collecting the additional information to the option value of future information. Such a situation is a type of real option problem. In the real option literature, it has been investigated in the specific case of a decision between making an irreversible investment now or waiting for more information, at the risk of seeing the investment opportunity disappear (McDonald and Siegel 1986). An empirical test of this theory was proposed by Oprea et al. (2009). They analyse the behaviour of agents who incur a fixed cost to seize an irreversible investment opportunity. Their main result is that despite poor performance at the start of the experiment, the subjects learn “intuitive heuristics” to approximate the optimal behaviour. In follow-up experiments, Della Seta et al. (2014) find that real option models’ predictions with respect to risk aversion are supported, while Viefers and Strack (2014) find that subjects are not always consistent and adopt a behaviour suggesting regret. Our paper extends this type of research by investigating behaviour in a general case where the DM faces two possible prospects with unknown values and has to weigh the value of collecting additional information against the cost of this information. The rest of the paper is as follows: Sect. 2 presents the conceptual framework of the optimal sequential sampling model, Sect. 3 describes our experimental design, Sect. 4 presents our results and how participants’ choices deviate from the optimal solution to the problem, Sect. 5 shows that the pattern of behaviour we observe can be explained by a the stochastic nature of choices to stop, and Sect. 6 concludes.",2
25.0,2.0,Experimental Economics,01 September 2021,https://link.springer.com/article/10.1007/s10683-021-09726-7,What is considered deception in experimental economics?,April 2022,Gary Charness,Anya Samek,Jeroen van de Ven,Male,Female,Male,Mix,,
25.0,2.0,Experimental Economics,28 September 2021,https://link.springer.com/article/10.1007/s10683-021-09731-w,By chance or by choice? Biased attribution of others’ outcomes when social preferences matter,April 2022,Nisvan Erkal,Lata Gangadharan,Boon Han Koh,Unknown,Female,Unknown,Female,"In many environments, the determinants of outcomes are not observable. Decision makers make unobserved choices under risk and uncertainty, and outcomes are determined by a combination of their choices and luck. For instance, a firm’s profits are determined by both the business strategies taken by its managers and the macroeconomic factors that are beyond their control. How are outcomes evaluated in such situations? Are there systematic biases in the attribution of outcomes to the decision makers’ choices versus luck? Do they receive too little or too much credit? We explore these questions in a leadership context where the choices decision makers make under risk affect their own payoffs as well as those of other individuals. Leadership is often about decision making for others and inherently involves assuming responsibility for the outcomes of others (Ertac & Gurdal, 2012; Edelson et al., 2018). In many cases, decision makers face a trade-off between maximizing their own payoff and those of other individuals. For example, society’s growing demand for corporate social responsibility, defined as sacrificing firm profits for social interest, demonstrates how decision makers in positions of power are expected to engage in prosocial activity (Bénabou & Tirole, 2010). We report findings from two experiments designed to examine how individuals who are affected by the choices of the decision makers form inferences about the decision makers. Our experimental design emphasizes the role social preferences play in leadership and aims to analyze the inferences formed about this important personality trait of decision makers. Uncovering biases in the attribution of outcomes is important for understanding the attitudes towards decision makers and the decision-making environment.Footnote 1 In Experiment 1, individuals in their role as decision makers make an investment choice on behalf of their group. They choose between two investment options with binary outcomes. The outcome to the group depends on both the decision maker’s choice, which is unobservable to the other group members, and luck. A high investment leads to a higher probability of the good outcome for the group but comes at a higher private cost to the decision maker. Hence, one can also think of the high investment decision as a costly effort choice made by the decision maker that increases the group’s surplus at a personal cost. Consequently, decision makers’ choices are affected by their social preferences. Using this design, we examine the group members’ initial beliefs about the decision maker’s type, and how these beliefs are updated after observing the outcome of the choice made by the decision maker. In our analysis of belief updating, we examine two issues. First, we study biases in the way prior beliefs are treated in the updating process when group members form inferences about decision makers’ prosocial preferences. More precisely, taking Bayes’ rule as the benchmark, we ask whether group members suffer from base-rate neglect (i.e., put too little weight on their prior beliefs) or confirmatory bias (i.e., put too much weight on their prior beliefs) relative to a Bayesian.Footnote 2 Second, we examine whether group members respond too little or too much to new information about the choice made by the decision maker, and whether there is an asymmetry in the way good and bad outcomes are treated. Responding too little to a good (bad) outcome, for example, would imply that they believe decision makers act selfishly (prosocially) and luck plays a bigger role in determining outcomes. Hence, if members exhibit a bigger response to bad outcomes (as compared to good outcomes), then this implies that they are more likely to blame the decision maker for acting selfishly when they see a bad outcome, but they do not attribute a good outcome to the decision maker’s prosociality. In Experiment 1, we find that group members consistently suffer from base-rate neglect. This indicates, for example, that members who are initially more optimistic about the likelihood that the decision maker made a high investment decision tend to over-update their beliefs about the decision maker’s behavior when they observe a bad outcome. After accounting for base-rate neglect, we find that on average, members under-respond to good outcomes and attribute them more to luck as compared to a Bayesian. In contrast, their response to bad outcomes is similar to a Bayesian. This asymmetry implies that members on average attribute good outcomes more to luck and bad outcomes more to the decision maker’s selfish choice. As a result, decision makers get too little credit for their successes. We also consider whether members’ belief-updating behavior depends on the process by which the decision maker is selected. For instance, it may be the case that members are more likely to blame decision makers for bad outcomes if they are not appointed by the group. Consistent with our theoretical framework, we find that the appointment mechanism affects the initial beliefs formed about the decision maker’s type. For example, members believe that a group-appointed decision maker is more likely to act in the group’s interest as compared to a randomly appointed decision maker. However, once we control for the impact of the appointment mechanism on the initial beliefs, we find that the appointment mechanism has no additional impact on the updated beliefs. A feature of our design in Experiment 1 is that the decision makers’ choices and members’ beliefs are elicited using the strategy method, where all individuals first make choices as decision makers before reporting their beliefs as group members. This allows us to examine the relationship between individuals’ choices as decision makers and their attribution of the decision makers’ outcomes as members. Using this design, we uncover that the asymmetry we identify in the evaluation of good and bad outcomes is driven by those individuals who make the less prosocial choice for the group. That is, those who make lower investment choices as decision makers are more likely to attribute others’ good outcomes to luck. This suggests that a consensus effect may be at play as individuals use their own behavior as the basis for updating their beliefs about others (Ross et al., 1977; Marks & Miller, 1987; Dawes, 1989).Footnote 3 We explore this result further in Experiment 2, where participants no longer play both roles in the experiment. We are interested in investigating whether the biases we observe in Experiment 1 still exist when group members do not have experience making choices as decision makers.Footnote 4 Participants are informed at the beginning of the experiment whether they have been assigned as a decision maker or a group member. These roles do not change throughout the experiment. To investigate whether different types (i.e., prosocial versus selfish individuals) form and update their beliefs differently, we ask group members to report their beliefs before asking them to indicate, hypothetically, what their investment decision would have been if they were the decision maker. This allows us to test whether there exists a correlation between each group member’s type and their belief irrespective of whether they play both roles or just one. That is, by eliciting these hypothetical decisions after the belief-elicitation stage, we are still able to examine the relationship between individuals’ effort choices as decision makers and their beliefs as members. Interestingly, a correlation between the attribution of good outcomes to luck and what members would have chosen as decision makers also emerges in Experiment 2. That is, those members who are more likely to attribute good outcomes to luck are also more likely to state afterwards that they would have chosen low effort if they were placed in the position of the decision maker. This leads us to conclude that with and without the experience of acting as a decision maker, the same biases exist and seem to be driven by a consensus effect. Our paper is related to three strands of the literature. First, our study substantially advances the research on attribution biases in beliefs in both economics and psychology. Studies in experimental economics have analyzed biases in beliefs and information processing by focusing mainly on ego-related beliefs, i.e., beliefs about one’s own ability or physical attributes where one’s ego can play a big role in shaping their beliefs (Eil & Rao, 2011; Ertac, 2011; Grossman & Owens, 2012; Möbius et al., 2014; Coutts, 2019). Both Eil and Rao (2011) and Möbius et al. (2014) find evidence of asymmetric updating, where agents are more responsive to good news than to bad news about their own performance in an IQ test or a beauty task. While Grossman and Owens (2012) find no evidence of asymmetry, Ertac (2011) and Coutts (2019) find that individuals tend to overweigh bad news.Footnote 5 The related literature in psychology has mainly focused on self-serving biases in the attribution of own versus others’ outcomes (see, e.g., Miller and Ross, 1975). Consistent with our findings, individuals tend to attribute others’ good outcomes to exogenous factors (such as luck). In comparison, they are more likely to attribute their own good outcomes to endogenous factors (such as ability). Similarly, Pettigrew (1979) finds that good outcomes of out-group members are attributed to luck, but the opposite pattern emerges for in-group members. Our novelty in relation to this strand of the literature in both economics and psychology is that we focus on the evaluation of others’ outcomes in a context where decision making is shaped by social preferences. We show that good and bad outcomes are treated asymmetrically in this case also, and attribution biases exist in the case of good outcomes only. Moreover, our findings reveal that individuals’ evaluation of others’ prosociality tend to be correlated with their own behavior. Second, our work is related to the literature on outcome biases, where researchers also find asymmetric evaluation of others’ good and bad outcomes. However, it is assumed in this literature that all determinants of outcomes are fully observable. Despite this, good outcomes are treated more favorably than bad outcomes, suggesting that evaluators are biased by luck (see, e.g., Charness and Levine, 2007; Gurdal et al., 2013; Brownback and Kuhn, 2019). Our research extends this literature by considering the (arguably more common) setup where determinants of outcomes are not observable. Finally, our paper is related to the literature which investigates how individuals respond to others’ favorable and unfavorable outcomes under uncertainty in contexts such as CEO compensation (Bertrand and Mullainathan, 2001; Leone et al., 2006), political elections (Wolfers, 2007; Cole et al., 2012), medical referrals (Sarsons, 2019), and soccer (Gauriot & Page, 2019).Footnote 6 In addition, a significant amount of attention has been devoted to preferences for redistribution under uncertainty. While some papers investigate the link between redistribution and beliefs about the determinants of inequality (Fong, 2001; Linos & West, 2003; Aarøe & Petersen, 2014; Alesina et al., 2018; Rey-Biel et al., 2018), others aim to identify the causal effect of varying the source of inequality (such as merit versus luck) on the level of redistribution (Cappelen et al., 2007; Almås et al., 2010; Durante et al., 2014; Almås et al., 2020). A key objective of all these studies is to examine how performance is evaluated under uncertainty. In contrast, our aim is to focus on the belief formation and updating process, and to study specifically the biases which may characterize it. Our experimental setting gives us the opportunity to examine attribution biases in a controlled environment with an objective signal generating process.",5
25.0,2.0,Experimental Economics,22 June 2021,https://link.springer.com/article/10.1007/s10683-021-09719-6,Voluntary redistribution mechanism in asymmetric coordination games,April 2022,Masaki Aoyagi,Naoko Nishimura,Yoshitaka Okano,Male,Female,Male,Mix,,
25.0,2.0,Experimental Economics,27 May 2021,https://link.springer.com/article/10.1007/s10683-021-09720-z,Obviousness around the clock,April 2022,Yves Breitmoser,Sebastian Schweighofer-Kodritsch,,Male,Male,Unknown,Male,"Strategy-proof (SP) mechanisms offer participants a simple solution: a dominant strategy. By design, such mechanisms avoid any of the widely documented mistakes due to strategic uncertainty and errors in equilibrium reasoning. Empirical studies of behavior in strategy-proof mechanisms nonetheless reveal systematic mistakes, i.e., deviations from the dominant strategy. For instance, Rees-Jones (2018) and Hassidim et al. (2018) provide field evidence of systematic preference mis-representation in the deferred-acceptance mechanism for the US medical residency match; in laboratory auctions, ever since the seminal work of Kagel et al. (1987), overbidding in the sealed-bid second-price auction is a well-established and robust finding. This evidence poses the important challenge of understanding how SP mechanisms can be improved to reduce such mistakes. Li (2017) addresses this challenge by offering a theory of obviousness of dominance in extensive-form games and of strategy-proofness in mechanisms. A dominant strategy is said to be obvious if, at any decision node along its path of play, the worst outcome under this strategy is always at least as good as even the best outcome under any alternative strategy that deviates there. By its nature, this theory is not set up to explain systematic deviations from dominance, but to capture a general source of difficulty in game cognition, whereby its implementation promises to also generally enhance dominance play. A key implication of requiring obvious strategy proofness (OSP) is that this selects dynamic/indirect mechanisms, and the theory also receives empirical support by the prominent experimental finding that, with private values, ascending-clock auctions, which are OSP, produce strikingly more dominance play than sealed-bid second-price auctions, which are not OSP (see Kagel et al., 1987; Li, 2017).Footnote 1 In this paper, we experimentally investigate how design actually matters for dominance play in this leading application of private-value auctions, and we quantify the extent to which this is captured by the theory of obviousness. Our starting point is the observation that the existing evidence concerns the joint effect of changing multiple design features at once, thereby conflating “feedback effects” due to presentation and information with the direct theoretical “obviousness effect” due to dynamic implementation. The comparison of ascending-clock and sealed-bid auctions is therefore hardly informative as to how design promotes dominance play, and it also constitutes only a weak test of Li’s theory. Our study overcomes these limitations by disaggregating the design differences via additional, intermediate auction formats. These separate the benchmark formats’ strategic properties (static v. dynamic bid elicitation) from their feedback properties (static v. dynamic bid resolution), whereby we can quantify their relative importance for behavior. In particular, we thus isolate the effect of the dynamic bid elicitation under ascending-clock auctions, which is theoretically necessary to cause dominance to become obvious. Based on our richer comparison, we find no significant behavioral effect of dynamic bidding, however; the observed behavioral differences are instead almost entirely due to theoretically irrelevant differences in “feedback design.” These results are corroborated by our structural analysis of bidding behavior, which also provides deeper insights into the cognitive channels through which feedback systematically affects mistakes. Our experiment uses the well-documented online materials accompanying Li (2017) to replicate his high-powered experimental comparison of the sealed-bid second-price auction (2P) and the ascending-clock auction (AC) with the same sets of (private) valuation draws. Our innovation is the introduction of three novel auction formats to decompose the overall difference between 2P and AC into the contributions of three key design steps: clock presentation (bidders watch an ascending price clock resolve the bidding), dynamic bidding (bidders bid live on the clock), and drop-out information (bidders observe the number of opponents still bidding on the clock). Figure 1 offers a quick overview of our experimental design and decomposition results. The novel auction formats are AC-B, 2PAC and 2PAC-B. AC-B is an ascending-clock auction just like AC, except that it is “blinded,” so that bidders do not observe their opponents’ dropping out. Given private valuations, this information is theoretically irrelevant, and AC-B is also OSP. Nonetheless, the bidders’ ability to condition on this information under AC implies that AC is not strategically equivalent to 2P, so the behavioral comparison of these two formats suffers from a potential confound, which our additional comparison of AC-B and AC isolates.Footnote 2 2PAC and 2PAC-B both have bidders submit sealed bids just like 2P. However, these bids serve as automatic exit prices in a subsequently run ascending-clock auction, similar to “proxy bidding” introduced by eBay (e.g., Roth & Ockenfels, 2002), which is displayed either with drop-out information as in AC or blinded as in AC-B. Since bidders then cannot act anymore, these merely present the auction’s outcome the same way as the dynamic ascending-clock formats, in terms of feedback; indeed, 2PAC and 2PAC-B are both formally identical to 2P and accordingly SP but not OSP. Importantly, moving from 2P to 2PAC-B isolates the effect of mere clock presentation, and then moving from 2PAC-B to AC-B isolates the effect of dynamic bidding. This latter step is here in fact necessary and sufficient for theoretical obviousness of dominance, whereby the comparison of 2PAC-B and AC-B affords a both cleaner and stronger test of the prediction that OSP reliably reduces mistakes. 2PAC, by comparison with 2PAC-B, allows us to also measure the role of drop-out information as part of the passive clock-presentation, where, in contrast to AC, bidders cannot respond to it live. This summarizes what we consider the most informative comparisons. However, we can also test the basic theoretical prediction that either OSP format significantly outperforms any of the mere SP formats in terms of dominance play, where the latter include 2PAC, of course. Overview of auction formats analyzed in the experiment and their contributions to reduction in mean absolute deviations from truthful bidding. Note: The decomposition of the total effect reported here is derived from the decomposition of the mean absolute deviation of bids from values when we move from second-price auctions (2P) to ascending-clock auctions (AC). Specifically, using the “standardized” mean absolute deviations as reported in Table 5 (Appendix B), the presentation effect is the average effect of moving from 2P to 2PAC-B after the first three auction rounds, and all other values are similarly derived, by moving from 2PAC-B via 2PAC or AC-B, respectively, to AC Reassuringly, we find that our 2P and AC replicate the respective behavior in Li’s study almost exactly. Following his analysis, we focus on mean absolute deviations from truthful bidding for non-winning bids. We find that, after a brief initial phase of around three auction rounds, where participants underbid severely in all formats (as observed also by Noussair et al., 2004, and, of course, Li, 2017; see also Cason & Plott, 2014), the relative contributions stabilize: Clock-presentation (2P \(\rightarrow\) 2PAC-B) accounts for around 50% in reducing deviations from truthful bidding, dynamic bidding (2PAC-B \(\rightarrow\) AC-B) promotes this further by 12%, which is statistically insignificant, and the remaining share of almost 40% is due to live drop-out information (AC-B \(\rightarrow\) AC). Even as pure feedback, drop-out information reduces deviations further, by 25% (2PAC-B \(\rightarrow\) 2PAC), which is twice the effect of dynamic bidding. Theoretical obviousness therefore fails our stronger test with minimally experienced participants. The live drop-out information under AC turns out to be a highly relevant design element and confound of OSP in prior evidence. Once this confound is removed, however, the entire benefit of the dynamic ascending-clock format (OSP) boils down to the effect of clock-presentation alone, as in AC-B v. 2PAC-B. Actual dynamic implementation, though theoretically required for obviousness, has no significant further benefit. What about initial behavior? As indicated, in the very first auction rounds, participants underbid severely across all auction formats, including 2P. Here, both OSP formats very significantly reduce this underbidding mistake compared to the three non-OSP ones.Footnote 3 This success of the theory regarding initial play is remarkable, but has to face the concern that dynamic auctions may be more familiar. Indeed, our evidence following initial play indicates that small differences in familiarity might produce strong behavioral effects, because minimal experience certainly does so: Deviations from dominance play generally drop sharply immediately after the first round, and both 2PAC and 2PAC-B more than catch up with initial play under AC and AC-B already by round 2; one round later even 2P does so weakly. This confirms our earlier conclusion that the key towards dominance is the clock presentation, which the OSP formats have before the very first bid, not the way bids are strategically elicited. Li also recognizes the concern of differences in familiarity, and he addresses it by additionally running “X-auctions,” which perturb the standard formats by a random mark-up element to remove any familiarity. We also include X-auctions in our study, and for these presumably unfamiliar auctions, we find no advantage to OSP even in initial play.Footnote 4 Finally, we move beyond the purely descriptive analysis, to gain a deeper understanding of how design drives behavior via game cognition and monetary incentives. We analyze a structural bidding model that allows for different decision processes and operationalizes obviousness as additional decision weight on the dominant strategy beyond expected payoffs. This delivers the following insights:Footnote 5 First, bidding behavior is well-explained by the actual monetary incentives that the bidders face (see also Harrison, 1989). Second, the effect of clock-presentation is to change how bidders process these incentives (expected payoffs), from a static evaluation of all possible bids at once under 2P, to a dynamic evaluation of iteratively deciding whether to continue to increment the bid or stopping; indeed, it occurs under all formats involving clock-presentation, even the strategically static 2PAC-B and 2PAC. In contrast to static evaluation, dynamic evaluation “mechanically” entails a tendency towards some underbidding, since incentives to further increment the bid vanish when approaching one’s valuation. Third, however, bidders also quickly recognize that underbidding is dominated, whereas they do not recognize this for overbidding. Moreover, the only significant difference in such obviousness is that they recognize this more strongly under AC than the other formats in standard auctions (and only there). Hence, the static decision process under 2P leads to overbidding, clock presentation reduces this mistake by changing game cognition to the dynamic decision process, and the ability to condition their bid on how many others are still in the auction under AC mitigates the resulting tendency to make underbidding mistakes. OSP ties the cognitively effective clock presentation to the requirement that bids are elicited in an indirect, dynamic manner. We consider our main finding that, rather fortunately, the latter is unnecessary. This seems like a subtle distinction in theory, but given the constraints in real-world settings (e.g., global online marketplaces), it is a potentially very useful one in practical design problems. It also relates to the apparent success of OSP regarding initial play, since initial behavior is always preceded by a cognitive stage (in fact, a cognitive biography) that may well include opportunities for pre-play learning (e.g., dry runs against robots) provided by the designer. Moreover, this result bears good news also for the practice of experimental economics: Li’s theory fundamentally challenges the widely used strategy method by predicting that it would render dominance non-obvious and introduce mistakes. Our finding that this prediction fails therefore supports the method’s validity, in line with the survey of experimental comparisons by Brandts and Charness (2011). Methodologically, our main innovation is to experimentally separate the feedback design (in particular, its framing) from the strategic design of a game, which we consider a generally promising approach for obtaining structural insights into game cognition. In doing so, we also contribute a quantification of the long suspected behavioral importance of theoretically irrelevant live drop-out information in the ascending-clock auction. Its large effect calls for caution in testing theories of game cognition such as Li’s theory of obviousness: To be most informative, such tests should compare only strategically equivalent mechanisms. The auctions we consider are mechanisms to allocate a single indivisible object among a set of agents with private valuations. This economic problem is both practically important and particularly suitable for testing OSP, as Li’s Theorem 3 characterizes the class of OSP mechanisms for precisely this allocation problem: Essentially, given quasi-linear preferences, all OSP mechanisms take the form of an ascending-clock auction. More broadly, Li provides a formal argument speaking to the larger literature suggesting that indirect implementations often have the advantage of being simpler for participants than direct ones (see, e.g., Ausubel, 2004, or Kagel & Levin, 2009, for the case of auctions).Footnote 6 The OSP requirement is supposed to circumvent cognitive limitations in contingent reasoning about hypothetical scenarios, which itself is a well-established phenomenon (Charness & Levin, 2009; Esponda & Vespa, 2014), though it may alternatively relate to violations of Savage’s “sure-thing-principle” (Esponda & Vespa, 2019; Martínez-Marquina et al., 2019).Footnote 7 Kagel et al. (1987) were the first to demonstrate that the ascending-clock auction outperforms the sealed-bid second-price auction in terms of dominance play. Li replicated their experiment with substantially enhanced statistical power and confirmed the results. Harstad (2000) found that prior experience with the ascending-clock auction partially carries over to sealed-bid auctions. He also investigated so-called “p-list” auctions: Bidders face an ordered list of prices and indicate which are acceptable/unacceptable, and their highest acceptable price serves as bid in a second-price auction. This sealed-bid design generated underbidding, with great variation, and experience with it was also not as helpful for the second-price auction, suggesting that it is not merely the yes/no nature of decisions in ascending-clock auctions that leads bidders to quickly adopt their dominant strategy. Kagel and Levin (2009) studied ascending-clock auctions without drop-out information, similar to our AC-B auctions, though with only 13 participants as a small add-on treatment in a study of multi-unit auctions. They already pointed to drop-out information as an important source of the greater prevalence of dominance play in the usual ascending-clock auctions, in line with our findings. Both Harstad (2000) and Kagel and Levin (2009) emphasized the general role of feedback information for whether participants recognize their dominant strategy (see also Kagel & Levin, 2015), which our design disentangles into the effects of clock-presentation, dynamic bidding, and drop-out information. 2PAC and 2PAC-B can be seen as strategy-method implementations of an ascending-clock auction. Related to this, Schotter et al. (1994) experimentally compared behavior in simple dynamic games and their normal-form/bi-matrix versions and found strong differences. None of their games was SP, however.Footnote 8 Nonetheless, in a small add-on to their baseline, where the “second” mover has a weakly dominant strategy that is obviously so only in the dynamically played version, they found that presenting the static game as though it was dynamic strongly enhances that player’s dominance play. We also find a strong such presentation effect here for SP mechanisms, but we find it to work only through feedback, not instructions. In recent complementary experimental work, Georganas et al. (2017) manipulate the losses from overbidding and show that “off-dominance” expected payoffs strongly affect behavior in 2P auctions, and McGee and Levin (2019) find that this manipulation has much smaller effects in AC auctions, showing that dominance is more obvious in the latter. Our structural analysis confirms and refines both of these observations. Schneider and Porter (2020) also document an important role of experience for dominance play in SP and OSP mechanisms, showing that it serves as a substitute for cognitive ability at the individual level. While they find that switching from 2P to AC has a larger effect than experience, we separate the strategic and the feedback experience, and we find the latter to be the main driver of dominance play.",5
25.0,2.0,Experimental Economics,10 June 2021,https://link.springer.com/article/10.1007/s10683-021-09721-y,"Health workers’ behavior, patient reporting and reputational concerns: lab-in-the-field experimental evidence from Kenya",April 2022,Isaac Mbiti,Danila Serra,,Male,Male,Unknown,Male,"Accountability is often cited as a necessary factor to ensure the efficient provision of public services in developing countries (World Bank, 2003). Low levels of accountability in the health sector are associated with low levels of effort by healthcare providers, resulting in high absence rates ranging from 20 to 40 percent on a given day (Chaudhury et al., 2006; World Bank, 2013), limited patient interactions (Das et al., 2016), and low quality of service relative to providers’ demonstrated ability (Das & Hammer, 2014; Leonard et al., 2007). Since the cost of monitoring service providers may be significantly cheaper for citizens than for government agents, well-designed bottom-up monitoring schemes could improve service delivery by enhancing accountability (World Bank, 2003). By relying on informal or non-monetary sanctions, they may also be easier to implement than other alternatives, such as top-down accountability systems, especially in countries characterized by weak state and limited bureaucratic capacities. We test the effectiveness of an accountability system that relies on non-monetary sanctions and leverages providers’ reputational concerns by conducting a specially designed lab-in-the field experiment with actual health providers and patients in Nairobi, Kenya. We recruited patients randomly from a set of about 1800 surveyed patients exiting public and private frontline health facilities (health centers and dispensaries). We then invited a subset of providers from the same facilities to participate in behavioral games with the patients. In designing a lab-in-the-field experiment that could parallel some aspects of the patient-provider relationship, our starting point was the recognition that clinical interactions are characterized by information asymmetries, uncertainty (for example, the provider may be absent from work), and uneven distribution of powers. These factors suggest that the level of trust between patients and providers will capture important aspects of clinical interactions. Patients place their trust in providers when they visit a health center. Providers can then reciprocate a patient’s trust by being present at the facility and by providing high-quality service. Although there are additional aspects of clinical interactions beyond trust, there is a growing body of literature that shows that patient’s trust (or mistrust) in medical providers is a critical determinant of health utilization, health seeking behavior, and ultimately health outcomes in both developed and developing countries (Alsan & Wanamaker, 2018; Alsan et al., 2019; Archibong & Annan, 2021; Lowes & Montero, 2018; Martinez-Bravo & Stegmann, 2018). These considerations led us to employ a modified trust game (Berg et al., 1995; Serra et al., 2011) where patients can send money to providers and providers can reciprocate by sending money back. Another reason for employing a trust game setting – as opposed to a dictator game, for instance – is that receiving money from the first mover (the patient) in the game generates a feeling of obligation in the second-mover (the provider) to send money back, which we believe could capture a provider’s contractual obligation to provide care to the patients visiting his or her facility.Footnote 1 Contrary to a standard trust game, in our Reporting Game, we allow patients to file costly complaints against providers if they perceive the amount returned to be too small. We employ different treatments where we modify the consequences of such complaints, with our primary treatment of interest being one where complaints lead to non-monetary sanctions in the form of peer shaming activated through the disclosure of complaints to other providers (Peer Disclosure treatment). Our Monetary Penalty treatment, where patients’ complaints lead to a monetary loss for the provider, provides a useful benchmark to assess the effectiveness of the Peer Disclosure system,Footnote 2 even though monetary punishment systems are unlikely to be implemented in real life.Footnote 3 We contrast both treatments to a baseline treatment where complaints have no tangible consequences (Complaint Box treatment).Footnote 4 We also examine whether the possibility of provider retribution against complaining patients negatively affects complaining behavior and reduces the efficacy of non-monetary sanctions (Peer Disclosure with Retaliation treatment). We use the amount of money that providers send back to patients (scaled by the initial transfer amount) as our primary measure of health-worker behavior toward patients. In addition, we use the patient’s decision to file a complaint as a function of the amount returned by the provider as our main measure of the patient’s willingness to use a reporting system. In light of the growing literature showing that public providers and private providers have different traits and motivations (Cowley & Smith, 2014; Serra et al., 2011; Brock et al., 2016), we also examine the potential for differential responses to accountability schemes across public and private sector health providers. Our study relates to the small but growing literature that employs lab-in-the-field experiments to generate direct measures of health providers’ motivations and responsiveness to different incentive systems in developing countries (Banuri & Keefer, 2016; Barr et al., 2009; Brock et al., 2016; Serra et al., 2011). Contrary to the existing studies, in which providers are the only decision-makers in the experiment,Footnote 5 we examine the strategic interactions between providers and actual patients under different incentive systems. We also contribute to a large body of evidence from experiments in both the laboratory and the field testing whether informal mechanisms, such as social observability and public disclosure of performance rankings, can promote prosocial behavior, even in the absence of formal incentives. The evidence is mixed, with some studies (e.g., Andreoni & Petrie, 2004; Ashraf et al., 2014; Carpenter & Myers, 2010; Charness et al., 2014; Della Vigna et al., 2012; Erikson et al., 2009; Gerber et al., 2008; Karlan & McConnell, 2014; Linardi & McConnell, 2011) showing positive effects and other studies (e.g., Brent et al., 2019, Cason et al., 2016; Charness et al., 2014; Dufwenberg & Muren, 2006; Noussair & Tucker, 2007) finding null or even negative impacts on behavior.Footnote 6 In our Peer Disclosure Treatment, we inform providers that we will share the number of complaints a provider receives, a measure of patient dissatisfaction, with their peers. This differs from other studies which typically focus on sharing information on provider behavior or performance directly with clients. This way, we simulate and evaluate a bottom-up accountability system that could easily be implemented by non-governmental organizations, as it does not require the actual measurement and quantification of providers’ performance. In a related study, Ashraf et al. (2014) also compare non-monetary and monetary incentives in a health context. In their study of sales of condoms by hairdressers in Zambia, the authors examine the impact of publicly displaying a “thermometer” that reports condom sales. Our non-monetary accountability mechanism differs from Ashraf et al. (2014) in a variety of ways. First, our Peer Disclosure treatment generates comparisons across providers based on negative rather than positive measures of performance, therefore relying on the activation of non-monetary costs rather than non-monetary rewards. Second, our system relies on subjective evaluations by clients (i.e. complaints) rather than objective performance measures, such as sales.Footnote 7 Third, our treatment simulates a mechanism where patients’ complaints are only disclosed to providers and not to the public. Hence, our Peer Disclosure mechanism works purely through the activation of reputational concerns (or fear of shaming) among professional peers (the providers), absent any concern about the impact that information about relative performance could have on existing or potential clients.Footnote 8 Through our Peer Disclosure with Retaliation treatment we also add to the literature on harassment and retaliation, which has shown that fear of retribution prevents subjects from engaging in costly punishment or reporting of others both in lab (Abbink & Sadrieh, 2009; Abbink et al., 2014; Balafoutas et al., 2014; Denant-Boemont et al., 2007; Nikiforakis & Engelmann, 2011) and field settings (Balafoutas & Nikiforakis, 2012). Our lab-in-the-field environment is especially interesting because, while it relies on a laboratory experiment, it allows for the norms and expectations that guide the longer-term relationships existing between patients and providers in the field to play a role in the complaining decisions patients make within the experiment. Our findings show that, relative to a simple Complaint Box, the Peer Disclosure system increases provider reciprocity in the game by 42 percentage points, or approximately 40 percent. The possibility of provider retaliation reduces the effectiveness of the Peer Disclosure reporting system to 26 percentage points relative to the Complaint Box (approximately 25 percent). However, the coefficients are not statistically different from each other. In addition, we find that provider reciprocity tends to be more responsive to the Peer Disclosure system compared to the Monetary Penalty system, although we cannot reject the hypothesis that they are equally effective. We also find that public sector health workers are more generous toward patients and more responsive to our treatments compared to their private sector counterparts. This is consistent with a growing empirical literature (Banuri & Keefer, 2016; Serra et al., 2011; Delfgauuw et al., 2013; Gregg et al., 2011; Kolstad & Lindkvist, 2012) that finds significant differences between public and private sector employees with respect to their prosocial motivations and their responsiveness to monetary and non-monetary incentives. In addition, we find that less prosocial health workers, as measured in a standard Trust Game that preceded the Reporting Game, are more responsive to the threat of both peer shaming and monetary sanctions. With respect to the complaining decisions, on the intensive margin, patients submit more complaints under all systems that attach tangible consequences to the complaints, relative to the Complaint Box baseline. On the extensive margin, the propensity of patients to file unfavorable reports is lower when complaints lead to tangible monetary or non-monetary consequences for providers. A note on the external validity of our experiment is warranted. We took a series of measures to enhance the extent to which behavior in the experiment could be informative of behavior in real world settings.Footnote 9 First, we conducted the lab-in-the-field experiment with actual health providers and patients rather than university students. Second, we conducted the experiment in a developing country context, where bottom-up accountability is often touted as a potentially cost-effective way to improve service delivery. Third, we involved a representative sample of patients, randomly selected from a larger set which was interviewed while exiting health centers. Although our primary research focus is on examining the behavioral responses to our treatments, we acknowledge that the decisions providers and patients made in the context of our experiment differ from outside-the-lab decisions in a variety of ways. First, due to the transparency of each player’s actions in the reporting game, patients are able to clearly evaluate and identify the provider behavior that they deem deserving of a complaint. Although this may be more difficult for patients in real life, it is exactly this design feature that allows us to clearly measure patients’ willingness to file complaints when reporting systems are available. Our study is more relevant for designing policies aimed at curbing observable provider misbehaviors, such as absence from work and request for bribes, which are commonplace in developing countries (Chaudhury et al., 2006; Transparency International, 2011). As a partial test for the external validity of our methodology, we conducted unannounced visits at health facilities and correlated absence data with our lab measure of provider performance. We find that providers—and especially public sector providers—who return more money to patients in our lab-in-the field experiment are less likely to be absent from work, which suggests that our lab measures capture salient aspects of real life behavior. Overall, our study provides important insights and guidance regarding the design of better accountability systems, as it assesses some of the central assumptions and hypotheses behind these systems. These include the willingness of citizens to actively use them and the responsiveness of providers to patient feedback in a controlled environment. Given the significant fraction of GDP devoted to personnel in the public healthcare sector, improving provider behavior, especially the quality of service, can have significant implications on public finance management.Footnote 10 Our findings show that reporting systems that trigger social sanctions through disclosing providers’ negative feedback to their professional peers have the potential to significantly and positively affect providers’ performance. As many developing countries are actively considering accountability systems that rely on social sanctions, these design insights could be especially relevant in the current policy climate.Footnote 11",
25.0,2.0,Experimental Economics,04 June 2021,https://link.springer.com/article/10.1007/s10683-021-09722-x,Cognitive heterogeneity and complex belief elicitation,April 2022,Ingrid Burfurd,Tom Wilkening,,Female,Male,Unknown,Mix,,
25.0,2.0,Experimental Economics,10 June 2021,https://link.springer.com/article/10.1007/s10683-021-09723-w,Magnitude effect in intertemporal allocation tasks,April 2022,Chen Sun,Jan Potters,,,Male,Unknown,Mix,,
25.0,2.0,Experimental Economics,29 June 2021,https://link.springer.com/article/10.1007/s10683-021-09724-9,Flip a coin or vote? An experiment on the implementation and efficiency of social choice mechanisms,April 2022,Timo Hoffmann,Sander Renes,,Male,Male,Unknown,Male,"Before a group takes a decision—whether it is a corporate board thinking about investments, an expert panel considering different policies, a cabinet discussing reforms, or a nation considering who to appoint as president—the group has to select a decision rule that aggregates individual preferences into a group decision. With an inefficient decision rule, the group is likely to end up with less than optimal outcomes, e.g. bad investments, inefficient policies, unpopular presidential candidates, or simply deadlocked in discussions and forced to delay important decisions.Footnote 1 If a group finds its existing decision rule does not work well, its members could choose to replace it. Therefore, it is natural to expect inefficient rules to be replaced. In practice, however, we regularly see inefficient decision rules persist. Examples range from veto rules or restricted voting rights that limit the amount of information combined in corporate boards and shareholder meetings (De Jong et al., 2007; Grüner & Tröger, 2019), mechanisms that are hindered by reputational concerns in expert panels (Visser & Swank, 2007), the much criticized US electoral college (Rathbone, 2018), and remarkably stable electoral systems despite changing circumstances (Rahat, 2011). In this paper, we use an experiment to shed more light on when we can expect inefficient decision rules to (not) be replaced by efficient ones. To improve group decision rules, we need to take two necessary steps: design and implementation. First, we need to design and test an efficient mechanism for the particular context. Given the decisions at hand and the composition of the group, it might be better to use simple majority voting or to aim for group consensus. Second, we need to ensure that group members are willing to use the efficient mechanism. The corresponding design and participation problems have been studied extensively both theoretically and experimentally in the literature on exchange mechanisms such as auctions, matching, and market design. In contrast, the experimental part of the literature on efficient mechanisms in social choice is limited. The overview of the experimental literature presented in Chen (2008) only found one paper that analyzes the efficiency of the theoretically optimal mechanism (Attiyeh et al., 2000). Similarly, tests of the participation decision only seem to have occurred in the class of voting rules (Engelmann & Grüner, 2017; Bol et al., 2020; Engelmann et al., 2020). Our paper addresses this gap in the literature on efficient social choice mechanisms in two ways. First, we use a revealed preference setup to measure subjects’ willingness to participate in several mechanisms that appear repeatedly in theory. Second, we measure and compare the empirical efficiency of these mechanisms. Together, the revealed preferences and achieved efficiency levels allow us to show how private information, expected benefits, and outside options—all difficult to observe outside the lab setting—influence participation preferences. Our experimental results thus shed light on the empirical efficiency and implementability of decision rules, both in a controlled lab environment. Our results clearly show how outside options and private information shape subjects’ revealed preferences over mechanisms. Subjects that know they dislike the public project prefer a mechanism that does not allow provision but provides a safe payoff over all other mechanisms, as is predicted by the Myerson–Satterthwaite impossibility theorem (Myerson & Satterthwaite, 1983). Subjects who know they like the public project are willing to flip a coin to decide on the project as long as this increases the likelihood of implementation. Furthermore, both subjects that approve the public project and those that want to stop it prefer having influence over the outcome over flipping a coin. Therefore, with risky alternative mechanisms, voluntary participation in more efficient mechanisms can be possible even in ad interim stages, as is predicted by Schmitz (2002), Segal and Whinston (2011) and Grüner and Koriyama (2012). However, our results also show that the mechanisms are not as efficient in the lab as theory predicts, and the difference between theoretical predictions and measured efficiency depends on the setting and the mechanism. In some settings, the predicted ranking is reversed and therefore theoretical expectations of (individual) preferences for mechanisms can be misleading. In our experiment, we study four mechanisms: the theoretical optimal Arrow-d’Aspremont–Gérard–Varet (AGV) mechanism,Footnote 2 Simple Majority voting (SM), a Non-implementation mechanism that mimics the theoretical effects of forcing the Status Quo to persist by non-participation in the mechanism choice (NSQ), and flipping a coin (RAND, random decisions). The AGV mechanism is theoretically optimal in our public good setting. It is often used as the theoretical benchmark to which the efficiency other mechanisms are compared. However, the fact that the mechanism is optimal and efficient in theory does not necessarily translate to efficient outcomes in a laboratory or in practice. Despite its theoretical importance, the empirical performance of the AGV mechanism has not received much attention. To the best of our knowledge, the only direct test of its efficiency is in Attiyeh et al. (2000). They find that the AGV’s empirical efficiency is no larger than the theoretical efficiency of sincere voting in SM.Footnote 3 Our experiment allows us to directly compare AGV’s and SM’s achieved efficiency. The results show that the AGV mechanism is indeed more efficient than SM when the private valuations for the project are skewed. However, when the distribution is symmetric, SM is more efficient. We also show that SM is not as efficient as predicted in theory, but the difference between its predicted and achieved efficiency in the lab is much smaller and much more stable across settings than with the AGV. These findings highlight the importance of controlled tests for proposed mechanisms. Such tests are already the standard in auctions and matching (e.g. Roth, 2012) and in the related setting of Voluntary Contribution Mechanisms (VCM) (e.g. Bracht et al., 2008).Footnote 4 The differences between theoretical and achieved efficiency of the mechanisms, mean that our subjects’ preferences over mechanisms could be difficult to predict through theory alone. Remarkably, subjects can predict relative efficiency levels and select the mechanism that maximizes their expected payoff in the lab, even when the efficiency deviates from theoretic predictions. When SM is close to efficient, it is selected much more often than in settings where the AGV clearly outperforms SM in the lab. Furthermore, in a direct comparison, the empirical payoffs of the mechanisms more accurately predict mechanism choices than Bayes–Nash payoffs. Another difficulty of predicting the participation decisions and efficiency through theory alone, is the possibility that individuals attach value to non-monetary aspects. Group members that have other-regarding preferences attach a higher value to efficient mechanisms and might play differently within a given mechanism than narrowly self-interested group members (Engelmann & Grüner, 2017; Messer et al., 2010; Bierbrauer et al., 2017). However, in our setting, where we cleanly identify participation decisions and see the play in the selected mechanisms, narrow self-interest is the most important predictor. In fact, subjects prefer complete randomness over arguably fairer and more efficient mechanisms, as long as randomness gives them a better chance to obtain their preferred outcomes. The rest of the paper is organized as follows: Sect. 2 discusses related literature. Sect. 3 outlines the experimental design and treatments. Section 4 states the predictions we test, Sect. 5 tests these predictions and discusses further findings. Section 6 concludes.",2
25.0,2.0,Experimental Economics,01 July 2021,https://link.springer.com/article/10.1007/s10683-021-09725-8,Investigating the failure to best respond in experimental games,April 2022,Despoina Alempaki,Andrew M. Colman,Briony D. Pulford,Female,Male,Female,Mix,,
25.0,2.0,Experimental Economics,15 September 2021,https://link.springer.com/article/10.1007/s10683-021-09729-4,Deliberative structures and their impact on voting under economic conflict,April 2022,Jordi Brandts,Leonie Gerhards,Lydia Mechtenberg,Male,Female,Female,Mix,,
25.0,2.0,Experimental Economics,01 September 2021,https://link.springer.com/article/10.1007/s10683-021-09730-x,Externalities in knowledge production: evidence from a randomized field experiment,April 2022,Marit Hinnosaar,Toomas Hinnosaar,Olga Slivko,Female,Male,Female,Mix,,
25.0,2.0,Experimental Economics,21 October 2021,https://link.springer.com/article/10.1007/s10683-021-09732-9,"Subsidizing unit donations: matches, rebates, and discounts compared",April 2022,Johannes Diederich,Catherine C. Eckel,Philip J. Grossman,Male,Female,Male,Mix,,
25.0,3.0,Experimental Economics,13 August 2021,https://link.springer.com/article/10.1007/s10683-021-09727-6,On the stability of risk and time preferences amid the COVID-19 pandemic,June 2022,Andreas C. Drichoutis,Rodolfo M. Nayga Jr.,,Male,Male,Unknown,Male,"Economic theory suggests that a broad set of decisions relating to important outcomes such as savings, investments, insurance, retirement plans, occupational choices, labor supply, health services purchase, health behaviors and other aspects of everyday life, can be explained by differences in agents’ budget constraints as well as Risk and Time Preferences (RTPs). RTPs are at the crux of consumer behavior and in standard economic analysis, individual preferences are considered to be stable over time. Andersen et al. (2008b) argue that the assumption of stable preferences lies in the ability to assign causation between changing opportunity sets and choices in comparative statics exercises or, in Stigler and Becker’s (1977) words, ‘no significant behavior has been illuminated by assumptions of differences in tastes’. The assumption of stability of RTPs has been challenged however by the empirical literature. For example, Chuang and Schechter (2015) find weak evidence of stability in experimental measures of time preferences, although they find strong evidence of non-stability of experimental measures of risk preferences over time. Schildberg-Hörisch (2018) take the view that the extent to which preferences are stable is ultimately an empirical question. It is therefore of great importance for the study of economic outcomes to understand whether RTPs are a stable individual characteristic, change over time, or whether they are affected by various negative shocks, such as financial crises, trauma from conflict, natural disasters or a pandemic. Andersen et al. (2008b) stressed the conceptual distinction of temporal stability of preferences as either having the same preferences over time (unconditional stability) or as preferences being a stable function of states of nature and opportunities that change over time (conditional stability). They proclaim the latter as the right metric since conditional temporal stability can be very important for populations with heterogeneous preferences. We adopt this approach in our study as well.Footnote 1 Our study focuses on the negative shock of the COVID-19 pandemic (caused by SARS-CoV-2) which has been extremely disruptive to public health and the global economy.Footnote 2 We originally set up our study in 2017 with the purpose of administering, on an annual basis, a battery of incentivized and unincentivized measures to the student population of the Agricultural University of Athens, Greece. Each year since 2017, we administered the exact same battery of measures to students of the university that had voluntarily enrolled to participate in surveys/experiments. Students were invited to participate in an online survey via Qualtrics and were invited to participate in batches in order to achieve a good spread of responses across the one and a half months that the elicitation took place. The original purpose of the study was to elicit a battery of psychological, behavioral and economic measures and traits that could be later matched with data from laboratory experiments conducted at the premises of the university. The study was initiated in 2017 and was then repeated annually at similar dates every year. About halfway the 2020 wave, our study was re-designed due to the COVID-19 pandemic. The first case of COVID-19 was confirmed on February 26 in the country and on March 12 the first death occurred which coincided with the end of the originally planned study for 2020. We decided to re-launch our study and invite back all subjects that had participated in the 2019 and the early 2020 wave. The re-launch of the study on March 23, 2020 coincided with a general curfew imposed by the government banning all nonessential transport and movement across the country. We extended the duration of the study until a few weeks after the opening up of the economy and restart of business activity. Thus, for a considerable number of subjects (we have more than 1,000 responses over the three waves), we have their response before the pandemic, during the pandemic, and after successfully flattening the curve of cases/deaths. This successful flattening of the curve was followed by a second phase of the pandemic that started building up during the summer and received exploding dimensions in Autumn, leading to a second curfew that was imposed on November 7, 2020. In terms of cases and deaths, the first phase of the pandemic was minuscule when compared to the second phase (see Figure A1b in the Electronic Supplementary Material).Footnote 3 Our study adds to the emerging stream of studies that examine how risk or time preferences have evolved over the course of the pandemic. Angrisani et al. (2020) study the stability of risk preferences by comparing choices in the Bomb Risk Elicitation Task (Crosetto and Filippin 2013) from undergraduate students (60 subjects) as well as professional traders and portfolio managers (48 subjects) in London. They elicited subjects’ choices before the pandemic in 2019 as well as on a specific 13-day time window in April 2020 when London was in a lock-down. They find no change in risk preferences during the pandemic. Shachat et al. (2021) administered incentivized lottery choice tasks in the gain and loss domain to 396 students from Wuhan University, that were equally split among five waves (79 subjects/wave). Data for each wave were collected right after key events starting in January 2020 and up to March 2020 (when WHO declared it a global pandemic) and were also contrasted to pre-pandemic data from May 2019. They observed significant increases in risk tolerance (risk is measured based on the switching point in the lottery choice tasks) during the early stages of the COVID-19 crisis. Lohmann et al. (2020) administered incentivized lottery choice tasks, incentivized convex time budgets and hypothetical investment games (investments that offered higher returns as well as chances of losing the investment) to student subjects from Beijing universities. Subjects participated in online surveys in October 2019 (wave 1), December 2019 (wave 2) and March 2020 (wave 3). In the third wave, subjects had been geographically scattered in various areas of China and Lohmann et al. (2020) use the balanced sample of 539 subjects along with information about virus exposure in the geographical region of subjects’ area to examine potential effects on preferences. They find no significant changes in either risk or time preferences across waves. Harrison et al. (2020) elicited atemporal risk aversion, intertemporal risk aversion and time preferences from 598 students at Georgia State University, USA. Subjects were split over the course of the pandemic to 112, 130, 117, 99, 81 and 59 subjects in each of five waves, respectively, for the period from May to October 2020. They also have pre-pandemic data from 2019 for atemporal risk preferences for 232 subjects that were common to the COVID-19 experiment, as well as for time preferences for subjects drawn from the same population but they do not overlap with the COVID-19 experiment. Overall, they find that time preferences and intertemporal risk preferences are stable over the course of the pandemic. They also find that subjects become more atemporal risk averse during the pandemic under an RDU model but not under EUT. This points to the importance of having risk measures that allow one to model the structure of risk preferences which is impossible to achieve with survey measures or even with incentivized tasks that do not admit structural estimation of atemporal risk preferences. Gassmann et al. (2020) elicit preferences for risk, ambiguity as well as time preferences from students at Burgundy in France. They collected data from 596 subjects split in three waves: during lockdown (217 subjects), after the lockdown (190 subjects) and four months later (189 subjects). They also have responses from pre-pandemic data that were collected in 2016. Gassmann et al. (2020) report decrease in patience, less risk aversion, less ambiguity aversion and less prudence during the lockdown. One should note however, that incentives in their study had a very small chance (1%) of being realized, so results should be viewed within this context. Li et al. (2021﻿) administered several games and individual choice tasks to student subjects at Xiamen University in July 2019. This first wave collected responses from 633 subjects that were seated in a large auditorium and the exact same games and tasks were repeated with a subsample of 585 of them in July 2020. The authors analyze the number of safe choices subjects did in a Holt and Laury (2002) type of task and find that subjects made more safe choices in the midst of the pandemic; i.e., they were more risk averse. Table 1 summarizes the attributes of the various studies in this literature and compares these with our study.Footnote 4 One unique feature of our study is that in each wave, we collected responses for a period longer than one and a half months, allowing us to record multiple key events in the timeline of the pandemic. Given that most studies are focused on samples from China and only two of them measure time preferences, our study is the only one providing structural estimates for risk and time preferences for subjects from Europe where we provide salient incentives to subjects. Harrison et al. (2020) also estimate structural parameters but for USA students. Gassmann et al. (2020) provide insights for French students although Harrison et al. (2020) note that these choices should be viewed as hypothetical for practical purposes. Furthermore, our sample size is fairly large for an incentivized study, and using students as our workhorse is not at odds with what almost all other studies cited in Table 1 did. Snowberg and Yariv (2021﻿) compare a sample from almost the entire undergraduate population of the California Institute of Technology with a representative sample of the USA population and an Amazon Mechanical Turk sample. They find that the student population exhibits less noise as compared to the other samples and while large differences in behaviors are observed, these differences had limited impacts on comparative statics and correlations between behaviors. Using a student sample to study stability of preferences during the pandemic is a direct result of being fortunate enough to be doing experiments with this specific sample when the pandemic started. However, students remain a sample of convenience and one critique might be that student subjects were better able to shield themselves from the risks of exposure to COVID-19 by staying at home, attending classes remotely etc. Other groups of subjects, like front-line health professionals, workers in essential public services, retail and manufacturing sector workers were not able to stay at home or work remotely. Thus, we cannot claim that results from our study are generalizable to the population. Nevertheless, our study contributes to this stream of research that will allow other researchers to build a complete picture of how the pandemic impacted people of various occupations in different parts of the world. Our paper is structured as follows. In Sect. 2, we provide details about the design of the study and elicitation of risk and time preferences. In the same section, we also provide information about how the COVID-19 pandemic evolved in the country of our study (Greece) as well as present how we go about with our structural econometric methods to model key parameters from the theory of RTPs. Results and various robustness checks specifications are presented in Sect. 3. We conclude and discuss our results in the last section.",19
25.0,3.0,Experimental Economics,07 January 2022,https://link.springer.com/article/10.1007/s10683-021-09738-3,Subjective beliefs and economic preferences during the COVID-19 pandemic,June 2022,Glenn W. Harrison,Andre Hofmeyr,J. Todd Swarthout,Male,Male,Unknown,Male,"What happened to economic preferences and subjective beliefs during the COVID-19 pandemic in 2020? Figures 1 and 2 display the striking evolution of the pandemic in terms of daily and cumulative infections and deaths, respectively, in the United States.Footnote 1 While this unfolded, were risk and time preferences unconditionally stable, did they vary with the progress of the pandemic in some stable but conditional manner, or were they apparently disconnected? Did subjective beliefs about the prevalence and mortality of the pandemic track the actual progress of the pandemic, the projections of widely publicized epidemiological models, or neither? These are core questions we evaluated with a series of online experimental waves at monthly intervals between May and November 2020: the vertical gray lines in Figs. 1 and 2 show when our waves were conducted.Footnote 2 Subjects were sampled at random from the same population, with no subject asked to participate twice; 598 subjects participated. Daily infections and deaths in the United States Cumulative infections and deaths Investigating the risk and time attitudes of individuals during COVID-19, and their beliefs about COVID-19 prevalence and mortality, is essential for public policy interventions. For example, if risk and time attitudes differ by demographics, then interventions need to take those differences into account. The same holds for beliefs, where forecasts of the path of the pandemic, relative confidence, and demographic differences are crucial for targeting educational interventions. In turn, rigorous, incentivized elicitation is important for reliable estimates of these attitudes and beliefs. Section 2 reviews the preferences we elicited: atemporal risk preferences, time preferences, and intertemporal risk preferences. A novel feature of our experiment is a recognition that the volatility of the pandemic could impact atemporal and intertemporal risk preferences differently. Section 3 reviews the subjective beliefs we elicited: belief distributions, to allow measurement of bias and confidence, with respect to prevalence and mortality effects of the pandemic. Beliefs were elicited for a fixed one-month horizon, as well as for a fixed date of December 1, 2020 that implied declining horizons over the waves of our experiment. Another novel feature of our experiment is to track the extent to which the bias and confidence of beliefs changed as people lived through the striking evolution displayed in Fig. 1. Section 4 explains the experimental design. Section 5 presents the basic results, tracking the trends in preferences and beliefs over the course of the waves of the experiment. Section 6 discusses the main results in terms of the conditional stability of preferences and beliefs during the pandemic, as well as some comparisons with pre-pandemic preferences. Section 7 concludes with some general lessons. An Online Supplement at https://cear.gsu.edu/gwh/covid19/ documents additional results, elicitation interfaces, choice batteries, and instructions. We have two major findings. First, atemporal risk preferences during the COVID-19 pandemic appeared to change significantly compared to before the pandemic, but only if one identifies the underlying structure of those preferences to be able to infer a shift from “global probability optimism” to “local probability optimism and local probability pessimism.” The effect of that change in foreground risk attitude towards probabilities is to increase the atemporal risk premium, and is consistent with theoretical results of the effect of increased background risk on foreground risk attitudes. Second, subjective beliefs about the cumulative level of deaths evolved dramatically over the period between May and November 2020, a volatile one in terms of the background evolution of the pandemic. Specifically, we observed a marked increase between waves 3 and 4 in the confidence with which beliefs were held about cumulative COVID-19 deaths by December 1, 2020. We also found statistically significant evidence of biased beliefs, in the sense that the mean of the estimated belief distributions was below the actual number of COVID-19 deaths in waves 1, 2, and 6, but above the actual number of deaths in waves 3, 4, and 5. The degree of (statistically significant) bias varied across waves, reaching a high in wave 5 and a low in wave 6. But given the diffuse nature of beliefs, all estimated belief distributions include significant probability density around the actual number of COVID-19 deaths by December 1, 2020, allowing us to assess the “economic significance” or “policy significance” of these biases in subsequent work. Investigating the risk and time attitudes of individuals during COVID-19, and their beliefs about COVID-19 prevalence and mortality, is essential for public policy interventions. For example, if preferences or beliefs differ by demographics, then interventions need to take this heterogeneity into account. This clearly matters for evaluating the risk perceptions that led people to take certain actions or, in the case of vaccination, not take those actions. Similarly, in §6.A we ask whether the massive “background risk” of the pandemic has effects on “foreground risk preferences,” as claimed by some literature and a lot of casual empiricism. Knowledge about beliefs is likewise important for public policy interventions, because forecasts of the path of the pandemic, relative confidence, and demographic differences are crucial for targeting educational interventions. Much of §5.D is about whether the subjective beliefs of individuals between May and November track the likely state of the pandemic as of December 1, 2020: do individuals correctly foresee what is to come? A key insight there is that one must account for the understandable lack of confidence that our subjects had about those beliefs, particularly earlier in the year, rather than just track average beliefs. Of course, rigorous, incentivized elicitation is important for reliable estimates of these preferences and beliefs.",13
25.0,3.0,Experimental Economics,22 April 2022,https://link.springer.com/article/10.1007/s10683-022-09753-y,"COVID-19 and pro-sociality: How do donors respond to local pandemic severity, increased salience, and media coverage?",June 2022,Maja Adena,Julian Harke,,Female,Male,Unknown,Mix,,
25.0,3.0,Experimental Economics,06 January 2022,https://link.springer.com/article/10.1007/s10683-021-09741-8,Gender preference gaps and voting for redistribution,June 2022,Eva Ranehill,Roberto A. Weber,,Female,Male,Unknown,Mix,,
25.0,3.0,Experimental Economics,27 September 2021,https://link.springer.com/article/10.1007/s10683-021-09733-8,Redistribution and beliefs about the source of income inequality,June 2022,Vanessa Valero,,,Female,Unknown,Unknown,Female,"Income inequality has increased almost everywhere over the past few decades (World Inequality Report, 2018). Rising inequality is a concern, as it is negatively associated with long-term growth and creates social problems (e.g., Piketty, 2014; Stiglitz, 2012). Where inequality is a concern, redistributive policies constitute a widely utilized tool for obtaining greater economic equality. An important determinant of the support for these policies is perceptions of deservingness. People are more inclined to support redistribution when they believe that income is due to circumstances beyond individual control (such as luck) rather than within individual control (such as work). This relationship has been documented in both survey data (Fong, 2001; Alesina & Angeletos, 2005, Alesina & La Ferrara, 2005 and Alesina & Giuliano, 2011) and experimental data (e.g., Cappelen et al., 2007, 2013; Fong, 2007). While it is well known that beliefs about the causes underlying income inequality influence the demand for redistribution, much less is known about how people form these beliefs. This paper investigates whether beliefs about the causes of inequality are, in turn, influenced by the demand for redistribution. Specifically, I investigate whether adopting favorable beliefs about the causes of income inequality provides individuals with a means to justify supporting personally favorable redistributive policies. For example, do the rich exaggerate the extent to which hard work yields success to morally justify not supporting redistribution? Because such causes are often difficult to assess, people must form judgments regarding the role that factors such as hard work and luck played in determining individuals’ success or failure. This offers an opportunity to engage in self-serving belief manipulation, whereby individuals adopt the beliefs that are most convenient and provide the greatest opportunity for favorable level of redistribution. According to standard economic theory, beliefs arise independently of the individuals’ self-interest. However, considerable evidence indicates that beliefs are shaped by self-interest in other settings, such that individuals can both act egoistically while feeling moral about their behavior (see, Gino et al., 2016 for a review and the review of literature in Sect. 2). For this reason, I introduce the possibility that beliefs about the causes underlying economic inequality are susceptible to self-serving biases known to influence belief formation in other settings. Consistent with this possibility, Piketty (2020, page 2) argues that: “the discourse of meritocracy and entrepreneurship often seems to serve primarily as a way for the winners in today’s economy to justify any level of inequality whatsoever while peremptorily blaming the losers for lacking talent, virtue and diligence.”Footnote 1 Previous literature demonstrates that people form self-serving beliefs about the causes underlying success or failure—they systematically attribute their success to circumstances within their control and failure to circumstances beyond their control. This tendency to feel responsible for success but not for failure is usually interpreted by psychological studies as overconfidence, to protect or enhance self-esteem (e.g., Miller & Ross, 1975; Weiner, 1985). However, there is no evidence on whether people also bias their perceptions of the roles of luck and effort in determining outcomes in order to justify adopting personally favorable redistributive policies. Understanding how people form judgments about deservingness is critical for understanding the extent to which they will support redistribution or how they will react to its implementation. Identifying a potential self-serving bias in beliefs is also important as it can contribute to polarization in attitudes towards redistribution. On the one hand, self-serving biases can reinforce the perception of deservingness of the wealthy, who can subsequently exert their political power to change the rules to make inequality more persistent. On the other hand, if the poor overestimate the role played by luck in determining income, they will reject inequality even more and pursue redistributive policies to a larger extent. A political consensus about redistribution might then be more difficult to obtain, increasing political tensions and class conflicts. I start by studying correlational evidence between attitudes toward redistribution and beliefs about the source of income using the World Values Survey. The results indicate that people who are less inclined to support redistribution are also more likely to believe that income is due mainly to work. This result is consistent with earlier findings that beliefs about the sources of income influence support for redistribution, but also with the possibility that attitudes towards redistribution affect the beliefs about what determine economic success or failure. The observational data therefore cannot provide any causal evidence of a relationship between a desire for redistribution and beliefs about the sources of income, and thus cannot provide any evidence of belief distortion. In this paper I investigate this relationship using a laboratory experiment, where it is possible to both directly measure beliefs and manipulate the incentives potentially underlying their formation. To the best of my knowledge, this is the first study investigating the causal effect of preferences for redistribution on the beliefs about the determinants of income inequality. In the experiment, participants perform a work task after which they receive either a high or low level of initial income. Income is determined either according to participants’ relative work performance or luck. Participants are aware of these two possibilities, but do not know whether, in their case, income was actually determined by work performance or luck.Footnote 2 I then elicit beliefs, in an incentive-compatible manner, regarding the role of work versus luck. If individuals are overconfident or motivated to maintain a positive self-image, their beliefs will demonstrate a self-serving bias: high-income individuals will systematically believe that work performance was more likely to be responsible for income determination than luck, while low-income individuals will believe the opposite. The main focus of this experiment is on whether a desire to support favorable redistributive policies further impacts beliefs. The treatment variation consists of manipulating whether beliefs are measured before or after informing participants that they will subsequently decide how much to redistribute from participants with high income to those with low income. In the latter case, high-income participants know that they may be financially harmed by redistribution at the time they form their beliefs. This offers them an opportunity to convince themselves that income inequality is more likely due to work than luck in order to justify redistributing less afterwards. In the former case, they do not know that they will have the possibility to redistribute afterwards, and thus do not have such an incentive to bias their beliefs.Footnote 3 This provides a clean test of whether beliefs reflect a self-serving bias when they are formed with knowledge that redistribution will subsequently occur. I also measure preferences over redistribution, as a way of testing whether these are influenced by the elicited beliefs. Focusing on high-income participants’ redistribution decisions allows me to directly address the question of whether the winners in today’s economy use the discourse of deservingness to justify income inequality and redistributing less, as proposed by Piketty (2020).Footnote 4 My experiment yields several insights. First, the results show that high-income participants believe that they have greater responsibility for their income than low-income participants. Specifically, participants attribute income inequality to a larger extent to their work performance when they receive a high income, and to a larger extent to luck when they receive a low income. This is consistent with the formation of beliefs in a manner that promotes a positive self-image, based on the idea that one’s success is due to factors within personal control and overconfidence regarding one’s relative ability. I also find that beliefs about what causes inequality influence how much participants redistribute. Specifically, people who believe that outcomes were determined by luck favor more redistribution than those who believe it is due to work. Both of these results are consistent with previous findings that I describe in more detail in the next section. However, I find no evidence that preferences over redistribution further influence the self-serving nature of beliefs. Specifically, participants who receive a high income do not overestimate the importance of work to a greater extent when they know that they will subsequently have the opportunity to engage in redistribution than when they do not know of such an opportunity. Instead, my results demonstrate a precisely estimated null effect of knowledge of the upcoming redistribution possibility on beliefs. My results thus imply that participants form self-serving beliefs about how much control they have over their outcomes mainly due to overconfidence or a desire to maintain a positive self-image, which is present both when they do and do not have knowledge of subsequent redistribution. Knowing that redistribution decisions are forthcoming does not strengthen the relationship between personal circumstances and beliefs. The role of financial self-interest when forming beliefs about the source of income inequality seems to be minor in my experiment.Footnote 5 Therefore, any observed polarization of beliefs about the causes underlying inequality between the rich and poor within societies should be attributed to alternative motivations, including overconfidence and self-image concerns, and not to a desire to justify self-serving policies.Footnote 6 My paper proceeds as follows. The next section briefly reviews some related work. Section 3 reports the analysis of survey data on the relationship between attitudes toward redistribution and beliefs about the source of income. Section 4 describes the experimental design, and Sect. 5 the hypothesis. Section 6 presents the results while Sect. 7 concludes.",1
25.0,3.0,Experimental Economics,02 February 2022,https://link.springer.com/article/10.1007/s10683-021-09736-5,The net effect of advice on strategy-proof mechanisms: an experiment for the Vickrey auction,June 2022,Takehito Masuda,Ryo Mikami,Takuma Wakayama,Male,,Male,Mix,,
25.0,3.0,Experimental Economics,28 October 2021,https://link.springer.com/article/10.1007/s10683-021-09735-6,Strategic thinking in contests,June 2022,David Bruner,Caleb Cox,Brock Stoddard,Male,Male,Male,Male,"Contests are useful models for studying a wide variety of situations in which agents can expend costly effort to win a prize. Many examples can be found in political, economic, and social spheres, including running for political office, research to develop a new patent, applying for admission to college, getting a promotion within a firm, and dating a partner. Decades of experimental research on contests shows that, on average, people expend more effort than theory predicts. Indeed, it is not unusual that the total amount of resources spent on trying to win the contest exceeds the value of the prize. Accordingly, theoretical and experimental research has focused on trying to understand what drives ‘overbidding’ in contests. We design an experiment that allows us to examine the relative importance of various theories of overbidding in winner-take-all contests (Tullock, 1980) by analyzing the strategic thinking of contestants. A number of theories have been proposed to explain overbidding in Tullock contests, including (1) the non-monetary utility of winning (Schmitt et al., 2004; Parco et al., 2005), (2) confusion and mistakes (Anderson et al., 1998; Potters et al., 1998), (3) risk preferences (Hillman & Katz, 1984; Millner & Pratt, 1989, 1991), (4) loss aversion (Kahneman & Tversky, 1979; Shupp et al., 2013; Eisenkopf & Teyssier, 2013), (5) impulsiveness (Frederick, 2005; Sheremeta, 2018; Cox, 2017), (6) limited cognitive ability (Gill & Prowse, 2016), and (7) competitive maximization of relative payoffs (Herrmann & Orzen, 2008; Eisenkopf & Teyssier, 2013; Mago et al., 2016). Experimental tests show support for some of the proposed theories when a single factor is considered in isolation, which we summarize in the next section. Given the variety of plausible explanations for overbidding in contests, it is important to determine the extent to which each contributes to this behavior. For instance, contest designers can better account for the overbidding phenomenon through a complete understanding of its underlying motivation. It is not possible to make such a determination from the existing literature which separately analyzes the explanatory power of each factor while omitting other potentially important motivations. Our aim is to understand the thought process individuals use when placing bids to gain insight into contestants underlying motivation. To achieve this goal, our experiment uses a “two-headed” approach to decision making in contests. In our two-headed (2H) contest, two subjects are paired together to ultimately make a single decision on how much to bid for an 8-token prize in a lottery contest. There are two stages, and in the first stage each player simultaneously submits their suggested bid to their partner along with a text message they can use to convey their reasoning behind their suggested bid. After their partner’s suggested bid and message are received, each player submits their final bid, and one of the two final bids is randomly chosen as binding for the pair.Footnote 1 The experimental design ensures that subjects’ incentives in the 2H contest are perfectly aligned with their incentives in a standard contest between individuals, which we refer to henceforth as a one-headed (1H) contest for clarity. By analyzing the message a subject sends to his/her partner with the suggested bid, we get a glimpse into an individual’s thought process to better understand how players reason in lottery contests between individuals and what motivates their bidding decisions. This “two-headed” approach to studying communication has been used in other settings (e.g., Cooper & Kagel, 2005; Kagel & McGee, 2016; Cox & Stoddard, 2018; Cason & Mui, 2019). Our design is most similar to Burchardi and Penczynski (2014), who use a two-headed method to examine strategic thinking in guessing games. Our two-headed approach can be likened to other methods used to elicit beliefs, preferences, or strategies in experiments. The underlying premise is that what our subjects write in their messages to their partner in the 2H contest reflects their motives for bidding in a 1H contest (Burchardi & Penczynski, 2014; Penczynski, 2016). Arad and Penczynski (2018) use a similar approach to examine multidimensional reasoning in Colonel Blotto games and multi-object all-pay auctions. They argue that the two-headed approach “reveals the individual reasoning before the players’ exposure to their teammates’ ideas”.Footnote 2 To the best of our knowledge, our study is the first to use this method to understand bidding in the Tullock lottery contest. A recent study by Sheremeta (2018) was undertaken with the same objective of jointly testing the predictive ability of the set of proposed theoretical explanations of overbidding behavior in lottery contests. He adopts a different method compared to our two-headed approach, using a set of decision tasks to elicit the preferences, beliefs, and cognitive tendencies of contestants. He then jointly estimates the correlation between bidding behavior and these explanatory variables. He concludes that impulsiveness is the primary driver of overbidding in lottery contests.Footnote 3 To complement the analysis of message content in the two-headed approach, and to better compare our results to those of Sheremeta (2018), subjects also participate in a set of experimental tasks to elicit preferences, beliefs and cognitive tendencies. Subjects participate in, among other tasks, a zero-prize contest to elicit their non-monetary utility of winning and the cognitive reflection test (CRT) to elicit their impulsiveness. Accordingly, a methodological goal of the paper is to jointly compare the individual bidding rationale we glean from the 2H contest with the results of established elicitation tasks, which should provide a richer understanding of what drives overbidding in contests. To the best of our knowledge, ours is the first study to compare the predictive capacity of message content analysis to that of standard elicitation tasks, and to examine the extent to which these two approaches are substitutes. For the 2H contest to meaningfully inform our understanding of decision making in a 1H contest, a player’s suggested bid in the 2H contest must reflect what they would bid in a 1H contest. To directly make this comparison, our subjects participate in a standard two-player 1H contest for an 8-token prize before the 2H contest. This feature allows us to compare individual’s suggested bids in the 2H contest with their bids in the 1H contest, as well as explore the extent to which the message content in the 2H contest explains their bidding behavior. We find no significant difference between the average bid in the 1H contest and the average suggested bid in the 2H contest. Although suggesting a bid in the 2H contest that was equivalent to one’s bid in the 1H contest was the modal behavior of contestants, it was not universal and we discuss some limitations to the approach in detail in the experimental design section. That said, we find that contestants’ behavior is sufficiently consistent in these two contests that we are confident the content analysis of the messages can help improve our understanding of overbidding. When controlling for the types of messages subjects send to their partners in the 2H contest, we find messages that emphasize winning are the largest and most robust predictor of overbidding behavior. That is, those players that send messages focused on winning without considering the costs (e.g., “let’s bet it all to make sure we win”) tend to bid more in the 1H contest and make higher suggested bids in the 2H contest. This finding is robust to specifications that consider message content categories separately, as well as joint specifications that also include the elicited values. Importantly, our joint specifications include two measures of the utility of winning; one for a participant’s bid in a zero-prize contest (e.g., Sheremeta, 2010b, 2018; Price & Sheremeta, 2011), and one for messages coded as emphasizing winning. The result that the messages that emphasized winning remains significant when bids for the zero-prize contest are controlled for suggests that the two metrics are not close substitutes. One possible reason the two metrics differ is that subjects may have a non-monetary utility of winning that is not completely independent of the prize value. For example, a subject may experience utility from winning, but only for non-zero prizes. Our results suggest that analyzing communication provides a rich window into an individual’s thought process when making decisions, and may help explain behaviors in ways other elicited values cannot. The finding that the utility of winning is the most significant driver of overbidding in Tullock contests may initially appear at odds with the primary conclusion from Sheremeta (2018) that overbidding is primarily driven by impulsiveness. It is worth noting that our CRT scores count the number of impulsive wrong answers given by a subject, while Sheremeta (2018) counts the number of reflective correct answers and finds a negative correlation with bids.Footnote 4 We confirm this result when we control for message content; that is, the number of impulsive wrong answers in the CRT is positively correlated with bids in the 1H contest.Footnote 5 Still, the estimated effect of messages that emphasized winning is about three times larger than that of impulsiveness when evaluated at the mean. Our results on the importance of the utility of winning in bidding behavior have implications outside of the laboratory. Examples of potential overbidding in real-world contests include political campaign spending, employees working very long hours to compete for promotion, and students taking on extracurricular activities to gain acceptance into college.Footnote 6 If there exists a non-monetary utility of winning, then the standard model of bidding in contests is misspecified and in cases where contests are designed to induce agents to exert effort, such as promotions within a firm, taking this into account may allow the principle to extract the desired effort with a smaller prize. Also, political systems could impose rules that limit the amount of campaign spending to mitigate this effect.Footnote 7 Similar rules have been used in US defense research and development. For example, for the competition to develop the new Joint Strike Fighter, the government allocated Boeing and Lockheed Martin each $750 million to develop their prototypes. The government put this cap on spending to prevent one or both companies from bankrupting themselves in an effort to win the contract (Nicholls, 2000). In the next section, we provide a summary of the experimental literature on Tullock contests and reasons for overbidding. We then introduce the experimental design and predictions, followed by the results and conclusion.",1
25.0,3.0,Experimental Economics,19 November 2021,https://link.springer.com/article/10.1007/s10683-021-09737-4,The cost of a divided America: an experimental study into destructive behavior,June 2022,Wladislaw Mill,John Morgan,,Male,Male,Unknown,Male,"Political polarization is a widespread global phenomenon (Carothers and Donohue 2019), with consequences of great interest to the public and social scientists (Bail et al. 2018; Dixit and Weibull 2007). Particularly, the US is useful to study polarization and its consequences due to increasing polarization in current years (Boxell et al. 2020). Recent studies show that the polarization of the Democratic and Republican Parties is increasing and is higher than at any other time since the Civil War (Hare and Poole 2014). A current survey by the Pew Research center also demonstrates that partisanship division increased in recent years and not only on the party level (Pew Research Center 2017a). The Pew Research Center (2017a) shows that in 1994 more than one-third of self-identified Republicans were more liberal than the median Democrat—compared to just 5% in 2017. The same trend can be seen for Republicans. The study also illustrates that “Republicans and Democrats both say their friend networks are predominantly made up of people who are like-minded politically”(Pew Research Center 2017a) and most Democrats and Republicans state that they have few or no friends in the opposing party. Partisanship does not only shape views (Bartels 2002) and influence political behavior (Campbell et al. 1980; Green et al. 2002) but also affects the decision where to live (Bishop and Cushing 2008), what to buy (Nunberg 2007) and how to name the own children (Oliver et al. 2016). Political polarization also has negative consequences. For example, it has been shown that political polarization corrodes civility in public discourse (Sunstein 2018), erodes public faith in institutions (Fomina 2019) and nourishes discontent with political parties (Carothers and Donohue 2019). Yet, it is an open question whether and how political polarization affects individual decision-making, particularly destructive behavior. Our research examines whether and to what degree political polarization “spills over” into non-political, especially destructive behavior. To examine this question, we recruit from the population of American online workers without revealing anything about our interest in partisan spillovers. We then compare individual choices outside of the voting context but based on the (revealed) partisan preferences of the co-player. Our main question is whether partisanship produces spiteful decisions among the population at large outside of political contexts. We, therefore, measure attitudes of supporters of Donald Trump and of supporters of Hillary Clinton towards coinciding voters and opposing voters. More importantly, we measure whether participants are willing to harm their counterparts (by reducing their payoff). We do so by adopting a design with no trade-off between the own and the others’ payoffs; that is, a setting of pure spite. To the best of our knowledge, we are the first to study spite in a political context, and more importantly, we are the first to study the “spill over” of partisan preferences into destructive behavior. Previous research has already focused on whether partisanship affects non-political behavior. For example, Fowler and Kam (2007) demonstrates that partisanship influences decisions in dictator games. They show ingroup favoritism among co-partisans in the sense that dictators share more money with partisan recipients than non-partisans. In a similar vein, Carlin and Love (2013) investigate the effect of partisanship on trust behavior. They show that partisanship biases trust behavior in favor of co-partisans.Footnote 1 While there is a trade-off between the own payoff and the other’s payoff in dictator and trust games, we use a game where there is no monetary benefit for the decider in destroying the payoff of the other. Thus, we look at a systematically different situation. While e.g., Fowler and Kam (2007) can show that partisanship influences decisions in dictator games, it seems like this behavior is to benefit the ingroup without harming the out-group. Yet, the dictator game does not allow for a distinction between positive and negative discrimination. We use a design that identifies the dysfunctional aspect of behavior. Hence, the difference between a dictator game and our game is comparable to the difference between omitting help and directly harming others. Thus, unlike Fowler and Kam (2007) and Carlin and Love (2013) the main goal of our paper is to study dysfunctional behavior directly. Investigating the effect of partisanship on non-political behavior is also closely linked to the literature on the ingroup–outgroup bias.Footnote 2 This literature investigates whether group affiliation (either induced or existing) leads to discriminatory behavior towards ingroup and/or outgroup-members.Footnote 3 One of the main findings in this strand of literature is that people discriminate positively towards the ingroup (ingroup favoritism). However, hostile discrimination against the outgroup (outgroup-hate) is rarely found.Footnote 4 One exception using natural groups is Weisel and Böhm (2015), who show that findings of outgroup-hate are actually more a form of help avoidance and less a form of direct harm to the outgroup. Similarly, Abbink and Harris (2019) study intergroup conflict among artificial and naturally occurring groups and find outgroup discrimination only among the naturally occurring groups. Moreover, Heap and Zizzo (2009) and Zizzo (2010) show negative discrimination in artificially induced groups and in cases where the perception of common fate is missing. Further, Brewer (2017) reviews the literature on ingroup-love and outgroup-hate and points out that most discrimination is positive and not negative. Balliet et al. (2014) apply meta-analytic techniques on cooperative decision-making studies of intergroup conflict and find support for ingroup favoritism, but little evidence for outgroup-hate. Most recently, Dimant (2020) provides evidence that ingroup-love and outgroup-hate function differently and are not necessarily two sides of the same coin. Specifically, Dimant (2020) finds, in a setting of political polarization, that ingroup-love occurs in the perceptional domain, whereas outgroup-hate occurs in the behavioral domain. In the economic literature, Chen and Li (2009) show that ingroup-bias also transcends to economic decision-making.Footnote 5 More specifically they show that a match with an ingroup-member results in greater charity concerns and lower envy.Footnote 6 The most current advancements are made by Kranton and Sanders (2017) and Kranton et al. (2018). Kranton and Sanders (2017) show that people exhibit groupy and non-groupy preferences. More specifically, Kranton and Sanders (2017) use a minimal-group paradigm and political affiliation to investigate whether a person’s social preferences change depending on the matched partner type. They show that 40% of participants exhibit no bias, i.e., the participants do not change their social preferences, while 60% of participants switch from one social-preference classification to another (e.g., selfish to inequality-averse). Interestingly, the results in Kranton and Sanders (2017) indicate that ingroup-bias might result in a higher likelihood of being classified as dominance-seeking. Thus our work can be seen as complementary to Kranton and Sanders (2017) and provides even stronger support for the importance of partisanship on changes in social preferences. Thus, while the literature so far has mainly provided compelling evidence for ingroup-bias, there has been no sole focus on the destructive side of this bias. Different from the previous literature, we focus particularly on negative discrimination in the form of direct harm. While the question at hand has not been answered so far, there has been some research on antisocial behavior. A prevalent theme in economic decision-making is the study of prosocial behavior. It has been demonstrated that people have prosocial preferences,Footnote 7 behave as conditional cooperators,Footnote 8 and are willing to sacrifices their own payoff for the benefit of another person.Footnote 9 However, the dark side of economic decision-making is emerging recently. This literature shows that a significant fraction of people are not maximizing payoff or behaving prosocially but are punishing antisocially (Herrmann et al. 2008), are burning money of others,Footnote 10 behaving spitefullyFootnote 11 and even are willing to pay for the antisocial behavior.Footnote 12 This literature has also established that even children display destructive behavior (Fehr et al. 2013), that observability might induce antisocial behavior (Bolton et al. 2018), and that social proximity matters for social preferences (Dimant and Hyndman 2019). Further, Dimant (2019) shows that antisocial behavior is more contagious than prosocial behavior and that social proximity amplifies this effect. Similarly, Bauer et al. (2018) provide evidence of social contagion of ethnic hostility, and Zizzo and Fleming (2011) show that social pressure is correlated with money-burning behavior. Most relevant for our paper in the context of antisocial behavior are the studies by Dimant (2020) and Gangadharan et al. (2019). Dimant (2020) focuses on political polarization (in the form of hate and love for Donald J. Trump) and finds outgroup-hate in a take-or-give dictator game. Gangadharan et al. (2019) study the effect of social identity (via university residential arrangements) on antisocial behavior and reveal that sharing the same social identity mitigates antisocial behavior. We contribute to this literature by focusing explicitly on the effect of political polarization on antisocial behavior. In line with the previous literature on destructive behavior, we find that some participants are willing to destroy the resources of coinciding voters. More importantly, however, we show that, on the aggregate, participants behave significantly more spitefully towards their voting counterpart (i.e., opposing voters)—they increase the probability of destroying an opposing voter’s payoff by almost 15% relative to the probability of destroying a fellow voters’ payoff—which constitutes our main finding. Yet, it is important to understand whether this effect is found throughout or is rather driven by a subgroup. Of particular interest is the behavior of the rather distinct subgroups of Clinton and Trump voters. We find that Trump voters, who generally exhibit higher levels of spite, do not behave more spitefully towards opposing voters—there is no statistically significant effect of their opponents’ partisanship on their choices. By contrast, Clinton voters do behave increasingly more spitefully towards opposing voters—they increase the probability of destroying a Trump voters’ payoff by almost 34% relative to the probability of destroying a fellow Clinton voter’s payoff. Thus, our main result is driven primarily by the behavior of Clinton voters. We also see that participants significantly dislike their voting counterparts. The difference in behavior between Clinton and Trump voters seems to arise from an asymmetry in the intensity of the ingroup–outgroup bias. Clinton voters express strong antipathy toward Trump supporters, whereas Trump voters have a weaker aversion towards Clinton voters. Thus, we offer three main results:  Participants report having more negative attitudes towards opposing voters. Participants behave significantly more spitefully towards their voting counterparts, which is mostly driven by Clinton voters. Clinton and Trump voters differ substantially in their attitudes and behavior towards opposing voters. Altogether, we provide first evidence of spiteful behavior due to political affiliation, and we find first empirical support for the destructive effects of political division.",
25.0,3.0,Experimental Economics,10 January 2022,https://link.springer.com/article/10.1007/s10683-021-09739-2,Experimental elicitation of ambiguity attitude using the random incentive system,June 2022,Aurélien Baillon,Yoram Halevy,Chen Li,Male,Male,,Mix,,
25.0,3.0,Experimental Economics,08 January 2022,https://link.springer.com/article/10.1007/s10683-021-09740-9,Attention and salience in preference reversals,June 2022,Carlos Alós-Ferrer,Alexander Ritschel,,Male,Male,Unknown,Male,"Uncovering individual preferences is fundamental for applied economics, and it is essential to allow for policy recommendations and positive analysis. In practice, different methods are used, some relying on actual choices and others on the elicitation of monetary equivalents (see Bateman et al., 2002, for an overview of elicitation methods and how they are used in applied work). It is well-known, however, that different elicitation methods might contradict each other. This is illustrated by one of the most important anomalies in decision making under risk, namely the classical preference reversal phenomenon (Lichtenstein and Slovic, 1971; Grether and Plott, 1979; see Seidl, 2002 for a detailed survey). This phenomenon refers to an empirically-robust pattern of decisions under risk where decision makers provide monetary values for long-shot lotteries which are above those of more moderate ones but then choose the latter. Such a pattern is in contradiction with any value-based theory as Expected Utility Theory or (Cumulative) Prospect Theory. A large literature has demonstrated the robustness of the preference reversal phenomenon and postulated different, sometimes competing, explanations (e.g., Tversky et al., 1988, 1990; Tversky and Thaler, 1990; Casey, 1994; Fischer et al., 1999; Cubitt et al., 2004; Schmidt and Hey, 2004; Butler and Loomes, 2007). The phenomenon is typically demonstrated in paradigms involving pairs of lotteries consisting of a riskier option (Fig. 1; left-hand side) offering a larger prize (a long shot), called the $ -bet and a relatively safe one (a moderate lottery; Fig. 1, right-hand side), called the P-bet (for “probability”). Individual preferences over such pairs are then elicited both through a choice task involving pairwise choices and by comparing valuations obtained separately for each lottery in an evaluation task eliciting (typically) stated minimal selling prices (Willingness To Accept, WTA). The anomalous pattern is that decision makers often choose the P-bet in the choice task but explicitly value the $ -bet above the P-bet in the evaluation task. This yields a contradiction since a decision maker should be indifferent between a lottery and its certainty equivalent. In contrast, the opposite pattern of choices and evaluations occurs much more rarely. A recent, prominent argument on the origins of the classical preference reversal phenomenon arises from Salience Theory (Bordalo et al., 2012, 2013). Essentially, it states that decision makers’ attention is drawn to salient payoff comparisons, and, as a consequence, true probabilities are replaced by decision weights distorted in favor of the corresponding states of the world. For this argument, it is essential that salience is determined by the visible outcomes. In the choice task both lotteries are present, while in the evaluation task employed in classical preference reversal experiments only the target lottery is present. Bordalo et al. (2012) assume that during evaluation the decision maker compares the lottery to an alternative of not having it with probability one (“a natural way to model the elicitation of minimum selling prices,” Bordalo et al., 2012, p. 1271). This results in a distortion of the decision weights, which in turn leads to an overpricing of both lotteries. That overpricing is particularly strong for $-bets, because the high outcomes generate more salient states. Salience Theory suggests that reversals should be more frequent when lotteries are evaluated in isolation compared to when they are evaluated in the context of another lottery. Two lotteries. The left lottery yields a large monetary amount with relatively low probability ($-bet) while the right lottery yields a moderate monetary amount with relatively high probability (P-bet) We conducted two preregistered preference reversal experiments, an online experiment (\(N=256\)) and an eye-tracking experiment (\(N=64\)), with two different treatments (varying the “salience” of lotteries) to provide direct evidence on the role of attention and salience on the classical preference reversal phenomenon. In the online study, we test the hypothesis that preference reversals should be reduced when evaluation of a target lottery happens while an alternative lottery is present. In the eye-tracking study, we additionally examine gaze data and test the hypotheses that the alternative lottery attracts attention and that this attention influences both the evaluation of the target lottery and the resulting preference reversal rates. We also make use of the fact that in Salience Theory the states of the world correspond to comparisons between the outcomes of the two lotteries in a choice pair, and hence we can link them to measurable transitions (eye movements). This allows us to use the latter to test the hypothesis that more salient states attract more attention. The results fail to support Salience Theory. Neither the online nor the eye-tracking experiment revealed any effect of the presence or absence of an alternative lottery during evaluations on the preference reversal rates or on the monetary valuations of the target lotteries. Additionally, our eye-tracking experiment included the exact lottery pair used in Bordalo et al. (2012) to illustrate the predictions of Salience Theory, and we also failed to detect an effect for this pair. A more detailed regression analysis of the effect of fixations revealed that attention on the alternative lottery reduced both the monetary valuation of the target lottery and the likelihood of a preference reversal when the target lottery was a long shot. This is a confirmation of the implications of Salience Theory, and in particular the prediction that evaluations in the presence of an alternative lottery should reduce overpricing. However, this effect failed to translate into a measurable difference in preference reversal rates and also failed to be significant when the target was a moderate lottery instead of a long shot. Our analysis suggests two possible reasons for the failure of the effect on valuations to translate into a difference in reversal rates. On the one hand, the effects are modest. The alternative lottery receives a relatively small number of fixations, and the effect of a fixation on the valuation of the target lottery is of a small magnitude. On the other hand, there might be countervailing effects. Bordalo et al. (2012) argued that salience should indeed impact monetary valuations of both types of lotteries but that the impact on long shots should be proportionally larger. When the target lottery is a moderate one (P-bet) instead of a long shot, attention to the alternative lottery should also decrease the monetary valuation of the target, which in this case should increase the likelihood of preference reversals. Although this effect failed to reach significance in our data, linear combination tests taking both channels into account show that they cancel out and overall there is no effect of attention on the alternative lottery on standard reversal rates. Thus, our data suggests that the relative difference in overpricing across lottery types is too small to have a large impact on reversal rates. Conceptually, our studies contribute to the literature examining the consequences and implications of attention and salience for economic decisions, and in particular Salience Theory as put forward by Bordalo et al., (2012, 2013). Methodologically, we add to the small but growing literature directly examining eye-tracking measurements in economics and related fields. For example, Glöckner and Herbold (2011), Ludwig et al. (2020), and Alós-Ferrer et al. (2021b) use eye-tracking to study decisions under risk, while Reutskaja et al. (2011) concentrates on consumer choice. Alós-Ferrer et al. (2021c) relied on pupil dilation to study effort allocation in a belief-updating task. Further, a growing number of contributions uses eye-tracking to examine decision making in games (Knoepfle et al., 2009; Polonio et al., 2015; Devetag et al., 2016; Polonio and Coricelli 2019; Fiedler and Hillenbrand 2020; Marchiori et al., 2021; Zonca et al., 2019). This category also includes Hausfeld et al. (2021), who gave their subjects eye-tracking information about another player that they competed against or cooperated with to analyze how subjects used that information, and Avoyan et al. (2021), who used eye-tracking to analyze how participants plan to allocate attention in a matrix-game setting introduced in Avoyan and Schotter (2020). This paper is structured as follows. Section 2 briefly reviews Salience Theory. Section 3 presents the design and results of the online experiment. Section 4 presents the design and results of the eye-tracking experiment. Section 5 presents the analysis of the lottery pair used in Bordalo et al. (2012). Section 6 concludes. The online supplementary materials discuss additional exploratory analyses, present detailed analyses of transitions, fixation durations, and individual heterogeneity. They also contain a transcript of the original experimental instructions.",3
25.0,3.0,Experimental Economics,28 February 2022,https://link.springer.com/article/10.1007/s10683-021-09742-7,Feature-weighted categorized play across symmetric games,June 2022,Marco LiCalzi,Roland Mühlenbernd,,Male,Male,Unknown,Male,"Experimental game theory studies patterns of behavior for agents playing games. In particular, the dynamics and evolution of choices from players who face a stream of one-shot games is usually viewed as an instance of learning: agents refine their play as they gather more experience. There are two major approaches to learning in games: reinforcement-based and belief-based (Feltovich, 2000). Reinforcement learning presumes that over time players tend to shift their play towards actions that have earned higher payoffs in the past (Roth & Erev, 1995; Erev & Roth, 1998). Belief-based models require a player to form beliefs about her opponents’ play and choose actions with higher expected payoffs (Boylan & El-Gamal, 1993). More recently, the two approaches have been blended by experience-weighted attraction learning and its later variants (Camerer & Ho, 1999; Ho et al., 2007). A dominant research theme has been to develop tractable models that describe (and predict) aggregate and individual play over time. This program has enjoyed a qualified success. There are simple learning models that can track “some movements in choice over time in specific game and choice contexts” (Camerer & Ho, 1999). While much work has gone into the calibration of parametric models fitting single experiments, no general theory has yet emerged. A body of research fixates on the special case where agents face a stream of identical games played against different opponents. The comment by Crawford (2002, p. 11) still rings true: “almost all analyses of learning, theoretical or experimental, have concerned learning to play a single, fixed game, with past plays perfectly analogous to present ones, and past behavior taken to be directly representative of likely present behavior.” This paper puts forth a novel approach, with the goal of contributing to the study of learning over similar games played against different opponents. The key idea is that agents (learn to) categorize games and choose the same action for all the games placed in the same category (Mengel, 2012). Categories are based on the individual experience: two agents may have different categorizations, and these may change over time. We demonstrate our approach with a simple model that simultaneously describes (and predicts) aggregate play over a large set of independent experiments from the literature, each based on a stream of identical \(2 \times 2\) symmetric games. Our main theoretical concern is robustness and generality, rather than ex post calibration over a single experiment: therefore, we apply the same parameter-free version of our model over all these independent experiments simultaneously. We test our results against suitable variants of the three major learning algorithms: experience weighted attraction, fictitious play, and reinforcement learning. The simulations based on our model match the experimental data on (overall and round-by-round) aggregate play and on the evolution of empirical behavior remarkably better than the competitors. We conclude that our simple model outperforms the major learning algorithms across a large body of evidence over identical games. Having validated our approach over identical games, we test it against the (so far) scant experimental evidence over similar games. Our model describes the empirical evidence better than the strongest competitor. Moreover, it is the only one that can reproduce the rapid adjustments in play from an individual agent who reacts to different payoffs in a stream of similar games. Related Literature. The literature on strategic interactions influenced by similarities is quite diverse, but there are a few unifying themes. The first theme is that players have a coarse understanding of their environment, usually because of bounded rationality or cognitive constraints: they cannot discriminate some elements and bundle them into categories or analogy classes. The Analogy-based Expectation Equilibrium (Jehiel, 2005) and many of its variants (Jehiel, 2020) assume that players have a coarse understanding of their opponents’ moves, and thus rely on ""averages"" for each analogy class. Mullainathan et al. (2008) and Hagenbach and Koessler (2020) study how a coarse understanding of the message space affects persuasion and cheap talk, respectively. Gibbons et al. (2021) analyze how a shared coarse understanding may facilitate or impede cooperation. Grimm and Mengel (2012) provides experimental evidence on how complexity and feedback affect whether agents’s coarse understanding lead them to bundle their beliefs about opponents’ moves or their own choice of actions across similar games. A second theme is similarity-based reasoning, used to draw inferences and decide a course of action across different situations. The seminal contribution is case-based decision theory (Gilboa & Schmeidler, 1995, 2001). Steiner and Stewart (2008) demonstrates how similarity-based reasoning can spread “contagiously” a mode of play across increasingly different global games. Similarity-based reasoning is also invoked to explain experimental evidence on the emergence of “conventions” (Rankin et al., 2000; Van Huyck and Stahl 2018). These contributions are consistent with agents learning within the bounds of their coarse understanding or their similarity judgments, but do not tackle the issue of learning categories or similarities for games. This goal characterizes the third theme. Mengel (2012) applies evolutionary learning to analyze how agents’ categorizations over a set of games change and possibly stabilize. Heller and Winter (2016) recast such categorizations as part of the agents’ strategic options. These works assume a quite small set of competing categorizations. Recent works expand agents’ search over wider sets of categorizations: LiCalzi and Mühlenbernd (2019) study the evolution of binary interval partitions over a space of games that can be mapped to a one-dimensional interval; Daskalova and Vriend (2021) model the categorization of the player’s own actions in a one-shot game under reinforcement learning, and test its fit with the experimental evidence. Our paper advances a simple mechanism to generate (and adapt) individual probability distributions over alternative categorizations, validated by a competitive test over independent experimental evidence. The fourth theme collects a few ambitious attempts to consider learning to reason across similar games. Samuelson (2001) uses finite automata to model how agents may select among their stock of models what works better across different games. Lensberg and Schenk-Hoppé (2021) recast this problem as the search for a solution concept, using genetic programming to sort out individual selections and develop an aggregate solution concept. Haruvy and Stahl (2012) provide some experimental evidence that more sophisticated players tend to move from non-belief-based to belief-based rules of play.",1
25.0,4.0,Experimental Economics,08 October 2021,https://link.springer.com/article/10.1007/s10683-021-09734-7,The determinants of multilateral bargaining: a comprehensive analysis of Baron and Ferejohn majoritarian bargaining experiments,September 2022,Andrzej Baranski,Rebecca Morton,,Male,Female,Unknown,Mix,,
25.0,4.0,Experimental Economics,24 May 2022,https://link.springer.com/article/10.1007/s10683-022-09745-y,Decomposing coordination failure in stag hunt games,September 2022,Ryan Kendall,,,,Unknown,Unknown,Mix,,
25.0,4.0,Experimental Economics,13 March 2022,https://link.springer.com/article/10.1007/s10683-022-09747-w,Beliefs and (in)stability in normal-form games,September 2022,Kyle Hyndman,Antoine Terracol,Jonathan Vaksmann,,Male,Male,Mix,,
25.0,4.0,Experimental Economics,15 April 2022,https://link.springer.com/article/10.1007/s10683-022-09750-1,The Influence of Indirect Democracy and Leadership Choice on Cooperation,September 2022,Fanny E. Schories,,,Female,Unknown,Unknown,Female,"Does the way a law is implemented influence its effectiveness? An experiment is used to quantify the behavioral difference between externally imposed institutions and those that are implemented through a democratic procedure. This behavioral difference has been coined democracy premium: When holding information and group composition constant, there is an increased willingness to cooperate after an institution was introduced through a democratic procedure. That a central authority can effectively improve cooperation is a stylized fact in experimental economics. However, most studies take these institutions as exogenously given. Recently, many experiments studied endogenous institutions which do not “fall from heaven” but are introduced by the affected parties themselves. The process that leads to an institutional setting might well influence to what extent it can fulfill its societal purpose. Should a law that was effective in one instance also be assigned in other situations? Consider the progressing European integration as an example. Is a reform introduced in a member state as effective when it is de facto prescribed from an external authority such as the “Troika” as if it was introduced autonomously by the elected national government? Previous studies found inconclusive evidence regarding the existence of a democracy premium in direct democracies (Dal Bó et al., 2010; Sutter et al., 2010; Vollan et al., 2017; Gallier, 2020). This study focuses on representation as another aspect of democratic decision-making. Indirect democratic procedures are commonly used: nations, firms, and clubs typically delegate at least parts of their decision-making processes to representatives. The paper contributes to two central questions: Is there a democracy premium when a representative chooses the relevant institution? And does the democracy premium depend on how the representative came into office? The two experimental treatments address the second question. The treatment variable is the way the group leader is appointed: via election or lottery. The treatments are therefore called indirect democracy (ID) and random dictator (RD). Indirect–or representative–democracy is the most common form of democracy today (Alizada et al., 2021). Using lots to assign political roles, also known as sortition, dates back to Athenian democracy, where it was promoted by Aristotle to achieve the democratic ideals of equality and fairness more effectively than elections (Barnes (1984)). The experiment has three stages. The first stage of the experiment consists of a prisoners’ dilemma played in small groups. In the second stage, subjects form preferences about a payoff modification for their group in the final stage. The modification transforms the prisoners’ dilemma into a coordination game, which makes both defection as well as cooperation incentive-compatible. In the indirect democracy treatment, subjects elect a group representative, whereas the group leader is determined by chance in the random dictator treatment. The leader’s preference about changing the payoffs becomes binding for the group but is only considered in 50 percent of the cases. If it is not considered, one of the two games is randomly assigned to each group for the third stage. This design feature–first introduced by Dal Bó et al. (2010) (hereafter: DFP) – can control for information and selection effects and thus allows a clean estimate of the democracy premium. Cooperation rates before and after the vote are analyzed conditional on individual policy preferences and the outcome of the random intervention to estimate the democracy premium for both treatments separately. I find that the payoff modification significantly increases cooperation and even more so when introduced by an elected representative. Moreover, the difference between the treatments is striking. There is a substantial effect of endogenous choice in the representative democracy: 78 percent of the increase in cooperation can neither be attributed to the payoff change itself nor to differences in group composition and thus remains as the democracy premium. In contrast to DFP (2010) especially those who initially do not prefer the coordination game respond strongly to a democratic payoff modification with an increased willingness to cooperate. Conversely, the randomly appointed leader in the second treatment does not cause an increase in cooperation beyond the exogenous payoff modification. The effect of the random dictator is negative because cooperation is further decreased in the case of endogenous non-modification. The results have important implications for policy-making but also for the methods used in experimental economics. The remainder of the paper is structured as follows. Section 2 reviews the related literature focusing on the effects of endogenous formal institutions in economic laboratory experiments. Section 3 presents the design of the experiment, including testable hypotheses. The analysis and results are presented in Sect. 4. Finally, Sect. 5 discusses potential explanatory approaches from economic theory and concludes.",1
25.0,4.0,Experimental Economics,18 March 2022,https://link.springer.com/article/10.1007/s10683-022-09749-8,Present bias for monetary and dietary rewards,September 2022,Stephen L. Cheung,Agnieszka Tymula,Xueting Wang,Male,Female,Unknown,Mix,,
25.0,4.0,Experimental Economics,26 December 2021,https://link.springer.com/article/10.1007/s10683-021-09743-6,Higher-order learning,September 2022,Piotr Evdokimov,Umberto Garfagnini,,Male,Male,Unknown,Male,"Beliefs about the beliefs of others arise naturally in strategic settings and feature prominently in coordination problems (Keynes, 1937; Morris & Shin, 2002), global games (Carlsson & Van Damme, 1993), and financial markets (Morris & Shin, 1998). While a number of papers have investigated higher-order beliefs both theoretically and experimentally, the question of how accurate they are and and how they are updated in response to new information has received little empirical attention. This question has crucial implications for equilibrium concepts under incomplete information like Bayesian Nash equilibrium (Harsanyi, 1967; Mertens & Zamir, 1985) and cursed equilibrium (Eyster & Rabin, 2005), which assume that players know the updating rules used by others, as well as dynamic coordination problems. Cripps et al., (2008), for instance, show that coordination may be impossible in absence of common learning, which they define as the event when the true state becomes approximate common knowledge. In this paper, we use an experiment to provide a first step toward exploring how accurate higher-order beliefs are, how their accuracy changes as more information about the fundamentals is received, and whether common learning is possible. The experiment proceeds as follows. In the beginning of the session, subjects are randomly matched into groups of three. Before any decision is made, an unknown state of the world is drawn at random and held fixed for 30 periods. It is common knowledge that the state is fixed for 30 periods, the same for every player in the group, and equally likely to take on one of two values. In each period, each group member observes a new signal about the state of the world, as in standard belief updating tasks (e.g. Holt & Smith, 2009). In the public treatment, all players in the same team observe the same signal in each period of the game. In the private treatment, Player 1, Player 2, and Player 3 observe conditionally independent signals from the same distribution, and each player can only observe her own signal. After receiving her signal, Player 1 is incentivized to choose an action as close as possible to the realized state, Player 2 is incentivized to choose an action as close as possible to Player 1’s action, and Player 3 is incentivized to choose an action as close as possible to Player 2’s action. Subjects receive no feedback about the behavior of their matched partners for the duration of the experiment. After 30 periods, the state is revealed. The experiment is designed so that the action of Player 1 corresponds to her elicited first-order belief about the state, the action of Player 2 corresponds to her second-order belief about the belief of Player 1, and the action of Player 3 corresponds to her third-order belief about the belief of Player 2. We use these treatments to address the basic question of whether players engage in higher-order reasoning, test predictions about the accuracy of higher-order beliefs, and study how their accuracy changes as more information is received. Our first prediction is that higher-order beliefs are closer to the prior when information is private, regardless of the number of received signals. This is because when information is private, Player k for \(k>1\) must account for the fact that her information may be different from that of Player \(k-1\)  by forming a belief closer to the prior, relative to what it would be if information was public. Because the only difference in the tasks of the higher-order players across the public and private treatments is information about signals received by other players, results in line with this prediction allow us to conclude that Players k for \(k>1\) engage in higher-order reasoning. Second, higher-order beliefs are predicted to be more accurate on average with public than private signals. In the Bayesian benchmark, beliefs of Player k and Player \(k-1\) are identical in the public treatment, but not in the private treatment, where the fact that other players observe potentially different sequences of signals must be accounted for. Even if subjects are non-Bayesian, higher-order beliefs are more difficult to update in the private treatment because the potential difference in histories must be taken into account. In addition to testing predictions about how higher-order beliefs are updated, we study higher-order learning, i.e., the evolution of the accuracy of higher-order beliefs as more signals are received. As we elaborate below, failure to correctly predict the beliefs of others might be attenuated in the long term. In the Bayesian benchmark, this is true in the private treatment, where higher-order beliefs are inaccurate in early periods but become more accurate as both higher- and lower-order beliefs converge to the truth. Even if subjects are non-Bayesian and follow heterogeneous updating processes, higher-order learning might be observed, depending on how much heterogeneity is present in the data and how good subjects are at forecasting the beliefs of others. We use the experiment to address this question empirically. We find that the first prediction is in line with the data, while the second is not. I.e., subjects account for the public vs. private nature of information, but higher-order beliefs are not more accurate when the information is public than when it is private. Moreover, the accuracy of higher-order beliefs does not improve over time in either treatment, even as a large number of signals is received; i.e., higher-order learning fails. We argue that the observed failure of higher-order learning in both treatments is rooted in failures of Bayesian thinking (e.g., base rate neglect), heterogeneity in information processing, and subjects’ failure to take this heterogeneity into account. Base-rate neglect has theoretically been shown to bound beliefs away from the correct state of the world (Benjamin et al., 2019). As first-order beliefs fail to converge despite accumulating evidence, heterogeneity in updating rules implies that different subjects will have very different long-run beliefs. This, in turn, implies that higher- and lower-order beliefs fail to converge. To study what assumptions subjects make about the updating rules of others, we run additional within-subjects treatments where each subject reports a belief both in the role of Player 1 and Player 2. We find that the vast majority of subjects show a median difference of zero between their first- and second-order beliefs. We also use a counterfactual exercise to show that even if subjects reported the optimal beliefs given the updating rules used by other subjects in the experiment, higher- and lower-order beliefs would fail to converge. In other words, even a player with knowledge about the distribution of subjects’ updating types in the experiment would fail to show higher-order learning. Finally, we address the question of whether the observed failure of higher-order learning can be mitigated by additional information. To this end, we use the counterfactual exercise to simulate higher-order beliefs in an experiment where 300 as opposed to 30 signals are observed. We find that higher-order beliefs initially diverge and eventually plateau. Thus, we find little benefit of receiving more signals in the counterfactual exercise. To test this prediction, we collect data from an additional online experiment in which subjects receive 10 signals in every period, for a total of 300 signals in period 30. We find that higher-order beliefs in this treatment are not significantly different between the first 15 and the last 15 periods. These results are in line with the predictions of the counterfactual exercise. Overall, the results suggest that higher-order learning is difficult to achieve, which in turn raises questions about the feasibility of common learning. This paper complements several strands of literature. Cripps et al., (2008) define common learning as the event where the true state becomes approximate common knowledge and provide conditions on the signal distributions of each player which guarantee that common learning is feasible. While other theoretical papers have followed the research agenda of Cripps et al., (2008) (e.g. Wiseman, 2012; Acemoglu et al., 2016), little is known about whether common learning occurs in practice. Because elicitation of an infinite hierarchy of beliefs poses an obstacle to any laboratory study, we restrict our attention to higher-order learning (i.e., increasing accuracy of higher-order beliefs). Higher-order learning is a necessary if not sufficient condition for common learning, so that a failure of higher-order learning in the laboratory would cast doubt on the feasibility of common learning, as well. A number of papers have investigated how subjects update their beliefs in response to new information (e.g., Grether 1980, 1992; Holt & Smith, 2009) and found violations of Bayes’ rule as well as substantial heterogeneity in updating rules. More recent contributions also find that decision makers process information about personal characteristics such as IQ asymmetrically, overweighting good news and underweighting bad news (e.g., Eil & Rao, 2011; Mobius et al., 2014; Coutts, 2018). In contrast to prior individual learning experiments, we investigate the updating of higher-order beliefs, expanding the existing literature to strategic settings. We also contribute to the growing experimental literature on higher-order reasoning. Nagel (1995) introduces level-k thinking in the context of guessing games and finds that few subjects exceed two levels of reasoning.Footnote 1
 Huck & Weizsäcker (2002) elicit subjects’ beliefs about the lottery choices of other subjects. Although subjects are able to correctly predict the choice frequencies of other subjects on average, they find a significant and systematic bias toward a uniform prior. Kübler & Weizsäcker (2004) study how subjects process information generated through their predecessors’ choices in a social learning framework. Using an error-rate model which allows to estimate how subjects reason about other subjects’ behavior, they find that subjects underestimate the rationality of their immediate predecessors (similar to what found by Weizsäcker, 2003) and that the average subject’s reasoning does not exceed two steps.Footnote 2 The difference in how subjects treat private and public information has been investigated in the experimental global games literature. Several papers find little differences in behavior across the two types of information in one-shot coordination games (Heinemann et al., 2004; Van Huyck et al., 2018), contrary to theoretical predictions. Cornand & Heinemann (2014) provide subjects with both public and private information about the underlying state of the world in a game with strategic complementarities. They argue that systematic mistakes in how subjects form higher-order beliefs can partly explain their observed deviations from equilibrium behavior. Our paper expands on this point by investigating how subjects update their higher-order beliefs in response to new information. We argue that beliefs about the beliefs of others may be persistently incorrect. Providing more information about the environment, e.g., the fundamentals of the economy in a global game setting (Angeletos et al., 2007), may prove inconsequential for the accuracy of higher-order beliefs, resulting in persistent mistakes in choice.",1
25.0,4.0,Experimental Economics,31 January 2022,https://link.springer.com/article/10.1007/s10683-022-09744-z,Correction to: Higher-order learning,September 2022,Piotr Evdokimov,Umberto Garfagnini,,Male,Male,Unknown,Male,"This erratum is published as at the bottom of page 29, Lemma B.2 (appendix) was missing. Original article has been corrected.",
25.0,4.0,Experimental Economics,26 March 2022,https://link.springer.com/article/10.1007/s10683-022-09748-9,Framing effects on risk-taking behavior: evidence from a field experiment in multiple-choice tests,September 2022,Pau Balart,Lara Ezquerra,Iñigo Hernandez-Arenaz,Male,Female,Unknown,Mix,,
25.0,4.0,Experimental Economics,12 July 2022,https://link.springer.com/article/10.1007/s10683-022-09763-w,Correction to: Framing effects on risk-taking behavior: evidence from a field experiment in multiple choice tests,September 2022,Pau Balart,Lara Ezquerra,Iñigo Hernandez-Arenaz,Male,Female,Unknown,Mix,,
25.0,5.0,Experimental Economics,08 August 2022,https://link.springer.com/article/10.1007/s10683-022-09765-8,"Willingness to compete, gender and career choices along the whole ability distribution",November 2022,Thomas Buser,Noemi Peter,Stefan C. Wolter,Male,Female,Male,Mix,,
25.0,5.0,Experimental Economics,13 September 2022,https://link.springer.com/article/10.1007/s10683-022-09769-4,Teams and individuals in standard auction formats: decisions and emotions,November 2022,Maria Karmeliuk,Martin G. Kocher,Georg Schmidt,Female,Male,Male,Mix,,
25.0,5.0,Experimental Economics,23 March 2022,https://link.springer.com/article/10.1007/s10683-022-09752-z,Revealing good deeds: disclosure of social responsibility in competitive markets,November 2022,Sören Harrs,Bettina Rockenbach,Lukas M. Wenner,Male,Female,Male,Mix,,
25.0,5.0,Experimental Economics,24 August 2022,https://link.springer.com/article/10.1007/s10683-022-09768-5,Risk preferences and contract choices,November 2022,Jean-Louis Bago,Bruce Shearer,,Unknown,Male,Unknown,Male,"Risk and risk preferences play a central role in economic models of agency and incentive contracts. Providing incentives implies risk sharing, imposing costs on risk-averse workers and raising contracting costs in risky settings (Prendergast, 1999). The empirical importance of risk in determining contracts is less clear (Prendergast, 2000). Empirical applications of agency models suffer from the fact that risk preferences are unobserved, complicating the testing of theoretical comparative static results. This has led some researchers to use proxies for risk preferences in contractual-choice regressions, with mixed results (Ackerberg and Botticini, 2002; Allen and Lueck, 1999). An alternative approach is to use experimental measures of risk preferences (Holt and Laury, 2002) and incorporate them into contractual choice models. Bellemare and Shearer (2013) take this approach, combining risk-preference revealing experiments with payroll data to identify and estimate a principal-agent model. The validity of such an approach depends on the ability of the experiments to identify risk preferences that are relevant to economic behaviour in the real economy. To date, relatively little is known about this issue. Bellemare and Shearer (2013) used their risk-preference estimates to evaluate the potential gains to matching workers to risk settings within the firm, taking as given the fact that the estimated risk preferences were relevant to worker contractual choices. Important work by Cadsby et al. (2007) and Dohmen and Falk (2011) provides supporting evidence using laboratory experiments.Footnote 1 Both papers reported results showing workers sort across different incentive contracts based on risk preferences that were measured using Holt-Laury (HL) methods.Footnote 2 Yet both papers also questioned whether their laboratory evidence would generalize to the real economy.Footnote 3 Similar concerns arise in other work studying the generalizability of laboratory evidence to the real economy; see, for example, Gneezy and List (2006), Levitt and List (2007a), Levitt and List (2007b), Falk and Heckman (2009) and Camerer (2015). We address this issue, using experimental methods in the field. We take agency theory as given and test whether or not experimentally measured risk preferences can predict actual contract choices in the real labour market. To do so, we exploit multiple field experiments, conducted within a real firm. In a first set of experiments, we measure workers’ risk preferences using HL methods. We repeat these experiments twice on each worker: once using a low-stakes (LS) lottery and once using high-stakes (HS) lottery. We then use the estimated risk preferences to predict worker choices between piece-rate and fixed-wage contracts during a follow-up experiment. Importantly, the workers’ experimental choices had real economic consequences: workers were paid the selected contract for a period of two working days. Our experiments took place within a tree-planting firm, located in British Columbia, Canada. The workers of this firm knowingly participated in the experiments, which can be described as framed field experiments (Harrison and List, 2004). Workers in this firm are hired to plant trees on recently logged blocks of land and are typically paid piece rates—daily earnings are the product of the piece rate and the number of trees planted. Since the number of trees a worker can plant depends on elements that are beyond his/her control (such as weather, the slope and the hardness of the ground) workers are exposed to daily income risk under a piece-rate contract. Fixed-wage contracts eliminate that risk to workers. We exploit our experimental data, matched to payroll data, to test the ability of experimental measures of risk preferences to predict contractual choices and to test for sorting in the real labour market. We find that the aggregate distribution of measured risk preferences among participants is stable across the LS and the HS lotteries. However, measures of individual risk attitudes change between the LS lottery and the HS lottery. We explore the ability of the risk preferences from each lottery to predict workers’ choices between fixed-wage and piece-rate contracts. We find that the risk preferences, as measured from the HS lottery effectively predict contract choices, while those from the LS lottery are statistically irrelevant. We interpret this as reflecting that workers took the LS lottery less seriously, creating measurement error. We exploit the instrumental variable methods of Gillen et al. (2019) to measure the effect of risk preferences on contractual choice in the presence measurement error. The results show that the risk preferences effectively predict the contract choice decisions: risk averse workers are more likely to select the fixed-wage contract. Worker ability, as measured by the average logarithm of each worker’s daily earnings in the firm’s payroll data, is also an effective predictor of contract choice, significantly increasing the propensity to select the piece-rate contract. Our work contributes to the literature investigating the relationship between risk and contracts. It presents evidence that risk preferences are an important determinant of contract choice in the real labour market, reinforcing theoretical models based on the risk-incentives tradeoff; eg., Holmstrom (1979); Stiglitz (1975). It also supports the work of Ackerberg and Botticini (2002) and Bellemare and Shearer (2010, 2013), providing direct evidence of heterogeneity in risk preferences among workers and that those preferences are relevant to contract choices.Footnote 4 Our work also contributes to the literature on contracts and sorting over ability. Previous work has been either in the laboratory (eg., (Cadsby, Song, and Tapon, 2007; Dohmen, Falk, Huffman, Sunde, Schupp, and Wagner, 2011)) or used non-experimental evidence from the field. For example, Lazear (1986) analysed this issue when workers are faced with piece-rate and fixed-wage contracts. He showed that high-ability workers would be attracted to piece-rate contracts when workers are risk neutral. His subsequent empirical work (Lazear, 2000) verified these results. Theoretical predictions for sorting over ability are less clear cut when workers are heterogeneous over risk preferences. Our results provide direct evidence on these effects using experimental evidence in the real labour market. Finally, our work contains important results for the literature investigating the relevance of experimental measures of risk preferences in explaining real world behaviour in risky situations. Path breaking work in this area was conducted by Binswanger (1980), who used gambling experiments to measure farmers’ risk preferences in India which he then related to economic outcomes, such as wealth. More recently, some authors have exploited survey responses on risky behaviour to investigate these links. Lusk and Coble (2005) found that risk preferences estimated from HL experiments predicted participants’ stated preferences for consuming risky genetically modified food. (Dohmen, Falk, Huffman, Sunde, Schupp, and Wagner, 2011) also used survey methods to generate information on risky behaviours and related the responses to different measures of risk preferences. Another branch of this literature considers actual risky behaviour in the real world. Petrolia et al. (2013) report that estimated risk preferences are correlated with decisions to purchase flood insurance in the U.S.A. Jin and Wang (2020) found that HL measures of risk preferences were correlated with decisions to purchase crop insurance in rural China. Similarly, Belzil and Sidibé (2016), used estimated risk and time preferences to explain the take-up decision of higher education grants. Other work finds that experimental measures of risk preferences perform poorly in predicting real-world decisions. Hardeweg et al. (2013) report that survey-based measures of risk attitudes performed better than experimental methods in predicting the probability of being self employed in Thailand. More recently, Charness et al. (2020) found that while HL measures of risk preferences predict financial decisions within the laboratory, they show little ability to predict real world decisions on financial, health and employment matters. Given the lack of an overall consensus on this issue, more research is clearly warranted. Our work complements these papers by exploiting experiments, both to measure risk preferences, and to analyse real-world decisions. This generates powerful statistical tests of sorting on the basis of risk preferences in the real labour market. Moreover, our design involves repeated lotteries for measuring risk preferences. This allows us to control for measurement error and to investigate the importance of scale in determining the ability of lotteries to predict real-world decisions. We present evidence that scale matters and that workers took the lottery less seriously at low stakes, leading to errors in the measurement of risk preferences. The rest of the paper is organized as follows: Section 2 provides institutional details on the tree-planting industry in British Columbia. Section 3 presents the experimental design while Sect. 4 presents our theoretical predictions. In Sect. 5, we present the experimental and payroll data. Section 6 analyses the stability of the measured risk preferences across lotteries. Section 7 relates the experimental measures of risk preferences and worker ability to contractual choice. Section 8 discusses our results and concludes the paper.",
25.0,5.0,Experimental Economics,25 July 2022,https://link.springer.com/article/10.1007/s10683-022-09759-6,Voluntary ‘donations’ versus reward-oriented ‘contributions’: two experiments on framing in funding mechanisms,November 2022,Maja Adena,Steffen Huck,,Female,Male,Unknown,Mix,,
25.0,5.0,Experimental Economics,10 May 2022,https://link.springer.com/article/10.1007/s10683-022-09757-8,Capital structure irrelevance in the laboratory: an experiment with complete and asymmetric information,November 2022,Arturo Macias,,,Male,Unknown,Unknown,Male,"In this paper we implement an experiment designed to test capital structure irrelevance  (Modigliani and Miller 1958; Stiglitz 1969) inspired by the structural credit risk framework of Merton (1974). In the first part of this Section capital structure irrelevance is introduced, while the second describes the experiment and its place in the Experimental Economics literature. 
Motivation
 An important feature of financial markets is the diversity of contracts representing different claims on the free cash flows of a firm.Footnote 1 In the 1950s, the incipient field of theoretical Corporate Finance revolved around characterizing the “optimal capital structure”, that is, determining the liabilities structure (especially the debt-equity ratio) that maximizes the value of a firm. The irrelevance theorem of Modigliani-Miller (Modigliani and Miller 1958) gives a counterintuitive solution to that problem: under some ideal conditions, the total market value of a firm is independent of its financial structure. In particular, debt-equity ratios (and decisions about dividend payments) do not affect the total market value of the liabilities of a firm. In their literature review, Ho and Robinson (1994) classify the hypotheses for capital structure irrelevance into two groups. First, there is a core set of general assumptions in the form of capital market perfection (no taxes, no bankruptcy, no transaction costs, no asymmetric information and no agency costs), exogeneity in the distribution of firm income (the theorem is about capital structure decisions given operational and investment decisions), and market value maximization as the objective of the firm. Capital structure irrelevance holds by arbitrage if one of the following additional hypotheses is verified: i) there are several copies of the same firm (“risk classes” in Modigliani and Miller (1958) terminology) and a safe asset, or ii) financial markets are Arrow-Debreu complete (Hirshleifer 1966) or completable (Titman 2002).Footnote 2 When markets cannot be completed and there are no “risk classes,” capital structure irrelevance is no longer proven by arbitrage, and there is a market equilibrium result instead (Stiglitz 1969): if a firm issues “risk-free” corporate debt (less debt than the minimum value of the firm in any possible future state of the world), the total value (value of debt plus equity) of the firm in equilibrium is not affected by the volume of (risk-free) debt it issues. In principle, for defaultable corporate debt the irrelevance proposition does not necessary hold, and a branch of Corporate Finance has been developed to deal with the economic decisions of firms in incomplete markets where the interests of different shareholders can be contradictory  (Bejan 2020). The Stiglitz theorem can be extended for defaultable corporate debt when market participants have mean-variance preferences (of which risk neutrality is a particular case).Footnote 3 Under mean-variance preferences, the two-fund separation property is satisfied, that is, the optimal portfolio is always a mix of cash and a scaled copy of the market portfolio (the more risk averse the market participant is, the higher the cash proportion). An investor who holds equal proportions of each class of a firm’s securities does not care about capital structure, and the definition of the cost of equity and debt is arbitrary (Hamada 1960). Capital structure irrelevance is a counterfactual property, and its empirical evaluation with real firm data is challenging because the data only provide actual capital structures.Footnote 4 Even capital market perfection is an overly demanding property for samples of real firms: non-neutral taxation between equity and debt and bankruptcy costs were identified as violations of capital market perfection in Modigliani and Miller (1963) and Kraus and Litzenberger (1973). Given that bankruptcy costs and fiscal non-neutrality exist in all of the world’s jurisdictions, real firm samples do not fit the hypotheses of the capital structure irrelevance theorems. By contrast, in laboratory experiments, it is possible to control the “value of the firm” and to replicate the “capital market perfection conditions”. 
Experiment, findings and contribution
 In the experiment presented in this article, each group of ten subjects trades (in double-auction markets) two assets (stocks and bonds) whose values are determined by a stochastic variable (the value of the firm). The debt of the firm is common knowledge, and a signal carrying incomplete information on the firm value is revealed to the participants at the beginning of each round. Over the course of the experiment, four indebtedness and three signal levels are considered. Half of the groups experience the Public treatment, where the signal is given to all participants. The other half experience the Private treatment, where the signal is disclosed to four participants randomly chosen anew in each round. In the Public treatment, the conditions for capital structure irrelevance hold under mean-variance preferences (including the particular case of risk neutrality, which is the customary hypothesis in experimental economics whenever small amounts of money are at risk), and portfolios can be switched as necessary for the two-fund separation property to hold. However, strict arbitrage is not possible: given a distribution of firm value (determined by the signal), different capital structures (determined by indebtedness) occur in different markets. As a consequence, capital structure irrelevance cannot be re-established by arbitrage. In the Private treatment, the literature provides two natural belief formation frameworks (Lintner 1969): rational expectations and prior information. There is ample literature about incomplete information in experimental markets, ranging from the early experiments of Plott and Sunder (1982) to more recent experiments comparing the expectation formation hypotheses  (Bossaerts et al. 2014) or the impact of the degree of information dispersion on market outcomes  (Corgnet et al. 2020). The results from the Public treatment do not support the rational expectations (risk-neutral) assumption: some securities (mainly those stocks whose theoretical value is close to zero) are significantly overvalued. However, the experiment supports capital structure irrelevance. Treatment effects are confirmed and the inter-treatment comparison shows that information is more efficiently embed in prices in the Public treatment. The analysis of the Private treatment is focused on a comparison between the rational expectations and prior information (risk-neutral) equilibria. The prior information equilibrium provides a better fit for the market prices of the securities and for the profits of informed and uninformed participants. An analysis of the final portfolios of the informed and uninformed participants suggests incomplete adjustment to the prior information equilibrium-predicted holdings. Finally, capital structure irrelevance under asymmetric information is tested in treatment Private, and the results are somewhat inconclusive but lean towards rejection. This article contributes to the experimental financial markets literature. To the best of our knowledge, there are two laboratory experiments testing the Modigliani-Miller proposition on capital structure irrelevance regarding leverage: First, Levati et al. (2012) explored the effect of leverage on the cost of capital. In their design, different leverage ratios are considered, but debt securities cannot default (consequently, the price of debt is not stochastic, and debt is not traded). The results of their experiment show that the subjects recognized the increased systematic risk of equity when leverage increased, as they demanded a higher return for bearing that risk. Moreover, the results support a U-shaped cost of capital curve, suggesting that individuals tend to underestimate the riskiness of low-leveraged equity and to overestimate the riskiness of high-leveraged equity. Second, Charness and Neugebauer (2019) considers the “bubble environment” of Smith et al. (1988) with two risky assets representing the equity of a leveraged and an unlevered firm. In the control treatment, the cash flows of the securities are independently drawn from two random distributions, while in the main treatment, the cash flows of the leveraged share are the cash flows of the unlevered share minus a fixed cash quantity. Consequently, the expected values of the leveraged and unlevered shares differ by the same amount in both treatments, but in the control treatment, the cash flow differences between both securities are stochastic, while in the main treatment (in line with the Modigliani-Miller proof), the difference is a constant. The experimental data show that in the main treatment (perfect correlation) case, the differences in value of both securities are closer to the theoretical prediction than in the (independent cash flows) control treatment. This result is evidence of the higher efficiency of “perfect” versus “statistical” arbitrage, and it supports the Modigliani-Miller theorem. There is also an experimental literature on dividend policy, where either dividend irrelevance or the impact of dividends in the leverage invariance proposition are tested (see Angerer et al. (2019), Asparouhova et al. (2016) and Neugebauer et al. (2020)). The experimental design presented in this article is different from those in previous experiments on capital structure irrelevance: i) the theoretical basis of our experiment is capital structure irrelevance (in particular, leverage irrelevance) in a market equilibrium framework, while previous experiments focus on capital structure irrelevance as a consequence of the Law of One Price, ii) both equity and debt are openly presented as such and traded independently: the possibility of default is considered and different levels of leverage are compared, iii) the effects of the exploitation of non-monopolistic asymmetric information are explored. The remainder of the paper proceeds as follows: in Sect. 2, we describe the experiment. In Sect. 3, the results for the Public treatment (where capital irrelevance and rational expectations valuation are tested) are presented. In Sect. 4, the results for the Private treatment (where alternatives to rational expectations are considered) are discussed. In Sect. 5, the conclusions are presented. Some statistical results are relegated to Online Appendix A, while the instructions to the participants are included in Online Appendix B.",
25.0,5.0,Experimental Economics,21 April 2022,https://link.springer.com/article/10.1007/s10683-022-09754-x,A reassessment of the potential for loss-framed incentive contracts to increase productivity: a meta-analysis and a real-effort experiment,November 2022,Paul J. Ferraro,J. Dustin Tracy,,Male,Unknown,Unknown,Male,"In the behavioral sciences, scholars have invoked loss aversion to explain behavioral patterns that appear to contradict traditional economic theories (Kahneman & Tversky, 1979; Thaler & Johnson, 1990; Hardie et al., 1993; Haigh & List, 2005; Jarrow & Zhao, 2006; Looney & Hardin, 2009; Chrisman & Patel, 2012). For a loss-averse individual, the disutility of a loss is larger than the utility of an equivalent gain. Although the evidence base for loss aversion has been challenged [e.g., Walasek and Stewart (2015), Gal and Rucker (2018) and Yechiam (2019)], scholars and practitioners working in the private and public sectors have argued that loss aversion can be harnessed to induce behavioral changes (Convery et al., 2007; Thaler & Sunstein, 2008; Jakovcevic et al., 2014; Homonoff, 2018). One application is the loss-framed incentive contract, in which incentives are framed as losses from an earnings benchmark rather than as gains from a zero earnings default (Hossain & List, 2012; Imas et al., 2016). For example, rather than offer a worker $1 for every unit of output, a firm can offer a loss-framed contract that pays workers $B for a performance benchmark of B units from which $1 would be subtracted for every unit short of the benchmark. As long as the worker does not exceed the benchmark, the contracts pay the same for a given level of performance. Given this equivalency, traditional economic theory predicts no difference in expected productivity under the two contracts. The theory of loss aversion, however, assumes workers assess outcomes relative to reference points, which differ under the two contracts. Loss-averse workers will work harder to avoid losses from $B than to achieve similar gains from $0 in the gain-framed contract. Consistent with this prediction, 21 of the 26 experiments that contrast productivity under loss-framed and gain-framed contracts report that loss-framed contracts induce greater productivity (Fig. 1). The average effect is 0.19 SD, with a third of the studies reporting effect sizes of about a half standard deviation (SD) or larger. Even after weighting the studies by the precision of their estimates in a meta-analysis, the summary estimated average effect size is 0.16 SD. Yet despite the experimental evidence pointing to a substantial productivity impact from a simple change in framing, loss-framed contracts are rare outside behavioral science experiments. If the impacts of loss-framed contracts were only observed in cases in which workers received the benchmark earnings up-front, a plausible explanation is that employers find it costly, financially or socially, to claw back the lost earnings from workers. Yet more than half of the experiments do not endow the workers with the earnings in advance; they simply change the contract wording. We can detect no difference in the estimated effect sizes between studies that advance the earnings and those that do not \((\chi ^2= 1.63,\, df =1,\, p= 0.20)\); see Sect. 2.2.2. In other words, with little cost, loss-framed contracts appear to be able to increase effort exerted by workers (or other agents, such as citizens targeted by government programs that aim to incentivize the supply of positive externalities). Indeed, this promise has inspired some behavioral scientists to encourage private and public sector actors to adopt loss-framed contracts. For example, in an incentive program to encourage South Africa citizens to exert more effort to drive carefully, the designers framed the incentives as losses (“loss aversion lotteries”) because people have a “tendency to be especially troubled by the risk of losing things that already belong to them.”Footnote 1 We postulate three reasons why, despite the abundance of evidence for productivity gains associated with loss-framed contracts, there is a paucity of such contracts in the private and public sectors: (1) the estimates that form the evidence base exaggerate the true effects or are not externally valid; (2) workers prefer gain-framed contracts, and thus any productivity gains at the intensive margin may be more than offset by increased costs at the extensive margin (i.e., employers offering gain-framed contracts attract more workers); and (3) the framing effect does not persist over time (most studies observe behavior for short time periods). In our study, we focus on the first two reasons and find evidence consistent with them both. In our meta-analysis, we observe that the average estimated effect masks substantial heterogeneity in effect sizes between laboratory and field experiments. Whereas laboratory experiments suggest that loss framing increases productivity by nearly 0.33 SD, the summary effect size for field experiments is 0.02 SD (Fig. 1). The laboratory experiment estimates are also much more variable, and thus the summary effect is much less precisely estimated among the laboratory experiments. Whether the difference between laboratory and field experiments reflects different types of workers, different types of working environments, or publication biases, these patterns imply that real-world organizations may expect more modest impacts from loss-framed contracts than implied by the academic literature. To shed more light on the performance of loss-framed contracts and worker preferences for these contracts, we designed a real effort laboratory experiment in which workers had an opportunity to select their contract after working under both contract types. In the experiment, workers were randomized to work under a loss-framed contract and then a gain-framed contract, or vice-versa (i.e., within-worker design). Our estimated effect size of 0.12 SD is roughly two-fifths the summary effect size of prior laboratory experiments and closer to the summary effect size from field experiments. Furthermore, when given the opportunity to choose the contract framing, only one in five workers chose the loss-framed contract. Three out of four workers chose the gain frame, and the rest reported indifference. This pattern, in which a minority of workers report preferring the loss frame, is consistent with prior claims (Lazear, 1991), survey studies (Tannenbaum et al., 2013; Evers et al., 2017), and two field experiments (Brownback & Sadoff, 2020; Van der Stede et al., 2020). However, two other incentivized experiments report that workers prefer loss-framed contracts (Imas et al., 2016; de Quidt, 2018). In Sect. S.2.1, we show how the designs and preference metrics used by these two studies can mask a majority preference for gain-framed contracts. Furthermore, we observe that the loss-framed contract effect is only detectable among the minority of workers who prefer the loss-framed contract. The estimated effect among the rest of the workers is indistinguishable from zero, in both practical and statistical senses. If the loss-framed-contract-preferring workers were more productive than the average worker, employers could offer loss-framed contracts to screen for productive workers. However, we find that loss-frame-contract-preferring workers are less productive under the gain-framed contract; a loss-framed contract only brings their average performance up to the average performance of the other workers. Even if the loss-framed contract cannot serve as a screen for high-productivity workers, it may serve as a commitment device for low-productivity workers (Imas et al., 2016). To exploit this commitment device, however, an employer would have only two options. It could offer two types of contracts simultaneously within the organization and let workers select their preferred contract. Or it could separate its operations into two units, each offering a different contract. In many contexts, the organizational complexities and fixed costs implied by such strategies could easily dominate the modest performance benefits from loss-framed contracts. Our experimental results also suggest that the difference between the summary effect size estimates from laboratory and field experiments may not be an artifact of the laboratory context per se, but instead may arise from the mix of underpowered designs and publication bias. Underpowered designs produce highly variable estimated effect sizes. This variability yields an exaggerated picture of the true effect size when combined with a bias against publishing estimates that are statistically insignificant or estimates of an unanticipated sign. Adjusting for this bias through a simple trim-and-fill method yields a summary effect size for laboratory experiments of 0.02 SD, which matches the summary effect size for field experiments. In summary, our meta-analysis and experimental results imply that the effects of loss-framed contracts on productivity may be real. Yet these effects are also likely to be, on average, modest and heterogeneous in ways that are difficult to exploit in a cost-effective manner. In the next section, we present the meta-analysis. We present our experimental design and results in Sect. 3. In Sect. 4, we discuss our results and outline paths for future research.",3
25.0,5.0,Experimental Economics,17 August 2022,https://link.springer.com/article/10.1007/s10683-022-09767-6,Assignment feedback in school choice mechanisms,November 2022,Daniel Stephenson,,,Male,Unknown,Unknown,Male,"Children in the United States are traditionally assigned to public schools based on where they live, but a growing number of school districts now allow parents to indicate their preferences over schools. In these districts, policy makers employ mechanisms that assign students to schools based on reported preferences and legally determined priorities. Participants can often submit and adjust their preference reports over a period of several days. School assignments are typically revealed after the end of the reporting period when all preference reports are finalized. This paper experimentally investigates whether real-time feedback about tentative assignments produces more frequent equilibrium outcomes than providing information only at the end of the reporting period in three student assignment mechanisms: deferred acceptance, top trading cycles, and the Boston mechanism. This hypothesis is motivated by adaptive models that describe dynamic behavioral adjustment according to simple rules. Adaptive models predict that all three mechanisms will achieve equilibrium assignments more frequently when participants are sensitive to tentative assignments during the preference reporting period. Consistent with adaptive predictions, real-time assignment feedback produced more equilibrium assignments in all three mechanisms. Consistent with equilibrium predictions, real-time assignment feedback increased efficiency in top trading cycles and it eliminated more justified envy in the deferred acceptance mechanism. No mechanism can guarantee both Pareto efficiency and the elimination of justified envy in equilibrium, so different mechanisms are designed to achieve different policy goals. The dominant-strategy equilibrium of the top trading cycles mechanism achieves Pareto efficiency, while the dominant-strategy equilibrium of the deferred acceptance mechanism eliminates justified envy. All Nash equilibria of the Boston mechanism eliminate justified envy. While truth-telling is a major objective in the design of school choice mechanisms, real-time feedback did not increase the frequency of truthful reporting. Truthful reporting is a weakly dominant strategy in both top trading cycles and deferred acceptance. In contrast, the Boston mechanism has no dominant strategy. In all three mechanisms, inaccurate preference reports often produce the same assignments as truthful preference reports. Consequently, all three mechanisms have Nash equilibria involving inaccurate preference reports and adaptive models predict that sensitivity to tentative assignments is insufficient to reliably produce truthful preference reports. Historically, calculating mechanism assignments was time consuming and computationally expensive, so assignment feedback was rarely provided during the reporting period. Technological advances have significantly reduced these obstacles. Many school districts already utilize online interfaces for on-demand preference reporting. The Wake County public school system provided real-time feedback about the first choices of other participants in a Boston mechanism (Dur et al., 2018). Inner Mongolia provided feedback in a dynamic queuing mechanism where participants apply to one school at a time (Gong & Liang, 2020). The results of this paper suggest that providing feedback about tentative assignments during the reporting period could help school choice mechanisms more reliably achieve policy goals. The remainder of this paper is organized as follows. Section 2 discusses the related literature. Section 3 presents the theory. Section 4 describes the experimental design. Section 5 covers the hypotheses. Section 6 presents the results and Sect. 7 concludes.",
25.0,5.0,Experimental Economics,07 September 2022,https://link.springer.com/article/10.1007/s10683-022-09770-x,Pandora’s rules in the laboratory,November 2022,Efthymios Lykopoulos,Georgios Voucharas,Dimitrios Xefteris,Male,Male,Male,Male,"Weitzman (1979), in his seminal work, provided a search model with the name of “Pandora’s box”. The design of the problem was simple, yet, its solution, up to that point, seemed complex and computationally tedious. Weitzman elegantly provided a solution to this perplexing situation with an optimal stopping algorithm, which was forward-looking, easy to follow, and intuitive.Footnote 1 Optimal search algorithms in such problems since then have been commonly referred to as Pandora’s rules. The “Pandora’s box” problem in its original form is as follows: An agent is confronted with N different boxes and may choose only one of them. Each box contains some reward that is randomly drawn from a box-specific distribution. The agent cannot observe the content of the box unless she decides to inspect it, but knows the distribution from which the reward is drawn. In each round, the agent is allowed to inspect only one box by paying a cost, while standard discounting is applied. The agent can stop the search at any point and, while she is not bound to keep the box which she last opened, she is constrained to choose one of the previously inspected boxes. Since information in this setup is costly, it is hardly ever optimal to open all of the boxes and simply pick the one with the highest prize. At the same time, following a search rule based on the expected reward of each box can lead to a sub-optimal outcome. Weitzman (1979) showed that, in such cases, what the agent needs to do is to simply compute the reservation value of each box, and apply the following algorithm: First, sort the boxes according to their reservation value in descending order. Then, open the box with the highest reservation value and compare the realized prize of that box with the reservation value of the next unopened box. If this value is greater or equal to the reservation value of the next-to-be-inspected box, the agent should keep the open box and terminate any further search.Footnote 2 Otherwise, the search continues. Pandora’s rule is easy to be identified in the setting of Weitzman (1979) but at the same time it is fairly sensitive to changes in the search conditions. Importantly, Doval (2018) demonstrated that by adding some flexibility (i.e. by allowing the agent to simply choose an uninspected box) Pandora’s rule can change dramatically. Indeed, in a simple setup with a safe (high probability of a moderate payoff, and low probability of a zero payoff) and a risky box (low probability of a high payoff, and high probability of a zero payoff), inspection tends to start more often from the risky box when the agent is additionally allowed to choose an uninspected box, while search starts more often from the safe alternative when she is constrained to choose only from previously inspected boxes. Notice that without the flexibility to choose an uninspected box, inspecting a box both resolves uncertainty and, at the same time, expands the agent’s choice set. However, when this type of flexibility is provided, the second channel is such that agents have more incentives to begin the search with more uncertain options. While pinning down the theoretical properties of providing extra flexibility in search problems is of paramount importance, one would further like to assess the empirical relevance of the derived Pandora’s rules. Unfortunately, this is not possible by analyzing observational data: Real search problems are characterized, not only by different search conditions, but also by heterogeneous agents’ backgrounds, behavioral traits, education, etc. Therefore, to identify the effect of a change in search conditions on the employed search pattern and the corresponding payoffs, one has to turn to more controlled environments. In this paper, we report results from a laboratory experiment suitably tailored to answer the questions above. We focus on a simplified search problem according to which a subject faces a risky and a safe option. The risky option has a 25 percent chance of containing 100 coins, and a 75 percent chance of containing nothing, while the safe option has a 50 percent chance of containing \(X \in \{41,42,...,99\}\) coins, and is empty with the remaining probability. That is, the maximum potential reward of the risky box is higher than that of the safe box, while the probability of a box being non-empty is higher for the safe box. The potential coins contained in the safe box, X, is known to subjects, but we allow them to vary in each round in order to extract more information regarding the employed search rule. We investigate two alternative types of search conditions: the strict and the flexible one. Under strict search conditions, the search is conducted a la (Weitzman, 1979); that is, a subject can only keep a previously inspected box. Under flexible search conditions, the search is conducted a la (Doval, 2018); a subject can keep an uninspected box, provided that she has inspected at least one.Footnote 3 To inspect a box a subject must sacrifice 20 coins and throughout the experiment, each subject decides according to the same set of search conditions (across subject design). Under strict search conditions, Pandora’s rule dictates that search should be initiated with the risky option if the potential content of the safe box X is below 60 coins, and with the safe box if X is above 60 coins. In contrast, Pandora’s rule under flexible search conditions is such that the search starts from the risky box independently of X.Footnote 4 What we find is that under strict search conditions search starts from the risky box when the payoff-to-riskiness index of the risky box exceeds the inspection cost. This index can be seen as the excess maximum payoff of the risky box (i.e. 100-X) over a measure of its relative riskiness. Meanwhile, under flexible search conditions search starts from the risky box when payoff-to-riskiness is positive, which by design is always true. These theoretical predictions lead to our main empirically testable hypotheses: a) under strict search conditions, the search process starts more often from the safe box than under flexible conditions, and b) under strict search conditions the first inspection choice will be more reactive to changes in X than under flexible conditions. The experimental results support, by and large, these theoretical predictions. Subjects, indeed, react to the set of search conditions in the predicted way. That is, even if they are boundedly rational, and their choices are affected by salient features of the options (e.g. expected reward of each box, etc.), they also react to the different sets of search conditions and exhibit a behavior that is comparatively similar to the corresponding Pandora’s rules. Additional to our main hypothesis, we test for differences in payoffs across treatments since flexible search conditions allow the agent to save on the inspection cost. Our results indicate that subjects operating under flexible search conditions enjoy higher payoffs than subjects operating under strict search conditions. Moreover, since the optimal search rule remains constant under flexible search conditions we test for the ease of identification of the correct order of inspection across treatments. To this end, we find empirical evidence supporting that Pandora’s rule is more often identified under flexible conditions than under strict conditions. Such findings confirm that the mathematical modeling of search problems can inform competently empirical research and policy design. In turn, this reaffirms the need for further formal analysis of search problems, since such studies do not only produce elegant results of theoretical interest, but also insights that are pertinent to search conducted by real subjects. Finally, it is worth noting that the experimental testing of Pandora’s rules –beyond its aforementioned general interest– also admits another –more applied– motivation. Traditionally, shopping has been an exploratory process where a potential buyer had to visit several stores before buying a good of uncertain quality. Nowadays, most stores (if not all) also provide the flexibility to the buyer to acquire a good online. That is, traditional shopping in physical stores is a search problem with little flexibility (one can only buy a previously inspected good), while contemporary shopping is a search problem with more flexibility (one can either inspect the good by visiting the physical store, or acquire the good online, saving the inspection costs at the expense of not resolving/reducing the uncertainty regarding product quality). Therefore, the empirical testing of adding flexibility in search problems generates novel insights regarding the non-trivial impact of market digitization on consumer behavior. For example, when two outlets provide both the option to shop online and in their physical stores, consumers are predicted to visit only the physical store of the riskier outlet; while when shopping online is not an option, starting the search from the safer outlet can be optimal. The rest of the paper is organized as follows: In Section 2 we review the relevant literature, in Section 3 we present the theoretical benchmark, in Section 4 we describe the experimental design and state the testable hypotheses, in Section 5 we develop our results, and in Section 6 we conclude.",
26.0,1.0,Experimental Economics,09 July 2022,https://link.springer.com/article/10.1007/s10683-022-09762-x,On the change of risk aversion in wealth: a field experiment in a closed economic system,March 2023,Tobias Huber,Johannes G. Jaspersen,Dennis Strümpel,Male,Male,Male,Male,"Assumptions on how risk aversion changes in wealth are common in decision theory. They are an important condition for many theoretical results in fields such as development studies (Ogaki & Zhang, 2001), insurance demand (Mossin, 1968), asset pricing (Basso & Pianca, 1997), or taxation (Hellwig, 2007). Such assumptions are common when using structural forms of utility functions in empirical calibrations of decision models (e.g., Handel, 2013; Lockwood, 2018) or in elicitations of risk preferences from choices (e.g., Cohen & Einav, 2007; Holt & Laury, 2002). This paper uses a new approach to gain empirical insights into the change of risk aversion in wealth. In a mobile game, players collect in-game currency, which they can spend on new game content. At the end of each successful round, players make a lottery choice which allows us to infer bounds on their risk aversion. Tracking the players’ decisions at different in-game wealth levels enables us to determine the effect of wealth on risk aversion. Although players cannot win real money, the in-game currency can be spent on game content so that our setting is salient in the sense of Smith (1982). The use of a mobile game with an in-game currency which derives value from in-game purchases lets us analyze a closed economic system. Because there is no external market for the in-game currency, the relevant wealth for players’ decisions can be measured in isolation, providing us with a novel setting for analyzing the impact of wealth on risk preferences. We find evidence for both decreasing absolute risk aversion (DARA) and decreasing relative risk aversion (DRRA). Our findings are robust to various econometric specifications, alternative definitions of wealth, and potential boredom by the subjects. We further identify a set of decisions made following a simple “always safe” heuristic and a tendency of players to believe in the “hot hand fallacy”. Our results on absolute risk aversion concur with the economic intuition originally presented by Arrow (1971) and the majority of other studies on the topic (e.g., Chiappori & Paiella, 2011; Guiso & Paiella, 2008; Levy, 1994). Our findings on relative risk aversion are in line with the results of Paravisini et al., (2017). Our study is complementary to previous analyses of the issue. How risk aversion changes in wealth has been analyzed by cross-sectional survey studies (e.g., Cicchetti & Dubin, 1994; Guiso & Paiella, 2008; Halek & Eisenhauer, 2001), panel analyses (e.g., Brunnermeier & Nagel, 2008; Paravisini et al., 2017), and laboratory experiments (e.g., Levy, 1994). However, each of these methods has some challenges. Survey studies often cannot establish causality in their analyses because just as wealth can influence risk aversion, the predominance of positive risk premiums on asset markets allows risk aversion to influence (average) wealth. Using panel data can, potentially, disentangle the endogeneity problem, but suffers from inertia in the financial choices made by individuals (Brunnermeier & Nagel, 2008). Even when only active decisions are analyzed and inertia can thus be excluded, it is not always possible to accurately measure the wealth of the analyzed individuals. Using general wealth fluctuations for the entire population is an alternative. These, however, are usually associated with changes in economic climate, which is known to affect risk preferences (Guiso et al., 2018). Evidence from laboratory experiments could potentially remedy some of the issues faced by studies of naturally occurring data. However, since stakes in the laboratory are often small, it is unclear whether revealed preferences approaches in this environment actually estimate utility curvature in the canonical sense or whether they reflect some other form of risk preference motive (Bleichrodt et al., 2019; Rabin, 2000). Even if utility curvature was measured, it is unclear whether the utility function over the small stakes of the laboratory is representative for the one applied to wealth outside of the laboratory. Hypothetical scenario choices can feature more substantial stakes. This methodology, however, has its own underlying problems, such as measurement errors (Bound et al., 2001) and distortions due to the lack of salience and incentive compatibility (Dohmen et al., 2011; Holt & Laury, 2002; Smith, 1982). Our results are complementary to the previous literature because our study is able to exclude many previous caveats, although it may have a lower external validity than studies of real monetary flows. In the analyzed game, players cannot increase their level of in-game wealth by making cash payments and in-game wealth cannot be converted to outside wealth. As a consequence, in-game wealth and personal wealth outside of the game are independent of each other, such that observing the players’ in-game wealth is sufficient to fully observe the wealth relevant to their choices. Further, letting the players make decisions in a simulated environment allows us to make observations financially unaffected by the current economic climate. Our setting also does not allow for inertia to determine the players’ choices. In naturally occurring data, this problem often arises, because households adjust their financial choices only very slowly following changes in wealth (Brunnermeier & Nagel, 2008). In our setting, players are required to make active choices before the game continues, ruling out inertia as a possible choice mechanism. Lastly, because we consider a simulated environment, we are not limited regarding the size of the lottery stakes and can use outcomes which are significant shares of the players’ wealth. We draw implications for theoretical models, empirical calibrations, and economic applications. Since the results of theoretical models often depend on how risk aversion changes in wealth, our findings can be used to make behaviorally valid assumptions. For empirical calibrations of decision models, utility functions with constant absolute risk aversion or constant relative risk aversion are the predominant parametric assumptions. Our results reject these types of utility functions, at least for our sample and our study setting. We suggest the expo-power function by Saha (1993) as an alternative parametric form.",2
26.0,1.0,Experimental Economics,28 December 2022,https://link.springer.com/article/10.1007/s10683-022-09788-1,Ingroup bias in a social learning experiment,March 2023,Wenbo Zou,Xue Xu,,Unknown,,Unknown,Mix,,
26.0,1.0,Experimental Economics,28 May 2022,https://link.springer.com/article/10.1007/s10683-022-09760-z,Communication in multilateral bargaining with joint production,March 2023,Andrzej Baranski,Caleb A. Cox,,Male,Male,Unknown,Male,"In many economic situations involving joint production, the distribution of benefits among partners takes place once parties have exerted effort or invested resources into their common task. Communication is an essential component in bargaining settings and it can significantly affect agreements (Andreoni & Rao, 2011; Charness, 2012), which may in turn affect incentives to exert effort into the production process. Evidence from previous studies in majoritarian bargaining shows that pre-proposal written communication leads to highly unequal outcomes (absent joint production see Agranov and Tergiman (2014)) while joint production without communication leads to equitable sharing in bargaining (Cappelen et al., 2007; Baranski, 2016). In this paper, we investigate whether communication during profit-sharing will affect productive incentives. The answer will depend on whether communication exacerbates competitive behavior or fosters fair sharing. One illustrative example may be found in Major League Baseball where the top 10 teams earn a rank-dependent bonus from the player’s pool which comes from ticket sales. “Once the money has been divvied out to each club, it’s up to the players to decide who gets a share of their team’s winnings (Elkins, 2018).” In a highly-publicized case of the music industry (Azerrad, 1992), Nirvana lead singer Kurt Cobain negotiated with the band’s members to receive 75% of the songwriting royalties retroactively since the launch of the album Nevermind, citing that he had written most of the songs.Footnote 1 Legal partnerships are also well-known for holding end-of-year profit-sharing meetings, once it is known how many hours each partner billed or how many trials were won. While anecdotal accounts of profit-sharing negotiations are common, field data on the process behind the agreements is virtually nonexistent.Footnote 2 A burgeoning experimental literature on bargaining over the benefits resulting from joint production has emerged to fill this gap. There is unequivocal evidence that entitlements matter to bargainers when assigning shares of the common surplus (Konow, 2000; Cappelen et al., 2007; Karagözoğlu & Riedl, 2014). Such entitlements are even evoked in settings where the surplus is framed as if it had been produced despite no investments or effort taking place (Gächter and Riedl, 2006).Footnote 3 Importantly, respecting other’s inputs in bargaining, absent a contract, has been shown to give rise to high efficiency gains in problems of collective action as shown in Baranski (2016, 2019) and Dong et al. (2019).Footnote 4 The role that verbal communication may play in the division of a common surplus remains understudied experimentally, despite its relevance to settings with joint production. Notable exceptions are Bolton et al. (2003), Bolton and Brosig-Koch (2012), Abbink et al. (2018), and Gantner et al. (2019), which we discuss in detail in our conclusion once we have described our game and established our results to facilitate comparisons. Instead, the literature has largely focused on communication in joint production absent an endogenous redistribution stage, such as linear public goods games in which the shares of the common fund are pre-specified. It is well established in the experimental literature that the possibilities to communicate prior to investments in public goods games leads to enhanced efficiency (Ostrom et al., 1992; Cason & Khan, 1999; Bochet et al., 2006; Gangadharan et al., 2017). Pre-game communication is also shown to increase efficiency in employee-employer games with moral hazard (Charness & Dufwenberg, 2006) and adverse selection (Charness & Dufwenberg, 2011), double-auction settings (Valley et al., 2002), prisoners’ dilemmas (Sally, 1995), and demand bargaining games with outside options (Feltovich & Swierzbinski, 2011). All these studies have one aspect in common: that communication precedes production decisions, and thus may be used to signal directly an intention to achieve efficient outcomes. Our setting precludes this possibility since communication occurs after productive decisions have been made. To our knowledge, there are no studies on how communication during majoritarian bargaining affects the division of a jointly produced surplus and its effect on productive incentives. To address this question, we conduct an experiment on the majoritarian bargaining game developed by Baron and Ferejohn (1989) (hereafter BF). In our experiment, subjects bargain in groups of three with proposals requiring two votes to pass. The surplus is created via individual investments: subjects may invest any amount up to their endowment, and investments are multiplied times 1.8. Our treatment variables are whether subjects may communicate or not during the bargaining stage and whether or not group members may observe individual contributions. The BF model of multilateral bargaining is one of the most widely-studied models in Economics and Political Science both theoretically and empirically.Footnote 5 Several experiments (Fréchette et al., 2003, 2005a, b; Diermeier & Morton, 2005; Bradfield & Kagel, 2015; Miller et al., 2018) have documented that, when bargaining to divide an exogenous surplus, the proposer typically holds a payoff advantage and modal allocations are those in which only the minimum number of voters required for approval receive a positive share. It has been shown in legislative bargaining that communication (absent joint production) leads to highly unequal outcomes (Agranov & Tergiman, 2014): proposers can extract larger rents because communication is used to induce competition between voters for a spot in the winning coalition. Confirming the previous study, Baranski and Kagel (2015) report that voters actively ask proposers to exclude redundant members.Footnote 6 On the other hand, joint production (absent communication) fosters equitable sharing: shares are typically correlated with contributions to the common fund (Baranski, 2016). As a result, all-way splits are modal and not minimum winning coalitions. Importantly, shares of the fund are positively correlated with investments when these are observable by all members, which creates the right incentives for efficiency gains, but absent observability, investments rapidly unravel. Given the opposing effects on bargaining outcomes that joint production and communication have (considered in isolation), our experiment will shed light on whether the competitive effect of communication crowds out the tendency to share the surplus based on individual contributions, or whether equitable sharing prevails. Absent communication, investments create a focal point on how to redistribute equitably. However, some players would benefit from alternative redistribution schemes such as minimum winning coalitions and thus may use communication to pursue them. Given that any division of the surplus is a Nash equilibrium in the bargaining subgame, and that one can also construct subgame perfect Nash equilibria to sustain any allocation (when players are patient enough), our question is empirical: how will subjects communicate and what impact will this have on equilibrium selection with and without joint production? Our results show that verbal communication at the bargaining stage is mainly used to promote fairness either as equality or equity (i.e. proportionality).Footnote 7 In all treatments, the proposer’s share is quite low relative to the benchmark and all-way splits are modal, not minimal winning coalitions. Communication content reveals that exchanged messages differ widely from those reported in experiments with an exogenous fund. Proposers are not actively seeking for the cheapest coalition partners, nor are voters actively requesting the exclusion of redundant members as one would conjecture if the competitive effect of communication prevailed. Instead, calls for equitable sharing are commonplace. Communication has a significant positive impact on efficiency when investments are unobservable because truthful reporting of one’s investment is quite common. Proposers tend to use this information to redistribute proportionally as they would under observability. Thus, a weaker version of the virtuous cycle that arises when investments are observable fosters efficiency gains. We find no treatment differences of communication on average efficiency when investments are observable, meaning that subjects do not use communication to coordinate on competitive outcomes as they typically do with an exogenous fund. The article is structured as follows. Section 2 describes the games that will be implemented in the laboratory and contains the equilibrium characterizations that will serve as experimental hypotheses. Section 3 presents details of the experimental implementation. The results are reported in Sect. 4 with a subsection devoted to the analysis of communication content. Section 5 concludes the article.",1
26.0,1.0,Experimental Economics,28 September 2022,https://link.springer.com/article/10.1007/s10683-022-09772-9,Competition with indivisibilities and few traders,March 2023,César Martinelli,Jianxin Wang,Weiwei Zheng,Male,Unknown,Unknown,Male,"Contrasting the conventional requirement of infinitely many traders for perfect competition, laboratory experiments using the double auction mechanism observe the convergence to competitive equilibrium and attainment of full efficiency with as few as eight traders (Smith 1962, 1982). Theories on strategic market games, which organize the market using a static double auction mechanism, have also verified the competitiveness of Nash equilibria outcomes with few traders in certain scenarios (Dubey 1982; Simon 1984; Benassy 1986, DSB hereafter). This paper reconciles the laboratory results with theories on strategic market games. We propose a strategic market game applicable to markets with indivisible commodities as in most double auction experiments, we derive conditions for the equivalence between Nash and competitive equilibrium that require as few as four traders, and we test the equivalence in the laboratory. In a strategic market game, buyers and sellers submit price-quantity pairs to a clearing house, which acts as a profit-maximizing middleman, and allocates trades accordingly. Like in a double auction, priorities are given to higher bids and lower asks in a strategic market game, inducing a price competition between traders. In line with Bertrand’s argument, DSB prove that having two active sellers and two active buyers in a Nash equilibrium is sufficient to make the outcome competitive in certain strategic market games. This paper addresses a few differences between the settings of the aforementioned strategic market games and the double auction mechanism. Firstly, the divisibility of the commodities. Proofs in DSB rely on the divisibility of commodities; in a conventional double auction market setting, there is an indivisible commodity for trade with divisible money. Analogous to what Mas-Colell (1977) finds when all but one commodities in the general equilibrium model become indivisible, we show that the “main results ...with divisible commodities and convex preferences remain valid” in the double auction setting. Unlike Mas-Colell (1977), we do not assume a continuum of traders. The condition DSB provide is a sufficient one for the competitiveness of outcomes from a Nash equilibrium, thus is not informative on the Nash equilibria where it is not met. We provide a necessary and sufficient condition for equivalence between the outcomes of the Nash equilibrium and the competitive equilibrium with indivisible commodities. Essentially, our condition requires that on each side of the market there are two inframarginal traders, in the sense that they are willing to trade at every competitive price.Footnote 1 Unlike previous work, our condition relies on the characteristics of the set of competitive equilibria, and place no requirement on Nash equilibria other than the occurrence of trade. Notably, our equivalence result includes contestable markets, in which a single active seller sells in the market at the competitive price.Footnote 2 Different from the common measures for market power such as residual demand elasticity or market concentration, the condition in DSB, which we show also holds in the indivisible setting, uses the minimum number of active traders as an indicator for the attainment of a competitive outcome. Our condition further differentiates the market structures that admit of monopolistic outcomes in the Nash equilibria and the ones that do not. When our condition is not met, i.e. a market structure allows for exploitation of market power in the equilibrium, it does not rule out the possibility of achieving a competitive outcome in a different equilibrium. Indeed, there always exists a Nash equilibrium with competitive outcome. Hence, when there is a monopoly in the market, whether the outcome is competitive can be a result of equilibrium selection. This helps to explain the convergence to competitive outcomes in some monopolistic markets in Smith and Williams (1990). Our condition is general and met in double auction markets such as in Smith (1962, 1976, 1982), as well as the stress-tests that yield supportive evidence for the institution’s robust convergence to competitive outcomes [for example, duopoly markets, swastika design and box design in Smith and Williams (1990), and swastika design in Smith (1965) and Holt et al. (1986)].Footnote 3 Parallel to the asymptote competitiveness in Cournot competition, when traders’ reservation values are independently drawn from certain distributions, our condition is more likely to be met as the number of traders grows.Footnote 4 While an exact model for the dynamic double auction institution remains absent, in our experiment, we examine the effectiveness of our condition in providing reference for the double auction markets.Footnote 5 Another important feature of the double auction is its demonstration of “the economy of knowledge with which it operates, or how little the individual participants need to know in order to be able to take the right action” (Hayek 1945, p. 527). To resemble the limited information each trader possesses in real markets, double auction experiments provide traders with private information of their own values of the commodity. However, when incomplete information solution concepts such as Bayesian Nash equilibrium are used in another strand of models for the double auction, the k-double auction, the attainment of full efficiency requires infinitely many traders (for literature reviews, see Satterthwaite and Williams 1993; Zheng 2020). As suggested by previous results from the experiments, “although traders’ information ...is far from complete, it is possible for them to learn to use the relevant ‘complete information’ strategies” (Friedman and Ostroy 1995, p. 23). To check whether the Nash equilibrium provides a good description of outcomes under the standard double auction settings, we run a laboratory experiment in which traders are informed about their own valuations but not about the valuations of other traders. We consider two market institutions: the clearing house institution, which is exactly the strategic market game we model, and a double auction following the rules of Smith (1962). These two institutions are static/dynamic versions of each other—the clearing house institution is indeed a call market with a pay-as-bid feature.Footnote 6 The double auction institution is known to facilitate learning of the relevant information for traders when compared to call markets (see e.g. Smith 1982; Smith et al. 1982). Under the clearing house institution, we provide traders with feedback on the trading prices and quantities at the end of each round, to check whether this information is enough for them to learn to act as-if there is complete information in the market as Hayek hypothesized. Each market in our experiment consists of two buyers and two sellers—the minimal size allowing for the equivalence of Nash equilibrium and competitive equilibrium outcomes, and thus adequate for a stringent test. Under each institution, we consider two market environments: one in which the two buyers and the two sellers are inframarginal, so that all Nash equilibrium outcomes are competitive, and one in which the two buyers but only one of the sellers are inframarginal (i.e. there is monopoly power) so that some Nash equilibrium outcomes are non-competitive. In the absence of monopoly power, the results from our experiment confirm the double auction institution’s convergence to competitive outcomes, though we have fewer traders than previous experiments.Footnote 7 Efficiency under the clearing house institution remains below efficiency under the double auction, but seemingly converges over time, in line with the results obtained by Smith et al. (1982) and Friedman and Ostroy (1995) for larger numbers of traders. Under both institutions, trading prices lay mostly in the competitive range in the absence of monopoly power, consistent with equilibrium predictions. When monopoly power exists, higher trading price, lower trading volume and an efficiency loss can be observed under the double auction compared to the environment without monopoly, as expected. Under the clearing house institution, trading volume is lower compared to the environment without monopoly, but the efficiency loss is not significant, and prices seem to converge to competitive levels over time. This surprising result may be either a consequence of the inability of the monopolist to gather enough information about the other side of the market to exploit monopoly power under the clearing house institution, or a consequence of coordination on a low-price outcome, which remains a Nash equilibrium outcome under monopolistic conditions. It is an interesting and open question whether the convergence to competitive outcomes for the clearing house institution even in the presence of monopoly power is robust to learning with a longer horizon and to variations in the parameters describing the economy. Gode and Sunder (1993) show the attainment of high efficiency when the double auction market is populated by simulated zero-intelligence traders who submit random non-loss-incurring bids and asks.Footnote 8 Our model helps to complete the story by focusing on the factor they omitted, the strategic behavior in the market, and how it may lead to the competitive outcome in double auction with a small number of traders. The convergence to competitive price by budget-constrained zero-intelligent agents was helped by a “Marshallian path,” i.e. trading occurring first to those who have more to gain from trade (Gode and Sunder 1993; Brewer et al. 2002). Given the static nature of our clearing house institution, the Marshallian path cannot have an impact within a round. However, there may be a similar effect through learning across rounds. Low cost and high value units are more frequently traded.Footnote 9 Based on previous transaction prices, traders may improve coordination and further narrow down the price range (Fig. 8). In the competitive environment under the double auction institution, we observe the Marshallian path to hold better in earlier rounds, as corresponds to a phenomenon linked to market learning out of equilibrium. Under the double auction institution, Coasian dynamics would prevent the monopolist seller from selling to the highest valuation buyer at above competitive prices since a buyer would anticipate that the monopolist would be willing to lower the price afterwards to trade with the other buyer. We do find some evidence of the monopolist occasionally lowering the price to sell a second unit. However, trading volume remains on average below competitive levels, and estimated price asymptotes suggest higher than competitive prices in the monopolistic environment under the double auction. In previous experimental work on market games, Duffy et al. (2011) explore a quantity strategic market game with divisible commodities, where traders retain market power. They compare outcomes with two and with ten traders per side, and obtain higher efficiency and more coherence to competitive behavior if there are more traders. Dufwenberg and Gneezy (2000) also obtain—somewhat surprisingly—a beneficial effect of the number of traders in an experiment on Bertrand competition, comparing outcomes with two, three, and four traders per side. We differ from both as we explore the boundary between competitive and noncompetitive environments. This paper is also closely related to Friedman and Ostroy (1995), which compares a strategic market game institution with double auction under various structural parameters with divisible commodities and larger number of traders, and draws a connection between DSB and results from double auction experiments. With the conditions we obtain for the equivalence of Nash and competitive equilibria outcomes, we are able to carry out a stringent test on their as-if complete information theory in the lab. The rest of the paper is organized as follows. Section 2 gives a formal description of the economy. Section 3 gives a detailed explanation of the strategic market mechanism. Section 4 contains the theorems of coincidence of Nash equilibrium and competitive equilibrium. Section 5 presents the experimental design and hypotheses. Section 6 describes the results. Section 7 concludes. Proofs are collected in “Online Appendix 1”, and experimental instructions and quizzes are collected in “Online Appendix 2”.",
26.0,1.0,Experimental Economics,21 November 2022,https://link.springer.com/article/10.1007/s10683-022-09780-9,Skewness expectations and portfolio choice,March 2023,Tilman H. Drerup,Matthias Wibral,Christian Zimpelmann,Male,Male,Male,Male,"Many models of investor behavior assume or imply that investors prefer assets with a positively skewed return distribution and dislike those with a negatively skewed return distribution (e.g., Arditti, 1967; Barberis & Huang, 2008; Brunnermeier et al., 2007; Dertwinkel-Kalt & Köster, 2019; Kraus and Litzenberger, 1976; Scott & Horvath, 1980).Footnote 1 Such models can explain a wide range of important and puzzling patterns in investor behavior such as the tendency to underdiversify (e.g., Conine & Tamarkin, 1981; Simkowitz & Beedles, 1978; Mitton & Vorkink, 2007), the surprising popularity of technical analysis (Ebert & Hilpert, 2019), or the pricing of skewness in the cross-section of stock returns (e.g., Boyer et al., 2010). Given the potential explanatory power of a preference for skewness, it is critical to provide an empirical test of its relevance for investor behavior. The key challenge for such a test lies in the fact that future returns to holding an asset are uncertain and investors have to form expectations about their distribution. Documenting the skewness of the expected return distribution at the level of the individual investor is therefore an important prerequisite for such a test.Footnote 2 However, despite a recently growing interest in measuring investors’ expectations and their relation to investment decisions (e.g., Drerup et al., 2017; Giglio et al., 2021; Kuhnen & Miu, 2017), the skewness of investors’ return expectations has so far received no attention. In this paper, we fill this gap using high-quality data on individual return expectations and portfolio choices from a series of repeated, financially incentivized experiments with a representative sample of the Dutch population. We characterize the heterogeneity in expected skewness and relate it to portfolio choice at the individual level, both in the cross-section and over time. Our data stems from a series of experiments which we ran with the Dutch LISS panel. In the experiments, we employed an intuitive graphical interface to obtain a fine-grained estimate for the distribution of each respondent’s return expectations for two risky assets, an index and a stock (Bellemare et al., 2012; Delavande & Rohwedder, 2008). We used these distributions to calculate estimates of the mean, standard deviation, and skewness of respondents’ expectations. In a complementary portfolio choice task, we asked respondents to allocate a fixed budget between the two risky assets and a savings account. To incentivize the belief elicitation procedure, we based respondents’ payouts at the end of the experiment on the accuracy of their beliefs. Likewise, we tied respondents’ payouts to the actual performance of their experimental portfolio at the end of the holding period, 1 year after the construction of the initial portfolio. Unannounced beforehand, we gave respondents the opportunity to update their beliefs and to change their portfolio allocations 6 months following the initial wave, adding a temporal dimension to the data. This paper has two main contributions. First, we document individuals’ expectations concerning the distribution of returns, and in particular its skewness, for a broad index and an individual stock in a large, representative sample. Second, we show how these expectations relate to individuals’ portfolio choices. Concerning the documentation of expectations, our selection of assets has several advantages. First, the chosen assets represent the main investment opportunities of a typical retail investor, i.e., well-diversified funds and individual stocks. Second, the fact that we elicit expectations for two assets also allows us to study the relation of beliefs across assets. In particular, we can explore whether expectations conform with common operationalizations of financial literacy stating that financially literate individuals should perceive investments into a mutual fund as safer than investments into a single company’s stock. The relation of expectations across assets might also help to shed light on the puzzling fact that many households invest significant fractions of wealth simultaneously in well-diversified mutual funds and in undiversified portfolios of individual stocks (e.g., Polkovnichenko, 2005). Skewness preferences have been used as one explanation for this observation but it has not been documented yet whether people actually perceive individual stocks as more skewed than funds. We find that respondents entertain very heterogeneous skewness expectations, disagreeing not only on its magnitude, but also on its sign. While some respondents expect quite positively skewed return distributions for an asset, others expect negatively skewed distributions with small chances of drastic losses. Our respondents expect higher levels of skewness than what is observed in historical data for both the stock and index. In line with previous research (e.g., Ameriks et al., 2019), respondents on average expect lower means and standard deviations of returns than historically observed. This underestimation is especially pronounced for the individual stock. Employing the LISS panel’s rich background information, we next ask whether the heterogeneity in expected skewness can be traced back to socio-demographic characteristics. However, we find that these characteristics only explain very little of the variation. Stock market expectations seem to be intrapersonally stable but interpersonally highly variable (Dominitz & Manski, 2011). We conclude our documentation of expectations with an analysis of how the expectations for the two assets relate to each other. The data confirm the common intuition that individual stocks are perceived as more risky than a broad index of stocks. Dividing respondents into eight groups based on whether they expect a higher mean, standard deviation, and skewness, respectively, for the stock or the index, reveals that 40% of the subjects expect a higher mean, standard deviation, and skewness for the stock. Another 21% expect higher standard deviation and skewness for the stock, but a higher mean for the index. To document the relation of stock market expectations and portfolio choice, we follow two approaches. First, we regress respondents’ initial investment in a given asset on the moments of respondents’ stated expectations for the asset. Respondents’ investments indeed increase with the skewness of the expected return distribution for each of the two assets. In other words, we find that respondents adjust their portfolio position in the stock in a way that is consistent with a preference for skewness. In a second step, we exploit the repeated nature of the experiments. Using data from both the initial and the follow-up experiments 6 months later, we relate changes in the moments of the expectations for the asset at the individual level to changes in each asset’s holdings. For the asset with more temporal variation in skewness expectations, i.e., the stock, we find a positive relation between the changes in the expected skewness of the return distribution and changes in the respondents’ positions. Our analysis contributes to several strands of literature. First, it complements a growing literature exploring the importance of skewness for investor behavior and asset prices. The study of skewness preferences in the stock market faces the problem that investors’ expected skewness is usually unknown. The existing empirical work uses an indirect approach to circumvent this problem by assuming that certain observable measures can serve as a proxy for expected skewness. Kumar (2009) and Barberis et al. (2016), for example, assume that investors form beliefs by extrapolating the skewness of past returns into the future. Bali et al. (2011) and Lin and Liu (2017) proxy expected skewness by the maximum return of a stock over a certain period in the past. Others have, in the spirit of rational expectations, used future returns (e.g., Mitton & Vorkink, 2007) or options market data (Conrad et al., 2013). These indirect approaches face two challenges. First, there is no consensus on what the best proxy for expected skewness is and over which time horizon it should be calculated. Second, models incorporating a preference for skewness are models about individual behavior. However, indirect approaches proxy expected skewness at the asset level and thus do not take heterogeneous expectations for a given asset across investors into account.Footnote 3 A number of papers in this literature document a negative correlation between the respective proxy for the expected skewness of an asset and subsequent returns. It is commonly suggested that a preference for skewness at the individual level is the mechanism driving this outcome, but this has been questioned (Barinov, 2018). One of our contributions is that we substantiate this mechanism by directly measuring expected skewness and showing that higher expected skewness at the individual level is linked to higher investment in an asset. More generally, our findings align with recent work (e.g., Das et al., 2019; Giglio et al., 2021; Kuhnen and Miu, 2017) that suggests that taking heterogeneity in expectations into account is an important and fruitful direction for future research. Second, our paper contributes to the literature studying higher order risk preferences in controlled experiments (see Trautmann & van de Kuilen, 2018, for a review). A number of studies find that subjects (ceteris paribus) prefer more positively skewed lotteries in simple binary choice tasks (e.g., Deck & Schlesinger, 2010; Ebert, 2015; Ebert and Wiesen, 2011; Noussair et al., 2014). However, these studies are by design mute on the role of expectations since probabilities, payoffs, and hence the skewness of the return distribution are known in these experiments. Our contribution to this literature is that we extend the analysis to a situation in which returns are unknown. Third, our findings also contribute to the literature on measuring stock market expectations. Due to the importance of stock market expectations as primitives of models of investor behavior, a substantial literature investigates how heterogeneous and accurate investors’ expectations are and how they relate to portfolio choices (e.g., Ameriks et al., 2019; Amromin & Sharpe, 2014; Breunig et al., 2021; Das et al., 2019; Dominitz & Manski, 2004; Drerup et al., 2017; Giglio et al., 2021; Hudomiet et al., 2011; Hurd & Rohwedder, 2012; Hurd et al., 2011; Kézdi & Willis, 2011; Kuhnen & Miu, 2017; Vissing-Jorgensen, 2003). However, possibly due to the prominence of the classical mean–variance framework (e.g., Markowitz, 1952), the literature has characterized investors’ subjective expectations using at most the first two moments. Our main contribution to the expectations literature is that we extend the study of stock market expectations to include skewness. To our knowledge, we are also the first to elicit expectations for a broad index and an individual stock and are thus able to analyse the relation of expectations. In addition, we differ methodologically from most of the literature by paying participants for the accuracy of their beliefs. Many qualitative features of our incentivized expectations (e.g., underestimation of mean returns) replicate those previously found without incentives, thus alleviating concerns about the reliability of the latter. Finally, unlike almost all previous work, we link expectations to investments in particular assets rather than stock market participation overall. In this aspect, our paper is closely related to Breunig et al. (2021). In their portfolio choice experiment, subjects can invest in a stock whose return is drawn from the actual historical return distribution of the German DAX. They show that the mean of individual respondents’ estimates for this distribution predicts investment in the experimental stock. However, they do not study expected skewness.",
26.0,1.0,Experimental Economics,16 December 2022,https://link.springer.com/article/10.1007/s10683-022-09784-5,Higher order risk attitudes: new model insights and heterogeneity of preferences,March 2023,Konstantinos Georgalos,Ivan Paya,David Peel,Male,Male,Male,Male,"It is now well established that many decisions in the fields of economics, finance and health are dependent not only on second order risk preferences but also upon higher order risk attitudes such as prudence and temperance (e.g. Bleichrodt et al., 2003; Deck & Schlesinger, 2014; Eeckhoudt & Schlesinger, 2013; Noussair et al., 2014; Trautmann & van de Kuilen, 2018; White, 2008). Models of Expected Utility Theory (EUT) that have been employed in the literature of higher order risk attitudes, such as mixed-risk aversion and mixed-risk loving, predict that a risk averse decision maker (DM) would exhibit prudence and temperance, while a risk lover DM would exhibit prudence and intemperance (see Deck and Schlesinger (2014), Eeckhoudt and Schlesinger (2006), Noussair et al. (2014)]. While there is some empirical evidence consistent with EUT models of mixed-risk aversion and mixed-risk loving, other experimental literature has provided evidence on higher order risk attitudes that are difficult to reconcile with EUT. For instance, many experimental studies reveal that imprudent choices, which are not consistent with the EUT models mentioned above, constitute a non-negligible share of total choices. The proportion of imprudent choices reported is sometimes close to forty percent, as illustrated in Table 1 of the review paper by Trautmann and van de Kuilen (2018). Other experimental findings reported are also inconsistent with models of EUT. Some examples are the lack of correlation between risk aversion and temperance (Bleichrodt & van Bruggen, 2022), a stronger correlation between third and fourth order risk preferences than between second and fourth order (Maier & Rüger, 2012; Ebert & Wiesen, 2014), strong correlation between odd and even moments of risk attitudes (Maier & Rüger, 2012; Noussair et al., 2014), and evidence of a “stakes effect” on higher order risk attitudes exhibited by the DM (Deck & Schlesinger, 2010). Reference-dependent models are more flexible and, in principle, capable of providing a framework able to accommodate this body of experimental evidence, as they have successfully accomplished with many other aspects of behavioural decision making. It is therefore surprising that, despite the relevance of reference dependence in explaining behaviour under risk and uncertainty, little is known about the predictions of those models regarding higher order risk preferences. Furthermore, taking into consideration that reference-dependent models lack clarity about the way the reference point is formed and that a variety of reference points ought to be considered when eliciting risk preferences [see Baillon et al. (2020)], our understanding of those models in relation to higher order risk attitudes under alternative reference points is even more limited. The norm in experimental research reported to date on lottery choices eliciting risk apportionment is that researchers have endeavoured to implement their appropriate reference point by experimental procedure and lottery design. This reference point is then assumed in the analysis of the responses of the experimental subjects. For example, Maier and Rüger (2012), Brunette and Jacob (2019), and Bleichrodt and van Bruggen (2022) report results where the status quo reference point is assumed. Alternatively, Deck and Schlesinger (2010) and Ebert and Wiesen (2014) assumed that the reference point was the expected value of the lottery. One of the early attempts to address the gap in the literature about reference dependence and higher order risk preferences can be found in Paya et al. (2022). They examine the behaviour with regard to third and fourth order risky choices of a DM using a cumulative prospect theory (CPT) specification under three alternative reference points. However, there are still empirical regularities and findings in experimental studies on higher order risk preferences that are not consistent with the predictions of the mainstream models of decision making under risk such as CPT and EUT. Our paper provides new insights within this literature and includes two contributions. First, we provide a comprehensive analysis about the implications for higher order risk preferences of a reference-dependent model consistent with Markowitz’s (1952) hypotheses, which we will refer to as the M model. These hypotheses include loss aversion, reference dependence, and the fourfold attitude to risk over binary lottery choices. Empirical and experimental evidence on second order lottery choices of decision making under risk is consistent with those hypotheses (e.g. Hershey & Shoemaker, 1980; Pennings & Smidts, 2000; 2003; Post & Levy, 2005; Scholten & Read, 2014).Footnote 1 Furthermore, the analysis by Georgalos et al. (2021) suggests that the M model can be a valuable complement to other reference-dependent and EUT models to explain risky binary lottery choices. The complementarity of the M model is also manifested by the fact that it can parsimoniously explain certain regularities observed in experimental studies that are difficult to reconcile with other models, such as violations of the separability principle underlying prospect theoryFootnote 2 [especially over gains, see Bouchouicha and Vieider (2017), Chark et al. (2020), Hogarth and Einhorn (1990)], or a reported high proportion of risk-seeking in lottery choices between a risky positive payoff(s) with a 0.5 probability(ies) and the safe alternative which has the same expected value (e.g. Battalio et al., 1990; Hershey & Shoemaker, 1980; 1985; Maier & Rüger, 2012; Vieider et al., 2015; Weber & Chapman, 2005). Given this potential for complementarity, Scholten and Read (2014) explore whether it is possible for a weighted-value model such as CPT exhibiting the fourfold attitude to risk over outcome probabilities, to also encompass the fourfold attitude to risk over outcome magnitude, as predicted by the M model. They present the condition needed for this to happen and suggest the use of a multiplicative model that includes a decreasingly elastic value function in conjunction with a probability weighting function. The applicability of such specification is, however, very limited because the condition is met for a very reduced range of parameter values (see Appendix A). Our analysis of the M model extends previous analyses in the literature since we examine for the first time its predictions over higher order risky choices from a number of alterative reference points. As noted by Baillon et al. (2020), when Markowitz (1952) introduced the reference-dependent utility theory, he was not explicit about the reference point to be used in his modeling framework, as was also the case for subsequent reference dependence models developed in the literature. We therefore examine the predictions of the M model assuming three reference points, namely, status quo, MaxMin, and expected value. All those three reference points are examined by Baillon et al. (2020), and two of those are identified as the most frequently employed when eliciting risk aversion under CPT, namely, status quo and MaxMin. The other reference point, the average payout, is incorporated in our analysis because it has previously been used in prominent studies on higher order risk preferences (Deck & Schlesinger, 2010; Ebert & Wiesen, 2014). We employ recently developed elicitation methods of risk preferences and show that the M model can explain high order risky choices as well as combinations of second with higher order choices not readily available from other reference-dependent models. This finding is particularly interesting from the status quo reference point. From this reference point, we are able to demonstrate a different prediction of the M model to that of the representative CPT, rank dependent utility (RDU) or EUT DMs. In particular, the representative CPT or RDU subject will, in common with the mixed-risk averse model of EUT, always make a prudent or temperate lottery choice whithin the gains domain. In contrast, a decision maker with M preferences can make choices consistent with any third and fourth order risk attitude from the status quo reference point dependent upon the precise magnitude of the lottery payoffs. Furthermore, the M model can accommodate a reflection effect on third order choices, a feature that is absent in other models [see Bleichrodt and van Bruggen (2022)]. Finally, the M model also enjoys the unique property of ‘higher-order reversals ’, implying that a given set of parameters predicts that the DM will exhibit a risk attitude for specific risks, and the reverse preference for others.Footnote 3 The second contribution is that this is the first paper we are aware of that uses an empirical approach that allows for more than one preference functional to fit experimental data on risk apportionment tasks eliciting higher order risk preferences. This is particularly relevant in this literature given there is no consensus about a single parametric specification that can accommodate the accumulated experimental evidence on higher order preferences [see Trautmann and van de Kuilen (2018)]. We employ the datasets from three of the most prominent studies on higher order risk attitudes, namely, Deck and Schlesinger (2010, 2014), and Nousssair et al. (2014) (DS10, DS14 and NTK, respectively, hereafter). The preference functionals include both an M specification and a CPT specification, each under three alternative reference points (and where EUT can be nested). Our results based on hierarchical Bayesian estimation methods show that both preference functionals are useful to describe the experimental evidence as there is not a single model that dominates the others across the three datasets. We point out that the higher the proportion and variety of (anti) risk apportionment choices of different orders, the more helpful the M model is to explain risky choices, and it therefore becomes a valuable complement to rank dependent preference functionals to account for high order risk attitudes. We assess the econometric method employed here through a set of simulations, and show that it can satisfactorily identify an assumed model specification, and discriminate among alternative ones. In addition to this empirical analysis, we set up a new experiment to further investigate heterogeneity of risky choices of different orders across subjects, and to discriminate between M and CPT specifications. The new insights about the M model we provide in this paper, together with existing analysis about higher order risk preferences in the CPT model (Bleichrodt & van Bruggen, 2022; Paya et al., 2022), reveal different predictions between the two models under the status quo reference point that are exploited in the design and analysis of the experiment to discriminate between the two models. A comprehensive simulation exercise shows that the experimental design has satisfactory discriminatory power between M and CPT specifications that can be considered as ‘representative’ of those models. The results of the experiment confirm the presence of heterogeneity of preference functionals and that, based on the Bayes factor, around a third of DMs are classified as consistent with the M specification and two thirds consistent with the CPT one. The rest of the paper is structured as follows. In Sect. 2, we set out the properties of a parametric M model of utility. In Sect. 3, we employ the framework developed by DS14 to illustrate the key properties of the M model for third and fourth order risky choices. In Sect. 4, we discuss the econometric method and the estimation results employing the experimental datasets from DS10, DS14 and NTK. Section 5 presents a new experiment on higher order risk preferences, and Sect. 6 provides a brief conclusion.",
26.0,1.0,Experimental Economics,04 June 2022,https://link.springer.com/article/10.1007/s10683-022-09758-7,Ostracism and theft in heterogeneous groups,March 2023,Alexandra Baier,Loukas Balafoutas,Tarek Jaber-Lopez,Female,Male,Male,Mix,,
26.0,1.0,Experimental Economics,21 November 2022,https://link.springer.com/article/10.1007/s10683-022-09775-6,Distributional preferences in adolescent peer networks,March 2023,Yonas Alem,Martin G. Kocher,Mikael Lindahl,Unknown,Male,Male,Male,"Many people have non-selfish preferences over the distribution of economic resources. These preferences are often synonymously called social preferences, other-regarding preferences, or distributional preferences (Fehr & Schmidt, 1999; Bolton & Ockenfels, 2000; Charness & Rabin, 2002; Camerer, 2003; Almås et al., 2010). Their existence and their specific nature are very important for economic behavior and outcomes, such as, among many others, cooperation (Boyd & Richerson, 2005; Fischbacher & Gachter, 2010), productivity (Carpenter & Seki, 2011; Bandiera et al., 2005; Dohmen & Falk, 2011), political preferences (Fisman et al., 2017; Kerschbamer & Müller, 2020), and well-being (Becker et al., 2012).Footnote 1 Recent studies have documented the evolution of these distributional attitudes in adolescence, from more malevolent at young ages to more benevolent when growing older. They have also stressed the large degree of individual heterogeneity of distributional preferences (Fehr et al., 2013; Almås et al., 2010; Martinsson et al., 2011; Sutter et al., 2018). There are far fewer studies on the effects of the social environment and peers on distributional preferences(Charness & Kuhn, 2007; Gächter et al., 2013; Fatas et al., 2018; Bicchieri et al., 2019). In particular, we know little about the early-life-peer influence on the emergence of distributional preferences and whether network members share distributional preferences (Hugh-Jones & Ooi, 2017). To fully understand how distributional preferences are shaped in adolescence, it is important to take the close social environment and its potential influence into account. Adolescent peer networks could be important in explaining adult inter-individual heterogeneity in distributional preferences, selection into friendship/professional networks, labor market status and political views later in life, on top of potential biological determinants (Balafoutas et al., 2012; Fisman et al., 2017; Kocher et al., 2013). Preferences could be correlated between members of social units, such as a child’s school or group of friends, beyond what is expected by the population preference distribution. Peer correlation in preferences can arise from selection into social networks whose members have similar preferences as one’s own, and through preference transmission. Besides composition, an adolescent’s position within the social network could itself be related to specific distributional preferences transmitted through various mechanisms. The potential impact of peer networks that are based on other-regarding attitudes goes beyond differential evolution of these preferences. If children are surrounded by like-minded peers, cognitive and non-cognitive abilities could also develop on different trajectories as a result of differences in cooperation and support within the network (Cunha et al., 2010; Thöni & Gächter, 2015). This paper investigates the distributional (“social”) preferences of children at primary schools in urban Tanzania and the role of peers in shaping these distributional preferences. We conduct a lab-in-the-field experiment and analyze to what extent distributional preferences of children are related to those of their peers at school, and what roles peer networks, school performance, and popularity play in explaining distributional preferences. The experiment involves choices between pairs of allocations that vary as to how much to allocate to oneself and to an anonymous passive agent (Kerschbamer, 2015). The variation in inequality in agents’ payoffs across allocations in the choice sets allows us to classify children into four broad distributional preference types: efficiency-loving, inequality-loving, inequality-averse, and spiteful. To study the prevalence and relationships of these types in peer networks, we ask children to name and rank their three best friends. We also use survey data on background characteristics and administrative data on school grades to investigate their relationship with distributional preferences and peers. The four distributional preference types that are used here capture a large set of potential distributional preferences under very mild assumptions see (Kerschbamer, 2015). Efficiency-loving preferences pertain to utility functions that put emphasis on the maximum of the sum of payoffs (also called “surplus maximizing motives”). Inequality-averse preferences put disutility on inequality, whereas inequality-loving preferences put positive utility on inequality. Finally, spiteful preferences capture a disutility that is increasing in the payoffs of others (also called “competitive preferences”). Our findings show that the majority of children exhibit choices consistent with inequality-averse (30.7%) and spiteful (42.3%) preferences. This pattern stems from a reluctance to accept disadvantageous allocations for oneself, even if they are Pareto improving. Peers’ preference types are also correlated. Even after controlling for a range of observable characteristics, we find that, if two children at the same school report a friendship link, they are 1.7% points (0.05 SD, mean = 0.33) more likely to exhibit the same preference type than if they do not. Thus, conditional on reporting a friendship link, distributional preference types of children are strongly related. This peer correlation in types is mainly driven by inequality-loving and spiteful types. Having a friend of the inequality-loving or spiteful type increases the likelihood of a child being of the same type by 6.7% points (0.2 SD) and 3.5% points (0.1 SD), respectively. The similarity in distributional preference types in peer networks differs by gender as well, with boys driving the overall peer correlations and showing stronger correlation coefficients for spitefulness and girls sharing inequality-loving preferences. Finally, our analysis shows that, besides network composition, the importance of the role of peers in explaining distributional preferences is linked to the position within the network. Worse relative performance in school relates positively to spiteful attitudes. The spiteful preference type is also more common when a child is central or popular within their peer networks. This suggests an importance of both social hierarchies and relative economic (human capital) position. Our contribution in this paper is threefold. First, we investigate the role peer networks play in shaping children’s distributional preferences. Hugh-Jones and Ooi (2017) study transmission of fairness preferences in teen friendship networks and show that observing others’ choices affects adolescents’ fairness norms. We build on Hugh-Jones and Ooi (2017) and contribute to a better understanding of the evolution of preferences with age, as well as their impact on (economic) outcomes. Second, we investigate the relationship between social hierarchies in networks and social preferences at a young age. An individual’s relative position within the social network may itself be related to distributional attitudes. We complement the view that parents’ socioeconomic status relates to the child’s social preferences (Benenson et al., 2007; Deckers et al., 2021) by exploring the structure of the child’s own social network and its relationship to distributional preferences. If children who are disadvantaged in terms of school performance or who are less popular among peers adopt antisocial attitudes toward peers, such attitudes could be reinforced and persistently shape outcomes of future interactions. Alternatively, in line with Girard et al. (2015), social structure and centrality in the social network can originate from individual preferences of children. Third, the documentation of nuanced measures of distributional preferences at a young age in a developing country context complements a series of studies that examine distributional preferences of children in high-income contexts (Martinsson et al., 2011; Fehr et al., 2013; Almås et al., 2010; Hugh-Jones & Ooi, 2017; Sutter et al., 2018). Distributional preferences in a setting with scarce financial resources, ethnic and religious diversity, and the absence of a welfare state, like urban Tanzania, may be of particular interest. Additionally, in an environment with high overall gender inequality, gender-specific preference formation at a young age may play an important role in explaining persistent outcome differences between males and females.Footnote 2 We therefore complement previous studies on overall and gender-specific distributional preferences of children (Benenson et al., 2007; Almås et al., 2010; Martinsson et al., 2011; Fehr et al., 2013; Sutter et al., 2018; Deckers et al., 2021).Footnote 3 Combining distributional preferences and social networks might ultimately provide a workable theory of reference groups. Standard models of distributional preferences remain silent on how reference groups are formed. Our results are a first step, and they show that empirical inference on reference group (network) formation is not easy, but that it can be achieved in an environment in which there is enough control. Schools are almost perfect laboratories in this sense, allowing us not only to study the emergence of distributional preferences, but also to learn more about general aspects of network formation along distributional preferences. The rest of the paper is structured as follows. In Sect. 2 we present our theoretical framework. Section 3 discusses the sample and data, Sect. 4 describes the experimental design in more detail, Sects. 5–6 present our results, and Sect. 7 concludes the paper.",
26.0,1.0,Experimental Economics,28 February 2022,https://link.springer.com/article/10.1007/s10683-022-09746-x,Correction to: The net effect of advice on strategy-proof mechanisms: an experiment for the Vickrey auction,March 2023,Takehito Masuda,Ryo Mikami,Takuma Wakayama,Male,,Male,Mix,,
26.0,2.0,Experimental Economics,20 February 2023,https://link.springer.com/article/10.1007/s10683-023-09794-x,Call for papers – Special issue in memory of Amnon Rapoport: Experimental Economics,April 2023,David Budescu,Ido Erev,Rami Zwick,Male,Male,Male,Male,,
26.0,2.0,Experimental Economics,20 September 2022,https://link.springer.com/article/10.1007/s10683-022-09761-y,How common is the common-ratio effect?,April 2023,Pavlo Blavatskyy,Valentyn Panchenko,Andreas Ortmann,Male,Male,Male,Male,"Expected Utility Theory (EUT) is one of the oldest and widely used criteria for decision making under risk. Bernoulli (1738) first proposed EUT to resolve the St. Petersburg Paradox. Von Neumann and Morgenstern (1947) provided a normatively appealing axiomatization of EUT. Yet, Allais (1953) challenged the descriptive accuracy of EUT with two examples. His first example (Allais, 1953, p. 527) is known as the Allais Paradox or common-consequence effect. Following MacCrimmon and Larsson (1979), his second example (Allais, 1953, p. 529) is known as the common-ratio effect. One illustration of this effect, due to Kahneman and Tversky (1979, p. 266), is that subjects choose $3,000 for sure over an 80% chance of winning $4,000 but choose a 20% chance of $4,000 over a 25% chance of $3,000 (when probabilities are scaled down by the same ratio, which gives the effect its name). A wide-spread perception is that the common-ratio effect is a robust behavioral regularity. More than 40 years ago MacCrimmon and Larsson (1979, Sect. 5.3, p. 369) concluded that “even though the common-consequence problem … is generally called the ‘Allais Paradox’, the [common-ratio effect] which is also due to Allais is more of a ‘paradox’ (if either is) in the sense that it elicits a higher rate of violation of the [EUT] axioms.” Given that the effect has since been tested in numerous studies, we can verify its experimental robustness. Our methodology is similar in spirit to meta-studies: we re-analyze experimental data collected in previous studies (meta-studies re-analyze previously published statistics). Blavatskyy et al. (2022) re-analyzed data from 29 articles (81 experimental designs/ parameterizations, 8,947 observations) on the Allais Paradox and found that the standard paradox was recorded in 38 designs (46.9%), no paradox was recorded in 27 designs (33.3%) and the reverse paradox was recorded in the remaining 16 designs (19.8%). Here we reexamine data from 39 articles with 143 experimental parameterizations of the common-ratio effect (14,909 observations). Out of these 143 designs, the standard common-ratio effect was found in 85 designs (59.4%), no common- ratio effect was found in 43 designs (30.1%), and the reverse common-ratio effect was found in the remaining 15 designs (10.5%). Our econometric analysis shows that the common-ratio effect is susceptible to similar effects of experimental design, implementation, and parameter choices as the Allais Paradox. Specific choices of an experimenter can make the common-ratio effect appear, disappear, or reverse. It is important to raise awareness of such effects to promote the development of appropriate non-expected utility theories that can rationalize these effects. The remainder of the paper is organized as follows. In Sect. 2, we briefly explain the common-ratio effect. In Sect. 3, we explain how we collected the data. In Sect. 4, we provide our regression analysis. In Sect. 5, we offer a concluding discussion.",1
26.0,2.0,Experimental Economics,15 April 2022,https://link.springer.com/article/10.1007/s10683-022-09751-0,Behavioral welfare economics and risk preferences: a Bayesian approach,April 2023,Xiaoxue Sherry Gao,Glenn W. Harrison,Rusty Tchernis,Unknown,Male,,Mix,,
26.0,2.0,Experimental Economics,20 April 2022,https://link.springer.com/article/10.1007/s10683-022-09755-w,Ambiguity and enforcement,April 2023,Evan M. Calford,Gregory DeAngelo,,Male,Male,Unknown,Male,"In the 18th century Jeremy Bentham proposed the panopticon as a mechanism for using monitoring uncertainty to influence the behavior of the monitored (Bentham, 1843). Bentham’s panopticon was a circular penitentiary where a single centrally located observer could watch the inmates in each cell without themselves being observed by the inmates. Inmates would be unaware of when or if the observer was watching their cell and, Bentham argued, would therefore behave as if they were always being watched. Evaluating Bentham’s hypothesis through the lens of modern utility theory, the success of the panopticon may depend on the uncertainty preferences of the monitored: an inmate who is sufficiently uncertainty averse might indeed behave as Bentham suggests, but an uncertainty seeking inmate might be relatively undeterred by an unknown probability of being monitored at any given instant.Footnote 1 If, instead, inmates were informed of the (truthful) instantaneous probability of being monitored the uncertainty seeking inmate may exhibit more compliant behavior because the revelation of the monitoring probability ameliorates some of the inmate’s uncertainty. Bentham’s panopticon illustrates the central question of this paper: when should a monitor reveal, or shroud, their monitoring probabilities? The decision to reveal or shroud monitoring probabilities depends not only on the uncertainty attitudes of the monitored, but is also affected by the incentives and motivations of the monitor. The monitor may seek to minimize non-compliant behavior, or they may seek to maximize the number of citations given for non-compliant behavior.Footnote 2 The applications of strategic manipulation of monitoring uncertainty are varied, although we focus on law enforcement as our leading example.Footnote 3 Law enforcement officers weigh their primary responsibility of “enforcing the laws that are enacted by elected officials ...and ...interpreted by the courts,” with other important duties including crime prevention (USDOJ, n.d.). Common policing strategies may support one goal yet hinder others. For example, hidden speed traps or unmarked vehicles have the potential to detect individuals that are behaving recklessly without providing forewarning about the presence of enforcement, while the use of signposted speed traps and marked patrol cars might allow the citizen to adjust their behavior to evade the law or even discourage individuals from breaking the law at all. Additionally, law enforcement activities could be driven by financial motives, especially if law enforcement revenues comprise a significant share of the financial inflows to a community (Goldstein et al. (2020); Kantor et al. (2017)). To explore the relationship between the use of unmarked police vehicles and law enforcement revenues, we present Fig. 1 which displays a negative relationship between the percentage of unmarked cars and the dependence of a location on revenues from law enforcement fines and fees.Footnote 4 Share of Government Revenues from Law Enforcement Fines and Fees & the Use of Unmarked Police Vehicles. \(N=1606\), bubble size corresponds to the number of observations in each bin. Data from the 2013 Law Enforcement Management and Administrative Statistics database and 2013 US Census Figure 1 is consistent with the logic underlying Bentham’s panopticon. When the fine-to-tax ratio is low, police officers are focused on crime prevention: the use of undercover vehicles increases uncertainty about the level of traffic monitoring which causes uncertainty averse citizens to behave more cautiously. In contrast, when the fine-to-tax ratio is high, police officers have an incentive to write more citations: the use of marked vehicles reduces uncertainty about the level of traffic monitoring which may lead to more traffic violations and increased revenue. However, drawing causal inferences from this data is extremely challenging. In each jurisdiction, the decision of whether to reveal or shroud monitoring activities will depend on the cultural, historical, legal and institutional environment in which the decision makers are embedded.Footnote 5 Nevertheless, the data represented in Fig. 1 is, to the best of our knowledge, the “best” observational data available on this topic. It is precisely because of the lack of data that allows for clean identification regarding the revelation of monitoring activities that we turn to a theoretical and experimental study of the question. We abstract away from the confounding factors mentioned above and use a unifying framework that can be broadly applied.Footnote 6 Indeed, there is a considerable body of similar work related to compliance and enforcement including Cason et al. (2021); Slemrod (2019) that we contribute to in this research.Footnote 7 Notably, Salmon and Shniderman (2019) study the use of ambiguous punishment in a framed tax compliance experiment and found only limited evidence that subjects exhibited risk aversion. We build a theoretical model to identify the equilibrium effects of the revelation or shrouding of monitoring probabilities, and then test the equilibrium predictions in a laboratory experiment. Importantly our model does not assume expected utility, and our experimental results include evidence of non-neutral ambiguity preferences. For concreteness, we present our model within the context of police monitoring and enforcement of speeding violations. We implement our model by positing that a driver has a choice of two roads, and that law enforcement may allocate their monitoring resources across both roads. The more resources placed on a given road, the greater the probability that a driver who speeds on that road will be caught and punished. The following describes the stages of the experiment: 
 Stage 1: Enforcement Officer chooses monitoring probabilities \(m_A\) and \(m_B\) for the first and second road, and chooses whether to reveal \(m_A\) and \(m_B\) or only reveal \(m=m_A+m_B\). Stage 2 : The Driver decides to either speed on the first road (choice A), speed on the second road (choice B) or not speed (choice C). Stage 3 : Officer and Driver payoffs are obtained. There are two payoff schemes that are implemented for the drivers, using a between treatment design, labeled the Prob and EV treatments. If a driver chooses A, then they either (Prob) earn 100 points with probability \(0.9-m_A\) and earn 0 with probability \(0.1+m_A\), or (EV) earn \(90-100m_A\). Similarly, B returns either (Prob) 100 points with probability \(0.9-m_B\) and nothing otherwise, or (EV) \(90-100m_B\) points. If the driver chooses C they receive either (Prob) 100 points with probability 0.5 and nothing otherwise, or (EV) 50 points. In each case, the EV treatment pays the expected value of the Prob treatment lottery. The officer pays a constant marginal cost of monitoring and can decide whether to reveal the monitoring probabilities (\(m_A\) and \(m_B\)) to the driver, or only reveal the total (\(m_A +m_B\)) monitoring probability.Footnote 8 We study two different payoff structures for the enforcement officer: one where the officer strictly prefers the driver to not speed and choose C (i.e. the officer wishes to minimize the number of speeding violations), and one where the officer’s payoff is increasing in the monitoring probability on the road chosen by the driver (i.e. the officer earns revenue from catching a speeding driver). This design allows us to address our central research questions. (1) How does monitoring uncertainty affect behavior? (2) Does an enforcer manipulate monitoring uncertainty to induce favorable behavior among the monitored? (3) Is there a distinction between enforcers with a revenue incentive and enforcers with a prevention incentive? The experimental design also allows the underlying components of uncertainty preference, risk preference and ambiguity preference, to be partially separated. We define a risky environment to be one where probabilities are known, such that there is uncertainty over outcomes, and an ambiguous environment to be one where probabilities are unknown such that there is uncertainty over probabilities. Thus, naturally, uncertainty (i.e. the absence of certainty) can be categorized as the union of risk and ambiguity. In the EV treatment the agent faces certainty when monitoring probabilities are observable, and faces uncertainty when monitoring probabilities are unobservable. Therefore, the difference in driver behavior between the two monitoring environments in the EV treatment identifies the union of risk and ambiguity preferences. In the Prob treatment the agent faces risk when monitoring probabilities are observable, and faces uncertainty when monitoring probabilities are unobservable. Therefore, the difference in driver behavior between the two monitoring environments in the Prob treatment identifies ambiguity preferences.Footnote 9 Given this structure of the experimental design, we infer preferences from driver behavior rather than measure preferences via a separate measurement task. We return to these themes in Sect. 3. Our key theoretical result is to identify when law enforcement should shroud monitoring activity and when law enforcement should reveal monitoring activity as a function of the underlying preferences of officers and drivers, which is summarized in Table 1. When the officer is motivated by revenue (prevention) incentives then, in equilibrium, they will reveal (shroud) their monitoring strategy when the driver is uncertainty averse and shroud (reveal) their monitoring when the driver is uncertainty seeking. Intuitively, the officer discourages law breaking (by shrouding when facing an uncertainty averse driver) when motivated by prevention incentives but encourages law breaking (by shrouding when facing an uncertainty seeking driver) when motivated by revenue incentives. The more people who break the law, the more people that can be caught.Footnote 10 Our experimental results are broadly consistent with our model’s equilibrium predictions. When monitoring levels are observed officers monitor at near equilibrium rates and drivers typically best respond, particularly in the EV treatment. When monitoring is unobserved, we find evidence that drivers do not exhibit uncertainty neutrality. In cases where the aggregate monitoring level is high, we observe evidence of uncertainty aversion. On the other hand, in cases where the aggregate monitoring level is low, we observe evidence of uncertainty seeking. This pattern of behavior could be explained by either heterogenous uncertainty preferences across subjects, or that some subjects exhibit prospect-theory-like uncertainty preferences.Footnote 11 Given the observed behavior of drivers it is optimal for officers to always reveal their monitoring strategies, and officers do reveal their monitoring levels approximately 70% of the time. While our theoretical model is necessarily highly stylized the intuition behind the equilibrium is both intuitive and broadly applicable. When monitoring agents are uncertainty averse (uncertainty seeking), increasing uncertainty in the level of monitoring at any given location or instant will cause the agents to undertake safer (riskier) actions. Knowing this, the level of monitoring uncertainty can be manipulated to induce favorable behavior from the monitored agents. While our theoretical results apply to a specific domain, our experimental results suggest that the underlying logic does indeed generalize to at least one related domain; specifically, the domain where utility maximizing agents are replaced with human decision makers. The paper proceeds as follows. Section 2 identifies connections between our paper and previous work on monitoring and enforcement in the domains of law enforcement and auditing, as well as existing work on the role of strategic uncertainty and ambiguity aversion. Section 3 describes our theoretical model and experimental design. Section 4 discusses the implementation of our experimental design. Section 5 presents the results of the experiment. Section 6 discusses the results and concludes the paper.",
26.0,2.0,Experimental Economics,11 July 2022,https://link.springer.com/article/10.1007/s10683-022-09764-9,Entry and exit decisions under public and private information: an experiment,April 2023,Aleksei Chernulich,John Horowitz,Manizha Sharifova,Male,Male,Unknown,Male,"A large number of retail transactions occur on centralized online platforms which allow sellers and buyers to seamlessly exchange goods and services. These transactions are easily recorded, often in real time, and offer sellers rich data on consumer preferences. In turn, these data can be used by sellers to forecast future demand with greater precision. Typically, the production decision involves the question of how much to produce, or the intensive margin, and whether to produce, or the extensive margin. The latter is also known as a market entry decision. In this paper, we conduct a laboratory experiment to study how different payoff information affects the market entry decision. Understanding how subjects respond to information is important for maximizing the profit of market participants, including buyers, sellers, and the designer of the market platform. Profit is often tied to the spell or length of market participation, which is state-dependent. Our experiment allows us to study both components: (i) market entry/exit and (ii) the length of market participation. In the field, studying market entry (or exit) decisions under different information sets may be costly, if not impossible when researchers cannot alter market characteristics. A laboratory experiment can overcome these difficulties by providing complete control over the information available to market participants. In this paper, we present a novel experiment in which the market participants have access to counterfactual information in one environment, but not the other, and in which this decision is reversible. Our primary research question is how the information provided to a subject affects their entry decision. While market transparency for buyers is an actively developing field,Footnote 1 our experiment allows us to study the impact of transparency on the market’s supply side. In our environment, market participants can switch between IN and OUT decisions at any time. The payoff to IN follows a stationary AR(1) process, which reflects market conditions. A participant does not have any market power, and therefore takes prices as given. The payoff to OUT is less than the expected payoff to IN. For simplicity, we assume zero transaction costs for switching between IN and OUT and provide subjects with information about the risky payoff generation process. In the private information treatment, which is a typical bandit problem, participants select between a safe arm (OUT), and a risky arm (IN), where the platform provides information in almost real-time. If a participant selects OUT, then the participant will not be able to observe the risky payoff. In order to observe the return to the risky arm, this option must be selected. In the public information treatment, we provide information on the risky payoff regardless of whether the participant is IN or OUT. To draw predictions for IN and OUT decisions across both treatments, we assume that agents forecast their future revenues under two different rules: (i) rational expectations, and (ii) behavioral expectations which allow for the well-known forecasting biases such as extrapolation and stickiness (Landier et al., 2019). While both rules suggest that IN should be observed more often under public information, we find that subjects select IN more often when information is private. This result suggests that there is a demand for information, and that participants do not exhibit a high degree of risk aversion since they are willing to explore the risky option. These results are also supported by other bandit experiments in the lab (Hoelzemann & Klein, 2021).Footnote 2 Our experimental design is motivated by the market entry decisions when information flows rapidly. We modify the standard bandit problem found in economics and finance (Bergemann & Välimäki, 2008) to study reversible entry decisions under different disclosure rules. In a related experiment, Grosskopf et al. (2006) find that providing counterfactual information in a bandit problem can increase risky behavior. However, this result disappears with more experience, suggesting that subjects become less sensitive to additional information over time. Yechiam and Busemeyer (2006) show that counterfactual information can increase risk taking when the negative outcome is rare and large. Biele et al. (2009) employ a Markov process with two states for the risky option, H and L, which are unknown to subjects. They find that players do not learn to become risk averse. In our experiment, we enrich the set of possible outcomes by providing more opportunities for the subjects to familiarize themselves with the payoff realization process due to the nearly continuous environment.Footnote 3 To formulate our predictions, we draw from literature on expectation formation when the predicted variable is exogenous.Footnote 4 According to Assenza et al. (2014) and Mokhtarzadeh and Petersen (2020), having access to historical data in forecasting experiments can encourage more adaptive and trend-chasing expectations. Landier et al. (2019) asked subjects to forecast 40 realizations of a risky asset and found evidence of both sticky (Coibion & Gorodnichenko, 2015; Bouchaud et al., 2019), and extrapolative (Bordalo et al., 2018) expectations.Footnote 5 We simplify the forecasting tasks by focusing on the binary decision of IN or OUT, which indirectly measures market expectations. Our findings complement existing experimental literature on switching behavior (Anufriev et al., 2016, 2018 and Anufriev et al., 2019). In these experiments, participants were offered an opportunity to switch between investment alternatives. In contrast to our paper, participants in these studies (i) were not informed about the payoff generating process, and (ii) were evaluated only in a public information environment.",
26.0,2.0,Experimental Economics,25 September 2022,https://link.springer.com/article/10.1007/s10683-022-09773-8,Improving the statistical power of economic experiments using adaptive designs,April 2023,Sebastian Jobjörnsson,Henning Schaak,Tim Friede,Male,Male,Male,Male,"Imagine an economist being involved in a research project that aims to improve an economic outcome, e.g the productivity of small businesses. Within the project, it is planned to carry out a field experiment to test how their productivity could be improved. During the planning phase, a number of potential treatments, for example educational interventions, are identified through discussions with project partners, colleagues and stakeholders. Unfortunately, the research funds of the project are too limited to test all treatments with sufficient power. Conventionally, the economist would either have to discard some of the treatments (or even consider only one treatment) or run an under-powered experiment. In practice, the researcher may not be confident enough in the prior beliefs about the treatments’ effects to make such a decision. It may also be the case that there are strategic reasons to not leave some treatments untested. This paper discusses a third option for such cases, which might be considered a compromise between the extremes of selecting treatments prior to the trial and selecting treatments once data of full-sized trials are available, namely to use adaptive designs, which aim to use the available resources to maximize the probability of demonstrating at least one of the expected effects with sufficient power. This is achieved by modifying the experimental design after one or several interim analyses. While such adaptive designs have been applied in other fields (such as clinical trials) for decades (cf. Bauer et al., 2016), the concept has only recently become a topic of interest in the economic literature (Kasy & Sautmann, 2021)Footnote 1. When investigating multiple hypotheses (potentially multiple times, in case of an adaptive design), an additional threat to the validity of the final conclusions stems from an inflation of the Type I error probability. This fundamental issue, also referred to as family-wise error rate (FWER) inflation, often arises in experimental research, where multiple treatments are frequently studied (List et al., 2019). Still, it has gained the interest of experimental economists only recently (List et al., 2019; Thompson & Webb, 2019). The main goal of adaptive designs is to improve the power of the experiment to detect any potential (at least one) statistically significant effect. Ensuring sufficient statistical power has been acknowledged as an important issue in economic research for some time (De Long & Lang, 1992), and remains a challenge in empirical work (Ziliak & McCloskey, 2004; Ioannidis et al., 2017). In the aftermath of the replication crisis in psychology (see e.g. Pashler and Wagenmakers, 2012; Open Science Collaboration, 2015) the issue of insufficient power has gained additional interest, particularly in experimental economics where researchers have been encouraged to account for it (Canavari et al., 2019; List et al., 2011; Czibor et al., 2019). In principle, higher levels of power can be achieved by simply increasing the sample sizes. However, these are often limited in practice. Researchers may face budget or time-related constraints, or have only a small available pool of potential experimental subjects, either due to a small target population or due to restricted access to the wider population. In experimental economics, methods to investigate the required sample sizes in order to detect hypothesised treatment effects predominantly assume an experiment which is of a fixed size in the planning phase (cf. Bellemare et al., 2016). In this contribution, we demonstrate how to use and evaluate adaptive designs in situations where one wishes to combine the selection of interventions with confirmation of their effects by hypothesis tests including data pre and post adaptation, while still controlling the Type I error probability at a specified level. Although the approach is not new from a methodological viewpoint, its application has mostly been in the area of medical statistics, in particular clinical trials. We wish to widen the scope of application by showing how it can be used to design and analyse economic experiments. As previously mentioned, the economic literature covering adaptive designs is currently limited. For a review of applications in other fields, see e.g. Bauer et al. (2016). Bhat et al. (2020) develop an approach to classical randomisation in experiments with a single treatment for situations where the treatment effect is marred by the effects of covariates and where individual subjects arrive sequentially. Similarly, for experiments where subjects in the sample are observed for multiple periods Xiong et al. (2022) develop an algorithm for staggered rollouts, which optimises the share of treated subjects at different time points. With respect to experiments with multiple treatments and a binary outcome, Kasy and Sautmann (2021) develop an algorithm for experiments with many data collection rounds. These methods are tailored to specific settings. The framework to be used here can be applied under a broader set of conditions but imposes stronger changes on the experiment. While the aforementioned approaches adjust the treatment assignment probability continuously, treatments are potentially dropped from the experiment (i.e. reducing the assignment probability to zero) in the following. Still, the framework is flexible with respect to the applied adaptation rules and allows for multiple interim analyses, which can be combined in different ways. Although it allows for an arbitrary number of interim analyses, we will restrict the illustrations to two-stage designs, containing only a single interim analysis. This simplifies our presentation, but is also a reasonable choice in many practical settings. We use available software (the R package asd) to illustrate the principal concepts and show how they can be applied to achieve a more efficient use of available resources in settings where other approaches are not feasible. We want to highlight that the present illustration focuses on settings in which the goal of the analysis is to select and test hypotheses regarding the experimental treatments. The improved power for the selected hypotheses comes at the cost of potential biases of the estimated treatment effects. Also, when hypotheses are tested that were not initially considered in the design, these biases can become more relevant (Hadad et al., 2021). Thus, it is important to note that effect estimates obtained from adaptive designs can require additional adjustments (cf. Sect. 4), which should be considered by investigators when applying an adaptive design. Throughout the paper, the potential of the method is illustrated by three examples. The scenario from the first paragraph will be used to develop a hypothetical example to illustrate the principle considerations researchers have to consider when designing adaptive experiments. The second and third examples use real data from two experimental studies in order to illustrate the potential of adaptive designs in real-data applications. The second application (Musshoff & Hirschauer, 2014) uses a framed field experiment, based on a business simulation game, to study the effectiveness of different nitrogen extensification schemes in agricultural production. The experiment was carried out with 190 university students and contained treatments varying in two dimensions: whether nitrogen limits were voluntary or mandatory, and whether the corresponding payments were deterministic or stochastic. The students were asked to act as if they were farmers in the experiment. Although the policy treatments do not differ in their impact on the profitability, the authors find that students respond differently to the treatments. Most importantly, the authors report that penalty-based incentives perform better than allowance-based ones. The third example (Karlan & List, 2007) uses a natural field experiment to study the effects of different matching grants on charitable donations. The experiment targeted previous donors of a non-profit organisation, and considered treatments which varied in three different dimensions (the matching ratio, the maximum size of the matching grant and the suggested donation amount). The original study is based on a large sample, including 50,083 individuals. The authors find that matching grant offers increase the response rate and revenue of the solicitations, but that different ratios of the matching grant have no effect overall. The two studies are chosen as they differ in several aspects relevant for economic experiments. First, they represent different experiment types, namely natural field experiment vs. framed field experiment (cf. Harrison and List, 2004). Although the experiment described by Musshoff and Hirschauer (2014) ensured incentive compatibility, the experiment contained hypothetical decision making, whereas the donation decisions in Karlan and List (2007) were non-hypothetical. Moreover, the study of Musshoff and Hirschauer (2014) analyses data from a so-called convenience group. Second, the experiments differ with respect to their complexity (participants having only to decide upon whether to donate and the respective amount vs. participants having to play a complex multi-period business simulation game). Finally, the studies vary with respect to the sample sizes as well as the reported effect sizes. This allows us to illustrate the general usefulness, and potential pitfalls, of adaptive designs for applications in experimental economics. In order to simplify the presentation and keep the focus on the illustration of the method, we only consider subsets of the original data in the applications. The remaining content of this paper is structured as follows. Section 2 introduces the necessary concepts for the design and analysis of adaptive designs (with a single interim analysis). This includes the necessary theory for the multiple testing and the combined testing of experimental stages, as well as the general concept of the simulations for the power analyses in the initial design phase. In Sect. 3, we apply and illustrate two-stage designs for the three examples and compare their performance to single-stage designs. Section 4 concludes with a discussion of more advanced variations of adaptive designs further explored in other parts of the literature.",3
26.0,2.0,Experimental Economics,21 October 2022,https://link.springer.com/article/10.1007/s10683-022-09777-4,Inequality as a barrier to economic integration? An experiment,April 2023,Gabriele Camera,Lukas Hohl,Rolf Weder,Female,Male,Male,Mix,,
26.0,2.0,Experimental Economics,26 October 2022,https://link.springer.com/article/10.1007/s10683-022-09776-5,"Paid and hypothetical time preferences are the same: lab, field and online evidence",April 2023,Pablo Brañas-Garza,Diego Jorrat,Angel Sánchez,Male,Male,Male,Male,"Patience is becoming a major topic in economics.Footnote 1 Patience refers to the preference for larger rewards in the future over smaller rewards at a sooner date. Time discounting (TD) is the loss of utility associated to any reward that is deferred in time. Patient individuals, therefore, display lower TD than impatient individuals. Time preferences are relevant in several domains. In the field of health behavior, there is evidence suggesting that experimental measures of individuals’ patience negatively correlate with alcohol consumption, smoking behavior, and BMI (Borghans & Golsteyn, 2006; Chabris et al., 2008; Sutter et al., 2013). In education, the evidence suggests that subjects with a high level of patience have better education outcomes (Castillo et al., 2019; Duckworth & Seligman, 2005; Golsteyn et al., 2014; Kirby et al., 2005; Non & Tempelaar, 2016; Paola & Gioia, 2014) and are less likely to receive disciplinary referrals in school (Castillo et al., 2011) or drop out of high school or college (Cadena & Keys, 2015). As regards the field of finance, patience is correlated with income, savings, and credit card borrowing (negatively) and is a good predictor of real-life wealth distribution (Ashraf et al., 2006; Epper et al., 2020; Giné et al., 2017; Meier & Sprenger, 2010, 2013; Tanaka et al., 2010). In other domains, patience has also been correlated with cognitive ability (Bosch-Domènech et al., 2014; Dohmen et al., 2010; Frederick, 2005), criminal behavior (Äkerlund et al., 2016), the probability of divorce (Paola & Gioia, 2017; Schaner, 2015), and social behavior (Dewitte & Cremer, 2001; Espín et al., 2012, 2015; Rachlin, 2002). TD is typically elicited using multiple price lists (MPL; Coller & Williams, 1999) where subjects decide whether to take (the “sooner” option) or save (the “later” option) a certain amount of money over a series of independent choices with increasing interest rates.Footnote 2 Although for experimental economists the use of monetary incentives has been practically a hallmark, some of the most acclaimed papers on TD (e.g., Ashraf et al., 2006; Cadena & Keys, 2015; Golsteyn et al., 2014; Kirby et al., 2005; Sunde et al., 2022) do not pay participants real monetary incentives, that is, decisions are hypothetical. But are choices over hypothetical rewards as equally informative as incentivized time preferences? Besides the obvious monetary and logistics costs, the use of monetary incentives in TD is challenging. First, transaction costs and payment reliability need to be constant across options regardless of the payment date (Cohen et al., 2020). For example, future payments must be just as reliable as immediate payments. This problem is more prominent in the field, especially in low-income populations where most people are frequently unbanked or change their cell-phone numbers. All these factors increase the uncertainty associated with future payments and hence the probability that a subject will prefer the sooner option for reasons other than time preferences. Second, subjects’ choices only reflect their time preference if they are liquidity constrained and cannot engage in intertemporal arbitrage (Frederick et al., 2002; Lührmann et al., 2018). Otherwise, subjects should compare the interest rate offered in the task (r) to the market interest rate (\(\overline{r}\)). For \(r< \overline{r}\), subjects can just take the money today and deposit it in the bank. For \(r> \overline{r}\), they should save all in the task. Third, a higher expectation of inflation may lead an individual to prefer sooner smaller rewards without the influence of time preference, simply because the money is worthless in the future (Frederick et al., 2002; Martín et al., 2019).Footnote 3 Fourth, there is a severe problem with data privacy. If subjects need to be paid by bank transfer, then they must disclose private information. Relatedly, there is growing interest in the development, malleability, and stability of time preferences (Alan & Ertac, 2018; Kim et al., 2018; Lührmann et al., 2018; Perez-Arce, 2017). These studies employ adolescent (or children) samples for which the use of real money requires special parental consent and additional requirements from ethics committees. Thus, if the use of real incentives in TD elicitation is expensive, may induce biased estimates, and is, in general, so problematic, why do we pay decisions at all? Previous evidence directly comparing both mechanisms (i.e., real vs. hypothetical) is typically based on lab experiments with small samples (6 to 60 subjects), and low statistical power. Using a within-subject design, Madden et al. (2003) compared hyperbolic discount rates estimated from students’ choices over incentivized vs. hypothetical rewards and found no differences. Johnson and Bickel (2002), also using a within-subject design, found no significant differences between incentivized and hypothetical choices. Madden et al. (2004) replicated the (null) results using a between-subject design. Bickel et al. (2009) compared TD choices with real and hypothetical money using neuroimaging and found no significant behavioral or neurobiological differences. Similar results were reported in Lawyer et al. (2011) and Matusiewicz et al. (2013). In contrast, Coller and Williams (1999) found a weak significant difference between both mechanisms, but assignment of subjects to treatments was not random. Evidence from field experiments is even scarcer. Harrison et al. (2002) examined in Denmark whether variations in the Between-Subjects Random Incentive System (BRIS) had an impact on TD. Keeping the number of paid subjects per session fixed, they found no significant effect of session size (i.e., of paying probability). Ubfal (2016) estimated discount rates for six different goods among rural Ugandan households and found no significant differences between hypothetical and incentivized choices. In both studies, however, participants were not randomly assigned to treatments. Besides, Ubfal (2016) does not control for order effects. Apart from these papers, Falk et al. (2016) validated a hypothetical TD task using a real incentives task. They found that both measures were highly correlated and predicted real-life behaviors similarly. In addition, Matousek et al. (2022) meta-analyzed 56 studies (lab and field experiments) and showed that discount rates do not differ between experiments using real and hypothetical rewards. This paper aims to shed light on this knowledge gap and quantify the impact of hypothetical vs. real rewards on TD elicitation. We study this in a comprehensive manner across different populations and settings. We analyze data from a lab experiment in Spain, a lab-in-the-field experiment in rural Nigeria, and two online experiments with participants from the UK. In addition, we examine the impact of another commonly used payment method consisting of paying only a fraction of subjects (BRIS). In all cases, subjects were randomly assigned to one of the three different payment mechanisms and the data allow us to test for effects on both short- and long-term discounting decisions as well as on the individual parameters obtained assuming quasi-hyperbolic preferences (i.e., the beta and delta discount factors). Our results contribute to the debate and suggest that choices over hypothetical rewards do not differ from choices over real rewards in TD tasks. In fact, we believe that the accumulated evidence now allows us to conclude that hypothetical rewards do not induce relevant estimation bias. Furthermore, while our results for BRIS payment schemes are slightly more ambiguous, they also suggest little if any difference with respect to real rewards. However, further studies should analyze BRIS in more detail, for example, considering different payment probabilities (we only test for 1/10 probability). A notable feature of MPLs (and other experimental methods) is that they may yield biased discount rates as a consequence of the linear utility assumption. This assumption leads to upward-biased discount rate estimates if utility is concave (Andersen et al., 2008; Andreoni & Sprenger, 2012a). A solution to this issue is to use a double MPL method, to jointly estimate risk and time preferences.Footnote 4 While our data do not allow us to accurately estimate time and risk preferences, in the Online Appendix (OA hereafter) we provide estimates of TD while controlling for risk preferences in some of our datasets.Footnote 5 The main results are not qualitatively affected. These results are more than a purely methodological contribution. They have important implications for the budgets of research groups running TD experiments, especially in the field in developing and volatile economies, as well as for the design of large-scale representative surveys which increasingly include behavioral tasks to estimate economic preferences in the population. The rest of the paper is structured as follows. The second section focuses on the main research questions and the third reports on general features of the design. Section four provides an overview of the four studies. In section five, we report the main results. In the last section, we discuss the results and the contribution of the paper.",3
26.0,2.0,Experimental Economics,23 November 2022,https://link.springer.com/article/10.1007/s10683-022-09756-9,What drives conditional cooperation in public good games?,April 2023,Peter Katuščák,Tomáš Miklánek,,Male,Male,Unknown,Male,"Casual observation as well as an extensive experimental literature (Ledyard, 1995) document that people voluntarily contribute to public goods. This observation is squarely at odds with the traditional model of self-regarding preferences. Under this model, each individual has a strictly dominant strategy of free-riding (i.e., contributing zero). Most of the existing explanations of this empirical regularity rely on existence of social preferences.Footnote 1 Although positive voluntary contributions can be explained by maximization of social welfare (Laffont 1975) or altruistic/warm-glow preferences (Becker, 1974; Andreoni, 1989, 1990), predictions of these theories within the linear public good game, a workhorse of research in this area, do not square well with empirical evidence. In particular, while these theories predict that an individual contributes the same amount no matter how much the others contribute, Fischbacher et al.,, (2001) (henceforth FGF) document that a sizable group of subjects contribute more if the others on average contribute more as well. They call this empirical pattern “conditional cooperation” (henceforth CC). The authors classify about one half of their subjects as conditional cooperators (henceforth CCs), one third as free-riders (contributing zero regardless of the average contribution of the other group members), and the rest as fitting other (or no particular) patterns. These findings have later been replicated by numerous laboratory studies (Thöni & Volk, 2018). Moreover, multiple studies in the labFootnote 2 and in the fieldFootnote 3 document a positive correlation between contributions and historical contributions or beliefs about current contributions of others, suggesting presence of CC. It is not very well understood, however, what preference and decision-making patterns drive CC and what their relative roles are. CC could be driven by reciprocity (to perceived intentions behind others’ contributions), conformity (to others’ contributions regardless of payoff consequences), aversion to payoff inequality (in comparison to others regardless of their intentions), and other residual factors. Reciprocity is a kind (unkind) response to an action by others that is perceived to be driven by their kind (unkind) intention (Rabin, 1993; Dufwenberg & Kirchsteiger, 2004; Falk & Fischbacher, 2006) or by their generous (ungenerous) type (Levine, 1998; Rotemberg, 2008; Gul and Pesendorfer, 2016). Conformity is an act of following an observed behavior of others. It could arise due to adherence to a (perceived) social norm (Axelrod 1986; Bernheim, 1994; Fehr & Fischbacher, 2004, a.k.a. “normative conformity”) or due to social learning about an optimal decision (Bikhchandani et al., 1998, a.k.a. “informational conformity”). Inequality aversion is a willingness to take action in order to reduce material payoff inequality between oneself and others irrespective of whether the inequality originates from intentions of the others or not (Fehr & Schmidt, 1999; Bolton & Ockenfels, 2000). Residual factors include any other alternative explanation of CC. Regarding the residual factors, we speculate that the most important ones include anchoring and confusion. Anchoring is an act of letting one’s decisions be influenced by payoff- and belief-irrelevant numerical cues (Tversky & Kahneman, 1974). (Subject) confusion (Andreoni, 1995; Keser, 1996; Houser & Kurzban, 2002) can be thought of as an imperfect “game form recognition” (Chou et al., 2009) in that subjects fail to properly understand how players’ strategy combinations map to their payoffs and, consequently, fail to recognize what would constitute an optimal strategy given one’s own preferences.Footnote 4 The possibility that laboratory-observed CC is driven by confusion has been illustrated by Ferraro and Vossler (2010) and Burton-Chellew et al., (2016). These two studies find that when subjects play the public good game against computers using the FGF design, with nobody else benefiting from their contributions, the classification into conditional contribution types results in a distribution remarkably similar to that of FGF and its replications. In particular, the share of CCs is approximately 50%. All this happens despite subjects having to answer control questions that are supposed to assure understanding of the instructions. Moreover, Burton-Chellew et al., (2016) document that CCs, as opposed to free-riders, are more likely to misunderstand the game. Knowing more about the relative strength of the four potential drivers, apart from being interesting on its own, has important implications for how to interpret, extrapolate, and, in a fundraising setting, also exploit empirical findings on CC. Traditionally, CC has been approached as an all-encompassing reflection of cooperative behavior or, at a more granular level, as a reflection of reciprocity, inequality aversion or conformity. In the laboratory setting, however, this view has been challenged by the results of Ferraro and Vossler (2010) and Burton-Chellew et al., (2016). The latter go as far as to suggest that laboratory-observed CC might, in essence, be a data pattern driven purely by confusion. Given the prominence of the FGF method in measuring CC, it is important to shed more light on the role that confusion (and anchoring) play in such measurement. In the fundraising field setting, understanding the relative role of various drivers of CC is likely to be useful for choosing the type of “social information” to be presented to would-be contributors. If CC is driven by conformity, then behavior of any present or historical reference group of contributors can be used to motivate higher contributions.Footnote 5 If CC is driven by reciprocity, it might be necessary to refer to a group of earlier contributors in the current fundraising campaign instead. If CC is driven by fairness concerns (i.e., inequality aversion), it is important to carefully consider which reference groups (for example, income- or wealth-wise) would be most relevant to would-be contributors. If CC is driven by anchoring, suitably suggesting a contribution amount might be all that is required. The aim of our study is to disentangle the four potential drivers of CC in a laboratory setting. We utilize a modified version of the FGF design (detailed in Sect. 3). In a within-subject design, each subject, after contributing unconditionally (treatment 1), is also faced with four conditional contribution treatments. In treatments 2 to 4, subjects condition on the average contribution of the three other members of their contribution group. What differs across these three treatments is how the contributions of the other three group members are determined. In treatment 2, the other group members’ contributions are equal to their unconditional contributions from treatment 1, as in the original design of FGF. All four explanations play a potential role here. In treatment 3, the other group members’ contributions are equal to unconditional contributions of three randomly chosen group non-members from treatment 1. This treatment eliminates reciprocity as an explanation of CC because the conditioning variable no longer reflects intentions of the other group members.Footnote 6 In treatment 4, the other group members’ contributions are randomly generated by computer. On top of treatment 3, this treatment also eliminates conformity as an explanation. Finally, in treatment 5, subjects no longer condition on the average contribution of the three other members of their contribution group, but, rather, they condition on the average of three randomly drawn numbers that are independent of the groupmates’ contributions. The other group members’ contributions are independently randomly generated by the computer. On top of treatment 4, this treatment eliminates also inequality aversion and leaves only residual factors as a potential explanation. We identify the impact of reciprocity by comparing conditional contributions in treatments 2 and 3; that of conformity by comparing treatments 3 and 4; that of inequality aversion by comparing treatments 4 and 5. Treatment 5 identifies the impact of residual factors. We do not attempt to separate anchoring from confusion as it is inherently difficult. Whenever anchoring is present, some type of confusion is very likely to be present as well.Footnote 7 Whenever confusion is present, there are some ex post patterns of conditional contributions that would allow one to argue that anchoring is not present.Footnote 8 However, it is hard to think of a reliable way to rule out anchoring by design ex ante. Based on the within-subject design, we find a strong CC behavior even in treatment 5 in which only the residual factors play a role. Adding inequality aversion in treatment 4 further increases the extent of CC behavior. Adding conformity in treatment 3 leads to a small further increase in CC behavior with borderline statistical significance. Finally, adding reciprocity in treatment 2 has a minimal impact on CC behavior. Based on the estimated slopes of the average conditional contribution schedules by treatment, we find that residual factors account for about two thirds, inequality aversion for one quarter and conformity for one tenth of the CC behavior. Reciprocity is estimated to play virtually no role. Next, we examine robustness of these findings to a possible imperfect perception of various treatments and their differences created by the within-subject design and presentation of the instructions. For this purpose, we collect additional data for treatments 2 and 5 using a between-subject design. Based on the estimated slopes of the average conditional contribution schedule by treatment, we find that residual factors account for about 59% of the CC behavior. If restricting the sample to only those who demonstrate a strong understanding of the instructions, the share is 45%. This robustness check therefore confirms the important role of residual factors in driving conditionally cooperative behavior in treatment 2. The paper proceeds as follows. Section 2 reviews the related literature. Section 3 outlines the experimental design. Section 4 reviews the utilized empirical methodology. Section 5 presents our results. Section 6 presents the design and results of the robustness analysis based on the between-subject design. Section 7 links the findings to the previous literature and discusses a potential alternative explanation of the results in treatment 5. Finally, Sect. 8 concludes.",
26.0,2.0,Experimental Economics,05 October 2022,https://link.springer.com/article/10.1007/s10683-022-09771-w,The effect of random shocks on reciprocal behavior in dynamic principal-agent settings,April 2023,Rudolf Kerschbamer,Regine Oexl,,Male,Female,Unknown,Mix,,
26.0,2.0,Experimental Economics,01 November 2022,https://link.springer.com/article/10.1007/s10683-022-09781-8,Correction to: The effect of random shocks on reciprocal behavior in dynamic principal-agent settings,April 2023,Rudolf Kerschbamer,Regine Oexl,,Male,Female,Unknown,Mix,,
26.0,3.0,Experimental Economics,27 February 2023,https://link.springer.com/article/10.1007/s10683-023-09793-y,Editorial: Symposium “Pre-results review”,July 2023,Urs Fischbacher,Irenaeus Wolff,,Male,Unknown,Unknown,Male,"As we pointed out above, pre-results review is meant to tackle various forms of questionable research practices. Having said this, our primary motivation for promoting pre-results review is to fight publication bias, which we consider the prime reason for replication issues in psychology and economics. In our view, far too many studies end up in the ‘file drawer’. These studies are not written up due to a lack of incentives because the (perceived) chances of publishing a null result or even inconclusive results are just too low. However, coming back to the above argument, if a stringent selection is applied during the pre-results review phase, the published results, positive or negative, will be among the most credible in the journal. And because the profession rightly is moving away from the idea that one study is generally enough to decide a question, even high-powered inconclusive results are important if the question is important enough. Widespread usage of pre-results review would have positive side effects. In particular, it would lead to a better-targeted use of resources. First, eliminating the publication bias would strongly reduce the multiplication of efforts of multiple researchers trying to demonstrate a non-existent relationship. Second, false-positive results would become rarer because power calculations are required in Stage 1, which are also reviewed. Consequently, the number of follow-up studies based on false-positive results would be reduced, too. Third, there would be fewer experiments with flawed designs because referees (and sometimes potentially even the authors themselves) would spot bad design choices before the experiments are run. Fourth, acquiring funds may become easier for young researchers if they demonstrate to a funding organisation that the research will be published.Footnote 6 Fifth, while the up-front investment needed for starting the project will increase strongly, obviating the need to re-write a study time and again is likely to decrease overall time costs. In Chris Chambers’ words: “Because the study is accepted in advance, the incentives for authors change from producing the most beautiful story to the most accurate one.”Footnote 7 In this context, the “most accurate” story means a neutral presentation of the data rather than a selection of analyses to fit a particular story. Sixth, in some cases, the procedure may even lead to a wiser choice of projects to be pursued because researchers think about a project more carefully before they start it, and the higher up-front costs are likely to deter them from working on less important research questions. Seventh, as Dufwenberg & Martinsson (2019) have pointed out, even the incentives to cheat are reduced. The basic argument is that if the article will be published irrespective of the results, there is a lower incentive to report particular outcomes and less of a need to fabricate a perfect story. Dufwenberg & Martinsson suggest results-free review exactly as a solution to the problem of existing incentives to cheat. And eighth, pre-results review would allow going back to a meaningful double-blind process for the initial submission, which might attenuate or even eliminate a seniority bias.",
26.0,3.0,Experimental Economics,15 August 2021,https://link.springer.com/article/10.1007/s10683-021-09728-5,Does choice change preferences? An incentivized test of the mere choice effect,July 2023,Carlos Alós-Ferrer,Georg D. Granic,,Male,Male,Unknown,Male,"The ability to recover preferences from choice data, and subsequently predict choices from preferences, is fundamental for economic analysis. The revealed preference approach (Samuelson, 1938, 1948; Houthakker, 1950; Arrow, 1959; Richter, 1966) essentially views preferences as nothing more than organizing schemes reflecting both observed and predicted choices. More recent accounts have shown that the ‘spirit’ of the revealed preference approach can be conserved under more general conditions. For example, the literature on stochastic choice and random utility models explicitly incorporates variability in choice—a key observation in real-world choice data (Tversky, 1969; Hey and Orme, 1994; Agranov and Ortoleva, 2017)—by adding random components to true, underlying preferences (McFadden, 1974, 2001). Provided that stable mechanisms govern choice variability, true preferences can still be recovered (e.g., Apesteguía and Ballester, 2018; Lu and Saito, 2020; Frick et al., 2019; Alós-Ferrer et al., 2021). In practice, choice data is universally used to estimate latent and derived concepts ranging from utility functions and risk attitudes to demand functions and social welfare (e.g., Harsanyi, 1955; Koopmans, 1960; Afriat, 1967; Varian, 1982; Andreoni and Miller, 2002; Cox et al., 2008; Deb et al., 2014, among many others). The use of choice data, however, entails an implicit but rarely-discussed assumption: stability. Predicting future choices from preferences which themselves are estimated from past choices is only warranted as long as economic agents display well-defined and stable choice patterns (or, at least, stable mechanisms governing choice variability) in the relevant time frame. Worryingly, the assumption of stable choice patterns (deterministic or stochastic) is at odds with fundamental theories in psychology, which postulate that choices can create and alter preferences (Festinger, 1957; Bem, 1967a, b; Slovic, 1995; Simon et al., 2004; Ariely and Norton, 2008). That is, the mere act of choice, even when no new information is revealed by or after the choice, can lead to fundamental changes in preferences, so that we do not only “choose what we like,” but mechanically also “like what we choose.” Empirical support for such feedback loops between choices and preferences appears to be widespread (Egan et al., 2010; Sharot et al., 2010; Nakamura and Kawabata, 2013; Johansson et al., 2014). These alleged preference changes occur within the time span of a few minutes and in the absence of any new, choice-relevant information. They are therefore fundamentally problematic for economics. If such effects extend to economic choices, every choice-based preference elicitation procedure bears the potential to interfere with the very concept it ought to measure. Observed economic choices may then permanently lag behind current preferences, and standard economic applications estimating utilities, demand, and social welfare may be systematically biased. In light of its potential consequences, it is of paramount importance to investigate the economic validity and significance of this mere choice effect. Evidence from psychology is insufficient to settle the question, due to difficulties with the experimental paradigms applied in that literature (see next section), the hypothetical nature of choices in such studies, and the non-economic nature of the alternatives they study. This paper undertakes the endeavor of establishing the validity of the mere choice effect (preference change due purely to the act of choice) relying on incentivized choices. We develop a parsimonious experimental design that allows researchers to isolate the effect of mere, uninformative choices on future choices in an economically-relevant domain (binary monetary gambles or lotteries). In essence, our experimental design first presents participants with two choice options (lotteries), but, crucially, the experimenter randomly determines whether a certain choice option is transparently inferior or superior (through stochastic dominance). As choices are incentivized, it is in the best interest of participants to choose the objectively superior option and hence follow the pre-determined, randomized choice patterns. Further choices in the experiment then test for preference change in favor or against the previous options. In this way, the design effectively randomizes uninformative (mere) choices. We hereby solve typical issues encountered in the existing literature: unreliable preference measures, hypothetical bias, and deception (we will elaborate on these issues in the next section). This paper reports the results of a large-scale, preregistered online experiment (the paper was evaluated at the journal previous to data collection) relying on the basic design described above. The mere choice effect was assessed by measuring whether merely-chosen options were subsequently chosen more often than merely-rejected ones. The results hence allow us to establish whether or not the mere choice effect is relevant for economics and whether or not it is warranted to maintain a unidirectional link between choices and preferences in the domain we study. Hereby, we contribute to a stream of literature that discusses the possibility of past experiences shaping future preferential choices. For example, the literature on preference discovery postulates that decision makers do not know their true tastes until they (incompletely) discover them through consumption experience (Plott, 1996; Braga and Starmer, 2005; Delaney et al., 2020). Naturally, past choices are a vital input source for the discovery process. Frick et al. (2019), on a related note, discuss in their concluding remarks that their dynamic random utility framework could accommodate endogenously evolving preferences (as a function of the agent’s past consumption level). This could take the form of habit formation or simply reflect the fact that past consumption provides payoff-relevant information. In contrast, in this paper we aimed to establish whether past choices affect future choices even when there is nothing to be learned from them. A possible mechanism would be that decision makers, to some extent, are used (or hard-wired) to learn from past choices and they mistake uninformative choices for informative ones. This could result in “decision inertia” as studied by Alós-Ferrer et al. (2016b) even in cases where objectively superior options are available (see also Jung et al., 2019). For example, Cerigioni (2017) studies how past exposure to a choice option creates inertia or stickiness towards the exposed option. This stickiness, known as the mere exposure effect (Zajonc, 1968,, 2001), may explain behavioral regularities like the status-quo bias, and could at least partially drive the mere choice effect (we will discuss this latter possibility below). We committed to our conditional conclusions prior to data collection. In case supporting evidence for the mere choice effect would have been found, the intention of our work was to contribute to the development of better and more predictively-accurate preference elicitation methods. For example, if preference change followed regular patterns, standard elicitation procedures could be corrected by taking into account quantitative predictions about the expected magnitude of preference change. However, our experiment found no supporting evidence for the mere choice effect. Merely-chosen and merely-rejected lotteries were subsequently chosen with almost identical frequencies, which in turn were almost identical in magnitude to a baseline measurement. Since our study had sufficient power, the conclusion is that the effects reported in psychology are likely to be too small for economic choices to merit sparking a major reevaluation of economic methods. The remainder of the paper is structured as follows. Sect. 2 reviews the existing literature on choice-induced preference change and briefly discusses the main theory underlying the effect. Sect. 3 presents our experimental design, including the derivation of our main hypothesis and the power analysis. Sect. 4 presents the statistical analyses (as planned before data collection and actually carried out) and discusses the interpretation of the results and Sect. 5 concludes. Additional results and supplementary experimental materials (experimental instructions and screenshots) are presented in the online appendix.",1
26.0,3.0,Experimental Economics,31 October 2022,https://link.springer.com/article/10.1007/s10683-022-09779-2,Measuring strategic-uncertainty attitudes,July 2023,Lisa Bruttel,Muhammed Bulutay,Adam Zylbersztejn,Female,Male,Male,Mix,,
26.0,3.0,Experimental Economics,29 November 2022,https://link.springer.com/article/10.1007/s10683-022-09785-4,Mis-judging merit: the effects of adjudication errors in contests,July 2023,Astrid Gamba,Luca Stanca,,Female,Male,Unknown,Mix,,
26.0,3.0,Experimental Economics,20 December 2022,https://link.springer.com/article/10.1007/s10683-022-09774-7,Strategic environment effect and communication,July 2023,Nobuyuki Hanaki,Ali I. Ozkes,,Male,Male,Unknown,Male,"In many economic decisions, there is a tension between what is individually rational and what is collectively optimal. As shown in the extant experimental literature, whether this dilemma can be resolved in favor of cooperation on collectively optimal outcomes may depend on a multitude of aspects in the specific context. This paper sheds light on the functioning of two well-known determinants of cooperative behavior in (dilemma) games with Pareto-inefficient Nash equilibria (NE). Particularly, we study the effect of communication in interaction with the strategic environment, i.e., whether strategic interactions exhibit complementarity or substitutability. Theoretically, provided that interactions are to be repeated a certain commonly known number of times, cooperation unravels in equilibrium due to backward induction, when communication is not possible.Footnote 1 However, ample experimental evidence demonstrates that participants reach and sustain cooperation to a significant extent.Footnote 2 In market games, for instance, this extent might depend on the type of goods, as in Holt (1993), who notes in a context of Bertrand price competition that the sellers who compete on the prices of substitute goods may find it easier to (tacitly) collude than sellers who compete on the prices of complement goods.Footnote 3 The former is a case of strategic complementarity, and the latter one of strategic substitutability, hence they represent different strategic environments. In situations that are described with dilemma games, communication is either desired and facilitated (e.g., in cooperation problems) or hard to avoid (e.g., in collusion problems). Thus, it is important to understand the workings of communication in this context. Although there are studies about the strategic environment effect, there was no preceding study investigating if this effect is dependent on the presence of (free-form) communication. This paper contributes to the literature in two ways: we build on previous findings by first exploring the behavioral underpinnings of the effect of the strategic environment on tacit cooperation, and second, by studying if and how communication interacts with this effect. In our benchmark setting without communication, we follow Potters and Suetens (2009) (henceforth PS), by focusing on the effect of the strategic environment by controlling for a set of previously discovered potential confounds. Our findings regarding this benchmark confirm previous results that suggest a higher tendency towards cooperation under strategic complementarity as opposed to substitutability.Footnote 4 We find that under complementarity, choices are higher than equilibrium levels, i.e., more cooperative, which, in turn, are higher than the choices under substitutability. Conversely, we find that reciprocity, as suggested by PS, does not explain this effect: changes in the partner’s choices are followed to the same extent under both strategic environments. However, we demonstrate using maximum likelihood estimations and simulations based on a simple reinforcement learning model that this can be driven by slow learning coupled with noisy choices. We implement an extension to the baseline setup by allowing subjects to chat before making decisions in each period, to determine whether the difference in the degrees of cooperation continues to hold under communication and to better understand how the strategic environment affects subjects’ reasoning about the game by looking into chat content. Although communication is generally found to enhance cooperation in the existing literature, its effect on the levels of cooperation is not definitive and is known to depend on the type, duration, and content of communication, as well as the game specifics.Footnote 5 In our experiments, subjects were given the opportunity to communicate in free form before making decisions, without any cost or binding agreement. Fonseca and Normann (2014) investigate the impact of communication in Bertrand markets of different sizes, and conclude that, free-from communication helps to obtain higher profits and that firms will succesfully continue collusion after communication is disabled. They note (Fonseca & Normann, 2012), however, referring to Farrell and Rabin (1996) and Whinston (2008), that the effect of communication on dilemma games is subject to debate among theorists. Waichman et al. (2014) note that there is very little attention devoted to the study of the impact of communication on Cournot markets and find that free-form communication boosts collusion levels (measured by both aggregate output and collusion counts), while standardized communication does not have a significant effect.Footnote 6 We find that when subjects can communicate, average choices and payoffs shift substantially and 70–80% of the pairs reach and sustain efficient cooperation in both strategic environments. Thus, communication has an “ironing effect”: the impact of the strategic environment on aggregate cooperation disappears. Considering the hardship of eliminating communication to avoid collusion in oligopolies, for instance, our finding points to the idea that the strategic environment may not be as of major significance as previously thought. Nonetheless, there are some differences to note. Firstly, communication is more effective in helping participants to cooperate, even if not at the efficient level, in substitutability than in complementarity. Secondly, under complementarity, efficient cooperation is more likely to be reached by gradual moves. We then aim at seeking an understanding about how communication works towards cooperation and if the strategic environment is relevant in this regard. To that end, we employ a set of text analysis methods, including analyses of the number of messages sent, the frequencies of most used words, and finally, a machine-assisted natural language processing (NLP) approach. Machine learning methods for text analysis are increasingly employed in economic research (see Gentzkow et al., 2019).Footnote 7 However, these methods have not been considered in analyses of communication records in experimental games until very recently.Footnote 8 We refer to an unsupervised learning method, a novel approach in the experimental economics literature, for content analysis of the chat records. In particular, we estimate a structural topic model (Roberts et al., 2016) that presumes that subjects’ chats are formed as a weighted mixture of topics that are in turn distributions over words. We find that the topical content of subjects’ chat records depends on the strategic environment and whether they achieve efficient cooperation in the game. For instance, we observe in the choice data that it is equally likely for subjects to realize and swiftly move to efficient cooperation in the two strategic environments. However, we also reveal evidence in chat content that subjects in complementarity treatment reach efficient cooperation by agreeing on gradual moves towards it, in case they do not start cooperating from the very beginning. Alternatively, in substitutability treatment, subjects either do not reach efficient cooperation or they may jump to it later. The remainder of the paper is organized as follows. Section 2 is devoted to the experimental design and procedure. Section 3 delivers our main findings on the strategic environment effect and its interaction with the effect of communication. Section 4 contains the chat analysis, and Sect. 5 concludes.",
26.0,3.0,Experimental Economics,21 December 2022,https://link.springer.com/article/10.1007/s10683-022-09782-7,Loss aversion in social image concerns,July 2023,Vasilisa Petrishcheva,Gerhard Riener,Hannah Schildberg-Hörisch,Female,Male,Female,Mix,,
26.0,3.0,Experimental Economics,03 February 2023,https://link.springer.com/article/10.1007/s10683-023-09791-0,Principal’s distributive preferences and the incentivization of agents,July 2023,Sophie Cêtre,Max Lobeck,,Female,Male,Unknown,Mix,,
26.0,3.0,Experimental Economics,03 February 2023,https://link.springer.com/article/10.1007/s10683-023-09790-1,A general revealed preference test for quasilinear preferences: theory and experiments,July 2023,Marco Castillo,Mikhail Freer,,Male,Male,Unknown,Male,"It is difficult to overstate the importance of the assumption of quasilinear (QL) preferences in both theoretical and empirical economics. The assumption plays a crucial role in mechanism design, the theory of the household, and applied welfare analysis. It is, for instance, a necessary assumption for the Revenue Equivalence theorem (Myerson, 1981; Krishna, 2009), the existence of the truth-revealing dominant strategy mechanism for public goods (Green & Laffont, 1977), and the Rotten Kids theorem (Becker, 1974; Bergstrom & Cornes, 1983). It is also a frequently invoked assumption in applied welfare analysis (Domencich & McFadden, 1975; Allcott & Taubinsky, 2015). It is also a crucial assumption in the empirical literature that uses bunching at kinks and notches of budget sets (see Kleven, 2016, for a review). The first contribution of our paper is to provide a new test for QL preferences with many empirical applications. The second contribution is to provide the first empirical test of QL preferences in a laboratory setting using nonlinear budget sets. The third contribution is to show the empirical relevance of QL, albeit not convex preferences. Contribution We provide criteria for a set of observed choices on nonlinear budget sets to be consistent by quasilinear preferences that are not necessarily convex. Our criteria are based on the observation that when preferences are QL, they must also satisfy the property of cyclical monotonicity (see, e.g., Rochet, 1987). Our contribution demonstrates that cyclical monotonicity holds for a larger class of choice environments than previously established. As a special case, we show that Forges and Minelli (2009)’s generalization of revealed preferences tests to nonlinear sets extends to tests of QL. This allows testing for QL preferences in some strategic environments. When the choice sets are linear, however, the condition we identify is equivalent to the definition of cyclical monotonicity in Brown and Calsamiglia (2007). These conditions are necessary and sufficient for choices to be rationalized by a QL utility function. We provide examples that show why it is essential to consider nonlinear budget sets. We conduct a real-effort labor supply experiment with nonlinear wages. We allow subjects to choose the terms of their contracts, where a contract specifies a wage and the number of tasks to complete. We generate nonlinear convex budgets sets to test for QL preferences and the convexity of preferences. We find that 77 percent of subjects are consistent with locally non-satiated utility (LNU), of those 86 percent are also consistent with QL and 82 percent are consistent with non-separable convex preferences and 56 are consistent with QL convex preferences.Footnote 1 While both assumptions of convex or QL preferences have significant empirical support, less is so for QL convex preferences. Related Literature Revealed preference theory is attractive because of its robustness to functional form assumptions regarding preferences. Beginning with the work of Richter (1966) and Afriat (1967), revealed preference theory has been used to test both individual and collective decision-making (see Chambers & Echenique, 2016, for a comprehensive overview of the results). It has also been used to test theories of consumer behavior in the lab. Applications include tests for social (e.g., Andreoni & Miller, 2002; Fisman et al., 2007; Porter & Adams, 2015), risk (e.g., Choi et al., 2007), and time preferences (e.g., Andreoni & Sprenger, 2012). Recent experimental research has used real-effort task to study time-preferences (Augenblick et al., 2015) and self-control Toussaert (2018). Our experiment shows that such an environment is useful to test other properties of labor-leisure decisions. The labor supply setting is particularly relevant in the analysis of household decisions (see e.g. Cherchye et al., 2012, 2017). Our experimental design can be adapted to study individual and group labor supply behavior. Recently, there has been interest in revealed preference tests for QL preferences in the context of linear budgets. Brown and Calsamiglia (2007) propose a revealed preference test for the case of concave preferences, while Nocke and Schutz (2017) discuss the corresponding integrability problem without assuming concavity. Cherchye et al. (2015) extends Brown and Calsamiglia (2007)’s test to generalized QL while maintaining the concavity of the utility and the linearity of budgets. Allen and Rehbeck (2018) provide a measure for QL misspecification and use scanner data to evaluate how this misspecification varies with the level of aggregation of the data. They find that while quasilinearity fails at the individual level, it represents the data well from the perspective of a representative agent. Chambers and Echenique (2017) show that QL preferences in the setting of combinatorial demand (with linear pricing) are equivalent to the law of demand, a condition that is simpler than cyclical monotonicity. As we will discuss, our approach provides the least restrictive test for QL preferences, and our experiment provides direct evidence that quasilinearity has empirical relevance at the individual level. Our experiments, therefore, complement these approaches nicely. Structure The remainder of this paper is organized as follows. Section 2 presents our theoretical framework, Sect. 3 presents our experimental design and Sect. 4 presents the empirical results. Section 5 concludes the paper.",
26.0,3.0,Experimental Economics,15 February 2023,https://link.springer.com/article/10.1007/s10683-022-09787-2,Morally monotonic choice in public good games,July 2023,James C. Cox,Vjollca Sadiraj,Susan Xu Tang,Male,Female,Female,Mix,,

Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
13,1,Journal of Business Cycle Research,10 March 2017,https://link.springer.com/article/10.1007/s41549-017-0012-y,"Interregional Trade, Specialization, and the Business Cycle: Policy Implications for the EMU",May 2017,Carlo Gianelle,Letizia Montinari,Simone Salotti,Male,Female,Female,Mix,,
13,1,Journal of Business Cycle Research,20 April 2017,https://link.springer.com/article/10.1007/s41549-017-0013-x,Are Microstates Necessarily Led by Their Bigger Neighbors’ Business Cycle? The Case of Liechtenstein and Switzerland,May 2017,Andreas Brunhart,,,Male,Unknown,Unknown,Male,"Liechtenstein with its 38,000 inhabitants represents a truly special case, especially when it comes to growth and business cycle issues, not only because of its remarkable economic growth history but also due to many special characteristics: It is a microstate with a very small open economy and not only with an important financial sector, but also with a large industrial sector accounting for about 40% of the total economy’s gross value added and employment. Additionally, as a very small nation, it exhibits a considerably high degree of international trade. From ex-ante considerations, the microstate Liechtenstein is not expected to have a business cycle lead to its neighboring countries, since small states are regarded as “importers” of the business cycles from bigger neighboring countries. But, if one considers the stylized facts that small countries are more volatile and feature a higher responsiveness to international economic fluctuations than bigger nations (e.g. Easterly and Kraay 2000; Thorhallsson 2010, p. 200; Brunhart 2013, pp. 16–17 and 23–24), then it could also be the case that they not only react more sensitively but also earlier.Footnote 1 This could imply a business cycle lead of the microstate Liechtenstein to its bigger neighbor Switzerland, with which it shares the strongest economic connections.Footnote 2 To examine (for the first timeFootnote 3) whether Liechtenstein’s business cycle features a different timing—namely if there is a potential time lag or lead in comparison to international fluctuations –, this paper includes the surrounding countries Switzerland, Austria, and Germany and other important trade partners such as Italy, France, and the US. Doing so, the timing of their business cycles, measured by both real GDP growth and output gap (annual data from 1972 onwards), are explored and potential statistically significant lags or leads between them targeted. Indeed, a statistically robust one year lead of Liechtenstein to Switzerland can be identified. This result provides an opposite understanding of the business cycle link between microstates and their “patron” nations, altering the traditional conception of small countries as business cycle takers and big nations as business cycle givers in terms of causal links (Graff 2006, p. 386). There is a huge body of international literature on the development of synchronicity of business cycles between countries over time and the respective determinants, such as globalization, economic and financial integration (e.g. Berge 2012; Tondl and Traistaru-Siedschlag 2006) or the introduction of the Euro (e.g. Gogas 2013).Footnote 4 Also, a lot of contributions on size and volatility of nations exist and even more are devoted to detecting variables or composite indexes that can be utilized as leading indicators for the same economy’s business cycle. In this paper, not the change of business cycles’ synchronicity over time or its determinants are analyzed, but the entire focus lies on the timing of the cycles’ phases instead, namely if there is a lag or a lead in the synchronous movement. Especially relevant for the analysis here is the Swiss business cycle. Graff (2006) explores the business cycle synchronicity as well as leads and lags between 26 countries using annual data on the capacity utilization from 1970 to 2000. He concludes that Switzerland exhibits rather lagging tendencies. As outlined in Graff (2011), Switzerland features a lagging pattern compared to at least some of the countries when cross correlations of the quarterly output gaps in an unbalanced panel of 40 countries from 1960 to 2011 are inspected. But according to the author, the evidence seems to be rather weak, also when economic interpretation is considered. Also Layton et al. (2015) find a Swiss lag (of three months) to the world business cycle. Indergand et al. (2013), applying a SUR-VAR model and quarterly data from 1992 to 2013, conclude that the Swiss economy fluctuates in unison with the international cycle and does not lag the international economy. Müller and Woitek (2012, pp. 130–174) examine the Swiss business cycle’s international connection on the base of annual GDP data from 1960 to 2000, but do not investigate the possibility of time differences in the countries’ phases. All the studies on the Swiss business cycle’s international relations mentioned in this paragraph do not include Liechtenstein into their data set, which is now done in this paper. Related studies on the other countries examined in this paper are manifold. Especially the business cycle connection between Germany and Austria has attracted scientific attention.Footnote 5 Prettner and Kunst (2012) reveal large effects on Austria caused by shocks to the German economy, while the transmission is weak for the other direction. Cheung and Westermann (1999) find that Germany’s monthly industrial production is leading Austria’s. Fenz and Schneider (2006) on the other hand argue that Germany’s economy was leading Austria’s in the 1970s by one quarter, but Austria has been leading by one quarter more recently.Footnote 6
 The structure of the paper is as follows: Sect. 2 deals with descriptive statistics (especially cross correlations) and explorative visual impressions. Section 3 covers the explanation of the methodology and the regression results and their interpretation with a special focus on Liechtenstein’s lead to Switzerland, while possible reasons for this lead are discussed in Sect. 4. Section 5 consists of conclusions, additional methodological remarks and future research questions arising from this paper.",
13,1,Journal of Business Cycle Research,08 April 2017,https://link.springer.com/article/10.1007/s41549-017-0014-9,Dating Cyclical Turning Points for Russia: Formal Methods and Informal Choices,May 2017,Sergey V. Smirnov,Nikolay V. Kondrashov,Anna V. Petronevich,Male,Male,Female,Mix,,
13,1,Journal of Business Cycle Research,11 April 2017,https://link.springer.com/article/10.1007/s41549-017-0015-8,The Predictive Content of Business Survey Indicators: Evidence from SIGE,May 2017,Tatiana Cesaroni,Stefano Iezzi,,Female,Male,Unknown,Mix,,
13,1,Journal of Business Cycle Research,18 May 2017,https://link.springer.com/article/10.1007/s41549-017-0016-7,Quasi-Real-Time Data of the Economic Tendency Survey,May 2017,Maria Billstam,Kristina Frändén,Pär Österholm,Female,Female,Male,Mix,,
13,2,Journal of Business Cycle Research,05 September 2017,https://link.springer.com/article/10.1007/s41549-017-0017-6,How Informative are Aggregated Inflation Expectations? Evidence from the ECB Survey of Professional Forecasters,November 2017,Sami Oinonen,Maritta Paloviita,,Male,Female,Unknown,Mix,,
13,2,Journal of Business Cycle Research,01 November 2017,https://link.springer.com/article/10.1007/s41549-017-0018-5,"Stylized Facts of the Business Cycle: Universal Phenomenon, or Institutionally Determined?",November 2017,Vadim Kufenko,Niels Geiger,,Male,Male,Unknown,Male,"Research on business cycles has a long and eclectic history concerning both theory and empirical work. If any consensus on the subject is to be pointed out, it is in the conclusion that the business cycle is a phenomenon related to, and appearing in association with, a multitude of movements in various economic variables. Nonetheless, there are apparent regularities which are noted in empirical work: Similar to Kaldor’s (1957) famous “stylized facts” of growth, i.e. the long-term trend movement of an economy, there are several “stylized facts” of “movements about trend in gross national product” (Lucas 1977, p. 9). These observations build on a long tradition in empirical business cycle research, often dating back to the seminal work of Wesley C. Mitchell and others. Much of the empirical work was originally focused on a few economies, particularly the United States, and even today still frequently revolves around U.S. trends. However, research on cross-country comparisons highlights important differences in the particular macroeconomic relations. Given the evidence of such differences, the question of what determines the precise shapes of business cycles arises. This paper investigates the universality of business cycle characteristics in a sample of OECD countries for the 1960–2016 period and attempts to provide a statistical and theoretical explanation of international differences by referring to institutional factors. Section 2 provides some theoretical background and puts the present research in context of related literature. Section 3 then presents both a descriptive overview and a more extensive statistical analysis of business cycle characteristics among the country sample. In a next step, observed international differences are associated with measures of countries’ institutional setups. The results are then discussed theoretically in Sect. 4, before the last section concludes with a summary of the results and an outlook pointing at further research.",1
13,2,Journal of Business Cycle Research,04 November 2017,https://link.springer.com/article/10.1007/s41549-017-0019-4,Q-Targeting in New Keynesian Models,November 2017,Burkhard Heer,Alfred Maußner,Halvor Ruf,Male,Male,Male,Male,"The financial crisis of 2007 has triggered renewed interest into a debate which started in the late 1990s: should central banks target asset prices? Bernanke and Gertler (1999, 2001) were among the first to ask how central bankers should react to asset price volatility. They argue that there is no need for concern, if asset price movements reflect changes in economic fundamentals. However, if asset prices were driven by non-fundamental factors, their influence could be destabilizing. They consider a bursting asset price bubble in a version of the model developed in Bernanke et al. (1999) and show that asset price targeting may even destabilize the economy. The more recent literature has contributed additional arguments in favor of or against asset price targeting. Gilchrist and Saito (2008) introduce imperfect information about the nature of technology shocks into the model of Bernanke et al. (1999) and employ a loss function with the variance of inflation and the variance of the output gap as arguments to assess monetary policy. A temporary positive shock to total factor productivity being partly considered as longer-lived impacts on inflation and the output gap via two distortions: the mark-up of prices on marginal costs and the financial accelerator. If the central bank reacts to inflation only, its impact on the fluctuations of output is limited as compared to a rule that changes the nominal interest rate also in response to the deviation of asset prices from their level in the frictionless economy. Christiano et al. (2010) study news driven cycles in a medium scale New Keynesian model with price and wage stickiness and the financial accelerator of Bernanke et al. (1999). News about a future improvement of factor productivity lowers expected marginal costs and reduces inflation. If the central bank responds by lowering the nominal interest it fuels an unwarranted boom. In order to moderate the effects of this kind of shock, the central bank should respond to the increased credit demand of borrowers and raise the nominal interest rate. Carlstrom and Fuerst (2007) argue that asset price targeting may increase the parameter region within which the rational expectations equilibrium is not unique so that sunspot equilibria arise. Machado (2012) considers learning in the model of Carlstrom and Fuerst (2007) and shows that asset price targeting may hamper the convergence to the rational expectations equilibrium. While these papers focus on particular effects of asset price targeting, Faia and Monacelli (2007) adopt a welfare economics approach. They rank different Taylor rules with and without an indicator of asset price movements according to the associated expected life-time utility of the representative agent. Their framework is the financial accelerator model of Carlstrom and Fuerst (1997) amended with quadratic costs of price adjustment. The interaction between the nominal friction and the financial friction requires a negative response of the nominal interest rate set by the central bank and the relative price of capital. Moreover, the welfare gains of targeting the price of capital in addition to inflation are very small as compared to a strict anti-inflation policy. In this paper, we also consider the desirability of asset price targeting with respect to its effect on the welfare of the representative household. Our study is most closely related to Faia and Monacelli (2007). With regard to the methodology, we employ the approach pioneered by Schmitt-Grohé and Uribe (2005, 2007) and compute the welfare effects of an extended Taylor rule relative to a simple Taylor rule that just reacts to the deviation of inflation from the central bank’s target. As Schmitt-Grohé and Uribe (2005, 2007) we disregard rules that i) lead to indeterminacy and ii) may hurt the zero lower bound. As Faia and Monacelli (2007) we consider Taylor rules with four arguments: the one period lagged nominal interest rate, the deviation of inflation from target inflation, the deviation of output from its steady state level, and the relative price of capital goods as an indicator of asset prices. In the parlance of Schmitt-Grohé and Uribe (2005, 2007) this rule is implementable, since the central bank reacts to observable variables only. Different from Faia and Monacelli (2007), we consider i) both additively separable and non-separable preferences and ii) a richer structure of shocks as a reduced form of capturing market incompleteness and saving behavior in presence of possible severe economic downturns as described in the Gourio (2012) disaster framework. In addition, iii) we merge adjustment costs of capital and financial frictions so that our model is closer in spirit to Bernanke et al. (1999). The financial friction which we consider follows Carlstrom and Fuerst (1998). They assume that the producers of goods must pay their factors of production in advance. The credit contract imposes liquidation costs that the producers pass on to the suppliers of factor services via an (inverse) mark-up on factor costs. The credit costs decrease with increasing net wealth of the producers. The net wealth, in turn, raises with the price of capital, i.e., Tobin’s q. Therefore, the inclusion of asset prices in the central bank’s decision rule has the potential to improve monetary policy.Footnote 1
 The paper is structured as follows. In the next section, we introduce a first model with the usual shock to total factor productivity and a government spending shock. The model features two nominal frictions (price and wage staggering as in Calvo (1983)) and a financial friction in the production of primary goods as proposed by Carlstrom and Fuerst (1998). Section 3 presents the calibration of the model. In Sect. 4, we present our results. Section 5 studies the robustness of these results with respect to the specification of the household’s preferences and with respect to the interaction of supply and demand shocks. The main conclusion of Faia and Monacelli (2007) remains intact in all our setups: the welfare gain of targeting the relative price of capital are negligible. Section 6 concludes.",2
13,2,Journal of Business Cycle Research,01 November 2017,https://link.springer.com/article/10.1007/s41549-017-0020-y,Sentiment and Uncertainty Fluctuations and Their Effects on the Euro Area Business Cycle,November 2017,Bas van Aarle,Cindy Moons,,Male,Female,Unknown,Mix,,
14,1,Journal of Business Cycle Research,29 January 2018,https://link.springer.com/article/10.1007/s41549-017-0022-9,Nowcasting Real Economic Activity in the Euro Area: Assessing the Impact of Qualitative Surveys,April 2018,Raïsa Basselier,David de Antonio Liedo,Geert Langenus,Unknown,Male,Male,Male,"In the field of economics, the term nowcasting generally refers to methods for monitoring the current state of the economy and developments in the short term, and it has become increasingly popular since the work by Evans (2005) and Giannone et al. (2008). Real-time estimates of economic growth are particularly relevant for both policy makers and market participants, as official national accounts data only come with a substantial delay. For instance, Eurostat releases the gross domestic product (GDP) ‘flash’ figure for the aggregate euro area only with an approximate delay of 45 days.Footnote 1 Also for individual euro area countries, flash estimates for GDP growth are published with a lag of at least one month. However, there is a wide range of higher-frequency variables containing either ‘hard’ or ‘soft’ information, that is being released at an earlier stage and journalists, analysts and financial market participants can already form their expectations on the basis of this dataflow. In this context, market participants and observers will continuously react to data releases in function of whether those are above or below their expectations, also depending on the perceived quality of the underlying data. The approach presented in this paper to formalise this behaviour will enable conclusions to be drawn about the relevance of every data release. As stressed by Bańbura and Rünstler (2011), the practice of nowcasting goes beyond the simple production of an early estimate and also requires an assessment of the impact of new data on forecasting updates over the time horizon. This paper aims to do so by converting the heterogeneous dataflow that enters the forecaster’s information set into a newsflow that can 
be interpreted and, most importantly, quantified. The news is defined as the difference between the released values and the predictions of a mixed-frequencies dynamic factor model (DFM). Models of this sort are successful at capturing the business cycle comovements in terms of few underlying factors and have been applied for many countries.Footnote 2 The analysis of contributions will be obtained in function of the Kalman filter gain, which translates news into forecasting updates. From a methodological point of view, however, our proposal is not restricted to DFMs, but it simply requires a fully specified dynamic system that can be written in state-space form. Hence, linking news and forecasting updates is impossible in the context of partial models such as bridge equations, MIDAS regressions and univariate models in general, which remain widely used in nowcasting applications. The incremental contributions of the news resulting from our empirical results then allow us to create a ranking for the multiple press releases. Such a ranking, which is determined by both the model parameters and the schedule for data releases, could provide a powerful tool for analysts and observers that are keeping track of the newsflow on a regular basis. The ranking would allow them to filter the huge amount of incoming information and mainly focus on the most important indicators. From a more formal point of view, we will investigate whether such a ranking could be used as a variable selection criterion. Modelling news and calculating the impact on forecasting revisions requires dealing with two key characteristics of real-time data. First, the dataset typically has a ragged edge, as potential predictors are released with different lags. Hence, any assessment of the contribution of a given predictor to GDP nowcasts will have to take into account the real-time data availability, as determined by the actual data release schedule. Second, data series are often revised: the current value may deviate from the first release, while the latter is used for nowcasts in real time. Hence, those first-release data should be taken into account when evaluating the importance of a given predictor. While addressing the first issue has become standard practice in nowcasting applications—e.g. in Giannone et al. (2008), Angelini et al. (2011) and Gayer et al. (2015)—, the presence of data revisions is often ignored and current values are used for the analysis. This is usually referred to as a pseudo real-time approach. To the best of our knowledge, we are the first to exploit information from a genuine real-time dataset of time series covering 15 years, constructed on the basis of original press releases. The use of first-release data is in our view necessary to quantify the precise impact of the various indicators in an actual nowcasting context. Our investigation of the role of qualitative surveys data when forecasting macroeconomic variables provides specific weights to all indicators included in the analysis. Thus, our work is connected to the results by Abberger (2007), Claveria et al. (2007), Giannone et al. (2009), Lui et al. (2011), Martinsen et al. (2014), Piette and Langenus (2014), de Antonio Liedo (2015) and Gayer et al. (2015), to name a few. However, we are the first ones to quantify a direct measure of surveys’ informative content. Gayer et al. (2015), for example, provide an indirect assessment of the usefulness of a whole block of survey data by quantifying the forecasting accuracy losses resulting from a model without this block. In turn, we use a unique model to determine the exact contribution of each individual predictor in the forecasts. Going beyond the precise quantification of each indicator’s contribution on the forecast, this paper also explores whether the incremental impacts defining our ranking could be used as a variable selection criterion, as Rünstler (2016) proposes in his alternative analysis of contributions. Interestingly, we find that the dynamic factor model containing only the highest-ranked indicators produces less accurate forecasts than the benchmark model. Hence, we argue that relying on a limited set of indicators may prove to be somewhat less beneficial in the real-time environment, as opposed to the conclusion reached by Rünstler (2016), who exploits the bridging with factors framework popularised by Giannone et al. (2008). Finally, we show that our workhorse dynamic factor model based on both hard and soft data produces euro area growth nowcasts that improve over time in function of the news distilled by the model. Those forecasting updates are not significantly different from the ones produced by the market. This implies that the parametric assumptions incorporated in our news-reading machinery are not at odds with reality, and our results can indeed be used as a measure of the relative importance of the various indicators. The paper is structured as follows. Section 2 describes in detail the real-time dataflow that is relevant for monitoring the business cycle in the euro area, and defines the standard impact concept that will be used to rank the different indicators. Section 3 presents the model, i.e. a dynamic factor model that is flexible enough to account for a substantial proportion of the dynamic interactions between all indicators. It is shown that the nowcasts provided by this model perform well in terms of forecasting accuracy, also relative to well-known benchmarks in the field. Section 4 discusses the standard impact on the euro area GDP nowcasts that result from this model. Those results allow us to construct our ranking for the data releases. Section 4 also shows the outcome of different counterfactual scenarios. These counterfactual exercises make it possible to disentangle the part of the impact that comes from the timeliness of the indicator, from the part that is driven by its quality. Section 5 investigates whether our ranking can serve as a tool for selecting variables, as suggested by Rünstler’s (2016) analysis. The last section concludes. The results presented in this paper can be easily reproduced and extended by installing a nowcasting plug-in into the JDemetra+ software, which was developed at the National Bank of Belgium.Footnote 3",8
14,1,Journal of Business Cycle Research,17 January 2018,https://link.springer.com/article/10.1007/s41549-018-0023-3,A Comparison Between Direct and Indirect Seasonal Adjustment of the Chilean GDP 1986–2009 with X-12-ARIMA,April 2018,Carlos A. Medel,,,Male,Unknown,Unknown,Male,"It is well known among practitioners that seasonal adjustment applied to economic time series involves several decisions to be made by the econometrician. Many of these decisions concern parameters to be fixed prior to the adjustment process and are included in traditional programs such as X-12-ARIMA or TRAMO-SEATS.Footnote 1 Despite these advances, there is no consensus on a particular method to obtain reliable and accurate results. Specifically, the econometrician’s decisions seem to be case dependent and based merely on empirics.Footnote 2 In the case of an aggregate series—already a weighted sum of disaggregates—there are several strategies to perform a seasonal adjustment; for instance, by: (1) adjusting an aggregate series by itself, (2) adjusting components of the aggregate with the same methodology and then aggregate to the original, and (3) adjusting components with a different methodology and adding up to the original. These strategies could deliver results that strongly differ from each other; while partial aggregations may be useful for many econometric purposes. Some reasons for this difference are nonlinearities in the components, different seasonal patterns through themselves, outliers, and difficulties in identifying the trading day effect. In this paper, I assess the question of which of these strategies yields the most robust (stable, reliable, and accurate) results for the case of the Chilean Gross Domestic Product (GDP) 1986–2009 quarterly dataset (base year: 2003). I perform an aggregate-by-disaggregate analysis under different schemes, based on the diagnostics for seasonal adjustment contained in the X-12-ARIMA program plus some statistical tests for robustness. These capabilities include spectral plots, sliding-spans-based diagnostics, and revision history diagnostics; all of them simple checks that cast for both quality and stability of results.Footnote 3 By performing this exercise, I will be able to provide an informed opinion about which scheme provides the most stable seasonal adjustment for the mentioned Chilean GDP vintage. The exercise performed in this article is relevant for several reasons. First, since 2009 the GDP dataset is released under the linked-chain methodology, losing its additive property. Therefore, this exercise should be read as a benchmark for further extensions using the datasets released with the new methodology (base years 2009 and 2013).Footnote 4 Second, to have an informed opinion on the reliability of seasonally adjusted disaggregations that compound total GDP. This is relevant for conjunctural economic assessment, as it concerns signal extraction from seasonal, noisy series; direction of change detection; and econometric applications (i.e. modelling) using seasonal variables. Finally, policymakers whose effectiveness is strongly attached to the current business cycle outlook, should be aware of the sources of instability and the difficulties for obtaining reliable information from both the demand- and supply-side.
 Note that as seasonal adjustment lacks an objective definition, the term “accurate” does not operate as in other traditional circumstances. Instead, I considered the more stable as the best result (Di Fonzo 2005). The exercise is carried out on the automatic X-12-ARIMA program default mode, according to the suggestions presented in Maravall (2002). Then, if a component does not fulfill the statistical criteria of an acceptable result, minimal interventions are made one-by-one until all tests are fulfilled, as probably made by most of the users. The question about how we know if it is better to use direct or indirect adjustment is similar to that treated in Astolfi et al. (2001), Hood and Findley (2001), Otranto and Triacca (2002), among other papers. Nevertheless, to the author’s knowledge, no similar study has been carried out with Chilean GDP data. It is worth mentioning that despite several testing procedures contained in X-12-ARIMA, choosing between direct and indirect adjustment constitutes a different, separate question. This is so because the challenge consists of choosing appropriate diagnostics to compare among several adjustments applied to the same variable. Some diagnostics would be replicated while others, as those of stability and quality, need more attention and careful treatment. This would lead to proceed without, for instance, the M1-M11 and Q statistics (Lothian and Morry 1978)—placed at the core of X-12-ARIMA. Furthermore, as Hood and Findley (2001) suggest, the ratio of one adjustment to another is also not valid, because spurious seasonality emerges. Many other examples remark the importance of a tailored quality assessment: different users would weigh different diagnostic outputs pursuing their own objectives. Take the case, for instance, of an adjuster making use of seasonally adjusted series in a turning point detection analysis. The smoothness in the resulting series will likely be a desirable outcome. For these and other reasons to be discussed later, I make use of diagnostics that allow a quality and stability assessment for indirect adjustment, namely, spectral analysis in the frequency domain, sliding spansFootnote 5, and revision history; all of them built-in X-12-ARIMA. I also use bias significance to statistically assess the distance to the direct method. The key is to have always in mind the goal of absence of residual seasonality; that is, absence of seasonality in series that theoretically should not have it. This is also analyzed through a rolling regression exercise. Finally, the exercise is re-done using the linked-chain dataset using a 1996–2017 sample (base year: 2013)—with evident aggregation bias in the aggregation schemes. The results show that, in terms of stability, it is advisable to use the first stage of disaggregation by supply-side. Moreover, the results for the second and third stage of disaggregation by demand-side are very poor, according to the standard automatic setup described below. A deeper supply-side disaggregation results in a noisy estimation with many outliers in the adjusted series, but provides useful information regarding the sources of instability. Particularly powerful tools to discriminate are spectral plots and sliding spans, both estimated to the final seasonally adjusted series. Some economic intuition behind the statistical results is also provided. When using a linked-chain dataset, the first stage of disaggregation by supply-side is less biased in both actual and adjusted series, showing even more stability than the GDP itself. The paper proceeds as follows. In Sect. 2, I provide some reasons about why the two kinds of adjustments (direct and indirect) may differ, along with some elements to consider reverting poor results. In Sect. 3, I apply these procedures to the Chilean GDP obtained by five different aggregation schemes—by supply- and demand-side—plus the GDP itself. I conclude in Sect. 4.",
14,1,Journal of Business Cycle Research,19 January 2018,https://link.springer.com/article/10.1007/s41549-018-0024-2,The Effects of Uncertainty Shocks on Daily Prices,April 2018,Dario Bonciani,Andrea Tafuro,,Male,Female,Unknown,Mix,,
14,1,Journal of Business Cycle Research,03 March 2018,https://link.springer.com/article/10.1007/s41549-018-0025-1,Would DSGE Models Have Predicted the Great Recession in Austria?,April 2018,Fritz Breuss,,,Male,Unknown,Unknown,Male,"It is common knowledge that the economic community was not able to forecast the Great Recession in 2009. The crisis evolved in a sequence of crises (see Breuss 2016): it started with the US subprime crises, followed by a banking crisis triggered by the Lehman Brothers’ crash on 15 September 2008 which induced a collapse of the interbank market. Then the stock market plunged and caused the Great Recession in 2009. Starting in the United States it spread to most industrial countries. Europe, in particular the Euro area generated its unique “Euro (debt) crisis”. As an excuse, one argued that because of the specificity of the crisis, the economic models then used were not able to forecast it.
 In the forecasting business, a variety of models are used, but primarily traditional macro econometric models. The now common workhorse of modern macroeconomic theory, however, are DSGE (Dynamic stochastic general equilibrium) models.Footnote 1 They are used to predict (forecast) and explain (story-telling) co-movements of aggregate time series over the business cycle (real business cycle theory) and to perform policy analysis (policy experimentsFootnote 2: IRF implications of shocks of fiscal and monetary policy and of technical changeFootnote 3). Whereas the two latter applications were in the forefront of applications since its inception, based on the work by Kydland and Prescott (1982),Footnote 4 the forecasting perspective is only recently topical. Most forecasting evaluations with DSGE models so far were executed for the US economy and for the Euro area (at the ECB). In the following we perform a post-mortem of DSGE model forecasts of the Great Recession (2009) in Austria. For this purpose, we use eight DSGE models with different characteristics (closed and open economy models; one and two-country models). Primarily, the development of the Austrian real GDP during the Great Recession of 2009 and thereafter is evaluated ex-ante with out-of-sample forecasts. The paper is structured as follows. Chapter 2 gives a brief overview of the literature on forecasting with DSGE models. Chapter 3 describes the eight DSGE models used for this forecasting exercise. In chapter 4 the forecasting performance of the different models for Austria during the Great Recession is evaluated. Additionally, in chapter 5 we check the forecasting performance of non-DSGE methods (Global Economic (Macro) Model of Oxford Economics and WIFO’s expert forecasts). Conclusions are drawn in the last chapter.",2
14,1,Journal of Business Cycle Research,06 April 2018,https://link.springer.com/article/10.1007/s41549-018-0026-0,Implementing an Approximate Dynamic Factor Model to Nowcast GDP Using Sensitivity Analysis,April 2018,Pablo Duarte,Bernd Süssmuth,,Male,Male,Unknown,Male,"GDP data is published quarterly with a lag of several weeks. Relying on relevant variables which are available earlier and at higher frequency, an “approximate” dynamic factor model (ADFM) can be used to deduce a common indicator to nowcast GDP. The outcome is a GDP series of higher frequency than its actual one (Aruoba et al. 2009; Camacho and Perez-Quiros 2010; Camacho and Doménech 2012; Camacho and Garcia-Serrador 2014).Footnote 1
 A central aspect of this class of small-scale nowcasting models is the pre-selection of indicators for the ADFM.Footnote 2 For example, Camacho and Perez-Quiros (2010, pp. 672–674) suggest a selection procedure based on “successive enlargements.” Due to ordering in their screening out, they do not treat candidate variables symmetrically. Our contribution is to overcome this deficiency by proposing to subject the selection of indicators to an extreme bounds analysis (EBA); see e.g. Levine and Renelt (1992). The selection of indicators is illustrated for a sample case in two steps. First, we reduce a set of 258 monthly time series related to Spanish GDP to 27 by keeping only the series showing a reasonable correlation with GDP, both contemporaneously and at its quarterly and yearly lags. In a second step, we apply an EBA procedure to drop “non-robust” indicators. To identify robust indicators, the strategy is to consider all possible regressions with quarterly GDP as dependent that can be estimated by taking combinations of a parsimonious subset (micro-scale) of pre-selected series (small-scale). The sensitivity analysis then consists in checking for each series whether the resulting distribution of parameter estimates has only positive or negative support, that is, whether the respective coefficient estimate does not change its sign in all regression runs. The latter should, at least, hold in an interval of so-called “extreme bounds” (Levine and Renelt 1992). We interpret and apply EBA as shrinkage device. Moreover, we prefer EBA to other more timely shrinkage devices such as the least absolute shrinkage and selection operator (LASSO) and ridge regressions or subset regressions based methods (see, e.g., Elliott et al. 2013) as for the latter the focus is on forecast performance, whereas it is on qualitative characteristics of series in the EBA case. EBA selects more economically meaningful indicator series into the micro-scale ADFM by assuring that they do not change, for instance, from procyclical to countercyclical (or from coinciding to either leading or lagging) indicators depending on covariate subset compositions during shrinkage. Against this backdrop, our central research questions can be summarized as follows: Does it pay to consider a symmetric shrinkage device in the pre-selection process? Does robust parsimonity improve nowcasts, even if we rely on a rather ad hoc pre-selection device and a shrinkage device with focus on selecting economically meaningful indicators rather than on pure forecast performance in going from small-scale to micro-scale ADFM?",4
14,1,Journal of Business Cycle Research,02 April 2018,https://link.springer.com/article/10.1007/s41549-018-0027-z,Large Shocks and the Business Cycle: The Effect of Outlier Adjustments,April 2018,Yoshihiro Ohtsuka,,,Male,Unknown,Unknown,Male,"Assessing business fluctuations and cycle phases has attracted much attention among macroeconomists and econometricians, because the magnitudes of economic variables are important to the manufacturing and service industries, central banks, and government. In particular, the severe recession in mid-2008 affected all industrialized countries. The impact of this recession on the business cycles in these countries was so large that it is difficult to measure the business fluctuations using conventional methods. Thus, research on business cycles has intensified, and is required in order to improve business index measures and to extend econometric methodologies. A composite index (CI) is widely used to measure trends and quantitative business fluctuations in countries such as the United States, United Kingdom, Japan, and so on. This study analyzes Japanese macroeconomic data series and business cycles. The Japanese economy has experienced severe crises in recent decades, for example, the financial crisis in mid-2008 and the major earthquake in March 2011. Although the Japanese economy was damaged by these crises, the shocks themselves vary in their characteristics. Business dropped off in mid-2008, because after the financial crisis, world trade began to shrink rapidly. This shock is one associated with business cycles, particularly recessions. In contrast, the Great East Japan Earthquake is an example of a natural disaster that affects isolated economic activity. The CI in Japan is an index that replaces outliers based on descriptive statistics. The adjustment identifies outliers as observations in component series that lie outside the normal range of expected observations, and replaces them with estimated values in order to measure robust business cycles. Since this adjustment automatically replaces extreme values, it may be unable to distinguish fully between shocks derived from isolated observations and those from recessions. Therefore, one implication is that using data with outlier replacement discards information on the business cycle. In other words, if the outlier is associated with a recession, then the adjustment reduces the depth of the recession. Surprisingly, despite some studies investigating the link between seasonal adjustments and business cycles, such as Franses and Paap (1999) and Matas-Mir et al. (2008), the impact of eliminating outliers has not been well discussed, with the exception of the study of Balke and Fomby (1994). This is an important issue, because the data adjustment may mislead an empirical analysis, resulting in erroneous conclusions. Thus, the first contribution of this study is to examine the impact of outlier adjustments on business cycle inferences. Large shocks in macro economic data series have been discussed since the 1990s. For example, Christiano and Den Haan (1996) pointed out that macro economic variables exhibit non-Gaussian behavior, which is called the fat-tailed problem. Recently, it has become necessary to consider excess kurtosis when analyzing macro economic time series data, such as industrial production and business cycles (e.g., Fagiolo et al. 2008; Watanabe 2014; Ascari et al. 2015), and when applying macro economic models such as the dynamic stochastic general equilibrium model (Cúrdia et al. 2014). Therefore, we need to consider the properties of the data distributions in the econometric models. For business cycle inferences, we employ the Markov switching dynamic factor (MSDF) model, as used by Kim and Yoo (1995), Chauvet (1998), and Kim and Nelson (1998). The MSDF model is widely used to simultaneously track business fluctuations as co-movements among individual economic indicators, and phases such as a recession regime or expansion. Using the MSDF model, numerous papers analyze the properties of business cycles, and these specifications are applied to empirical analyses in macro econometrics (e.g. Kaufman 2000; Camacho et al. 2014). The MSDF model is extended by incorporating Student’s t-distribution and a stochastic volatility (SV) process. The investigation is related to that of Watanabe (2014), who presented evidence supporting the use of heavy-tail error distributions, based on the CI in Japan. He also showed a model with good fit that used an SV and a fat-tail distribution. We assume that the error distributions of both the idiosyncratic equation and the factor equation follow Student’s t-distribution. This formation allows us to compute the outliers for the latent factor noise and the idiosyncratic noise. Moreover, including a t-distribution for the error distribution and an SV process in the factor equation makes it possible to decompose shocks to business cycles into unexpected shocks and conditional expected shocks. The second contribution is to extend the MSDF model to include large shocks. Once the SV process and non-Gaussian distribution are introduced, it becomes difficult to evaluate the likelihood. We employ the Markov chain Monte Carlo method in the Bayesian inference to estimate the model. In our empirical analysis, we apply our MSDF model to 11 coincident economic indicators from February 1985 to December 2014, as in the Economic and Social Research Institute (ESRI) data set. From the results of the estimated parameters, the posterior mean of the degree of freedom in the factor equation is close to 9, denoting that the latent factor follows fatter distribution than normal. Compared with the CI of the ESRI, we show that the use of outlier adjusted data leads to poorer performance in estimating business fluctuations and in detecting cyclical characteristics. The lack of information on outliers causes an underestimation of business fluctuations. Moreover, the results of the decomposition of shocks shows that the impact of a financial crisis on the business cycle is large, and is derived from both clustering and unexpected shocks. On the other hand, the outlier of the Great East Japan Earthquake in 2011 is associated with an idiosyncratic shock, and did not cause a recession. Our model is able to distinguish between shocks associated with a recession and the disaster shocks. The outlier adjustment misleads the inference of business cycle if the sample includes various types of outliers. The rest of paper is organized as follows. Section 2 explains the CI in Japan and the outlier adjustment by the ESRI, and shows the effect of the adjustment using descriptive statistics. Section 3 extends the MSDF model by incorporating a heavy-tailed error distribution and an SV process. Section 4 introduces the Bayesian method used to analyze this model. Section 5 fits the model to macroeconomic data in Japan and summarizes the results. Finally, Sect. 6 concludes the paper.",
14,2,Journal of Business Cycle Research,24 July 2018,https://link.springer.com/article/10.1007/s41549-018-0030-4,Which Indicators Matter? Analyzing the Swiss Business Cycle Using a Large-Scale Mixed-Frequency Dynamic Factor Model,November 2018,Alain Galli,,,Male,Unknown,Unknown,Male,"For policy institutions such as central banks, it is important to have a timely and accurate measure of past and current economic activity and the business cycle situation. The most prominent example for such a measure is gross domestic product (GDP). However, GDP is only released at a quarterly frequency and with a substantial delay. In Switzerland, for instance, quarterly GDP is published approximately two months after the respective quarter has ended. In addition to this timeliness issue, there is a second potential caveat regarding the use of GDP as the main business cycle measure: GDP also captures elements that are not directly linked to the business cycle and the underlying momentum of the economy. Examples of such elements are non-cyclical GDP components such as public spending and value added of the public sectors, or transitory components (idiosyncratic events, weather effects, etc.). Finally, the specific way quarterly GDP is calculated usually implies that it is revised late, often quite substantially, e.g., by benchmark revisions, new annual GDP figures or changes in the underlying calculation methods. Such ex-post changes in a business-cycle measure are problematic from a policymaking perspective. Therefore, to assess the current state of the economy (a) in a more timely manner and at a higher frequency, (b) in a more cycle-oriented manner and (c) in a way that makes them less prone to revisions, policymakers often use alternative measures of economic activity and conditions. Prominent examples of such business cycle measures are the Aruoba-Diebold-Scotti Business Conditions Index (see Aruoba et al. 2009), the Federal Reserve Bank of Philadelphia State Coincident Indexes (see Crone and Clayton-Matthews 2005) and the Conference Board Coincident Economic Index (see The Conference Board 2001) for the US or the EuroCOIN index (see Altissimo et al. 2001, 2010), the EuroMInd index (see Frale et al. 2010) and the EuroSTING index (see Camacho and Perez-Quiros 2010) for the euro area. For Switzerland, the KOF barometer (see Abberger et al. 2014) is currently the only (publicly available) broad-based business cycle indicator. It is calculated on a monthly basis by extracting the first principle component from a large set of selected indicators, where the ragged edge issue is treated by a vertical realignment of the data and the mixed frequencies are taken into account by assigning the observations of quarterly variables to all months of the respective quarter. In this paper, I construct a new business cycle measure for the Swiss economy that uses state-of-the-art methods: It is based on a version of the dynamic factor model (DFM) that (a) can be calculated in real-time, even when some indicators are not available yet for the most recent periods (ragged edge setup), (b) is available at a monthly frequency, (c) incorporates a very large number of economic indicators (large-scale setup) and (d) includes both quarterly and monthly indicators (mixed-frequency setup). The two last points are necessary since the index uses a large and broad-based set of monthly and quarterly indicators. The use of a such a large data set and the specific choice of model are motivated by the results from an out-of-sample forecasting exercise in Galli et al. (2017). These showed that, for the case of Switzerland, a DFM based on a large data set (including quarterly indicators and using four factors) significantly outperforms a DFM based on a small data set. The importance of quarterly variables comes from the fact that for Switzerland, several important indicators are only available at the quarterly and not the monthly frequency. From a detailed assessment of the resulting business cycle index, six findings emerge. First, an analysis of the relative importance of the indicators for the business cycle index suggests that the 100 most important indicators account for only 52% of the index. The obtained business cycle index is therefore driven by a large and broad based set of indicators and not only by a small subset of variables. This makes the index much more robust. Furthermore, it is also in line with the findings in Galli et al. (2017, 2018) that show the importance of modeling the Swiss business cycle using a broad set of indicators instead of only a small subset, which is in contrast to the results for other countries. Second, the contributions of the different indicators to the index over time show that although there is a considerable degree of comovement across indicators, there can also be periods over which the indicator categories (e.g., industry sector, foreign trade, construction sector, labor market, etc.) and indicator types (e.g., surveys, hard data, financial indicators, foreign indicators) differ more substantially from each other. Third, an analysis of the contribution of news within the real-time data flow sheds light on how the business cycle index takes into account new data that have been released since the last estimation. This suggests that financial variables are the main source of news to the model within the target month itself, surveys in the month after and hard data two months after. Fourth, a more detailed investigation of the role of financial and foreign variables shows that, overall, an index based only on domestic non-financial indicators would be very similar to the index including all variables. However, focusing solely on the peaks and troughs, there are considerable differences. For instance, not including financial and foreign variables would miss the recession in 2002/3 and point to a much stronger acceleration after the financial crisis in 2008 than was actually the case in terms of GDP developments. The reduced business cycle index also sees a somewhat more intense downturn after the exchange rate appreciation in 2011 and 2015 and a more sluggish recovery thereafter than the official GDP figures suggest. Furthermore, in terms of the role of news, an index excluding foreign and, in particular, financial variables moves forecast revisions a full month later. Fifth, analyzing the sensitivity of the new business cycle index to the real-time data flow reveals that the index becomes quite accurate already 30 days after the respective target month has ended. Accuracy gains are highest in the target month itself and the month after. Sixth, assessing the revisions of the business cycle index in real-time shows shows that the index is substantially less prone to revisions than quarterly GDP: while annualized quarter-on-quarter Swiss GDP growth is revised by 1.4 percentage points on average from its first release after three years, the business cycle index is only revised by 0.5 percentage points. The remainder of this paper is organized as follows. Section 2 outlines the modeling approach and Sect. 3 describes the data. Section 4 presents the resulting business cycle index and its key characteristics, both in terms of its performance as a measure of economic activity and with respect to the contributions of the various indicator groups. Finally, Sect. 5 concludes.",7
14,2,Journal of Business Cycle Research,21 May 2018,https://link.springer.com/article/10.1007/s41549-018-0028-y,Do Global Crude Oil Markets Behave as One Great Pool? A Cyclical Analysis,November 2018,Niyati Bhanja,Arif Billah Dar,Aviral Kumar Tiwari,Unknown,Male,Unknown,Male,"The marked change in the price spread between West Texas Intermediate (WTI) and Brent Crude, the two most influential light crude oil benchmarks of North America and Europe respectively, has become a hot topic in the current international energy forums. One related aspect is whether the crude oil markets have become regionalized or globalized (one great pool) in the recent years. Globalization in crude oil markets refers to the presence of strong co-movements. On the contrary, if the markets are fragmented and display least association, they are considered to be regionalised. An insight into the nature of the global oil market is critical in understanding the dynamics of international crude oil prices as well as its economic and financial implications. The integration of global crude markets is pertinent in determining the efficiency of certain energy policies, for example, drawing down of the strategic petroleum reserve and inventory-sharing plan of the International Energy Agency (Weiner 1991). Furthermore, the co-movement of oil markets across regions bears significant implications on portfolio allocation and hedges through spot and future oil contracts (Reboredo 2011). The globalization–regionalization debate in the crude oil markets has led to the emergence of two strands in literature. One strand led by Adelman (1984) advocates that the markets behave as one great pool. The idea hinges on the active arbitrage activity that induces long term co-movement in the crude oil markets, hence, making them behave as one great pool. The other strand led by Weiner (1991) visualizes the crude oil markets as regionalized units. Theoretically, this second viewpoint sprouts from the idea that the long-term contracts between importers and exporters, as well as the supplier diversification policy adopted by certain oil importing countries, would not have been important had the crude oil markets behaved like one great pool. The quality differential in crude oils and the transportation costs have also been frequently cited as the key reasons behind regionalization. The historical patterns of crude oil data have supported market globalization. The WTI and Brent crude oil price movements have been in tune with each other in the past, thereby maintaining the price spread in the $5 per barrel band until late 2009. Since early 2010, the WTI, however, has consistently been traded at a discount against the Brent. The price divergence in fact escalated to the extent of pushing the spread to a historic high of $29 per barrel in late September 2011. Given the implications, these unconventional developments in the benchmark oil prices have attracted considerable attention from policy-makers and professionals in the energy industry. Janzen and Nye (2013) have documented the impact of this unusual price spread on the economy of the Organization of the Petroleum Exporting Countries (OPEC). Given that the crude oils produced by these countries are priced in line with the WTI, the substantial plunge in the WTI price had placed their investment and fiscal budget on a challenging track. Moreover, while formulating the energy policy, the oil producers rely heavily on the benchmark oil prices (Brigida 2014), and this uncommon price divergence indicates the limitation of the WTI in reflecting the global demand–supply balance (Fattouh 2011) and acting as a representative crude oil price. The recent atypical patterns in the crude oil markets thus present a suitable case for testing whether the crude oil markets behave like one great pool. In this paper, we have attempted to gauge whether the global crude oil markets are regionalized (a set of different markets) or tend to behave as one great pool (globalized). We have provided a methodological improvisation for studying the market behaviour by decomposing the prices into different time horizons. One advantage of this approach is that it offers an understanding of the time specific market dynamics. Oil prices are affected by a multitude of factors which operate at different time horizons, including few days of labour strike in the oil producing nations, month-long infrastructure bottlenecks, and years of recession in the importing countries. We have further improvised our procedure by testing the dynamics with wavelet multiple and cross-correlation and not merely with simple pair-wise wavelet correlation. Conventional wavelet based correlation comes with certain drawbacks. For instance, this analysis involves the evaluation of some plots and the comparison of a large number of wavelet and cross-correlation graphs. Moreover, within the multivariate context, when dealing with a number of crude oil markets, the pair-wise correlation may turn spurious due to its possible relationships with other crude oil markets. Multi-scale wavelet correlations are also susceptible to type-1 errors due to the experiment-wise error rate (please refer Cohen et al. 2003). Given these disadvantages of pair-wise correlation, in this research, we have relied upon wavelet multiple correlation and cross-correlation proposed by Fernandez (2012) to test the globalization–regionalization hypothesis in the international crude oil markets. The methodology has recently been applied for testing the integration in stock and commodity prices (Tiwari et al. 2013, 2015; Bhanja and Dar 2015). To the best of our knowledge, this paper is the first one to analyse the globalization–regionalization debate in the crude oil markets using this innovative approach. We have ascertained the robustness of the results by estimating the pair-wise coherence at different time horizons. It has been proven that the crude oil markets behave as one great pool at the longer time horizons; hence, the results are robust and have also been supported by the convergence test. The rest of this paper has been divided into various sections. Relevant literature has been reviewed in Sect. 2. The methodology has been given in Sect. 3. The data on crude oil prices have been listed in Sect. 4. In Sects. 5 and 6, the results have been discussed and concluded, respectively.",7
14,2,Journal of Business Cycle Research,03 November 2018,https://link.springer.com/article/10.1007/s41549-018-00035-2,Structural Change and Business Cycles in Japan: Revisiting the Stylized Facts,November 2018,Satoshi Urasawa,,,Male,Unknown,Unknown,Male,"Apart from the question why economies grow, which is the focus of growth theory, a key issue in macroeconomics is why economic activity fluctuates—that is, why there are business cycles. Understanding the source and nature of business cycle characteristics is important not only from an academic perspective, but also from a policy-making perspective in order to gauge the overall state of the economy and to devise effective economic policies. Advances in economic theory and empirical techniques have substantially increased our understanding of business cycles, and scholars have identified a range of basic business cycle characteristics that to a degree can be regarded as universal and constant over time. At the same time, however, it is also easily conceivable that if there are substantial changes in the underlying structure of the economy, the characteristics of business cycles may also change as a result. Figures 1 and 2 show the cyclical component and the trend component of developments in Japan’s real gross domestic product (GDP) from the mid-1980s onward. The figures indicate that whereas business cycles, which are represented by the cyclical component of real GDP, appear to fluctuate with some regularity within a certain range, the overall trend of the economy displays a decline in the slope around the mid-1990s. Given this change in the trend growth rate of the economy against the background of a host of structural changes, the question naturally arises whether any changes in business cycle developments can be observed. Cyclical component of real GDP. Notes: (1) The cyclical component of real GDP falling within a range of 6 to 32 quarters is defined as the business cycle. The figure shows the business cycle component obtained using the CF filter. (2) The thick line is based on benchmark year 2005 data, while the thin line is based on benchmark year 2011 data. (3) The vertical lines show the official business cycle peak and trough dates determined by the Cabinet Office (the same applies to Fig. 2) Real GDP and its trend component. Note: The solid line shows real GDP (on a benchmark year 2005 basis, in trillion yen), while the broken line shows the trend component of real GDP obtained using the CF filter The aim of this study is to examine this question in detail. To this end, the study starts by dividing the period from 1980 onward into two subperiods—with the period up to 1999 regarded as the period before and the period from 1999 onward as the period after substantial structural change—and investigating and comparing the characteristics of business cycles before and after this year. The analysis shows that particularly notable changes can be observed in the way firms adjust labor input. Therefore, in the next step, a time-varying parameter structural vector autoregression (TVP-VAR) model taking the time-varying nature of the underlying structure of the economy into account is estimated in order to examine these changes in the way labor input is adjusted in more detail. In the literature on business cycle studies, there was an active debate before the global financial crisis on the so-called “Great Moderation” referring to the decline in output volatility observed from the 1980s onward in the United States [see, e.g., Kim and Nelson (1999), McConnell and Perez-Quiros (2000), Chauvet and Potter (2001), and Stock and Watson (2002)].Footnote 1 The literature has offered various explanations for this stabilization of the macroeconomy, focusing primarily on changes in economic structure, improved macroeconomic policies, and the absence of major economic shocks. For example, Kim and Nelson (1999) argue that the volatility of economic shocks has declined and that differences between growth rates during booms and busts were also smaller than in the past, while McConnell and Perez-Quiros (2000) argue that the volatility of durable goods production including inventories has decreased. Meanwhile, Stock and Watson (2002) cited improved economic policies as well as the absence of negative economic shocks such as productivity and commodity price shocks. While the “Great Moderation” had been regarded as a permanent phenomenon, following the global financial crisis, the focus of the debate appears to have shifted to the role of structural issues as exemplified by the “secular stagnation” hypothesis by Summers (2014). For Japan, from a somewhat different perspective, Morita (2014) investigates the sources of Japanese business cycles since the 1990s, taking into account both external shocks and domestic supply and demand shocks, using a sign-restricted VAR model. Morita (2014) shows that approximately 30–50 percent of the forecast error variance of output can be explained by external shocks. Using data on industrial output, Iyetomi et al. (2011) investigate the factors driving business cycles in Japan to find that the main factor is real demand shocks. Meanwhile, Hasumi et al. (2018) decompose output into a trend and a cyclical component to investigate the reasons for the decades of stagnation since the 1990s using a medium-scale new Keynesian dynamic stochastic general equilibrium (DSGE) model. This study investigates developments in Japan’s business cycles under structural change, including the impact of economic shocks such as the global financial crisis, based on the conventional approach of frequency domain analysis. Employing more than 60 macroeconomic time series, the analysis seeks to identify in which areas changes in business cycle characteristics can or cannot be observed. Although business cycle characteristics, as mentioned above, to a degree can be regarded as universal, the results indicate that they are not immutable. Specifically, notable changes can be observed in hours worked and employment; on the other hand, in other areas the basic mechanisms driving the business cycle appear to have remained largely unchanged, as indicated by the link between cyclical fluctuations in other series and cyclical fluctuations in output. Given these findings, in order to examine the causes and implications of changes in business cycle characteristics where they are observed, a TVP-VAR model is developed, allowing the analysis of business cycle trends in a robust manner. While TVP-VAR models have been mainly used in the field of monetary policy analysis, they also lend themselves to the analysis of business cycle dynamics. The remainder of this study is organized as follows. Section 2 attempts to identify the point in time at which the observation period should be split into a period before and a period after structural change. Section 3 then outlines the frequency domain analysis employed in this study. Next, Sect. 4 empirically examines the business cycle characteristics before and after the structural change and investigates changes in these characteristics. Section 5 then focuses on the mechanism through which firms adjust their labor input, which is where especially notable change can be observed, and presents analyses using the TVP-VAR model. Finally, Sect. 6 concludes.",2
14,2,Journal of Business Cycle Research,29 May 2018,https://link.springer.com/article/10.1007/s41549-018-0029-x,Price-Setting Behavior in Brazil: Survey Evidence,November 2018,Arnildo Correa,Myrian Petrassi,Rafael Santos,Unknown,Unknown,Male,Male,"Price-setting behavior is a key variable to any central bank, as it determines the nature of price stickiness and the effect of monetary policy on inflation and real variables. The macroeconomic effects of nominal shocks in many of the models used for policy analysis rely on the assumption of nominal rigidity. Thus, understanding the extent and causes of price stickiness has been the objective of extensive literature dealing with micro price data (Bils and Klenow 2004; Dhyne et al. 2006; Klenow and Malin 2011; Nakamura and Steinsson 2008; among others).Footnote 1 Some aspects of the behavior of price setters, however, cannot be fully characterized only by observing the price adjustments registered on large micro price datasets. Issues such as the rationale behind price setters’ decisions, the importance of different theories of price rigidity, the type of information used in price reviews, and the main factors driving price changes have been better addressed in the literature using qualitative data through price surveys. After the seminal work of Brinder (1991) and Blinder et al. (1998) using data from the United States, many national surveys have explored the detail of firms’ price-setting practices. Examples include surveys in Sweden (Apel et al. 2005), the Euro Area (Fabiani et al. 2005, 2006), Canada (Amirault et al. 2006), and the United Kingdom (Greenslade and Parker 2010). This paper reports the findings of a survey with Brazilian firms conducted by the Central Bank of Brazil. It covers the responses of 7002 firms surveyed from July 2011 to April 2012 in three economic sectors: manufacturing, services and commerce. After analyzing the price-setting behavior of Brazilian firms, the paper discusses some policy implications. Regarding firms and market characteristics, our survey analysis suggests the following findings. The Brazilian economy is diversified, but individual companies concentrate their businesses in a few products, especially in the services’ sector. Most firms report having some market share, which means that monopolistically competitive markets seem to be the rule in Brazil. The country’s economy is considerably closed: less than 20% of Brazilian firms report to sell their products to the rest of the world and nearly 74% operate only in the domestic market. Furthermore, it is worth noticing that the Brazilian economy experienced, in the surveyed period, a decrease in the 12-month consumer price inflation, which went from 6.87% in July 2011 to 5.10% in April 2012, approaching, but still above, the center of the 4.5% p.a. inflation target. Brazil, like other emerging economies, has experienced high levels of inflation for long periods, including hyperinflation in the 80s and 90s. Inflation memory is still present and, in this environment, 38.9% of Brazilian firms reported past inflation as important, while 37.5% attributed importance to expected inflation. These numbers seem to be closely related to the numerical estimations of parameters for the Brazilian Phillips curve (see, for example, Minella and Souza-Sobrinho 2013). 
Gouvea (2007), using a large dataset of Brazilian CPI price quotes, found no evidence of relevant asymmetry between price increases and decreases when considering both the direction and the frequency of price changes. However, the magnitude of positive price changes is significantly greater than that of the negative changes, pointing to a potential source of asymmetry between price increases and decreases. Gouvea associates this with the Brazilian inflation environment. To understand these potential asymmetries, we asked firms if the criteria used for implementing price increases are the same ones used for implementing price decreases, and the reasoning behind that choice. Firms reported different criteria for increasing and decreasing prices.Footnote 2 Moreover, they reported that the costs of gathering and processing the information involved in price-adjustment decision-making are low. In fact, more than half of the firms answered that review their prices frequently. We also found a negative relationship between adjustment costs and frequency of price changes, while the degree of competition seems to be correlated with more frequent price reviews and changes. With regard to price-setting practices, firms change prices 3.6 times per year on average, which is in line with studies of price-setting behavior in Brazil involving micro price data (Gouvea 2007; Barros et al. 2009). The services’ sector seems to be more rigid, while commerce is more flexible. The survey investigated the form of pricing rules. The majority of Brazilian firms adjusts prices using state-dependent practices, but some also consider elements of time dependency, particularly in the services’ sector. Markup pricing seems to be the dominant price-setting strategy followed by firms. Another important element is the price adopted by competitors: around 67% of the respondents consider it is crucial to change prices only after acknowledging the competition’s repricing. We evaluated the main drivers of price changes. Considering the aggregate economy, results suggest that the two most important determinants are the costs of intermediate goods and the inflation rate. The relevance of the first factor is compatible with the markup-over-cost strategy followed by firms; and the second factor should not be a surprise in the case of Brazil, considering its long history of high inflation. The survey also evaluated the relevance of the most important theories of price rigidities proposed in the literature. Four theories seem to be more appropriate in the case of Brazil: menu-cost, cost-based pricing, explicit contracts, and implicit contracts. Beside the relevance of these theories, problems of coordination and adjustment in non-price factors are minor obstacles to faster price changes in Brazil. The structure of the paper is the following. Section 2 presents the survey design, the sample selection and the main characteristics of the firms. Section 3 investigates the price reviewing stage and provides evidence on both the costs of reviewing prices and the time- and state-dependent nature of pricing rules. Section 4 deals with the actual price changing stage, documenting how often firms change prices and the strategies followed when they reprice, and exploring the relations between survey and theory. The main factors driving price changes are analyzed in Sect. 5, and Sect. 6 concludes with a summary of results and a discussion of some policy implications.",4
15,1,Journal of Business Cycle Research,11 October 2018,https://link.springer.com/article/10.1007/s41549-018-00033-4,Inspecting the Relationship Between Business Confidence and Industrial Production: Evidence on Italian Survey Data,April 2019,G. Bruno,L. Crosilla,P. Margani,Unknown,Unknown,Unknown,Unknown,,
15,1,Journal of Business Cycle Research,17 October 2018,https://link.springer.com/article/10.1007/s41549-018-00034-3,Measuring Brazilian Economic Uncertainty,April 2019,Pedro Costa Ferreira,Raíra Marotta B. Vieira,Ingrid C. L. de Oliveira,Male,Unknown,Female,Mix,,
15,1,Journal of Business Cycle Research,25 March 2019,https://link.springer.com/article/10.1007/s41549-019-00036-9,Consumers’ Perception of Inflation in Inflationary and Deflationary Environment,April 2019,Ewa Stanisławska,,,Female,Unknown,Unknown,Female,"Inflation perception usually does not attract as much attention as inflation expectations, although understanding how consumers perceive price changes proves to be important as many studies document significant divergence of consumers’ opinions from the actual price developments captured in official statistics. The unexpected rise in inflation perception in the euro area countries after the introduction of new banknotes and coins in 2002 triggered a wave of research on this issue and resulted in a better understanding of the process of inflation perception formation. The period of deflation offers another interesting case. Therefore, the aim of this paper is to analyze a link between consumers’ inflation perception and the actual price developments in an inflationary and deflationary environment. To this end we exploit the fact that Poland recently experienced a mild but relatively long-lasting deflation (about 2 years) and use survey data to gain more insight into the process of formation of inflation perception by consumers. There are at least three reasons for studying inflation perception. First of all, it is well documented that consumer inflation expectations are to a certain degree backward-looking. For example, Łyziak and Mackiewicz-Łyziak (2014), based on quantified survey data on inflation expectations, estimate a degree of forward-lookingness of consumers in the European Union countries at about 10% (higher in the advanced economies and lower in the economies in transition) and confirm a significant role of past inflation and inflation perception in shaping inflation expectations. Another piece of evidence comes from analysis of micro data: Jonung (1981), Bryan and Venkatu (2001) and Stanisławska (2010), among others, show that inflation perception is one of the most important factors affecting consumers’ inflation expectations. Therefore, better knowledge of inflation perception formation leads to a more comprehensive understanding of inflation expectations. Secondly, inflation perception plays an important role in the quantification of survey data on inflation expectations. The main source of information on inflation expectations of the European consumers is the European Commission Consumer Survey in which the main question on inflation expectations is formulated in a qualitative way, in relation to the perceived inflation (respondents are asked whether future prices will rise faster than currently, rise at a similar rate, rise more slowly, show no change or fall). In order to transform this qualitative information into a number directly comparable to inflation figures, some quantification procedure is needed. The probability method uses perceived inflation as a scaling factor to the function of percentages of responses to the survey question, while the regression method exploits the empirical relation between qualitative data on the perceived inflation and the actual inflation in order to quantify inflation expectations based on qualitative opinions (see e.g. Nardo 2003). Finally, the divergence of inflation perception by consumers from official inflation estimates might signal problems with credibility of official price statistics (Brachinger 2008). The main research question of our paper focuses on the impact of deflation on consumers’ inflation perception—an issue, to the best of our knowledge, not addressed in the literature so far. Therefore, firstly, we examine the relation between inflation perception and the actual headline inflation, and show that it was disrupted when prices started to fall (in terms of annual changes in the CPI). As a part of this analysis we test for asymmetric adjustments in inflation perception with regard to the direction of change in inflation and size of the change. Secondly, we compare quantitative inflation perception with alternative price indexes which incorporate some cognitive biases postulated in the literature on consumers’ inflation perception. It turns out that they do not explain changes in inflation perception during the deflationary period. Thirdly, we estimate subjective weights of 12 components of the consumer price index to test which groups of goods and services affect inflation perception to the greatest degree. The contribution of our research to the literature is twofold. Most importantly, we broaden the understanding of non-linearities in the process of formation of inflation perception. This issue remains under-researched for both inflation perception and inflation expectations. To the best of our knowledge only few articles investigate the impact of the level of inflation on inflation expectations or inflation perception (Carroll 2003; Dräger et al. 2014; Easaw et al. 2013; Ehrmann 2015) and none of them pays attention to deflationary period, because samples under investigation do not cover deflationary episodes or authors focus on other aspects. Moreover, apart from the analysis of inflation perception formation during deflationary period, we test for presence of two other types of asymmetries: with regard to direction of change in inflation and to size of change. Secondly, we complement the literature on determinants of inflation perception. We follow Antonides et al. (2006) and Hałka and Łyziak (2015) to learn about the importance of CPI components on consumer inflation perception, but instead of employing balance statistics derived from qualitative opinions about prices, we use quantitative responses to the survey question.Footnote 1 Such direct measurement of inflation perception facilitates the interpretation of data and does not require applying any quantification procedures, which inevitably lead to arbitrary assumptions. The importance of a proper measurement of inflation perception is shown in Dias et al. (2010) who find contradictory evidence on the breakdown in the relationship between inflation perception and the actual inflation during the euro changeover when inflation perception is measured as a balance statistic and as a quantified series. Again, we investigate differences in determinants of inflation perception in inflationary and deflationary period. The remainder of this paper is structured as follows. Section 2 contains literature review. Section 3 describes methods applied in the research, while Sect. 4 introduces data on inflation perception and two price indexes employed as alternative reference series to headline inflation. Sections 5 to 7 present empirical results. We start by establishing the relationship between perceived and actual CPI inflation and testing its stability during deflation. Next we regress inflation perception on CPI inflation employing both symmetric and asymmetric specifications. In Sect. 6 we estimate subjective weights of 12 CPI sub-indexes and provide supportive evidence for the claim that more frequently bought goods and services and those making up a large share in consumers’ expenditures have greater impact on inflation perception. Section 7 shows that substituting CPI inflation with alternative price indexes, such as frequent out-of-pocket price index, does not help to explain consumers’ inflation perception during deflation. The last section concludes.",4
15,1,Journal of Business Cycle Research,18 August 2018,https://link.springer.com/article/10.1007/s41549-018-0031-3,CAMPLET: Seasonal Adjustment Without Revisions,April 2019,Barend Abeln,Jan P. A. M. Jacobs,Pim Ouwehand,Male,Male,Male,Male,"Seasonality, which Hylleberg (1986, p. 23) defines as ‘the systematic, although not necessarily regular or unchanging, intrayear movement that is caused by climatic changes, timing of religious festivals, business practices, and expectations’, is often considered a nuisance in economic modeling. Consequently, a whole industry has come into existence that is devoted to seasonal adjustment. The U.S. Census Bureau Basic Seasonal Adjustment Glossary (https://www.census.gov/srd/www/x13as/glossary.html) describes seasonal adjustment as ‘the estimation of the seasonal component and, when applicable, also trading day and moving holiday effects, followed by their removal from the time series. The goal is usually to produce series whose movements are easier to analyze over consecutive time intervals and to compare to the movements of other series in order to detect co-movements.’ Large seasonal movements may hide other movements of importance and it is easier to see related movements in different series after seasonal adjustment. Therefore macroeconomic time series are typically seasonally adjusted before being used in economic and econometric analyses. Several procedures are in use, varying from the Census X-11 family (U.S. Census Bureau, Bank of Canada; for a brief overview see Monsell 2009) to TRAMO/SEATSFootnote 1 and STAMP (Andrew Harvey and collaborators; http://stamp-software.com/). Recently, the two most popular methods, Census X-12-ARIMA and TRAMO-SEATS, merged into X-13ARIMA-SEATS, to become the industry standard. Underlying all these seasonal adjustment methods is the decomposition of an observed series into latent non-seasonal and seasonal components. The aim is to extract the unobserved components from the observed series. The methods produce seasonal effects that are relatively stable in terms of annual timing, within the same month or quarter, direction and magnitude. Trend-cycle and seasonal components are traditionally extracted using sequential centered moving average (CMA) filters and recently ARIMA and Unobserved Components (UC) models. The series are pretreated to adjust for outliers and trading-day and holiday effect, and forecast and backcast to deal with the beginning and the end of the series to avoid phase shifts in the series. One consequence of using CMA filters and ARIMA and UC models is that past values of the unobserved components change when new observations become available, thus causing revisions in real-time data. The current practice of changing seasonal factors only once a year implies the existence of annual revisions in vintages of time series, going back some three years; see, e.g., Croushore (2011). This property of seasonal adjustment is well-known and well-documented in the seasonal adjustment literature, see e.g. Bell and Hillmer (1984), Bell (1995) and more recently Czaplicki (2015) and Czaplicki and McDonald-Johnson (2015).Footnote 2 This paper presents CAMPLET, a new method, especially focusing on the feature that the method does not produce revisions when new observations become available, and on its ability to deal with changes in seasonal patterns. The method consists of a simple adaptive procedure to extract the seasonal and the non-seasonal component from an observed time series. Once this process is carried out, there will be no need to revise these components at a later stage when new observations become available. The main strength of CAMPLET is in seasonal adjustment when revisions are totally inacceptable, for example inflation realizations should not be revised after a wage agreement has been reached, or implausible. Economic Tendency Survey data are not revised over time, hence seasonal adjustment should not lead to revisions (Abeln et al. 2017). A second application of CAMPLET is on series that have a changing seasonal pattern. Section 4 gives an illustration for the international retailer Ahold. CAMPLET might also be employed to seasonally adjust Chinese economic statistics, which suffer from moving holidays due to the Chinese New Year (Roberts and White 2015). Other applications are the seasonal adjustment of short (volatile) series or series in which seasonality is correlated with trend and cycle (Hindrayanto et al. 2018). Recently, CAMPLET has been used to check the robustness of results obtained with other seasonal adjustment methods (Hecq et al. 2017; Smirnov et al. 2017). Although turning points are obtained in the same period for many series, differences in the dates of turning points, i.e. phase shifts, do occur. The remainder of this paper is structured as follows. The next section presents CAMPLET. In Sect. 3 we evaluate the outcomes of CAMPLET and X13-ARIMA-SEATS in a controlled simulation framework using a variety of data generating processes. Section 4 shows the potential of CAMPLET by focusing on similarities and differences with respect to X13-ARIMA-SEATS in the analysis of three time series: U.S. non-farm employment, operational income of Ahold, an international retailer, and real GDP in the Netherlands. Section 5 concludes.",5
15,2,Journal of Business Cycle Research,09 May 2019,https://link.springer.com/article/10.1007/s41549-019-00037-8,The Determinants of Optimal Exchange Rate Regimes in High and Low Oil-Producing Countries,December 2019,Eman Elish,,,Female,Unknown,Unknown,Female,"The choice between a fixed and a flexible exchange rate arrangement has been argued in a wide scope of the literature. Both regimes have their own merits and demerits, and the choice is even more challenging for oil-producing and exporting countries. Oil-producing countries face a major challenge when using a fixed exchange rate regime (see Habib and Strasky 2008; Kandil and Nandwa 2015; Numammadov 2012; Aliyev 2014). Such regimes lead to the strengthening of the procyclical macroeconomic conditions that occur when oil prices and revenues fluctuate. When oil prices ‘boom’, government revenue increases, and, consequently, a rise in spending occurs, leading to inflationary pressure and a negative real interest rate. In contrast, a fall in oil prices leads to a fall in revenue, reduced spending, disinflation and even sometimes deflation. Additionally, a rise in real interest rates will normally occur. Frankel (2011) argued that diversified economies supported by a sound financial pillar will be most likely to use a flexible exchange rate regime. Such a regime would help these economies overcome any oil price shocks and protect the real sector from spill-over effects. Elbadawi and Gelb (2010) studied the effect of oil shocks on Arab oil-exporting countries and found that for countries that have a ratio of oil exports to total exports greater than 20%, a standard deviation in oil prices of 30 to 35% can cause a 6% increase or decrease in the GDP. Arab oil-producing countries have not been able to address the countercyclical pressures of oil shocks with appropriate stabilization policies. Data analyses of oil revenues, fiscal revenues and domestic liquidity have revealed that fixed exchange rate or relatively inflexible exchange rate systems lead to the transmission of oil shocks through these channels. Kandil and Nandwa (2015) used Norway as a benchmark for oil-producing and exporting countries. Notably, Norway smoothly reduced the spill-over effect of oil shocks from 1980 to 2010, with short output cycle durations and little GDP volatility, by using a flexible exchange rate regime. The choice of the exchange rate regime in oil-producing countries has always been made from a holistic perspective (Klein and Shambaugh 2009; Frankel 1999). Empirical studies have begun to emphasize the differences with respect to the level of oil exports and the region in which the study is conducted, as noted by Frankel (2011) and Kandil and Nandwa (2015), who investigated Arab oil-exporting countries. In these studies, the differences among the economic behaviours and monetary regimes of high oil-producing Arab Gulf countries and low oil-producing Arab countries, such as Egypt and Tunisia, were considered. Aliyev (2014) conducted a study of resource-rich countries and found that these countries are more likely to use a fixed exchange rate regime than are resource-poor countries. Output volatility played a significant role in this determination in resource-rich countries versus resource-poor countries. Moreover, the roles of democracy and central bank independence were found to be two significant determinants in resource-rich countries associated with fixed exchange rate regimes. Al-Abri (2014) used a consumer welfare-based model to examine the optimal choice of exchange rate regime in small oil-exporting countries. The analysis revealed that a flexible exchange rate regime is optimal for mitigating external shocks and that a peg to a basket of currencies would enhance consumer welfare in his model. The analysis of how both HOP and LOP countries can be classified based on the exchange rate regimes they implemented in two different years in the model is shown in Tables 1 and 2. Table 1 shows the distribution of HOP countries according to their de facto and de jure exchange rate regimes used in 1996 and 2015. The regimes are classified in terms of fixed, intermediate and floating exchange rates. In 1996, all HOP countries were using de facto fixed exchange rate regimes. However, the de jure regimes exhibited a different categorization among HOP countries, where 29% of the countries used a fixed exchange rate regime, 57% used an intermediate exchange rate regime, and only 14% used a floating exchange rate regime. In 2015, this pattern changed for HOP countries in both the de facto and the de jure systems. In the de facto system, 38% of HOP countries used a fixed exchange rate regime, 50% used an intermediate fixed exchange rate regime and 12% used a floating exchange rate regime. The de jure system results, on the other hand, indicated that 22% of HOP countries in 2015 used a fixed exchange rate regime, 64% used an intermediate exchange rate regime and 14% used a floating exchange rate regime. This finding suggests that in both years, HOP countries generally used fixed or intermediate exchange rate regimes, whether in a de facto or de jure system. Table 2 shows the regime distribution for LOP countries in 1996. All of these countries adopted a fixed exchange rate system, and 29, 58 and 13% had fixed, intermediate and floating exchange rate regimes, respectively. This distribution was similar to that for HOP countries at this time but more flexible one. According to Mabro (1998), a severe oil shock occurred that began in 1996. Therefore, both HOP and LOP countries implemented more flexible exchange rate systems to combat this crisis. However, in 2015, all LOP countries were using intermediate exchange rate systems, although 37% of them proclaimed to use a fixed exchange rate system and 63% implemented an intermediate exchange rate system. This finding suggests that all LOP countries developed intermediate and floating exchange rate systems and avoided fixed exchange rate systems. Alesina and Wagner (2003) were the only researchers that used governance indicators in a different area and found that some countries claimed to use de jure exchange rate systems while actually implementing de facto systems. Taking a thorough analysis of the governance variables (based on the percentile rank) that are used in the models of HOP and LOP countries, the frequency analysis showed that these variables greatly differ. Specifically, HOP countries displayed higher percentile rankings than LOP countries in the sample of countries used in the model. From the literature discussed above, credibility and accountability are two important dimensions that support the use of a fixed exchange rate system. Thus, in this study, it is assumed that the differences between both categories of countries at the governance level lead to different choices of exchange rate regimes, as shown in Figs. 1, 2, 3, 4, 5, 6. When analysed for HOP versus LOP countries, all histograms of governance indicators were unimodal with one peak in the highest interval. In Fig. (1), the majority of the observations for HOP countries are concentrated in the interval between 50 and 100, and the peaks are from 50–60 to 90–100. The minimum value is − 1.219512, and the maximum is 97.66545. For LOP countries, the majority of the observations are concentrated in the interval between 150 and 200. Notably, the highest interval is from 150 to 200, the minimum is 1, and the maximum is 390. Figure (2) represents the government effectiveness for HOP versus LOP countries. The majority of the observations in HOP countries are concentrated in the intervals from 0–15 to 30–60, with the highest interval from 40 to 50. The minimum value is 0.7317073, and the maximum is 134.0938. For LOP countries, the majority of the observations are concentrated in the interval from 65 to 80, with the highest interval from 65 to 70. The minimum value is 14.63415, and the maximum is 87.80488. The political stability for HOP countries is shown in Fig. (3), and the majority of the observations are concentrated in the interval from 0 to 50, with the highest interval from 0 to 10. The minimum value is 0, and the maximum is 116.4675. However, in LOP countries, the majority of the observations are concentrated in the intervals from 0–30 to 50–80, with the highest interval being 60–65. The minimum value is − 0.7246374, and the maximum is 99.75845. For the regulatory quality shown in Fig. (4), in HOP countries, the majority of the observations are concentrated in the intervals from 0–10, 40–60 and 90–100, the highest interval is 50–60. The minimum value is − 2.205882, and the maximum is 97.59615. For LOP countries, the majority of the observations are concentrated in the interval from 40 to 90, with the highest interval from 60 to 65. The minimum value is 12.5, and the maximum is 96.56863. The HOP trends in Fig. (5) show that the majority of observations are concentrated in the interval from 0 to 40. The histogram is bimodal due to having two peaks from 60–70 to 90–100, which indicates that HOP countries fall into two categories. The minimum value is.4784689, and the maximum is 97.15639. For LOP countries, the majority of the observations are concentrated in the interval from 45 to 65, and the highest interval is from 45 to 55. The minimum value is 6.937799, and the maximum is 89.47369. Finally, the voice and accountability indicator for HOP countries is shown in Fig. (6). The majority of the observations are concentrated in the interval from 0–30, and the highest interval is from 20–30. The minimum value is − 3.846154, and the maximum is 99.75962. For LOP countries, the majority of the observations are concentrated in the interval between 45 and 80, and the highest interval is from 75 to 80. The minimum value is 9.615385, and the maximum is 89.42308. Distribution of observation for Corruption HOP versus LOP Distribution of observation for Government effectiveness HOP versus LOP Distribution of observation for Political Stability HOP versus LOP Distribution of Observation for Regulatory rules HOP versus LOP Distribution of observation for Rule of Law HOP versus LOP Distribution of observation for Voice and accountability HOP versus LOP Based on the theoretical background regarding the role of a sound choice of exchange rate regime to overcome oil price shocks and the recent finding that not all regimes can perfectly match the requirements of all countries, especially considering their oil production status and classification, this study will investigate the determinants of the exchange rate regime in oil-producing countries from two perspectives. The first perspective is from the level of production by classifying countries as HOP or LOP to see how their determinants differ. The second perspective involves the quality of governance and whether this factor leads to different regime choices. The paper is organized as follows. Section 2 discusses the theoretical foundation for exchange rate regime choice determinants and different exchange rate classifications. Section 3 explains the hypothesis and choice of model variables. Section 4 discusses the methodology and model results, and the final section presents the conclusions and policy recommendations.",
15,2,Journal of Business Cycle Research,02 July 2019,https://link.springer.com/article/10.1007/s41549-019-00038-7,What has Changed After the Great Recession on the European Cyclical Patterns?,December 2019,Ana Rodríguez-Santiago,,,Female,Unknown,Unknown,Female,"The study of economic cycles is currently a hot policy issue in Europe, as some countries are rethinking the new role played in the European Union (EU), and new member states are reconsidering the effects of economic integration. In some sense, one could expect that economic integration should lead to similar patterns in macroeconomic dynamics. The inexistence of a common cycle should imply the adoption of different treatments for member countries of the European Union. It is important to know whether every country follows the same cyclical pattern to adapt the economic policies that are developed by the European Union Commission for the purpose of common welfare and, especially, the European Central Bank, responsible for monetary policy and financial supervision of the economic and monetary union (EMU). The analysis of heterogeneity between member countries is especially relevant for the application of monetary policy, since the application of a “one-size-fits-all” policy by the EMU or European Commission could not have the expected effect if the countries are showing diverging economic dynamics, in words of Feldstein (1997): “Uniform monetary policy and inflexible exchange rates will create conflicts whenever cyclical conditions differ among the member countries”. In this context, this piece of research attempts to provide empirical evidence on the existence, or not, of a business cycle pattern among member countries and on the potential effect of the 2008-financial crisis on such a pattern. The finding of patterns in business cycles, the study of linkages among different formations of countries and the provision of explanations for these stylized facts have been the core of a body of theoretical and empirical literature devoted to the study of the business cycle (see the survey conducted by De Haan et al. 2008). Most authors studying European economic cycles have focused on synchronization, rather than on any other characteristic of the cycle, with few exceptions, such as Krolzig and Toro (2005) or Camacho et al. (2008). After the enlargement of the European Union, the focus was placed on the search for a core-periphery pattern in the group but taking into account only the synchronization between countries or some presupposed central core (Camacho et al. 2006; Darvas and Szapáry 2008; Gomez et al. 2012; König and Ohr 2013; Wortmann and Stahl, 2016). There is still a gap in the literature about the effects of the Great Recession on the cycles of European Union countries, with some exceptions, such as Antonakakis et al. (2016), who explained the debt crisis by the shocks on the periphery countries, Grigoraş and Stanciu (2016), who stated that European economies are less synchronized after the crisis, or Ahlborn and Wortmann (2018), who applied fuzzy clustering to explain changes in European clusters after the crisis. The issues we address in this paper are how to provide evidence on the business cycle similarities and dynamics before and after the beginning of the Great Recession. To this end, first, we check if the length, depth, shape and synchronization of business cycles across European Union countries are now following more similar dynamics than before; second, and after more than a decade, we can extend the analysis to the new entrants and provide a comprehensive analysis once the lately adhered economies to the European Union have been established; and last, we explore if the 2008-financial crisis has introduced some changes in the cyclical linkages across this set of European countries and whether the crisis has changed the way in which relationships between countries were established before the crisis. Our results point to the reconsideration of linkages across economic cycles of member countries of the European Union. The Great Recession seems to have changed patterns in cyclical linkages since the results obtained show a desynchronization between European economies, despite the fact that the countries are now showing more similar business cycle characteristics, as a consequence of the severity of the crisis experienced during the late 2000s. The rest of this paper is structured as follows: Sect. 2 describes the methodology and data for analysing business cycle characteristics and synchronization. Section 3 develops and applies the methodologies for identifying groups of countries with similar patterns in cycles. Finally, Sect. 4 presents some general conclusions about the research and the study.",
15,2,Journal of Business Cycle Research,21 September 2018,https://link.springer.com/article/10.1007/s41549-018-0032-2,A PMI-Based Real GDP Tracker for the Euro Area,December 2019,Gabe J. de Bondt,,,Male,Unknown,Unknown,Male,"Timely and reliable signals about the current state of the economy are essential for analysts and policymakers. Survey indicators are therefore closely monitored, because they provide up-to-date and often unique monthly signals of current economic developments. This report takes a closer look at a tracker for quarter-on-quarter real GDP growth in the euro area using only the Purchasing Managers’ Index (PMI) composite output released by IHS Markit. This PMI-based GDP tracker for the euro area was introduced by de Bondt (2012). For other studies on the use of the PMI for tracking real GDP growth see the references in de Bondt (2012) and D’Agostino and Schnatz (2012; US) Lahiri and Monokroussos (2013; US), Müller (2013; Switzerland), Camacho and Garcia-Serrador (2014; euro area), Gajewski (2014; euro area, Germany, France, Italy and Spain), de Bondt and Schiaffi (2015; euro area and US), Chien and Morris (2016; US and China), Chudik, Grossman, et al. (2016; 48 countries), and Vernazza (2018; euro area and US). The PMI surveys are the most closely monitored business surveys in the world, mainly because they are the first indicators of economic conditions published each month and are therefore available well ahead of GDP data compiled by statistical institutes. Another important survey source for the euro area is the European Commission. A detailed comparison between the two surveys reveals that the PMI is best suited for quarter-on-quarter changes in euro area real GDP and the European Commission survey for year-on-year movements (European Commission 2017). Given that this study analyses quarter-on-quarter GDP growth, the focus is exclusively on the PMI composite output. It relates to other studies that nowcast real GDP growth on the basis of a wide range of soft as well as hard data. This field contains a wide list of euro area studies as referred to in Basselier et al. (2018). This recent example (see Angelini et al. 2010) for an older publication in this journal) shows that survey data contain relevant information not captured by hard data and thus have additional value beyond their timeliness. The latter finding is consistent with the euro area evidence reported by Girardi et al. (2016) and de Bondt and Kosekova (2018). Chudik, Grossman, et al. (2016) show that regardless of the data-rich forecasting methods considered, including Lasso, Ridge, partial least squares and factor-based methods, PMIs are useful for nowcasting GDP growth in 48 countries. This report contributes to the nowcasting literature in four ways. Firstly, formal forecast evaluation tests for the euro area indicate that a real-time real GDP growth tracker only on the basis of the PMI composite output index is of similar accuracy for final GDP as the first GDP vintage. The latter can be viewed as the “ultimate” real-time nowcast evaluation test. The equal nowcasting accuracy is not only found for the total sample, but also for before and after the 2008/2009 crisis. The PMI-based GDP tracker beats the first official GDP number in real time in seven out of the fourteen calendar years since the start of the flash GDP estimate for the euro area in 2003. This is a surprising outcome, also because the size of the panel used for the euro area PMI composite is small. It contains 2000 private service sector firms and 3000 manufacturing firms. The second contribution is that no evidence in favour of an instable tracking performance is found, with the exception around the free fall in GDP in 2008/2009. The third contribution is that it draws seven conclusions from a closer look at what is driving the outstanding PMI-based GDP track record. For the first time PMI data broken down by firms size as well as export status are used, with the former showing a further improvement in the real-time tracking record. Another conclusion is that the PMI composite output helps in explaining revisions to GDP, an aspect so far ignored in the literature. It implies that PMI surveys are potentially valuable for statisticians to improve the accuracy of the first preliminary flash estimate of euro area real GDP. The fourth and final contribution is that the reported evidence is not only interesting on its own, but also valuable as contrasting evidence of nowadays popular “big data” analytics in economic forecasting; see, among others, Chudik, Kapetanios, et al. (2016), Elliott and Timermann (2016), Bok et al. (2017) and Kim and Swanson (2018). Technical advanced tools using more and more series do not necessarily perform better in real time than a simple, single indicator-based GDP tracker. The remainder of this report introduces a PMI-based GDP tracker for the euro area. It presents not only the in-sample tracking record for the final GDP from the PMI composite output index but also from the first GDP vintage. Section 3 presents the real-time performance of the PMI-based GDP tracker and compares it with that of the first GDP vintage. Section 4 analyses the stability of the relationship between real GDP growth and the PMI composite output. The next section describes seven conclusions which can be drawn from the PMI-based tracking performance. Section 6 provides concluding remarks.",4
16,1,Journal of Business Cycle Research,01 February 2020,https://link.springer.com/article/10.1007/s41549-020-00041-3,Has the Financial Crisis affected the Real Interest Rate Dynamics in Europe?,April 2020,Nektarios Aslanidis,Selva Demiralp,,Male,Female,Unknown,Mix,,
16,1,Journal of Business Cycle Research,14 March 2020,https://link.springer.com/article/10.1007/s41549-020-00042-2,Consumers Confidence and Households Consumption in Brazil: Evidence from the FGV Survey,April 2020,Aloisio Campelo Jr.,Viviane Seda Bittencourt,Marco Malgarini,Male,Female,Male,Mix,,
16,1,Journal of Business Cycle Research,12 March 2020,https://link.springer.com/article/10.1007/s41549-020-00043-1,The Challenge of Pairing Big Datasets: Probabilistic Record Linkage Methods and Diagnosis of Their Empirical Viability,April 2020,Yaohao Peng,Lucas Ferreira Mation,,Unknown,Male,Unknown,Male,"Brazil is the fifth largest country in the world in terms of territorial extension and population; in this sense, there are abundant databases of administrative records, which register personal information regarding labor, healthcare, social assistance programs, among other aspects. Therefore, the same person is often listed in several different databases—for example, the same individual may receive medical services, be a beneficiary of a wealth transfer or housing distribution program; that person may be a formally signed employee, involved in a judicial process, among many other possible situations. Given the amount of data in the Brazilian context, the aggregation of the various existing information sources to a single database is a challenging and worthy task, as this unified data source would represent a very useful tool for studies related to public policy evaluation and the analysis of socioeconomic structures. Concerning continentally extensive countries such as Brazil, the quality of the data used in these studies is a fundamental element for the quality of posterior policy analysis. However, those various databases may have quite different structures. For example, a citizen may be a beneficiary of a welfare program, while the administrative records do not register his name, registering the name of the household head in which he lives instead, or the name of his parent. Besides, any typos or misunderstandings (e.g.: providing the wrong birth date) make record linkage across different databases a challenging task. Given the large Brazilian population, even a small margin of error in this regard can lead to great deviations during the processing of data for studies and research. Especially for the Brazilian case, the registry inconsistency in different databases has facilitated cases of fraud schemes involving social benefit concessions, such as the recent case of the Brazilian Social Security Fraud (uncovered in August 2019) that led to 12 arrests and a loss of approximately 1 million Dollars to the Brazilian government. Studies performed by the Brazilian Federal Court of Accounts (TCU) under the aegis of Judgement N° 2587/2018—which addresses the use of data integration analysis in the public administrations and fraud detection—indicated that more than 6 million people were registered with different names in the Brazilian Internal Revenue Service (RFB) and the National Employment Movement Registry (CAGED). Moreover, over 1.5 million people had different birth dates in the aforementioned datasets. In this sense, the use of record linkage techniques can help to diminish those inconsistencies, thus improving the quality of the matches, and ultimately lead to better allocation of public budget and effective social programs. In this sense, finding a way to perform record linkage efficiently—both in terms of the quality of the crossing and in terms of temporal viability—is a relevant issue. Therefore, this paper seeks to list the techniques developed by the specialized scientific literature for this task, presenting good practices at the international level and the empirical feasibility of those methods in real-world applications, notably in databases with a big number of observations and features. Moreover, we described recent advances in the related scientific production that can be explored for the integration of Brazilian administrative records. Finally, we performed a study case involving Brazilian data and verified the classification performance under various settings of blocking, phonetic algorithms and computational implementations.",
16,1,Journal of Business Cycle Research,28 January 2020,https://link.springer.com/article/10.1007/s41549-020-00040-4,Information Content of Russian Services Surveys,April 2020,Liudmila Kitrar,Tamara Lipkind,Georgy Ostapkovich,Female,Female,Male,Mix,,
16,2,Journal of Business Cycle Research,30 November 2020,https://link.springer.com/article/10.1007/s41549-020-00051-1,Editorial,November 2020,Michael Graff,,,Male,Unknown,Unknown,Male,,
16,2,Journal of Business Cycle Research,04 May 2020,https://link.springer.com/article/10.1007/s41549-020-00046-y,Predicting U.S. Business Cycle Turning Points Using Real-Time Diffusion Indexes Based on a Large Data Set,November 2020,Yongchen Zhao,,,Unknown,Unknown,Unknown,Unknown,,
16,2,Journal of Business Cycle Research,25 August 2020,https://link.springer.com/article/10.1007/s41549-020-00047-x,Disagreements in Consumer Inflation Expectations: Empirical Evidence for a Latin American Economy,November 2020,Juan Camilo Anzoátegui-Zapata,Juan Camilo Galvis-Ciro,,Male,Male,Unknown,Male,"Central banks are focusing more on understanding the formation of inflation expectations of market agents to improve the transmission mechanisms and effectiveness of monetary policy (Coibion et al. 2020). Expectations management in an inflation targeting regime is essential for controlling prices because these reflect the expected behavior of the economy. In this regard, consumers are the agents with the highest influence on aggregate demand, and consequently, anchoring their expectations is key for central banks that want to strengthen their credibility (Coibion et al. 2019). Although increased anchoring is seen on average in inflation expectations, some issues still subsist due to heterogeneity between agents and, especially, to significant differences between inflation expectations of the so-called experts, or professionals, and other agents, such as consumers. In that respect, recent evidence shows that consumers make predictions with information from mainstream media and that their expectations have more dispersion, inertia and instability (Lamla and Lein 2014, 2015; Dräger and Lamla 2017). Due to high opportunity costs, consumers seek to absorb accurate and easy to understand information from communication channels that are not those used regularly and directly by the central bank (Sims 2003; Carroll 2003; Blinder and Krueger 2004). Thus, consumers decide to search for information on inflation and filtered monetary policy in informal channels to form their expectations (Dräger et al. 2016; Binder 2017a). Literature on the determinants of disagreement in inflation expectations is scarce in the case of emerging economies, even though it is important for macroeconomic stability (Montes et al. 2016). Therefore, this article fills a void in the literature on the formation of inflation expectations with the investigation on how searches on the Internet for information on the Central Bank of Colombia, Market Basket, living cost, and observed inflation reduces the disagreement in consumer inflation expectations. The Central Bank of Colombia has been concerned about building and maintaining its reputation and credibility since adopting inflation targeting in 2000 (Galvis and De Mendonça 2017; Galvis and Anzoátegui 2018). In addition, the Colombian economy is the third largest economy in South America, with significant economic activity, such as tripling the GDP per capita between 2000 and 2018, reaching the lowest country risk in its history, adopting fiscal deficit targets, joining the OECD due to best business practices and improving the dissemination of news and economic data. Therefore, Colombia is a good research laboratory for investigating the effects of the media on disagreements in expectations. The remainder of this paper is organized as follows. Section 2 presents the methodology and data implemented in this study. In Sect. 3, we present the econometric estimates of the proposed models, and, finally, the Sect. 4 concludes.",1
16,2,Journal of Business Cycle Research,09 November 2020,https://link.springer.com/article/10.1007/s41549-020-00049-9,Machine Learning and Nowcasts of Swedish GDP,November 2020,Kristian Jönsson,,,Male,Unknown,Unknown,Male,"Business tendency surveys are standard ingredients in nowcasting and short-term forecasting of macroeconomic variables (see e.g. Kaufmann and Scheufele 2017, and the references therein). When nowcasting Swedish GDP growth, several different models have utilized survey questions and indices from the Economic Tendency Survey (ETS) of the National Institute of Economic Research (NIER). For example, survey questions have been employed in linear indicator models by Österholm (2014), and in factor models and mixed-frequency models by Hansson et al. (2005) and den Reijer and Johansson (2019), while different sentiment indices have been employed in linear indicator models by Billstam et al. (2017). The current article adds to this literature by probing the question if sentiment indices, when used in a machine learning algorithm, similar to the approach of Richardson et al. (2018), can match the nowcasting ability of linear nowcasting models. The machine learning algorithm studied in the current article is the nearest neighbor algorithm. This algorithm forecasts GDP growth by comparing current sentiment indices to historical observations of the same indices. The average of the GDP growth numbers observed during the periods where the current ETS indices are close to the historical record is set to be the nowcast of the current GDP growth. The results indicate that the nearest neighbor algorithm compares very well to linear indicator models when nowcasting Swedish GDP growth, producing measures of forecasting accuracy that are as good as those of the linear indicator models. The results indicate that it could be beneficial to augment standard nowcasting model suits with machine learning methods. In Sect. 2 of this article, the standard set of linear indicator models is presented, while the nearest neighbor algorithm is presented in Sect. 3. Data and nowcast accuracy results are presented in Sects. 4 and 5, respectively, while Sect. 6 concludes.",3
16,2,Journal of Business Cycle Research,21 April 2020,https://link.springer.com/article/10.1007/s41549-020-00045-z,Spanish Economic-Financial Crisis: Social and Academic Interest,November 2020,Noelia Araújo-Vila,Jose Antonio Fraiz-Brea,Arthur Filipe de Araújo,Female,Male,Male,Mix,,
16,2,Journal of Business Cycle Research,31 January 2020,https://link.springer.com/article/10.1007/s41549-020-00039-x,Cyclical Dynamics and Trend/Cycle Definitions: Comparing the HP and Hamilton Filters,November 2020,Kristian Jönsson,,,Male,Unknown,Unknown,Male,"Decomposing aggregate time series into long-term trends and short-term fluctuations, and then studying the different parts, constitute an integral part of many economic analyses. Over time, several different approaches, ranging from simple univariate methods to elaborate structural approaches, have been suggested for performing such decompositions (some examples are found in Beveridge and Nelson 1981; Blanchard and Quah 1989; Baxter and King 1993; Hodrick and Prescott 1981, 1997; Álvarez and Gómes-Loscos 2017; EU Independent Fiscal Institutions 2019). The specific choice of filter can depend on a wide range of factors including the need for structural interpretations, existence of theoretical underpinnings regarding the decomposition and modeling effort being put into the decomposition.Footnote 1 The filter of Hodrick and Prescott (1981, 1997) (the HP filter) has been used extensively in many applications over the years and is still frequently applied in economic analyses (see for example Cornea-Madeira 2017, for a list of recent applications). However, the filter has also been argued to have several drawbacks. For example, the filter has been documented to give rise to real-time instability (see e.g. Orphanides and van Norden 2002; Mise et al. 2005) and it has been argued that the filter gives rise to spurious dynamics (see e.g. Harvey and Jaeger 1993; Cogley and Nason 1995). Some of the drawbacks of the HP filter have been recently summarized by Hamilton (2018), who also suggests that a regression-based filter (here denoted the Hamilton filter) should be used instead. The suggested filter is proposed to achieve all the goals sought by the users of the HP filter without any of its drawbacks. The current paper highlights and compares the HP filter and the Hamilton filter with regard to the dynamics in the cyclical component of a filtered time series. More specifically, the current paper studies three different time series processes, that at least under some definition of trend and cycle components can be argued to have no cyclical dynamics, and compares how the dynamics look in the cyclical part of the HP- and Hamilton-filtered series. The main result indicates that both filters can give rise to cycle dynamics in filtered series. Looking closer at the Hamilton (2018) filter and the associated trend and cycle components, it is clear that it is the definition of trend and cycle components used in the Hamilton filter that causes the dynamics. Hence, the dynamics occur as a consequence of how the filter is constructed in terms of defining trend and cycle components. However, it is not obvious that the Hamilton definition of the components is always innocuous. Instead, it is easy to find examples where the definition can be problematic. Hence, the results indicate that unconditionally replacing the HP filter with the Hamilton filter, based on an argument that the sheer magnitude of the cyclical dynamics of the HP filter is a problem, may not be a better practice. The rest of this paper is organized as follows. In Sect. 2, the two filters are discussed and the cyclical dynamics problem of the HP filter is discussed. The Monte Carlo simulations, that are used the shed light on the dynamic properties of the cycle components of the two filters, are described in Sect. 3. The simulation results are presented in Sect. 4. Finally, Sect. 5 offers some concluding remarks.",7
17,1,Journal of Business Cycle Research,22 November 2020,https://link.springer.com/article/10.1007/s41549-020-00050-2,On the Aggregation of Survey-Based Economic Uncertainty Indicators Between Different Agents and Across Variables,April 2021,Oscar Claveria,,,Male,Unknown,Unknown,Male,"The analysis of economic uncertainty has gained renewed interest since the Great Recession. Despite the evidence that uncertainty shocks have an effect on real activity (Baker et al. 2016; Bloom 2009; Paloviita and Viren 2014), the elusive nature of uncertainty and the difficulty of measuring it, has meant that until recently its impact on the economy were not further explored. Dibiasi and Sarferaz (2020) emphasise the importance of defining what is understood by economic uncertainty. Based on the different ways in which economic agents form their expectations regarding unknown future events, Knight (1921) differentiated risk from uncertainty. While under risk, agents are able to allocate probabilities over future outcomes, uncertainty would be defined as the state in which agents are no longer able to form expectations about future events. As noted by Rossi et al. (2020), disagreement on the probability distribution of future outcomes would be a special case of Knightian uncertainty, since disagreeing on probability distributions automatically implies that the probability distributions are not correctly specified. The unobservable nature of economic uncertainty has given rise to different approaches to proxy it. Some authors have opted to gauge economic uncertainty by using the realized volatility in equity markets (Basu and Bundick 2017; Bekaert et al. 2013; Yıldırım-Karaman 2017), while others in oil and natural gas prices (Atalla et al. 2016; Hailemariam and Smyth 2019). Other authors have used the conditional volatility of the unforecastable components of a broad set of economic variables (Chuliá et al. 2017; Jurado et al. 2015). The ex-post nature of the latter approach has generated a strand of research that looks for ways to approximate economic uncertainty ex-ante. Most of this research makes use of quantitative expectations made by professional forecasters (Clements and Galvão 2017; Dovern et al. 2012; Krüger and Nolte 2016; Lahiri and Sheng 2010; Oinonen and Paloviita 2017). Jo and Sekkel (2019) used the forecast errors of consensus survey forecasts of various economic indicators to capture a real-time measure of uncertainty surrounding subjective forecasts. Rossi et al. (2020) proposed an uncertainty index based on density forecasts, which measures the distance, on average across forecasters, between the forecast distribution provided by an individual forecaster and the perfect forecast. As this proxy allows distinguishing between uncertainty and risk, the authors used it to analyse how both concepts relate to each other. Recently, Altig et al. (2020) proposed measuring uncertainty using the subjective probability distributions of managers about their own firm outcomes at a 1-year-ahead horizon. Castelnuovo (2019) and Cascaldi-Garcia et al. (2020) provide an overview of recent developments regarding the measurement of uncertainty. Kozeniauskas et al. (2016) distinguished between three types of uncertainty: micro uncertainty (firm-level shocks), macro uncertainty (aggregate shocks) and higher-order uncertainty (disagreement). Glas (2020) has carried out an in-depth analysis of the relationship between forecaster disagreement and macroeconomic uncertainty. Disagreement indicators based on survey expectations make use of prospective information, and in this sense are especially appropriate to evaluate the anticipatory properties of uncertainty proxies. One of the main sources of survey expectations are economic tendency surveys (ETS). Consequently, in recent years a growing number of studies have used the information coming from ETS to approximate uncertainty (Bachmann et al. 2013, 2018, 2020; Binding and Dibiasi 2017; Claveria 2020; Dibiasi and Iselin 2019; Girardi and Reuter 2017; Meinen and Roehe 2017; Mokinski et al. 2015). In ETS, agents’ expectations are elicited by asking about the expected direction of economic variables. Respondents are asked whether they expect a broad set of variables to rise, fall or remain unchanged. Dibiasi and Iselin (2019) have recently proposed using the non-response category in forward-looking questions to directly approximate Knightian uncertainty. Their approach relies on firm-level data and measures the share of firms that do not formalise expectations about their future demand. The most important ETS in Europe are the business and consumer surveys conducted by the European Commission. Firms are asked about production and other variables concerning developments in their sector, while households are asked about their spending intentions and the general economic situation influencing those decisions (price trends, unemployment expectations, etc.). We use information coming from both surveys to elicit agents’ economic expectations in eleven European countries and the Euro Area (EA). By focusing on two independent surveys, the industry survey and the consumer survey, we can simultaneously measure disagreement about a wide range of economic variables for both firms and households, combining the perspective of the supply and the demand side of the economy. We use two alternative measures of expectations’ dispersion to compute the level of disagreement in order to construct several indicators of discrepancy among firms’ and households’ expectations that gauge their perception of uncertainty. This study contributes to the existing literature by analysing the effects of aggregating the level of disagreement across variables and different types of agents. We provide a comparative view of firms versus households of the dynamic relationship between innovations in their expectations about future economic uncertainty and the evolution of economic growth. We use a bivariate vector autoregressive (VAR) framework to compute the impulse response functions (IRFs) and to generate out-of-sample forecasts of economic growth. We compare the forecasting performance of the uncertainty proxies and evaluate whether the aggregation of discrepancies among firms’ and households’ expectations, as well as across variables, helps to improve forecast accuracy.",2
17,1,Journal of Business Cycle Research,20 February 2021,https://link.springer.com/article/10.1007/s41549-021-00052-8,Intertemporal Cointegration Model: A New Approach to the Lead–Lag Relationship Between Cointegrated Time Series,April 2021,Takashi Oga,,,Male,Unknown,Unknown,Male,"In business cycle analysis, long-run relationships are often observed with time differencesFootnote 1 such as those between investment and products. Economists believe accurate lead–lag analysis improves economic forecasting; however, studies of how to identify the lead–lag interval are at a nascent stage. Our study presents a theoretical framework, Monte Carlo studies, and empirical analysis for analyzing the long-run relationship with lead–lags. specifically, we present a new type of cointegration model to analyze the long-run equilibrium relationships between a pair of leading and lagging integrated time series. It begins with Engle and Granger (1987) single regression model and then introduces a lagging dependent variable and a leading independent variable, both of which follow an I(1) process. For this regression model, if the error term is stationary, these two time series are considered to be in a long-run relationship with the lead–lags (called intertemporal cointegration). The analysis of the integration process has been one of the most important developments in recent econometrics research. The cointegration analysis developed by Granger (1981) and Engle and Granger (1987) introduces the notion of long-run relationships between pairs of time series; this analysis is called the cointegration-to-time-series analysis. Engle and Granger (1987), Engle and Yoo (1987) and Phillips and Ouliaris (1990) provide statistics and tables for cointegration testing. The seminal model of Engle and Granger (1987) consists of a regression in which both the dependent and the independent variables have a unit root with stationary disturbance. Thus, the Engle–Granger test is simple for a pair of time series: unit root tests are required for each time series. If both series have a unit root, then one should be regressed on the other to yield the estimated residuals. A unit root test is again required for the residuals if the result of subsequent testing indicates stationarity (i.e., the null hypothesis is rejected). The pair of time series is then recognized as a cointegrated system. Since the Engle–Granger test is easy to use, it has been widely employed by empirical analysts. From the perspective of business cycle analysis, the cyclical movement of macro variables has garnered attention, as the variables often have lead–lags. The effect of the treatment for economic events does not always emerge immediately (Belke 2000). Thus, analysts believe that leading variables can help predict current business conditions. For example, the widely used method of Bry and Boschan (1971) defines the peaks and troughs of the individual time series of business conditions to provide information on the lead–lag intervals between variables. Thus, the time difference between a pair of peaks (or troughs) in each series is just a lead–lag. However, determining the lead–lags between pairs of time series can be challenging, as each series has a different number of peaks (or troughs). As the peaks and troughs derived by the Bry–Boschan method are not econometric but descriptive statistics, it is difficult to evaluate their accuracy. Indeed, as the Markov switching model of Hamilton (1989) yields the econometrical turning points of business cycles, the same issues may still persist. To counter these issues, the model proposed in this study is simple and does not require adjustment in the corresponding peaks and troughs in the business cycle. The lead–lag interval between two series is identified by adopting the following procedure: (i) regressing several models that have one independent variable of different lead–lags with a fixed dependent variable using ordinary least squares (OLS) estimations; (ii) estimating parameters and testing statistics and the adjusted coefficient of determinations (\(\bar{R}^2\)) of the respective candidates; and (iii) selecting the lead–lag interval of the best fitting model using \(\bar{R}^2\) to estimate the true lead–lag among the models with a slope parameter that is significantly different from 0. After the regressions, we verify whether the estimated residuals are stationary as diagnostics. If they are stationary, we regard the pair of series as intertemporally cointegrated or cointegrated with lead–lags. Hence, we do not have to match the corresponding peaks and troughs of each series and do not require longer peaks or troughs in the dataset. Our theoretical work makes several important contributions:  The statistical characteristics of the model are similar to those of Engle and Granger (1987), that is, the model is superconsistent. The estimators of the candidates have distributions similar to the unit root test statistics at the time difference in the lead–lag interval between the true lead–lag and candidates. Larger differences render the distortion more severe. The variances of the disturbance of the candidates tend to increase proportionally with the digression between the true lead–lag and candidates. A larger estimated variance indicates a bigger digression in the lead–lag interval. These characteristics enhance the ability to identify the true lead–lag interval. The autocorrelations of the disturbance of the candidates are 0 near the true interval. Similar to the variance, the autocorrelations tend to increase proportionally with the absolute digression from the true lead–lag and are truncated. Thus, the cointegration test works well without any distortions only near the true intervals. The Monte Carlo simulations show that the aforementioned findings are appropriate and that the proposed procedure can identify the true interval with high probability; the probability of being correct (PoC), which is the probability of identifying the true lead–lag interval in the Monte Carlo simulations, is almost 1 under the parameter setting regularly used. Indeed, for the 264 parameter settings, including severe cases, the average PoC is over 0.9. We also use the Monte Carlo simulations to investigate why the procedure proposed in this study can identify the lead–lag intervals correctly. First, assuming that the data generating process (DGP) of cointegration with the intertemporal model is true, we show that the OLS estimators of the coefficients are equivalent to those of Engle and Granger (1987) (i.e., the model is superconsistent), which means that the estimator converges to the true parameter faster than in typical regressions. We also investigate the OLS estimator of the coefficients concerning the model of the candidates that have incorrect lead–lags. Under the true model, the estimator consists of the sum of the true parameter and small sample bias, which is the source of superconsistency; under the candidates, it consists of the product of them. This bias multiplied by the true parameter is similar to a unit root regression coefficient. While the regression coefficient is typically derived from a variable one period before the present one, in this study, it is generated from a variable several periods before the present one. From the empirical distribution in the Monte Carlo study, if the difference in the lead–lag intervals between the true lead–lag and candidates increases, the bias also increases in a negative direction from one. To estimate the disturbance variances, the empirical distributions are well separated, as they are proportional to the difference in lead–lag intervals between the true lead–lag and candidates. Therefore, these three properties, namely superconsistency, negative bias in the estimation of coefficients, and proportional increase in variance estimation, lead to successful identification. These Monte Carlo results are consistent with those from our theoretical works noted in Sect. 2. After the identification procedure, we refer to the diagnostics of the estimated residuals based on the cointegration test of Phillips and Ouliaris (1990). The tests on all models, regardless of whether the lead–lags are true, should indicate that the residual would be theoretically stationary. As the candidates have correlated disturbances, it is suggested that the test may be distorted. The unit root test and cointegration test are sensitive to autocorrelation and lag selection, as Emerson (2007) indicates sensitivity in the choice of lag selection. In the case of the candidates that are far from the true lead–lag interval, the null hypotheses, namely that the disturbance is not stationary, are accepted with a high probability; hence, the existence of long-run relationships is rejected. Consequently, we are confronted with the excess acceptance of the null in the conventional cointegration test with no lead–lags. However, by allowing nonzero lead–lags in the cointegration system, we can find new and significant long-run relationships between a set of time series. As a numerical example, we select leading and coincident business indexes of Japan, namely the Indexes of Business Conditions from the Economic and Social Research Institute (ESRI), Cabinet Office of Japan. These series typically illustrate long-run relationships with lead–lags between economic time series. The ESRIFootnote 2 reports that the lead–lag between the leading and coincident series is only a “few months.” We apply our procedure to a monthly dataset from 1985 through 2019 and identify the lead–lag interval as three months, consistent with the views of Japanese economists. For this dataset, long-run relationships with no lead–lag are also observed. However, our intertemporal model is preferred by an adjusted coefficient of determination. We conclude that the intertemporal cointegration model suggests a new relationship for economic time series, while the conventional cointegration tests without considering the lead–lags lead to incorrect relations. The remainder of this paper is organized as follows. In Sect. 2, we present the theoretical work on the intertemporal cointegration model. In Sect. 3, Monte Carlo simulations using our model with lead–lags are shown and a method for identifying the true intervals is proposed. In Sect. 4, we analyze the time series of the Japanese business index using the procedures suggested in the previous section as a numerical example. In Sect. 5, we provide conclusions and suggest future research directions.",
17,1,Journal of Business Cycle Research,19 March 2021,https://link.springer.com/article/10.1007/s41549-021-00053-7,Growth in US Durables Spending: Assessing the Impact of Consumer Ability and Willingness to Buy,April 2021,Hamid Baghestani,Sehar Fatima,,Male,Unknown,Unknown,Male,"This study investigates the role of detailed consumer durables-buying attitudes from the long-running Surveys of Consumers in explaining the behavior of US growth in durables spending. Our research is motivated by the psychological consumption theory, which asserts that spending, especially on durables, depends on consumer buying attitudes (Katona 1957, 1968, 1975). This theory, like the life-cycle consumption theory and the permanent-income hypothesis, maintains that consumers are forward-looking. Katona distinguishes between consumer “ability to buy” and consumer “willingness to buy.” While “ability to buy” depends on consumer income and assets, “willingness to buy” depends on such psychological factors as consumer confidence about future personal finance and overall economic conditions. Garner (1981) notes that consumer confidence primarily affects durables spending, because purchases of durable goods are often discretionary, and consumers can postpone such purchases, if they are uncertain about the economic future. In line with the psychological consumption theory, we maintain that consumer durables-buying attitudes inform “willingness to buy” and thus help explain the behavior of growth in durables spending. The literature contains studies that use consumer sentiment as an additional explanatory variable in consumption models (Baghestani and Kherfi 2015; Batchelor and Dua 1992; Burch and Gordon 1984; Fereidouni and Tajaddini 2017; Throop 1992). With the exception of Burch and Gordon (1984), the remaining studies find consumer sentiment useful in enhancing the explanatory power of consumption models. The primary contribution of this study is to use instead detailed consumer durables-buying attitudes. In line with Garner (1981), we maintain that the use of detailed data helps better understand the role that psychological factors play in explaining growth in durables spending. The University of Michigan Surveys of Consumers (MSC) provides the attitudinal data. Every month, this survey takes a nationally-representative random sample of at least 500 US consumers in order to collect responses to approximately 50 core questions. One core question that is included in the “Household durables-buying conditions” category asks, “About the big things people buy for their homes—such as furniture, a refrigerator, stove, television, and things like that. Generally speaking, do you think now is a good time or a bad time for people to buy major household items?” The follow-up questions include several options for participants to choose as reasons for their responses. These options focus on current and expected prices, current and expected interest rates, prosperous times, uncertain future, and affordability. As for the methodology, we employ four augmented-autoregressive models, with growth in durables spending as the dependent variable. Three of these models comprise two sets of independent variables. The first set includes income and wealth growth as the determinants of consumer “ability to buy” in addition to changes in real interest rates. The second set includes detailed consumer durables-buying attitudes as the determinants of consumer “willingness to buy.” Our estimation results for 1988–2019 reveal that income and wealth growth in addition to real interest rates significantly explain durables spending growth. Attitudinal factors that significantly explain durables spending growth include the difference of opinion of consumers as to whether prices/interest rates are low or high in addition to the difference of opinion of consumers regarding prosperous times versus uncertain future. There are four noteworthy aspects of this study. First, the literature contains numerous studies that assess the predictive information of consumer sentiment for consumption (e.g., Mueller 1963; Mishkin 1978; Garner 1991; Carroll et al. 1994; Bram and Ludvigson 1998; Ludvigson 2004; Easaw et al. 2005; Al-Eyd et al. 2009; Dees and Brinca 2013; Lahiri et al. 2016). We instead investigate how consumer durables-buying attitudes explain growth in durables spending. Second, understanding the behavior of durables spending is important, since research shows that changes in such spending significantly contribute to cyclical fluctuations in economic activity (Cutherbuston 1980; Blanchard, 1993; Hall 1993; Easaw et al. 2005). Third, our models include psychological factors that influence consumer “willingness to buy” in addition to factors that influence consumer “ability to buy.” This helps provide insights about the independent information that psychological factors contain for explaining growth in durables spending. Fourth, in line with Garner (1981), we maintain that one should look at detailed responses in order to fully understand the role that psychological factors play and the policy implications that follow for stabilizing growth in durables spending. We proceed by reviewing the related literature in Sect. 2. Section 3 discusses the data. Section 4 presents the empirical results. Section 5 concludes.",5
17,1,Journal of Business Cycle Research,09 April 2021,https://link.springer.com/article/10.1007/s41549-021-00054-6,Measuring the Business Cycle Chronology with a Novel Business Cycle Indicator for Germany,April 2021,Agnieszka Gehringer,Thomas Mayer,,Female,Male,Unknown,Mix,,
17,1,Journal of Business Cycle Research,04 April 2020,https://link.springer.com/article/10.1007/s41549-020-00044-0,Are the European Commission’s Business and Consumer Survey Results Coincident Indicators for Maltese Economic Activity?,April 2021,Aaron G. Grech,Reuben Ellul,,Male,Male,Unknown,Male,"Though Malta is the smallest euro area economy, its economic performance during recent years has diverged significantly from that of its neighbours. Economic growth has been between three and four times the euro area average, fuelled both by rising exports but also increasingly by domestic demand. To a large extent, this performance was reflected by trends in optimism, as measured in the European Commission’s business and consumer surveys carried out in Malta during this period. However, while there is a very extensive literature that suggests that business and consumer confidence indicators can be very useful in forecasting economic activity, to date there have been no attempts to study this in relation to Maltese data. In this light, this report will try to see how indicative is the European Commission’s Economic Sentiment Indicator (ESI) of the current and future state of the Maltese economy. This is studied using both the first estimates of GDP growth and later vintages. The report then assesses the usefulness of the Maltese ESI and European Union ESI in forecasting the first vintage of Maltese GDP using a dynamic factor model, and checks whether the use of ESI data improves upon naïve forecasts in both a step-ahead and current quarter setting. This is of particular interest, given Maltese macroeconomic data are characterised by rather significant revisions. The report then assesses whether sectoral business survey results shed light about future activity in that particular sector. The final section looks specifically at results from the consumer confidence surveys and tries to determine whether Maltese household economic expectations are reflective of current conditions, or if they could be used as leading indicators. Since 1962 the European Commission has conducted regularly a monthly survey among tens of thousands of private sector firms and consumers in order to provide an indication of current and expected trends in national economies. The questions posed tend to ask for a qualitative reply, such as whether a firm expects to increase, reduce or maintain the same employment level over the next few months, or whether a consumer feels that the general economic situation will improve, worsen or remain the same.Footnote 1 The replies from a selected number of these questions are used to create sectoral indicators of confidence, which are in turn converted into a synthetic indicator of overall business conditions known as the ESI.Footnote 2 In November 2002, the European Commission extended its surveys to cover Malta. Initially only the manufacturing sector and households were surveyed. Subsequently in May 2007 the survey was extended to services firms, followed by construction companies a year later and finally the retail sector in May 2011. At present the manufacturing survey is carried out among 347 firms, covering 89% of manufacturing employment (with an unweighted response rate of 31%). The sample of services firms covers 612 entities, accounting for 64% of employment (unweighted response rate of 27%). 266 construction firms, or 80% of registered firms in this sector, are included in the sample, constituting nearly 96% of total employment (unweighted response rate at 24%). All registered wholesale and retail firms are surveyed (with an unweighted response rate of 34%). To gauge consumer confidence, one thousand Maltese households are contacted (with a response rate of 40%).Footnote 3 This makes these surveys the most comprehensive monthly economic surveys of Maltese firms and households. Given its timeliness and the forward-looking nature of some of the questions, the surveys published by the European Commission have been frequently used to supplement or complement macroeconomic data. Several studies have also been undertaken to assess the extent to which replies from these surveys can be used as coincident or leading indicators of economic activity. A detailed analysis of the European Commission surveys indicates that compared with other indicators, the surveys are best suited for year-on-year movements (European Commission 2017), fitting with this study’s investigation of annual growth in Maltese GDP. This study also relates to other work which considers survey replies as indicators for the depth and strength in GDP growth (de Bondt and Forsells 2017), the better weighting of components to improve forecasts (Soric et al. 2016), or the general assessment of survey or confidence indicators for forecasting developments in GDP in terms of techniques (Posta and Pikhart 2012), particular countries (Kuzmanovic and Sanfey 2012), or over the short-term (Mourougane and Roma 2002). The primary contribution of this research is the focus on Malta as a small open economy, with very diversified sectors and undergoing considerable structural changes. A priori one would expect aggregate confidence indicators to not be that useful to predict economic activity, as a small open economy should be more exposed to significant idiosyncratic external shocks.Footnote 4 Moreover given the volatility of economic data, one would expect that consumer sentiment indicators would not be that good a guide for macroeconomic developments.",1
17,2,Journal of Business Cycle Research,27 July 2021,https://link.springer.com/article/10.1007/s41549-021-00058-2,Trend-Cycle Interactions and the Subprime Crisis: Analysis of US and Canadian Output,November 2021,Max Soloschenko,Enzo Weber,,Male,Male,Unknown,Male,"The subprime crisis led to the most severe slump in the world economy since the Great Depression of the 1930s. On the empirical side, it is of particular interest to ascertain which shocks led to a near worldwide economic collapse. The main focus of the current study lies on an examination of the course of the subprime crisis in the US and Canada and its effect on their growth trend and business cycles. Another important aspect is the analysis of the simultaneous interactions between these unobserved trend and cycle components. For this purpose, a particular structure is implemented, designed to capture the causality between the innovations of both output components. The inclusion of Canadian data makes it possible to investigate the impact of the recent financial crisis on an economy with a recognised stable banking system.Footnote 1 This could be of interest since the spreading of the recent financial crisis to the real economy had an origin in a dramatic crisis of confidence among banks. Moreover, it will be also interesting to see whether the US results can be confirmed, given the country's traditionally strong economic integration with the USA. The following questions will be addressed: Were the two components, trend and cycle, driven by permanent or transitory shocks, or indeed both? Do the same innovations in the USA and Canada influence trend and cycle? In addition, are the respective components affected purely by their own shocks or do spillovers also play a significant role? It will be shown that during the subprime crisis trend and cycle were driven in both countries by permanent shock alone, but that different explanations are required. Moreover, a strong negative correlation of output components for Canada and the USA will be confirmed. Furthermore, with regard to trend and cycle, the USA has a higher volatility than Canada. For both countries there is a strong decrease in the influence of the structural cycle shock over time. The underlying paper makes use of the class of unobserved components (UC) models in its empirical analysis. Traditionally, uncorrelated trend and cycle components were assumed (cf. Harvey, 1985, Clark, 1987). The same independence of cyclical shocks and trends is usually assumed in DSGE models (e.g. Smet & Wouters, 2007). In later developments, such as Balke and Wohar (2002) and Morley et al. (2003), this assumption was relaxed and subsequently extended by Weber (2011) in the framework of simultaneous unobserved components (simult UC) models, which capture the contemporaneous causality structure of trend and cycle shocks. These authors established that it is possible to take into account the correlation between the permanent trend and transitory cycle innovations, while maintaining the identifiability of the structural model. In this paper the identification of the simult UC model is achieved through heteroscedasticity, i.e. the necessary information is taken from time-varying variances of the structural shocks. Additionally, drift breaks will be introduced into the structural model in accordance with criticism levelled by Basistha (2007). Moreover, the existence and number of breaks will be endogenously determined and statistically verified. In their influential work, Stock & Watson (1988) comment on the interconnection between trend and cycle as follows: „Multivariate empirical analysis suggests that trend variations and business cycle movements appear to be related. One interpretation of this link is that business cycle fluctuations might be caused by innovations in growth. An alternative explanation–equally consistent with the empirical results–is that cyclical fluctuations cause changes in long run growth.” In the first case (that is, if causality goes from trend to cycle) RBC theory can be taken as a plausible explanation. This theory regards business cycles as a reaction to changes in the prospects for long-term economic growth (see Prescott, 1987). In a UC model, a negative spillover from trend to cycle implies a lagged reaction of output to the shock, as the cycle exhibits mean-reverting adjustment after an impulse. The causal effect from cycle to trend cannot, however, be ruled out in any case. As Proietti (2006) notes, negative trend-cycle correlation would go in line with adverse effects of temporary shocks on the permanent GDP component. This paper focuses on such negative linkages, as will be found in the applications. As an explanation, e.g. an expansive fiscal policy may lead to a positive demand effect in the short-run, but have a negative effect (because of the raising tax and interest rates) on the potential output in the long-run (see Clark, 1987). The same may hold true for inflationary (e.g. monetary) shocks, if they provoke increased uncertainty, dampened trade development or inefficient product and labour substitution under price staggering. Moreover, increases in unemployment compensation (or disability benefits, following Clark, 1989) might trigger short-run consumption-based upturns but discourage productive work in the long run. Importantly, the simultUC model makes it possible to distinguish empirically between the respective components as well as causality directions. As will be shown later, the impact direction can change with time or even run in both directions. To anticipate some interesting empirical results, it can be stated that during the subprime crisis for both countries we determine a strong increase of the structural trend variance compared to the previous period. This underlines the permanent effect and, thus, structural problems as a potential cause. Moreover, both components are more volatile in Canada than in US. With regard to the Great Moderation, for both countries the applied structural framework shows that the strong negative correlation between the trend and cycle can be traced back primarily to the structural trend shock. This indicates a causal effect from trend to cycle and supports theories according to which business cycles are caused by real permanent shocks. This paper is organised as follows: The subsequent section presents the model setup and deals with the identification problem. Section 3 then applies the theoretical framework to the industrial production (IP) of the US and Canada, interprets the results and examines robustness. Lastly, a short overview of the key findings is provided.",1
17,2,Journal of Business Cycle Research,09 July 2021,https://link.springer.com/article/10.1007/s41549-021-00057-3,"The Time–Frequency Relationship between Oil Price, Stock Returns and Exchange Rate",November 2021,Sudipta Das,,,Female,Unknown,Unknown,Female,"Economic decisions differ significantly at different time horizons. Markets are composed of the actions taken by different participants. Each of these participants operates on different time scales at each moment. For example, a trader operating in the market may have a very long view of years; the chartist may operate with a time horizon of days or weeks. The nature of relationship between various economic variables might vary over different time horizons or hold at several time scales from longest to shortest horizon or vice versa. The relationship between two variables is a function of time at different time scale. However, standard measures are not appropriate to distinguish between the short-term and long-term components of risk and co-movement of variables under study. Economic shocks produce distinct effects on the dynamics of time series at different time horizons, suggests that frequency-specific measures of correlation may yield several new insights (Chaudhuri & Lo, 2015). The wavelet transform is an appropriate analytical tool applicable where both the time horizons of economic decisions and the strength of relationships between variables vary with time and frequency (Ramsey, 2002). The wavelet transform decompose a time series into their time scale coefficients, each associated to a specific frequency bands. It captures variations across frequencies and has ability to capture events which are local in time. Wavelet analysis is more detailed and provides more information than the correlation that does not take into account the evolution over time of the relationship (Arnold et al., 2018). The wavelet analysis is a model-free approach of estimating time-varying correlations. This property makes it a very powerful technique in comparison with other standard methods that rely on the parameter estimation method (Vacha & Barunik, 2012). Further, the wavelet analysis is especially relevant to the analysis of non-stationary time series in economics and finance. The wavelet approach can track how the different scales related to the periodic components of the signal changes over time (Cazelles et al., 2008). In recent, several financial crises have triggered active discussions among academics, practitioners, and policymakers about the relationship between stock market, crude oil prices, and exchange rates. Measuring the dynamic link between stock market, crude oil prices, and exchange rates has prime importance in terms of practical implications in portfolio management, asset allocation, and risk management. The time–frequency analysis provides insights on risk management at different time horizon and a framework for portfolio diversification across these time scales. The existing literature on dynamic links between stock market, crude oil prices, and exchange rates is mostly restricted to standard econometric methodologies with the stationarity assumption. The advantage of wavelet analysis is that it does not require any stationarity assumption. By stationarity we mean time series whose frequency content does not change in time are called stationary series. The purpose of this paper is to focus on the potential use of wavelet technique in the context of time-series analyses in economics and finance. In this context, this study investigates the co-movements between: (a) stock market returns and crude oil prices, and (b) exchange rates and crude oil prices in the time–frequency space and identify the direction (in-phase or out-phase) of correlation. Further, this paper tries to reveals information on the effect–result relationships across time scales and over time. Crude oil plays a vital role in economy for almost every country in this world. The fluctuation of international crude oil price has effect on the economic stability of a country. In addition, according to Qiang et al. (2019), “Oil has become an important bargaining chip in contemporary international political, military, and diplomatic relations.” Further, stock market return in a country reflects health of the economy. There is a strong link between oil price fluctuations and stock markets (Bagirov & Mateus, 2019; Thorbecke, 2019). Similarly, the oil importing or exporting countries get affected because of fluctuating exchange rate in the foreign exchange market. As appreciation of US dollar weakens the purchasing power of oil importing countries, this dependency on crude prices adversely affect any economy and have spillover impact on the monetary policy, consumption and investment behavior of the economy. Crude oil is considered as one of the most important energy commodities. Fluctuation of crude oil prices caused by the dynamics of supply and demand have a direct impact on economic efficiency, energy policy, energy markets and a indirect impact through governmental programs responding to the perceived crisis (Sweeney, 2004). Moreover, supply disruptions in the world oil market and the subsequent business cycle fluctuation has impact on the economy. In the light of recent several financial crises, it becomes more important for countries to understand this relationship with a great extend. India is among the fastest growing countries of the world and a high oil importer. It has a strong financial system and any oil price shocks are followed by an immediate impact on its stock market volatility and foreign exchange market. It is important therefore, for countries like India to study this relationship for preparing a better policy framework. In order to achieve the objectives, we consider daily returns of S&P Bombay Stock Exchange 500 index (BSE 500), foreign exchange rate of Indian rupees per unit of US dollars, and crude oil prices in US dollars from the period February, 1999 to March 2021. The results show several strong co-movements between oil price and stock market and between oil price and foreign exchange rate. Each of these associations is linked with some important economic or geopolitical issues in the world economy. The phase relationship indicates stock returns are in phase with oil prices and exchange rates are in out of phase with oil prices. We also find that economic shocks in developed market have a spillover effect on Indian market. However, the periods of these impacts are different at different time interval. The impact in volatility has a short term effect with a period less than 128 days. We find the wavelet coherency at high scale has a slower changes and long term effect on the relationship between the variables of our interest. Using wavelet analysis, we have shown that it is a powerful tool for analyzing dynamic and cyclical time series that cannot be analyzed by other classical spectral techniques. The remainder of this paper is organized as follows. In Sect. 2, we review the literature related to the topic. Section 3 describes the wavelet transform methods adopted in this paper. Section 4 describes about data that we use for our analysis. Section 5 depicts experimental results, and discusses about the findings, and Sect. 6 concludes.",3
17,2,Journal of Business Cycle Research,02 November 2021,https://link.springer.com/article/10.1007/s41549-021-00059-1,Does Hamilton’s OLS Regression Provide a “better alternative” to the Hodrick-Prescott Filter? A New Zealand Business Cycle Perspective,November 2021,Viv B. Hall,Peter Thomson,,Unknown,Male,Unknown,Male,"Hamilton (2018) makes a case for why you should never use the Hodrick-Prescott filter (Hodrick & Prescott, 1997), with his key arguments being the following. (a) The Hodrick-Prescott (HP) filter introduces spurious dynamic relations that have no basis in the underlying data-generating process. (b) Filtered values at the end of the sample are very different from those in the middle and are also characterised by spurious dynamics. (c) A statistical formalization of the problem typically produces values for the smoothing parameter vastly at odds with common practice. (d) There is a better alternative. His better alternative is to use the regression of a variable at date t on the four most recent values as of date t-h since this would achieve “… all the objectives sought by users of the HP filter with none of its drawbacks”. Hamilton provides illustrative empirical results for a long-run quarterly U.S. total employment series (1947q1–2016q2), and for quarterly GDP and GDP component series of similar length. He uses his proposed regression method (H84) and his companion 8-lag difference method (H8). The regression can also be used to project forward eight more quarters of observations. Growth cycle volatility and contemporaneous cross-correlation results are provided (Hamilton 2018, Table 2), and benchmarked against random walk results, but perhaps surprisingly not against results from using the HP filter. Nor does he provide standard errors or non-contemporaneous cross-correlation results and there is also no explicit assessment of the economic circumstances in which end-point issues might be empirically concerning. An increasing number of studies have provided theoretical and empirical evaluations of Hamilton’s proposed methodology. See, for example, Phillips and Shi (2021), Hodrick (2020), Jönsson (2020a, 2020b), Quast and Wolters (2020), Schüler (2018), and Drehmann and Yetman (2018) among others. Phillips and Shi (2021) provide detailed responses to Hamilton’s (2018) critique of the HP filter, and analyse the performance of Hamilton’s regression approach relative to their boosted HP filter (bHP). Their findings show a clear preference for the bHP filter over the Hamilton regression, and they also conclude that the HP filter may continue to be used as a helpful empirical device for the estimation of trends and cycles. Hodrick (2020) has used simulations approximating the natural logarithm of U.S. real GDP to examine the properties of the HP filter and Baxter-King filter (Baxter & King, 1999) (BK), relative to those from Hamilton’s H84 filter. He finds that the HP and BK filters are quite similar, that the H84 filter performs much better in environments with straightforward first difference stationary time series, and that the reverse is true for more complex time series. This leads him to conclude that the choice of methodology might depend on one’s priors about the nature of the series. But for the purpose of developing stylised business cycle facts to assist in building and evaluating growth cycle models, he has expressed a preference to use the HP and BK methods.Footnote 1 Jönsson (2020a) has assessed the extent to which HP and Hamilton filters introduce spurious dynamics to a business cycle’s cyclical component, and report that similar dynamics can be found in the cyclical component of HP-filtered and Hamilton-filtered series. The paper takes no stand on which of the two methods should be used when decomposing a series into trend and cycle components, and concludes that choosing between the two filters may turn out to be harder than at first thought. Jönsson (2020b) compares the HP and Hamilton filters with respect to real-time stability in US GDP gap estimation and finds that the Hamilton filter outperforms the HP filter when it comes to real-time revisions. The source of the inferior performance of the HP filter is that trend and cycle estimates close to the end of the sample are revised to a large extent as more data are added to the series, a finding also documented by other authors. For U.S. log GDP and credit-to-GDP data, Schüler (2018) has compared the cyclical properties of Hamilton’s regression filter with those from the HP filter. Overall, he finds that while Hamilton’s filter is not subject to the same drawbacks as the HP filter, it too reflects ad hoc underlying assumptions. Specifically, he singles out the two-year regression filter for excluding two-year cycles and emphasising cycles which are longer than typical business cycles fluctuations, thereby being at odds with stylised business cycle facts, such as the one-year duration of a typical recession. Drehmann and Yetman (2018) have assessed whether an HP trend or a Hamilton linear projection of the credit-to-GDP gap performed the better in providing an early warning indicator for crises. While acknowledging that it is an empirical question as to which of a range of measures performs best on this question, they also find that no other gap outperforms their baseline measure (a one-sided HP filter with smoothing parameter \(\lambda\) = 400,000). Further, they find that credit gaps based on linear projections in real time perform poorly. Against the above background and for New Zealand data, we assess whether Hamilton’s H84 filter has provided a “better alternative” to the HP filter. In particular, we evaluate comparative performance in two areas. In the body of the series we compare the stylised business cycle facts produced by the H84 filter relative to those from the HP filter and also the BK filter. The latter provides direct estimates of the business cycle based on the definitions of Burns and Mitchell (1946). At the ends of series, we compare the relative performance of the HP filter in the case where the series has been augmented with forecasts (HP forecast-extension) from the H84 regression and other forecast-extension methods. Our stylised facts evaluation is for a wider set of key macroeconomic model variables than the real GDP, output gap and credit-to-GDP gap variables investigated by others, and both evaluations use data from a small, open economy rather than from the considerably larger economies examined by others. Specifically, we address two questions. For post-1987q2 New Zealand, does Hamilton’s H84 filter produce business cycle volatility and bivariate cross-correlation measures which are materially different from those obtained using the HP and BK filters? At the ends of series, does HP forecast-extension using the Hamilton H84 predictor perform better than other forecast-extension methods, including the informed forecasts of three leading New Zealand economic agencies, two methods based on models of past data, and the HP filter with no extension? The latter question is investigated for three representative end-point environments: New Zealand’s post-2009q1 business cycle expansion path, and two business cycle turning point periods encompassing the peak and trough associated with New Zealand’s five quarter classical Global Financial Crisis (GFC) recession 2008q1–2009q1.Footnote 2 We are not aware of other evaluations which have focussed directly on end-of series issues at business cycle turning point periods. Section 2 provides a brief description of our methodological framework. Empirical results are presented in Sects. 3 and 4, and Sect. 5 concludes.",3
17,2,Journal of Business Cycle Research,14 September 2020,https://link.springer.com/article/10.1007/s41549-020-00048-w,"The Industry Life Cycle in an Economic Downturn: Lessons from Firm’s Behavior in Spain, 2007–2012",November 2021,Caridad Maylín-Aguilar,Ángeles Montoro-Sánchez,,Female,Female,Unknown,Female,"The Industry Lifecycle (ILC) provides a model to incorporate the dynamism of the environment into the competitive analysis of the context a company operates (Miles et al. 1993). Building in the initial phases of the product life cycle (Levitt 1960), the ILC model combines the analysis of the sector’s structure (Porter 1980) with the study of the behavior and results of companies (Teece 2007). Although some fundamental criticisms to the evolutionary, and somehow fatalistic, pattern of the ILC (Levitt 1960, p. 45), the stylized shape of industry evolution gives an overall picture of the potential path of industries and a foresight of the national economy (Dieli 2020), a motive of internationalization (Cuervo-Cazurra, Narula and Un 2015; Vernon 1966) and a framework to compare within industries (Karniochina et al. 2013). To strategy scholars, analyzing the feasibility of models as a guide, or at least a recommendation, for public policy makers and practitioners is important given a general landscape of stagnation in the advanced economies (International Monetary Fund, IMF 2017) and the forecasted impact of the current COVID-19 crisis (Fernandes 2020), in economies largely populated by mature industries. These deep, large jolts in finance or in demand, can accelerate the transition from stability to decline (McGahan 2004) trapping important domestic industries in a nightmare of reducing demand, therefore jobs, and finally profits. In this work we review the adequacy of ILC as theoretical framework and model to analyzing and comparing firm’s actual behavior and results with theory’s prescriptions, focusing in a large and traditional industrial sector and considering the effects of 2008 recession in Southern European economies. The economic recession left companies without the resources required given the need to change or cease operating (Bloom 2014) in a “post-growth”, scenario (persistent stagnation, stalled growth, high unemployment and inequality, Jackson 2019). Spanish financial crisis worries the general population during the worst of the peak but also throughout its duration (Araújo-Vila et al. 2020). Within that context, we regard specifically the situation of Food and Beverages (F&B) industry, as one of the most important industries in the world economy. We focus in an industry that supplies basic, sustenance goods, to distinguish cyclic trends from conjuncture jolts. Recent reports on the COVID-19 consequences show an overall stability (Reportlinker 2020), confirming its consideration of countercyclical (Oster 1999). The industry processes raw materials to create food products and then package and distribute them through various distribution channels to both individual customers and establishments. The global F&B market was estimated to be worth over $5,650 billion in 2017 and segments include alcoholic and non-alcoholic beverages, pet food, tobacco, grain products, meat, poultry and seafood, fruit and vegetable canning, pickling, and drying, frozen food and dairy. Globally, the bipolar structure of the sector consists of large multinationals on the one hand and a large number of domestic small and medium-sized (SMEs) companies on the other. The industry has witnessed a continuous pattern of growth with two different speeds, around 3% in major Western economies, while the emerging and new industrialized economies grow twice that rate (Reportlinker 2018). In Spain, the F&B industry is the largest in terms of turnover and employment with some 28,000 companies, annual production amounting to 119 billion euros, and more than  500,000 people employed in the sector (Spanish national institute of statistics, INE 2017). The industry’s exports have shown positive growth since the mid-1990s as part of an ongoing, long-term effort, and not just as a reaction to the recent effects of the recession (Simon-Elorz et al. 2015). That said, both the percentage of income dedicated to consumption and the volume of food consumed fell in the period from 2008 to 2018. In the empirical part of the paper we share the learnings from a study of behavior and results of Spanish F&B firms during the last decade. Using a mixed methods approach, we follow the ILC model and its empirical research, and particularly, the model for mature and declining industries developed by Harrigan and Porter (1983). The research question simply asked “Can the ILC theoretical model help policy makers and managers identifying and reacting to the signals of the environment?”. In order to answer that, first objective of the study was to confirm if the drop down of sales and profits observed in the period was part of industry’s evolution or a consequence of the 2007–2008 shock and lack of public and private financing, the abandonment of active innovation and productivity policies (Tong et al. 2016). A parsimonious application of the ILC thresholds (Dieli 2020; Hall 1980; Harrigan 1980; Karniouchina et al. 2013) to public data on consumption in a period previous and during the shock (2000–2010) revealed the existence of nine declining segments in a seemingly stable and counter-cyclical industry. Second objective refers to the second part of the question, reaction. Drawing from the prescriptions of the model for declining business (Harrigan 1980; Harrigan and Porter 1983; Porter 1980), the answer conveys analysis and comparison of the actual strategic conduct of the firm with the one proposed by the model. In order to do so, a primary-source survey was addressed to top management of firms active in any of the nine declining business. The empirical comparison unveils differences between the model prescribed conduct and the enacted one, leaving an open question of the consequences of this imbalance between environment, firms’ competitive position and conduct. The analysis and discussion with experts of possible reasons and solutions for this imbalance lead to the conclusions of the study, that reinforces us about the validity of studying the environment through academic models as the ILC, however the strategic prescriptions have to accommodate to the long-lasting persistence of stagnant and declining demands and the behavior of companies that challenge the deterministic view of the model renewing the value proposition (Henderson 1995), adding services (Cusumano et al. 2015) or introducing disruptive, innovative business models (Christensen and Raynor 2003). This renewal and the shared discussion of findings with industry experts and practitioners will place us in a better position to be able to deal with the future and create new trade opportunities for firms and their products. Our paper contributes to the review of the economic scenario from a company’s viewpoint. Foreseeing the characteristics of the industry and forecasting the possible evolution of demand can add clarity to a scenario which is, to put it mildly, uncertain. Events as the global economic shock of 2007 and the COVID-19 pandemic, destabilize the institutional environment and noticeably modify the formal and informal rules of the game for those operating in the market and change the evolution dynamics of the industry (McGahan 2004). Therefore, a shared understanding of decreasing demands due to a temporary jolt, or due to a persistent reduction, is a relevant first step. Growth stagnation is a complex situation which is hard to manage as the lack of munificence in their environments reduces the options open to companies (Porter 1980). On the second hand, reviewing the options and opening opportunity windows (as internationalization, Cuervo-Cazurra et al. 2015), alliances with competitors, buyers and suppliers, customers’ cocreation, servitization of products (Argyres et al. 2015; Christensen and Raynor 2003; Cusumano et al. 2015) and collaborative networking are some of the recipes that can change the bitter taste of decline, when “a fractionally lower price gets the business” (Levitt 1980). The article is organized as follows. The first part is the literature review, with a summary of the ILC model postulates and revisions, and after there is a summary of the main traits of the framework and strategic model for declining business, as introduced by Harrigan (1980). The literature review ends with the proposition of the empirical validation of framework and model, that is covered in the third section, Empirical study. Discussion of results and conclusions are the final part of the paper.",1
17,2,Journal of Business Cycle Research,15 April 2021,https://link.springer.com/article/10.1007/s41549-021-00055-5,Predicting the German Economy: Headline Survey Indices Under Test,November 2021,Robert Lehmann,Magnus Reif,,Male,Male,Unknown,Male,"Survey indicators are a well-established source to derive early predictions on current and future development of macroeconomic variables such as gross domestic product (GDP) (see, for example, Angelini et al. 2011). For Germany, two survey providers—namely the ifo Institute (ifo) and IHS Markit (IHS)—and their corresponding headline indices (the ifo Business Situation, the ifo Business Expectations, the ifo Business Climate, and the PMI Composite Output Index) receive considerable media attention each month and are found to be important for tracking economic activity in both the Euro Area and Germany (see, for example, Basselier et al. 2018; de Bondt 2019; Fritsche and Stephan 2002; Lehmann 2020). Recently, the indicators of both survey providers are listed on Bloomberg’s “12 Global Economic Indicators to Watch”.Footnote 1 However, two further and very important survey providers that publish monthly headline indices are mostly neglected in the public debate: the Directorate-General for Economic and Financial Affairs of the European Commission (DF ECFIN) and the Centre for European Economic Research (ZEW). Whereas the indicators of the former provider are also based on either business or consumer surveys, the latter one uses a different source of information: the assessment of financial experts. In the case of the ifo Institute and IHS Markit, an ongoing debate across analysts takes place on which indicator is better suited to track the aggregate Germany economy (see, for example, the Wall Street Journal or tradingfloor.com)Footnote 2 or certain branches of the economy. An analysis by J.P. Morgan concludes that IHS Markit’s service sector indicator is better in explaining movements in sectoral gross value added than the ifo Business Expectations.Footnote 3 This exemplary analysis, however, only investigates the in-sample fit of both indicators, that is, how much they can explain a variable’s past fluctuations. From a forecaster’s perspective such an analysis is of minor help as it does not take a stand on the indicators’ forecasting properties, that is, on the reliability of the indicators’ signals for a variable’s current and future development. We conduct an out-of-sample, real-time forecast analysis which compares the forecasting properties of the headline indices of the four very important survey providers in Germany for the current quarter and one quarter-ahead predictions. Instead of solely analyzing the properties for total GDP growth, we further study the forecasting performance of the providers’ headline indices for private sector GDP growth (that is market-traded output excluding public activities) and gross value added (GVA) in the most important branches of the Germany economy, that is, manufacturing and services. Furthermore, we discuss the indicators’ performance from an applied forecasting stance, investigate the impact of two indicator transformations on the forecast performance, and the accuracy of the real-time forecasts for revised data. Our results can be summarized as follows. For total real GDP growth all providers publish meaningful and practical relevant leading indicators, with some advantages for the ifo indicators in case of one quarter-ahead predictions. A similar picture emerges for market-traded GDP, but in this case, the PMI Composite Output Index is advantageous for nowcasts in one of the two models, whereas the Economic Sentiment Indicator of DG ECFIN is preferred for next quarter forecasts. For the manufacturing sector, the ifo indicators—and here especially the ifo Business Situation Manufacturing—are clearly superior with respect to the other providers’ headline indices for both the nowcasting and the forecasting setup. For services, all providers publish headline indices with a similar nowcasting performance; the Economic Sentiment Services of ZEW is superior in case of one quarter-ahead predictions. The paper is organized as follows. In Sect. 2 we introduce the headline indices of the four providers and the forecast experiment. Section 3 presents our baseline results, followed by a discussion in Sect. 4. The last section concludes and gives an outlook on future research activities.",1
17,3,Journal of Business Cycle Research,02 November 2021,https://link.springer.com/article/10.1007/s41549-021-00062-6,The Evolution of US and UK Real GDP Components in the Time-Frequency Domain: A Continuous Wavelet Analysis,December 2021,Patrick M. Crowley,Andrew Hughes Hallett,,Male,Male,Unknown,Male,"The relationship between national income component variables lies at the heart of the study of the business cycle in macroeconomics, and has given rise to theories of consumption, the accelerator theory and numerous other hypothesized relationships. Although econometric studies using macro variables are extensive, there is little in the way of time-frequency analysis in this area. The early spectral analysis studies by Granger (such as Granger & Hatanka, 1964; Granger, 1966) and the follow up studies by Levy and Dezhbakhsh (2003a, b) are the most notable, but these studies now appear datedFootnote 1 given the more recently developed time-frequency domain techniques such as wavelet analysis and empirical mode decomposition. Also, with the lengthening of data sets that inevitably comes with the passage of time, there is now scope to use these new and improved techniques to evaluate the macroeconomic cyclical interactions across a wider frequency range and in greater depth. The purpose of this research, therefore, is exploratory and to establish some “stylized facts” regarding cyclical fluctuations in the growth of the GDP expenditure components, if any exist. The paper is a companion paper to analagous research done using discrete wavelet analysis. In a series of papers undertaken to decompose the growth in US and UK national income components, the cyclical features of GDP growth were obtained by decomposing economic growth and component growth (Crowley & Hughes Hallett, 2015); this was then used to analyze the cross correlation features of US and UK GDP growth components (Crowley & Hughes Hallett, 2016). The first part of this paper is the continuous wavelet analysis equivalent to Crowley and Hughes Hallett (2016), as it studies the coherence and phasing of cyclical features of US and UK GDP component growth from the late 1960s to the present, but with some similar and different results than obtained when using discrete wavelet analysis. The second part extends the analysis to look at the coherence and phasing of GDP component growth between the US and the UK, which to our knowledge, has not been done in any other time-frequency research. But why should time-frequency analysis of macroeconomic components provide any interesting results, given that the aggregate results in general do not? In broad scope, although it would be naïve to expect any systematic pattern to emerge in the correlations between GDP internationally, or between the components of GDP in any one economy as theory might lead us to suppose, there are relationships between the components that emerge, but at specific frequencies and over a series of specific time intervals across the sample. This is to be expected; it is quite possible that we observe strong and significant correlations between a pair of GDP components (or between a pair of national economies) in one period, but reduced and insignificant correlations in other periods. The upshot then may well be weak or insignificant correlations over the sample as a whole, which would tend to suggest there was nothing interesting evident to explore.. The advantage of time-frequency cyclical decompositions is that they show you exactly when the time-frequency equivalent of correlations, that is, coherences, are significant or at least strong, and when they are not. This avoids concluding the there is no meaningful coherence (correlation) between two components on average; and that theory is not supported by evidence when in fact it is supported—but only significantly so, at certain time periods and policy episodes, or at certain cycle lengths.Footnote 2 A second observation that arises from our work which answers this question is that the same thing can easily happen with phase shifts. If two GDP components were normally thought, on the basis of theory or reasoning from first principles, to have a high (positive) coherence between them, then it would only take a quarter cycle phase shift between them to reduce that coherence to something smaller and insignificant because only one component is contributing any power to the coherence at the point of observation. And it would only take a half-cycle shift to negate their coherence/correlation because the power contributed by that component is offset (if incompletely) by the power contributed by the other. So again, it will appear as if there is no coherence/correlation between the two components, whereas they would have the expected coherence were it not for the phase shifts. But as above, we will never see that unless the analysis is made to focus on a time-varying cyclical decomposition of the data; or the theory underlying our standard models will get unfairly rejected. A third and very likely possibility is the power of the different cycles that constitute the GDP component pairs that we are looking at can vary over time depending on; the type and timing of shocks that hit the economy; on the parameters that control the dynamics of that economy; and most importantly on any changes in the policy rules that get applied to that economy. In earlier work (Crowley & Hughes Hallett, 2018) we have given a number of worked examples to show how easily that can happen in our standard models. Fairly obviously, if events do conspire to weaken the power of certain cycles, or to extend them to a different length, then the coherence of a GDP component principally composed of that cycle will be reduced numerically, together with other components whose cycles (power, length) are not affected or equally affected. So to preview the specific results that we obtain in our research, first it is noted that (as with discrete wavelet analysis) various different cycles are apparent at different frequencies within and between GDP component growth, and some of these cycles appear to wax and wane over time and many of these cycles do not accord with traditional notions of business cycles. The second key result is that the largest amount of cyclical co-movement appears to occur between C growth and I growth for both the US and the UK The third key result is that an international business cycle appears to be at work at a frequency somewhere between 8 and 14 years, and that this began to emerge in the early 1980s, and has continued throughout the remainder of the time period under study. In terms of content, Sect. 2 of this paper describes the continuous wavelet analysis approach, while Sect. 3 presents the data. Section 4 presents the cross-spectral analysis for the US and UK component growth separately, and then Sect. 5 does an international comparison of the cyclical features of US and UK GDP component growth. Section 6 concludes.",2
17,3,Journal of Business Cycle Research,27 October 2021,https://link.springer.com/article/10.1007/s41549-021-00061-7,Predicting Recessions in Germany Using the German and the US Yield Curve,December 2021,Martin Pažický,,,Male,Unknown,Unknown,Male,"The slope of the German yield curve, expressed as the spread between the yields on long-and short-term nominal treasury securities, has flattened since the beginning of 2018, and in 2019, it even became inverted at the short maturities. The yield curve also flattened in the US in 2017, which raised concerns among some participants in the FOMC meeting in December 2017 that a possible inversion of the yield curve could portend a slowdown in economic activity (see Minutes of the Federal Open Market Committee, 2017). As the graphs in Appendix A show, the inversion of the yield curve has preceded the economic recessionFootnote 1 in both Germany and the US over the last few decades.Footnote 2 However, several studies have shown that one of the consequences of large-scale asset purchases by central banks is flattening the treasury yield curve (Gilchrist & Zakrajsek, 2013). This means that the inversion of the yield curve in 2019 did not necessarily signal a recession but may rather be driven by large-scale asset purchases. The recent flattening of the yield curve motivated us to verify the ability of the German yield curve to predict a recession in Germany, considering the period of large-scale asset purchases by the European Central Bank (ECB). We consider several extensions to the baseline yield curve model by adding individual components of the yield curve (i.e. the expectations and the term premium components), the short-term interest rate, the indicator of the stance of monetary policy or orders in manufacturing representing the real economy. We also compare the predictive power of the German yield curve with its counterpart in the US, which has been increasingly considered in academic research. Finally, since the US financial market greatly influences global markets through financial spillovers, we also aim to determine whether the US yield curve can predict a German recession. Many studies on financial crises and the business cycle have been conducted, for instance, Muir (2017), Trebesch (2019), Jannsen et al. (2019) and Bruno et al. (2019). Many studies have documented the relationship between the slope of the yield curve and the probability of a subsequent recession. One of the first approaches with an outstanding track record is the term spreadFootnote 3 model of Estrella and Hardouvelis (1991).Footnote 4 Their model predicted that the recession would begin 12 months after the yield curve inversion. They viewed the yield curve inversion as a situation in which the yield on a three-month treasury bill rises higher than the yield on a ten-year treasury bond.Footnote 5 Many other studies have documented the predictive power of the slope of the US yield curve for forecasting recessions, for instance, Estrella and Mishkin (1998), Estrella and Trubin (2006), Rudebusch et al. (2007), Rosenberg and Maurer (2008) and Rudebusch and Williams (2009). More recent evidence has been provided by Ergungor and Thomson (2006), Benzoni et al. (2018), Johansson and Meldrum (2018), Miller (2019) and Cooper et al. (2020). Although the literature has convincing evidence of the predictive power of the yield curve, there is no success in explaining why such an empirical relationship holds. There seems to be a consensus that the yield curve slope (or term spread between long- and short-term maturities) contains information on current and expected monetary policy paths. The changes in key policy rates of interest are linked to expectations about future business cycle outcomes. However, the yield curve is not affected only by monetary policy expectations. In particular, the term spread also reflects market attitudes towards various sources of risk, which influence the economic outcome. The individual components of the term spread (i.e. the expectations and the term premium components) could contain additional information on future economic performance that might improve forecasts (Bauer & Mertens, 2018). The existing literature has frequently dealt with estimating the probability of recession using a yield curve. Less often, some studies have examined extensions to components of the yield curve (e.g., Bauer & Mertens, 2018; Rosenberg & Maurer, 2008) or short-term interest rates (e.g., Wright, 2006). However, there is room to examine other aspects that have not yet received so much attention. For example, as far as we know, while several studies have investigated the predictive power of the US yield curve, only a few have examined the ability of yield curves in other countries to predict a recession in a domestic country.Footnote 6 Also, the predictive power of individual components of yield curves has limitedly been examined in countries other than the US. As Germany is the largest European economy, we are particularly interested in the ability of the German yield curve to predict a recession in Germany. In particular, we focus on the signal coming from the German treasury yield curve through the following research question: Does the slope of the German yield curve indicate future economic activity in Germany? Historically, an inverted yield curve meant that the expected future policy rates would be lower than the current policy rates and, hence, a key signal of imminent economic slowdown. However, if the term premium is negative, the inverted yield curve would not necessarily indicate that future policy rates will be lower than the current policy rates. For example, if market participants expect that future neutral rates will rise over the coming years but will fall over the longer term,Footnote 7 it would point to a downward sloping yield curve. An inverted yield curve could therefore reflect longer-term structural rather than short-term cyclical expectations. As in similar studies,Footnote 8 we isolate the expectation and the term premium components from the yield curve signal. After examining the ability of the yield curve to predict a recession in Germany, we aim to investigate whether expectations or term premia generate the signal. The signal from the yield curve can either be amplified by both components or they can interfere with each other. We also include some other variables (such as short-term interest rate or interest rate differential between nominal effective federal funds rate and its time-varying (nominal) neutral value estimated by Holston et al. (2017), or orders in manufacturing) that could affect longer-term expectations and thus contribute to a more accurate recession forecast. When evaluating the ability of the German yield curve to predict a recession, we primarily refer to the results of the US yield curve, which have been extensively documented. Also, we verify the predictive power of the US yield curve (and its components) and compare the results with our prediction models for Germany. It means that we use the results of the US as a benchmark to evaluate the predictive power of the German yield curve. As already mentioned, the US financial market is key to global financial markets due to financial spillovers. The ability of the US yield curve to predict a recession in the US has been confirmed in several studies. However, can its predictive power extend beyond US borders? As far as we know, the ability of the US yield curve to predict a recession in other countries has not been verified, which leads us to pose the second research question: Can the US Yield Curve Predict Recessions in Germany? We organise our study as follows. Section 2 presents the decomposition of the term spread (or yield curve) from a theoretical viewpoint, while Sect. 3 describes the methodological procedure for decomposing the term spread into the expectations and term premium components. Section 4 explains how we link the signal from the term spread and its components with economic performance, which allows us to evaluate the yield curve’s predictive power. Our results and discussion are presented in Sect. 5. Finally, in Sect. 6, we conclude our results.",2
17,3,Journal of Business Cycle Research,19 October 2021,https://link.springer.com/article/10.1007/s41549-021-00060-8,A Wavelet Evaluation of Some Leading Business Cycle Indicators for the German Economy,December 2021,Jens J. Krüger,,,Male,Unknown,Unknown,Male,"Leading business cycle indicators have traditionally received a great deal of attention in the macroeconomic forecasting community. The literature on both single leading indicators and composite leading indicators is substantial (see, among many others, Diebold and Rudebusch (1989, 1991), Emerson and Hendry (1996), Garnitz et al. (2019), Lahiri and Moore (1991), Oppenländer and Poser (1984), Stock and Watson (1989). Marcellino (2006) provides a comprehensive survey. Especially concerned with German experience are Bandholz and Funke (2003), Entorf (1993), Kholodilin and Siliverstovs (2006) and Lehmann (2020), inter alia. The main purpose of leading indicators is short-term business cycle forecasting and identifying the turning points. Marcellino (2006, p. 885) defines six desirable properties of leading indicators: (i) consistent timing, (ii) conformity to the general business cycle, (iii) economic significance, (iv) statistical reliability of data collection, (v) prompt availability without major revisions, (vi) smooth month to month changes. For our analysis we may add the availability of long time series. Central to the performance of leading indicators is their ability to lead the cycle and the variability of this lead over time. In this paper we apply wavelet analysis to analyze the properties of popular leading indicators for the German economy. We intend a purely in-sample investigation of the properties of the leading indicators. Applying wavelet analysis requires long time series at a monthly frequency which are only available for a limited set of leading indicators. From the results we can infer the cyclical properties of the leading indicators and their relations to the reference cycle. The results also allow to determine the lead (or lag) of the indicators and to investigate the stability of this lead (or lag) over time. Wavelet analysis is a refinement of traditional frequency domain (spectral) analysis.Footnote 1 It appears to be particularly adapted to the analysis of lead-lag relationships among variables at a specific band of frequencies for at least three reasons. First, wavelets allow for the analysis of relationships differentiated across time and frequencies in a unified methodological framework. Second, wavelets allow to focus on a particular band of frequencies which is particularly relevant for the application at hand (which, in our case, pertain to the frequencies relevant for business cycle analysis). Third, no parameters have to be estimated which implies that there is no need to be concerned about parameter stability over the sample period or the application of time-varying or regime-switching models. The remainder of this paper is organized as follows. We start by introducing the toolbox of wavelet analysis provided by the continuous wavelet transform in Sect. 2. In Sect. 3 the data of the reference cycle and the four leading indicators which are in the focus of this paper are discussed. The univariate wavelet results for these data are discussed in Sect. 4, followed by the bivariate relations of the leading indicators to the reference cycle in Sect. 5. Section 6 concludes. An appendix provides an example with artificially generated data.",2
17,3,Journal of Business Cycle Research,16 April 2021,https://link.springer.com/article/10.1007/s41549-021-00056-4,The Impact of the Global Financial Crisis on Investment in Finland and South Korea,December 2021,Gene Ambrocio,Tae-Seok Jang,,,Unknown,Unknown,Mix,,
18,1,Journal of Business Cycle Research,25 March 2022,https://link.springer.com/article/10.1007/s41549-022-00065-x,New Findings Regarding the Out-of-Sample Predictive Impact of the Price of Crude Oil on the United States Industrial Production,March 2022,Nima Nonejad,,,,Unknown,Unknown,Mix,,
18,1,Journal of Business Cycle Research,07 February 2022,https://link.springer.com/article/10.1007/s41549-022-00064-y,Constructing and Characterising the Aggregate South African Financial Cycle: A Markov Regime-Switching Approach,March 2022,Milan Christian de Wet,Ilse Botha,,Male,Female,Unknown,Mix,,
18,1,Journal of Business Cycle Research,27 January 2022,https://link.springer.com/article/10.1007/s41549-021-00063-5,A Customized Machine Learning Algorithm for Discovering the Shapes of Recovery: Was the Global Financial Crisis Different?,March 2022,Gonzalo Castañeda,Luis Castro Peñarrieta,,Male,Male,Unknown,Male,"Financial analysts and the media pay much attention to the different shapes that are likely to appear in the output dynamic of recessions and their subsequent recovery.Footnote 1 A ‘soup of letters’ is commonly mentioned: L-shaped to describe an initial sharp fall in GDP and then a prolonged (long-term) path below the trend level observed in the pre-recession peak; V-shaped to describe a snap-back dynamic in which a quick fall comes along with a symmetrical speedy upturn so that the initial trend path is recovered; J-shaped (Nike logo or checkmark) when the fall is limited to a few quarters and the economy recovers steadily; W-shaped to describe ‘double-dip’ recessions in which the initial recovery is interrupted by another drastic fall, perhaps due to the presence of additional complications in the economy’s performance; U-shaped to describe a steady fall and a sluggish recovery; K-shaped when a general output drop in the economy is followed by some sectors recovering and others stagnating or falling even further.Footnote 2 It seems to us that some of these letters are part of the folklore and that their frequent reference is due to media hype. Therefore, in this paper, we use a customized technique to find out whether these shapes or others are truly observed in the data of a sample of 47 advanced and emerging economies that have experienced significant (mild or severe) recessions in their business cycles. For this, we appeal to the concept of ‘shapelet’ from the machine learning literature. A shapelet is a subsequence of a time series that has been recognized as a representative segment of a class of series and can be used for classification purposes.Footnote 3 For instance, in Fig. 1, we compare the series of interest (X) to the shape of shapelet S through the normalized distance between the series and the shapelet (d); if the distance is below a certain threshold (δ), then that series belongs to shapelet S. Shapelet discovery in a subsequence of a GDP time series Consequently, in a first step (discovery), it is necessary to identify similitudes between different segments in a set of series. The seminal papers on shapelets are Ye and Keogh (2009, 2011), which have produced a flourishing line of research with applications in several disciplines: geology, biology, meteorology, etc. This classification method has the advantage that the discovered shapes can provide information for the analysts to interpret. This concern is highly relevant when the objective is to identify shapelets in subsequences that coincide with recession events in the GDP series.Footnote 4 A classification procedure for recession events based on shapelets is ideal for the following reasons: it deals with sequential observations and, thus, it can identify shapes in the output dynamics; it does not require synchronicity for two events to be considered similar; it can deal with the noisy component of the series that might distort the identification process; it is a semi-automatized method and hence minimizes the use of subjective judgment for classifying events with specific shapes; it makes use of statistical metrics to determine if two shapes (letters or symbols) associated with empirical spells are substantially different; it allows the identification of several shapes of recovery within the same GDP series but in different historical periods; it reduces the possibility that two subsequences can be considered similar only for random reasons when a set of theoretical and interpretable shapelets is specified in advance. Consequently, by studying recession events with shapelets, we can identify which shapes are more frequently observed in the data. Moreover, we can distinguish if a specific shape is more commonly observed in certain types of crises, economic or otherwise. For instance, in this paper, we analyze whether the global financial crisis of 2007–2008 produced a distribution of shapes, across countries, different from those observed in other recession spells included in the database. Besides, we show through descriptive statistics and a multinomial model that the clusters of empirical spells based on our shapelet classification scheme are strongly associated with different features (e.g., depth, output loss, mean drop, duration) of the recession-and-recovery events. With this association, we can establish a ranking of the severity of the recession (a comprehensive measure of the recessions’ features), which helps provide economic interpretability to the empirically relevant shapes of recovery. Since the study of recession events is constrained by relatively short-time series, we decided to modify a traditional unsupervised machine learning technique (nearest-neighbors) for the classification of these events. Another reason for preferring this approach is that we have additional information on the quarters in which recession spells start. Hence, we do not need a procedure to discover the position of the shapelets, as done in the traditional ML techniques. Therefore, in the initial stage of our methodology, we define ex-ante (i.e., from the visual inspection of empirical spells) ten theoretical shapelets to be considered as potentially representative shapes. Then, we automatize the method for the classification of spells and the depuration of clusters through different tools: the customized ML algorithm, a Relative Adjustment Score (RAS) that measures the separability of such clusters, and a multinomial model the measures the out-of-the sample predictability of clusters using recession features. Through the RAS method and the multinomial model, we conclude that a grouping with four shapelets is capable of differentiating the nature of the recession events, in terms of their economic meaning and the specification of clear (non-blurred) borders between clusters. The method has a better performance than other classification techniques based on features (scaled KMeans, transformed KMeans through dimension contraction) for the following reasons: (i) it can produce a larger number of clusters (four versus two), which allows distinguishing more dynamics in recession events; (ii) the four clusters discovered in the shapelets approach exhibit good predictability when using recession features; (iii) the shapelets-clusters have economic interpretability, which scaled KMeans do not provide. The rest of the paper is structured with eight more sections, besides the online supplementary material. In the following section, we present a literature review on other methods for identifying the shapes of recovery. In the third section, we extend on the concept of shapelet and explain how it can be associated with the output dynamic of recessions and their recovery within a prespecified time window. In the fourth section, we describe the database used for the study and define empirically a recession event. In the section on methods, we formulate analytically the configuration of theoretical shapelets and elaborate a metric for determining the quality of the empirical clusters. In the sixth section, we present our results concerning three classification schemes which vary in terms of the number of clusters considered. In the seventh section, we present some descriptive statistics for recession features and how they are related to the shapes of recovery. In the eighth section, we present the estimation of a multinomial model that validates the separability between clusters. In the ninth section, we review the relative frequency of shapelets in events associated with specific crises, like the Global financial crisis, and make comparisons between them. In the section of conclusions, we synthesize our main results and make suggestions on a possible research venue where the concept of shapelets could be handy.",1
18,1,Journal of Business Cycle Research,27 April 2022,https://link.springer.com/article/10.1007/s41549-022-00067-9,Co-movement of Cyclical Components Approach to Construct a Coincident Index of Business Cycles,March 2022,Koki Kyo,Hideo Noda,Genshiro Kitagawa,Male,Male,Unknown,Male,"Following the seminal work of Burns and Mitchell (1946), the measurement of business cycles has been recognized as an important issue in macroeconomic studies. Furthermore, favorable or unfavorable business conditions are of great interest to many, because the assessment of business conditions has a significant influence on government economic policies. However, conventional indices may not accurately capture business cycles. Therefore, the question that arises is: what is a better way to produce useful and reliable indices? This is a long-standing problem and was a central question addressed by Stock and Watson (1989). In this paper, we present an alternative approach to constructing an index of business cycles (IBC) using coincident economic indicators. Specifically, we attempt to measure the growth cycles of the Japanese economy using the proposed approach. To the best of our knowledge, a monthly coincident index of growth cycles in Japan is a new development; hence, it may be of broad interest to macroeconomists and policy makers. Conventionally, business conditions are assessed using summary measures for the state of macroeconomic activity in Japan and the United States (US). The composite index (CI) and diffusion index (DI) are representative summary measures. Although both have the advantage of manageability, they have faced criticism because they are not based on a structured statistical model. Given this, since the 1980s, many studies have been conducted on statistical methods for business cycle analysis. In Japan, business conditions are typically measured using the business cycle indices CI and DI, which are compiled by the Economic and Social Research Institute (ESRI) of the Japanese Cabinet Office. Since April 2008, the ESRI has placed greater emphasis on the CI than the DI in assessing business conditions in Japan. According to the Cabinet Office, the indicators for indices of business conditions are re-examined after each business cycle and changed if it is expected that the performance of the indices will improve. The DI and CI consist of three indices: a leading index, which is constructed based on 11 indicators, for predicting the prospective business conditions; a coincident index, which is based on 10 indicators, for assessing the present business conditions; and a lagging index, which is based on 9 indicators, for reconfirming the previous assessment of the business conditions. The DI represents the ratio of the number of indicators that increased in value during the most recent three-month period to the total number of applied indicators. Pioneering works in this area are by Stock and Watson (1989, 1991), who developed a statistical method to construct an IBC based on a state space model. Stock and Watson (1989, 1991) defined the business cycle as a co-movement of macroeconomic variables, and the Stock–Watson index is constructed by extracting the common factor hidden in multiple macroeconomic time series. Thus, their proposed model is commonly called the dynamic factor model, and it was first applied to analyze the US business cycle. Ohkusa (1992) and Fukuda and Onodera (2001) applied the Stock–Watson dynamic factor modeling approach to analyze Japanese business cycles. The dynamic factor modeling approach has since been extended by Kanoh and Saito (1994), Mariano and Murasawa (2003, 2010), Watanabe (2003), and Urasawa (2014). Kanoh and Saito (1994) extended the dynamic factor model to include qualitative data from the Short-Term Economic Survey of Enterprises (abbreviated to Tankan), which is a statistical survey conducted by the Bank of Japan. Focusing on companies’ judgments about business conditions in Tankan, Kanoh and Saito (1994) considered that if such judgments reflect an overall assessment of actual business conditions, then a business index that statistically extracts the actual state of the economy from such judgments would be a more appropriate measure of the overall state of the economy than conventional indices. They concluded that the peaks and troughs projected by their proposed index have systematic relationships with the business cycles identified by experts from the Japanese government. Mariano and Murasawa (2003) indicated that the CI and Stock–Watson coincident indices have two shortcomings: first, they ignore information contained in quarterly indicators, such as quarterly real gross domestic product (GDP); and second, they lack economic interpretation. Therefore, Mariano and Murasawa (2003) extended the Stock–Watson coincident index by applying maximum likelihood (ML) factor analysis to a mixed-frequency series of quarterly real GDP and monthly coincident business cycle indicators. The resulting index is related to latent monthly real GDP. Furthermore, Mariano and Murasawa (2010) estimated Gaussian vector autoregression (VAR) and factor models for latent monthly real GDP and other coincident indicators using observable mixed-frequency series. For the ML estimation of a VAR model, the expectation-maximization algorithm helps to identify a good starting value for a quasi-Newton method. Mariano and Murasawa (2010) concluded that the smoothed estimate of latent monthly real GDP is a natural extension of the Stock–Watson coincident index. To obtain early estimates of Japan’s quarterly GDP growth in real time, Urasawa (2014) estimated a dynamic factor model using mixed-frequency data for GDP, industrial production, employment, private consumption, and exports. The results of a real-time forecasting exercise suggested that the model performs well. Another prominent approach that differs from the dynamic factor modeling approach is the regime-switching modeling approach developed by Hamilton (1989). The dynamic factor modeling approach is associated with a CI-type index; however, the regime-switching modeling is associated with a DI-type index. Kim and Nelson (1998) developed a method that combined the above two approaches. Watanabe (2003) applied Kim and Nelson’s (1998) approach to the Japanese economy, and estimated a dynamic Markov switching factor model using macroeconomic data. We note a data treatment problem regarding the Stock–Watson dynamic factor modeling approach and its extensions. Specifically, many earlier studies used differencing in time series data to obtain stationarity in cases where nonstationary data (e.g., the mean) was used for convenience. This results in a loss of significant information. In this paper, we propose an alternative approach to construct a coincident IBC via the decomposition of time series data into several possible components. In contrast to the Stock–Watson index, we consider an IBC that has the following properties: (1) it is globally stationary in the mean; (2) it is constructed using the co-movement of all the relevant coincident indicators; and (3) it has variations that are as large as possible so that it contains a relatively large amount of information for business cycle analysis. In particular, we emphasize the importance of the measurement of growth cycles rather than classical cycles, and contribute to the field of estimation methods for a monthly coincident index of growth cycles in Japan. Major studies on issues of classical cycles and growth cycles include Harding and Pagan (2005) and Zarnowitz and Ozyildirim (2006). Harding and Pagan (2005) mentioned that many problems arise regarding a lack of clarity as to what cycle is being studied, and provided a classification scheme for cycle research. Zarnowitz and Ozyildirim (2006) indicated the importance of measuring growth cycles in business fluctuation; that is, growth cycles are more numerous than classical cycles because all recessions involve slowdowns, but not all slowdowns involve recessions. Our approach is also related to the unobserved component (UC) model of Morley et al. (2003) and multivariate UC model of Ma and Wohar (2013). Although the model used in our approach is different with the UC model in the formula, it is in fact a multivariate UC model so it belongs to the class of the UC models. The remainder of this paper is organized as follows: In Sect. 2, we present a review of existing approaches. In Sect. 3, we explain the framework of our new approach for constructing a business cycle index. Then, we describe the parameter estimation procedure in Sect. 4. In Sect. 5, we present the results of the constructed IBC and compare the performance of the IBC based on the newly proposed approach with the CI for coincident indicators using Japanese economic data. In Sect. 6, we discuss our new approach. We present our conclusions in Sect. 7.",1
18,2,Journal of Business Cycle Research,02 May 2022,https://link.springer.com/article/10.1007/s41549-022-00066-w,Machine Learning Dynamic Switching Approach to Forecasting in the Presence of Structural Breaks,July 2022,Jeronymo Marcondes Pinto,Jennifer L. Castle,,Unknown,Female,Unknown,Female,"Advances in information sciences are allowing substantial improvements in statistical analysis, with special emphasis on time-series forecasting. Current computational resources allow analysts to estimate a plethora of models, raising the challenge of comparing and ranking different forecasting methods. A number of new computationally intensive techniques have been proposed and evaluated including Wan et al. (2020) and Wang et al. (2020). Therefore, there are many choices of models used to forecast. Even though many models have been presented in the recent literature, they can be subject to forecast failure. The concept of forecast failure is highlighted by Hendry (2006), Castle et al. (2015b) and Castle et al. (2016). It is based on the fact that in a non-stationary world subject to structural breaks, where models and mechanisms differ, classical econometric models are a risky forecasting tool. Essentially, popular econometric models, with well defined long-run solutions may be harmful when there are shifts in equilibrium means, which results in forecast failure. In this paper we focus on the forecast failure caused by the type of structural break called Location Shift or Mean Shift, which are changes in previous unconditional means of data. Hendry (2006) provides a method to address the issue of structural breaks, based on double differencing the variable of interest. This operation tends to remove mean shifts in the data generating process (DGP), which could result in an unbiased forecast and is especially useful when a structural break occurs. However, using this method comes at a cost: an increase in the forecast error variance, which generates a noisier forecast. In this sense, it appears to be more reasonable to use the method proposed by Hendry (2006) only during structural break periods and to change back rapidly to a non-robust model, which is well-specified in the absence of structural breaks, as soon as the mean shift stops. Detecting structural breaks could be undertaken by employing a variety of methods, like Bai & Perron (1998) and Perron & Yabu (2009), but recent literature has shown increasing usage of machine learning methods to perform this task, such as Chiu et al. (2019) and Talagala et al. (2019). Aminikhanghahi & Cook (2017) highlight that cluster analysis is a possible way to detect a structural break. Several studies address the use of cluster analysis to detect a structural break, including Corneli et al. (2018), Rakthanmanon et al. (2011), Tran (2019), and Wang et al. (2006). This paper builds on these approaches to select between two types of forecasting models: robust and non-robust to structural breaks. Based on the work of Li (2015), we propose a method to select between a set of robust and non-robust forecasting models. This method uses time-series clustering to identify possible structural breaks in a time series and switch between types of models depending on the series dynamics. This analysis is performed at every forecast step and the switch is based on the clustering result. Our main interest is to develop a method that could immediately detect a structural break when it is starting to show its effects and rapidly change to a robust model. Our mechanism should be able to change back to a non-robust model as soon as the structural break effects on parameters cease to occur. This is a streaming data approach, in the sense that we should be able to process the upcoming information and choose the best forecast device as soon as the new data comes in. The proposed approach differs from the broader literature concerning regime switching models, as discussed in Chauvet & Potter (2013). The most common approach of dealing with structural breaks is defining different types of regimes that a models can estimate and changing between them as different types of ""signals"" come. However, regime switching models have the restriction that they assume a finite number of states and that the future is like the past. Song et al. (2011) claims that structural break models allow the dynamics to change over time, however, they may incur loss in the estimation precision because the past states cannot recur and the parameters in each state are estimated separately. It is important to test the proposed new forecasting methods with data that are used for decision making in the real world. Macroeconomic indicators are of major importance to many stakeholders in modern society, including policymakers, politicians, and academics. Forecasting the future values of those indicators can be a powerful tool for government and business planning. Several recent studies address macroeconomic indicator forecasts, such as McKnight et al. (2019), Panagiotelis et al. (2019), and Smeekes and Wijler (2018). Therefore, we perform a rigorous empirical test with the proposed method. First, we undertake a Monte Carlo experiment to simulate 400 series with mean shifts, which allow us to verify if the proposed method is robust against those types of structural breaks. We also test the proposed technique with a set of macroeconomic features extracted from the OECD database: Industrial Product Index and Consumer Price Index. We perform a forecasting exercise for different windows and for all Western European countries present on the OECD database.Footnote 1 We compare our results with a range of benchmarks models. Our study contributes to the discussion of new forecasting methods combined with machine learning techniques by proposing a new method robust to structural breaks. This study also aims to improve the methodology behind output growth forecasting techniques, one of the most important macroeconomic variables for policymakers. We test our algorithm against a plethora of models in order to guarantee the robustness of our results, including Autometrics, Exponential Smoothing, Impulse Indicator Saturation, ARFIMA, Robust model from Oxmetrics and the Robust version of the Impulse Indicator Saturation. We also test it against a classical set of benchmarks, including average forecast combination, AR1, Bates and Granger combination method and Spline Regression. The rest of the paper is organized as follows. Section 2 gives a theoretical explanation about robust forecasts and how this method works, building on Castle et al. (2016). Section 3 outlines our proposed strategy, including time series clustering in §3.1, the proposed forecasting algorithm in §3.2, the range of forecast methods in §3.3 and the benchmark comparisons in §3.4. Section 4 reports the results of our strategy. Section 5 provides a discussion of the results and finally, some concluding remarks are drawn in Sect. 6.",1
18,2,Journal of Business Cycle Research,11 July 2022,https://link.springer.com/article/10.1007/s41549-022-00071-z,COVID-19 and Seasonal Adjustment,July 2022,Barend Abeln,Jan P. A. M. Jacobs,,Male,Male,Unknown,Male,"Economic time series are typically seasonally adjusted before being used in economic, econometric and policy analyses, where seasonality is defined as systematic, although not necessarily regular or unchanging, intrayear movement that is caused by climatic changes, timing of religious festivals, business practices, and expectations (Hylleberg, 1986). Seasonal adjustment consists of the estimation of the seasonal component and, when applicable, also trading day and moving holiday effects, followed by their removal from the time series. The goal is usually to produce series whose movements are easier to analyze over consecutive time intervals and to compare to the trajectories of other series in order to detect co-movements (U.S. Census Time Series and Seasonal Adjustment https://www.census.gov/topics/research/seasonal-adjustment.html; Wright, 2013). For common guidelines for seasonal adjustment within the European Statistical System, see Eurostat (2015). Several seasonal adjustment methods have been proposed, which we broadly classify in three groups. The first group is the X11 family, i.e. methods based on moving averages like X11 (see e.g. Ladiray and Quenneville, 2001), X12-ARIMA (see the appendix of Wright, 2013) and X13-ARIMA-SEATS (for details see the X-13ARIMA-SEATS Seasonal Adjustment Program homepage at the U.S. Department of Commerce Census Bureau https://www.census.gov/srd/www/x13as/), and methods based on ARIMA models like TRAMO-SEATS (Gómez & Maravall, 1996). The methods are applicable for quarterly and monthly time series. Ladiray et al. (2018) present some ideas to adapt the X11 family to weekly and daily data. A second group of methods is based on STL (a Seasonal-Trend decomposition procedure based on Loess), a non-parametric method introduced by Cleveland et al. (1990). This group of methods is quite flexible. Cleveland and Scott (2007) and Cleveland et al. (2018) perform seasonal adjustment of weekly time series building upon the seminal contribution of Pierce et al. (1984). Ollech (2021) proposes a method for daily time series based on STL. A third class of methods employs structural time series models or unobserved components models, which are also quite flexible in dealing with time series with different frequencies although calendar effects may be less easy to handle. Examples are Harvey et al. (1997), Koopman and Ooms (2003), De Livera et al. (2011), McElroy et al. (2018) and Proietti and Tommaso (2021).Footnote 1 Recently, Abeln et al. (2019) presented a new seasonal adjustment method CAMPLET, an acronym of its tuning parameters. The method consists of a simple adaptive procedure to extract the seasonal and the non-seasonal component from an observed time series. Once this process is carried out, there will be no need to revise these components at a later stage when new observations become available. CAMPLET can be applied to quarterly, monthly, weekly and daily data.Footnote 2 In this paper we study the impact of COVID-19 on seasonal adjustment. We focus on whether special adjustments are required to treat the COVID-19 crisis as an outlier as recommended by Eurostat (2020) to three seasonal adjustment procedures: X13-ARIMA-SEATS, STL and CAMPLET. In addition we investigate whether revisions occur when new observations become available. We apply Census X13ARIMA-SEATS (henceforth X13), the combination of Census X12-ARIMA and TRAMO-Seats which has become the industry standard, and CAMPLET, the method we proposed recently to the quarterly series of real GDP in the Netherlands. For weekly data, Lewis et al. (2021) recommend to transform series to logs, take annual or 52 weeks differences, and manual adjustment for problem weeks (moving holidays etc.). In this paper we use STL and CAMPLET for seasonal adjustment of the weekly series U.S. initial claims. We find that seasonal adjustment with X13 and CAMPLET requires adjustments in the implementation of the standard procedure to treat the COVID-19 crisis as an outlier; STL can be applied straightforwardly. In addition we observe that differences in seasonally adjusted values around the COVID-19 crisis are small. From the analysis of the weekly series U.S. Initial Claims we learn that STL seasonal adjustments follow observed values closely, whereas CAMPLET attributes part of the increase to a change in the seasonal pattern. Seasonal adjustments of X13 and STL are subject to revision, which probably will lead to the COVID-19 crisis becoming less deep when new observations become available. The remainder of this paper is organised as follows. Section 2 discusses seasonal adjustment methodology and describes the seasonal adjustment methods used in this paper. Section 3 provides an illustration with a quarterly and a weekly series. Section 4 concludes.",1
18,2,Journal of Business Cycle Research,08 July 2022,https://link.springer.com/article/10.1007/s41549-022-00072-y,A Wavelet Method for Detecting Turning Points in the Business Cycle,July 2022,C. Colther,J. L. Rojo,R. Hornero,Unknown,Unknown,Unknown,Unknown,,
18,2,Journal of Business Cycle Research,02 May 2022,https://link.springer.com/article/10.1007/s41549-022-00068-8,Bond Yields Movement Similarities and Synchronization in the G7: A Time–Frequency Analysis,July 2022,João Martins,,,,Unknown,Unknown,Mix,,
18,3,Journal of Business Cycle Research,24 June 2022,https://link.springer.com/article/10.1007/s41549-022-00070-0,The Effect of Communication and Credibility on Fiscal Disagreement: Empirical Evidence from Colombia,November 2022,Juan Camilo Galvis-Ciro,Juan Camilo Anzoátegui-Zapata,Cristina Isabel Ramos-Barroso,Male,Male,Female,Mix,,
18,3,Journal of Business Cycle Research,05 September 2022,https://link.springer.com/article/10.1007/s41549-022-00074-w,A New Theory of Expectations,November 2022,Richard T. Curtin,,,Male,Unknown,Unknown,Male,"Expectations play a prominent role in the collective research programs of CIRET members as a determinant of macroeconomic cycles. Aristotle wrote more than 2000 years ago that forward-looking decisions must be based on expectations, and that the accuracy of those expectations depend on human reasoning alone, stripped of the corrupting influence of passion. Decisions about nearly every aspect of contemporary life are now based on judgements about the expected likelihood of a given outcome. No contemporary of Aristotle, nor for that matter of Adam Smith, or even Maynard Keynes, would recognize the remarkable developments in our empirical arsenal of expectations. Everyone, from Aristotle to present day analysts, have recognized that the basic foundation of economic forecasting depends on expectations formed by rational calculations. Although theories of rational expectations have become more exacting and sophisticated over time, they still mirror Aristotles basic insights that he advanced more than two millennia ago. Despite the widespread agreement on the importance of rationality in economic decisions, there has been no consensus among social scientists about how rationality should be defined. Economics has avoided the details of the formation process, opting to judge rationality based solely on the accuracy of the formed expectation. Psychologists have taken the opposite approach, defining rationality by whether agents use a rational procedure to form their expectations, shifting the accuracy tests from the outcomes to the inputs. Given these fundamental differences in how rationality is conceptualized, empirical tests based on either theory cannot reject the other. This has meant that there has been disjointed theoretical guidance for the practitioners who measure and use expectations for predicting economic cycles. My research program at the University of Michigan has focused for the past half-century on understanding how consumers form expectations, the accuracy of their expectations, and their impact on shaping macroeconomic trends. Many scholars are skeptical about the ability of consumers to form rational economic expectations, based on either concept of rationality. Perhaps skepticism is too generous an assessment since many believe that the average consumer is simply incapable of gathering the necessary information and interpreting its meaning to form a rational expectation. Most believe that the expectations of consumers are simply uninformed guesses. I must admit that I was initially drawn to that view, partly based on hearing respondent mention that the formation process was tedious, the data too costly to obtain, too difficult to interpret, and the calculations were too hard to perform. Herbert Simon (1955) introduced the notion of bounded rationality, which recognized these difficulties, and Kahneman et al. (1985), along with many other scholars, produced the heuristics and bias approach to describing the formation of expectations that recognized the importance of these limitations. Although bounded rationality is now the dominant alternative to John Muths (1961) rational expectations hypothesis, I have always found the bounded rationality alternative unsatisfactory as it gave theoretical credence to the notion that people never learn from their repeated mistakes. The persistence of such maladaptive behavior goes against the overwhelming evidence of human evolutionary progress. After years of research, the data convinced me that a third alternative was needed to replace the simplistic choice between bounded and unbounded rationality. The first hint of the necessity of a new theory came from survey data that appeared to support both bounded and unbounded rationality. The result depended on whether the analysis was conducted at the micro or macro levels. At the micro level, focusing on the behavior of individual consumers, the data provided a clear rejection of the rational expectations hypothesis and showed greater consistency with the bounded rationality approach. At the macro level, focusing on aggregate economic trends, the analysis came to an opposite conclusion. The macro analysis found that the way expectations changed over time were consistent with the rational expectations hypothesis. This mixture of micro failures and macro successes was first noticed in the 1950s based on analysis of the University of Michigans Sentiment Index as a predictor of consumption. Given the limited number of time-series observations, the analysis focused on the micro data to determine the relationship between the Sentiment Index and consumption expenditures of individual households. James Tobin (1959) interpreted the evidence as a clear rejection of the predictive power of consumers’ economic expectations. Many agreed with his argument that the prediction failures among individuals could not be sensibly aggregated into positive results at the macro level. These empirical results led economists to eventually reject the positive findings at the macro level as illusory. But given its predictive performance at the macro level, survey data on expectations became an indispensable component of systems of leading economic indicators now conducted by nearly every country in the world (Baghestani & Fatima, 2021; Campelo et al., 2020; Cesaroni & Iezzi, 2017; Curtin, 2007; Lahiri & Zhao, 2016). The initial goal of this paper is to resolve this micro/macro duality by advancing a more robust and comprehensive explanation of the empirical results. A good deal of my critique is based on correcting erroneous interpretations of how expectation theories have been conceptualized in empirical tests. The more significant elements of my critique, however, involve our failure to incorporate the scientific advances made by other disciplines, especially neuroscience. Importantly, the theoretical shortcomings that I will highlight are not simply a matter of degree, but represent radical departures from orthodox theory. As such, they constitute a disruptive change. The most important changes include a more comprehensive definition of cognitive deliberation, the inseparability of reason from passion, the critical importance of contextual factors, revised standards for judging accuracy, the recognition of innate mental processes that guide revisions, and a more accurate assessment of costs and benefits. Each of these fundamental challenges to orthodox theory are justified by an increase in the explanatory power of the revised theory. This is an ambitious agenda. The most complete account of these changes is book length, which I entitled Consumer Expectations: Micro Foundations and Macro Impacts (Cambridge University Press, 2019). I will not repeat the supporting empirical data and analyses in this brief paper. My goal is to simply convince you that such a disruptive overhaul of the theory of expectations is necessary and the changes suggested are appropriate. Importantly, these suggested revisions apply equally to expectations measured in firm surveys as well as household surveys. I will begin with the least controversial departures from current conventions, and end with the most significant departures which will entail the greatest reformulation of empirical research designs. It is useful to start this discussion by summarizing the common elements of the conventional theories of expectations. The orthodox approach holds that firm and household agents form expectations about economic factors that could potentially influence their economic decisions. Economic agents are hypothesized to depend on the resources and expertise of federal statistical agencies to collect the data. The latest information from these statistical agencies are widely available, immediately disseminated in the mass media, and are used by households and firms to update their expectations. This process is repeated across a range of statistics so that a wide variety of expectations are formed and ready for immediate use when needed for making economic decision. An important property of the process is that the resulting expectations are formed to be independent of any contextual factors and thus thought to be appropriate for any subsequent use. The costs of forming expectations are considered to be lower than the average benefit derived from their use, so no cost–benefit calculations are usually necessary. Finally, the accuracy of any formed expectation is based on a comparison with the data collected by the federal statistical agency. One of the most important of the conventional assumptions, and the least challenged, is that economic agents use the data published by federal statistical agencies as the standard to form their own expectations, and to subsequently assess the accuracy of the formed expectations. A careful reading of conventional theory, however, makes it clear that the relevant expectations are defined by the economic conditions consumers or firms actually face in the marketplace. Who could arrive at an optimal decision if they based their expectations on economic conditions that they did not actually face? The differences are often substantial. For example, the data on unemployment collected by the U.S. Labor Department over the past several decades indicate that the mean rates of unemployment were three times as high among those with the least education as among college graduates. Why would those substantial differences be ignored when people form their own expectations? A more plausible expectation is that people would judge their job opportunities as comparable to other people with similar skills and abilities, and would usually restrict their focus on their own local areas. The same is true for inflation and income expectations and many other economic series. The use of national data has often been indirectly justified by the common use of representative agent models: with just one agent, the national data becomes the appropriate standard. Alternatively, this shortcut is sometimes justified by the high costs and impossible task of any one household or firm collecting the required personalized data. Nonetheless, the impact of these assumption are often ignored when interpreting the results, with some analysts incorrectly concluding that the heterogeneity of expectations across agents constitutes robust evidence of irrational expectations (Mankiw et al., 2004). Under a more appropriate analytic assumption based on the personalization of expectations, the heterogeneity of expectations would be seen as the normal outcome for rational expectations. Just this one correction has an enormous impact on the interpretation of the micro–macro split in the empirical results mentioned earlier. It was simply incorrect to assert that the macro relationship must be illusory given the lack of evidence at the micro level. What was illusory was the presumption that all expectations would be identical with the national figures. This finding was simply due to the empirical misspecification of the theory. That is not to imply that every respondent provided accurate data on the economic conditions that they actually faced. That would be a highly improbable claim. Even the rational expectations hypothesis does not require the absolute accuracy of every formed expectation; rather it requires unbiased expectations, meaning prediction errors are random and tend to balance out over the longer run. The proper analysis of the accuracy of expectations would require much more specific data collected within local areas and for various demographic subgroups. Federal agencies in the U.S. are now moving in this direction and it may become routine in a future era of big data. Another common assumption is that the national media provide extensive coverage of the economic releases of the statistical agencies. The coverage is so extensive, it is argued, that most people could not avoid hearing about the latest quantitative numbers for unemployment, inflation, GDP, and so forth. That assumption has been shown to be empirically false (Curtin, 2008; Curtin, 2010). I tested the assumption by searching twenty-two newspapers with the largest U.S. circulations, and five major TV and cable news stations. This task was easier than it sounds given that digital records were available for both print and video news sources. Each source was checked for coverage of the official releases that specifically cited the quantitative data for the rates of unemployment, inflation, and GDP. The time periods covered were two sixteen month periods, from January 2006 to April 2007, during an expansion, and from January 2008 to April 2009, during a recession. It was anticipated that economic news would draw much greater attention during downturns than during upturns. Surprisingly, only one media source cited quantitative information on all three statistics for every data release the Washington Post, a newspaper which had a daily circulation of less than one million at that time. Of the five TV networks, none included a quantitative figure for all releases. The average newspaper and TV network included quantitative figures for about half of the releases. The official websites of the statistical agencies were accessed much less frequently than the print and video media included, and had a much higher concentration of users from non-U.S. locations. Rather than citing numerical figures, the media most commonly described the data using subjective adjectives, such as surging inflation, strong job growth, soaring interest rates, or a sinking economy. These verbal descriptions were invariably supplemented with either a photograph or video of a person who either suffered or benefitted from the changing performance of the economy. This communication strategy was based on research on how the media could communicate their message as effectively as possible. Neuroscience has confirmed that people have an innate ability to understand another persons facial expressions and body language. Mirror neurons are activated that simulate those same emotions as though they themselves had those same feelings (Rizzolatti & Craighero, 2004). The important role played by emotions in how economic information is consciously perceived will be discussed in more detail in a later section. Many studies have found that consumers have a surprising lack of knowledge about official economic statistics (Blinder & Krueger, 2004). In a survey conducted at the University of Michigan, one-third of the population reported that they had never heard of the Consumer Price Index (CPI) or Gross National Product (GDP), and half as many reported that they had never heard of the official unemployment rate in 2007 and 2009 (Curtin, 2019). Another four-in-ten reported hearing of those statistics, but did not know the current rate. This data was first collected in 2007 near a cyclical peak, so it was thought that low inflation and high GDP didn’t attract much attention. The questions were repeated in 2009, with the hypothesis that the deepening financial crisis would greatly heightened peoples awareness of these statistics. This hypothesis was rejected as knowledge of the latest data on unemployment, the CPI or GDP was not significantly changed from 2007. Perhaps even more surprising, among those who reported that they had recently heard the official announcements, the errors in the unemployment, inflation and GDP rates were twice as large in 2009 than in 2007 despite excessive media attention to these economic statistics during the Great Recession. This lack of knowledge was for the official statistics. Needless to say, people become aware of changes in unemployment, inflation, and economic growth in many ways other than based on the official releases. I will discuss in more detail how, where, and why people acquire their information about the economy, but for now I will just indicate the results of alternative questions about prospects for inflation. When asked about the expected inflation rate during the year ahead, virtually every respondent gave a percentage figure. As many of you already know, the University of Michigans measures of inflation expectations are quite accurate at the macro level, rivaling forecasts by professional economists (Curtin, 2010; Mehra, 2002; Thomas, 1999). This implies that the lack of knowledge applies only to the official statistics and not to the underlying economic concept, a surprisingly positive result for economic theory. How consumers accomplish this feat represents the most challenging reinterpretation about how people process information and form their economic expectations. Prior to that discussion, I need to finish my discussion of the conventional means to assess the accuracy of expectations. Tests on the accuracy of expectations have suffered from the presumption that accuracy should be based on the official statistics. I have already noted that the national results are a poor proxy for the economic conditions that people actually face. Accuracy, however, is an even more demanding concept. Expectations are formed to improve decision making. Expectations have no intrinsic value, their value is derived from use. A more appropriate gauge for cost–benefit analysis is how the degree of accuracy improved the outcome of the decision. Economics has instead shifted the primary focus to the accuracy of expectations themselves, independent of any use. In sharp contrast to the implications of the contextual independence assumption, the degree of accuracy required varies depending on the actual decision. Some decisions need only a ballpark estimate, while other decisions need to be based on highly accurate expectations. The cost of forming expectations varies depending on its intended accuracy, whereas conventional theory assumes the costs associated with complete accuracy. Those costs of forming fully accurate expectations, however, are typically assumed to be negligible as the media provides the data at a near zero marginal cost and mental calculations were incorrectly assumed to be costless. The best interpretation of standard theory is that the costs of forming fully accurate expectations could be considered a form of insurance to guard against higher costs and lower accuracy when an economic expectation had to be estimated immediately for a pending decision. The same argument applies to the maintenance of a wide range of economic expectations. As a result, households and firms are assumed to maintain a mental or physical file containing a large number of expectations, with regular updates based on the latest data released by federal statistical agencies. Although the mental costs of maintaining all expectations are often consider negligible, the opportunity costs of using mental resources for maintaining a full range of expectations are substantial. One could easily imagine that opportunity costs are high enough to place rather severe limits on the number of expectations people consciously maintain based on the repeated data releases from federal statistical agencies. The new theory holds that expectations are context sensitive, not context independent. Elicited expectations are naturally subject to priming, framing, and anchoring effects. This sensitivity has been extensively documented by Kahneman, (2011) as well as by many others. These contextual factors have invariably been interpreted as a source of bias. This interpretation is misleading. Expectations are intentionally designed to be context sensitive. Adaptive behavior requires that expectations be tailored to specific situations. This contextualization of expectations is consistent with how the human brain processes and stores information. Who could deny that context counts in many situations, for example the price of identical products, identical homes, or wages for identical work all depend on where the product is purchased, where a home is located, and where the work is performed. Context independent expectations are only correct on average, and generally not optimal for any specific decision. Overall, the undue simplification of the theory of expectations has again resulted in self-inflicted errors in their interpretations of empirical analyses. Peoples cognitive abilities have played a dominant role in assessing an agent’s capability to form economic expectations. Cognitive abilities are involved in all phases of the formation process, from gathering the relevant information, interpreting the data, and performing the necessary calculations. The usual presumption is that the conscious mind has the most advanced cognitive abilities. Differences in cognitive abilities are difficult and expensive to measure in population surveys. The typical proxy to determine differences in cognitive abilities has been the level of formal education. Each analysis I undertook using education as a proxy, rejected the hypothesis that conscious cognitive differences were important for forming inflation and unemployment expectations (Curtin, 2019). The most surprising results were based on empirical tests of the rational expectation hypothesis (Curtin, 2010). This analysis was based on time series models of inflation expectation fitted separately for those who differed in educational attainment. From those who did not finish high school to those who completed post-college graduate studies, the estimated equations were not able to significantly dismiss the rational expectations hypotheses for any education subgroupthe average errors across the entire period were insignificantly different from zero, and the expectations of every educational subgroup were insignificantly different from the actual inflation rate over a period of twenty-five years. Similar tests were performed for unemployment expectations for the U.S. plus 14 EU countries, and arrived at the same conclusions: the level of education was not associated with differences in the accuracy of expectations (Curtin, 2019). Needless to say, many psychologists as well as economists were surprised by these empirical outcomes. Even with an imperfect proxy for cognitive differences, most would not have anticipated that conscious cognitive abilities would have no differential impact on forming economic expectations. These results depend on a distinction that many may not have considered. Most have assumed that cognition required conscious deliberation, but cognition can be undertaken by the nonconscious mind as well. Cognition is defined as the ability to acquire information, interpret information, and make decisions (Neisser, 1967). Developments in neuroscience have shown that all cognitive processes can be conducted either consciously or nonconsciously (Kihlstrom, 1987). Nonconscious cognition occurs without any awareness by the conscious mind. Indeed, these nonconscious cognitive processes represent the vast majority of cognitive activity, with the conscious mind fully controlling only about 10% of all behavioral decisions (Lindsay, 2020). The dominance of nonconscious cognitive processes resulted from millions of years of evolutionary development, whereas the prefrontal cortex, responsible for conscious deliberation, is a relatively late mental addition, added only about 250,000 years ago (Massey, 2002). An important implication is that the much longer evolutionary development of nonconscious cognitive abilities has resulted in more evenly distributed nonconscious cognitive abilities across the population. In contrast, the recent addition of conscious cognitive abilities varies considerably more across the population. Importantly, it is this longer and more equal development of the nonconscious that is hypothesized for its dominance in forming expectations. All current theories of expectations are restricted to conscious cognitive reasoning for gathering data, interpreting data, and calculating the resulting expectation. There have been two reasons advanced for these limitations, although neither reason can withstand scientific scrutiny. The primary reason is that the details of nonconscious reasoning are unknown, either to the person directly involved or to an impartial observer. The telltale difference, it is alleged, is whether people can provide cogent rationales for forming their expectations, such as the data and the assumptions used to generate their expectations. In contrast, it has been held that people who describe their expectation as merely a guess or a hunch are unlikely to have formed that expectation by rational deliberation. Unfortunately, all such inferences are unreliable since no person is knowledgeable of even their own nonconscious processes. More importantly, people have been known to favor deductive logic and causal models when asked to justify their expectations; they have learned and prefer those scientific explanations even when the expectation was formed nonconsciously (Nisbett & Wilson, 1977). This implies even among those who offer cogent justifications for their expectations, those responses may not indicate that conscious rational deliberation was involved. Aside from this natural preference for causal reasoning, there is another reason to doubt that people base their expectations on conscious rational deliberation. Conscious reasoning requires a degree of effort that people find difficult to sustain over long periods of time (Kahneman, 2010). Moreover, even relatively short periods of conscious deliberation cannot be completed without interruptions from other sensory inputs. The interruption of conscious deliberation typically means that the cognitive task are transferred to the nonconscious for additional processing; and then, at some future time, the issue may be returned for additional conscious processing. Indeed, the process of forming rational expectations, from gathering data, interpretation, and calculating expectations is likely to be a rather long process, with many interruptions in conscious processing. No person can accurately judge the relative influence of conscious versus nonconscious processes had in shaping the final result. The second reason for the reliance on conscious reasoning is the notion that what cannot be objectively observed, cannot be part of a scientific theory. That justification is simply incorrect. The true principle is that conjectures that are not falsifiable cannot be part of a scientific theory (Popper, 1935). Economics has long used scientific methods to empirically test the structure and impact on behavior of many black boxes, whose contents were not directly observable. Perhaps the most famous recent example has been the Higgs boson which was finally observed a half-century after its critical role in advancing particle physics was theorized. Continued progress in neuroscience is also likely to render features of black box of the nonconscious mind observable in the future. The evolutionary purpose of expectations is to provide a means for people to efficiently and effectively use their mental faculties to adapt to a changing environment. Innate mental processes automatically form expectations among the youngest of babies, even before they can walk or talk, and by the oldest of adults (Reber, 2002; Bargh & Chartrand, 1999). There are three distinct uses of expectations. First, expectations determine which mental faculties are used to process incoming information. Second, expectations are automatically updated by innate mental processes to maintain their accuracy. Finally, expectations are formed and organized in memory by contextual factors to achieve an optimal impact on behavioral choice. Before detailing these evolutionary purposes of expectations, some more general points need to be mentioned. Perhaps the most important is that all expectations are formed and revised by the same mental processes, even though they may be based on very different sources of information. The number and range of expectations is enormous, with economic expectations forming only a very tiny portion of the total. If the mental process for forming expectations was different for every type of expectation, it would easily overwhelm the resources of the human mind. Similar to modern computer algorithms, standard subroutines provide consistency in the formation and revisions of all expectations. The first function of expectations is to automatically sort information. The sorting criterion depends on whether the information represents events that were expected or unexpected. Fully expected events provide no new information about the environment. Fully expected events are still processed by the nonconscious and used for Bayesian updating of existing expectations. This sorting function of expectations is predicated on the fact that conscious information processing is severely limited, so much so that nearly all information is processed by peoples nonconscious cognitive resources. There is wide agreement among neuroscientists that the capacity for nonconscious information processing is orders of magnitude larger than for the conscious mind. It has been estimated that the advantage of the nonconscious information processing compared with conscious processing is as high as 11.2 million to 40 bits per second (Dijksterhuis, 2004; Lewicki et al., 1992; Wilson, 2002). The severe limitations on consciousness means that the sorting process cannot be accomplished by the conscious mind. Such a task would represent a needless waste of a very limited and valuable resource. This sorting function is done at a pre-conscious stage of awareness. This function is controlled by an area of the brain called the limbic system, which evolved millions of years prior to the prefrontal cortex which is responsible for conscious reasoning (Massey, 2002). The limbic system processes information a few milliseconds prior to peoples conscious awareness, and can alternatively route the information to conscious or nonconscious processing. The limbic system, sometimes called the emotional brain, also attaches an evaluation to the information it processes, which will be discussed in a subsequent section on the impact of affect on expectations. The outcome of the sorting process, as well as the attached evaluation, are not irreversible, but they are not random as they are based on the acquired experiences and learning of the individual (Zajonc, 1980). The second innate mental process tests the accuracy of current expectations and revises them when needed. The neurotransmitter dopamine automatically responds to the prediction error or gap between what was expected and what was experienced. Expectations are then updated based on this information by Bayesian procedures (Lindsay, 2020; Solms, 2012). Expectations are continually and automatically updated with each bit of additional information that is acquired based on the gap between expected versus realized outcomes (Friston, 2009). The acquisition of new information does not simply replace prior information, but it is modified based on the probability characteristics of the new and old information. These revisions are automatic, part of the natural functioning of the brain, with the evolutionary purpose of maximizing human adaptation to a changing environment. The third evolutionary function of expectations is to maximize the probability of a rational decision. Learning how to optimally respond to complex changes in peoples personal, social, or physical environments can be accomplished either consciously or nonconsciously. For the most difficult and complex decisions, shifting between modes is as common as it is automatic, even a repeated series of back-and-forth shifts. A sense of confidence or uncertainty about a pending decision is often reflected by the degree of consistency people feel between conscious and nonconscious assessments; commonly described by people as differences between my head and my heart. People recognize the advantages of sleeping on it before making their final decision; sleep has been shown to integrate both forms of learning (Fischer, et al., 2006). Moreover, nonconscious processes have some advantages over conscious processes: conscious deliberation is effortful, nonconscious is effortless, conscious deliberations is subject to interruptions by other events, nonconscious deliberation can remain focused on decisions over extended periods of time (Kahneman, 2011). Many observers may admit that nonconscious cognitive process may dominate when trivial or noncritical decisions are made. Conscious deliberation, it is argued, would dominate when decisions involve large dollar amounts and for decisions whose consequences involve life-or-death. A moment of introspection would indicate that such a simplistic view is faulty. People hold far more nuanced views about the appropriate integration of their conscious and nonconscious cognitive resources (Hogarth, 2001). Imagine you are driving to work along a familiar route. When arriving at the office, most people cannot recall whether they stopped at some traffic light or what traffic conditions existed when making turns, but simply assume they made appropriate decisions since they arrived safely at their destination. Such driving is controlled by nonconscious processes, while you were listening to the news, enjoying music, or conversing with a fellow passenger. If something out of the ordinary happened, conscious attention would occur; it could be a dog darting into the road or that you recognize a friend or colleague at an intersection. If an accident threatens, your nonconscious mind immediately caused your foot to move toward the break even before you became consciously aware of the impending accident. People implicitly trust their nonconscious to control their driving decisions, and if necessary, to alert their conscious mind to take control if an unexpected event occurred. These automatic processes must be learned over time, with new drivers paying greater conscious attention to avoid mishaps. Most examples of learning about the economy typically assumes the conscious use of deductive methods and casual models. The standard drill for school children is that correlation does not mean causation. The long accepted wisdom is that no causal relationships can be based on associations. Since nonconscious learning is driven by associations, it is met with considerable resistance. Nonetheless, recent innovations in econometrics as well as the increased availability of big data has begun to change these opinions. This shift has been due to both the failures of conventional econometric models as well as by the advantages of analytic methodologies based on associations. The rapid development and use of large econometric models during the past half century has led to the recognition that the underlying estimates are subject to serious errors due to model misspecification and coefficient instability. Hendry & Krolzig, (2005) and Sims, (1980) have advanced the notion that the models should be conceptualized as relationships among unobserved variables. These unobserved variables are poorly approximated by official statistics, but can be estimated by factor models based on their observed inter-correlations. These models are judged by predictive accuracy, just like traditional models. Importantly, these newer factor models follow the same methods used by nonconscious learning. Studies of nonconscious learning, such as artificial grammar (Reber, 1967) or studies of operating sugar factories (Berry & Broadbent, 1984), have confirmed Milton Friedmans oft cited example of the accurate billiards player: people can learn the outcomes of complicated problems by experience, and then play the game expertly, without ever knowing any of the underlying principles. This same prowess of nonconscious learning based on associations was confirmed when people could correctly recognize grammatical errors after some practice with artificially devised grammar rules but nonetheless could not verbally state the underlying rules. When subjects learned to maximize profits at a sugar factory, they could not specify the rules for profit maximization. When pressed to explain how they could perform so well and yet not know the underlying rules, subject were likely to cite explanations that relied on guesses, hunches, or intuitions. The same type of responses were given by respondents when asked to explain their economic expectations that were presumably formed nonconsciously. Even more surprising was when subjects were told the underlying rules of the artificial grammar or how to maximize sugar factory profits, that knowledge did not help them to perform any better than those who were not told the underlying rules. Associative learning has been shown to approximate optimal solutions. Some observers have seen this as a positive result and others as a shortcoming. The greater precision of standard econometric models is due to their ability to estimate an absolute maximum, if one exits. For analyses based on associations, the approximate solution excludes any claims of point accuracy. It could be argued that innate updating of expectations by nonconscious Bayesian methods could be improved by conscious deliberations, but the associated opportunity costs would usually be too large to incur. Rather than approaching the problem in a piecemeal fashion, people would maximize the accuracy of the total number of expectations formed by relying on the less costly mental procedures that are innate and automatic. The elimination of passion from reason has long been held as a critical element of rational decisions. The separation of reason from passion is not only unwarranted, it is impossible. The insistence of the separation is usually motivated by examples where the influence of emotions caused disastrous outcomes for a household or firm. There is no denying these extreme miscalculations are present, and many believe even common. Incorrect choices due to emotional influences are often grouped with losses from decisions which were rational at the time they were made but nonetheless caused disastrous losses. Although the stock market is the premier example of the intrusion of emotional responses, other examples include the extreme political partisanship that has influenced economic expectations as well as the influence of the covid-19 pandemic. Economic theory has tried to avoid these complications by the assumption of fixed preferences. This assumption has the dual benefit of isolating economic analysis from the influence of emotional responses and the impact of subjective preferences on behavioral goals. The static nature of orthodox economic theory also precludes how changing economic circumstances influence shifts in economic preferences and goals. These exclusions by assumption, however, do not change the fact that even simple behavioral choices have been found to be impossible without the influence of passion (Damasio, 1994). The separation of passion from reason is impossible since every bit of information that is processed, automatically comes with an attached evaluation. The limbic system, as mentioned earlier, initially sorts information to conscious awareness or to nonconscious processing. When the information is sorted, the limbic system also adds an evaluation of that same information (Zajonc, 1980). The conscious mind can reject the evaluation, but this is rare since those automatic evaluations are based on past experiences and learning in similar contexts. A comparable procedure is used in computational models to speed the convergence of its estimates: the model is initialized with estimates of the coefficients based on past experience or learning. The same is true for mental reasoning: it is made more efficient and effective to update revised expectations based on priors under similar contexts. This is the essence feature of Bayesian updating. The overall impact of affect on economic behavior is much larger than the simple impact of evaluations on perceptions. The range of affective influences on an individuals behavior is quite large, although most of those expectations are irrelevant to the functioning of the macro economy. A macro impact requires an emotion to have a similar influence on the behavior of a vast number of people at the same time. Three different types of affective influences that are germane to cyclical economic activities: evaluations, moods, and emotions. Evaluations are judgements about specific issues or events. They are brief generalized summaries, such as the information has good or bad implications, or signals a better or worse outcome. Survey questions often mimic these natural evaluations when asking about finances, jobs, GDP, and many other economic and noneconomic topics. Their use enables respondents to easily answer the questions and yields reliable and robust measures of expectations. Since the response scales are not very detailed, the responses are often seen to limit cross-section or micro analysis of the data. Nonetheless, they have been shown to have substantial predictive power in macro analyses. In contrast, economists favor more detailed response scales, with expectations elicited in terms of probability distributions. In macro tests of prediction errors of qualitative versus probability scales, the qualitative responses had far smaller prediction errors and were far less costly to administer in population surveys (Curtin, 2019). The second type of affect is mood. Moods are general affective states that have multiple causes and persist over time. Consumer Sentiment is often called the mood of the consumer. The importance of moods lies in how it influences information processing in the different phases of economic cycles. Upswings in economic activity not only support optimistic moods but also cause people to mainly use nonconscious processes to monitor information on the economy. In downswings, rising pessimism encourages people to mainly use conscious information processes. This difference has an important implication. The most significant is that shifts from nonconscious to conscious information processing can occur almost instantly across the entire population, but shifts from conscious to nonconscious information processing occurs very slowly. Even aside from sudden catastrophic changes, downturns in economic sentiment occur much more rapidly than upturns. The speed of sentiment downturns is due to how emotions increase the likelihood of conscious information processing. The resulting shift also acts to give greater weight to potentially negative information. These differences means that sentiment declines are much faster, usually enabling greater lead time for warnings of a potential recession. It also implies that sentiment recovers much more slowly and provides later signals about the start of a potential recovery. It should be noted that the shift to conscious information processing does not mean that people are more likely to seek out or devote greater attention to the economic statistics published by state agencies. It was already shown that consumers gave less accurate responses about GDP, inflation, and the unemployment rates in the midst of the 2009 recession than during the 2007 expansion. The increased conscious attention is for subjective evaluations of economic trends, not the underlying official statistics. Emotions represent the final type of affect that has a bearing on economic behavior (Elster, 1998). Emotions have a known referent and are intense. They can cause an immediate and discontinuous shifts in the behavior of households or firms. Emotions are much more likely to be negative than positive, suggesting that the evolutionary purpose is to signal potential threats rather than potential benefits (Ekman, 1992). Like evaluations and moods, to influence the course of the macro economy, emotions must be widely shared by a significant portion of the population. A few decades ago I used the examples of military conflict and terrorist attacks as examples, recently adding the covid-19 pandemic as a generator of emotional reactions. These events typically generate extreme and rapid economic dislocations as well as social discontent. Indeed, the emotions surrounding the pandemic has transformed the behavior of households, firms, and economic policies in ways that would have been unthinkable in prior years. As one example, federal and state stimulus and relief payments have soared to levels that have far exceeded any past recession, with citizens likely expecting the great expansion in payments to be repeated in response to not only future recessions but to become part of a sustained government welfare program. I have argued that the advantage of sentiment variables is primarily due to the added benefit that passion provides to reason, especially around cyclical turning points. Specifically, affect can substitute for reason in making decisions where risks are ambiguous or unquantifiable. Such situations are commonly called Knightian uncertainty. In these situations orthodox theory holds that no rational decision is possible. Economists have believed that such situations are extraordinarily rare, and found the most appropriate reaction was to simply postpone making any decision until the ambiguity is resolved. However, Keynes indicated long ago that rationality was effectively replaced by affect (animal spirits) for making decisions in these circumstances. For most consumers, the assessment of risks at turning points is often viewed as unquantifiable, but not making a consumption decision is usually not an option. It is precisely in those situations that affect substitutes for reason. It should be remembered that the content of these affective influences is based on Bayesian processes that update priors based on newly available information. Presumably, those are the same methods were envisioned by Keynes when he anticipated the actions of entrepreneurs when faced with Knightian uncertainty. Secular as well as cyclical economic trends can be influenced by emotions. I had for years dismissed the impact of emotions on cyclical developments in the U.S. economy. The dismissal was based on the notion that emotions were short-lived responses. To influence economic cycles, a sizable portion of households and firms must have the same emotional response over an extended period of time. The classic example is war, along with pandemics and economic inequality. While most U.S. presidents have had both fans and foes, Donald Trump converted those political differences into intense and lasting emotions. I documented at the last CIRET conference in Brazil, how those emotions had a significant impact on differences in optimism across political parties. Under the Biden presidency, those same intense emotional responses have continue, although optimism and pessimism have simply switched sides on the political aisle. The extreme partisan divide in the U.S. can be tied to differences in preferred economic policies with regard to the distribution of income and wealth. These policy differences are solely based on economic values not rational calculations, and are likely to have a strong impact on secular growth rates. Strong emotions have maintained and heightened these pronounced differences in economic policy preferences. The covid pandemic has heightened partisan calls for substantial changes, and already produced a number of policies unmoored to conventional economic principles. The implementation of these monetary and fiscal policies will serve as a political test of their effectiveness. Whatever the eventual outcome, economic policies directly reflect the importance of affective influences on the macro economy.",1
18,3,Journal of Business Cycle Research,18 October 2022,https://link.springer.com/article/10.1007/s41549-022-00077-7,"Determination of Quebec's Quarterly Real GDP and Analysis of the Business Cycle, 1948–1980",November 2022,Mario Fortin,Marcelin Joanis,Luc Savard,Male,Male,Male,Male,"Business cycle dating has long been a topic of interest to researchers. In the United States, the Business Cycle Dating Committee of the National Bureau of Economic Research (NBER) has been establishing and maintaining this chronology since 1920. Between the recession of 1854 and the one associated with the financial crisis of 2008–2009, this committee has identified 34 troughs and 33 peaks. It is the availability of long historical series on economic activity that has made it possible to reconstruct 170 years of business cycle history. In the Euro zone, this work is carried out by the Euro Area Business Cycle Dating Committee. In Canada, this role was held from the 1980s onwards for three decades by Philip Cross, Chief Economist at Statistics Canada. It was in 2012, at the initiative of the C.D. Howe Institute, that the Business Cycle Dating Council was created and took over from Statistics Canada. Dating can be monthly, depending on data availability, but is generally quarterly, requiring high-frequency aggregate economic data available over a long period of time. Compared with the United States, Canada is relatively deficient in this area, since the active series of quarterly real GDP and its components began in 1981 only. These series must be linked to earlier quarterly estimates available from 1947 to 1997 to construct a coherent chronology. By combining Canadian real GDP data with other coincident indicators of real economic activity, Cross and Bergevin (2012) were able to establish the timing and severity of twelve recessions in Canada between 1927 and 2012. Canada is a large country whose economic conditions are heavily influenced by terms-of-trade shifts resulting from changes in commodity prices (Baldwin & Macdonald, 2012). Yet the importance of resource industries in the economy differs greatly by region and is considerably larger in Alberta and Saskatchewan than it is in Ontario and Quebec. This means that there is a substantial difference in the way each region of the country responds to the Canadian business cycle (Lefebvre & Poloz, 1996), a situation that has been present for a very long time (Blain, 1978). However, the lack of historical data on provincial real GDP makes it difficult to accurately track provincial business cycles. Recent works have then relied on multiple coincident indicators, as in Lamy and Sabourin (2001), or looked at employment behavior (Kneebone & Gres, 2013) to make this analysis and their work focus on the post-1980 period. Because of the national attachment of Franco-Canadians to Quebec, the only province with a French-speaking majority in Canada, most of the prior provincial business cycle analysis have been performed in the province of Quebec. Key studies on this issue include, Raynauld (1961), Fortin (1980, 2001, 2011) Fortin (1980, 2001, 2011), Hébert (1989), Linteau et al (1989), Dickinson and Young (1992), Dauphin (2007), Haddow (2015), Geloso (2017), Dean and Geloso (2021). Dating the Quebec business cycle is problematic since provincial GDP data prior to 1981 were published on an annual basis and only nominal values are available. Our paper is set in this context. We use the available information on the annual values of Quebec's GDP covering the period 1926–1986 as well as certain quarterly variables related to Quebec economic activity that have been produced and published since 1948 to construct a quarterly estimate of Quebec's real GDP from 1948 to 1980. These estimates are then used to establish the peaks and troughs of Quebec's GDP, with the aim to enrich our understanding of post-WWII economic history. This study also provides some insights on strong support for the North American Free Trade Agreement (NAFTA) in Quebec in the 1980s and 1990s as documented in some papers including Martin (1995), Gagné (1999) and Lachapelle (2011). To carry out this work, we use the methodology proposed by Ginsburgh (1973) as modified by De Carufel and Lizotte (1982). This method uses movements in quarterly indicator variables to construct quarterly estimates of real GDP that are both fully consistent with the annual estimates while minimizing interannual discontinuities. The indicator variables used in the reconstruction are the value of retail trade, the industrial employment index for Quebec, the number of housing starts, and quarterly real GDP for the United States. From the estimates of Quebec's real GDP it is possible to compute by subtraction the real GDP of Canada without Quebec (Rest of Canada-ROC). This procedure will show that for the period under study, the Quebec business cycle has more similarities with the U.S. cycle than with the cycle in the rest of the country. We can therefore conclude that the Canadian dating of Cross and Bergevin (2012) includes important regional variations. The outline of the article is as follows. In the next section, we review existing studies on the Quebec business cycle. The modified Ginsburgh method is presented in Sect. 3 and we follow in Sect. 4 with a description of the data used. In the next section (Sect. 5), we present the quarterly real GDP series and analyze the results. We then compare the Quebec cycle with that of the ROC and the United States (Sect. 6) before concluding (Sect. 7).",
18,3,Journal of Business Cycle Research,01 September 2022,https://link.springer.com/article/10.1007/s41549-022-00076-8,The Chemistry of the Macroeconomy,November 2022,Robert Gmeiner,,,Male,Unknown,Unknown,Male,"For centuries money has been used to facilitate transactions. In the modern era, debt, stocks, bonds, and other assets have become prevalent alongside money to enhance economic systems. These assets serve some of the same functions as money, such as a store of value, but their prices can fluctuate independently of prices in the real economy. Recently, asset booms and busts have occurred alongside stable, low inflation. These assets also augment the role of money as a medium of exchange by offering liquidity from once-illiquid assets. The prevalence of non-money assets divides the role of money as a medium of exchange into two related, but distinct functions, (1) to facilitate the production and exchange of goods and services and (2) to facilitate “nonproductive transactions.” The residual calculation of money velocity misses asset transactions, thus understating the frequency with which a dollar changes hands, which is this paper’s starting point. Throughout the paper the terms “nonproductive transactions” and “nonproductive payments” refer to interest payments and transactions for nonproductive assets. Nonproductive assets include both backed and unbacked financial assets. More specifically, they are any form of debt, stock, or other asset that is not a factor of production or direct contributor to GDP. Specifics of what nonproductive transactions include are given in Sect. 3.1. This paper answers the question of whether a high prevalence of nonproductive transactions is detrimental to long run economic growth. I define nonproductive transactions into a new variable that augments Fisher’s equation of exchange to produce more accurate measures of money velocity. In order to achieve sustained long run growth, central banks must pay attention to transactions for nonproductive assets when formulating stabilization and growth policies. The premise underlying the hypothesis that nonproductive transactions adversely affect long run economic growth relies on the link between the financial sector and the real economy. A large financial sector houses a secondary market for nonproductive assets that results in price fluctuations and changes in financial wealth that are at least somewhat independent of the real economy. In 2019, value added by the financial sector amounted to 7.9% of GDP in the United States, and this figure jumped to 8.6% in 2020 during the COVID-19 recession, which shows the size and economic significance of the financial sector. From the peak of 8.6%, it has only diminished to 8.5% in 2021, and its low point since it was first reported in 2006 came in 2005 at 6.0%.Footnote 1 This nonnegligible sum is only the value added by these services, and does not include the concurrent changes in asset prices. Asset price changes underpin changes in financial wealth, and they can affect consumption and investment. Without a corresponding change in investment and domestic production, consumption changes may also affect the current account. To answer the research question, I develop an economic model with roots in chemistry equations. In relating economic ideas to chemistry, I continue the long tradition of adapting models from other disciplines, such as physics, for use in economics, for example, Bouchaud (2001). My goal in developing a model from chemistry equations is not simple creativity, but to explain macroeconomic events and trends and offer new decision rules and policy targets beyond what existing macroeconomic models have done. I establish the link between chemistry and economics by beginning with known economics equations, from which I derive equations similar to chemistry relationships. Section 2 discusses relevant literature, after which I develop the theory in Sect. 3. Section 4 empirically validates this theory, and I present a policy discussion in Sect. 6. This analysis provides insights regarding modern macroeconomic events, including trends that set the stage for the global financial crisis and Great Recession decades. The course of these events coincided with increasing amounts of nonproductive transactions. Section 5.1 documents major changes in macroeconomic trends that occurred beginning in the 1980s, when the subject matter of this paper became persistently important.",
18,3,Journal of Business Cycle Research,09 May 2022,https://link.springer.com/article/10.1007/s41549-022-00069-7,Adjustment Speed toward Target Leverage Throughout the Vietnamese Corporate Life Cycle: Under-Versus Over-the-Target Firms,November 2022,An Thai,Radu Burlacu,,,Male,Unknown,Mix,,
18,3,Journal of Business Cycle Research,30 November 2022,https://link.springer.com/article/10.1007/s41549-022-00078-6,Political Budget Cycle: A Sub-National Evidence from Pakistan,November 2022,Rabia Nazir,Muhammad Nasir,Idrees Khawaja,Female,Male,Male,Mix,,
19,1,Journal of Business Cycle Research,03 August 2022,https://link.springer.com/article/10.1007/s41549-022-00073-x,Monetary Policy in Oil Exporting Countries with Fixed Exchange Rate and Open Capital Account: Expectations Matter,March 2023,Omar Chafik,,,Male,Unknown,Unknown,Male,"The UAE authority has been pegging the country currency to the US dollar continuously since 1971. The assessment of the costs and benefits of alternative exchange rate regimes has been hotly debated for decades. On one hand, supporters of fixed regime have argued that the credibility and lower inflation provided by this type of regime lead to a more stable economic environment and faster economic growth. On the other hand, supporters of flexible exchange rate claimed that the insulation properties of this regime are so far better than those of the fixed one. Overall, no clear answer has been given regarding the superiority of any regime as discussed in Appendix 1. The implications of different regimes are probably country and period dependents. For UAE, the fixed exchange rate helped to stabilize inflation and achieve an average CPI inflation of 3.8% between 1980 and 2019. This rate falls to 1.1% between 2010 and 2019 with a real GDP growth of almost 4%. Despite the economic implications of the UAE policy orientation about fixing the exchange rate, this choice is challenging the central bank which should set the optimal interest rate consistent with UAE economy. In fact, an important spread between domestic and foreign interest rate will create an arbitrage which will encourage FX speculation and trigger a speculative attack. Theoretically, the UAE central bank just needs to fit the uncovered interest rate parity (UIP) condition by equalizing its interest rate to the FED’s oneFootnote 1 augmented by the risk premium and the expected exchange rate differential. In practice, the decision is more complex because of the unknown evolution of the risk premium and the exchange rate expectations. Hence, central banks in countries such UAE rely on some assumptions to set the nominal interest rate (NIR). One simple assumption is to assume that a relatively higher domestic NIR comparing to the foreign NIR is enough to eliminate the arbitrage and guarantee the sustainability of the fix exchange rate regime. In this case, if the NIR is too high then monetary conditions will be very tight and discourage the domestic demand, and if the NIR is not enough high then the arbitrage will not be eliminated. For example, the UAE central bank could set a permanently 2 percentage points higher NIR relatively to the FED interest rate, but how to be sure if this level is optimal for UAE economy? Another assumption which is more common is to assume that the expected exchange rate differential is null and consider only the risk premium. This assumption is motivated by idea that the fix exchange rate regime will anchor the expectations and equalize them to the policy exchange rate. Alekhina and Yoshino (2019) challenged this assumption and underlined the significant effect of oil price fluctuations on exchange rate and other macroeconomic aggregates for Russia which is an oil exporting country. Still, Russia has an inflation-targeting framework and the exchange rate is floating. For UAE, the exchange rate is fixed, and this could effectively lead exchange rate expectations to be static. However, the impact of the oil price on UAE economy is important and oil revenues is one of main determinants of UAE GDP growth, and we need to know if oil price changes does impact exchange rate expectations also or not? Moreover, Kim et al. (2017) showed an important impact of oil price on NIR for China which has a pegged exchange rate regime. Then, two main questions are raised: does the reaction of exchange rate expectations to oil price fluctuations explain this impact? and is this mechanism works also for UAE? The modelling framework used by Kim et al. (2017) as well as the other economic models such the ones developed by Roger et al. (2009) or Benes et al. (2013) cannot answer those questions. A different analytical framework is needed for UAE to verify if the oil price has an impact on the exchange rate expectations or not? And to what extent those expectations needs to be considered when setting the NIR? This paper suggests a market-expected exchange rate (MEER) mechanism which considers the effects of the policy exchange rate and the oil price fluctuations to set the exchange rate expectations. This mechanism is introduced within the UIP condition into a new-Keynesian model (NKM) consistent with a fixed exchange rate and an open capital account economy.Footnote 2 The model is estimated for UAE and shows a relevant impact of oil price on the exchange rate expectations despite the significant effect of the policy exchange rate. Moreover, the simulation results show that the assumption of a null expected exchange rate differential partially holds facing demand, inflation and foreign NIR shocks but it hardly does for oil price shocks. The retrospective analysis between 2010 and 2019 highlights the relatively over-valuated policy exchange rate with regards to the MEER. The assessment of NIR during this period shows that considering the MEER mechanism would have advised different monetary policy decisions. The rest of the document is presented as follows. The second section highlights some particularity of oil exporting economies such UAE with fix exchange rate and open capital account. The third section presents the details of the theoretical specification adopted in this work. The last section discusses the results and presents some shock simulations and MEER policy implications.",
19,1,Journal of Business Cycle Research,13 March 2023,https://link.springer.com/article/10.1007/s41549-022-00080-y,Are German National Accounts informationally efficient?,March 2023,Roland Döhrn,,,Male,Unknown,Unknown,Male,"Macroeconomic analyses, as well as forecasts, are to a large extent based on national accounts (NA) data. However, national accounts data are usually revised several times after first release, which may lead policymakers to draw incorrect conclusions (Runkle, 1998), as well as forecasters (Döhrn, 2019). In Germany, NA data are typically designated as ‘final’ three and a half years after the end of the reported year. Thereafter, the data will only be subject to benchmark revisions. The differences between first release and final data are substantial: The mean absolute revision of the year-over-year growth rate for the quarterly real GDP from 1994 to 2013 was 0.47 percentage points in Germany, which is at the lower end among OECD countries (Zwijnenburg, 2015). Particularly in small countries, the extent of the revisions is much larger. Seemingly, there was also little progress made in reducing the size of revisions. For Germany, almost the same results were found for earlier periods of time as today (Ahmad et al., 2004; York & Atkinson, 1997). Given the volume of revisions, it is worthwhile to look for ways to reduce them. One option is to establish additional surveys in areas that are particularly prone to changes. However, this would place new reporting responsibilities on the economy. Another option might be to use big data, i.e., data that are byproducts of business and administrative systems, social networks, and the internet of things. These data may shed some light on activities not yet covered by statistics, however, this is still an evolving field of research (for an overview, see Hammer et. al., 2017). A third option, which is included in the focus of this paper, is improving NA compilation methods. An evaluation of revisions may indicate how to do this (George, 2004, Hoven, 2008). From a methodological point of view, the third option is closely linked to the question of whether revisions are noise or result from new information (Mankiew and Shapiro, 1986). If all information is used efficiently, revisions should be noise, i.e., there is no systematic pattern, and in that case, a more efficient use of data sources would not help reduce revisions. For the question analyzed here, the opposite is interesting: If revisions are not noise, this suggests there is room for their reduction by using data sources more efficiently. However, judging whether revisions are noise or the result of new information is not that simple. As an analogy to the forecast evaluation literature, a distinction could be made between weak efficiency, strong efficiency (Stekler, 2002), and—as introduced by Nordhaus (1987)—the Nordhaus efficiency. Weak efficiency results when first release data are orthogonal to the final data. Strong efficiency, in the present case, means that revisions are uncorrelated with any data that are already known when the NA are calculated. Finally, Nordhaus efficiency differentiates between vintages of revisions, and results when a revision at time t-1 is uncorrelated with the preceding revision at time t. Although tests for weak efficiency can be found in most papers analyzing NA revisions, tests for strong efficiency are scarce. Faust et al. (2005) analyzed covariations of GDP revisions in G7 countries with oil prices, stock rents, and interest rates, finding significant coefficients for some countries but not for Germany. York and Atkinson (1997) found in their G7 study some covariations of GDP revisions with inflation in the case of Germany and of Canada. Nordhaus efficiency has not been investigated yet in the context of revisions, to the author’s knowledge.Footnote 1 This paper contributes to the existing literature in four ways. First, the study focuses on strong efficiency and, thus, on the question of what could be done to reduce revisions. Second, whereas most literature concentrates on GDP revisions (an exception is York & Atkinson 1997), this paper analyses many demand side and some production side components of GDP. Third, this study makes a distinction between benchmark revisions and current revisions. Benchmark revisions are internationally coordinated and guided by methodological considerations. They cannot be reduced through better compilation methods. The lever to decrease revisions is the reduction of current revisions, which can be achieved, for example, through better estimation methods. Therefore, this paper uses an intuitive approach described by Döhrn (2019) that splits total NA revisions into benchmark and current revisions. Finally, this paper takes into account the characteristics of the German NA revision process. As a rule, the initial missing primary statistics come from annual surveys. When these become available, the annual NA data are first revised and thereafter, the results are broken down to quarters. Finally, the quarterly data are seasonally adjusted using time-varying seasonal factors.Footnote 2 Earlier papers like York and Atkinson (1997), Faust et al. (2005), Garatt et al. (2008), Glass (2018), and Strohsal and Wolf (2020) analyse the total outcome of these steps in the revision process. Thus, they take the user’s view asking what size of revisions can be expected and how the data must be interpreted. Focusing on annual NA revisions, since only these can be reduced by a more efficient use of the available data, this paper provides hints, how revisions could be reduced in the future. This paper is organized as follows. Section 2 will scrutinize how “efficiency” is measured in the context of revisions, and will propose tests of the properties of data revisions. In Section 3, the data are presented, along with a description of how current revisions are separated from benchmark revisions. Section 4 details the tests for weak efficiency, strong efficiency, and Nordhaus efficiency. Section 5 concludes and provides recommendations for producing NA.",
19,1,Journal of Business Cycle Research,15 December 2022,https://link.springer.com/article/10.1007/s41549-022-00079-5,The Forecasting Power of the ifo Business Survey,March 2023,Robert Lehmann,,,Male,Unknown,Unknown,Male,"The usage of business survey indicators is common in applied forecasting work. Also the existing academic literature certifies business surveys to be powerful tools for economic forecasting or tracking economic activity. However, most of the studies focus on a rather small number of economic aggregates such as gross domestic product (GDP), (un-)employment or inflation (see, for example, Hansson et al., 2005; Claveria et al., 2007; Angelini et al., 2011; Martinsen et al., 2014; Österholm, 2014; Lehmann and Weyh, 2016; Basselier et al., 2018; de Bondt, G. J., 2019). Furthermore, most of the media attention is gained by the headline indices of a business survey. Nevertheless, the bulk of business surveys do not only provide leading indicators for the most obvious macroeconomic aggregates, they rather comprise a large pool of indicators that mirror the development of other economic variables at hand. The aim of this paper is to illustrate the large possibilities a business survey offers for economic forecasting both from an academic and an applied perspective. In the following case study for Germany, I do so by giving a systematic overview over existing studies that evaluate the forecasting power of the indicators provided by the German ifo Institute. Its very old and accepted ifo Business Survey is mainly known for one of the most important leading indicators for German GDP growth: the ifo Business Climate Germany. In contrast to rather traditional literature overviews, I do not focus on one question for one economic aggregate across several countries, but present the large universe of indicators one survey for one country offers for academics and practitioners. In the end, my overview tries to accomplish that any user of the ifo Business Survey might go beyond the usual applications for rather standard macroeconomic aggregates such as GDP. This overview is not the first to evaluate the forecasting power of the ifo Business Survey: two major literature reviews are provided by Abberger and Wohlrabe (2006) and Seiler and Wohlrabe (2013), both written by researchers that have been employed at the ifo Institute at that time. Both articles have in common that they exclusively focus on studies for the performance of the ifo Business Climate Index to forecast either German GDP or industrial production (IP). But to date, a large body of literature exists that either studies the forecasting properties for other economic variables (for example, export growth) or focus on the regional level. The survey at hand aims to enhance the existing literature reviews with respect to two dimensions. First, I list all articles that have been published until or are in preparation at the end of December 2021. And second, I will also review the studies that go beyond GDP or IP in order to point up to the variety the ifo Business Survey offers in terms of questions, sectors, or even regions. The surveyed articles are divided in seven categories: (1) GDP, IP, and turning points; (2) expenditure components of GDP (for example, exports) and prices; (3) labor market outcomes; (4) variables for manufacturing and trade; (5) service sector outcomes; (6) regional economic variables; and (7) revisions of economic variables. I allocate the existing studies to at least one category and summarize the main results of each study concerning the forecasting power of the applied ifo indicator(s). For each study, I additionally give a detailed overview of the applied method(s) and the time period under investigation. In sum, the majority of existing studies certify the ifo indicators a high forecasting power. For German GDP especially the three headline indices (ifo Business Climate, ifo Business Situation, and ifo Business Expectations) either in delimitation of Industry and Trade (sum of manufacturing, construction, and trade) of for Germany (industry and trade plus services) provide accurate forecasts. On the expenditure side of GDP, the ifo indicators are valuable leading indicators (for example, the ifo Export Climate to forecast German export growth). On the production side of GDP, the ifo Institute provides good leading indicators for a multitude of different industries (for example, the ifo Business Climate Manufacturing). Next to these outcome variables, the ifo indicators are also able to accurately forecast labor market variables (for example, the ifo Employment Barometer to forecast employment growth) or inflation (for example, the ifo Price Expectations as leading indicator for producer price development). The good forecasting power of the ifo indicators is not solely confirmed for the German economy but also for three regional entities (the German states Baden-Württemberg and Saxony as well as Eastern Germany) or several sectors. Overall, the ifo Business Survey offers a large variety of indicators that might enter a practitioner’s toolbox. The literature survey at hand is organized as follows. I briefly introduce the main features of the ifo Business Survey in Sect. 2. Section 3 defines the criteria for the selection of the articles and the subsequent categorization. For each of the seven categories I discuss and present the existing studies in Sect. 4. Section 5 concludes and outlines the possibilities that arise from this overview.",2
19,1,Journal of Business Cycle Research,07 September 2022,https://link.springer.com/article/10.1007/s41549-022-00075-9,Relative Performance of Business and Consumer Economic Expectations Across EU Countries,March 2023,Richard T. Curtin,,,Male,Unknown,Unknown,Male,"The European Union (EU) has a long history of conducting surveys to assess the current economic conditions facing each member country as well as expected economic developments (European Commission, 2021). Harmonized monthly surveys are conducted within each member country of the business and the consumer sectors. This paper will focus on the questions that assess expected changes in jobs and the overall economic growth rate in each EU country. It has been long held view in the literature that the expectations of business firms would normally be more accurate than those held by consumers (Simon, 1955). Respondents to business surveys are thought to have easy access to more relevant information and are more motivated to form accurate economic expectations. Unfortunately, there has been little research based on an empirical comparison of the economic expectations of firms and households. This article aims to address that void by focusing on the results of surveys of economic expectations conducted within each member country in the EU. Importantly, the relative performances will be solely based on the results for each individual country, in contrast to the more common approach based on the aggregated data for the entire EU community. There are significant differences and some commonalities in the economic circumstances faced by firms and households. Economic recessions as well as economic expansions act to shape the behavior of all economic agents, regardless of whether the decisions are made within firms or households. The underlying difference has always been related to the timing of decisions, with the advantage usually going to business. The usual hypothesis is that firms form expectations of economic conditions more rapidly and accurately. Expectations held by households, in contrast, exhibit more sluggish responses to economic conditions, tending to lag actual economic developments, or even exhibit no correspondence with actual developments (Carroll, 2003). The analysis is based on country-by-country comparisons of the economic expectations of firms and households as published by the EU (EU Commission, 2021). The harmonization of question wording is a necessary but not a sufficient condition to compare survey findings across firms and households. There are bound to be significant differences across countries in the composition and size of firms and households, in the manner and frequency that economic information is communicated, the ability and willingness of households to obtain the necessary information to form economic expectations, as well as many other cultural and social differences across countries included in the EU. Moreover, since the firm and household surveys are typically collected by different survey organizations, there is substantial research that has documented “house” effects among professional survey organizations. These “house” effects often reveal different response distributions to identical questions (Hippler & Schwarz, 1987; Schumann, et al., 2020). The analysis undertaken is necessarily exploratory. The primary research question that will be addressed is whether a country’s firm and consumer data bear any significant relationship with one another, and whether they display a significant impact in predicting the statistics on employment and GDP published by each country’s official statistical agency. All of the analysis will be limited to the information content of each country’s firm and consumer surveys, with no pooling of data across countries or within any country for the four different types of firms sampled: industrial, construction, service, and retail firms. For each countries and industries, the research design call for the same analytic techniques and assumptions to be used. While this approach may entail some specification errors for individual countries, the analysis is meant to explore the size and extent of the raw differences. Subsequent research can then focus more clearly on any revealed differences as well as the comparative strengths of the firm and consumer data collected by the EU to enhance data quality and economic policy responses. A broadly similar analysis was done by Fair and Shiller (1990), with one key difference: instead of comparing 17 different econometric forecasting models for GDP of a single country (U.S), this analysis compared expectations based predictions of GDP and the unemployment rate across 28 counties based on the same forecasting model. There are three essential tasks when forming economic expectations: the acquisition of the relevant data, the interpretations of the recent data trends, and the ability to perform the calculations required to construct and revise expectations. Business firms are assumed to have the required skills and resources to perform each of these tasks and thus generate more accurate expectations. In contrast, consumers are assumed to be more likely to base their expectations on incomplete data, use naive interpretations of the data, and employ a simplified method to form their expectations (Kahneman & Tversky, 1982; Simon, 1955). Heuristics or rules-of-thumb are commonly thought to be widely used by consumers as well as extrapolation or learning from past errors. All of these shortcuts produce biased expectations. Even if firms also use these shortcuts, the resulting biases are often judged to be relatively smaller among firms than households due to the use by firms of more informed rules-of-thumb. The presumption of firm superiority in forming economic expectations is widespread and plainly evident in most media reports of economic expectations. Indeed, Carroll (2003) theorized consumers form their expectations in response to media reports of experts, usually associated with firms or governmental agencies. Consumers do not need to depend on forming their own forecasts as they can simply adopt the forecasts made by firms or agencies as reported in the media. This implies that households would imitate expert expectations, indicating that consumer expectations would lag behind the expectations held by firms. The firm’s advantages in forming expectations are supported by a variety of additional factors. The first advantage is motivation: specific employees are usually assigned to maintain economic forecasts, with their salary and career advancement associated with successful forecasts. Even within very small firms, say owner-operators, accurate expectations could mean that the profitability of the firm would be higher, providing a monetary incentive for accurate expectations. Moreover, firms must repeatedly form the same economic expectations relevant to their business, offering greater learning opportunities. Firms need to make repeated decisions that depend on how economic expectation may have recently changed. Not only is the frequency of updates greater, but the level of concern about accuracy is higher for firms. There is another theoretical argument that supports the lagged response of households compared with firms, namely the implied dynamics of conventional macroeconomic theory. Production plans of firms are necessarily prior to the purchase decisions of households. Business firms are assumed to make independent decisions on future investments and production, whereas consumer spending is endogenous, with their decisions depending on the evolution of incomes and prices (Curtin, 2019). This view implies that expectations of firms should precede its formation by households. Finally, it should be noted that Muth’s (1961) rational expectations hypothesis implies more equal footing for the formation of expectations by firms and consumers. Economic rationality is theorized to be a universal characteristic, and departures from rationality could be anticipated to be just as likely among households or firms. Indeed, households may be just as sensitive to some economic developments as firms, say potential cyclical downturns in employment and incomes. As a result, households as well as firms are motivated to hold accurate expectations of the economic factors that influence their own situation. Conventional economic theory posits that economic agents should hold accurate expectations about the conditions they actually face in decision making (Curtin, 2019). The difference between how firm’s and consumer’s economic expectations are measured in the EU surveys reflects commonly assumed differences in the economic situations of firms and households. Perhaps the most common is that households are all treated identically, while differences across firms require that their unique features be recognized and handled appropriately in the wording of questions. Importantly, the questions asked to firms used the generally accepted jargon, presumably in common use across EU countries. For example, the firms output was described as production, order books, turnover, or sales depending on the type of firm. In contrast, all households were treated equivalently. The consumer surveys included no variations in question wording despite sharp differences in economic behaviors across consumers, for example, those in the bottom versus the top of the income distribution, or the youngest just starting households versus older and retired consumers. Although non-response weights are used to achieve a representative sample, the goal of the sampling procedure is to give each household an equal probability of selection (European Commission, 2021). In sharp contrast to the firm samples, where the firm’s size is taken into account when aggregating across firms, no recognition of the differential economic impact of a household’s income on spending is taken into account. The notion that the typical firm in the business samples has at its disposable more detailed and more recent information on which to base their expectations is also suspect. To be sure, large firms are likely to have the type of personnel with an identified job function to prepare economic forecasts, which the firm uses to guide their future production, employment, and investment expenditures. As such, these business respondents are assumed to form expectations using more rational methods and be least susceptible to biased forecasts. Such large non-financial firms are relatively rare, however, with firms with 250 or more employees accounting for less than one-half of one percent of all EU firms in 2018. At the other extreme, just over nine-in-ten nonfinancial firms in the EU had fewer than 10 employees. Nonetheless, a tiny proportion of large firms employ slightly more people than the smallest firms—35% versus 29%. Moreover, large firms accounted for twice as much value added compared with small firms—47% versus 19% (Eurostat, 2021). Differential selection probabilities as well as the sample weights given to individual firms are employed by the EU to provide estimates that are representative of overall employment and value added. Needless to say, differences in weights across firms can be expected to be very large when fully aggregated to EU totals. This paper’s analysis avoids those complications by focusing the entire investigation based on each firm type within each country, and never across countries or types of firms. The European Union has collected monthly data on business and consumer expectations based on harmonized surveys administered by each member country. Monthly survey data are available starting in January 1985 for many members, with new countries added as the size of the European Union increased; all of the data are available in the EU website (EU Commission, 2021). The data series used for this analysis covers 35 years, ending in December 2019, for a total of 420 time-series observations. The end date was selected to avoid data collected during the covid-19 pandemic. All 28 member countries were included in the analysis. A minimum of 120 monthly observations on questions asked to industrial firms were required. The countries included were: Austria, Belgium, Bulgaria, Cyprus, Czechia, Germany, Denmark, Estonia, Greece, Spain, Finland, France, Croatia, Hungary, Ireland, Italy, Lithuania, Latvia, Malta, Netherlands, Poland, Portugal, Romania, Sweden, Slovenia, Slovak Republic, Turkey, and the United Kingdom. There were a few of these countries which had Industry data for at least 10 years, but three had slightly fewer observations for some types of firms: the lowest was 104 observations for Turkey (construction, services, and retail) and Malta (retail), and 116 observations for Denmark (services and retail). Luxembourg was eliminated since it had no observations on service and retail firms, although it met the 120 observation requirement for industrials. The harmonized wording for questions on employment and production are shown in Table 1. No identically worded questions were asked in both the household and firm surveys. The two key differences were how the surveys described the concept and the time horizon. The household surveys used a one-year time horizon, and the firm surveys used a three-month horizon. Firms were asked about trends in employment, and households were asked about trends in unemployment. Although trends in employment and unemployment often diverge, the analysis assumed that time trends in one was approximately the inverse of the other; given the range of responses that were included under the broad terms of “increase” or “decrease,” any mismatch in cyclical trends was effectively minimized. The various measures of “production” differed depending on the characteristics of the industries. Presumably, the differences in wording were introduced to provide a better match between the desired concept and the appropriate language used by firms across EU industries. For firms, the measurement objective was the anticipated behavior of their own firm, which when aggregated across the entire sample in each country, provides an estimate of expected contributions to changes in GDP growth. Although households do not directly participate in production planning, they need some knowledge of the outcomes of those production plans as inputs to forming their own economic plans. For the household surveys, consumers were asked about how they expected the general economic situation in their country to evolve during the next twelve months. This approach to the measurement of expectations has been formalized as “tailored expectations,” since the questions are tailored to the specific decisions and contexts faced by firms, not the economy-wide expectation (Curtin, 2019). When aggregated, tailored expectations sum to the national totals, even if the means for various subgroups show significant and persistent differences. Rather than a sign of irrationality, differences in mean expectations simply represent the differences in unemployment expectations between low and high skill workers, across different regions, and so forth. Importantly, despite these mean differences, time-series correlations across subgroups are quite high, indicating that the expectations of each subgroup moves in tandem across business cycles (Curtin, 2019). As a preliminary step in the analysis, the co-movements in expectations of business firms and households within each country were examined. The correlations indicate a substantial overlap between the responses of firms and households for both employment expectations as well as the anticipated rate of growth in the national economies (see Tables 2 and 3). This overlap was observed despite significant differences in the wording of the questions: the firm question asks about expected employment, and households were asked about expected unemployment; the firm question has a three month time horizon, and the household question covers the next 12 months. For these analyses, the responses to the business question on employment were inverted so all correlation would be positive. The correlations for the questions on employment expectations indicate a close correspondence of results for firms and households. The range of correlations across the four types of firms was quite small, ranging from a low of 0.612 for retail firms to 0.675 for industrial firms. Across the 28 countries just one country for each firm type had correlations of less than 0.3, and only 7 countries had correlations less than 0.5. In contrast, the correlations for the questions on economic activity differed to a greater extent than for unemployment. The range of correlations by firm type for overall production expectations was quite small, except for construction whose average was 0.417; the other three correlations were in the tight range of 0.537 for industrial firms to 0.577 for service firms. The distinctive industry was construction, as 10 of 28 countries had correlations of less than 0.3 and 16 countries had correlations of less than 0.5. It should be noted that only Finland had three of the four correlations below 0.3, and four countries had two of the four correlation below 0.3: Bulgaria, Italy, Malta, and the Slovak Republic. It is tempting to conclude that these much lower correlations were due to question wording differences that fit some combinations of country and firm types better than others. That judgement would be premature before the evidence of forecast accuracy is taken into account. The purpose of examining the co-movements was to determine whether a significant overlap exists that could form the basis of comparisons of the relative accuracy of firm versus household economic expectations. A sufficient overlap was indeed found. This finding indicates that for the vast majority of instances, the commonality across firms and households indicated that both groups had focused on the same expectations. To be sure, the association was only moderately high, with the possibility that other differential factors were at play in determining the expectations of households and firms. How to explain the significant overlap given the differences in time horizons? The pertinent hypotheses are that households based their answers on a shorter time horizon than a year, or firms effectively used a longer time horizon than three months, or both errors were present acting to narrow the difference in time horizons. Similar to distributed lag models, differential weights may be involved. Households could have based their answers on declining weights, with the next few months having a much higher impact, and firms could have anticipated that the same rate would persist for a longer period of time. Another hypothesis is that neither group forms such precisely dated expectations, effectively using the same short-term time horizon. Whatever the source, it would be anticipated that the more likely source of error lies with the household data. Although the statistical procedure is usually termed “Granger causality tests,” that is a misleading description. The test can only indicate whether changes in one variable significantly leads changes in another variable. Granger tests were performed for each of the 28 countries, and each of the four types of firms to determine whether the firm’s expectations preceded the household’s expectations, household’s expectations preceded firm’s expectations, or whether each simultaneously influenced the other’s expectations. The rational expectations theory would hold that both firms and households would form accurate expectations, with each basing their revisions on the same data at the same time. This hypothesis would result in a Granger finding of simultaneous causation. As already noted, some theorists would give firms the advantage since they have greater access to the appropriate data, are more experienced in calculating expectations, more quickly learn from their past errors, and can more easily afford computer programs or consultants to accomplish this task. In contrast, since households are commonly viewed to possess none of these skills, the best that they can accomplish is to simply adopt their economic expectations from media reports based on the forecasts of firms, financial institutions, government agencies, or consulting firm (Carroll, 2003). These theories would imply that business expectations would lead household expectations. The alternative view that changes in household expectations would significantly lead changes in firm expectations has few adherents, despite the common tenet of business practice that encourages firms to follow the lead of their customers. The dismissal is based on the evidence that consumers lack knowledge of the economic data produced by statistical agencies, even on such common topics of inflation and unemployment. I have developed an alternative theory that holds households base their economic expectations on their own personal experiences, and have developed extensive social networks of people that face the same economic circumstance which provide them with early warnings of potential economic changes (Curtin, 2019). Based on this alternative hypothesis implies that household expectations at least change simultaneously with firm’s expectations. The results of the Granger tests for the expectations on unemployment and overall economic activity are shown in Tables 4 and 6. Each table only includes the significance levels if they were less than or equal to 0.05; otherwise is simply indicates “ns” for not significant. A three month lag was included for both variables, matching the time horizon in the questions asked of firms. Since most readers are familiar with the underlying equations, they are not reproduced in favor of verbal descriptions. Importantly, the equations that generated the statistical results were identical for all countries. Such a one-size-fits-all approach was used as an exploratory tool that treats all countries equally, with the notion of tentatively identifying raw results that are inconsistent. The results for the Granger tests on the connection between the employment expectations of firms and households are shown in Table 4 and summarized in Table 5. Significant bidirectional links were reported for 11 out of 28 countries for industrial firms, with bidirectional links among other types of firms ranging from just 3 to 5 firms. On the issue of unidirectional leads, the data provide a clear indication that household’s unemployment expectations lead firm’s expectations for construction (13), services (14), and retail (18) firms. Firm expectations showed many few unidirectional links than households, for construction (1), services (3) or retail (1) firms. Industrial firms had an equal number of countries showing links in both directions (6). Combining simultaneous and unidirectional links indicates that households influenced firm expectations in 17 to 21 out of 28 countries. The comparable data for firms was a comparable 17 firms for industrial firms but for the other three firm types is was between just 4 and 8 firms. The analysis confirmed the notion that the expectations of firms and households were significantly interrelated for most countries; just 5 to 9 countries recorded no statistical relationship between the expectations of households and firms. Perhaps even more surprising was that household expectations had a significant advantage in leading firm’s employment expectations. These exploratory results hardly represent a final verdict; even after the predictive ability of household and firm data is discussed, the results will still be tentative. Table 6 shows the results for comparable Grange tests on expectations for economic growth among households and firms, while Table 7 includes the summary statistics for the tests on GDP across 28 EU countries. Note that the overall findings of this analysis closely mirrored the data on employment expectations. For GDP output expectations, industrial firms garnered the most bidirectional results (9), and the household data was found to have a large unidirectional advantage over firm data for construction (13 vs 3 countries), services (13 vs 1) and retail (16 vs none). When the simultaneous and unidirectional links were summed, the households held a slight edge for industrial firms (16 vs 15 countries), and a substantial advantage for construction (16 vs 6 countries), services (20 vs 8), and retail (21 vs 5). No relationship between household and firm GDP expectations was found for between 6 (industrial) and 9 (construction) firms. The substantial matching of the results for expectations about prospects for (un)employment and GDP growth rates indicated at least a partial measurement invariance of these expectations when judged by time-series correlations (as opposed to mean expectations). Nonetheless, the data also suggested that some aspect of the measurement methodologies did not provide sufficient harmonization. The potential to identify and improve measurement procedures is an important outcome of any exploratory research. The final judgement about the measurement methodology depends on an assessment of the comparative accuracy of these economic expectations. Expectations are typically judged by economists by the accurately that they can predict the actual trends in the corresponding economic series. Since GDP was only available on a quarterly basis for all countries, a quarterly format was used to test the prediction power of unemployment expectations as well. All regressions included one lag of the dependent variable as well as one lag of the independent variables to correct for serial correlation. As with the prior analysis, the widely known format of the equations are omitted in favor of a verbal description of the results. The same analytic methods were used for all countries, accepting that it may mask the presence or absence of some underlying relationships in the 140 individual equations across countries and types of firms. For this analysis, the differences in question wording, clearly favored firms: the prediction test was one-quarter in advance, consistent with the three month horizon used for the firm surveys, while household surveys were asked about the following twelve months. Moreover, the minimum prediction horizon of one quarter was selected so as not to set too high a bar. The purpose of the exploratory research is to identify an inability to predict at all rather than the ability to predict over an extended time horizon. The use of national data for accuracy comparisons may bias the results since not all industries share the same timing or impact from changes in GDP, just as not all households share the timing or impact from changes in unemployment. This would be a fruitful area of future research. Nonetheless, the goal of the EU surveys is the prediction of aggregate GDP and unemployment. Heterogeneity across industries and population groups is also an important topic for public policies, and data and models need to be develop to test for differences from the overall national trends. The coefficient estimates may be subject to aggregation bias in the sense that the relationship at the aggregate time-series level may be inconsistent with the relationship at the individual cross-section level. The goal of this analysis, however, is to determine the relationship of the survey expectations aggregated by country with the aggregated (un)employment rate and GDP rate of growth. If the accuracy of each individual respondent in each country were the main issue, the analysis would need to be based on the time-series of cross-section observations. This difference in research designs was debated by George Katona and James Tobin in the 1950s, and revolved around the claim that unless the relationship was confirmed at the individual level, any positive macro relationship was inadmissible evidence (Katona, 1957). This acted to establish the dangers of the aggregation bias by the oft repeated stricture that macroeconomic theory must be consistent with microeconomic evidence, the so-called micro basis of macroeconomics. The purpose of the EU expectation surveys is not theory but the more practical needs of government policies required to maintain full employment and stable prices. The need for such supplemental information is due to the absence of any theoretical rationale for recessions, other than the impact of exogenous forces. The results of the regressions predicting each countries unemployment rate are shown in Table 8, with the results summarized in Table 9, while the comparable results for predicting each country’s GDP are contained in Tables 10 and 11. The expectations of firms and households are shown to be significant predictors most country’s respective unemployment rate (see Table 9). Households held the predictive advantage over surveys of all four types of firms: households were more likely to be the sole predictor of the national unemployment rate rather than firms for each of industrial (11 vs 4 countries), construction (11 vs 3), services (17 vs 3), and retail firms (13 vs 2). The second most common finding was that both households and firms significantly added to the prediction of the national unemployment rate: industrial (12 countries), construction (12), service (7), and retail (10). This finding underscored the fact that both firms and household had independent and significant information that could significantly predict the national unemployment rate. At the other extreme, neither the expectations of households or firms significantly predicted developments in unemployment in just one country for the industrial and construction samples (Malta), one country for services (Denmark), and for three countries for retail (Denmark, Malta, and Romania). These three countries certainly deserve a more detailed examination to determine the potential causes and corrections. The dominance of household data in predicting unemployment was impressive: household expectations displayed a significant relationship for all four firm types in 18 of 28 countries. In contrast, for only three countries were the expectations of firms significant across all four types (Germany, Estonia, and Turkey). The most dismal showing was by just one country whose household expectations were never significant for all types of firms (Malta), and three countries where firm expectations were never significant (Austria, Belgium, and Sweden). A significance firm expectations were found in just one of four firm types in 10 countries (Denmark, France, Croatia, Hungary, Italy, Lithuania, Malta, Poland, Romania, and Slovenia), but for just one country for the household expectations (Turkey). In a similar fashion to the results on the predictions of the national unemployment rates, the regressions predicting each country’s annual GDP growth rates were also dominated by household expectations. Indeed, household expectations proved to be the sole significant predictor of GDP for between 12 and 15 countries for construction, services, and retail firms, whereas many fewer firms, just 1 to 4, were the sole predictor of GDP. Industrial firms again represented a different pattern, recording a much closer split between household and business surveys: for 8 countries households were the sole predictor of GDP and for 6 countries the expectations of firms were the sole predictors. The record of predicting GDP was on balance quite impressive, with households, firms, or both demonstrating significant relationships: for industrial and construction firms it totaled 22 of 28 firms, 19 for services, and 20 for the retail sample. Nonetheless, there was a somewhat greater number of countries in the service and retail industries that were unable to provide significant predictions, between 8 and 9 countries. The prediction record was much less impressive for the firm surveys. There was no country whose firm surveys demonstrated a significant predictive relationship with GDP in all four firm samples. Even a significant prediction of GDP for three of the four firm samples was found for just 2 firms (Czechia and Spain). In stark comparison, household expectations proved to be a significant predictor of GDP four all four samples in 13 countries (Austria, Bulgaria, Czechia, Spain, Finland, Hungary, Ireland, Italy, Lithuania, Netherlands, Portugal, Sweden, and the United Kingdom). No significant relationship between firm expectations and GDP growth was found for six countries (Bulgaria, Cyprus, Greece, Ireland, Netherlands, and Portugal). While no relationship between household data and GDP was found for seven countries (Belgium, Germany, Greece, Croatia, Romania, Slovenia, and Turkey). The worst showing was for Greece, which recorded no significant predictive relationship for all four industries across both household and firm samples. An obvious limitation of this research design was that the official statistics on jobs and GDP were not measured separately for each of the four different types of firms. To be sure, national rates sometimes hide substantial divergences across the selected types of firms. These differences in levels, however, do not imply differences in time-series correlations. Indeed, empirical analyses have found extensive and persistence differences in unemployment levels among subgroups defined by skill and experience, but nonetheless have also found extremely high time-series correlations (Curtin, 2019). It should be no surprise that nearly all subgroups rise or fall in tandem when an economy is expanding or contracting.",

Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
43,1,Journal of Industrial and Business Economics,25 January 2016,https://link.springer.com/article/10.1007/s40812-016-0026-9,"Resources (mis)allocation, innovation and the competitiveness of Europe",March 2016,Chiara Criscuolo,Angelo Secchi,,Female,Male,Unknown,Mix,,
43,1,Journal of Industrial and Business Economics,13 January 2016,https://link.springer.com/article/10.1007/s40812-015-0024-3,R&D as an investment in knowledge based capital,March 2016,Albert N. Link,Christopher A. Swann,,Male,Male,Unknown,Male,"A recent report issued by the OECD (2013b) documents clearly the importance of knowledge based capital (KBC) as a driver of economic growth and development. According to OECD (p. 22): Knowledge-based capital comprises a variety of assets. These assets create future benefits for firms but, unlike machines, equipment, vehicles and structures, they are not physical. This non-tangible form of capital is, increasingly, the largest form of business investment and a key contributor to growth in advanced economies. The OECD report also acknowledges (p. 17) that firm investments in KBC help boost growth and productivity and transform the resources used to make firms competitive. Countries that invest more in KBC are more effective in reallocating resources to innovative activities, and that reallocation brings about an increase in their market value. Corrado et al. (2005) emphasized that knowledge acquired through scientific research and development (R&D) and subsequent creative activities are important components of KBC. There is significant evidence that private sector investments in R&D are an important driver of economic growth and development, and R&D is one element of KBC that can be enhanced through public policy.Footnote 1 Corrado et al. (2005) define three categories of assets that make up KBC and create future benefits for firms. They are computerized information (e.g., software, databases); innovative property (e.g., R&D, patents); and economic competencies (e.g., firms-specific human capital, networks).Footnote 2
 In this paper we examine one dimension of KBC, namely investments in in-house R&D. More specifically, we examine the importance of investments in in-house R&D for exploring new business opportunities. Our approach to understanding the R&D component of KBC begins at the entrepreneurial level; our approach to formulating policy recommendations for enhancing that component begins with a theoretical and empirical analysis of covariates with R&D. Identifying and understanding these covariates might identify potentially relevant policy target variables, and those target variables might engender relevant policy recommendations. We view reliance on R&D as an indicator of entrepreneurial action; investments in R&D are a purposeful activity by firms to enhance the creative dimensions of their economic growth strategy.Footnote 3 And, following the theoretical arguments of Lazear (2005) and Leyden and Link (2015), it is the human capital within a firm that is a critical determinant of its effort to pursue such action. From a methodological perspective, we rely on a rich and unique database related to knowledge intensive entrepreneurial firms to examine empirically the relationship between human capital and the importance of investments in R&D activity for exploring new business opportunities. We are aware that investments in R&D are not the only component of KBC and that the results from such investments are multifaceted in the sense that technical knowledge is not only a basis for new business opportunities, but also it leverages a firm’s ability to be competitive. That said, the empirical evidence nevertheless shows a strong positive cross-country relationship between annual changes in multi-factor productive and R&D investments as a percentage of GDP (OECD 2013a). However, there is a conspicuous void of empirical evidence on covariates with cross-country investments in R&D, and we argue that such evidence is needed to begin to promulgate meaningful growth policies. We attempt to begin to fill this void by identifying systematically across European countries and industrial sectors covariates with investments in R&D for the purpose of making policy recommendations for economic growth and development. In Sect. 2 we describe the AEGIS database, which is the basis for our empirical analysis. That database contains information about the importance of a firm’s in-house R&D activities for their pursuit of new business opportunities, information about the human capital embodied within each firm, and other firm characteristics. In Sect. 3 we present our analytic framework and discuss the relevant variables of interest to explain cross-firm differences in the importance of R&D activity. Descriptive information on all variables, as well as our econometric findings, are presented. We find that, among other things, human capital, measured in terms of the educational background of the firm’s founders, is a positive and statistically significant covariate with the importance of R&D. The paper concludes in Sect. 4 with a brief summary and discussion about potential policy to enhance in-house investments in R&D by enriching the human capital embodied in the firm. In particular, we build on the U.S. experience of creating and operationalizing the manufacturing extension program (MEP) to transfer public-sector knowledge to small- and medium-sized firms to help them identify, adopt, and adapt new technologies to enhance their competitiveness. Building on the MEP concept we suggest, based on our empirical findings, that European nations create infrastructures to similarly enhance the human capital of each entrepreneur and thus build his/her firm’s KBC.",11
43,1,Journal of Industrial and Business Economics,05 February 2016,https://link.springer.com/article/10.1007/s40812-015-0022-5,FDI and heterogeneous performance of European enterprises,March 2016,Valeria Gattai,Giorgia Sali,,Female,Female,Unknown,Female,"The last several decades have documented a notable increase in firms’ commitment to foreign markets, drawing researchers’ attention to the characteristics of international versus domestic enterprises. The seminal paper of Bernard and Jensen (1995) started a literature on the link between internationalization and performance at the firm-level. Contributors to this literature generally suggest that globally engaged enterprises are a minority but they outperform purely domestic players. Interestingly, this evidence holds irrespective of the year and the country of the analysis and it is robust to several measures of internationalization and performance (for a survey, see: Greenaway and Kneller 2007; Hayakawa et al. 2012; Lopez 2005; Singh 2010; Wagner 2007, 2012). While the first contributions mostly drew on US data, large longitudinal datasets have recently become available in Europe as well, triggering new academic research on the topic. Vogel and Wagner (2010), Muuls and Pisu (2009), Eaton et al. (2004), Castellani et al. (2010), Ruane and Sutherland (2005), Damijan et al. (2007), Esteve-Perez et al. (2013), Andersson et al. (2008), Breinlich and Criscuolo (2011) and Bekes et al. (2011) are just a few examples of recent papers dealing with the internationalization-performance nexus of European enterprises. Despite the large body of evidence that has accumulated over time, we believe a couple of gaps still wait to be filled. For instance, even though single country analyses are quite numerous, only a few papers take a cross-country perspective; still, they consider only a small subset of European countries (see, for instance: Barba Navaretti et al. 2011, 2012; Haller et al. 2014; ISGEP 2008; Mayer and Ottaviano 2008; Temouri et al. 2013). Moreover, performance measures rarely include economic, innovation and financial variables, adding to productivity. Lastly, the definition of internationalization tends to be quite narrow: while there are dozens of papers on trade, evidence on Foreign Direct Investment (FDI) in Europe is rather scant. In light of the above discussion, this paper empirically investigates the link between FDI and performance of European enterprises. Our data, downloaded in 2014, rely on the global company database Orbis and include 12,465 firms overall. We are not aware of any previous study on the topic drawing on the same data. To address the above-mentioned gaps, we hazard three main departures from the existing studies. First, we consider Europe as a whole, to exploit country- adding to industry- and firm-level heterogeneity. Second, we do not restrict attention to productivity, but rather measure performance through a wide array of economic, innovation and financial variables. Third, and most importantly, we focus on FDI, rather than trade, and we introduce a sophisticated taxonomy of foreign direct investment that accounts for both the FDI direction—inward versus outward—and the number of FDI. Although inward FDI and outward FDI have been previously studied in the internationalization-performance literature, we are not aware of any single study integrating them in a unified empirical framework. Our descriptive statistics document that the FDI experience is quite notable throughout Europe, involving the majority of the sampled firms. Our estimates further suggest that there are systematic performance differences among firms with heterogeneous FDI involvement. Interestingly, firms experiencing some FDI enjoy a superior performance compared with purely domestic enterprises. Moreover, within the FDI class, the deeper the firm’s involvement in FDI, the larger the performance difference with purely domestic enterprises. These results are consistent through several econometric models, performance measures and definitions of FDI; moreover, they are robust to many alternative specifications including firm, industry and country controls. The remainder of the paper is structured as follows. Section 2 provides a brief literature review. Section 3 is devoted to the data description. Section 4 introduces our taxonomy of FDI and provides some descriptive statistics. Section 5 describes the econometric analysis. Section 6 concludes and sets forth future lines of research.",3
43,1,Journal of Industrial and Business Economics,15 January 2016,https://link.springer.com/article/10.1007/s40812-015-0021-6,"Ownership, productivity and firm survival in China",March 2016,David Audretsch,Xiaodan Guo,Xingzhi Xiao,Male,Unknown,Unknown,Male,"Firm survival has been discussed for more than 30 years in the field of industrial organization, and much research concerns the entry and exit of firms (Geroski 1995). When some important stylized facts and rules are applied, the econometric methods used in this field have become more sophisticated (Manjon-Antolin and Arauzo-Carod 2008), and the results have become more consistent (Audretsch et al. 2000). However, several important issues remain unresolved by the literature. In particular, previous studies in the literature remain ambiguous whether the extant findings concerning firm survival are dependent upon the institutions specific to a particular country. Since most research on survival pertains to either developed countries or developing countries with established market economies, the results are consistent across samples, areas, and periods. However, virtually nothing is known about firm survival in the context of a non-market economy or in the institutional context of a communist political system. The purpose of this paper is to provide the first study analyzing firm survival in the context of a non-market economy—China. In transitional countries, such as those in East Asia as well as Central and Eastern Europe, the factors, patterns and path of development are evolving from a planned economy to a market economy, and have complex dual characteristics. In the period of economic transition, private ownership, public ownership, and market economy coexist. This is contradictory to the traditional context of a market economy, which is based on private ownership. In the case of this kind of coexistence, ownership has different meanings and implications. There are state-owned (or state-controlled), private-owned (including stock company), and foreign-owned variations. Ownership in a market economy usually refers to the single-establishment and multi-establishment firms, or the domestic and foreign firms. This is the major difference in research analyzing the role of ownership and survival issues in transitional economies. In a market economy, only inputs and outputs of private production are discussed, but in a transitional economy the influence of political factors cannot be ignored. The relationship between public resources and private goods should also be taken into consideration. Using private-owned firms as reference, we want to discover the special characteristics of state-owned and foreign-owned firms. We also focus on whether market selection works under government control in China and, if so, how efficient those firms are in both the short-term and long-term. It should be considered that if the influence of market selection becomes stronger, then the transitional process will be successful to some extent. At the same time, certain firm and industry characteristics, such as size, innovation, exports, industry growth and so on, could exert different influences in a transitional economy. This paper offers two important contributions to the literature. The first is to analyze if those factors that have been found in a plethora of studies influencing firm survival in the context of a market economy still hold under a very different institutional context of communism. The second contribution of this paper is to explicitly link the relationship between government ownership of firms to survival, which enables us to identify how government ownership of firms influences the likelihood of firm survival. The organization of the paper is as follows. Section 2 briefly reviews the literature on ownership, productivity and firm survival. Section 3 describes the data and method we employed. Section 4 presents the findings of the paper and provides discussion. Section 5 concludes.",17
43,1,Journal of Industrial and Business Economics,08 February 2016,https://link.springer.com/article/10.1007/s40812-015-0023-4,The impact of patenting on the size of high-tech firms: the role of venture capital and product market regulation,March 2016,Massimo G. Colombo,Kourosh Shafi,,Male,Male,Unknown,Male,"The innovation economics literature suggests that returns to innovation depend on the ability of innovating firms to attract the complementary resources they need to exploit commercially their innovative knowledge (Teece 1986). Hence, the ease with which these resources are accessible to innovative firms is extremely important and is partly a function of the public policy and the regulatory environment (Restuccia and Rogerson 2008; Arnold et al. 2011; Bartelsman et al. 2013). This is so because research indicates that flows of labor and capital resources to patenting firms across countries depend on regulations governing product and labor markets (Andrews et al. 2014). Given the potential role of public policy to influence the ability of national economies to reallocate resources towards innovative firms over time and to facilitate commercialization of new ideas, this paper builds on prior work of Andrews et al. (2014) and investigate the extent to which product market regulations (PMR) influence the ability of young firms to grow in size in proportion to their patenting activity. Patenting firms grow in size (Balasubramanian and Sivadasan 2011), but they are less able to do so in a policy unfriendly environment (Andrews et al. 2014). The unresolved question remains whether young and innovative firms can devise strategies to overcome regulations that make competition more difficult and impede their innovation-driven growth. This question deserves attention because young firms are more sensitive to these regulatory burdens compared to old firms (Andrews et al. 2014) and are viewed as an important contributor to job creation and economic growth (Haltiwanger et al. 2013; Criscuolo et al. 2014). In this paper, we ask whether obtaining resources from venture capital (VC) investors offers a counter-force and thus, a possible strategic solution to innovative firms to overcome growth-related impediments from PMR. VC investors not only provide substantial financial resources to their portfolio firms; but they also coach portfolio firms in areas where these firms lack expertise (e.g., Gorman and Sahlman 1989), and provide quality signals (Megginson and Weiss 1991) and business contacts to portfolio firms, making it easier for them to obtain additional resources. Whereas research indicates that VC-backing has a positive effect on firms’ performance in terms of innovation (Kortum and Lerner 2000), growth (Puri and Zarutskie 2012), productivity (Chemmanur et al. 2011; Croce et al. 2013), and the likelihood of going public (Chemmanur et al. 2010; Puri and Zarutskie 2012; Cumming et al. 2014), it is to the best of our knowledge unexplored whether VC-backed firms are less affected compared to non-VC-backed firms by PMR. Whether VC-backed firms are better able to cope with regulatory burdens of PMR in attracting resources to grow in proportion to their innovation activity is not as straightforward as one might assume. As noted above, the support provided by VC investors to portfolio firms in attracting valuable resources may also help them overcome policy and regulatory constraints. However, VC-backed firms have also the greatest growth potential and therefore they might suffer disproportionately from these constraints. Indirect evidence in line with this latter argument comes from studies inspired by institutional theory according to which related differences across countries in the functioning of the VC market is tied to the existence of an unfavorable policy and regulatory environment (e.g., Bruton et al. 2005; Armour and Cumming 2006). In line with our research question, we explore the moderating role of PMR in the realization of returns to innovation for VC-backed firms. We focus on the country-level regulations of product market because stringent PMR raise the costs of inputs and/or lower the quality of these inputs for firms (Arnold et al. 2011). These regulations also are likely to increase the failure costs associated with experimentation of new technologies by young high-tech firms, and reduce the returns to innovation. In turn, these higher costs influence the propensity of young high-tech firms to invest in the development of high risk, radical innovations and put constraints on their ability to exploit the business opportunities potentially created by their innovation activity. There are a few noteworthy remarks with respect to our operationalization of this research question. First, we analyze the sensitivity of firm size to the patent stock as moderated by PMR, and firm size represents total assets or sales. Second, we compare a sample of VC-backed firms with a matched sample composed of twin non-VC-backed firms. Third, we also perform a fine-grained analysis of indicators of PMR, distinguishing the effects of restriction to competition through barriers to trade and international investment (hereafter, BTI), barriers to entrepreneurship (hereafter, BTE), and the extent of state control of enterprises (STC). Data for this analysis are obtained from the VICO database, a large-scale longitudinal dataset on young high-tech firms located in seven European countries that was created by the 7th FP VICO project (http://www.vicoproject.org/). Results of our econometric estimates show that both VC-backed and non-VC-backed young high-tech firms suffer from a regulatory environment that hinders competition. There is a decrease of the sensitivity of sales and total assets to the patent stock with increasing product market regulatory burdens for both set of firms; the magnitude of this negative effect is stronger for VC-backed firms relative to the control sample of non-VC-backed firms. More specifically, we show that stringent regulations are more disruptive for VC-backed firms’ growth in sales and total assets in all dimensions of PMR except BTI. Our results confirm that PMR policies can disproportionally affect those firms with the highest potential for growth. Therefore, we recommend lenient PMR policies, as regards notably BTE and STC.",3
43,2,Journal of Industrial and Business Economics,20 February 2016,https://link.springer.com/article/10.1007/s40812-016-0030-0,Public engagement in electricity network development: the case of the Beauly–Denny project in Scotland,June 2016,Wenche Tobiasson,Christina Beestermöller,Tooraj Jamasb,Female,Female,Unknown,Female,"The future of energy networks holds important technical, economic, and social challenges. Across Europe, the electricity grids are in need of modernisation to support the ongoing development of low carbon energy systems. In order to connect the large number of emerging renewable energy plants and integration of energy markets the networks need to expand. This holds across both distribution and transmission networks.Footnote 1 Meanwhile, larger development projects such as expansion of transmission networks tend to dominate the public debate, much due to their greater impact in terms of national importance, economic costs, environmental impacts, as well as their negative effect on neighbouring communities. Network developers face a number of constraints which may extend the planning process and delay a project—considering alternative project designs, negotiations with the regulator to justify the need case and cost efficiency, environmental constraints which will have to be considered, or a change in government policy—are a few of the potential issues. The focus of the present paper is on public and community opposition and, although not a new issue, it is proving to be an increasingly important aspect in planning and development of grid projects. Objections to large projects often relate to environmental, visual and health aspects (Soini et al. 2011), particularly from communities in close proximity to a planned development. Failure to agree between stakeholders leads to costly delays or even causes projects to be abandoned altogether. As a result, the potential for reaching the targets set for reducing carbon emissions and climate change, thus a green energy future, is in jeopardy (European Commission 2008). Grid development projects tend to affect a number of stakeholders—from state and local communities to NGOs, landowners and corporations—each with different objectives and perceptions of the project and surrounding matters. The existing decision-making processes and institutions have proven ineffective at resettling conflicts that appear between stakeholders, causing uncertainty and delays. Increased information provision and public engagement in transmission grid planning can increase public trust in network companies, public acceptance and therefore accelerate the realisation of new grid developments (RGI 2012; Newig and Kvarda 2012; Cotton and Devine-Wright 2010). The Aarhus Convention (European Community signed and implemented in 1998 and 2003 respectively) advocates early and effective public participation to increase the transparency of the planning and decision-making process. Public engagement implies the involvement of members of the public in policy-forming and policy development. The concept is not new but is becoming increasingly important in infrastructural developments. For example, a recent United Nations legal tribunal found that the UK government had failed to provide sufficient information and decision-making powers to the public regarding two major wind developments (UN 2014). However, despite the pressure through the Aarhus Convention there are no established guidelines, rules or frameworks defining how public participation ought to be formalised. Recent high-profile projects, such as the Norwegian Hardanger line and the Scottish Beauly–Denny line,Footnote 2 show that transmission projects increasingly involve vested social, economic and political interests. Noticeably, there is a need for new approaches for defining and organising the role and tasks of the actors, including the public and affected communities. Exploring project characteristics and stakeholder relations using an economic approach is previously untested but it can potentially generate several efficiency improvements. Based on the seminal works by Coase (1937) and Williamson (1979) on the role of institutions, rules, and norms, this paper analyses economic characteristics of transmission projects following an Institutional Economics approach. Additionally, using the contentious Beauly–Denny High Voltage Transmission Line project and input from previous literature, this paper outlines how public engagement may be approached to allow for a more efficient planning process. The paper is outlined as follows: Sect. 2 outlines the analytical framework and the economic characteristics of transmission developments and public engagement. Section 3 presents and discusses the case of the development of the Beauly–Denny transmission line. Section 4 concludes.",5
43,2,Journal of Industrial and Business Economics,25 January 2016,https://link.springer.com/article/10.1007/s40812-016-0028-7,CEO incentives in European energy utilities: evidence from regulated versus unregulated firms,June 2016,Carlo Cambini,Sara De Masi,Laura Rondi,Male,Female,Female,Mix,,
43,2,Journal of Industrial and Business Economics,23 April 2016,https://link.springer.com/article/10.1007/s40812-016-0032-y,Regulatory biases under local partial privatization,June 2016,Margherita Boggio,,,Female,Unknown,Unknown,Female,"Starting from the UK in the 1980s, countries all over the world (Bortolotti and Milella 2006) began privatizing many local services in the name of rationalizing and reducing public debt and expenditure (Vickers
and Yarrow 1991; Bortolotti et al. 2003), as well as increasing efficiency. However, governments still control -through voting rights and/or golden shares- a large portion of these privatized firms, particularly in strategic sectors such as utilities and transportation (Boubakri et al. 2009) which are characterized by bigger and more valuable companies. Bortolotti and Faccio (2009) show that not only have governmental shares not changed over time, but also that this phenomenon—defined as reluctant privatization—does not seem to have decreased the firms’ market value nor their performance. Different levels of government are involved: national governments, governmental agencies, and also local authorities. This is particularly true at the municipal level, with the local authority acting as the main shareholder in the majority of cases (Boubakri et al.2005). This could be due to the fact that the firms operating in some of these sectors (i.e. gas, electricity) are highly profitable; hence, local administrators can use them to counterbalance the negative results of firms operating in other (costly) sectors, or to increase their revenues through dividends, using them for other purposes, helping the local government to somehow elude budget constraints imposed by the central governmentFootnote 1. A recent research on the firms participated by the most important Italian local governments (Mediobanca 2014) highlights that in the 2003–2012 period local governments, as shareholders, have benefited from 2368 millions of euro from dividends, net of placement revenues, from the listed firms. The dividend yield is generally higher than that of a Government Bond. The capitalization of these firms at the beginning 2014 was nearly 9.3 billions of euro, representing around 3 % of the total value of listed industrial companies. Nonetheless, given that one of the key objectives of privatization is to make profit maximization the sole objective of the firm while also eliminating the political ones that typically characterize State Owned Enterprises (Shleifer and Vishny 1994; Boycko et al. 1996), the presence of this residual governmental control can create contrasting objectives. Parallel to the privatization wave, the European Commission started to promote independent regulation for public utilities, once again to improve their efficiency, and to increase quality and investments. Regulation has been gradually transferred from governmental branches to independent regulatory agencies, with varying degrees depending on sectors and industries. Liberalization for the telecommunication sector started with Directive 90/388, while Directive 96/92 and Directive 98/30 aimed at the electricity and gas market, respectively. In sectors such as water and local transport, liberalization is facing more resistance. In Italy, even if an independent regulatory agency has been set up for each of those sectors, local agencies still decide upon many regulatory issues. In local public transport, prices and quality are still a local matter. This gives rise to a twofold issue. The fact that municipal governments still retain high shares in the firms they should have privatized and that they in many cases still regulate, decreases not only the transparency of their relation with the firm’s manager (since they can distribute rents across players and also have better access to the manager’s information), but also the transparency to their electorate, since the local government’s political orientation can change the effect on outcomes, depending whether the political agenda is aligned or not with its objectives as a shareholder. The aim of this paper is to analyze how ownership structure and the level of regulatory decentralization interact when a local government is a member of the shareholders board in the regulated firm. For this purpose, we will approach a very common problem in regulation: the choice of the cost reimbursement rule under incomplete information regarding the efficiency of the firm. Our main results can be summarized as follows. Under asymmetric information, an important role is played by the social costs of public funds and by the multiprincipal problem. However, when regulation is decentralized, the double role of the local government generates a positive externality, mitigating the distortions generated by the incentive scheme. The rest of the paper is organized as follows. Section 2 gives an overview of the literature. In Sect. 3, the basic model is presented. In Sect. 4, the model will be extended allowing for a non-benevolent government. Section 5 draws the conclusions.",
43,2,Journal of Industrial and Business Economics,10 March 2016,https://link.springer.com/article/10.1007/s40812-016-0031-z,Multi-unit franchising strategies: a real options logic,June 2016,Francesco Baldi,,,Male,Unknown,Unknown,Male,"Franchising is an organizational form based on a contractual arrangement between two legally independent firms in which one party (franchisor) grants the other (franchisee) the right to sell a product or service developed and owned by the former using her business format (e.g., brand name, process) in a given location for a specified period of time in return for a lump sum payment and an annual, sales-based royalty fee (Rubin 1978; Hoffman and Preble 1993; Shane 1996a; Blair and Lafontaine 2011). Franchising is a proven business strategy worldwide. It has a multiplier effect in terms of enterprise creation, job generation and contribution to GDP growth. Indeed, about 30,000 franchise systems operate globally generating at least 2 million firms (World Franchise Council 2013) and accounting for almost 10–25 % of the GDP of most OECD countries. The U.S. franchising industry,Footnote 1 where a new franchise business opens approximately every 5–8 min of each business day, grows twice as fast as the rest of the economy since 2010 (3 % or $ 472 billion of GDP as of 2013, with a 4.5 % growth expected in 2014).Footnote 2 Besides fuelling entrepreneurial opportunities under favorable circumstances, it has also shown resilience throughout the recent recessions (e.g., the downturn of 2008–2009) maintaining job growth and minimizing losses.Footnote 3
 Research work on franchising, spanning various fields (including economics, strategy and marketing) and aspects (e.g., bargaining power, governance) (see for all Yin and Zajac 2004), can be classified into three main streams: (a) resource scarcity (RS); (b) administrative efficiency (AE); (c) risk management (RM). According to their combined interpretation of this phenomenon, the franchisor leverages capital of others, more motivated outlet owners and a portfolio of owned vs. franchised units properly allocated depending on the riskiness of locality.Footnote 4
 Growth and expansion go to the very essence of franchising.Footnote 5 Extant research has shown that firms that have created an easily replicable business model typically face two strategic alternatives to rapidly expand their operations and leverage their brand so as to reach new geographical or product markets with the aim of improving or preserving performance and competitive advantage. The first is the opening of new company-owned stores through which managers pre-commit to network expansion today passively exposing their firm to the future outcomes of such decision. The second entails pursuing a multiple (or multi-unit) franchise strategy, wherein the franchisor gives franchisees the right to own several outlets.Footnote 6
 Four main theories have been so far applied to help explain franchising: agency theory (AT), transaction cost economics (TCE), property rights (PR), resource-based view (RBV). Franchising typically occurs when: (1) it involves a trade-off between the costs of monitoring outlet managers and the cost of free riding (AT—Rubin 1978; Lafontaine 1992a); (2) parties make investments in specific assets that bond them together and align their interests in anticipation of future profit streams (TCE—Klein 1995); (3) franchisors are exposed to incomplete contracts due to the presence of non-contractible assets offered by franchisees (PR—Windsperger and Dant 2006); (4) centralized and local activities that can be loosely coordinated are combined to enhance value creation (RBV—Combs et al. 2004). Some take a franchisor’s perspective (AT, PR), others a franchisee’s (TCE, RBV). What such theories struggle to enlighten are operational extensions of franchising (e.g., multi-unit, plural form
Footnote 7) and their connectedness to the growth opportunities pursued by the parent company via establishment of a franchise system. Recent attempts exploit agency theory (Perryman and Combs 2012). In this domain, however, the corporate decision to grow via multiple franchising has not been analyzed with a view to accounting for how the flexibility to franchise or not (vis-à-vis the rigidity of investing into new own outlets), as uncertain market circumstances warrant, can drive performance. Indeed, to the best of our knowledge, no prior research has applied the real options logic to franchising.Footnote 8 In this study, we seek to fill the gap by proposing a theoretical framework and empirically investigating about the real options that underlie multi-unit franchise strategies. Our study makes three key contributions to the franchising literature. First, we combine the real options perspective with an organizational mechanism predominantly used by firms such as franchising advancing an options-based classification of multi-unit franchise strategies in an effort to better explicate franchising and its performance consequences. In this regard, we offer an integrative tool helping managers to identify and evaluate franchise investment opportunities that need to be made under uncertainty based on growth acceleration potential, network control features and type/degree of embedded managerial flexibility. Second, our analysis provides evidence drawn from the U.S. franchising industry that, besides supporting classical findings on franchising, highlights the key source of extra value brought in by optionality associated with multi-unit arrangements and their impact on network performance, which has not received attention in prior empirical research. Finally, we respond to the recent call for researchers to deliver complementary insights into what makes franchising work using alternative theories other than economic-based (e.g., agency theory), strategy-based (e.g., resource-based view) and social-psychological (e.g., relational marketing) approaches (Nijmeijer et al. 2014). To this end, the application of real options theory borrowed from finance in the context of franchising contributes to that “greater theoretical diversity” pointed out by Combs et al. (2004). This article proceeds as follows. Section 2 provides an overview of multiple franchising techniques. Section 3 develops the real options framework. Section 4 formulates the hypotheses grounded in the classical as well as the real options theory (ROT) of franchising. Section 5 contains the empirical analysis. Section 6 discusses the results and related implications for researchers, managers and policy-makers leading to conclusions drawn in Sect. 7.",3
43,2,Journal of Industrial and Business Economics,14 January 2016,https://link.springer.com/article/10.1007/s40812-016-0025-x,The dark side of social media game: the addiction of social gamers,June 2016,Mehrnaz Kalhour,Jhony Choon Yeong Ng,,Female,Unknown,Unknown,Female,"We used theoretical sampling to choose our participants. As we were interested in finding the drivers of the near-irrational virtual consumption behavior of social gamers, we focused our investigation on “big spenders”. By big spenders, we refer to those who have spent at least USD 1000 on a social game. We believed that this approach could help us in getting a better view of the behaviors of social gamers who have spent a lot of money on their social games, and this approach could also help in keeping our research focused and manageable. We first interviewed several social gamers whom we know personally to get their opinions on the type of social games that had the most “big spenders”. These interviewees were selected because they were “veteran social gamers” who had played many different types of social games, and they were thus a good source of information. Based on the results of the interviews, reflecting on what we gathered from the interviewees, we zeroed-into one specific social game that they considered “money burning”. Prior to the start of the data collection process, the first author of this paper created a game account and joined the game as a social gamer. To gain the trust of the other social gamers, and to have a better understanding of the rules and norms of the game, the researcher spent about half a year playing the game before the data collection phase was initiated. This gaining of trust phase was important to the data collection phase because a person’s lack of trust in another person will result in the restriction of information flow between the interacting parties (Ng, 2013), thus limiting the amount of valuable insights that one can obtain from an interview. Social gamers of the game compete with each other by using “power”. The information on a social gamer’s power can be found easily in the game’s “information of other players” function. One can increase one’s power by completing “tasks” in the game. The power accumulated will be reduced by the “battles” that social gamers took part in. The game has a variety of tasks that social gamers can undertake (e.g. research). Each task requires a different number of “waiting time” to complete. The task that has the shortest waiting time can be “completed” 1 day after the player has initiated the task, and the task that has the longest waiting time takes more than 1000 days. An alternative to wait for more than 1000 days is to spend money to buy “gold packs”. A gold pack that costs USD 100 can cut the waiting time of a task by 200 days. That is, if a social gamer wants to complete a 1000 day task immediately, instead of waiting, that social gamer can just spend USD 500 on the game to buy five gold packs. To become stronger than the other social gamers, one just has to spend more money on the game. “Dump in the money, and you will be the hero.” Social gamers of the game can gather to form communities called “alliances”. It is a type of guild in which members can communicate with each other and take part in group tasks. We studied the behaviors of big spenders by focusing on the alliance that was made up of social gamers who had the greatest amount of power. To be qualified to join the alliance, it was necessary for a social gamer to have more than 100 million powers (costs around USD 1000). The most powerful member of the alliance had 500 million powers (costs around USD 5000). Thus, if the 500 million powers social gamer lost a battle, the social gamer would need to spend another USD 5000 to get the power back. To stay in the alliance, the members had to engage in battles actively. This increased the chances of them losing their powers. If a social gamer’s power stayed below 100 million powers for a few days, the alliance leader would kick that person out of the alliance. Thus, to remain in the alliance, a social gamer had to spend a lot of money every now and then to keep their power high, so they can continue to stay in the “prestigious group”. Qualitative data were collected using semi-structured interviews. Samples of the guiding questions that we used include: How do you find the game? In average, how much time do you spend on the game? What does power means to you? What motivates you to spend continuously on the game? How do you feel when you are defeated by the other social gamers? What would you usually do? All interviews were conducted in the game’s chat channels (cf. Balci & Ali Salah, 2015; Ross & Collister, 2014). The first strength of this approach was that it was easier for the researcher to build relations with social gamers because of the “common interest” between the researcher and social gamers (the game). The affinity that the researcher had built with them over the past few months of interactions had reduced the possibility of the social gamers giving “fake answers”, and it also had increased the willingness of social gamers to participate in this research. The second strength of this approach was that the chat channel feature of the game saved the time of the researcher to transcribe the interviews conducted because all “interviews” conducted were “typed conversations” between the researcher and social gamers. We were interested in how social gamers would behave during their “normal games”. Hence, instead of asking a social gamer to talk in the game for a long period of time, we conducted our interviews in the form of several “small-chats”. For example, we would ask one question and discuss it over 10 min or so, and then we would discuss the rest of the questions over another short period of time when we met the person again. Another reason for us to conduct our interviews in this way was to fit the context of social games. Social gamers would usually access the game for multiple short durations. For example, one social gamer might go online and complete some “tasks”, and then go offline to continue with one’s usual live. After a couple of minutes, sometimes after a few hours, when the social gamer has the free time to go online, the social gamer will then access the game again to complete some other tasks. Thus, rather than asking social gamers to talk with us for a long period of time, it was better for us to conduct our interviews over several “small-chats”. In addition, different from the other traditional forms of academic research, we did not report the data on the participants’ demographics (for e.g., gender, wealth, education, etc.), nor did we rely on these data for our analyses. We did not make use of these data because of the questionable nature of the data that we might collect. Social gamers came from virtually all parts of the world. They came from all walks of lives. The beauty of social game is everybody from everywhere can get together and play the game together without bearing the burden of one’s background. It is unusual for one to mix one’s real life with one’s avatar. That is, it is unusual for one to really tell the other social gamers where one comes from and what one does. Even if one does so, we have practically no way to confirm these data. Thus, for the sake of prudency, we chose not to rely on the demographic data of the participants for this research. We admit that our approach has limited our ability to differentiate the nuances in the participants’ behaviors. For example, we could not explain the influence of factors such as gender, wealth, and education on the behaviors of social gamers. Nevertheless, insofar as virtual consumption behavior is concerned, we believed that our approach has not affected the quality of our research.",3
43,3,Journal of Industrial and Business Economics,08 July 2016,https://link.springer.com/article/10.1007/s40812-016-0048-3,Perspectives on industrial policies in Italy and in Europe: a forum,September 2016,Mario Pianta,Antonello Zanfei,,Male,Male,Unknown,Male,,3
43,3,Journal of Industrial and Business Economics,01 July 2016,https://link.springer.com/article/10.1007/s40812-016-0047-4,Industrial policy and technology in Italy,September 2016,Matteo Lucchese,Leopoldo Nascia,Mario Pianta,Male,Male,Male,Male,"Industrial policy is a theme of continuing research activity by the authors (Pianta 1996, 2014; Lucchese and Pianta 2014, Pianta et al. 2016; Nascia and Pianta 2014, 2015). Ideas have been presented at Industrial Policy workshops at Sapienza University of Rome (May 2014, June 2014, May 2015, May 2016) and at a seminar at WIIW in Vienna (May 2014). This article is produced as part of the ISIGrowth project on Innovation-fuelled, Sustainable, Inclusive Growth that has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 649186—ISIGrowth. This article does not necessarily reflect the view of the affiliating institutions of the authors. The crisis started in 2008 has accelerated the decline of Italy’s industry and technological capabilities and has opened up a growing divide within Europe between a ‘core’ around Germany and a weakening ‘periphery’. This article—constructed as a ‘position paper’—documents the impact of the crisis on Italy’s industry and sets it in the context of the demise of post-war government intervention. In the last two decades industrial policy has been replaced by fragmented measures with a ‘horizontal’ approach and modest resources; current initiatives appear unable to support a revival of production and investment and to reduce Italy’s gap in technological activities. Could a new industrial policy help contain this decline and lead to the emergence of new economic activities? This question appears to be increasingly relevant in Europe. As a result of the crisis, a renewed interest in industrial policy has emerged; in this context, there is a need to frame the debate on industrial policy in new terms, combining sound economic arguments, clear evidence, a consideration of the complexity of the issues and the development of feasible policy proposals. These are the goals of this article and of the contributions to the forum that follow. But, first of all, a restatement of the goals of industrial policy is needed. The economic rationale for industrial policy is that it can steer the evolution of the economy towards activities that are desirable in economic terms—improving efficiency, in social terms—addressing needs and reducing inequality, in environmental terms—assuring sustainability—and in political terms—protecting key national interests. The economic rationale includes the search for improvements in static and dynamic efficiency (especially in the cases of market failure); in coordination of decisions; in the framework conditions of economic activities. Gains in dynamic efficiency are the most important argument for industrial policy. Public policy can expand available resources, favouring the growth of firms and industries that are characterised by strong learning processes, technological change, productivity increases, scale economies, internationalisation and rapid demand growth. The resulting benefits include faster growth of production, incomes, employment and competitiveness. An important set of contributions has recently addressed the challenge to redefine industrial policy with a broadly converging perspective (see in particular Chang 1994; Hausmann and Rodrik 2003; Rodrik 2008a; Cimoli et al. 2009; Aghion et al. 2011; Dosi and Galambos, 2013; Mazzucato, 2013; Stiglitz and Lin 2013; Greenwald and Stiglitz 2013; Lundvall 2013; Aiginger 2014; Pianta 2014, Mazzucato et al. 2015).Footnote 2 Most of these contributions share the view that markets alone can fail to develop new technologies and production capacities; they argue that an active role of public policy is necessary and ‘horizontal’ measures—treating all firms and industries on the same footing—are often insufficient. Choices have to be explicitly made—in fact they are always implicitly made—on which activities have to be supported. Differently from market liberalisation measures that have been assumed to work in all contexts, there is not a unique set of appropriate policies which are applicable in all countries; conversely, there is a need in each economy to identify specific capabilities and develop appropriate institutions (Rodrik 2008b). Among these studies there is a consensus that today industrial policy is closely related to technology policy, supporting the development of knowledge, learning and innovation. Authors also generally agree that it cannot mean support for manufacturing alone—although capabilities in this field remain important for most countries’ progress. And there is also agreement on the view that policies should not target whole industries, nor be tailored—except in exceptional cases—to the needs of individual firm. Rather, policies should support sets of well-defined technological and production activities—that may be carried out by both public organisations and private firms—in the pursuit of important economic and social goals, addressing needs, assuring efficiency, protecting the environment and public health, etc. In addition, various contributions argue that policies should promote the diffusion of knowledge, learning and technologies, provide key infrastructures, support public and private investments, create appropriate institutions and forms of “bottom-up” coordination, assuring transparency, monitoring and accountability. This growing debate on the need for a new industrial policy provides the backdrop for this article that examines the case of Italy in the European context. Section 2 documents the impact of the crisis on Italian industry, Sect. 3 looks back at the evolution of Italy’s intervention in industry, Sect. 4 explores the current return of interest for European industrial policies, Sect. 5 maps the tools currently used in Italy; finally, Sect. 6 calls for a new policy direction in this field.",42
43,3,Journal of Industrial and Business Economics,15 June 2016,https://link.springer.com/article/10.1007/s40812-016-0041-x,Beyond the ‘magic of the market’. The slow return of industrial policy (but not yet in Italy),September 2016,Giovanni Dosi,,,Male,Unknown,Unknown,Male,,4
43,3,Journal of Industrial and Business Economics,24 June 2016,https://link.springer.com/article/10.1007/s40812-016-0034-9,From manufacturing nostalgia to a strategy for economic transformation,September 2016,Bengt-Åke Lundvall,,,Unknown,Unknown,Unknown,Unknown,,
43,3,Journal of Industrial and Business Economics,02 June 2016,https://link.springer.com/article/10.1007/s40812-016-0040-y,Science policy as a prerequisite of  industrial policy,September 2016,Stefano Bianchini,Patrick Llerena,,Male,Male,Unknown,Male,"After the global financial crisis in 2008, most European countries have witnessed an industrial decline and, though there are feeble signs of recovery in the Euro Area, the current stagnation is likely to persist in the coming years. The paper by Lucchese, Nascia, and Pianta well illustrates the worrisome situation of an Italian economy which has been severely hit by the crisis and is struggling to get back to its historical growth trends. The authors provide an in-depth description of the historical evolution of industrial policy in Italy, and more in general in Europe, that clearly illustrates how the general consensus about the role of the State as market “regulator” has led to a policy response which has failed to restore competitiveness and promote innovation. The alternative policy framework proposed by Lucchese, Nascia, and Pianta puts the active role of the State in the context, with a full legitimization to public actions at the country and the European level for influencing economic activities. In this view, the “entrepreneurial State” is a State which is expected not only to fix market imperfections but, in particular, to create and shape new markets by favouring the evolution of knowledge, technologies and take on board the risks associated to long-term visionary investments (Mazzucato 2015). In this respect, science and innovation are two key ingredients. Not surprisingly, along the whole article and especially in their concluding remarks, Lucchese, Nascia, and Pianta stress the importance of science and innovation as pivotal elements for sounder industrial policies. Among the proposed new directions for policy the authors put forward that “[t]he new industrial policy could finance a range of activities, possibly in combination with private investment, including R&D in universities, public and private institutions; innovation and its diffusion in private and public organisations…”. We concur with this perspective. Our comment aims to expand the discussion in this direction. The starting point of this short article is that industrial policy and dynamic efficiency have a necessary condition: the reconstruction of the knowledge base through an appropriate science policy. Here we wish to provide some reflections on two different, though interrelated, issues which Lucchese, Nascia, and Pianta touch only marginally in their review: (a) the importance of high-quality domestic science, and (b) the efficiency of the system for transfer of technology and knowledge from universities and public research organisations (PROs) to private sector. Both elements are ancillary to each model of industrial policy and crucial to fulfil the well-known priorities set within the Europe 2020 framework. In what follows, we elaborate a discussion mainly by using the French and (to a lesser extent) the Italian scenario as benchmark but we elaborate a set of conclusions which may be ad hoc for the European landscape. It will be shown that any attempt to design an effective industrial policy must deal with the following facts: most European countries do have a relatively weak system of scientific research; most European countries face difficulties to transfer results from public research to corporate actors, and probably even more the vice versa. Thus the ultimate outcome of any industrial policy action, whether or not in the vein of Lucchese, Nascia, and Pianta, deeply depends upon these structural weaknesses. We will conclude our comment with some recommendations on what, in our view, might be an appropriate science policy as a support of industrial policy.",2
43,3,Journal of Industrial and Business Economics,15 June 2016,https://link.springer.com/article/10.1007/s40812-016-0043-8,Industrial policy options for sustaining growth in Italy,September 2016,Harald Gruber,,,Male,Unknown,Unknown,Male,"The debate about industrial policy in Italy and the feasible policy options in the follow up to the recent and still unresolved financial crisis, as discussed in Lucchese et al. (2016), is timely and the provision of good answers is fundamental to preserving the high level of economic welfare in the country. Productivity growth in industry is still the key determinant of economic welfare. Industrial policy, to be effective and sustainable, needs to be embedded in the overall society’s principles and support commonly accepted goals. Industrial policy is therefore certainly subject to changes over time because it depends on the overall economic and social evolution of a country, but it nevertheless needs to have a clear direction. This short note will briefly comment the structural break that the Italian industry has undergone since the 1990s, the (dwindling) resources it has to count on and the options available for the promotion of industrial competitiveness.",
43,3,Journal of Industrial and Business Economics,29 June 2016,https://link.springer.com/article/10.1007/s40812-016-0046-5,Industrial policy in Italy and Germany: yet another look,September 2016,David B. Audretsch,Erik E. Lehmann,,Male,Male,Unknown,Male,"Like most European countries, Italy has experienced stagnant GDP growth precipitated by the financial banking crisis of 2007 and the long-lasting Euro crisis of 2008. Despite small glimmers of hope, the stagnation is unlikely to end quickly. Several initiatives have been launched both by the European Union and national governments to stimulate industrial production, with only modest success. This motivated Lucchese and Nascia (2016) to advocate a paradigm shift in industrial and innovation policy for Italy, shifting the European horizontal approach towards a national and vertically focused approach, and by empowering the national government. The proposal offers a fruitful and critical review of different aspects and measures of industrial policy in Europe and Italy and initiates an open and necessary discussion. The authors offer their opinion as a counterpart to the recent industrial policy approach in Europe with a clear message: less market (and Europe) and more hierarchy, i.e. national governmental policy. This claim reflects a recent trend in the European Union where countries try to turn their back on the open and market oriented policy of the supranational and European level towards a more centralistic and national government policy to support and protect domestic interests and industries. While some European countries have succeeded in reviving their economies to cope with the challenges of globalization and the technological change like Germany, which was widely referred to as the “Sick man of Europe” little more than a decade ago (Audretsch and Lehmann 2016), others are still suffering. In this paper, we argue that like Germany, Italy could transform itself from the “Real sick men of Europe”, as titled by the Economist in 2005,Footnote 1 to a competitive economy by leveraging the opportunities afforded by globalization and technological change, rather than succumbing as a victim. We aim to contribute to the discussion initiated by Lucchese and Nascia (2016) in several ways. First, we take another look at the data in Sect. 2. Extending the time period of the data reveals that the decline and lack of competitiveness of the Italian industry had begun long before the advent of the Euro crises. Section 3 provides a brief overview of why and how globalization and technological change have shaped the competitiveness of countries in the past decades and explores the implications for the design of contemporary innovation and industry policy. Instead of a national and hierarchical approach, we argue for a more disaggregated level highlighting the importance of sunrise sectors and regions. In Sect. 3 we focus on Germany as a role model in transforming its economy from a “sick man” into a “phoenix”. Section 4 summarizes and concludes.",7
43,3,Journal of Industrial and Business Economics,01 June 2016,https://link.springer.com/article/10.1007/s40812-016-0036-7,Industrial policy in France: in search of lost time,September 2016,Philippe Mustar,,,Male,Unknown,Unknown,Male,"In France, the issue of whether deindustrialization exists or not has been the subject of lively debate over the last few years. Some argue that deindustrialization is a statistical artifact resulting from changes in the very nature of industry and the increasingly blurred line between industry and services. A letter from the CEPIIFootnote 1 (2014) bore the title, “Industry is becoming less … Industrial?”, indicating that the traditional distinction between industry and services is particularly hazy. In fact, a broader and increasing proportion of companies are producing both goods and services. This is because numerous manufactured goods today incorporate multiple services that are required to produce and use them. Very recently, Michelin’s CEO announced that his company no longer sold tires, but a service: “Customers don’t buy tires any more, they buy kilometer efficiency.” The claim is that deindustrialization is not really happening; instead we are faced with the servicization of industry. This hybridization process makes the line between industry and services less distinct. “This more modern vision of industry calls for a new approach to industrial policy” (CAEFootnote 2
2014). In short, the problem could simply be that statistical tools are incapable of taking into account the way industry is changing. More conventionally, numerous economists play down the scale of the deindustrialization phenomenon, which in any case affects all high-income countries. They consider that industrial jobs losses—like agricultural job losses in the past—are linked to the growth of productivity in industry. For them, this shows a move towards a post-industrial society that develops services (Landier and Thesmar 2013). In the opposite camp, some studies are concerned by the phenomenon. Their authors, e.g. Giraud and Weil (2013) and Cohen and Buigues (2015), link the loss of manufacturing jobs with bad economic performance. They suggest setting up policies to limit them. For France, the issue is particularly critical because its deindustrialization process is taking place faster than elsewhere and appears to have started around 20 years ago. To understand the debate, we need to look closely at the causes of job losses in the manufacturing sector. Lilas Demmou (2010) studied the decline in industrial employment in France from 1980 to 2007. Her study centers on three main factors (outsourcing, productivity gains, foreign competition) to explain the loss of jobs in industry. It shows that over the period 1980–2007: 25 % of industrial job losses were due to outsourcing (e.g. a simple transfer of industrial jobs towards services). These job losses are therefore not real. Close to 30 % of the job losses can be imputed to productivity gains made in the industrial sector. This indicates that industry’s share in GDP can go down without a decrease in industrial production. Foreign competition has also contributed to the drop in industrial employment in France although its impact is harder to quantify (with estimates varying from 13 % (taking an accounting approach) to 40 % (taking an econometric approach) of job losses.Footnote 3
 The author’s analyses also conclude that, over the more recent period of 2000–2007, the impact of outsourcing dropped sharply (from 25 to 5 %), that of productivity gains doubled (from 29 to 65 %), and the effect of international competition increased significantly using the accounting method (from 13 to 28 %), but only slightly using the econometric method (39–45 %). This study illustrates that for France, the tertiarization of industrial activity, or changes in industrial productivity in relation to changes in services, cannot fully explain the phenomenon of deindustrialization. International competition—especially from 2000 to 2007—played a significant role that, depending on the method used, is considered responsible for 28–45 % of industrial job losses. In their publication, Giraud and Weil (2013) observe that the most pertinent indicator to measure industrial decline is industry’s balance of trade. This issue of international competition is also highlighted by Cohen and Buigues (2015) in their description of “décrochage industriel” (or industrial drop-off): “France is the only major European country in which the share of goods exports in GDP is low and went down from 2000 to 2014”. Indeed, in 2014 the goods exports share of GDP was 40.5 % for Germany, 25.4 % for Italy, 24.1 % for Spain, and only 21.3 % for France and the United Kingdom. Based on Eurostat figures, the authors show that in 2012, “France imported 45.6 billion euro more than it exported in manufactured industrial goods”. Cohen and Buigues (2015) also show that France is the country with the highest deficit for manufactured industrial products on the intra-European market. In other words, French industry’s loss in competitiveness is most manifest vis-à-vis European Union countries. “In 2012, France recorded a deficit of 85.9 billion euro on the intra-EU market and an extra-EU surplus of 40.3 billion euro” (the same 45.6 billion euro deficit mentioned above). Germany achieved both an intra-EU surplus (83.6 billion euro) and an extra-EU one (211.5 billion euro), as did Italy (respectively 19.3 and 73.3 billion euro). A number of factors contribute to this decline in French industrial companies’ competitiveness. In our studies of entrepreneurship, two factors stand out in particular. The first is the difficulty that hundreds of new technological companies we studied come up against to grow and become medium-size enterprises. During the 2000s, the creation of technological companies, mainly academic spin-offs, was at the heart of French policy on technology (Mustar 1994). After some 15 years of experience, we observe that the vast majority of public-research spin-offs are and stay very small enterprises; very few are funded by venture capital (Mustar and Wright 2010). The results in this field of public policy focused on new research-related firms are disappointing and far below expectations. These medium-sized businesses (which we know play an important role in innovation and exports) represent a much smaller proportion of the industrial composition in France than they do in Germany and the United Kingdom. The French industrial composition remains split in two, with on the one side, very large, fully internationalized companies, and on the other, a multitude of very small SMEs. A second underlying factor that we cannot develop here is the labor market and working relations within industrial companies (Cahuc 2015). The main reason that many of today’s young graduates would rather join a start-up than enter a big group is the attraction of less hierarchy, and easier, more cooperative working relationships. This deficit of manufactured goods in the French trade balance is a major problem because it is difficult to compensate with a surplus in the balance of services. As Giraud and Weil (2013) point out, three-quarters of international trade concerns the exchange of manufactured goods. Our imports cannot only be paid for by the sale of services. It is indispensible to offer competitive manufactured goods on the international market. In other words, high-range, innovative goods, accompanied by quality service and benefiting from a strong image. Goods that customers are prepared to pay more for. Deindustrialization therefore poses a problem because manufactured products play a decisive role in the external balance of trade and growth. For this reason, it is important to maintain a competitive industrial footing in exports. Even though we know that it will not create massive amounts of industrial jobs. The term “industry” should also be understood more broadly because most industrial companies today sell manufactured goods and services, and new technologies play an increasingly significant part in them. The issue of keeping in France activities to design and produce innovative goods and services does not just concern our balance of trade. It also means we can stay in today’s industrial revolution race, because without industry there can be no innovation. Clearly, when a country loses its industrial production, it loses its capacity for innovation, it loses expertise, and it loses ground in tomorrow’s learning. As shown by Suzanne Berger (2013), everyday confrontation with technical problems connected to industrial production constitutes a major source of innovation. Thus, learning takes place when, for example, engineers and technicians in a factory go back to see the engineers responsible for design to find better solutions together. When industrial production is relocated, the capacity to innovate is ultimately relocated too. That is why deindustrialization is a concern. It is also the reason why a totally “tertiarized” economy would not be viable. Overall, two subjects seem crucial for public authorities: French industry’s economic competitiveness for exports, and the ability to maintain robust industrial capacity on French soil which means a capacity to innovate.",3
43,3,Journal of Industrial and Business Economics,08 June 2016,https://link.springer.com/article/10.1007/s40812-016-0042-9,A polarized country in a polarized Europe: an industrial policy for Italy’s renaissance,September 2016,Dario Guarascio,Annamaria Simonazzi,,Male,Female,Unknown,Mix,,
43,3,Journal of Industrial and Business Economics,08 June 2016,https://link.springer.com/article/10.1007/s40812-016-0038-5,Innovation policies: the national and regional dimensions,September 2016,Philippe Laredo,,,Male,Unknown,Unknown,Male,"The article addresses the Italian situation covering three different aspects. First, there is the context of European policies, and it is argued that Italy’s industrial decline is associated to choices entailed by the Monetary Union, the Maastricht treaty, the working of structural funds, the focus on horizontal policies and the overall ‘inadequacy of such measures’ (p. 9). I do not wish to comment this first argument which would require more robust analyses., Second, the object of the article raises the question of what ‘industrial policy’ we are speaking of? What is technology meaning here? I rather understand it as dealing with public interventions to support the competitiveness of manufacturing firms. Third, an important component lies in the type of support to the innovation capacities of manufacturing firms put in place. There are thus two central aspects in the article: the focus on manufacturing firms, and the relevance of Italian policies, as the objective of the paper is to “provide an account of the main industrial policy-related measures … (which) tend to be fragmented, unstable and funded with modest resources” (p. 10).",1
43,3,Journal of Industrial and Business Economics,03 June 2016,https://link.springer.com/article/10.1007/s40812-016-0037-6,Southern Europe in crisis: industrial policy lessons from Italy and Portugal,September 2016,Manuel Mira Godinho,Ricardo Paes Mamede,,Male,Male,Unknown,Male,,4
43,3,Journal of Industrial and Business Economics,01 June 2016,https://link.springer.com/article/10.1007/s40812-016-0035-8,The bumpy ride to the knowledge economy,September 2016,Cristiano Antonelli,,,Male,Unknown,Unknown,Male,"The transition to the knowledge economy seems to be far bumpier than it had been anticipated. The effort of identification of the stylized facts of both the starting points and the destination seems most useful to try and guide the journey. The knowledge economy is characterized by the central role of the production and use of knowledge. Knowledge is both the central input and output of the economic activity. Knowledge based services constitute the bulk of both the demand and the supply. Not only final goods are more and more constituted by knowledge intensive services. Supply also is more and more based on knowledge intensive services that are used as intermediary inputs for the eventual production of final goods. Advanced countries specialize in the production of knowledge based services as intermediary inputs that are exported to industrialized countries where the production of both capital and final goods takes place. Manufacturing industry declines sharply to about a 10 % share of the economy while the share of the new knowledge-intensive business-services (KIBS) grow—often at a lower pace—towards 10 %. The traditional share of the manufacturing industry is replaced by the combination of advanced manufacturing cum KIBS (Antonelli and Fassio 2014, 2016a). The emergence of knowledge economy rests upon five distinct and highly intertwined processes: (1) the globalization of product markets; (2) the globalization of financial markets; (3) the diffusion of new information and communication technologies; (4) the introduction of directed technological change biased towards the intensive use of locally abundant factors, i.e. in advanced countries knowledge; (5) the mobilization of the large stock of knowledge embedded in advanced economy. Let us analyze them briefly in turn. The globalization of international product markets has exposed advanced countries to the competition of new industrializing countries endowed with a large supply of cheap labor. The globalization of financial markets has increased the access of industrializing countries to a large supply of financial resources. The sequential combination of the globalization of product and financial markets has accelerated the emergence of the knowledge economy. Advanced countries discovered that their international competitive advantage could not rely any longer upon high levels of intensity of fixed capital. The international variance of availability and rental costs of capital declined rapidly. Industrializing countries could access and use financial resources at costs and conditions that quickly converged towards the –low- levels that were once available exclusively in advanced countries. The twin globalization of product and financial markets shook the foundations of the traditional division of labor undermining the competitive advantage based on high capital intensity and induced the introduction of biased technological change directed the most intensive use of knowledge i.e. the production factor that emerged as relatively cheaper in advanced economies with respect to the rest of the globalized economy. Advanced countries experienced a dramatic shrinking of their manufacturing industry no longer able to compete with the efficient and highly capital intensive supply by industrializing and industrial countries. The multinational growth of the large companies of advanced countries played a twin role in the process: (1) it favored the rapid displacement of manufacturing industry towards industrializing countries enhancing the transfer of technological competence and favoring the access of their output—as imports—to domestic markets; (2) accelerated the pace to the knowledge economy with the ‘headquarter effect’ by means of which the knowledge intensive functions of the corporations based in the home countries became the hub of the new knowledge-based specialization of advanced countries. The rich and rooted endowment of the knowledge stock provided the foundation for a new specialization based on the intensive use of knowledge as a key input. The competitive advantage of advanced countries could rely upon the low comparative costs of knowledge. The capability of industrializing and industrialized countries to accumulate and access the stock of quasi-public knowledge that characterizes the advanced countries is in fact much lower. The Leontieff paradox experienced by the US economy well before the globalization of financial markets generalized to the rest of advanced countries. The specialization of rich, capital abundant countries, in skill intensive activities with high levels of knowledge content, experienced by the US as the leading country in the international innovation race, spread to the countries that the twin globalization eventually exposed to the erosion of the traditional foundations of their competitive advantage based upon the relative abundance of capital (Antonelli and Fassio 2011; Antonelli and Colombelli 2011). The generation and use of knowledge as the key input and output of advanced economies seems characterized by high levels of: (1) skilled labor intensity and low levels of fixed capital intensity; (2) high frequency of interaction: not only vertical interactions between bottom-up learning processes that enable the accumulation of competence and top-down technological applications of scientific knowledge, and between users and producers, but also horizontal between public and academic research and corporate R&D activities. The knowledge economy relies upon the intensity of recombination of the variety of knowledge items held by each unit in the system. The path to towards the knowledge economy based upon the central economic role of knowledge has major structural implications for advanced economies: The shift away from an industrial economy based upon a strong manufacturing industry in terms of sectoral composition of the economy, with decline of the manufacturing industry and the rise of the knowledge intensive business services. The share of employment in the manufacturing industry of advanced countries started its decline since the late years of the XX century. In UK it declined from the 15.7 % of 1995 to 9.6 % in 2007, in France in the same time interval from 15.0 to 11.5 %, in The Netherlands from 12.9 to 9.6 %. The crisis of 2007 had only marginal effects on the process: in UK the share of employment in manufacturing in 2014 dropped to 8.1 %, in France to 9.9 %, in The Netherlands to 8.8 %. The share of manufacturing in investments drops even more sharply in the same time interval. In the UK it drops from 11.6 % in 1996 to 5.6 % in 2007 and 6.2 % in 2014, in France from 10.8 to 7.2 % in 2007 to 6.9 % in 2014, in the Netherlands from 9.8 % in 1995 to 5.8 in 2007 to 5.9 % in 2013. The drastic part of the change had occurred already before the crisis The increasing role of knowledge intensive business service industries (KIBS) that substitute and complement the smaller manufacturing industry. The strength of the KIBS is crucial to support the smaller but highly productive manufacturing industry. The decline of tangible investments. Investments in tangible capital appear to be intrinsically associated to the industrial economy. The knowledge economy is light in many ways. Capital is less and less fixed and more and more intangible. The shift from fixed capital investments to intangible investments has serious effects on the derived demand for machinery and traditional capital goods. Investments are less and less tangible. Investments per person employed in ICT services in 2013 are 18.6 thousand euro in Norway, 18.4 in Switzerland, 14.2 in France. Investments in professional, scientific and technical activities reach 8.2 thousand euro per person employed in Switzerland, 7.2 in Norway, 6.0 in France, 3.7 in UK. After two decades from its preliminary emergence, the transition to the knowledge economy reveals to be more problematic than expected. The demise of the Fordist model takes place among delays and negative reactions. The radical decline of the manufacturing industry as the heart of the economy engenders major problems in labor markets with strong mismatches between demand and supply in terms of skills and competences. The exclusion of manufacturing workers with low opportunities for reskilling leads to long-term unemployment. At the same time the supply of skilled manpower able to participate in the growth of KIBS is scarce with significant effects on wage levels. The parallel decline of investment in fixed capital augments the depressive effects of the transition for its negative effects on the aggregate demand. The radical structural change based upon the shift away from manufacturing industry and the specialization in the generation and use of knowledge has a strong impact on the production process. Intangible investments become a central driver of production. Tangible investments consist in the actual purchase of capital goods that are being produced by an array of specialized industries. Intangible investments are but the capitalization of expenditures consisting mainly if not exclusively of labor costs. The increase of intangible capital stock parallels the reduction of current expenditures, actually substitutes them from the viewpoint of the established accounting procedures. The production function, in other words, is shifting away from the traditional composition of a stock of machines and labor, to the composition of two basic factors: labor and the capitalization of the expenses in skilled labor specialized in the generation and use of knowledge. This microeconomic revolution has major effects at the aggregate level as it implies the fall of the component provided by tangible investment in the aggregate demand. The increase of intangible investments is nothing else but the relabeling of former current labor expenditures. The latter do not add to the aggregate demand. The hypotheses and the evidence about the secular stagnation of advanced economies recently put forward by Gordon (2016) find new support in the analysis of the effects of the decline of investment in fixed capital that takes place in the transition to the knowledge economy. The share of tangible investment on GDP falls from levels around 35 % typical of late industrializing countries such as India and China in the first decade of the XXI century, or of early industrializing countries such as the US and UK, in the XIX and early XX century, to the current levels—around 20 %—typical of advanced countries in their the transition of to the knowledge economy. This has strong implications for growth.Footnote 1 The actual reduction of inputs associated to the fall of investment in fixed capital—not compensated by the rise of intangible ones that consist simply in past current expenditures- is itself a major determinant of the secular stagnation. The negative trend of output is aggravated by short-term factors. First of all, the mismatches in labor markets between the excess supply of blue collars competences and the excess demand of white collar ones have determined at the same time the increase of unemployment and of the skewness of income distribution with major negative consequences in terms of aggregate demand, Second, the decline of the share of investment affects aggregate demand which misses a major component. The skewness of income distribution together with the decline of investment opportunities aggravates the accumulation of excess liquidity in financial markets searching for new investment opportunities. Third, the traditional mechanisms of diffusion of embodied technological change are delayed if not streamlined. The contraction of investment rates and the share of manufacturing, as a matter of fact, are two parallel and intertwined processes. The eventual completion of the transition is likely to dry up the negative effects of the twin squeeze. The end of the transition to the knowledge economy should level off the negative consequences outlined so far. The understanding of structural change is strictly necessary to grasp the long term dynamics of economic systems. The support of the aggregate demand and the identification of sustainable investment opportunities able to channels the excess liquidity seems indispensable targets of an economic policy able to meet and contrast the specific problems raised by a typical Schumpeterian structural transition.",4
43,3,Journal of Industrial and Business Economics,17 June 2016,https://link.springer.com/article/10.1007/s40812-016-0045-6,Lost in transition: systemic innovations and the new role of the state in industrial policy,September 2016,Enzo Rullani,Claudio Cozza,Antonello Zanfei,Male,Male,Male,Male,"We live in times of transition. The globalization of markets and the digitalization of knowledge are changing production, consumption as well as social and economic relationships. This transition has been examined from a variety of perspectives and its expected outcome has been dubbed in the literature with a number of terms. These include “network capitalism”, “knowledge economy”, and “post fordism”. A working definition we might adopt is that of “networked, knowledge intensive, global capitalism”, which puts together the key aspects of the emerging paradigm. Discussing the nature, characteristics and consequences of such a transition is beyond the scope of this note.Footnote 1 We would rather make the point that adopting the lenses of transition (from declining modes of production and economic governance towards a new yet to be shaped techno-economic and institutional paradigm) helps us to better understand the present economic crisis and the role of industrial policies in dealing with it. The transition generates uncertainty as well as new opportunities. Some countries are in a better position to benefit from transition, while others will lose ground in a dynamic process that will eventually yield new equilibria. Moreover, within each country, some businesses, people and territories can take advantage of the transition, whereas others suffer its penalizing effects and may fall behind. Italy is one of the latter category of countries, as shown by Lucchese et al. (2016, in this issue). The loss of competitiveness (they say the “decline”) of the Italian industrial system began before the 2008–14 crisis, and is therefore due to earlier causes. The economic conditions have been improving in 2015–16, but we have not yet reached a clear turning point. While exports have increased, the Italian recovery goes on at a lower rate than recorded in other more developed European countries. The question boils down to: how can this country—like others that face the transition from a position of relative weakness—be able to change trajectory, setting in motion an economic engine which has been ill-performing? And, above all, who can?",6
43,4,Journal of Industrial and Business Economics,21 November 2016,https://link.springer.com/article/10.1007/s40812-016-0059-0,Public sector entrepreneurship: introduction to the special section,December 2016,Albert N. Link,,,Male,Unknown,Unknown,Male,,4
43,4,Journal of Industrial and Business Economics,16 July 2016,https://link.springer.com/article/10.1007/s40812-016-0050-9,Public cluster policy and new venture creation,December 2016,David B. Audretsch,Erik E. Lehmann,Matthias Menter,Male,Male,Male,Male,"Public cluster policy has been recognized as an effective instrument to foster innovation and regional economic growth (Wolman and Hincapie 2015; Leyden and Link 2015; Leyden 2016; Chatterji et al. 2013). Empirical studies estimating the performance of cluster policies focus either on the overall economic performance of regional clusters (Delgado et al. 2014; Vaz et al. 2014) or on cluster effects on firm performance (Garone et al. 2015; McCann and Folta 2011). However, despite their role in absorbing knowledge spillovers, transforming them into new ideas that drive innovations, the important role of entrepreneurial activities has often been neglected in this literature. This paper addresses this issue. We analyze the impact of the ‘Leading-Edge Cluster Competition’ (Spitzencluster-Wettbewerb) as the most prominent and recent example of a public cluster policy initiative and examine the impact on new venture creation. In particular, in high-technology and knowledge intensive industries, entrepreneurship is identified as a key driver of new ideas, products and innovation by permeating the knowledge filter (Braunerhjelm et al. 2010; Acs et al. 2013). Without taking entrepreneurial activities into account, performance measures of public cluster policies remain incomplete. While public cluster policies increase the amount of knowledge produced by incumbent firms and research institutions, the problem of penetrating the knowledge filter still exists, i.e. the transformation of knowledge into ideas and marketable goods and services. In this paper, we empirically analyze whether and how entrepreneurial activity, as measured by new venture creation, is shaped by public cluster policies. We hypothesize that public cluster policy shows a positive and significant effect on new venture creation, as proposed by the initiative. New venture creation in regions treated by public cluster initiatives should be stimulated and thus we expect higher entrepreneurial activities after the treatment effect compared to the control group. We base our empirical study on the impact of public cluster policy on all 150 labor market regions in Germany, as defined by Eckey et al. (2006). Our treatment effect is the ‘Leading-Edge Cluster Competition’, a public initiative of the years 2008 and 2009. In accordance with the EU’s ‘Horizon 2020 Framework Program for Research and Innovation’ (McCann and Ortega-Argilès 2016), the general objective was to promote regional clusters with a strong focus on future markets and technologies, like ICT, biotechnology, industry 4.0, mobility or the climate and energy sector, to defend and improve the position of Germany as a worldwide innovation leader. In 2008 and 2009, 10 clusters have been selected by the government and labeled as ‘leading-edge clusters’ (Spitzencluster). They encompass 21 metropolitan areas (from a total of 150) and are assumed to play an outstanding role in at least one of the five selected industrial areas. Besides the ‘Signaling-Effect’, these clusters benefitted and participated from public and private funds exceeding 1.2 billion Euros. Thus, in addition to fostering innovation and technology transfer, the focus is on establishing and developing a positive climate to promote new venture creation in high-tech industries (BMBF 2015). To estimate the treatment effect, we use a dataset covering a time period from 1998 to 2012. We assume that new venture creation in all labor market regions follows a regional specific path until the treatment in 2008/09. After the treatment—the policy intervention—we postulate that new venture creation shows a positive and significant deviation from the previous path for the treatment group, but not for the control group. In addition, we hypothesize that new venture creation in the treatment group is higher compared to the control group. At first glance, our results confirm that public cluster policy positively affects new venture creation and that one of the objectives of the mission—fostering new venture creation—is fulfilled. New venture creation in the treatment group is almost always positive and statistically significant. However, the results are not as clear and robust when taking the treatment effect and the treatment period into account. The results strongly differ according to the different estimation models. Our results suggest that a selection effect has occurred in advance, i.e. a self-selection of promising clusters. We conclude that not just the money—or public funds—gained by the winners of the contest is important. Rather, it is the contest itself that selects the best and most promising clusters. The funds provided to the selected “leading-edge clusters” only partially explain abnormal growth of new venture creation. Instead, key local characteristics, such as industry–university networks, population density, employment and innovativeness remain statistically significant and robust over all estimations for both groups, as well as the treated and the control group. Public cluster policy and associated financial subsidies thus do not automatically foster entrepreneurship or create “the next Silicon Valley”. Our study adds to the literature in several ways. Our first and main contribution is the evidence that public cluster policy does not necessarily enhance new venture creation per se. While the empirical results add to the growing literature linking entrepreneurship policy to regional development issues (Acs et al. 2016; Audretsch 2015; Autio et al. 2014; Ebbekink and Lagendijk 2013; Rocha 2013), our findings lead us to conclude that public cluster policy is neither a sufficient nor a necessary condition for new venture creation. Our findings also contribute and add to the literature on positive externalities of universities in shaping new venture creation (Bonaccorsi et al. 2013, 2014; Calcagnini et al. 2016; Rasmussen et al. 2014; Rasmussen and Wright 2015; Ghio et al. 2015; Acs et al. 2013) and to the growing literature emphasizing the importance of entrepreneurship as a means to penetrate or circumvent the knowledge filter (Acs and Plummer 2005; Acs et al. 2013). Our results reinforce the knowledge spillover theory of entrepreneurship, arguing that regions characterized by more tacit knowledge generate more entrepreneurial opportunities and thus exhibit a higher rate of entrepreneurial activities and new venture creations (Acs et al. 2013; Audretsch and Keilbach 2007; Audretsch and Lehmann 2005; Ghio et al. 2015). Our results further add to the literature on entrepreneurial ecosystems (Acs et al. 2014; Acs et al. 2016a, b), by highlighting the importance of local networks, regional density, and the level of innovativeness as main drivers of new venture creation. Our results also point out that a “one-size-fits-it-all” policy does not exist (see Audretsch et al. 2015). Some important variables in explaining new venture creation in the literature, like employment or innovativeness, are not robust in all estimations or at least enter with a negative sign. This indicates a trade-off between the costs and benefits of the included variables and that increasing one factor does not necessarily lead to an increase in the output variable, new venture creation.Footnote 1
 Finally, our study adds to the growing literature by applying treatment and difference-in-difference estimation techniques to isolate effects of policy implications (Angrist and Pischke 2008). We contribute to this literature by highlighting that selection effects and pre-contest behavior may be more effective than is the amount of money spent. However, incentives are not independent from the size of the pie. The remainder of this paper is structured as follows. In the next section, we present the background literature about public cluster policies and new venture creation and develop our main hypotheses. The third section contains the research design, with the details of the research sample and the methodology. Section four summarizes our findings and finally, we conclude by highlighting the main implications and limitations of this research.",28
43,4,Journal of Industrial and Business Economics,06 October 2016,https://link.springer.com/article/10.1007/s40812-016-0054-5,Publicly funded principal investigators allocation of time for public sector entrepreneurship activities,December 2016,James A. Cunningham,Paul O’Reilly,Vincent Mangematin,Male,Male,Male,Male,"Public sector entrepreneurship through the implementation of specific programs can support the generation of economic growth and can be transformational in nature (Leyden 2016). Leyden and Link (2015:14) define public sector entrepreneurship as: ‘Innovative public policy initiatives that generate greater economic prosperity by transforming a status-quo economic environment into one that is more conducive to economic units in creative activities in the face of uncertainty.’ The affect of public sector entrepreneurship is the creation new knowledge networks, advancement science and knowledge as well as mobilizing different actors that will result more innovation (Audretsch and Link 2016:1; Link and Link 2010). Taking a lead from Klein et al. (2010), this paper considers that the research project and programme leaders in publicly funded research systems, from here on referred to as principal investigators (PIs), are in effect public agents and nominal stewards of resources owned by the public. Entrepreneurship in this public context is enacted principally through envisioning future scientific trajectories and research commercialization. Publicly funded PIs are key transformative actors in the shaping and implementing of public sector entrepreneurship, through their scientific endeavors and the exploitation of these through technology transfer mechanisms (Cunningham et al. 2016b). Such activities have an array of impacts such as market, economic and societal impacts. PIs are boundary spanners and engage with a wide variety of actors such industry, policy makers, technology transfer specialists, technology transfer offices etc. PIs are beneficiaries of public sector entrepreneurship programmes through the allocation of public resources on a competitive basis to purse research activities that will advance knowledge and now increasing that will have an impact beyond academia. Traditionally, public research provides some autonomy for PIs as is it seen as the ‘freest form of support’ (Chublin and Hackett 1990). However, being a publicly funded PI places additional responsibilities on scientists and they have to deal with additional managerial challenges and complexities (Cunningham et al. 2015). The university environment that enables PIs to deliver on publicly funded research programme also matters (Cunningham and Harney 2006). Previous studies highlight the importance of high quality research and its benefits to entrepreneurship and start-ups (see Colombo et al. 2010; Di Gregorio and Shane 2003). The changing university environment in which PIs work is typified now as increasingly dynamic, managerialist and commercialized (Deem and Brehony 2005; Fitzgerald and Cunningham 2016; Kok et al. 2010; Kolsaker 2008). For universities there is a brain drain concern that academic effort and time is being utilized for activities such as entrepreneurship that diverts them away from knowledge development (see Toole and Czarnitzki 2010). The allocation of time of PIs matters to universities, the implementation of public entrepreneurship programmes and on knowledge production. Set against this context our paper focuses on how publicly funded PIs allocate their time when they are beneficiaries of public sector entrepreneurship through public funded programs nationally and internationally. We examine their allocation of time in general to research activities and specifically at a project level in relation to the type of research, knowledge transfer activity, project impact, deliberate technology transfer strategy and boundary spanning activities. Our project level focus on the allocation of time is unique and this has not been done before in other studies of time within the academy. Our paper is structured as follows. We begin by focusing on the multiple roles of PIs, PI responsibility and the allocation of time. In Sect. 3 we outline our research methodology and then present our key findings in Sect. 4. We conclude our paper with some concluding remarks.",14
43,4,Journal of Industrial and Business Economics,06 June 2016,https://link.springer.com/article/10.1007/s40812-016-0039-4,Creativity for invention insights: corporate strategies and opportunities for public entrepreneurship,December 2016,John T. Scott,,,Male,Unknown,Unknown,Male,"This paper describes the discovery of invention insights in the context of a description of the invention-insight sample space. An invention insight is defined as the essential combination of elements of knowledge to envision the basic working configuration of an invention—the working idea for a new technology.Footnote 1 Section 2 describes invention insights and gives examples. Section 3 describes the invention-insight sample space and the creative process of discovering invention insights. Section 4 uses the description of the creative process to explain how competition—defined as potential inventors who can freely enter the discovery process and freely share ideas as they strive to discover invention insights—increases the discovery of invention insights. Section 5 discusses evidence consistent with the description of the creative process that emerges from the discussion of the invention-insight sample space. Section 6 concludes by using Sect. 3’s description of the invention-insight discovery process to identify a novel new opportunity for public entrepreneurship that would speed the pace of technological progress, and by using Sect. 4’s observations about competition to delimit the role of government in supporting invention-insight discovery and the opening up of altogether new areas of science and technology.",3
43,4,Journal of Industrial and Business Economics,12 July 2016,https://link.springer.com/article/10.1007/s40812-016-0051-8,Universities as partners in research joint ventures,December 2016,Dennis Patrick Leyden,,,Male,Unknown,Unknown,Male,"Sustainable economic growth ultimately depends on a continual flow of innovations arising from entrepreneurial action (Acs et al. 2009; Braunerhjelm et al. 2010), and hence requires a balance between research and the application of that research to the innovation process (Michelacci 2003). One source of knowledge for entrepreneurs to draw upon to complement their research is universities. Empirical evidence suggests that universities can play a significant role in the process by which entrepreneurs acquire as well as apply knowledge in the innovation process (Audretsch and Lehmann 2005; Tassey 2008; Link and Ruhm 2009; Audretsch et al. 2013; Link and Welsh 2013; Link 2015).Footnote 1
 However, little is known of the characteristics of research joint ventures (RJVs) involving universities and how university participation might vary by RJV characteristics. One of the few studies that has investigated this issue is Link and Scott (2005).Footnote 2 Perhaps the most significant finding of this study was that larger RJVs, measured in terms of numbers of members, were more likely than smaller RJVs to invite a university to join. While Link and Scott attributed this result to universities providing higher marginal value and lower appropriability problems to larger RJVs, work by Boardman and Bozeman (2006) suggests that the explanation might be more complex and tied to the presence of a multiplicity of motivations and incentives including those not associated with profit maximization. Neither Link and Scott nor Boardman and Bozeman explored the policy implications of their findings. This paper offers a preliminary formal model, based on a profit-maximizing approach, to further the investigation into why large and small RJVs might behave differently with respect to involvement with universities. As such, it can serve as a foundation for future empirical work that explores the degree to which a profit-maximizing approach might be useful in explaining the relationship between RJV size and university participation. This empirical work, in turn, can provide a baseline from which to explore the relative explanatory value of RJV models based on alternative behavioral foundations. In addition, the model in this paper provides a framework that can be used for evaluating empirically the policy implications of the Link and Scott (2005) findings. Among the possible policy implications that could be explored are those connected to evaluating alternative institutional and financial structures, connected to alternative types of knowledge transferred from universities to RJVs and the property rights associated with such transfers, and connected to industry idiosyncratic factors that might play a role the decision to invite universities to participate in RJVs.",
43,4,Journal of Industrial and Business Economics,20 January 2016,https://link.springer.com/article/10.1007/s40812-016-0027-8,The determinants of open access publishing: survey evidence from countries in the Mediterranean Open Access Network (MedOANet),December 2016,Thomas Eger,Marc Scheufen,Daniel Meierrieks,Male,Male,Male,Male,"The way scientists communicate has certainly changed since the advent of the internet and in the presence of digital information goods. Most interestingly, this also affects the business model used for disseminating academic works. First, the academic publishing market has seen a vast increase in subscription prices over the last two decades.Footnote 1 Together with budget cuts affecting library systems in several countries, this trend has inevitably reduced access to scientific knowledge (the so-called serials crisis). Second, digitization has created opportunities to disseminate academic works at no time loss and virtually no costs. Several initiatives in academia have been advocating the open access (henceforth OA) model, with proposals ranging from a stronger financial support of OA to changes in the copyright for academic works such as the establishment of an inalienable authors’ right of secondary publications. Some scholars even question the role of copyright protection in academic publishing as a whole (Shavell 2010). Despite an ever-growing literature analyzing the various consequences of a regime change related to academic publishing (for a review see Mueller-Langer and Scheufen 2013), several questions regarding specific characteristics and differences between scientific disciplines and countries in the systems of academic publishing remain unresolved. The principles of OA were first defined by the Budapest Open Access Initiative (2002). According to its definition, open access involves “free availability on the public internet, permitting any user to read, download, copy, distribute, print, search or link to the full text of these articles, crawl them for indexing, pass them as data to software, or use them for any other lawful purpose, without financial, legal, or technical barriers other than those inseparable from gaining access to the Internet itself” (BOAI 2002). Following Harnad et al. (2004), there are two complementary business models of OA: (1) OA journals (the so-called gold road) and (2) self-archiving or repositories (the so-called green road). As the OA movement has gained momentum especially since the beginning of the twenty-first century, it has also become an object for academic research. From an economic point of view, academic works are specific goods characterized by non-rivalry in consumption. Whenever a work is produced, it makes no sense to exclude consumers from access to it because including more consumers increases their individual welfare without lowering anybody else’s. However, incentives have to be provided to produce these works. Copyright law solves this trade-off by awarding for a limited time a specific bundle of rights to the authors. The crucial question is whether existing copyright law solves this trade-off under the changed conditions in an efficient way.Footnote 2 Apart from the general effects of digitization and the internet on all types of publishing, there is an important difference between academic authors and most other authors. Academic authors typically do not receive a direct honorarium for their articles. They are, instead, motivated by the increase in reputation from their publications. However, the publishers still need sufficient incentives to engage in the business of publishing academic works, i.e., they should be able to cover their publication cost and earn a market rate of return on their investment. The literature investigating the OA model can broadly be categorized along three lines of research: (1) studies that assess the effectiveness of the alternative publishing models in providing a rapid and cost-efficient system for the dissemination of scholarly work, (2) studies investigating how online and free online access affect readership and citations and (3) studies focusing on researchers’ attitudes towards OA and alternative publishing models (Mueller-Langer and Scheufen 2013). Our paper focuses on the latter literature strand, analyzing the awareness of and experience with OA in different scientific disciplines at universities and research institutes of the Mediterranean Open Access Network (MedOANet) in six countries: Spain, France, Italy, Turkey, Greece and Portugal. In particular, we study the differences between disciplines and countries to also evaluate the effectiveness of different OA policies in these countries. Here, we use the results from a survey in Germany by Eger et al. (2015) as a benchmark. The remainder of this paper is organized as follows. Section 2 provides an overview of the MedOANet system and its policies to foster OA publishing. Moreover, we relate our research to other surveys on OA publishing. In Sect. 3 we look at the research setting and show the cross-country descriptive statistics of our survey results. Section 4 empirically investigates both the use of and attitudes towards OA journals (gold road) and online repositories (green road). We conclude in Sect. 5 by evaluating the MedOANet policies in the light of our findings.",4
43,4,Journal of Industrial and Business Economics,16 September 2016,https://link.springer.com/article/10.1007/s40812-016-0057-2,Industrial policy and the future of manufacturing,December 2016,Antonio Andreoni,Ha-Joon Chang,,Male,Unknown,Unknown,Male,"The dramatic transformations in the global manufacturing landscape started in the mid-1990s, but have been reinforced by the financial crisis and the subsequent recession. During the first phase of the crisis (2008–2009), the manufacturing loss estimate reveals the collapse of industrial production worldwide with respect to both the zero growth scenario and the sustained growth rate scenario (based on the average annual growth rate achieved in the pre-crisis period between 2000 and 2007). Specifically world manufacturing loss was between US $361.32 billion (with respect to the zero growth rate scenario) and US $875.72 billion (if we compare it with the sustained growth rate scenario) (Andreoni 2015a). However, the manufacturing loss was uneven across countries. Among the top 20 industrialised nations (see Table 1), Italy, followed by Spain and the UK has experienced the most dramatic manufacturing loss. Against this background, the paper aims at reconstructing the industrial policy debate by focusing on a number of theoretical issues, in particular the contested nature of industrial policy—its selectivity—also in relation to manufacturing and the different rationales for industrial policy making. The paper concludes by looking ahead into the future of manufacturing and focuses on the need for rethinking our understanding of global production and emerging technologies for increased prosperity.",49
44,1,Journal of Industrial and Business Economics,01 February 2017,https://link.springer.com/article/10.1007/s40812-017-0070-0,Obituary: Giacomo Becattini (1927–2017),March 2017,Marco Bellandi,,,Male,Unknown,Unknown,Male,,
44,1,Journal of Industrial and Business Economics,08 February 2017,https://link.springer.com/article/10.1007/s40812-017-0069-6,International perspectives on venture capital and bank finance for entrepreneurial firms,March 2017,Joern H. Block,Douglas J. Cumming,Silvio Vismara,Unknown,Male,Male,Male,"Entrepreneurial firms are the ‘engines’ of economic development. To fulfill this goal and achieve their full potential, they need capital in the form of human, social, environmental, and financial capital. How to provide this capital has led to an intense discussion and enhanced interest among policymakers, regulators and academic scholars (Block et al. 2017a, b). The number of papers on venture capital (VC) and debt financing of entrepreneurial firms has quadrupled over the last 15 years (Fig. 1), which has been spurred on by recent research on different types of entrepreneurial finance and entrepreneurial finances in different regions around the world (Cumming and Zhang 2016; Bellavitis et al. 2017; Block et al. 2017b). Google Scholar hits on venture capital and entrepreneur debt. This figure presents the number of Google Scholar hits on various search terms. The search was carried out on June 30, 2016, and the 2016 figures are raw and not adjusted for a partial year. The search is normalized to index values at 100 for 2007 to show comparative growth rates. The search is only for select journals in finance and entrepreneurship: Entrepreneurship Theory and Practice, Strategic Entrepreneurship Journal, Strategic Management Journal, Journal of Business Venturing, Journal of Corporate Finance, and Journal of Finance Although there is a general agreement about the importance of entrepreneurial firms for the economy and, consequently, about the importance of entrepreneurial finance, there is a strong need to understand how exactly each finance instrument and player impacts entrepreneurial firms and their growth trajectory. The entrepreneurial finance literature is so far very segmented by the source of financing from which entrepreneurs obtain their funds (Cumming and Vismara 2017). Existing studies focus, almost exclusively, on a single source of financing neglecting the dependencies and interconnections between the different types of financing instruments and players (Moritz et al. 2016). VC funding has been considered as a significant and almost natural step in the growth of entrepreneurial firms. VCs have been established as intermediaries between the demand and supply of financial resources, particularly in turbulent market and technology environments. However, focusing exclusively on VCs as providers of entrepreneurial finance is not enough. First, entrepreneurs often raise financing from multiple sources (Hanssens et al. 2016; Moritz et al. 2016). The fact that most VC studies do not acknowledge that the firm may have received money from other sources of capital is becoming an important limitation, particularly if we consider that the recent financial crisis has increased the difficulty for entrepreneurial firms to raise seed and early-stage financing. In times of economic and financial crisis, VCs become risk adverse and focus more on later- versus early stage investments (Block and Sandner 2009). Second, though previous research has investigated the significant roles of VCs and their success factors in the entrepreneurial process, VCs are often assumed being a homogeneous group, which is not true. In fact, VCs as a group are very heterogeneous in their experiences, resources, capabilities, goals and objectives. VCs exhibit strong variation in the quality and effectiveness of their value-added activities as well as in their goals and objectives (Colombo et al. 2016). Third, entrepreneurs do not necessarily follow the “traditional” funding cycle, as they can time their fund raising, approaching different means of financing at various points in time. The funding cycle is arguably not as straightforward as often assumed. For example, some entrepreneurs may enter an incubator or a science park while others access consumers directly through a reward-based crowdfunding campaign, or raise equity capital from the crowd followed by VC funding, or vice versa (Vismara 2016a, b). Since the interests and time horizons of different capital providers are different, it is valuable to study how these financing sources interact with each other. External equity through VCs, Business angels (BAs) or initial public offerings (IPOs) has traditionally been considered as the most valuable form to finance innovation. Debt was not considered being an option because entrepreneurial firms often lack collaterals. Recently, however, bank financing for innovative ventures has gained relevance. There is growing evidence that even early stage entrepreneurial firms rely extensively on bank debt (e.g., Cassia and Vismara 2009; Cosh et al. 2009; Deloof and La Rocc 2015; Robb and Robinson 2014; Hanssens et al. 2016). Not all (innovative) entrepreneurs are able to attract VC or BA financing. Some may not even want VC funding. They want to avoid excessive intrusion in the firm’s management or conflicts about the firm’s strategy (Hellmann and Puri 2000), are reluctant to dilute their control or simply have problems in structuring the deal in terms of valuation or appropriability hazards. Though neglected, bank financing constitutes sometimes a possible source of financing also for entrepreneurial firms. In particular, nowadays entrepreneurial firms require fewer resources, as the costs of communication and transportation have decreased. Debt is arguably more an option for risky but smaller investments. This article, and the related special issue, deal with bank financing and how it relates to VC financing. In addition, similar to VCs as a group, banks as a group are very heterogeneous as well. For instance, larger national and smaller local banks have different functions and roles in the IPO market. Most IPOs in Europe (82.5%) are underwritten by small, domestic banks (Migliorati and Vismara 2014). Most underwriters in London’s second market (AIM) are “financial boutiques” that do not take companies public to the London main market. On the contrary, large, established investment banks typically manage only a few big IPOs, where they are needed as only they have enough market power the to generate sufficient demand from investors (Liu and Ritter 2011). The remainder of the editorial is divided into three sections. Section 2 compares VC and bank financing, Sect. 3 summarizes the five special issue papers, while Sect. 4 concludes and discusses relevant and fruitful areas for further research.",13
44,1,Journal of Industrial and Business Economics,21 September 2016,https://link.springer.com/article/10.1007/s40812-016-0056-3,Financing R&D investments: an analysis on Italian manufacturing firms and their lending banks,March 2017,Giacinto Micucci,Paola Rossi,,Male,Female,Unknown,Mix,,
44,1,Journal of Industrial and Business Economics,13 July 2016,https://link.springer.com/article/10.1007/s40812-016-0052-7,Smart finance for smart places to foster new venture creation,March 2017,Erik E. Lehmann,Nikolaus Seitz,Katharine Wirsching,Male,Male,Female,Mix,,
44,1,Journal of Industrial and Business Economics,20 October 2016,https://link.springer.com/article/10.1007/s40812-016-0060-7,Complementary or conflictory?: the effects of the composition of the syndicate on venture capital-backed IPOs in the US stock market,March 2017,Sunny Hahn,Jina Kang,,Male,Unknown,Unknown,Male,"Venture capital (VC) positively affects many economic and managerial phenomena such as the foundation of public companies, technological innovations and economic growth (Kortum and Lerner 2000). In a fast changing market and technology environment, the role of VCs in supporting high agility startups has been enormously strengthened. Further, VCs have a beneficial effect on the investee’s exit through IPO, which is considered as an important factor related to a startup’s performance early in its life (Ritter and Welch 2002; Sutton and Benedetto 1988). VCs provide financial and non-financial support to their investees (Large and Muegge 2008), and play a certification role in IPOs (Megginson and Weiss 1991). Further, VC-backed firms are more efficient and have more effective corporate governance and independent boards, which leads to a higher possibility of successful exit through IPO (Baker and Gompers 2003; Campbell Ii and Frye 2009; Chemmanur et al. 2011; Suchard 2009). Though previous research has investigated the significant roles of VCs and their critical success factors (Baum and Silverman 2004; Sapienza 1992), the basic assumption has been that VCs are homogeneous. In practice, however, VCs are heterogeneous in their experiences, resources, capabilities and objectives (Elango et al. 1995). VCs exhibit strong variation in the quality and effectiveness of their financial investment and non-financial value-added. Hence, the objective of this paper is to investigate the effects of the VC type on the performance of VC-backed startups. Prior research has classified VCs into financial investors, such as independent venture capitalists (IVCs), and strategic investors, such as corporate venture capitalists (CVCs), and identified their characteristics and differences (Alvarez-Garrido and Dushnitsky 2016; Arping and Falconieri 2010; Chemmanur et al. 2014; Hellmann 2002; Maula et al. 2005; McNally 1997). Although previous studies have investigated the characteristics and different effects of IVCs and CVCs, they put less attention on the syndicate investments among IVCs and CVCs. In this paper, we analyze syndicate investments among IVCs and CVCs and their influence on the IPO of the investee firms. When IVCs and CVCs engage in syndication, the resource-based view provides a foundation for explaining the complementary relationship between them, as both hold different but complementary resources and capabilities (Alvarez-Garrido and Dushnitsky 2016; Teece 1986; Penrose 1959). On the other hand, the agency theory perspective suggests that syndicate investment among IVCs and CVCs, who have different objectives and time horizon, may face conflicts of interest (Eisenhardt 1989; Jensen and Meckling 1976; Masulis and Nahata 2009; Wright and Lockett 2003). Hence, we discuss and test the two conflicting hypotheses of either a complementary or conflictory relationship in syndicate investment among IVCs and CVCs and the resulting effect on the IPO of the investee firms. The data sample used in our study is composed of 188 VC-backed US firms which attracted their first VC investment from 2001 to 2010 and achieved IPO exit. To investigate the effects of the syndicate investment on the startups’ performance, this paper conducts survival analysis and treats ‘time to IPO’ as the dependent variable that measures the months between the first VC investment and IPO exit.Footnote 1 The results indicate that syndicate investments among IVCs and CVCs delay the IPO exit due to agency costs and conflicts among syndicate partners. Overall, this study makes a number of contributions. First, it analyzes the syndicate investments among different types of investors such as IVCs and CVCs, and their impact on the investee’s exit through IPO which for the most part has not been a focus of previous literature. We believe this research provides a valuable theoretical and empirical extension of the existing literature on entrepreneurial finance and IPOs. Second, this paper uses various theoretical lenses such as the resource-based view, agency theory, and the relational view to shed light on the relational characteristics among syndicate partners.",4
44,1,Journal of Industrial and Business Economics,02 December 2016,https://link.springer.com/article/10.1007/s40812-016-0063-4,A drop in an empty pond: Canadian public policy towards venture capital,March 2017,Douglas Cumming,Sofia Johan,Jeffrey G. MacIntosh,Male,Female,Male,Mix,,
44,1,Journal of Industrial and Business Economics,20 September 2016,https://link.springer.com/article/10.1007/s40812-016-0055-4,Financial constraints in family firms and the role of venture capital,March 2017,Annalisa Croce,José Martí,,Female,Male,Unknown,Mix,,
44,2,Journal of Industrial and Business Economics,16 July 2016,https://link.springer.com/article/10.1007/s40812-016-0049-2,The market power requirement in antitrust enforcement and its usefulness,June 2017,V. Bageri,Y. Katsoulacos,,Unknown,Unknown,Unknown,Unknown,,
44,2,Journal of Industrial and Business Economics,24 November 2016,https://link.springer.com/article/10.1007/s40812-016-0065-2,"Price variability, generic substitution, market size and multimarket contacts in the Finnish beta blocker market",June 2017,Mikael Linden,,,Male,Unknown,Unknown,Male,"Price variability or volatility is typically considered to be harmful for both households and firms. It can be expected that drug price variability is no exception to this rule. Price uncertainty, manifested by price variability, generally causes firms to employ fewer inputs, forgoing expected profits in order to hedge against price volatility (Baron 1970). Likewise, search costs increase for households when they seek the optimal price for their purchases (Salop and Stiglitz 1982). Thus, both the customers and firms in drug markets value price stability. Optimal buying and pricing decisions require stable market information to back up the efficient market process. Price stability in drug markets has not recently been the target of governments across European Union (EU) countries. Rather, the target has been to encourage price differentiation and competition in the markets to reduce drug expenditures. Regulation instruments employed by governments, such as reference pricing systems, prescribing budgets, generic substitution, patient co-payments, monitored pharmacy sales and physician prescriptions, can be effective in stimulating generic uptake to lower drug prices. However, it is not possible to rule out that this is at the cost of increasing price uncertainty and variability. This is in conflict with the price stability targets of the European Central Bank (ECB), but EU’s competition policy and low inflation targets of the ECB are not necessarily violated. The regulatory systems that predominate in areas such as Europe indicate that application of these policies, although leading to price reductions, may also constitute a barrier to innovation and to dynamic competition in consumer prices (Puig-Junoy 2010). The extent of price variability under these generic substitution systems is an issue that needs to be examined. This is not addressed adequately in the literature. Only Toumi et al. (2014) provide some findings in this context, based on price simulations for the EU and some Organization for Economic Cooperation and Development (OECD) countries. In Finland, generic substitution with a regulated price system was introduced on 1 March 2003. Medicines are categorized by the national health authority (FIMEA) into chemical or therapeutically equivalent product subgroups. These groups with identical or similar active ingredients—molecules or active therapeutic chemicals (ATCs)—are called “reference” or “substitution” groups. Pharmacists have to suggest a substitute for the original drug if the price exceeds that of the least expensive product in the substitution group by a margin of 2 or 3 euros, depending on the level of the package price. The consumer can refuse the replacement. Most importantly, many brand name products to be dispensed are outside the price bands determined by the least expensive product. Thus, it is expected that product competition among firms and relevant price adjustments take place within these substitution groups. At the general level, the effects of generic entry and its factors are well-known. The number of brand names in the market and expected revenues increase the number of entrants. In contrast, the number of generic firms already in the market, regulated markets with price caps and reference pricing systems with price convergence effects lower the number of entrants (Grabowski and Vernon 1992, 1996; Moreno-Torres et al. 2009; Reiffen and Ward 2005). The effects on aggregate market segments such as brand vs. generic products are not self-evident from the firms’ perspective as many large and dominant firms operate in the multimarket context, i.e. they have many drugs in many different therapeutic and molecule-based classes across the markets. Thus, established firms meet in several submarkets and they may find it profitable to redistribute their market power across the markets in which they are operating. Bernheim and Whinston’s (1990) multimarket rivalry theory (see e.g. Jens and Rosenbaum 1996; Pilloff 1999) predicts that firms engaging in price competition in several markets might find it optimal to transfer and redistribute market power from more collusive to more competitive markets. Firms in a multimarket coalition can increase their prices in more competitive markets as long as the pooled coalition constraint is respected (Coronado Saleh 2010). In this case, the average prices in the market are expected to be higher than in that in which multimarket contacts are irrelevant. However, the price variability or price uncertainty implications of multimarket contacts are not well-known in general. Our target is to combine the implications of stochastic linear demand models and the multimarket conduct of the same firms in many different markets. We argue that both increase price variability in the drug price submarkets. Generally, the presence of firms with many brand names and generics among multimarket contacts results in a complex playing field, meaning that firm and product market shares and prices do not necessarily decrease in the presence of generic substitution. Dominant firms can still easily operate, selling their brand and generic products across the submarkets (the “own generics” phenomenon, Hollis 2005; Mestre-Ferrandiz 1999). Also by adjusting their product prices in substitution groups in response to generics, they can maintain their average group-level market shares. Evidently, a policy change introducing generic substitution may leave market shares and contacts intact. As a result, policy-induced price reductions are less obvious, but price variability may increase in the substitution groups because the number of different firms in these submarkets varies and the firms aim to maximize their profits across the whole market.",
44,2,Journal of Industrial and Business Economics,30 May 2016,https://link.springer.com/article/10.1007/s40812-016-0033-x,"Traditional banks, online banks, and number of branches",June 2017,Stefano Colombo,,,Male,Unknown,Unknown,Male,"Internet banks are a quite recent and increasing phenomenon in the banking industry. The Internet banks (also known as on-line banks, or direct banks) do not have branches with tellers. In contrast, the customers have access to their money in several ways, including ATMs, web pages and mobile apps. The essential characteristic that differentiates the Internet banks from the traditional bricks-and-mortar banks is that the depositors have not to walk to physical branches in order to operate financial transactions (i.e. move money between deposit accounts, check accounts, pay bills…). Internet banks are now relevant players in the banking industry. In a recent study (TNS 2012), it has been shown that the US four major direct banks—Ally Bank, Discover Bank, Capitol One 360, and USAA—have gained share in the past decades among retail customers establishing their primary banking relationships, whereas the traditional banks have decreased their share. The deposits at the US major direct banks have more than doubled from 2008 to 2012, and their growth rate is three times the industry average in the period. Similar figures can be found in Europe (BLUECAP 2013). On the other hand, traditional banks are still significant actors. Traditional banks operate through physical branches. The branches have been traditionally seen as the principal place of interaction between the bank and its clients, and traditional banks willing to enlarge their customer base typically went through an expansion of the number of branches. While practitioners seem to suggest that the existence of Internet banks should induce a reduction in the number of branches of the traditional banks,Footnote 1 the empirical evidence is far from being univocal. For example, in the US, the number of branches increased by a significant 27 % in the period between 1994 and 2006, and it decreased only after that year (Hannan and Hanweck 2008). In France the number of branches increased constantly from 1996 to 2008, while it has remained almost constant after that year. Similar trends can be observed also in Spain and Italy, whereas an opposite trend—with a reduction in the number of the existing branches in the mentioned period—is observed in Germany, Sweden and UK (ECB 2013). That is, the picture is quite complex and it does not seem that the existence of on-line banks has been accompanied everywhere by a reduction of the branches of the traditional banks.Footnote 2
 The present paper discusses the optimal strategy of a traditional bank when it has to decide whether to operate with one or two branches and it is faced by an Internet bank that customers may have access to without incurring in transportation costs. In particular, we show that the mixed picture of the trend of the number of branches can be explained by the existence of countervailing incentives of the traditional banks, where the relative strength of these incentives is related to the heterogeneity of customers with regard to the disutility they sustain when accessing the direct bank. We build on Bouckaert and Degryse (1995) and Degryse (1996), and we develop a parsimonious competition model where the disutility of a depositor choosing the traditional bank depends on the physical distance between the customer’s location and the position of the branch in the space.Footnote 3 Therefore, the disutility that a depositor sustains when he deposits in the traditional bank depends on his location in the space and on the location of the branch. On the other hand, if a depositor chooses the Internet bank, he does not pay any transportation cost, as he can operate from everywhere without moving to a branch. At the same time, there is a source of (fixed) disutility when operating through an Internet bank, which is due to the absence of face-to-face interaction with bank’s employees. The depositors are heterogeneous with respect to this kind of disutility. For example, for customers who are familiar with the Internet, having no direct interaction with tellers does not imply a significant disutility cost, but the opposite is true for those clients which rarely use the Internet. Moreover, having a personal relationship with a banker may represent a relevant benefit for some people, especially for those customers that want to get personal financial advice,Footnote 4 whereas other customers are less worried about the absence of a personal relationship with bank’s employees. For these reasons, we distinguish between two types of depositors, type-Y (“young”) depositors, which suffer a low fixed disutility when they choose the Internet bank, and type-O (“old”) depositors, which suffer a high fixed disutility when they choose the Internet bank. Neither type-Y depositors nor type-O depositors sustain transportation costs when they choose the Internet bank. Therefore, the disutility that a depositor sustains when he deposits his money in the Internet bank does not depend on his location in the space, but it depends on which type of depositor he is. We consider a two-stage game where in the first stage the traditional bank decides whether to compete against the Internet bank with one or two physical branches, while in the second stage the banks compete through the deposit interest rates. We show that it may be optimal for the traditional bank to operate with one branch instead of two, even if building new branches is costless, depending on how many customers with a strong disutility from accessing the Internet bank (the “old” customers) there are with respect to the customers with a weak disutility from accessing the Internet bank (the “young” customers). In particular, we identify two driving forces behind the decision of the traditional bank about whether or not to open new branches when an Internet bank is a player in the market. On one hand, by opening a new branch, the traditional bank reduces the transportation costs that some customers have to sustain when going to the traditional bank. On the other hand, the Internet bank reacts to this strategy by increasing the deposit interest rate. As the deposit interest rates are strategic complements, the traditional bank also increases its own deposit interest rate, thus paying a higher interest rate to those clients that would go to the traditional bank even in the case of a single branch. The former effect dominates when there are many customers with a strong disutility from using the Internet: in this case, it is better for the traditional bank to have two branches. In contrast, when there are many customers with a weak disutility from Internet, the latter effect dominates, and the profits of the traditional bank are higher when it operates with only one branch. The literature on the determinants of the number of bank branches is quite scarce. Avery et al. (1999) and Damar (2007) focus on the impact of mergers between banks on the number of branches existing in a market by adopting an econometric perspective, and find that mergers of banks are associated with a reduction of the branches. Hannan and Hanweck (2008), by using a US banking industry panel data from 1988 to 2006, find that the number of branches is positively associated with the rate of return that the banks are able to obtain from their investments, and inversely related to market concentration. Kim and Vale (2000), by using a data set of Norwegian banks for the period 1988–1995, show that the banks act strategically in their branching decision, as they take into account the reaction of the rival banks to their strategic decision. Rice and Davis (2007) show that the deregulation of previously existing branch restrictions impacts positively on the creation of new branches. None of these papers however considers the role of direct banks in traditional banks’ branches proliferation or reduction. The present paper is also rooted in the literature discussing the applications to banking of spatial competition models.Footnote 5 Ali and Greenbaum (1977) generalize the Hotelling model to generate an equilibrium banking structure and an adjustment path reflecting a variety of industry characteristics, and test the predictions of the model by using US banking industry data. Kim et al. (2007) model banking competition through a generalization of the Salop model, where identical traditional banks compete via the deposit interest rate, the loan interest rate and the number of branches, and estimate the model with Spanish data in order to investigate the effects of deregulation of both interest rates and branches on the strategic conduct of the banks. Barros (1999) proposes a spatial model of multi-branch and multi-market competition based on the Salop circle: the theoretical framework is then applied to data of the Portuguese commercial banking industry. One of the main conclusions of the paper is that the measurement of the market power and the explanation of the margins in the banking industry must take into account the local and the spatial nature of the banking industry. We enlarge the existing theoretical literature on banking competition by including a non-spatial player (direct bank) with no branches that compete with a spatial player (traditional bank) adopting one or two branches. This paper is closely related to Bouckaert and Degryse (1995) and Degryse (1996). As these papers, we take the existence of the banking firms as given and we focus only on the liability side. Moreover, as in Bouckaert and Degryse (1995) and Degryse (1996), we take into consideration technology advancements (Internet banking) that allow reducing the transportation costs for the customers when they obtain financial services. However, in Bouckaert and Degryse (1995) and Degryse (1996) each traditional bank has to decide whether to introduce such technological improvements (like phone-banking, or Internet services) in its own offer. In contrast, we consider a situation where the innovative services are provided by a different bank (the direct bank), and we investigate the optimal branch decision of a fully traditional bank that offers its own services entirely through physical branches. The rest of the paper proceeds as follows. In Sect. 2 we introduce the model. In Sect. 3 we discuss the deposit interest rate equilibrium when the traditional bank has one branch only, whereas in Sect. 4 we consider the case where the traditional bank has two branches. In Sect. 5 we discuss the optimal choice of the traditional bank in terms of number of branches, by finding the conditions under which it is more profitable for the traditional bank to have two branches instead of one, and vice versa. In Sect. 6 we extend the basic model by allowing the transportation costs to be positively or negatively correlated with the customer’s type. Section 7 concludes. The proofs are relegated in the Appendix.",1
44,2,Journal of Industrial and Business Economics,17 November 2016,https://link.springer.com/article/10.1007/s40812-016-0058-1,The importance of trademark protection for product differentiation and innovation,June 2017,Dirk Crass,Franz Schwiebacher,,Male,Male,Unknown,Male,"Trademarks protect commercial signs that identify firms and their products and distinguish them from other undertakings. A positive empirical association of trademarks with firm value and performance is well documented (Millot 2009; Helmers and Rogers 2010; Schautschick and Greenhalgh 2013). Little evidence exists as to the sources of competitive advantages and superior performance deriving from trademark protection, however. Trademarks mark the introduction of new product varieties (Gao and Hitt 2012). They protect the public recognition and evaluation that is attached to brands of established products (Cohen 1991).This suggests that positive performance effects from trademarks are due to proliferating product variants and horizontal differentiation from competition. A favorable public assessment of firms relative to their competitors supports firms to attain and sustain superior financial performance and constitutes a valuable firm resource that may underlie an advantageously differentiated market position (Fombrun and Shanley 1990; Obloj and Capron 2011; Roberts and Dowling 2002; Semadeni 2006). This suggests that trademarks protect intangible assets that support firms to differentiate themselves from competitors and entrench market segments. We investigate therefore empirically whether firms’ use of trademarks fosters product differentiation. The use of trademarks appears more important for firm value and performance in service sectors than in manufacturing (Greenhalgh and Rogers 2007). Service production tends to exhibit more experience characteristics. Then the product characteristics are regularly difficult to verify for customers before services are delivered. If production exhibits experience characteristics, a favorable public evaluation is more important for customers’ choice among products and suppliers (Podolny 1994; Lemley 1999). This suggests that the fact that firms operate in services rather than in manufacturing is an important moderating factor for the relation between the use of trademarks and product differentiation. Landes and Posner (1987) ascribe a self-selection mechanism on firms that register trademarks as trademark protection is more important for firms that compete on quality characteristics. A favorable public reputation is crucial in order to leverage the benefits from delivering high quality products. Besides investments in favorable brand associations, product innovation is furthermore exemplary for vertical product differentiation (Sutton 1998). This suggests that trademark protection is more important for product-innovating firms and that trademarks complement other strategies to protect product innovation rents (see Hall et al. 2014 for a review). However, few studies on appropriability mechanisms take trademarks explicitly into account. Our empirical approach draws on 4453 observations obtained from the 2011 German innovation survey. Surveyed firms have been asked about their use of formal intellectual property protection and various qualitative dimensions of their competitive environment. We estimate generalized ordered logit models on firm’s assessment of product differentiation as characteristic for their competitive environment. The estimation samples distinguish between firms in manufacturing and service sectors and between firms without product innovation, firms with new-to-the-firm product innovation and firms with new-to-the-market product innovation, respectively. We find for firms in service sectors that the use of trademarks significantly increases (decreases) the probabilities of competitive environments characterized as fully or rather product-differentiated (characterized fully or rather by easy product substitutability). The same holds for service sector firms that introduce new-to-the-market product innovations. Trademarks exhibit no significant differentiating effects for firms without product innovations. This is consistent with conjecture that trademarks are performance-relevant for service sector firms because they protect horizontal product differentiation and support the protection of rents from vertically-differentiating product innovation. Regarding firms in manufacturing, the evidence on a product-differentiating effect from trademark protection is not conclusive. We proceed in the following way. The hypothesis development provides a brief review on the legal scope of trademark protection. We argue that trademark protection sustains valuable intangible assets by preventing rivals to free-ride on a favorable public recognition. We hypothesize that the differentiating capacity of trademark protection is more important in service sectors and for product-innovating firms. Section 3 describes the data, variables and methodology. Section 3.3 provides the econometric evidence and Sect. 4 concludes.",5
44,2,Journal of Industrial and Business Economics,18 November 2016,https://link.springer.com/article/10.1007/s40812-016-0062-5,Clustering strategy and development of subsidiaries in China,June 2017,Francisco Puig,Borja Portero,Miguel González-Loureiro,Male,Male,Male,Male,"World economic activity today is becoming increasingly integrated due to globalisation and rapid improvements in transport and communications (Buckley and Ghauri 2004). This situation highlights the increasing presence of multinational enterprises (MNEs) virtually everywhere and the importance of choosing the best location for each slice of their value chain (UNCTAD 2015; Dabic et al. 2014). Globalisation also highlights the importance of selecting the right entry mode according to the firm’s experience and the host country’s features (He 2003; Buckley and Ghauri 2004; Nishiyama and Yamaguchi 2013). MNEs normally decide to locate in areas where there is a high density of firms (agglomerations) such as industrial clusters (co-location of firms of related industries) and country-of-origin clusters (co-location of firms of the same country in the host country) (Crozet et al. 2004; Majocchi and Presutti 2009; Meyer et al. 2011; Alcácer and Chung 2013). Clusters entail a “hybrid” or intermediate organizational form that helps minimize problems deriving from size and control (Mudambi 2008). This is widely accepted as the main reason of the importance of clusters in international business (IB) literature. Another reason is that co-location with other firms belonging to the same or related industries helps the investing firm gain access to both local industry-specific knowledge and specialized industry-related resources, a requirement for success when entering a foreign market (Meyer et al. 2011; Wang et al. 2014). In other words, clustering can provide an important source of competitive advantage to both host and guest firms in the highly competitive environment of a specific area. This can reduce the handicap stemming from being abroad—the liability of foreignness. As a result of these positive spillovers, renovation and growth of the population of firms take place from the inside out, taking advantage of the flow of resources and business opportunities created by and in the local market. A more homogeneous space is then formed where there is an exchange of employees among the clustered firms, which share the same value system and fully understand the rules governing the business activity in the cluster (Puig and Marques 2010). The main objectives for setting up business in clusters abroad are diverse, including cost savings, market proximity and investment opportunity (Kedia and Mukherjee 2009; Yang et al. 2011). The survival of the binomial MNE-subsidiary relies increasingly on its fit with the location features (Coucke and Sleuwaegen 2008). This may take the form of transferring certain activities of its value chain to foreign locations despite host and guest nationalities being culturally distant (Coucke and Sleuwaegen 2008; Puig et al. 2014), such as is the case of Spanish firms in China. This process entails the internationalization of the production, the distribution and marketing of good and services, and even the organization of economic activities in the value chain. Literature on the conditions that must be met for the effective transfer of activities abroad has strongly relied on Dunning’s eclectic paradigm (Dunning 2006). This states that international production financed by foreign direct investment (FDI) and undertaken by MNEs is based on three drivers. First, there is a set of (net) competitive advantages stemming from the country-of-origin features and the position of the MNE to take advantage of them (O), both of which influence the investment motivation, namely, efficiency seekers versus market seekers. Second, there are the host-country location advantages (L) that lead the MNE to choose a given location for certain value-adding activities because that location offers the best conditions for undertaking those activities. Third, it considers the advantages derived from the internalisation (I) of markets and assets that MNEs tend to exploit, which partly shape the dynamics of the ownership structure, such as evolving from an international joint venture to a wholly-owned subsidiary. Since the writings of Alfred Marshall (1890) there have been numerous studies on the “location effect” (spillovers and advantages of clustering) impacting on host-country firms. However, the study of this effect on foreign MNEs has received scant attention to date (McCann and Folta 2008; Dunning 2009; Jean et al. 2011). This is noteworthy because of the different processes and institutional changes taking place in China (Li and Park 2006; Bellandi and Caloffi 2010): while inward FDI in China is expected to keep following an upward trend in the coming years (Davies 2013; Dencik and Spee 2014), the patterns of location may shift due to new policies to locate FDI outside the usual industrial parks for foreign MNEs (Bellandi and Caloffi 2010). Furthermore, the problems and failures that some firms have experienced deserve further attention. Examples are the Spanish firms Orbea and Fagor who decided to disinvest and/or close their factories in China in March 2015.Footnote 1
 Hence the objective of this paper is to explore the association between decisions to locate in clusters and decisions about whether or not to reinvest in subsidiaries. For methodological reasons, a sample of 31 Spanish subsidiaries located in China was chosen for the empirical study. Our contribution helps clarify whether the decision of location is more associated with issues related to the MNE’s strategic objective to invest in the host country (O issues), to locational advantages (L issues) such as the existence of a cluster, or to internalising issues (I issues) linked to the entry mode. Our results help clarify the potential influence of this portfolio of decisions—entry mode, host-country location type, economic activity—on the outlook of reinvestment decisions in subsidiaries in foreign countries in future studies. The article is organized as follows. In Sect. 2, a literature-based theoretical framework of location and clusters in emerging countries is provided based on findings from both research streams. Section 3 details the empirical study addressing the mode of entry, location and types of activities chosen by Spanish firms when entering China. In the last two sections, implications and conclusions are discussed.",2
44,2,Journal of Industrial and Business Economics,05 January 2017,https://link.springer.com/article/10.1007/s40812-016-0068-z,The intensity of business R&D in Italy: why reducing the gap with the EU is possible and worthwhile,June 2017,Alessandro Sterlacchini,,,Male,Unknown,Unknown,Male,"Business companies are involved in an array of innovative activities: R&D, patenting, acquisition of licenses, adoption of ICT and innovative machinery. Recent literature has also emphasized the role played by other activities giving rise to the so called soft innovations, such as pre-production developments, designs and trademarks, organizational changes and new marketing tools. In spite of this variety, R&D investment remains a core variable for many reasons. First of all, very seldom R&D performing firms do not significantly invest in other innovative activities too. Moreover, R&D investment is strongly correlated with that on human capital (approximated by the level of education of firms’ employees). Secondly, to carry out R&D in-house is necessary not only to introduce relevant innovations but also to exploit the inventions or absorb the new pieces of knowledge generated by other private or public subjects. For instance, an effective process of technology transfer from universities to private firms is quite unlikely when the latter do not own some research competencies. Finally, R&D activities generate positive externalities. However, the limited appropriability of research outcomes, one of the main justifications for R&D policies, should not be emphasized too much. In fact, if the absorption role of R&D is recognized, only the firms involved in research will be able to take advantages of the results achieved by others. In this sense, the generation of relevant innovations is, tacitly or intentionally, a collective process through which R&D externalities are somehow internalized. Especially for ambitious and expensive projects, characterized by uncertain long-term returns, such a process it very often intentional as witnessed by the increasing use of collaborative research, R&D joint ventures and open innovation systems. For all the above reasons, business R&D remains a fundamental pillar of a national innovation system, not matter whether the country is a technology leader (like Germany in the EU) or follower (like Italy or Spain). In this note, stimulated by the policy contributions recently hosted by Economia e Politica Industriale (Issue 3, 2016) as well as previous articles published in the journal, I will focus upon the level, trend and size distribution of business R&D expenditures in Italy. In Sects. 2 and 3, also on the basis of international comparisons, I shall contend that the main reason of the Italian backwardness in terms of business research intensity is due to the unsatisfactory and declining contribution of the largest firms. If the Italian performance has not deteriorated over the last years, this has been due to an increasing number of SMEs performing R&D as well as a group of not too large companies (having between 250 and 499 employees) which have invested more resources in this activity. In the light of this positive phenomena as well as the need of inducing a change in the behaviour of the largest companies (both national and foreign), Sect. 4 contains some policy considerations.",1
44,2,Journal of Industrial and Business Economics,17 February 2016,https://link.springer.com/article/10.1007/s40812-016-0029-6,The effects of corporate social responsibility on entry,June 2017,Luciano Fanti,Domenico Buccella,,Male,Male,Unknown,Male,"The widely observed adoption of firms’ corporate social responsibility (CSR) behaviours has recently, and more frequently raised the debate among scholars (e.g. (Baron 2001, 2009; Benabou and Tirole 2010; Goering 2007, 2008; Jensen 2001; Lambertini and Tampieri 2010). Actually, according to KPMG (2011, 2013), CSR behaviours are undeniably a prevailing global business practice: in 2013, 71 % of 4100 companies surveyed in 41 countries have reported the realisation of CSR activities. This figure represents an increase of 7 % points since 2011, when 64 % of the companies surveyed issued CRS reports. The conventional wisdom as regards the welfare effects of the introduction of CSR rules in the firms’ behaviour argues that, in general, increasing social concern or responsibility of the firms is necessarily welfare improving. For example, this is the view prevailing in the European Union and between most scholarsFootnote 1. In this paper we show that this conventional wisdom does not necessarily hold true, since the effects of adopting CSR behaviours on social welfare may be also dependent upon the long run effects of such adoption on the market structure and the shape of competition.
 In particular, provided that the market size is adequately large, entry can be more easily deterred by the existence of social concerns in the industry, and the higher the social concern the less likely is the entry. On the other hand, when the social concern is relatively high and the consumers and social welfare would be higher under duopoly, then such a choice, by impeding entry, results to be welfare damaging, despite it aims to protect the consumers’ interests. These results are in contrast to the conventional wisdom. Therefore, these findings may be important for the current policy debate on firms’ social concern and of particular interest for antitrust and competition policies. The rest of this paper is structured as follows. Section 2 introduces the monopoly and duopoly models with CSR rules and the analysis discusses the issue of entry in this context. The last section concludes our findings.",13
44,3,Journal of Industrial and Business Economics,02 December 2016,https://link.springer.com/article/10.1007/s40812-016-0066-1,How do governance factors affect inefficiency? Stochastic frontier analysis of public utility firms in Japan,September 2017,Fumitoshi Mizutani,Eri Nakamura,,Male,Female,Unknown,Mix,,
44,3,Journal of Industrial and Business Economics,13 April 2017,https://link.springer.com/article/10.1007/s40812-017-0071-z,Comparing feed-in tariffs and renewable obligation certificates: the case of repowering wind farms,September 2017,Teresa Romano,Tim Mennel,Sara Scatasta,Female,Male,Female,Mix,,
44,3,Journal of Industrial and Business Economics,08 August 2016,https://link.springer.com/article/10.1007/s40812-016-0053-6,Unilateral effects of partial acquisitions: consistent calculation of GUPPI under horizontal merger guidelines within the EU,September 2017,Panagiotis N. Fotis,Michael L. Polemis,Konstantinos Eleftheriou,Male,Male,Male,Male,"The adverse unilateral price effects in the U.S. Horizontal Merger Guidelines (2010) (see DoJ and FTC 2010) rely primarily on the value of sales diverted from one product to another after an increase in the price of one of the products of the merged entity. Section 6 of the Horizontal Merger Guidelines (2010) deals with several common types of unilateral effects of which none of them assumes Herfindahl–Hirschman Index (HHI) and market shares of the merging firms to be appropriate tools for evaluating merger specific unilateral effects. Particularly, Section 6.1 of the Horizontal Merger Guidelines (2010) states,  inter alia, that “...[T]he Agencies rely much more on the value of diverted sales than on the level of HHI for diagnosing unilateral price effects in markets with differentiated products. If the value of diverted sales is proportionately small, significant unilateral price effects are unlikely.[...]Where sufficient data are available, the Agencies may construct economics models designed to quantify unilateral price effects resulting from the merger...These merger simulation methods need not rely on market definition...”. Within the European Union (EU), the Council Regulation Horizontal Merger Guidelines (2004) (see OJ C 31/5, 2004) are based on a number of factors which may influence unilateral effects (or non-coordinated effects in the guidelines’ jargon), if the latter are likely to arise from a merger. One of them is the market shares of the merging firms. According to Paragraph 27 of the Council Regulation Horizontal Merger Guidelines (2004) “....[T]he larger the market share, the more likely a firm to possess market power. And the larger the addition of the market share, the more likely it is that a merger will lead to a significant increase in market power.” (OJ C 31/5, 2004, para 27, p. 8). Moreover, with respect to mergers in markets with differentiated products, even though the Council Regulation Horizontal Merger Guidelines (2004) state that “[t]he higher the degree of substitutability between the merging firms’ products, the more likely is that the merging firms will raise price significantly.” (OJ C 31/5, 2004, para 28, p. 8), they do not explicitly state that HHI and market shares of the merging firms are not appropriate tools for evaluating unilateral effects of horizontal acquisitions. Therefore, in most merger cases the Agencies within EU use HHI and the market shares of the merging firms to evaluate merger specific unilateral effects, even in markets with differentiated products. This contradicts the economic theory and as a consequence lawyers and policy makers find difficulties in supporting merger cases in the courts. The motivation of this paper is to derive a more consistent formula of the Gross Upward Pricing Pressure Index (GUPPI) with Council Regulation Horizontal Merger Guidelines (2004) for assessing the unilateral effects of partial acquisitions. For this purpose we (i) express the amount of acquired equity stake as a function of the market share of the victim firm and (ii) assume a logit demand function. Regarding the former, the amount of acquired equity stake in a rival is an increasing function of the acquired firm’s market share. Following Willig (1991) firms decide to link the amount of acquired equity stake with respect to the market share of the victim firm since the greater the market share of product 2 the more attractive the product is in the eyes of the consumers and the greater the probability to be acquired by the owner of product 1. In other words, the greater the attractiveness of product 2, the higher the number of marginal consumers who will divert to it after an increase in the price of product 1. Therefore, under this view, victim’s market share plays a crucial role in determining the degree of consumer diversion from product 1 to product 2. Moreover, the partial acquisition increases the profitability of the acquiring firm depending on the market share of the victim firm.Footnote 1
 The rationale behind the use of a logit demand function to assess the unilateral effects of partial acquisitions stems directly from Paragraph 28 of the Council Regulation Horizontal Merger Guidelines (2004). According to it “...a merger between two producers offering products which a substantial number of customers regard as their first and second choices could generate a significant price increase.”. Under a logit demand function, the greater the market share of product 2, the greater the number of consumers of product 1 who value product 2 as the second best choice (assuming that product 1 is the best choice for the consumer). The results of this paper show that the anti-competitive effects of partial acquisitions are higher when equity stakes are expressed as a function of the market share of the acquired firm than under constant equity stakes. Particularly, we show that the GUPPI is downward biased when the percentage of equity stakes of the acquirer in the target firm is assumed to be constant. Expressing the acquired equity stake as an increasing function of the acquired firm’s market share implies that given the market share of the acquiring firm, the higher the market share of the acquired firm, the higher the market share of the merged entity. The paper is organized in the following way. Section 2 reviews the literature and Sect. 3 presents the basic model. In Sect. 4, we derive the formula of GUPPI and provide some basic comparisons between GUPPI with and without constant equity stakes. Lastly, in Sect. 5 we conclude the paper by providing some policy implications.",4
44,3,Journal of Industrial and Business Economics,07 June 2017,https://link.springer.com/article/10.1007/s40812-017-0073-x,"Innovation, skills and investment: a digital industrial policy for Europe",September 2017,Harald Gruber,,,Male,Unknown,Unknown,Male,"Europe’s sluggish growth performance and high unemployment rates resulting from financial crisis are seriously worrying policy makers and economists. It is not a surprise that industrial policy is now high on the political agenda. In particular, there is a revival of interest in “re-industrialisation” in consideration of the well proven productivity growth effects manufacturing is able to provide.Footnote 1 Indeed, many countries have formulated their own strategies of industrial policy and the European Commission has set “industrial revival” as a target: manufacturing in total value added should account again for 20% of GDP by 2020 (European Commission 2014). This appears particularly challenging as currently this measure is down at 15%. The relevant questions therefore are whether the appropriate target has been identified and whether this is not backward oriented. The twentieth century was great for product innovation for very well identified items such as cars, aircraft, radio, TV and computers, as the related technologies led to very high labour productivity growth. But in the twenty-first century the prevailing technologies are digital, with mostly intangible goods or services that are not comparable with the tangible world of the manufacturing industry. Also the production factors change, as they are increasingly of non-tangible nature, in particular knowledge. Any meaningful industrial policy measure should acknowledge this fact and identify any market failure that need to be compensated for. Any policy attempt of boosting measured value added share of the manufacturing industry has been proven as particularly challenging: first, because the traditional industrial policy objectives of promoting particular sectors or companies are difficult to achieve politically in terms of domestic consensus; second, because of external constraints posed by trade agreements and international organisations. At the same time, measurement issues may not fully account for the actual value added as many functions have been absorbed by the expanding service sector. However, manufacturing success is unlikely to lead to net employment growth in the sector. Productivity growth in manufacturing industry typically outpaces growth in demand for products, shrinking direct employment in the manufacturing sector (Peneder and Steicher 2016). Employment growth is expected to rather occur in the non-traded service sector, to the extent that this is able to support the tradeable sector in an efficient manner (Summers 2013). The sectors with strongest expected labour demand growth are health care and social assistance, not least because of the increase in average age of population. Other growth sectors identified are local government, construction and education. They are all mostly related to the public sector and productivity increases may be achieved by increased recourse to information and communication technology (ICT). An efficient non-tradeable and public service sector capable of providing the key resources for a knowledge based economy is therefore important to effectively support a competitive manufacturing industry. In particular, a good education system is essential to provide individuals with skills to update specific knowledge over a lifelong learning horizon. This long term trend of decline in manufacturing does not undermine the argument that the manufacturing sector is essential for productivity growth. However, one needs to take into account the structural shifts that have occurred. The future manufacturing environment needs to be rich with knowledge assets. Several propositions are now advocating reindustrialization through industry-oriented ‘integrated’ policies (Aiginger 2014; Andreoni and Chang 2016). This should be achieved at European level by mainstreaming European sectoral policies. Countries have typically defined their national strategies for industry centred on their main factor of production, leveraging thereby the so called “competitive advantages”. As these can significantly differ across European countries, the policy preferences of countries may get into conflict. For instance, different labour market legislations may induce completely different recommendations for industrial policies aimed at furthering international competitiveness. This pro-activist stance for labour market flexibility is represented by the success of German industry in export markets in the aftermath of significant labour market reforms undertaken by government at the beginning of the century (Rinne and Zimmermann 2012). Other countries, in particular France and Italy, are unable to elaborate a policy consensus on reform and are struggling with introducing similar measures that lead to better cost competitiveness of industry. Labour market reform is only one of the challenges that industrial policy has to tackle. Technological change is another factor that has a differentiated impact on countries. All countries have in common that industry is undergoing a deep transformation, driven by the pervasiveness of digital technologies. The impact of digital technologies and their applications is no longer confined to early adopters in key sectors, such as telecommunications, electronics and automation. The internet, e-commerce, mobile broadband, social media and big data have started to penetrate all sectors and countries benefit to a varying degree of them, depending on the relative size of the sectors and the innovation absorption capacity of the industrial structure. New buzz-words have been coined such as “Industry 4.0” referring to the fourth industrial revolution: after the steam engine, electricity and robotics automation, it is now the turn of digital technologies to connect devices in an integrated industrial setting to set the scene for future manufacturing opportunities.Footnote 2 In the context of economic activities, this induces a fundamental change. The Internet enlarges the information sphere and allows for an almost immediate match of demand and supply of information. This poses challenges for traditional industrial policy: firms are increasingly operating on online platforms and can cater for a large variety of real or virtual services both for suppliers and for buyers. Such firms become able to exploit a position in the middle of a multi-sided market setting and thus shape the conditions under which the markets function. As information is the “raw material”, the ensuing low transaction costs challenge many of the main tenets of the traditional theory of firm, blurring also the boundaries of producers and consumers. Because the economic value of firms involved in the supply of such services does not depend on the actual sales, but rather the perceived value and potential of the network, an astonishingly rapid growth of indicators such as market capitalization could be observed. Several internet based companies that did not even exist 10 or 20 years ago are now in top positions in the world rankings of market capitalisation, ahead of industrial companies. US based companies are here the undisputed leaders with perhaps some exceptions. Europe is occupying lower ranks, certainly not in line with the high level of welfare that the region is currently commanding on a global scale. The problem for Europe is that the digital sector is relatively small compared to for instance the US. Europe has been reluctant to adopt digital technologies in the industrial sectors and to adapt their production processes to fully benefit from the network effects that come along with digital technologies. The adoption of digital technologies becomes a distinctive industrial policy goal and the failure to pick up this challenge could have wide-ranging economic consequences. In any case, it is most likely to undermine the sustainability of the European model of welfare society. This paper illustrates industrial policy measures, in particular by taking advantage of the digital technologies, to tackle the lack of economic growth performance in the Europe. The paper is organised as follows. Section 2 illustrates the evolution and role of the manufacturing industry in the European economy, showing the marked trends of deindustrialisation and slowdown of total factor productivity. Section 3 discusses the lag in digitalization of the European economy, along with the reasons for this and the consequences thereof. Section 4 looks at market failures that may emerge in the digital economy and how the slow down the speed of digitalisation. Section 5 provides some suggestions for a new approach to industrial policy centred on the efficient adoption of digital technologies and the resources required. Section 6 concludes.",17
44,3,Journal of Industrial and Business Economics,18 October 2016,https://link.springer.com/article/10.1007/s40812-016-0061-6,"SMEs’ growth in international markets: export intensity, export diversification and distribution strategies",September 2017,Eleonora Di Maria,Roberto Ganau,,Female,Male,Unknown,Mix,,
44,3,Journal of Industrial and Business Economics,02 December 2016,https://link.springer.com/article/10.1007/s40812-016-0064-3,Corporate social responsibility in a game-theoretic context,September 2017,Luciano Fanti,Domenico Buccella,,Male,Male,Unknown,Male,"The increasing presence of firms adopting Corporate Social Responsibility (CSR) 
behaviours has been observed worldwide. Indeed, according to KPMG, CSR behaviours are surely a dominant global business practice: in 2015, 73% of the top 100 companies surveyed in 45 countries have reported the realisation of CSR activities, demonstrating an increase of 9 percentage points since 2011, when 64% of the companies surveyed had issued CRS reports. Moreover, 92% of the 250 Global Fortune Index companies, the world’s 250 largest companies, have performed CSR actions (KPMG 2011, 2015). The economic literature has recently tried to offer explanations, usually beyond the standard short-term profit maximization, to justify the growth of this phenomenon (e.g. Benabou and Tirole 2010). Nonetheless, it is widely observed that firms adopting CSR may present extremely different levels of engagement in their “socially concerned” activities; moreover, the simultaneous presence of both CSR and non-CSR firms within the same industry occurs. For example, according to the Reputation Institute (2015), in the automotive sector, two German carmakers, BMW and Daimler, rank among the top 10 companies with the best CSR reputation (1st and 3rd place, respectively) with respect to the Japanese Toyota (35th) and Honda Motor (61st), US companies Ford (65th) and General Motors (96th), and the absence of the top 100 ranked companies of other giants in the industry such as French companies Renault and PSA Peugeot Citroën, Korean manufacturers KIA and Hyundai, and the FCA Group. This fact is not immediately intuitive and promptly explained. It can be easily shown (as this work does) that a firm, in most cases, may not have any convenience in following CSR rules (for example, taking care of the consumers’ welfare), because profits would be reduced in the case of oligopoly. Hence, if the adoption of CSR rules is unprofitable, it is puzzling to explain the widespread (although not universal) presence of a socially concerned firm in several industries (unless not strictly profit maximizing reasons are considered). The present paper contributes by providing an explanation in “strategic” terms of the multi-faceted phenomenon of CSR and complementing the different perspectives already identified in the recent industrial organization literature. One line of research in economics recognizes CSR as private provision of public goods and private elimination of a public bad (Bagnoli and Watts 2003; Kotchen 2006; Besley and Ghatak 2007). A second line of the research has focused on the motives that lead companies to adopt CSR behaviours (Baron 2001, 2009; Porter and Kramer 2006; García-Gallego and Georgantzís 2009; Reinhardt and Stavins 2010; Benabou and Tirole 2010; Lambertini and Tampieri 2010; Kopel and Brand 2012). In particular, the contributions of Baron (2001, 2009) highlight that companies may strategically follow CSR rules simply guided by their pure selfishness, where CSR is a response to social pressure directly exerted by social activists—e.g. such as Non-Governmental Organizations (NGOs), defined by Baron (2001) as actors in “private politics”—especially through threats (boycotting, negative propaganda) or rewards (endorsements). Moreover, according to Baron (2009), NGOs can also distinguish between morally motivated CSR and CSR induced by social pressure, and thus they tend to target their campaign against morally managed firms, because they are more vulnerable to the campaign than the self-interested firms. On the other hand, from the view of Benabou and Tirole (2010), CSR is a pro-social behaviour resulting from several interacting motivations, from pure altruism to material motivation, social and self-esteem concerns. Moreover, as argued by Reinhardt and Stavins (2010), among others, CSR may be a strategy like any other form of product differentiation if there are customers—identifiable by firms—willing to pay for ethical goods which may be profitable: the opportunity arises, for instance, because of the presence of asymmetric information, economies of scale, intellectual-property protection, and the successful defense of the resultant niche against imitators, according to basic economics. More interestingly, García-Gallego and Georgantzís (2009) consider the role of the “pressure” exerted by heterogeneous consumers in a vertically differentiated duopoly with firms’ strategic choice of their degree of corporate social responsibility. Those authors study the effects of an increase in consumers’ willingness to pay for products sold by socially responsible manufacturers. The consumers’ preferences towards CSR behaviours increase the demand; however, a trade-off exists because the adoption of CSR rules implies a quadratic cost. They find that shifting towards socially responsible consumer preferences is, in most cases, welfare-improving. Conversely, it can be welfare-reducing when such an increase changes the market structure. Therefore, the above-mentioned literature, and in particular García-Gallego and Georgantzís (2009), examine the demand-side (the consumers’ or NGOs) CSR preferences while the current work focuses on the supply-side (the firms’) CSR preferences in the sense that supplied quantities are chosen by firms to maximise also consumers’ interests. Moreover, in a recent paper to appear in this journal, Fanti and Buccella (2016) investigate the effect that the incumbent’s CSR adoption can have on a potential entrant decision. On the other hand, the appearance of universal adoption of CSR behaviours in an industry, leading to a “prisoner’s dilemma” situation, has been identified by Kopel and Brand (2012), which seems to be the unique paper dealing with this strategical analysis, to the best of the authors’ knowledge. In the current paper, we examine if a correct game-theoretic approach to this issue may explain the above-mentioned stylised facts; that is, the emergence of different equilibria in different industries (i.e. all firm adopt CSR rules, all firms do not follow CSR rules or asymmetric equilibria). Making use of a simple Cournot duopoly model with identical firms and differentiated products, we show that the strategic nature of the adoption of CSR rules may explain the occurrence in the equilibrium of situations in which (1) both firms follows CSR rules, (2) both firms do not follow CSR rules, (3) asymmetric equilibria in which one firm adopts CSR while the rival does not, and (4) multiple symmetric equilibria in which both firms may decide whether to adopt CSR, depending on the degree of product differentiation. On the other hand, in the case of a Bertrand duopoly (analysed in the “Appendix”), we find that profit maximization is the dominant strategy for firms. In other words, unilateral deviation from the profit maximisation rule is always unprofitable for firms. The rationale for this result is rather intuitive: given that under Bertrand the nature of competition is in strategic complements, the unilateral adoption of CSR behaviours (taking care of consumers’ surplus) leads the firm to produce a larger amount of output than the rival, with a negative impact on the firm’s price and profits. However, in the presence of strategic complements with complement goods, if the social concern is not too strong, there is an area in the parameter space in which the overall adoption of CSR rules payoff-dominates the profit maximisation equilibrium; that is, firms are cast into a classical prisoner’s dilemma game. We find that the adoption of CSR rules always implies a more intensive product market competition, under both the Cournot and Bertrand conjectures. However, the role of the product differentiation is crucial. Under Cournot competition, with substitute goods, the reaction functions have a standard downward sloping shape: goods are strategic substitutes. In the presence of complement goods, the reaction functions become upward sloping, and goods behave as when strategic complements. In both cases, the increasing weight attached to the stakeholder-consumers’ interests leads the socially concerned firms to an aggressive market behaviour via output expansion, which is stronger with substitute than with complement products. On the other hand, under Bertrand competition, with substitute goods, the reaction functions are upward sloping (strategic complements), while with complement goods, the reaction functions become downward sloping, and goods behave as when strategic substitutes. Also in these cases, an increase of the social concern level yields aggressive product market behaviour of the CSR firm through a price reduction, which is more intense with complements than substitute products. Thus, these results may be of great interest for the current policy debate on firms’ social concern. Concerning social welfare, under Cournot competition, the firms’ strategic interaction leads to the most desirable welfare outcomeFootnote 1, when the goods are sufficiently substitutes and sufficiently complements and, in particular, if the level of social concerns are adequately low. However, while the former case is in contrast to firms’ interests, in the latter case the firms’ and social interests coincide. Therefore, when the emergent equilibrium is the best achievable and the interests of all the agents (both firms, consumers and social welfare as a whole) coincide, the best achievable outcome can be also defined as the Pareto-superior outcome. Nonetheless, there is a wide area of the relevant parameter space characterised by low-medium degrees of goods’ substitutability/complementarity in which universal profit maximisation arises as endogenous equilibrium, driving the economy to the third best achievable social welfare outcome.  On the other hand, in the case of Bertrand competition, product market interactions always determine the rise of universal profit maximisation as endogenous equilibrium, and this outcome generates the third best achievable outcome in terms of welfare for a wide set of the parameter space. The remainder of the paper is organized as follows. Section 2 presents the basic ingredients of the duopoly model, with firms deciding whether to adopt CSR rules. Section 3 describes the analysis of the game equilibria. Section 4 discusses the implications in terms of social welfare. Finally, Sect. 5 concludes with a brief discussion of the main results and an outline of the future research directions.",32
44,4,Journal of Industrial and Business Economics,03 October 2017,https://link.springer.com/article/10.1007/s40812-017-0080-y,"Introduction to the special issue on Giacomo Becattini, industrial economics, and local development",December 2017,Marco Bellandi,Joan Trullen,,Male,Female,Unknown,Mix,,
44,4,Journal of Industrial and Business Economics,22 September 2017,https://link.springer.com/article/10.1007/s40812-017-0077-6,Co-operation and competition in production and exchange: the “district” form of industrial organization and development,December 2017,Sue Konzelmann,Frank Wilkinson,,Female,Male,Unknown,Mix,,
44,4,Journal of Industrial and Business Economics,11 October 2017,https://link.springer.com/article/10.1007/s40812-017-0082-9,New forms of industrial districts,December 2017,Marco Bellandi,Lisa De Propris,,Male,Female,Unknown,Mix,,
44,4,Journal of Industrial and Business Economics,04 October 2017,https://link.springer.com/article/10.1007/s40812-017-0079-4,"Districts, multinationals and global/digital networks",December 2017,Davide Castellani,Enzo Rullani,Antonello Zanfei,Male,Male,Male,Male,"In the seventies, the crisis of the mass production model named “Fordism” has forced scholars and practitioners to concentrate their attention on the role of “territories” where the generation of economic value occurs (Burrows et al. 1992; Jessop 1992). A greater emphasis has been placed on such expressions of the real world as: the industrial districts in Italy; just-in-time supply circuits orchestrated by the customer, in Japan; urban and regional innovation clusters, such as the Silicon Valley in California and the Glenn Valley in the UK. Territorial ecologies are complex systems of complementary structures and resources inherited from the history of the site. They involve businesses and people who interact on the basis of a set of social, cultural and economic factors facilitating communication and transactions, including: trust, a tacit sharing of meanings and languages, networks of contractual links and of social relations among the different participants. The competitive success of these ecologies in reacting to the crisis of Fordism has persisted for a relatively long period of time (roughly from 1970 to 2000), revealing ways of generating value that can hardly be consistent with conventional theorizing in economics. In Italy, Giacomo Becattini—along with several others (including Brusco, Fuà, Bagnasco and Messori)—has given a fundamental contribution to a first silent and then more and more visible revolution in industrial economics, by placing the role of industrial districts at centre stage in his analysis. This Journal is proud to have hosted, since the early 1980s, several essays in which Becattini’s different way of thinking about economics emerges with great lucidity (Becattini 1984, 1985, 1990; Becattini and Rullani 1993). In these seminal contributions, he has brought to the forefront the generative capacities of individuals and of their sets of relations in local society, as the driving forces of the new post-fordist production systems. The key point is that this vision of production—where individuals and local societies have an active role that makes each ecology unique—goes beyond the conventional dualism of markets and hierarchies. That is the idea that industrial organization can be reduced to two basic forms: the competitive market, where equilibrium prices are set to select the optimal choices of many independent operators; and the hierarchy of large enterprises, which, with its own command, governs and finalizes the behavior of its employees. In the traditional view, prevalent until the seventies, tertium non datur: modern capitalism is expressed through a mix of market and hierarchy, giving a residual role to all other forms of production, considered ineffective and bound to be overcome. Instead, other organizational patterns have proliferated and their competitive strength, well rooted in history, has become particularly apparent in the 1970s. In Italy, in particular, the demand for flexibility—in response to the turbulence of the markets and to the rigidity of the hierarchies—fueled the growth of polycentric systems of widespread entrepreneurship, localized in places which had long specialized in given sectors (industrial districts). To the most careful observers, this evolution—dominating the eighties and nineties—shows that market and hierarchy do not exhaust the possible forms of modern production, but a third form is possible. In particular, in the case of Italian industrial districts, this third form relies on the cultural, historical, and solidarity based resources of local societies, which play a key role as productive forces, capable of using modern techniques, and at the same time managing complex situations. It is an organizational form that best expresses its competitive potential in all cases where it is necessary to elaborate flexible, intelligent and shared solutions to problems that are largely unforeseen and non codified (and often un-codifiable). In other words, problems that market prices and prescriptive programs have poor ability to handle. A clear conceptualization of such phenomena emerges only gradually in the theoretical reflection of the 1970's. Initially, what captures the attention of scholars is the outsourcing process that reverses a previously observed trend of large corporations to internalize all functions and production phases, in the logic of maximum vertical integration. Outsourcing means that the big organization focuses on a particular core business on which it invests to maintain its competitive edge. Skills, services and investments are decentralized to external parties, building a stable collaboration network that allows the chain to generate value in an efficient and flexible way at the same time. From this perspective, the third organizational form that accompanies the market and the hierarchy is contractual networking, based on agreements, alliances and trust-based linkages that are necessarily more heterogeneous and personalized than those characterizing “pure market” transactions. Network relations rely on a set of transactions whose nature needs to be adapted to the tasks to be handled. Fixing an equilibrium price (through market exchanges) or defining a program dictated by the organizational command (as in a hierarchy context) may not be effective when it comes to tackling problems that are not clearly defined ex ante, imply the combination of dissimilar but complementary resources (Richardson 1972) and require mutual trust between the parties involved. The network between interconnected operators implies transactions of a different nature. The theory of transaction costs—which at the beginning focused on the market/hierarchy alternative (Williamson, 1975)—adopted an “institutional” interpretive key, which considered market, hierarchy and networking as different and complementary ways of organizing transactions (Williamson 1985). The enterprise can then be seen as a nexus of treaties (Aoki et al. 1990), i.e. as a system that utilizes various contractual forms to connect the parties involved in production both within the enterprise (business units and stakeholders), and in external relations (suppliers, distributors, partners and competitors). This organizational form—networking—is not entirely new: in order to define its characters, one can draw inspiration from the model, elaborated in particular by the Swedish school of industrial business, looking at the collaborative forms between suppliers and buyers in all cases where a relationship of mutual interdependence is established (Hakansson and Johanson 1993; Hakansson and Snehota 1989). However, it is noteworthy that networking through which the division of labor occurs, tends to concentrate in specific places that are able—more than others—to attract and self-generate activities related to a certain sector or function. Thus, some other local and non-ubiquitous drivers are in action—and help organise contractual networking on a territorial basis. In fact, empirical observation shows that, in the post-Fordist network, not all sites are the same: some involve more numerous relationships, they catalyze more capital flows, talents, and commodities. These more intense linkages reflect the amount and quality of knowledge embedded in those places, and their capacity to use specialized work and skills. Hence, the post-Fordist contractual networking nodes are not equally distributed in the geographic space of the national and global economy, whereas more complex and intense relationships can be observed in a number of localized clusters (Porter 1998). This is not just the effect of proximity relationships, which favor location and co-location decisions. There is a lot more here: attractiveness is indeed a function of the distinctive quality of the context in which each person or each enterprise lives and works. A territorial context rich in craftsmanship and of historical tradition, translated into a high entrepreneurial propensity, plays a completely different role from standard agglomeration economies. The latter are normally generated by the concentration of dynamic consumers on the demand side, of advanced research at the frontier in key technological fields (such as computing, mechatronics, biotechnology), or the availability of a market for specialized labour and production inputs on the supply side. On top of these important attractors, one should emphasise a more comprehensive and powerful division of labor between territories that relies on the distribution of skills, functions and processes in relation to the unique quality of the context where production and social life occurs. In the case of Italian industrial districts, localized clusters are of a sectoral nature, because vendors and contractors in the same sector focus on a few square miles of space, thus communicating and interacting without difficulty to handle complex situations that are largely unforeseeable in advance. In Japan, geographical clusters reflect the connective role played by large system integrators mobilizing networks of small or medium-scale suppliers. In the field of information technology yesterday, and the web economy today, the places of excellence are dictated by the recent history of the most successful innovations and “champions”, which generate knowledge externalities and a technological environment conducive to experimentation and innovation. Similar conditions apply in the case of logistics: airports, harbors, rail and road nodes with the highest traffic create territorial convergences between users, and generate network externalities. In many urban centers that have strong attractiveness in the field of creativity, fashion, and artistic expression, network consolidation reflects the need for many companies in the sectors concerned to maintain direct contact with embedded knowledge in the different territories of excellence (Florida 2005; Glaeser and Gottlieb 2009). Clusters may also derive from the geographic distribution of R&D activities in particular fields and in specific regions or cities, based on cognitive pre-existence (embedded in place) or R&D investment programs in particularly attractive technologies. MNEs distribute their R&D investments according to the peculiarities of the countries where they are present, focusing on certain R&D activities in a specific local cluster, to take full advantage of the differences that characterize them. Thanks to their belonging to a transnational structure, R&D labs can participate to a division of labor and are enabled to share useful knowledge flowing through cognitive pipelines managed by the multinational through its internal lines (Castellani and Zanfei 2006; Meyer et al. 2011; Cano-Kollmann et al. 2016). The local nodes participating in this global division of labor tend to be characterized by a specific “atmosphere”, in terms of both cultural values and “tacit” or contextual knowledge accumulated over time and made available to people and companies active in the area (Becattini 1989, 1991). The local circulation of ideas among competent producers promoting creative solutions in technologies and products, which is a key dimension characterizing the Marshallian concept of “atmosphere”,Footnote 1 is often referred to in more recent international literature by the term “buzz” proposed by Storper and Venables (2004). The ‘local buzz’ tends to lead to an ever-increasing geographical concentration of innovation activity in a few regions. However, it may be both unrealistic and undesirable to rely only on ‘local buzz’ for developing their knowledge base, and successful clusters need to combine knowledge internal and external to the cluster. To this end, ‘global pipelines’ need to be established in order to allow external knowledge to flow into the clusters (Owen-Smith and Powell 2004; Bathelt et al. 2004). Global pipelines refer to channels of communication used in the interaction between firms in different knowledge‐producing centres located at a distance from one another. They can ‘pump’ information about markets and technologies into the cluster, making the ‘buzz’ more dynamic, by providing access to a more variegated set of knowledge pools from which to draw. What these authors emphasise is exactly the localized nature of innovations that are generated in a given place, and are transferred, applied and developed on a global scale through trans-territorial networks. The atmosphere characterizing each place may thus be a key asset for innovation and competitiveness, to the extent that it facilitates interactions and exchanges between local actors bearing unique competencies, and provided that it does not hinder cross-border transmission of knowledge.",10
44,4,Journal of Industrial and Business Economics,27 September 2017,https://link.springer.com/article/10.1007/s40812-017-0076-7,The Marshallian industrial district and inclusive urban growth strategy,December 2017,Joan Trullén-Thomas,Rafael Boix-Domenech,,Female,Male,Unknown,Mix,,
44,4,Journal of Industrial and Business Economics,04 October 2017,https://link.springer.com/article/10.1007/s40812-017-0081-x,Cluster policies and cluster institutions: an opportunity to bind economic and social dimensions?,December 2017,Anastasiia Konstantynova,James R. Wilson,,Female,Male,Unknown,Mix,,
44,4,Journal of Industrial and Business Economics,26 September 2017,https://link.springer.com/article/10.1007/s40812-017-0078-5,"System-based, light and complex: industrial and local development policies in the thought of Giacomo Becattini",December 2017,Annalisa Caloffi,,,Female,Unknown,Unknown,Female,"System-based, light and complex: this is how Giacomo Becattini thinks industrial and local development policies should be. The system-based nature of the policies arises from the fact that the ability to produce and innovate is not necessarily a quality of the individual enterprise but also primarily occurs due to the interaction among different organizations that collaborate and compete in a system such as the industrial district, which the author has defined throughout his career. System-based policies should be light to avoid excessively altering the mechanisms of the system on which policies are intended to act. However, they must be complex because the intervention in a system where enterprises operate and people live necessarily embraces different domains, such as industry, labour, lifelong learning, education, industrial relations, and logistics and transport infrastructures. The adjectives light and complex could also be used to describe how Becattini discusses policies. It is light because, as Becattini’s object of analysis scarcely involves policies, the latter enters his writings in a non-predominant way. The complexity stems from the author’s belief that ideas and theories—even the most complex ones—must always be tested in practice in a continuous back-and-forth manner from theory to reality. The reflection on policies, which is scattered throughout Becattini’s work, is the ideal moment when scholars deal with the reality (Becattini 1998). This article examines Becattini’s main contributions to industrial and local development policies and takes into account the reception of such contributions among both scholars and policymakers. Although many scholars understand local development policies in a way that is not far from Becattini’s approach, this article maintains a close focus on the literature that refers more explicitly to this author. The article is developed as follows. After explaining the central aspects of Becattini’s thinking (Sect. 2), the paper focuses on its legacy. Particularly, Becattini’s reflections on local development policies contain many elements that somehow anticipate the current debate on system-based and place-based policies, which are briefly discussed in Sect. 3. The same section maps out examples of system-based, light and complex policies in Italy. Section 4 presents the conclusions by highlighting some promising future avenues of research that are inspired by Becattini’s works.",1
45,1,Journal of Industrial and Business Economics,29 January 2018,https://link.springer.com/article/10.1007/s40812-018-0091-3,Introduction to the special issue: Public Procurement—new theoretical and empirical developments,March 2018,Stéphane Saussier,Paola Valbonesi,,,Female,Unknown,Mix,,
45,1,Journal of Industrial and Business Economics,29 December 2017,https://link.springer.com/article/10.1007/s40812-017-0087-4,Implications of Third Parties for Contract Design,March 2018,Marian W. Moszoro,Pablo T. Spiller,,Male,Male,Unknown,Male,"The literature on contracts has focused on transaction costs and contractual hazards (Crocker and Reynolds 1993; Saussier 2000; Bajari et al. 2014), incentives and commitment (Laffont and Tirole 1993; Bajari and Tadelis 2001; Guasch et al. 2008), incompleteness, verifiability, and reference points (Fehr et al. 2011; Halonen-Akatwijuka and Hart 2013), and relational contracting and the value of future business (Macaulay 1963; Baker et al. 2002; Gil and Marion 2012) to explain contract features and the degree of discretion—e.g., the choice of first-price versus cost-plus (Crocker and Reynolds 1993) contracts or auctions versus negotiations (Bajari et al. 2009)—but it has been largely agnostic about the implications of third parties for contract design. Third parties are the players tacitly engaged, but not formally involved in a contractual relationship—e.g., watchdogs, minorities, advocacy groups, consumers, past and future transactors, and opponents and competitors to the contractees—who may challenge, either informally through the media or formally in a court, the legitimacy of the transaction, which typically carries a costly demand for proof of the validity of the contracting process (Williamson 1999). In McCubbins and Schwartz (1984) and McCubbins et al. (1989), the monitoring of the agent is outsourced to third parties to save time and valuable resources. Third parties are desirable instruments of oversight, which take advantage of procedural mechanisms (i.e., “fire alarms”) designed to hold politicians and public managers accountable. In contrast, the pernicious effect of opportunistic third parties in public procurement and regulation has been highlighted by Spiller (2008, 2013) and Moszoro and Spiller (2012, 2014): Third parties increase the political risks of the incumbent politician, who increases contractual proceduralization to avoid successful challenges at court and externalizes the increased costs to the public at large. In this paper, we extend the framework to the analysis of the implications of third parties for contract design. We also contribute with a set of proofs that in the presence of opportunistic third parties the agents will seek additional, but finite, contractual rigidity and applications to private quasi-political setups with multiple stakeholders (e.g., corporations).",
45,1,Journal of Industrial and Business Economics,29 December 2017,https://link.springer.com/article/10.1007/s40812-017-0088-3,Procuring price and quality using scoring auctions: where do we stand?,March 2018,Riccardo Camboni Marchi Adani,,,Male,Unknown,Unknown,Male,"On average, public procurement accounts for 13.4% of OECD countries’ GDP. A sector as large, undoubtedly, warrants extended and focused analyses. Public procurement lies at the crossroad of industrial organization (IO) and political economics. From an IO perspective, we use incentive theory and its principal–agent framework pioneered by Laffont and Tirole (1993) to model governments’ behaviors. However, since government’s motivations are political in means, we need political economics insights into incentives that often drive the decisions of a public buyer. These motivations, which are not always aligned with a social welfare-maximizing approach, are responsible for public procurement agents having far less discretionality than that in the private sector. Rules and regulations dictate what a public buyer can and cannot do, transactional approaches are often uses and, above all, auctions. Thus, a specific branch in the economics literature addresses awarding mechanisms used in public procurement and their impact on public spending efficiency. Related studies adopt tools that are based on auction theory. Indeed, auction theory has probably in public procurement it’s most important application, although the rise of internet auctions may question this supremacy in the future. We gained a comprehensive understanding of the use of a First Price Auction (FPA) to award contracts in which price is the public buyer’s only concern. A simple and standardized example is the repaving of a road. Once a contracting authority has defined its requirements in the tender contract, minimizing the price to be paid using an auction remains the best possible approach. By contrast, a complex service is characterized by several quality dimensions. For example, an IT system management provided by a specialized external firm generally includes hardware inventories, server availability, software distribution and upgrading, user profile management, backup and recovery, and security and network management. A public buyer is usually constrained to use a transactional approach and thus, it may not simply bargain on all those quality dimensions with one or more suppliers. Thus, it is important to design efficient public procurement mechanisms for quality as in the case of simpler goods and services where price is the only concern (e.g., FPAs). Several awarding procedures that incorporate quality have been applied in practice. The simplest mechanism used to incorporate a specific quality provision is to maintain it as a minimum requirement in the tender and then, use an FPA to award the contract. However, when using this mechanism, a buyer loses out the price–quality trade off. To elaborate, since a supplier incurs a cost for quality provision, it is provided at a level equal to the minimum stipulated in the contract. On the other hand, in a beauty contest, for example, suppliers submit an offer where price and quality are specified. Then, the public buyer decides which offer to choose. However, since the importance of quality is not declared in the awarding mechanism, firms cannot submit their most efficient bid given the buyer’s preferences, which are not known ex ante. Finally, a direct negotiation with one firm is seldom possible in a public procurement. Nevertheless, while quality can be negotiated in this case, the price is expected to be much higher given the absence of competition. According to Asker and Cantillon (2008), all these mechanisms are dominated by a Scoring Rule Auction (SRA). In a SRA, all participants submit a multidimensional bid comprising a price and levels of one or more qualities. These elements are weighted using a linear combination, that is, a scoring function. The result of this function is a score and the bid with the highest score is awarded the contract. In this study, we analyze the literature on SRAs with focus on their optimal design, impact on costs and quality provision, key limitations, and research opportunities for economists while accounting for recent changes in certain countries’ legislation. A key feature of SRA is that quality importance (i.e. the parameters of the scoring function) is known ex ante by all participants. Thus, firms can submit the optimal (i.e., the most efficient) quality–price combination according to their cost structure. As a result, SRAs seem to position themselves to cover procurement when competition exists, but it is not limited to price. SRAs, in fact, are not fit to procure goods or services for which only one firm can deliver. Consider a patented technology as an example. Here, any auction mechanism would perform poorly, unless the buyer can set a reserve price equal to the supplier’s marginal cost. Given the monopoly granted by the patent, the supplier is aware that the auction will have only one participant (i.e., him/herself) and accordingly, will bid at the reserve price and may not provide quality above the minimum level stipulated in the tender. SRAs also require that buyers have a good understanding of their needs because they force them to give a precise shape (the scoring function) to its price–quality preferences. However, this is not always possible, particularly in the case of complex works or services or when innovation is important.Footnote 1
 Notwithstanding these limitations, SRAs are being increasingly used in Europe and the United States. In Europe, SRAs are commonly referred to as the most economically advantageous tender (MEAT). The European Union (EU) Directive 2014/24/EU supports moving away from contract awards based on the “lowest price” toward tenders based on quality and price criteria, that is, the scoring rule. According to Tender European Daily (TED) Data, in 2016, 72% of all auctions above the value of € 150.000 used scoring rule as an awarding criterion. Similarly, in Italy, the new code on public procurement (D.Lgs 50/2016) defines a shift to including quality in the awarding process for not only services but also works and supplies. Scoring rule has become the standard tender, unless there is a definite reason to move toward direct negotiation on the one hand and traditional first price auctions on the other. As a result of this change, the use of scoring auctions in Italy, according to TED data, increased from an already high 67% in the last two quarters of 2015 to 80% in the same period the next year.Footnote 2
 In the United States, SRAs are used by the California Department of Transportation to award highway maintenance work. Known as the “A+B bidding” or “cost-plus-time bidding,” the mechanism effectively works as a bidimensional scoring rule with time to complete work as a single quality dimension plus price. We believe that a better understanding of the economics of SRAs has relevant policy implications given the large diffusion of such a mechanism. Drawing on Dastidar’s (2014) brief survey of SRAs, this study makes the following contributions. First, it offers a detailed presentation of the literature and all novel findings for the past three years. Second, it describes a structural estimation using a step-by-step procedure, which can be reproduced by researchers and practitioners alike. Further, it discusses the key challenges a practitioner must consider when implementing SRAs and those a researcher faces when assembling data for SRAs. Finally, we highlight the gaps in the literature that warrant the attention of researchers. The remainder of this paper is organized as follows. Section 2 presents the equilibrium in SRAs using a multidimensional setting. In addition, we discuss the optimal design of SRAs and unanswered questions that remain in the literature. Section 3 provides select empirical results for SRAs, with focus on SRAs efficiency with respect to FPAs and on impact on the procurement process. Section 4 introduces the structural estimation, an empirical methodology which has just began to be applied to SRAs. Further, we highlight how new data sources can be exploited to examine market structure and firms’ private information using bids data. Section 5 points to the three main limitations of SRAs. Section 6 concludes.",1
45,1,Journal of Industrial and Business Economics,03 January 2018,https://link.springer.com/article/10.1007/s40812-017-0085-6,Green public procurement and multiple environmental objectives,March 2018,Sofia Lundberg,Per-Olov Marklund,,Female,Unknown,Unknown,Female,"“While research and development is in progress to improve and deploy cleaner and more efficient technologies, it is also important to influence our consumption and production patterns so as to minimise the damage caused to the environment while maintaining an economic equilibrium at the same time. GPP was introduced as part of an effort to take some concrete steps in this direction.” The European Commission on why there is a need for Green Public Procurement (GPP).Footnote 1
 Policy makers can employ different instruments to address environmental policy targets. Recently, and as illustrated by the above quotation from the European Commission, public procurement has been highlighted and promoted by politicians as such an instrument.Footnote 2 This is an ongoing worldwide trend (see, e.g., Tukker et al. 2008; Fischer 2010; Qiao and Wang 2011). In 2014, nearly 84% of the OECD members had developed a national strategy to support public procurement as an environmental policy instrument (OECD 2015). Purchasing processes by which public authorities procure products with a reduced environmental impact but that are perfect substitutes in all other dimensions formally defines green public procurement (GPP).Footnote 3 The purpose of this paper is to assess whether GPP can be used to effectively achieve multiple environmental objectives, given that the real cause of an authority’s procurement is the need for goods, services, or public works in order to carry out its operations. Public authorities implement GPP by considering environmental concerns when allocating contracts to private suppliers. The allocation mechanism is competitive tendering, and these concerns may be expressed as environmental requirements on the producer (potential supplier), the production process (vertical dimension), or the supply of the good, service, or public work. Consequently, the environmental performance of the potential supplier and the product are parts of the quality offer, i.e., the auction rule takes the form of multidimensional bidding with a price bid and a quality offer.Footnote 4 Throughout this paper, there will be references to two sectors identified by the European Commission as priority sectors for implementing GPP—internal regular cleaning services (the sector is Cleaning Products and Services) and food (the sector is Food and Catering Services).Footnote 5
 The arguments in favor of using GPP as an environmental policy instrument is based on the purchasing power of the public sector (European Commission 2016). In 2013, public expenditures in EU Member States were 13.7% of the GDP on average.Footnote 6,
Footnote 7 In the exercising of this power, GPP is considered to be an important instrument to achieve objectives that are related to climate change, resource use, and sustainable production and consumption (European Commission 2016). The OECD (2013) suggests that GPP contributes to national and international environmental objectives and that it is a major driver for innovation and a vehicle for economic growth. Based on the political ambitions, GPP is clearly seen as a political environmental instrument that can be designed to target multiple objectives. The question of whether GPP can be used to achieve a certain objective effectively has previously been addressed in a general manner. In a seminal paper, Marron (1997) concludes that public procurement is an imperfect environmental policy instrument. Lundberg et al. (2016) provide a more nuanced picture of GPP by developing the work of Marron (1997) to consider the auction process and the potential bidders’ decision of whether or not to participate in the procurement and to submit a tender. The results of these previous studies have all suggested that GPP is an ineffective instrument for meeting the stated environmental goals. However, these studies did not specifically address the possibility of using GPP to target several environmental objectives simultaneously in one and the same procurement auction. An important question, then, is if such a possibility might motivate the use of GPP as an environmental policy instrument. In the present paper, we add to this literature by explicitly considering the multiple-objective feature of GPP. It is entirely possible to address many different environmental objectives within the same procurement auction, and this is often the case in practice. The ambition might be to achieve more than one objective by stipulating one single environmental requirement, e.g., when procuring foods (Scenario 1 in Sect. 3.1). The requirement might be that the delivered food must meet certain criteria on organic production, and this might expressed as asking for specific labeling of organic products (or similar). Additionally, asking for organic food can be a means to achieve other objectives. As an example, in February 2017 the Swedish Government presented a national food strategy with the main aim to increase food production and to contribute to a competitive food chain characterized by increased employment, exports, innovation, and profitability, all while achieving relevant environmental objectives (Government Bill 2016/17:104). A certain orientation within the strategy is, according to the Swedish Government’s action plan (The Swedish Government 2017), that 30% of Swedish agricultural land be certified organic farmland by 2030Footnote 8 and that 60% of public food consumption be certified organic products by 2030.Footnote 9
 The ambition with GPP might also be to achieve more than one objective by stipulating several different requirements, e.g., when purchasing internal cleaning services (Scenario 2 in Sect. 3.1). According to Lundberg et al. (2015), studying public procurement of cleaning services in Sweden, this might include requirements related to environmental management systems, eco-labeling of the products used, eco-driving and performance of vehicles, the use of chemicals, or the way in which the cleaning service has to be carried out. Turning back to our food example, and based on Swedish data, green criteria for food procurement might include organic food, recycled packaging, the use of aquaculture and marine products, integrated production, and animal welfare (Lundberg and Marklund 2013). Two guiding principles when designing effective environmental policies are the following. First, policy instruments should be designed to operate as close to the source of the environmental problem(s) as possible, i.e., they should target the source as directly as possible. It follows from this that it is motivated to study GPP in the framework of a single environmental requirement and several, i.e., a multiple set of, environmental objectives. Second, the number of policy instruments should equal the number of objectives, i.e., what the literature refers to as the Tinbergen Rule (Tinbergen 1952). This rule leads us to ask whether we should regard GPP as a single policy or a multiple policy instrument. The current paper contributes to the scarce number of studies that consider GPP from an economics perspective. Evaluation of the economic consequences of using GPP as a strategic instrument to pursue environmental objectives is easily motivated and important because it takes its point of departure in the fact that all resources are scarce and that society must conserve them in order to create a sustainable future. Conserving scarce resources includes environmental, energy, and climate policies that actually have an impact and that the impact is achieved in a cost-effective manner.Footnote 10 It is in this perspective that the evaluation and potential of GPP as an environmental policy instrument should be understood. The general conclusion that follows from the analysis is that GPP is not an effective environmental policy instrument, and this conclusion holds even if consideration is given to the multi-objective character of GPP. The rest of the paper is organized as follows. In the next section, the literature on GPP as an environmental policy instrument is reviewed. In Sect. 3, GPP is discussed in relation to some guiding principles for a consistent system of environmental policy instruments. One of the principles is referred to as the Tinbergen Rule, which is formally outlined in Sect. 4. Finally, in Sect. 5 some concluding remarks are provided.",13
45,1,Journal of Industrial and Business Economics,05 January 2018,https://link.springer.com/article/10.1007/s40812-017-0086-5,Is competition able to counteract the inefficiency of corruption? The case of Italian public works,March 2018,Massimo Finocchiaro Castro,Calogero Guccio,Ilde Rizzo,Male,Male,Female,Mix,,
45,1,Journal of Industrial and Business Economics,16 January 2018,https://link.springer.com/article/10.1007/s40812-017-0089-2,Barriers towards foreign firms in international public procurement markets: a review,March 2018,Chiara Carboni,Elisabetta Iossa,Gianpiero Mattera,Female,Female,Male,Mix,,
45,2,Journal of Industrial and Business Economics,21 May 2018,https://link.springer.com/article/10.1007/s40812-018-0096-y,In memory of a friend and distinguished scholar: Luigi Orsenigo (1954–2018),June 2018,Giovanni Dosi,Franco Malerba,,Male,Male,Unknown,Male,,1
45,2,Journal of Industrial and Business Economics,13 May 2017,https://link.springer.com/article/10.1007/s40812-017-0072-y,Does university prestige foster the initial growth of academic spin-offs?,June 2018,Alice Civera,Michele Meoli,,Female,Female,Unknown,Female,"“Academic spinoffs are not gazelles” (Colombo et al. 2010, p. 2). The majority of academic spin-offs remains very small or grow so slowly that their contribution to the job creation or to the financial returns for the public research sector is shown to be mediocre (Colombo et al. 2010; Degroof and Roberts 2004). By nature, academic spin-offs tend to lack resources that, together with the scarcity of investment capital, technical knowledge and skills (Lockett and Wright 2005), translates into weak performances. To overcome these obstacles to the development of academic spin-offs, Degroof and Roberts (2004) suggest paying attention to the early stages of the firm’s life, because they strongly influence the growth process, which is a path-dependent process by nature. The decisions taken at the very beginning, during the incubation period, deeply affect the successive potential development (Roberts 1991). Then, nothing can jeopardize or nurture the success of an academic spin-off more than the parent university. The issue of spin-offs’ poor performance, together with the awareness of the key role played by universities in fostering regional development, has led policy makers to ask what appropriate type of strategy needs to be adopted to stimulate spin-off activity (Mustar and Wright 2009) starting from policies within the universities (Mustar et al. 2006, 2008). In line with these arguments, there are several university features that scholars investigated, trying to assess the impact on academic spin-offs’ success: the presence of Technology Transfer Offices (Mustar et al. 2006), institutional resources (O’Shea et al. 2005), human capital (Powers and McDougall 2005) as well as academic policies (Mustar and Wright 2009). Among university features, university prestige (Kenney and Patton 2011) has been often identified as one of the characteristics deserving the most attention. From a sociological point of view, organization prestige is “the relative esteem in which an organization is held in an ordered total system of differentiated evaluation” (Sine et al. 2003, p. 481) and accordingly several are the motivations why prestige matters. First, in virtue of the so-called “halo effect”, which is the phenomenon through which the prestige of an organization reflects in its product (Crane 1965; Perrow 1961). Second, prestige increases an institution’s visibility, which once again increases the prestige of its product. Third, prestige enhances the credibility of an organization and subsequently its perceived quality (Hovland et al. 1953). Lastly, an organization’s prestige enhances not only the prestige of its product but also of the agents with whom gets in touch (Podolny 1993). By extending the concept of organization prestige to university it is possible to define university prestige—that often coincides with university reputation—as how this kind of entity is perceived (Smidts et al. 2001). University eminence has not to be confused with that of university excellence, namely the way in which an academic institution fulfils its functions of teaching and research (Taylor and Braddock 2007). Several studies in the field of entrepreneurship and higher education have academic prestige as their mean topic (see Kenney and Patton 2011). The favorite measure adopted by scholars is university rankings (e.g. Sine et al. 2003) because, although the question of rankings’ representativeness of quality is source of debate, they have been increasingly accepted as indicator of academic reputation (Bowman and Bastedo 2011). University prestige has a fundamental impact on the missions of university, both on the most traditional ones such as teaching (Alves and Raposo 2010; Azoury et al. 2014) and research (Allison and Long 1990; Merton 1968) and on societal engagement activities as licensing (Sine et al. 2003). Among this letter group, the relation between academic spin-offs and eminence of parent university has been extensively studied from two point of view: funding and birth rate. From the financing perspective, academic spin-offs are attractive for external investors when higher education institutions have the ability, in virtue of a powerful effect of signaling, to enhance the legitimacy of affiliated companies (Wang and Shapira 2012; Di Gregorio and Shane 2003). By contrast, concerning the birth rate, it is well established that reputation of the entire university as well as that of a department positively and significantly influences the birth rate of affiliated firms (see for example Stuart and Ding 2006; O’Shea et al. 2005; Di Gregorio and Shane 2003; Zucker et al. 1998). Academic status has repeatedly been found essential in terms of number of spin-offs established, but, to the best of our knowledge, no evidence is available with respect to the broad concept of university prestige and initial performance. This is exactly the gap in literature that our paper aims to address, by investigating whether the affiliation with a prestigious university affects not yet the birth rate but rather the initial growth of affiliated firms. In general, the prestige of a university is expected to matter: prestigious universities are believed to introduce into the market high-quality firms also merely because their reputation is at stake (Podolny 1993). Further, eminent institutions can count on intangible assets, such as network, contacts, knowledge to share with affiliated firms, which then become more attractive for foreign investors (Cattaneo et al. 2015) and better able to exploit intellectual property of innovation (Di Gregorio and Shane 2004). Moreover, since higher education institutions are multifunctional, we study whether prestige moderates the university orientation for its traditional missions—teaching, research and third mission. For its part, prestige lends itself to an analysis of this type in virtue of its many-sided nature. We explore this issue using a sample of 508 academic spin-offs from 85 Italian universities in the period 1999–2015, and we find a positive correlation between the growth of academic spin-offs and the prestige of their parent institutions. Furthermore, we find that the effect of prestige positively moderates the contribution to the growth of teaching resources and societal engagement. Finally, we check the robustness of our findings by using different proxies for the growth of academic spin-offs and for university prestige. The remainder of the paper is organized as follow. Section 2 is dedicated to the literature review and the hypotheses development. Section 3 describes the research design and Sect. 4 shows the results. Finally, Sect. 5 concludes and provides policy implications.",10
45,2,Journal of Industrial and Business Economics,20 September 2017,https://link.springer.com/article/10.1007/s40812-017-0074-9,The effect of information asymmetries on serial crowdfunding and campaign success,June 2018,Vincenzo Butticè,Carlotta Orsenigo,Mike Wright,Male,Female,Male,Mix,,
45,2,Journal of Industrial and Business Economics,14 February 2017,https://link.springer.com/article/10.1007/s40812-016-0067-0,Collecting data on TMTs’ organizational design: good practices from the StiMa project,June 2018,Paola Rovelli,Cristina Rossi-Lamastra,,Female,Female,Unknown,Female,"In recent decades, scholarly interest in firms’ Top Management Team (TMT)—the team, formed by the Chief Executive Officer (CEO) and the managers who report directly to her/him (TMT members), responsible for the strategic decision-making (Amason 1996; Collins and Clark 2003)—has grown steadily. Many contributions in this field have related the demographic characteristics of TMT members (e.g., age, nationality, and education) to firms’ strategies and performance (e.g., Bantel and Jackson 1989; Boeker 1997; Carpenter and Fredrickson 2001; Eisenhardt and Bourgeois 1988; Hambrick 2007). Other works have investigated behavioral aspects, such as CEOs’ leadership styles or integration among TMT members (e.g., Battilana et al. 2010; Carmeli and Schaubroeck 2006; Carmeli et al. 2011; Hambrick 1994, 1997; Simsek et al. 2005). On the contrary, we know comparatively less on TMTs’ organizational design, namely the ways in which the CEO and other TMT members organize their managerial work at the top of the corporate hierarchy [see Guadalupe et al. (2013), Souitaris and Maestro (2010) and Wong et al. (2011) for some notable exceptions]. This is a relevant gap. Indeed, to efficiently run their firms and, thus, ultimately achieve superior performance, TMTs in general and CEOs in particular must set up both formal and informal coordination and communication mechanisms, agree upon the division of their managerial labor, choose the level of specialization within the TMT, define the formalization of decision-making processes, and so on. Moreover, CEOs, who receive authority over strategic decisions by the board of directors, must choose whether to delegate this decision authority to other TMT members, depending, for instance, on their decision-specific knowledge and managerial experience (Colombo et al. 2016a). In light of the importance of the topic, we think that one reason behind the limited research on TMTs’ organizational design resides in the difficulties accessing and collecting data in this realm. Indeed, scholars can easily retrieve demographic data on top executives from secondary sources. Commercial databases (e.g., CorpTech,Footnote 1 Compustat,Footnote 2 S&P Capital IQ,Footnote 3 and ExecucompFootnote 4) provide detailed information on top executives, including their names, biographies, and compensation, although these databases have good coverage primarily for large firms located in the US. Currently, researchers can retrieve CEOs’ and managers’ biographies also on LinkedIn. Conversely, these sources rarely report information on how TMTs organize, thus the only viable way to obtain these data consists in asking them to CEOs and TMT members (Cycyota and Harrison 2006; Norburn and Birley 1988). In fact, since the seminal contribution of the ASTON group (e.g., Pugh et al. 1968, 1969), directly gathering data from employees and managers has become the gold standard in the field of organizational design, regardless of the organizational aspects that researchers intend to investigate (Baruch and Holtom 2008; Colombo and Delmastro 2008; Kraut 1996; Norburn and Birley 1988). Such a method poses specific challenges that are even more severe when one seeks to collect survey data on TMTs’ organizational design. First, retrieving contact information (e.g., email addresses) to invite CEOs and other TMT members to participate in surveys is far from simple, especially in the case of unlisted firms. The latter neither have to disclose information on their top executives by law nor use to report this information on their websites (most of the smallest firms do not even have a website). This lack of information is made more worrisome by the fact that in many countries, including Italy, unlisted firms account for a large part of the productive system. Compared with listed firms, unlisted ones are usually smaller, suffer from resource constraints (Mahérault 2000), and do not have to comply with regulatory norms (Buzby 1975). Therefore, they tend to have small and poorly specialized TMTs and a simple organizational structure. Given these specific characteristics, research on the organizational design of unlisted firms’ TMTs can hardly benefit from the generalization of results obtained on listed firms. Second, once a researcher has managed to establish contact with CEOs and TMT members, she/he must convince them to devote time to participate in the survey (Baruch 1999). This usually requires considerable effort due to the severe time constraints that individuals at the top of the corporate hierarchy usually face (Baruch 1999; Baruch and Holtom 2008; Cooper and Payne 1988; Falconer and Hodgett 1999) and to their reluctance to disclose what they perceive to be sensitive information. Third, setting up a questionnaire that is effective in collecting organizational design data is a challenging task. As we illustrate in the following section, in the literature, there are tested scales and questions, which researchers have developed and used to collect these types of data. However, these scales and questions need to be adapted to the specific context of the research (for instance, by translating them into the language of the country where the research is conducted) and, more generally, made understandable by the top executives who are going to complete the survey. Indeed, key informants usually require considerable specialized knowledge to correctly approach organizational design concepts and constructs. From the discussion above, it comes as no surprise that empirical studies on TMTs’ organizational design based on surveys continue to lag behind. Few notable exceptions exist (e.g., Bandiera et al. 2008, 2011, 2013; Carpenter and Fredrickson 2001; Wiersema and Bird 1993). However, these studies have mainly targeted large firms and have analyzed organizational design elements such as hiring and firing practices, top executives’ incentives and payment schemes, time management by CEOs (Bandiera et al. 2008), TMT’s functional heterogeneity (Carpenter and Fredrickson 2001), or turnover (Wiersema and Bird 1993). In so doing, they have disregarded other important elements, such as the allocation of decision authority within the TMT or the coordination mechanisms that TMT members put in place. Moreover, extant contributions have typically analyzed organizational design elements separately, whereas assessing these elements in conjunction would allow a deeper understanding of complementarities (Ennen and Richter 2010) at work in the TMT’s organization. Finally, some of the surveys used by scholars to study TMTs’ organizational design show unresolved biases depending, for instance, on the use of convenient samples. The SPEC project on the organizational design of the TMT of high-tech entrepreneurial ventures in Silicon Valley is a case in point (Beckman and Burton 2008; Burton and Beckman 2007). Moving from these premises, this paper aims to offer scholars good practices for improving the research design of their surveys intended to collect data on TMTs’ organizational design. In so doing, we hope to increase the availability of reliable data on this topic, thus paving the way for solid empirical research that can advance academic knowledge and inform practitioners’ conversations. To this end, we are grounded on the experience that we gained by working on the StiMa project, which, from 2013 to 2016, collected data on TMTs’ organizational design through a large-scale survey sent to CEOs of a representative sample of Italian firms with 20 or more employees. Specifically, the StiMa project gathered comprehensive data on delegation of decision authority, coordination and communication mechanisms, functional specialization of TMT members, formalization and functioning of the decision-making processes, including decision speed and comprehensiveness (see Appendix 1 and Appendix 2 for a detailed description of the constructs/concepts we gathered and of their underlining dimensions). Then, this primary data collection was integrated with the data gathered from secondary sources related to firms’ performance and the individual characteristics of CEOs and TMT members (e.g., educational background, work experience, and personality traits). The paper describes the steps that StiMa researchers followed in data collection with the aim of proposing a step-by-step procedure that other researchers can replicate. In so doing, it also highlights the main challenges that StiMa researchers faced in conducting the survey and in collecting additional data from secondary sources. We describe how these problems were solved, thus identifying a set of good practices that can orient actions in this area of research. We hope that our work paves the way for further data collection efforts, which can overcome the scarcity of information and empirical studies on TMTs’ organizational design, especially outside the US (Hambrick 2007) and on unlisted firms. The reminder of the paper is structured as follows. In the next section, we briefly illustrate the overarching aim of the StiMa project and the design of the StiMa questionnaire. We then describe the steps we followed to develop and administer the survey and to create the complete database. Specifically, we first explain how we selected the target population, extracted the sample frame and created the contacted sample. Then, we report on the administration of the survey. Following that, we describe in detail the database construction, the data cleaning procedure, and the checks that we made to minimize biases and ensure data reliability. We also report how we collected additional data from secondary sources. We conclude by summarizing the main lessons that we learned from our experience within the StiMa project and that we intend to transfer to other researchers as good practices.",6
45,2,Journal of Industrial and Business Economics,14 November 2017,https://link.springer.com/article/10.1007/s40812-017-0083-8,Non-traditional business models for city-scale energy storage: evidence from UK case studies,June 2018,Andrew Burlinson,Monica Giulietti,,Male,Female,Unknown,Mix,,
45,2,Journal of Industrial and Business Economics,22 September 2017,https://link.springer.com/article/10.1007/s40812-017-0075-8,Dark fiber price regulation in the absence of facilities-based competition,June 2018,Vitor Miguel Ribeiro,,,Unknown,Unknown,Unknown,Unknown,,
45,2,Journal of Industrial and Business Economics,22 November 2017,https://link.springer.com/article/10.1007/s40812-017-0084-7,"Mobile telephony, economic growth, financial development, foreign direct investment, and imports of ICT goods: the case of the G-20 countries",June 2018,Rudra P. Pradhan,Mak B. Arvin,Sara E. Bennett,Unknown,Unknown,Female,Female,"From 1990 to 2015, worldwide, mobile phone subscriptions grew from 12.4 million to over 7.6 billionFootnote 1 (Saylor 2012; The Mobile Economy 2016). New technologies and innovative service providers often challenge the provision of traditional services. For example, mobile phones have had a tremendous impact on fixed telephony services. Thirty years ago, fixed-line (i.e. land-line) networks seemed to meet all communication needs, but in the last 20 years, mobile phones have revolutionized the market (Hoernig et al. 2015; Yamakawa et al. 2013; Ward and Woroch 2010; Kalba 2008). Current projections suggest that the growth of mobile phones will far outpace that of landlines. In fact, the next billion new phone users will primarily use mobile phones (Donner 2008; ITU 2003; Lanvin 2005). Mobile phones have emerged as a vital component of leapfrogging technology (Gray 2006; ITU 2007; James 2009a, b). This is particularly prevalent in developing countries as these latecomers did not have substantial investment, compared with developed nations, in earlier technologies (fixed-line communications). Developing nations leapfrog by directly investing in the latest version of the technology. For example, Waverman et al. (2005) find that mobile lines are a substitute for fixed-lines in developing nations while they are simply a complement to fixed-lines in developed nations. Therefore, the positive and significant impact of mobile phones on economic growth is much more pronounced in developing nations as compared with developed nations. In addition to potentially allowing developing countries to leapfrog technology, mobile phones can have other impacts on these countries. For instance, improved access to and use of mobile phone technology may affect agriculture outcomes, which often comprise a significant portion of the gross domestic product of these economies. For example, mobile phones could reduce farmers’ search costs as they have quick access to price information in multiple markets which, in turn, allows farmers to sell in the markets with the highest prices net of transportation costs (Tack and Aker 2014). Similarly, mobile technology may allow farmers to conclude sales using mobile phones which reduces uncertainties with selling in a distant market (Tadesse and Bahiigwa 2015; Aker and Mbiti 2010). StudiesFootnote 2 on the relationship between the diffusion of mobile phones and economic growth began in the early 1990s, particularly during the era of globalization. Many of these studies have confirmed a positive correlation between the increased use of mobile phones and economic growth (see, for example, Al-mutawkkil et al. 2009; Andrianaivo and Kpodar 2012; Gruber and Koutroumpis 2011; Pradhan et al. 2013; Sridhar and Sridhar 2007; Vu 2011). However, these studies investigate only the link between the diffusion of mobile phones and economic growth, and do not investigate the direction of causality. Furthermore, previous studies have linked the diffusion of mobile phones and economic growth to either foreign direct investment or to improvements in the financial system of an economy. Our study is motivated by a gap in the existing empirical evidence about the interrelatedness of the diffusion of mobile phones and economic growth to both financial development and foreign direct investment. To fill this void in the existing empirical research, we propose an empirical model where the relationship between the diffusion of mobile phones and economic growth is considered in the presence of financial development, foreign direct investment, and information and communication technology (ICT) goods imports. We consider foreign direct investment, financial development, and ICT goods imports to be critical variables that may affect mobile phone diffusion, and that, conversely, may themselves be affected by mobile phone diffusion. Therefore, we consider the causal relationship between five variables: mobile phone diffusion, foreign direct investment, financial development, ICT goods imports, and economic growth. Our key question concerning the causal connection between mobile phone diffusion and economic growth is whether mobile phone diffusion has contributed towards economic growth, or whether the increased diffusion of mobile phones is simply a consequence of rapid economic growth. The rest of this paper is organized as follows: the literature is reviewed in Sect. 2; the data and model are presented in Sect. 3; the econometric analysis and empirical results are discussed in Sect. 4; and Sect. 5 concludes the paper and suggests some policy recommendations and implications.",25
45,3,Journal of Industrial and Business Economics,17 February 2018,https://link.springer.com/article/10.1007/s40812-018-0094-0,"High-tech entrepreneurial ventures seeking external equity: whether, when, where… and why not?",September 2018,Anita Quas,Diego D’Adda,,Female,Male,Unknown,Mix,,
45,3,Journal of Industrial and Business Economics,24 January 2018,https://link.springer.com/article/10.1007/s40812-018-0090-4,Are Italian firms performances influenced by innovation of domestic and foreign firms nearby in space and sectors?,September 2018,Anna M. Ferragina,Giulia Nunziante,,Female,Female,Unknown,Female,"This paper aims to address three main issues. First, we evaluate the relevance of spatial total factor productivity (TFP) spillovers determined by firm closeness in specific Local Labour Systems of production (Textile) in Italy. The second aim is to measure within these contexts TFP premia deriving from firm innovation proxied by immobilisations in total intangible assets, accounting for spatial and sectoral clustering of firms. Finally, we take into account how far spatial productivity spillovers and TFP premia spur from innovation of foreign and domestic multinationals located in the LLS. To this purpose, we use a panel of Italian manufacturing firms provided by AIDA located in 18 Made in Italy Local Labour Systems (LSS) of Textile as classified by Istat. We also use administrative data to include variables related to the geographical context and to agglomeration economies and Istat data to get the municipalities belonging to districts.Footnote 1
 We investigate the influence of TFP and the impact of investment in innovation on TFP to take into account spatial spillovers of productivity and innovation. We allow the TFP of each firm to be affected by the TFP, by the innovation and by other features of nearby firms by using a spatial econometric approach (Pisati 2001; LeSage and Pace 2009; Elhorst 2010; Autant-Bernard and LeSage 2011; Autant-Bernard 2012). To investigate the mechanisms of transmission of innovation at spatial level we use the geographical coordinates (latitude and longitude) at firm level. The spatial dimension enter the analysis by two different ways: by using the latitude and longitude position of firms for building spatial econometric matrices of distance within spatial econometric models and by measuring the innovation at spatial/sector level focusing on the LLSs. Most of our sample is made up of firms operating in districts and clusters. In such production environment the circulation of knowledge happens through (formal or informal) networks of division of labour. Besides, knowledge transmission also occurs through the exchange and contagion of ideas and of innovation embedded in the local district atmosphere giving rise to knowledge spillovers.Footnote 2 Hence, in the districts agglomerations innovation spillovers juxtapose the operating of networks. We find strong evidence of productivity spillovers at local level and a relevant impact of innovation on firm productivity. Evidence (albeit weaker) is also found on the impact due to intra-sectoral innovation at local level and to spatial concentration of foreign firms innovation in the LLSs. The paper is organized as follows. Sect. 2 analyses the productivity innovation nexus at local level. In Sect. 3 we describe the data and the model specification adopted to estimate the total factor productivity and the role of innovation at firm level. In Sects. 4 and 5 we develop our spatial analysis first by applying an univariate analysis for spatial dependence and then developing the spatial econometric analysis. Some conclusions follow.",3
45,3,Journal of Industrial and Business Economics,23 June 2018,https://link.springer.com/article/10.1007/s40812-018-0098-9,The geography of technology-intensive start-ups and venture capital: European evidence,September 2018,Massimiliano Guerini,Francesca Tenca,,Male,Female,Unknown,Mix,,
45,3,Journal of Industrial and Business Economics,15 February 2018,https://link.springer.com/article/10.1007/s40812-018-0093-1,A stochastic frontier estimator of the aggregate degree of market power exerted by the US meat packing industry,September 2018,Dimitrios Panagiotou,Athanassios Stavrakoudis,,Male,Male,Unknown,Male,"Red meat production is the largest segment in the agricultural sector of the US economy and one of the most researched industries. It encompasses the farm-to-retail transformation of beef and pork.Footnote 1 The US meat industry accounts for more than $100 billion (USDA 2014) in annual sales and half a million employees. Over the last years, meat-packing firms have increased in size and scope through mergers, acquisitions and vertical coordination. More cattle and hogs are now procured through contracts, also know as captive supplies, giving rise to concerns that packers are “manipulating” cash prices in order to influence the base price used to negotiate contracts. As a result, the US meat packing industry has many times been at the center of controversy regarding the conditions of competition in both the livestock procurement and wholesale meat markets (Azzam 1998; U.S. Government Accountability Office (GAO) 2009). Regarding the US beef industry, between 1980 and 2012, the number of plants decreased from 704 to 168 and the four-firm concentration ratio (CR4) increased from 35.7 to 85% (United States Department of Agriculture-Packers and Stockyards Program 2014). The CR4 has remained around 80% in the last 10 years.Footnote 2 At the same time, the US hog industry has also undergone major structural changes in the last 30 years. According to the Daily Livestock Report released by CME Group (2014), the top four packers control two thirds of the market. Estimating the degree of oligopsony and oligopoly power along the US meat supply chain has been the focus of many studies. The most influential research in the past few years has been the New Empirical Industrial Organization (NEIO), which is an econometric approach that treats market power as a parameter to be inferred from single industries data (Bresnahan 1989). Azzam (1998) summarizes the results of NEIO studies that tested for the presence of market power in both the input (livestock) and the output market (processed meat) in the meat, beef and pork packing industries. The majority of the studies report evidence of market power in the input and/or in the output market. In the US beef sector, Schroeter (1988) finds evidence of oligopsonistic and oligopolistic power exercised by beef processors. Azzam (1992) reports significant findings of oligopsonistic power but finds no evidence of oligopolistic power. Cai et al. (2011a, b) have concluded that processors exert oligopsonistic power when purchasing finished cattle for slaughter. In the US pork packing industry, the empirical results of Azzam et al. (1989) reveal significant evidence of oligopsonistic as well as oligopolistic power exercised by pork processors. Schroeter and Azzam (1990) report statistical significant evidence of oligopsonistic power exercised by pork packing firms. In the (aggregate) meat packing sector, Azzam and Pagoulatos (1990) find evidence of oligopsonistic and oligopolistic power exercised by the US meat packing industry. According to some studies, the magnitude of market power in the US red meat industry is relatively small or is not large enough to warrant concern (Schroeter 1988; Azzam and Schroeter 1991; U.S. Government Accountability Office (GAO) 2009). But, as Sexton (2013) points out, even modest departures from perfect competition—relatively weak oligopoly or oligopsony power especially in the red meat industry—should matter. According to Ward (2010), a small degree of market power can translate into large transfers from livestock producers to packers: a seemingly small impact in dollars per hundredweight can make a substantial difference (losses) to livestock producers. Lastly, there are studies that found no evidence of market power or concluded that the efficiency effects are larger than the market power effects of increased concentration (Paul 2001a, b; Schroeter and Azzam 1991; Sperling 2002; U.S. Government Accountability Office (GAO) 2009). In the light of the preceding, the objective of the present work is to estimate the aggregate degree of market power—oligopolistic and oligopsonistic—in the US meat packing industry, with the use of the recently developed stochastic frontier approach (SFA) by Kumbhakar et al. (2012). In their original work, Kumbhakar et al. (2012) propose a new method of market power estimation. They draw on the stochastic frontier methodology from the efficiency literature in order to estimate mark-ups in the Norwegian saw-milling industry. The authors use both primal and dual specifications to represent the technology and consequently estimate the degree of oligopoly power. Both approaches reveal statistically significant evidence of market power. The primal and dual specifications of the technology is a big advantage of the stochastic frontier approach of market power estimation: in an output market, based on duality theory of cost and input-distance functions, either input price data or quantity price data can be used. On the other hand, duality of revenue functions and output distance functions can be utilized for an input market. This methodology has been applied to measure market power exerted by the Grammen Bank (Bairagi and Azzam 2014) and in the measurement of market power in the Brazilian milk market (Scalco et al. 2017). In the latter study, the authors develop a stochastic frontier model to measure and decompose market power into buyer and seller power. There are two recent studies that apply the stochastic frontier approach on the estimation of market power in the US food industry. Lopez et al. (2017) used the stochastic frontier approach in order to estimate oligopoly power in the US food industry for the period 1990–2010. The stochastic frontier estimator of market power was evaluated with the use of panel data in 42 US food processing industries at the six digit Standard Industrial Classification System (SIC) provided by the NBER-CES Manufacturing Industry Database. The estimated value of the overall average degree of Lerner index was approximately 21%, indicating that all 42 food industries, in the sample, exercise some degree of oligopoly power. Panagiotou and Stavrakoudis (2017) used a stochastic production frontier estimator in order to estimate the mark-down in an input market at aggregate level. The methodology was then employed in order to estimate the degree of oligopsony power in the US cattle industry. The authors used annual time series data from the US cattle/beef industry between 1970 and 2009. The estimated value of the Lerner index was approximately 23%. The empirical findings indicated that beef packers exerted market power when purchasing live cattle for slaughter, for the time period considered in the study. To the best of our knowledge, there has been no published work which has used the stochastic frontier approach in order to explicitly estimate the degree of aggregate (oligopsony and oligopoly) market power in the US meat packing industry. Furthermore, the present study shows that, starting from the basic inequality (\(P>MC\)) of Kumbhakar et al.’s (2012) model, the SFA estimator can measure the sum of the mark-up in the output market and the mark-down in the related primary input market. The present work is structured as follows: Sect. 2 contains the theoretical framework, Sect. 3 the aggregate model and Sect. 4 the data and estimation results. Conclusions are presented in Sect. 5.",4
45,3,Journal of Industrial and Business Economics,17 May 2018,https://link.springer.com/article/10.1007/s40812-018-0095-z,Analysis of the relevance of China’s development for main European automotive manufacturing countries,September 2018,Andoni Maiza,Ricardo Bustillo,,Male,Male,Unknown,Male,"China is the most important automotive market in the world and the one that has attained the highest growth rates during the last decade. Vehicle registrations have risen from 5.8 million units in 2005 to 24.6 million in 2015 because of the growing middle class demand, with millions of consumers often purchasing their first personal passenger vehicle (OICA 2016a). At the same time, the People’s Republic of China (PRC) is the largest single-country producer of vehicles since 2009: in 2015, China produced 24.5 million units, doubling the US in the second place (OICA 2016b). Similar to China, during the 2007–2012 economic recession production and sales grew in several other developing economies such as India, Thailand, Indonesia, Brazil and Russia, whereas they stagnated or declined in most developed economies such as Western Europe, USA and Japan (Peridy and Abedini 2008; Coffin 2013; Eurofound 2013). In these latter countries per capita car ownership is relatively close to saturation levels (Haugh et al. 2010). In fact, the emergence of continent-sized fast-growing new markets altogether with a move towards alternative energies and driving modes represent what some authors have called “a second automobile revolution” (Freyssenet 2009). Inside Western Europe, figures show a contrasting picture (Table 8). In Germany, and, to a lesser extent, the UK, production and employment have experienced a moderate decrease. Meanwhile, in France, Spain and Italy those same variables have registered a substantial decline due to overcapacity problems (Haugh et al. 2010; OECD 2013). To a certain extent, the better evolution of employment in Germany responds to reductions in working hours per employee instead of cuts in the workforce (Burda and Hunt 2011; Brown 2011; Carstersen 2013), but those practices are not enough to explain such different results. The automotive sector “remains of strategic importance and a cornerstone for the EU industry and economy, providing quality employment to millions of workers in the EU” (European Commission 2012a, b). Hence, the objective of achieving a strong manufacturing base in the EU for road vehicles and components requires a deeper understanding of not only the German automotive industry success in China but also the negative results in the rest of the countries. The objective of this article is to analyse the impact of the rising Chinese car market on European automotive manufacturers. More precisely, we determine the relevance of the Chinese market to European car manufacturers, and the participation of the PRC in the international product fragmentation of European producers. To this purpose, the next sections of this paper are organised as follows: firstly, a literature review on international product fragmentation in the automotive sector; next, an exam of EU-China automotive trade figures; afterwards, a detailed analysis of intra-industry trade (IIT) and, finally, the main conclusions. In this research, we confirm that employing national capacity to satisfy the increasing Chinese demand is a key factor to explain the higher performance of German automakers.",1
45,3,Journal of Industrial and Business Economics,30 January 2018,https://link.springer.com/article/10.1007/s40812-018-0092-2,Anthropogenic influence on global warming for effective cost-benefit analysis: a machine learning perspective,September 2018,C. Orsenigo,C. Vercellis,,Unknown,Unknown,Unknown,Unknown,,
45,4,Journal of Industrial and Business Economics,01 November 2018,https://link.springer.com/article/10.1007/s40812-018-0106-0,KIBS for public needs,December 2018,Dmitri Vinogradov,Elena Shadrina,Marina Doroshenko,Male,Female,Female,Mix,,
45,4,Journal of Industrial and Business Economics,11 June 2018,https://link.springer.com/article/10.1007/s40812-018-0097-x,Spatial autocorrelation and clusters in modelling corporate bankruptcy of manufacturing firms,December 2018,M. Simona Andreano,Roberto Benedetti,Federica Piersimoni,Unknown,Male,Female,Mix,,
45,4,Journal of Industrial and Business Economics,07 November 2018,https://link.springer.com/article/10.1007/s40812-018-0105-1,Time allocation behaviours of entrepreneurs: the impact of individual entrepreneurial orientation,December 2018,Evila Piva,,,Unknown,Unknown,Unknown,Unknown,,
45,4,Journal of Industrial and Business Economics,29 October 2018,https://link.springer.com/article/10.1007/s40812-018-0103-3,Mixed duopoly and the indirect effect in linear supply function competition,December 2018,Keita Yamane,,,Male,Unknown,Unknown,Male,"Following the privatization reforms in the United Kingdom in the 1980s, waves of privatization occurred in many other countries and regions. The philosophy of the reform differed slightly in each region, but the common policy aim was to improve consumer and social welfare performance.Footnote 1 While the literature roughly concludes that the privatization policy was successful for the sake of social welfare, and has to be continued, some empirical data and analyses show that the market performance, such as with prices and consumer welfare, between the waves of privatization did not always improve. For instance, although Liberati (2005) investigates the change in real prices in public utilities in the United Kingdom after the privatization, Fig. 1 (pp. 229) shows that the United Kingdom experienced price increase after privatization in electricity, gas, bus, railway, and water services. In fact, the prices in bus, railway, and water service on average increased when compared with 1979, the benchmark year. The results also reveal that consumer welfare in transportation services (i.e., bus and railway) after privatization, from the short, middle, and long period viewpoints, worsened. More stylized facts can be observed in Florio (2007). Although the work investigates the change in market performance in the electricity industry between privatization in the United Kingdom, Italy, France, Germany, and Sweden, it shows that market reform, such as privatization, liberalization, and vertical disintegration, does not always improve prices and welfare. For instance, from 1992 to 1993 in Italy, prices for domestic consumption increased, in spite of the decrease in fuel costs. A similar phenomenon is seen in price for industrial consumption from 1989 to 1992, while in Germany, it is seen in both prices from 1987 to 1988. In Sweden, surprisingly, the price for domestic consumption in the research period (1985–1997) increased by 35%. Drawing from these stylized facts, we must undertake theoretical work to investigate how the structural change of privatization affects market performance. Privatization generally means that the stock holding is alienated to the private sector from the public sector. Thus, theorists have often used the partial privatization model.Footnote 2 However, privatization (and liberalization, the well-known deregulation of conduct) itself can change two other aspects of the economic structure. The first is the degree of product differentiation. Privatization can realize higher degree of product differentiation that the previous public management could not. The second is the competition mode. Privatization can bring firms to be price-oriented, apart from quantity-oriented, even if the number of firms remains unchanged. While theorists have investigated the first two aspects, the change in the stock holding rate and the degree of product differentiation, the third viewpoint, has rarely been the subject of research. Thus, our primary research question is: How does the change in the competition mode/intensity affect market performance? One of the common properties in most privatization is that the market is a mixed oligopoly. Numerous theorists reveal that a mixed oligopoly market—in which profit-maximizing private firms compete against welfare-improving public firms—has some unusual properties that differ from an ordinary oligopoly market.Footnote 3 For example, privatization of a public firm raises consumer prices in the mixed oligopoly framework. However, in this study, we focus on price changes under a condition in which the degree of privatization and the number of firms are given. That is, we are interested in the pure change in performance when competition becomes aggressive, and quantity competition changes to price competition, which extant literature overlooks. We believe that this analysis will help the mixed market grasp its price-changing mechanism from a more accurate perspective.Footnote 4 We thus adopt a linear supply function equilibrium (SFE) approach to compare equilibrium prices between quantity and price competition in a mixed duopoly with product differentiation. SFE has been the focus of a stream of literature, with Cournot and Bertrand equilibria as special cases.Footnote 5 Thus, a variety of competition equilibria are observed in the model, but this model is difficult to treat as an analytical tool because of its mathematical heaviness. However, Menezes and Quiggin (2012), Delbono and Lambertini (2015), and Delbono and Lambertini (2016b) focused on the linear version of SFE (LSFE). They insisted that this approach is optimal because it is tractable, with Cournot and Bertrand equilibria as special cases. There are also numerous studies on mixed markets, though most are devoted to finding the differences between private oligopoly markets through a comparison of Cournot and Bertrand equilibria. Ghosh and Mitra (2010) and Haraguchi and Matsumura (2016) compared quantity and price competition in a mixed oligopoly market and found that reversal phenomena can occur.Footnote 6 Delbono and Lambertini (2016a) compared the Cournot and Bertrand equilibria in an increasing marginal cost case and found the state in which prices and/or firm profit under Bertrand competition are higher than under Cournot competition, which occurs between the two in a private oligopoly market. Additionally, Häckner (2000) analyzed a case of differentiated products. In this study, we combine the mixed market model with the LSFE approach to analyze a more realistic pricing strategy and obtain the results in the privatization context. Thus, this study is an extension of Ghosh and Mitra (2010) using LSFE. By adopting the LSFE approach, we can parameterize the competition intensity and assess infinitesimal variations between Cournot and Bertrand equilibria. Although we concentrate on the differential analysis in Cournot equilibrium, which is the counterintuitive state, we observe theoretical evidence of price fluctuation in our simple analysis. We target the mixed duopoly market using LSFE; however, we adopt our assumption based on Menezes and Quiggin (2012), whereby each firm determines the intercept of the linear supply function, and the slope is given, such as \(q_i=\alpha _i+\beta p\). The intercept-determining behavior implies, for example, the extent of the benchmarked supply capacity that firm i determines in the production planning stage. Here, if one firm expects competition to be aggressive, it will lower its supply capacity, predicting that the variable production \(\beta p\) expands, keeping the balance of residual demand and supply. Thus, aggressive competition implies a drop in \(\alpha _i\), which implies the reduction of production \(q_i\) and an increase in equilibrium prices. Hence, the supply function competition includes the distortion effect (indirect effect) toward price change, in addition to the well-known price reducing effect (direct effect). Additionally, when the indirect effect exceeds the direct effect, the reversal phenomenon occurs (e.g., more aggressive competition yields higher profits for private firms). To our knowledge, many firms do not determine production or prices directly, but indirectly through other tactics, such as the supply capacity. Hence, the LSFE approach elegantly considers real circumstances. We now present the intuitive glimpses on the results in the mixed market case. On the one hand, when a semi-public firm promotes privatization, the market is an ordinary private duopoly. Thus, the competition can be passive. On the other hand, if it promotes nationalization, the competition can be aggressive, since the firm tries to lower its price to improve the consumer surplus. Hence, the indirect effect will be stronger when nationalization holds. We show that the counterintuitive domains are located in areas with high degrees of nationalization and product differentiation. Moreover, the right-upwardness of the threshold curves depends on the degree of the semi-public firm’s cross-price effect, a kind of externality that makes firms’ market power weak owing to the other firm’s pricing. When products are highly differentiated, firms can internalize the effect so that the private firm’s monopolistic pricing, while decreasing intensity of competition for the semi-public firm, enhances its incentives to lower its prices to improve social welfare. The non-monotonicity appears in the prices and profits of both firms. The remaining paper is organized as follows. The next section explains the LSFE model that underpins this study. Section 3 solves the model and presents the equilibria. Section 4 analyzes the change in equilibria when the competition intensity moves from quantity to price and confirms non-monotonicity. Section 5 considers the non-monotonic phenomenon. Section 6 concludes the paper.",1
45,4,Journal of Industrial and Business Economics,25 October 2018,https://link.springer.com/article/10.1007/s40812-018-0102-4,"Industrial spatial dynamics, financial health and bankruptcy: evidence from Italian manufacturing industry",December 2018,Greta Falavigna,Roberto Ippoliti,,Female,Male,Unknown,Mix,,
45,4,Journal of Industrial and Business Economics,15 October 2018,https://link.springer.com/article/10.1007/s40812-018-0099-8,Outside (option) in the orchard: lemons or peaches?,December 2018,Christian Di Pietro,Marco Maria Sorge,,Male,Male,Unknown,Male,"Since Akerlof (1970), an extensive literature has investigated the operation of credit markets in the presence of asymmetric information across market participants (e.g. Jaffee and Russell 1976; Stiglitz and Weiss 1981; Mankiw 1986; Azevedo and Gottlieb 2017).Footnote 1 Following Stiglitz and Weiss (1981), most models of competition under adverse selection point to under-lending (or credit rationing) as the relevant market failure. In a series of influential papers, de Meza and Webb (1987, 1990, 1999, 2000) demonstrate that equilibrium over-lending is bound to occur when distributions of returns to entrepreneurial ventures are ranked by first-order stochastic dominance (FOSD) rather than by increasing risk. Put simply, since ability-increasing (expected) payoffs emerge in entrepreneurship relative to a constant (type-independent) outside option, the marginal entrepreneur is the least talented one (favorable selection), so that good entrepreneurs (the peaches) draw bad ones (the lemons) in.Footnote 2
 From a modeling perspective, de Meza and Webb (1987) consider a heterogeneous ability framework in which entrepreneurs’ outside option is taken to be safe investment, and show that a competitive (zero-profit) equilibrium must be market clearing. Most remarkably, and unlike adverse selection models (e.g. Wilson 1979, 1980; Rose 1993), even when all lenders act as price-takers the credit market equilibrium always exists and is unique, provided the supply of funds to the banking sector is positively sloped. This market feature is robust to the presence of type-dependent outside options whose expected payoff increases with hidden characteristics of individuals, as discussed in Parker (2003). We study equilibrium properties—existence, uniqueness and (in)efficiency—of a credit market model à la de Meza and Webb (1987) when entrepreneurs’ outside option is endogenously determined by occupational choice. In a similar vein, Parker (2003) introduces occupational choice in credit markets with FOSD-type returns to entrepreneurship and shows that, when employers can perfectly screen individual types prior to hiring and optimally decide to do so, greater separation in paid employment than in entrepreneurship may yield credit rationing. In sharp contrast to Parker (2003), we establish that, when individual characteristics are valued in the alternative occupation and yet remain ex-ante unobservable, over-lending is inescapable, and multiple competitive equilibria can arise. Intuitively, under the FOSD assumption, the debt contract enforces favorable selection in entrepreneurship, as in de Meza and Webb (1987); different from the latter, however, the value of the outside option now depends positively on the average ability of individuals who rather sort themselves into paid employment. Thus, a rise in the (uniform) price for credit ameliorates the (conditionally) expected productivity of would-be workers, which in turn drives up the wage, while possibly lowering the average ability of would-be entrepreneurs, consistent with zero expected profits for both lenders and employers.Footnote 3 Our findings show that over-lending proves robust to the introduction of occupational choice in an otherwise standard heterogeneous ability model, when neither banks nor employers can ex-ante screen individual abilities. Under these circumstances, pervasive informational asymmetries will not prevent high-ability individuals from pursuing entrepreneurship and yet induce socially excessive credit provision to low-ability ones. This prediction is consistent with recent applied scholarly work, which attempts to empirically measure the relative importance of credit rationing vis-à-vis over-lending for credit seeking, informationally opaque firms (Bonnet et al. 2016). The remainder of the paper is organized as follows. Section 2 lays down the framework of analysis, whose equilibrium properties are then investigated in Sect. 3. Section 4 concludes.",
46,1,Journal of Industrial and Business Economics,29 October 2018,https://link.springer.com/article/10.1007/s40812-018-0104-2,Improving the developmental impact of multinational enterprises: policy and research challenges,March 2019,Rajneesh Narula,André Pineli,,Unknown,Male,Unknown,Male,"The role of multinational enterprises (MNEs) as catalysts of economic development came to the forefront of development policy thought no more than fifty years ago. A landmark of this “novel” view was a 1973 United Nations’ report titled Multinational Corporations in World Development. According to its preface, The United Nations Economic and Social Council, […] requested the Secretary-General to appoint a Group of Eminent Persons to study the role of multinational corporations and their impact on the process of development, especially that of developing countries, and also their implications for international relations; to formulate conclusions which may possibly be used by governments in making their sovereign decisions regarding national policy in this respect, and to submit recommendations for appropriate international action. […] In conclusion, the report reviews existing policies in respect of multinational corporations and includes proposals for national, regional and international action. (UNDESA 1973, p. vi). Although few today have read this report in its original form, the issues it raises and the fundamental challenges it outlines, remain contemporary concerns. This report proved inspirational in many circles. Almost all of the challenges then outlined have come to shape the policy and academic debate in both Economics and International Business (IB), and through organizations such as the UN Centre for Transnational Corporations (now UNCTAD’s Division on TNCs), into policy discussion. The fact that understanding the role of MNEs in development remains a contemporary issue within research areas such as economics, finance, sociology, ethics and management studies indicates the growing importance of MNEs, and their capacity to inflict change, both positively and negatively. We currently know that MNEs can be engines for knowledge transfer, capital deepening, employment and structural change, but their investments can also be a double-edged sword. Through their actions, both active and the unintentional, MNEs may also produce adverse effects in host countries. They can be anti-competitive, and instead of promoting growth within a sector, they may simply “hollow-out” entire sectors of (weaker) domestic firms. Quoting the United Nations’ report once again “[…] the power concentrated in their hands and their actual or potential use of it, their ability to shape demand patterns and values and to influence the lives of people and policies of governments, as well as their impact on the international division of labour, have raised concern about their role in world affairs” (UNDESA 1973, p. 2). These concerns remain as relevant today as they did in the early 1970s. Policies towards MNEs were a central concern of the foundational work that gave birth to the field of IB studies as a separate discipline. Discussions about government-foreign investor relations and the scope for international regulation of foreign direct investment (FDI) were key issues in the research agenda during the 1970s (UNDESA 1973; Sagafi-Nejad 2008). However, while the MNEs have become the hallmark of globalisation, and the importance of MNE-assisted development now a mainstream plank of almost every country’s development strategy, these policies do not reflect that there are a number of important caveats from the empirical literature. Broadly speaking, most development and FDI policy has not taken into account that the dynamics of FDI and development have evolved, and the presence of MNE investment is still not a sine qua non for development. If successful outcomes of FDI-assisted development were the norm, this faith in the power of the MNE would be justified. On the contrary, numerous relevant matters remain controversial, and only a handful of countries have used a FDI-driven development successfully. The fact is that a burgeoning empirically-driven economics-related literature has tackled the subject with great zest, although the collective weight of these findings has not led to a fine-tuning of policy implementation. In this paper, we bring together a variety of issues that are relevant for the formulation of policies related to MNEs, and to assess their impact on the economic development of host economies (with a particular focus on the developing countries). Our intention here is, on the one hand, to highlight what scholarship in the area knows with a satisfying degree of certainty. On the other hand, we will also delve into what we do not know, as well as topics for which the evidence on the matter is still quite blurred. The analysis we do here is not a purely intellectual one: the economics literature increasingly informs policy studies done by a variety of national governments and supranational agencies (such as UNCTAD, OECD, ASEAN, EU), and itself draws on theoretical contributions from the economics of IB. Secondarily, we are also concerned that a number of insights from these economics studies do not always find their way back into the academic literature in IB and do not always inform new developments in international business theory, or to policy implementation in developing countries. Finally, by identifying the weaknesses in the economics-focused empirical literature, we are able to suggest areas for future research. We examine two aspects of the literature: the microeconomic literature, which is strongly centred on the analysis of FDI spillovers and the macroeconomic literature, which is mostly focused on the FDI-GDP growth relationship. Although they are clearly related, as noted elsewhere, there is a certain degree of ambiguity between the micro and the macro evidence (Narula and Driffield 2012) and sometimes reach discrepant conclusions. They will therefore be discussed separately, what means that inevitably there is some overlap. We do not intend to be exhaustive on the wide empirical literatures in both streams as our focus is on findings that can be informative to policy-making in developing countries. Much of the literature has focused on externalities and spillovers, and deemphasised the direct effects of MNE activity, taking as an implicit assumption that MNEs are almost always beneficial for development. Yet, despite the abundance of empirical studies (of increasing sophistication), in many respects the evidence continues to be ambiguous, with more areas of doubt than certainties. To avoid being repetitive, most recommendations for further research are presented together with the discussion of the extant literature. We conclude by offering some observations about the reasons for the absence of clear evidence, and some of the most pressing challenges facing researchers today.",27
46,1,Journal of Industrial and Business Economics,01 November 2018,https://link.springer.com/article/10.1007/s40812-018-0107-z,Behavioral barriers and the energy efficiency gap: a survey of the literature,March 2019,Laura Abrardi,,,Female,Unknown,Unknown,Female,"Energy efficiency has long been considered a large, low-cost energy resource for the economy of a country. However, despite policymakers’ efforts to improve it, the progress in actually achieving energy savings so far has been relatively slow. While the EU-28’s final energy consumption between 2005 and 2013 achieved roughly a 7% reduction, the performance of the residential sector was limited to 3% (EC 2015a, p. 174). This estimate is particularly disappointing considering the crucial role played by the residential sector in the debate for energy efficiency, both for the weight it has on the total consumption of energy (according to EC 2015b, households in EU-28 account for the 26% of the final energy consumption), and for presenting the highest potential for cost-effective savings. The wedge between potential and actual energy savings derives from two distinct market failures. The first is related to the underinvestment in energy efficiency, namely to the gap between the cost-minimizing level of energy efficiency and the level actually implemented. The second stems from the fact that, given the investment in efficiency, overconsumption occurs. Estimating the actual size of the energy efficiency gap is a challenging task, as it requires to identify all the relevant costs and the correct individual decision making models (Gillingham and Palmer 2014). The possible model and measurement errors triggered a debate around the so called energy paradox, that puts in question the very existence of the gap (Gillingham and Palmer 2014). Although unobserved costs of adoption and modeling flaws could certainly explain part of the gap, there is a broadly held view that various barriers have contributed to the inadequate performance in terms of energy savings (Gerarden et al. 2017). In general, barriers can be defined as “mechanisms that inhibit a decision or behavior that happens to be both energy efficient and economically efficient” (Sorrell et al. 2004). The traditional economic literature identifies these barriers in informative and market failures. Informative failures weight heavily because of the nature of the investment in energy efficiency, being it a complex and unfamiliar good, purchased infrequently in a market with multiple suppliers and intermediaries where the rate of technical change is rapid relative to the purchase interval (Hewett 1998). The inefficiency resulting from informative barriers is often compounded by a lack of accountability problem (the so called split incentives), that occurs when the potential adopter of an investment is not the party paying the energy bill, as in the case of rental houses (Jaffe and Stavins 1994; Davis 2010; Gillingham et al. 2012). A further source of inefficiency is related to the presence of unpriced environmental externalities from the use of fossil fuels or renewable energy, due to tariffs that offer time-invariant prices, not reflecting the variation of marginal costs between expensive peak periods and lower cost off-peak periods (Brennan 2011; Chu and Sappington 2013; Abrardi and Cambini 2015). Fortunately, the traditional literature is also able to suggest a variety of solutions to address these failures. In the presence of rational consumers, information deficiencies could be addressed by education programs, minimum efficiency standards and mandatory labelling. The split incentives problem can be lessened by the use of submetering, which increases the accountability of the energy use, and by competition between rental property owners, which leads to offer appliances that allow a reduction of the energy bill for tenants (Brennan 2016). Time-varying tariffs such as time-of-use (the tariff varies according to the time of day), Critical-peak-pricing (higher tariff during critical peak times) or peak-time-rebate (a flat tariff with rebates if the usage is lower than a certain benchmark at critical times) help to achieve higher efficiency by reducing the energy usage at peak hours, while Pigouvian taxes (such as the carbon tax) and tradeable permits specifically address the problem of environmental externalities. However, it has become painfully obvious that these instruments alone are unable to fill the gap between optimal and actual energy usage and investment decisions. The residual inefficiency resulting in overconsumption and underinvestment decisions can be largely imputed to behavioral barriers, which explain the consumer’s suboptimal choices by challenging the traditional hypothesis of perfect rationality. Apparently, consumers fail to make optimal investment or consumption decisions for a number of reasons, whose common ground is that people make mistakes and fail to act in their own best interest. Then, behavioral barriers pertain to the consumers’ difficulties in the collection and processing of information and to psychological biases. If consumers’ rationality fails, traditional policy measures are often insufficient or inadequate. In the case of informative barriers, for example, information provision does not automatically imply consumers’ understanding. Indeed, when evaluating the US labelling program, Egan (2001) finds that consumers do not appear to fully understand continuous scales. Then, policies directed at providing information to consumers have to account for the ways consumers deal with it. As bounded rationality severely undermines the success of the intervention programmes specifically designed to improve energy efficiency, a deeper examination of the nature of the different failures involved is important to design an appropriate policy response to each of them (Stern et al. 2016a, b; Cambini 2016). This paper reviews the main economic literature on behavioral barriers, focusing on those that are most relevant to explain the persistence of the efficiency gap in the energy sector. We discuss the policy implications of these behavioral failures, and examine the empirical evidence and the policy instruments that have been used to promote energy efficiency. The remained of the article is organized as follows. In Sect. 2 we review behavioral failures that are driven by nonstandard decision making mechanisms, namely decision heuristics, limited attention, inertia and framing and salience effect. In Sect. 3 we analyze the behavioral failures due to nonstandard preferences and beliefs, in the form of loss aversion, present bias and motivation. Section 4 concludes.",13
46,1,Journal of Industrial and Business Economics,24 October 2018,https://link.springer.com/article/10.1007/s40812-018-0101-5,Necessary demand and extra demand of public utility product: identification using the stochastic frontier model,March 2019,Eri Nakamura,Fumitoshi Mizutani,,Female,Male,Unknown,Mix,,
46,1,Journal of Industrial and Business Economics,01 February 2019,https://link.springer.com/article/10.1007/s40812-019-00110-4,Investment and market power in mobile mergers,March 2019,Georges V. Houngbonon,Francois Jeanjean,,Male,Unknown,Unknown,Male,"Telecommunications services generate significant benefits for the whole economy.Footnote 1 As a result, the effects of mergers between telecoms operators on the intensity of competition and welfare is heavily scrutinized by antitrust authorities. For instance, mergers between Orange and Three in Austria (2012), and between Telefonica and E-Plus in Germany (2014) were approved, whereas the proposed merger between Telefonica and Hutchinson in the UK was not approved.Footnote 2 These decisions generate important policy debates as, to a certain extent, they reflect a lack of empirical evidence on the effects of mergers in the mobile industry. However, ex-post evaluations of approved mergers found ambiguous effects on price per user and investment, and did not investigate the impact on consumer surplus.Footnote 3 Contrary to many other industries, price is not the only strategic variable in the mobile industry. The provision of mobile services requires significant investment in mobile network technologies. This investment can benefit consumers in terms of higher quality and a lower marginal cost of production, but is affected by competition as shown by Genakos et al. (2018), Houngbonon and Jeanjean (2016) and Jeanjean and Houngbonon (2017). Typically, in symmetric markets, investment in mobile networks falls with the number of operators. As a result, a reduction in the number of operators caused by a merger is expected to lower static efficiencies, due to greater market power, but also to raise dynamic efficiencies, due to greater investment. The overall welfare effect depends on consumers’ valuation of quality or the magnitude of the impact of investment on the marginal cost. In this paper, we estimate a model of oligopolistic competition to evaluate how an exogenous reduction in the number of mobile operators affects consumer surplus. We use a Cournot model with investment in cost-reducing technologies. This assumption is made in order to overcome the lack of data on network quality. At symmetric equilibrium, this simple model yields the essential features of pricing and investment in the mobile industry demonstrated by previous studies. In particular, investment falls with the number of operators, and the effect of the number of operators on price and consumer surplus depends on a trade-off between static efficiencies, stemming from a change in market power, and dynamic efficiencies, stemming from a change in the incentive to invest. Using data on mobile Internet traffic from 18 European markets, as well as data on operators’ market shares and investments, we were able to recover estimates of demand elasticity for each market. Demand estimation relies on the generalized method of moments estimator (GMM) with the lagged of price and mobile termination rates as instruments. We used the market level demand elasticities to obtain estimates of marginal cost. Using a non-parametric approach, namely locally weighted scatter-plot smoothing (Lowess), we identified a log-linear relationship between marginal cost and investment. The parameters of the marginal cost function were estimated using the GMM estimator with the lagged of investment as instruments. The estimates of demand and cost parameters determine the relationship between the number of operators and investment. For an average European market, we found that the absolute demand elasticity of mobile Internet is 1.02, and that the elasticity of marginal cost with respect to investment is 1.36. We used these parameters to simulate the effects of an exogenous reduction in the number of operators on price and consumer surplus in a symmetric market. We found that price per megabyte (MB) falls whereas price per user increases with a 4-to-3 or 5-to-4 merger. However, we found the reverse for 3-to-2 merger. This means that price per MB is minimized and consumer surplus is maximized in markets with 3 symmetric operators. The findings of this paper contribute to the literature on the effects of market structure in dynamic frameworks such as those in Vives (2008) and Schmutzler (2013). In particular, it provides complementary evidence to the strand of the literature that investigates the empirical effects of mergers in the mobile industry. Genakos et al. (2018) find that mobile merger tend to raise price per user as well as investment per operator. Grajek et al. (2017) investigate the impact on price per minutes of voice and investment.Footnote 4 In this paper, we focus on the impact of the number of operators on price per MB and consumer surplus. The remainder of the paper is organized as follows. Section 2 provides background information about investment and pricing in the mobile industry. Section 3 presents the Cournot’s model with investment in cost-reducing technologies. Section 4 presents the empirical model, estimation results and counterfactual simulations. Finally, Sect. 5 concludes.",1
46,1,Journal of Industrial and Business Economics,10 November 2018,https://link.springer.com/article/10.1007/s40812-018-0108-y,The determinants of corporate profitability in the Italian domestic appliances industry,March 2019,Fabio Pieri,Riccardo Verruso,,Male,Male,Unknown,Male,"The disappointing performance of the Italian economy over the past 25 years in reference to the dynamics of GDP per capita, productivity growth and competitiveness has been discussed in both academic works and the popular press (see, Daveri and Jona-Lasinio 2005; Krugman 2012; Hassan and Ottaviano 2013; Pinelli et al. 2015; Calligaris et al. 2016; Bugamelli et al. 2018; among others). Part of this economic sluggishness has been ascribed to the Italian specialization (Onida 1998; Faini and Sapir 2005; Di Maio 2014) in industries with an intensive use of medium and low-skill labor and characterized by a prominent role of the internal demand (Onida 1998, p. 280; Sica 2014, p. 36). These industries have been very much affected by the relevant changes happened in the global scenario from the mid-1990s (i.e., the introduction of the euro and the process of enlargement of the European Union (EU); China’s entry in the World Trade Organization; the end of the Multi Fibre Arrangement; see De Nardis 2014). Notwithstanding the industrial specialization has been found to explain part of Italy’s economic poor performance in several empirical works, recent contributions have challenged the idea of a pivotal role of it in favor of a higher relevance of other factors (e.g. the exceptional relevance of micro-sized firms in all industries; see, Bugamelli et al. 2012; De Nardis 2014; among others). Besides, the specialization of the Italian economy in medium and low-skill labor-intensive productions appears quite stable over time (De Benedictis 2005; Bugamelli et al. 2010; Lucchese et al. 2016) and a significant change of it seems a long-term and difficult to be managed process (De Nardis 2014). Given the aforementioned reasons and the empirical evidence on the remarkable within-industry firm heterogeneity (in terms of productivity, innovation, size, and profitability) across all sectors (Dosi 2007; Syverson 2010), it is relevant to assess the determinants of firm success within those industries historically characterized by an Italian comparative advantage in the international markets. Within each of these “traditional” sectors, some firms have been successful, positively contributing to the Italian aggregate performance, while many others have performed poorly and decreased the country’s competitiveness. Thus, it is key to understand which factors have differentiated the former from the latter group. Among the industries that Fortis (2005) has defined as the “4F’s” of Italian specialization (Fashion and cosmetics; Food and wine; Furniture and ceramic tiles; Fabricated metal products, machinery and transport equipment), domestic appliances have a prominent role. In the 1950s and 1960s, a diffused entrepreneurial ability together with an abundant and adequate workforce were able to exploit a fast-growing demand for appliances to make the industry as one of the most relevant for the Italian economy. Over the 1980s, the demand became mostly for the replacement of old appliances and the Italian manufacturers began to face a rise in import competition from other countries. These factors created a pressure on price and on firm profitability. From the beginning of the 2000s, the major competitors for the Italian manufacturers came from Turkey and—due to the process of EU enlargement to the east—countries such as Poland, Hungary, Slovenia, Romania and Czech Republic. Thanks to an advantage in cost competitiveness, these countries gained larger shares of the European market in terms of number of firms and turnover (Sica 2014, pp. 61–62). From 2002 onwards, this fierce competition produced a contraction in the total output of the Italian manufacturers of domestic appliances and a reduction of their export shares in the global market. Being durable goods, the consumption of domestic appliances has been severely hit by the Great Recession (Petev et al. 2011): as for Italy, apparent consumption of domestic appliances decreased from 10947 to 8029 million euro in the period 2007–2013 (CECED Italia 2015, p. 14). In this paper, we inquire into the determinants of firm profitability in the domestic appliances industry in Italy. Profitability is a widely used metric to asses a firm’s success in the market. The domestic appliances industry is an interesting case study for the analysis of profitability for several reasons. First, it is a relative mature and rather concentrated industry: waves of mergers and acquisition (M&A) took place during the 1970s and 1980s (see Sect. 3) and these processes may have affected profitability through a change in the market structure. Second, in 2004 and 2007, the steps in the process of EU enlargement have raised competition for the Italian producers due to the entry of firms from Eastern European countries: this may have negatively affected the profitability of the Italian manufacturers. Third, the focus on a single industry allows one to conduct the analysis in a rather homogenous set of firms, whose managers should face similar external conditions. Making use of a dataset of about 140 Italian producers of domestic appliances observed in the period 2007–2016, we perform a set of regressions that take the dynamics of firm profitability, together with endogeneity and unobserved heterogeneity into account. Results show that firm productivity and the labor cost per employee are key determinants of profitability in this industry. Firm productivity is positively related to profitability, and this is coherent with both the maturity of the industry and the relevance of those firms’ characteristics (such as innovation, the adoption of new technologies, higher quality of employed inputs and better managerial practices) aimed at establishing and maintaining a sustained advantage with respect to competitors. Ceteris paribus, a higher labor cost per employee is associated with lower profitability and we put this in perspective with the fierce competition coming from countries characterized by a low cost of labor. Moreover, we find that the firm’s (absolute) size, its financial structure and its market share, all play a role in determining differences in profit rates among firms. The higher profitability of producers with a smaller productive capacity (proxied by total fixed assets) may be the result of the adoption of assembly technologies developed starting from the early 1980s, which granted more flexibility to smaller firms and, at the same time, the operation of larger firms below their full capacity in a period of sluggish demand. More leveraged firms show, ceteris paribus, lower profit rates and this may be related to liquidity constraints and, as a consequence, the lack of profitable investments. Finally, a producer’s market share (in terms of revenues) is positively (albeit weakly) related to profitability, given its relationship with the ability of the firm to establish customer loyalty (by investing in “quality reputation”) in an industry with marked brand segmentation. Our results provide insights into the study of the determinants of firm success in a mature industry. Both managerial strategies and policy interventions, as outlined in Sect. 6, should take the maturity of the domestic appliances industry (Baden-Fuller and Stopford 1991; Paba 1991) into account in order to take proper decisions, aimed at adapting firms to the ongoing changes in consumers’ needs and environmental requirements, and sustaining the sectoral competitiveness in the global market. This may be particularly important for countries like Italy, where mature and “traditional” industries represent a relevant share of manufacturing (Fortis 2016, pp. 87–88). The structure of the paper is the following. Section 2 overviews the main theoretical and empirical contributions about the determinants of firm profitability. Section 3 describes the Italian domestic appliances industry. Section 4 presents the data used in the analysis and provides some descriptive statistics. Section 5 presents the empirical framework and the econometric results. Section 6 offers some insights for managers and policy makers. Section 7 concludes.",2
46,1,Journal of Industrial and Business Economics,09 February 2019,https://link.springer.com/article/10.1007/s40812-019-00111-3,Digital piracy in Asian countries,March 2019,Koji Domon,Alessandro Melcarne,Giovanni B. Ramello,Male,Male,Male,Male,"This paper presents a cross-country investigation on peer-to-peer attitude (p2p) in four Asian countries: China, Vietnam, South Korea and Japan. This specific perspective is rather non-conventional in the pre-existing literature on piracy, where the focus has been essentially directed towards western society: mainly, the US and Europe. The present study essentially relies upon a behavioral investigation on copyright infringement in the music domain, exploiting a unique survey data collected through field research. The peculiar geographical setting appears useful in order to disentangle not only the characteristics that differentiate western consumers from Asian ones. The cross-country perspective will also try to unveil why, within the same continent, dissimilarities in consumers’ behavior exist among countries. Interesting results emerge, suggesting that differences in the local specificity that we can shortly define as social norms seem to play a fundamental role in explaining different behaviors with similar legal frameworks and may be very important in predicting the success of new law amendments. This finding in turn seems to be substantially relevant, especially when considering that the normative attitude has leaned so far towards a worldwide harmonization of the legal framework by means of international agreements, chiefly lead by the TRIPs agreement. With some caveat, we are herewith able to suggest that “one size fits all” copyright policies, extensively adopted by WTO and other international institutions in order to foster the legal harmonization, do not work equally even within the same geographical area, since even historically related countries can differ significantly. Moreover, empirical evidence shows that a longer tradition in protecting creative works in a given country does not have a relevant impact on P2P behavior. On the whole the above suggests that social norms seem the most relevant element determining piracy across all countries considered in our analysis. Policy making should also give more consideration to this locally specific source of regulation rather than simply amending the formal legal framework. The rest of the article is organized as follows: Sect. 2 provides a short description of the interplay between copyright and piracy and the way of technological changes, legal frameworks and the social context play in this phenomenon. Section 3 describes the methodology and the data gathered while Sect. 4 deals with the empirical analysis and the results. Section 5 thus tries to draw the major policy implications, while Sect. 6 concludes.",1
46,2,Journal of Industrial and Business Economics,06 April 2019,https://link.springer.com/article/10.1007/s40812-019-00117-x,Grand challenges and new avenues for corporate governance research,June 2019,Marc Goergen,Laura Rondi,,Male,Female,Unknown,Mix,,
46,2,Journal of Industrial and Business Economics,14 March 2019,https://link.springer.com/article/10.1007/s40812-019-00115-z,Corporate governance: what we know and what we don’t know,June 2019,Julie Elston,,,Female,Unknown,Unknown,Female,"Corporate governance refers to the system of rules, practices and processes by which a firm is operated and controlled. As such it inevitably involves balancing the various interests of the firms’ stakeholders, including shareholders, management, customers, suppliers, financiers, government and the community. These stakeholder relationships, while driven by economic incentives, invariably involve the country specific legal and economic institutions which surround the firm, which is why so many empirical studies seek to exam firm governance problems at the country-specific level. It is also why cross-country comparisons can enlighten us about the manner in which firms around the world experience and solve governance problems in different institutional environments. The Shleifer and Vishny (1997) survey of corporate governance frames the agency problem rather specifically as “the ways in which suppliers of finance assure themselves a return on investment”. Here the underlying issue lies in the concern that the professional manager may effectively divert funds or free cash flow from the firm to themselves rather than return it to investors. This is commonly referred to as the agency problem of the modern firm, a problem which in a Berle and Means (1932) sense is characterized and driven by the inherent separation of ownership and control of the stock held firm. Empirical studies in the literature often focus on identifying and measuring how specific corporate governance mechanisms can be used to mitigate this fundamental agency problem. These governance mechanisms which are broadly discussed across in literature include (Shleifer and Vishny 1997; Thomsen and Conyon 2012; and others): Regulation (company law, shareholder protection laws, laws against self-dealing, contracts), Ownership (large or dominant shareholders, shareholder activism and takeovers), boards. Compensation practices (incentive based). Stakeholder pressure (credit monitoring, auditors, analysts, competition) and Informal governance measures (social norms, codes, reputation and trust). And since there is empirical evidence of interdependence between these mechanisms (Agrawal and Knoeber 1996), best practices suggest that when possible these mechanisms should be examined in tandem. Empirical studies have found that specific types of ownership (concentrated or dominant owners) and their associated voting rights can weigh heavily in firm decisions in providing a source of control. Dominant shareholders may include: institutional investors like banks (Germany), the state (China), foundations (Denmark), families (Italy, Germany, and China), and corporate ownership (Japan-Keiretsu, France, and Sweden). Generally speaking, we consider good corporate governance as one which creates a transparent set of rules and controls, and in which shareholders, directors and officers have aligned incentives. Bad corporate governance casts doubt on management’s integrity in sharing firm profits and is often associated with less than transparent systems. Poor corporate governance can lead to executive compensation packages which fail to create optimal incentive for corporate officers to make decisions in line with shareholder values; while poorly structured boards make it difficult for shareholders to oust ineffective incumbents. An extensive theoretical and empirical literature has ensued to identify these corporate governance mechanisms and examine how well they address agency issues.Footnote 1 Regarding use of contracts, Grossman and Hart (1986) and Hart and Moore (1999) suggest that designing contracts in which all foreseeable contingencies knowable is unrealistic, rendering a complete contracts solution as non-feasible. In this case they suggest, it is the residual control rights -the right to make decisions in circumstances not foreseeable or covered in the contract that may provide the efficient allocation of control rights. Another solution is to provide incentive contacts that better align the managers interests with those of the investor. Executive pay and firm performance studies are examples of studies on the effectiveness of incentive contracts. Early studies including Murphy (1985); Coughlan and Schmidt (1985), and Jensen and Murphy (1990) provide empirical evidence of a positive link between pay and performance in the US. Many subsequent pay-for-performance studies have found similar links in Germany, Japan and other countries. However, one problem with the incentive contract mechanism is that they can also create opportunities for management self-dealing (Yermack 1997). In response to self-dealing opportunities of managers, many countries have engaged in building both regulatory and judicial systems to address abuses stemming from excessive self-dealing. Many countries, including the US and elsewhere, have responded to these shortcomings by providing legal frameworks in the form of company law, shareholder protection laws, and other laws to reduce management self-dealing. There are also a number of studies which provide empirical evidence that agency problems can be aggravated by threats to managers of their loss of private benefits or control of the firm (Shleifer and Vishny 1997). These studies indicate that particular types of owners are better able to maintain control over management and minority shareholders. For example, concentrated ownership (France, Germany, Nordic countries and China) or dominant shareholders (like banks in Germany or the state in China), employee ownership, or compensation boards (like in China), are all means of maintaining control over the firm and management. Overall, the evidence on the importance of agency costs is compelling, and Shleifer and Vishny 1997 concludes that clearly “control is valued”. Further, Zingales (1995) provides empirical evidence for the US that superior voting rights trade at a premium and that controlling management can lead to benefits not available to minority shareholders. There is also empirical evidence that suggests that voting premiums may be even more striking in other countries, including Israel, Italy and Russia (Levy 1983; Zingales 1995; Boycko et al. 1993). One of the most important rights of shareholders is the right to vote in matters of strategic importance to the firm, including mergers, acquisitions and election of the board of directors. These boards structures also tend to vary between countries, and in some countries such as Germany, Austria and in Scandinavia employees are on the governing boards, which provides another means of control. Each corporate governance system has its own nuances which both impact and reflect the corporate governance structure and practices of that country. In the US and UK hostile takeovers are a common means of removing management from control of the firm, where poorly managed target firms are bought by the acquiring firm in what is a market solution to the agency problem. In terms of board structures, the US, UK, France and Japan have single tiered board systems, while Germany, Scandinavia, and China have two tiered systems—but each is quite different from each other in spite of similar tiers. Japan for example is known for its’ insider dominated boards and cross-ownership, while the US clearly does not have these features. Overall, evidence is rather mixed on which countries may have the best system of corporate governance or the limits of convergence of these diverse systems, suggesting a need for future studies to fill gaps in our knowledge of these systems systems around the world.",8
46,2,Journal of Industrial and Business Economics,22 October 2018,https://link.springer.com/article/10.1007/s40812-018-0100-6,"Dividend policy, corporate control and the tax status of the controlling shareholder",June 2019,Christian Andres,André Betzer,Marc Goergen,Male,Male,Male,Male,"There exists a vast body of empirical research on the dividend behaviour of US and UK corporations. However, much less is known about the dividend policy of firms based elsewhere. The literature is even sparser on the link between dividends and control across the world, and in particular minority shareholder expropriation via the dividend policy. As DeAngelo et al. (2008, p. 218) state, “there is much yet to be learned about the nature and scope of minority stockholder exploitation”. This lack of evidence is highly surprising given that theory predicts that there should be a link between dividend policy on the one side and large shareholder control and the danger of the minority shareholders being expropriated on the other side. In addition to the absence of a large body of literature, the only two cross-country studies on the link between dividend policy and control—La Porta et al. (2000) and Faccio et al. (2001)—have both considerable limitations in terms of how they measure corporate control. While La Porta et al. (2000) use the quality of law which is correlated with the average concentration of control in a country, they do not directly measure corporate control at the level of the individual firm. In contrast, Faccio et al. (2001) measure control at firm level. However, while they account for the fact that control over a firm may be held indirectly, via e.g., pyramids of ownership, they make the fairly strong assumption that any unlisted firm holding a stake in one of their sample firms is family controlled. Hence, their study likely overestimates the importance of family control in countries where other types of large shareholders also hold control indirectly. Our study suggests that Germany is one of these countries. In addition, neither study adjusts for the tax status of the controlling shareholder. However, as e.g., Korkeamaki et al. (2010) document, the tax incentives of the firm’s largest shareholders may explain aspects of dividend payout behaviour. By investigating a major tax reform in Finland, they show that firms adapt their dividend policy to the modified tax incentives of their largest shareholders. Our study purports to address these two limitations by investigating the impact of actual ultimate control as well as the tax status of the controlling shareholder on the dividend payout for the case of German firms during 1984–2005. Our sample covers more than 90% of the market capitalization of all German exchange-traded non-financial firms during the whole sample period and is therefore highly representative of the German capital market. Germany is a rich laboratory for the study of the effects of control on dividends as there is a sizeable percentage of firms with concentrated control in the hands of families, banks or other corporations as well as a sizeable percentage of firms that are widely held. In contrast, given the dispersion of control in the UK and the US studies on those countries are typically only able to analyse the impact of managerial and relatively small institutional stock holdings on dividend policy. In addition, our study also adjusts for country-specific characteristics that are normally omitted in cross-country studies. These characteristics include the existence of guaranteed dividends on preference stock, so called ‘specially designated dividends’ and ‘control agreements’ that are frequently in place in firms controlled by other firms. Hence, if there is a link between control and dividend policy our study is well equipped to detect it. We find strong evidence of a non-linear link between dividend payout and corporate control. In particular, our results suggest that firms controlled by families have higher dividend payouts if the proportion of votes held by the family ranges between 25 and 50%, whereas a voting stake below 25% or beyond 50% is associated with lower dividend payouts. In addition, we find a positive relation between corporate control and dividend payout, if an industrial or commercial corporation holds the majority of votes in the company. Finally, we do not find evidence that the controlling shareholder’s tax status matters for dividend payout decisions. Our results are in marked contrast with those of Barclay et al. (2009) for the USA. They do not find that large shareholders have an impact on dividend policy. One reason why they do not find such a relation may be that 68% of their sample firms with a blockholder do not pay dividends. The equivalent percentage for our study is less than 21% of firm-year observations. Another reason may be that ownership and control is highly concentrated in our German sample with more than three quarters of firm-year observations with a blockholder holding at least a 25% stake compared to only about 20% of firms in the Barclay et al. sample. Our results also stand partly in contrast to those of Schmid et al. (2010) who report an overall positive and linear impact of family control on dividend payments for Germany. However, their analysis differs from our analysis in at least four aspects. First, they do not employ a dynamic panel data approach. Second, they do not allow for the impact of ownership on firms’ agency costs to be non-linear. Third, they focus on the impact of founding family ownership on the payout behaviour while we look at a whole range of shareholder types. Finally, they ignore the tax incentives of the largest shareholder. The remainder of this paper is organised as follows. The next section reviews the literature. Section 3 discusses sample and other data issues as well as the measurement of control. The following section contains the descriptive analysis. Sections 5 and 6 are about the multivariate analysis, focusing on the methodology and the estimation results, respectively. Finally, Sect. 7 concludes the paper.",4
46,2,Journal of Industrial and Business Economics,18 February 2019,https://link.springer.com/article/10.1007/s40812-019-00112-2,The role of venture quality and investor reputation in the switching phenomenon to different types of venture capitalists,June 2019,Annalisa Croce,Elisa Ughetto,,Female,Female,Unknown,Female,"Venture capital investors (VCs) devote considerable effort to deciding in which entrepreneurial ventures to invest in order to filter out poor investment opportunities. In principle, a positive sorting mechanism characterizes the VC market: on the one hand, more reputable and experienced VCs tend to “cherry-pick” the best companies (Bertoni et al. 2016), on the other hand, higher quality entrepreneurial ventures actively seek VC and favour likewise more reputable VCs (Hsu 2004; Sørensen 2007).Footnote 1 Entrepreneurial ventures can be funded by one VC or by a group of VCs acting in syndication, where a lead investor is responsible for the deal (i.e. sets the price and terms of the investment, provides a large part of the capital, and usually agrees to represent the entire round on the board). Before each follow-on round takes place, investments are carefully re-evaluated. A large amount of investments is quickly exited because the extant VC prefers to focus its own resources on prospective “home runs”. Ventures that are able to raise money in follow-on rounds can continue to be funded by the same VC or by a new one. Whenever the lead VC in previous financing rounds changes in the current round, a switching of lead VC takes place. While the mechanisms affecting the initial selection of the ventures by VCs have been largely investigated (Fitza et al. 2009; Fried and Hisrich 1994; Shepherd 1999), the subsequent dynamics concerning the decision of both VCs and entrepreneurial ventures to maintain the extant relationship in follow-on rounds rather than to exert the abandonment option have received relatively little attention. This is quite surprising given the relevance of the phenomenon. In fact, it has been observed that from 1991 to 2002 in the 23% of the cases lead VCs of follow-on rounds of financing are different from those of previous rounds (Cumming and Dai 2013). To our knowledge there are only two works explicitly studying the switching phenomenon in the VC industry (Abrardi et al. 2018; Cumming and Dai 2013). Another related small group of studies look at the dynamics that stimulate the investment by other VCs in follow-on rounds, also when mixed funding applies (Brander et al. 2015; Guerini and Quas 2016). Cumming and Dai (2013) analyse the switching phenomenon using a sample of 1385 US VC investment rounds. The authors, who consider only investments by independent venture capital funds (IVCs), find that ventures whose perceived quality is upwardly revised are more likely to switch to more reputable IVCs and to accept lower pre-money valuation and smaller investment size in follow-on rounds. Abrardi et al. (2018) propose a formal theoretical model to explain why an incumbent VC should “dump” a venture and a new VC should take it on. They concentrate on the circumstances in which the switching occurs from a lead IVC to a governmental VC (GVC) and vice versa. The model is empirically tested on a sample of 15,218 rounds of financing in 10,912 entrepreneurial ventures in the US between 1998 and 2010. Their analysis sheds light on the puzzling evidence that new and more reputable lead GVCs are more likely to invest in low economic return ventures (previously backed by an IVC), but at the same time they are more inclined to drop them in favour of a less-reputable IVC. We build on the empirical findings of Abrardi et al. (2018) by examining the switching phenomenon in the light of different VC investor types. VCs represent a heterogeneous crowd with different governance structures. They can be independent firms where a management company (general partner) is investing capital raised from limited partners (IVCs). In this type of governance structure, general partners are independent in their investment selection and management and are not subject to any interference by limited partners. The governance of captive (i.e. non-independent) firms implies that a parent company (i.e. a financial institution in case of BVCs, a corporation in case of CVCs or a governmental body in case of GVCs) can exert a substantial influence on the management of the fund. A growing body of research has started to recognize the heterogeneity of VC firms (Bertoni et al. 2015; Manigart et al. 2002; Norton and Tenenbaum 1993), focusing on how they differ in terms of evaluation methods, competences, investment patterns, governance mechanisms and objectives. However, these different governance schemes, which affect investors’ investment strategy and portfolio management (Bertoni et al. 2015; Manigart et al. 2002) are likely to influence switching behaviours as well. So far, there has not been any scholarly attempt in the literature to relate structural differences in the VC funds’ governance to switching dynamics. Discussing why switching occurs and the way it affects different types of VCs, driven by different governance mechanisms, is central to study the interplay between strategy and financing in entrepreneurship. Moreover, exploring the complementarity between the perspectives of incumbent and new VCs and that of entrepreneurs provides insights that have not been combined so far in the context of previous literature on VC switching. In this study, we are not able to identify which is the subject who actually takes the initial decision to switch. In that regard, we have made an effort to provide strategic considerations of why some entrepreneurial ventures may switch from one investor to another one, looking at two relevant dimensions: the perceived quality of the venture and the reputation of the incumbent VC. We thus assume that the roles played by the incumbent VC’s reputation and by the ventures’ perceived quality may explain the patterns of the two sided-matching between entrepreneurs and VCs and its potential dynamic adjustment. However, so far, little is known on the effects that venture quality and VC reputation entail on the change of lead investor and how these effects vary when different types of VC governance structures are considered. Differently from Abrardi et al. (2018), we investigate whether and how the perceived quality of a venture and the reputation of the incumbent VC are affecting the propensity to switch from an IVC to another IVC, or to other types of VC investors (e.g. GVC, BVC and CVC). The discussion on the switching between an IVC and a GVC is reintroduced for allowing a comparison of the switching dynamics across all the different types of investors. In addition, we also intend to provide some evidence on the effect of switching the type of lead VC investor on both the performance of entrepreneurial ventures [proxied by the probability of successful exit via an initial public offering (IPO)] and the evaluation that they receive at the time of financing. The remainder of this paper is organized as follows. The second section puts forward some testable hypotheses in the context of prior research. The third section introduces the dataset and the summary statistics. The fourth section presents the econometric models used and illustrates the results. The final section concludes the paper and discusses the implications of our findings.",2
46,2,Journal of Industrial and Business Economics,19 February 2019,https://link.springer.com/article/10.1007/s40812-019-00113-1,The changing patterns of venture capital investments in Europe,June 2019,Fabio Bertoni,Massimo G. Colombo,Francesca Tenca,Male,Male,Female,Mix,,
46,2,Journal of Industrial and Business Economics,17 December 2018,https://link.springer.com/article/10.1007/s40812-018-0109-x,Voting rights delivery in investment-based crowdfunding: a cross-platform analysis,June 2019,Alice Rossi,Silvio Vismara,Michele Meoli,Female,Male,Female,Mix,,
46,2,Journal of Industrial and Business Economics,27 April 2019,https://link.springer.com/article/10.1007/s40812-019-00118-w,The impact of governance signals on ICO fundraising success,June 2019,Giancarlo Giudici,Saman Adhami,,Male,,Unknown,Mix,,
46,3,Journal of Industrial and Business Economics,20 July 2019,https://link.springer.com/article/10.1007/s40812-019-00126-w,"Digitalizing industry? Labor, technology and work organization: an introduction to the Forum",September 2019,Valeria Cirillo,José Molero Zayas,,Female,Male,Unknown,Mix,,
46,3,Journal of Industrial and Business Economics,08 June 2019,https://link.springer.com/article/10.1007/s40812-019-00123-z,Is this time different? A note on automation and labour in the fourth industrial revolution,September 2019,Luigi Marengo,,,Male,Unknown,Unknown,Male,"Nearly 90 years ago John Maynard Keynes published a short and visionary article where he tried and imagine economic life in two generations. He envisaged a sharp decline of labour demand due to technological substitution, that he called “technological unemployment”: “We are being afflicted with a new disease of which some readers may not yet have heard the name, but of which they will hear a great deal in the years to come—namely, technological unemployment. This means unemployment due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour.” (Keynes 1930, p. 360, emphasis in original). Technological unemployment is indeed an unavoidable and ubiquitous consequence of technological change based on the introduction of new capital goods. At the outset of the first industrial revolution Luddites destroyed textile machinery because it was stealing their jobs and wages. Indeed, we can now say that Luddites were right in the short run but utterly wrong in the long run. The machines which the first industrial revolution was based on did destroy jobs of the workers they replaced, but they hugely increased the productivity of labour, increased its value and remuneration and, especially, lots of new jobs were created by the new organization of production. Machines had to be produced by the rising machine sector. The factory system was “delegating” many simple routine production tasks to machines but also creating opportunities for new less simple and routine tasks. For instance, the factory system was a complex organization which required coordination jobs which simply did not exist before. The internal division of labour needed coordination, the factory had to work on a continuous basis and input and output markets had to be constantly monitored and secured in order to avoid disruptions, financial management also became far more complex. It was the birth of the industrial and service sectors white collar class. Today we are entering the fourth industrial revolution,Footnote 1 accompanied by a new wave of substitution between labour and capital and we can even encounter new forms of Luddism (Jones 2006). Can we expect that, alike the previous industrial revolutions, job losses will only characterize a temporary short run phase and that new jobs will be finally created in a number at least equal to those that will be lost? And that the new jobs will be, on the whole, of better quality than the lost ones, i.e., less routine, more interesting, more creative, more productive and therefore better paid? An “optimistic” vision tends to answer yes to such questions, claiming that roughly the same virtuous adjustments that took place in the previous industrial revolutions will operate also in the present one. Such an adjustment may be painful and require one or more generational turnovers and profound institutional changes in the education system and labour markets, but it will finally happen. According to this view, technological unemployment is a temporary frictional phenomenon, and Governments, Trade Unions and other institutions, especially those devoted to education, should operate to facilitate such transition and to help make it socially acceptable by providing support to the “losers” of the transition. But an opposite view claims that this time it is different. The fourth industrial revolution has some common characteristics with the previous ones, but also some important peculiarities that make its nature and in particular its long term impact on the labour market different from what we observed in the past revolutions. In particular, past mechanizations could only substitute routine manual work, while today’s automation is likely to concern jobs which involve more and more sophisticated cognitive skills and even learning. According to this view, digital technologies, AI, machine learning, big data, internet of things, have some important features which will mark a sharp discontinuity with previous industrial revolutions. Persistent technological unemployment and growing inequality will characterize a long historical phase and the consequences on the organization of production and society will go well beyond frictional adjustments. Keynes himself in his 1930 essay gave a fundamentally optimistic view and considered technological unemployment as “a temporary phase of maladjustment.” If we are going towards a world in which production requires little labour, then “[a]ll this means in the long run that mankind is solving its economic problem” (Keynes 1930, emphasis in original). However, his optimism was founded on a rather radical change in the organization of society, i.e., massive reduction of working hours per week and massive redistribution programs, funded by high taxes on capital income. In this short note I will briefly review the main arguments of the two visions. Personally, I am more convinced by those put forward by the “pessimistic” view, mainly because I think that the underlying “microeconomic” features of the new technologies are profoundly different from those of the previous industrial revolutions. I will argue that some of virtuous circles which have been activated in the past are unlikely to operate this time. Needless to say, I am absolutely aware that long term forecasts should always be taken cautiously and that reality often surprises us. New kinds of virtuous circles may well arise in the future that we do not envisage today. Thus, my position is that yes, this time is different and we are going to face some serious societal challenges caused by the new technologies, but of course we do not know what the long term consequences will be.",16
46,3,Journal of Industrial and Business Economics,05 June 2019,https://link.springer.com/article/10.1007/s40812-019-00121-1,"Micro-work, artificial intelligence and the automotive industry",September 2019,Paola Tubaro,Antonio A. Casilli,,Female,Male,Unknown,Mix,,
46,3,Journal of Industrial and Business Economics,30 May 2019,https://link.springer.com/article/10.1007/s40812-019-00120-2,Control in the era of surveillance capitalism: an empirical investigation of Italian Industry 4.0 factories,September 2019,Angelo Moro,Matteo Rinaldini,Maria Enrica Virgillito,Male,Male,Female,Mix,,
46,3,Journal of Industrial and Business Economics,22 July 2019,https://link.springer.com/article/10.1007/s40812-019-00124-y,A systemic perspective on socioeconomic transformation in the digital age,September 2019,Rita Strohmaier,Marlies Schuetz,Simone Vannuccini,Female,Female,Female,Female,"It is safe to say that the innovative principle underlying most, if not all, recent technological breakthroughs in areas such as advanced robotics, micro- and nanoelectronics or biotechnology is digitization, the conversion of information from analog to digital. Continuous improvements in “More Moore” and “More-than-Moore” technologies have enabled the pervasive application of this principle to virtually all areas of the economy and society, a trend commonly referred to as digitalization. In this paper, we apply a novel methodology that combines indicators and network theory in order to measure the systemic impact of digitalization. Digitalization induces broad socioeconomic change, setting the stage for a fully-fledged technological revolution, meant as “a set of interrelated radical breakthroughs, forming a major constellation of interdependent technologies; a cluster of clusters or a system of systems” (Perez 2010, p. 189). Indeed, the widely discussed rejuvenation of manufacturing into a “smart manufacturing”, or “Industry 4.0”, is one of the transformations characterizing the later stage of the digital revolution that according to Carlota Perez (2002) already started out with the invention of the Intel Microprocessor in 1971. What we experience nowadays is the deployment period of this revolution—a potential “golden age” (Perez 2013, p. 11), marked by technological diffusion, dynamic sectoral expansion, and economies of scale. In this crucial phase, entrepreneurship should dominate financial capital while institutions should safeguard strong markets and social well-being. However, such golden age lies on a path of creative destruction and destructive creation that can affect societies in costly ways. Digitalization shapes many aspects of society, not least employment and the future of work, restlessly engaged in a “race against the machine”. In fact, beyond the ongoing debate on the “effect size”, smart technologies are projected to have vast implications for the labor market (see, for example, Brynjolfsson and McAfee 2014, Autor 2015, Frey and Osborne 2017, Frank et al. 2019). At the international level, innovation-leading countries in the Global North are currently facing a race for the machine. In this race, some Asian countries, especially the four Tiger states, are projected to occupy the pole position in the future, as—according to the political scientist Kishore Mahbubani—these economies have acquired the main pillars of Western societies, such as the free market, science, education and the rule of law, whereas the latter have been “gradually walking away from these pillars.”Footnote 1 The need to understand the capacity of our socioeconomic systems to tackle the profound changes currently unfolding calls for a systemic perspective on the digital revolution that comprehends technological change as a process shaping and being shaped by social and economic factors. In this context, we developed a tool and a novel indicator to grasp and represent the impact of the digital age on the interdependent structural components of contemporary socioeconomic systems. Our motivation is thereby twofold: On the one hand, to complement the systemic approaches in the literature on technological revolutions with an empirically tractable framework; on the other hand, to add up to the existing toolkit of assessing social and economic development by taking into account the linkages between different pillars of the socioeconomic system. While we do consider digitalization a stage of the latest technological revolution, our aim is neither to provide a measurement of the longue durée of technologically driven transformations nor to supplement Long Wave theory with a less historical and more quantitative approach. Contrarily, we aim at obtaining a snapshot of the structural effects of the digital transformation ‘in the making’ on socioeconomic systems. Our contribution is to use a novel methodology to envelop the complexity of change as it percolates over the socioeconomic structure, rather than digging into its ultimate determinants. We apply our tool to a set of Western (the innovation leaders in the European Union, namely. Denmark, Finland, Germany, the Netherlands, Sweden, as well as the US) and Asian countries (China, India, Malaysia, Singapore, South Korea and Taiwan) for the period between 2007 and 2016, in order to assess their progress in the main pillars of the socioeconomic system at the presumably first stage of deployment period of smart technologies. In particular, we will try to answer the following questions: (1) What was the impact of digitalization on overall socioeconomic development, what the impact of employment and work organization, and how are they related to the digital transformation? (2) How similar are countries in terms of their absorption pattern of technological change related to the so-called Industry 4.0? The paper proceeds as follows. In Sect. 2, we describe the theoretical background that motivates our systemic approach while in Sect. 3 we briefly sketch the methodological framework and outline related data issues. Section 4 presents and discusses the results, while Sect. 5 summarizes the essence of the paper and provides the reader with concluding remarks.",16
46,3,Journal of Industrial and Business Economics,04 June 2019,https://link.springer.com/article/10.1007/s40812-019-00119-9,"Fourth industrial revolution concepts in the automotive sector: performativity, work and employment",September 2019,Tommaso Pardi,,,Male,Unknown,Unknown,Male,"This article is part of a research project on the future of work in the automotive sector sponsored by the International Labor Organisation. On this topic see also Pardi et al. (2019). A substantial scientific literature has promoted the idea of an imminent digital revolution, or of a “second machine age” (Brynjolfsson and McAfee 2014). This vision has been incorporated in fourth industrial revolution concepts such as Industry 4.0 in Germany, Advanced Manufacturing in the United States, and Made in China 2025 where they have informed new sets of industrial policies. According to global consultants, the automotive industry will be at the forefront of this “fourth industrial revolution” as one of the largest capital-intensive industries, which concentrates alone around 40% of the world stock of operational robots but still employs a sizeable amount of unskilled and relatively well-paid workers (Sirkin et al. 2015). These new technologies are presented as “disruptive”, but their impact is expected to be positive for all the existing players in terms of productivity gains, better and more diversified products, better work conditions. The only problem seems to concern employment. According for instance to BCG, “fewer than 8% of tasks in the US transportation-equipment industry are automated, compared with a potential of 53%” (Sirkin et al. 2015, 6). At the world level this potential would rise to 85% (2015, 15) and since robots are becoming “cheaper, smaller and more flexible” BCG forecasts that the rate of automation of all these tasks will increase exponentially worldwide to reach “near saturation in the late 2020s” (2015, 20). These alarming forecasts do not raise though great concern as they translate into straightforward scenarios and policy recommendations. Governments and firms are expected to promote these technologies in order to benefit from stronger productivity growth and should anticipate the related massive jobs losses by introducing or reforming lifelong training schemes that will also provide the fewer but more skilled workers who will interact with collaborative robots and smart technologies (Brynjolfsson and McAfee 2014). There are softer and harder versions of this view, but it is difficult to find in political arenas many controversies and debates concerning the fundamental direction of change. As we will see below, so far the impacts of industry 4.0 technologies in the automotive industry is small, and there do not seem to be clear prospects for their future implementation and diffusion, at least in mass production. Nevertheless, according to this powerful vision, the present state of manufacturing does not matter because what we are looking at are “disruptive” transformations that only visions of the future can grasp (Rüssmann et al. 2015). Such a normative position leads to a paradox since the relationship between the present and the future is reversed. It is not anymore the future that is understood and envisioned as the product of present evolutions, but it is the present that is shaped by visions of distant futures based on the promises of digital technologies. What we would like to propose in this article is to reverse back this perspective and reconnect empirically grounded studies of the evolution of automotive manufacturing with the future of work, employment and manufacturing. We argue that such an approach is necessary not only to produce more realistic scenarios for stakeholders and policymakers, but even more important to bring back politics and work in the debate about the future of manufacturing.",10
46,3,Journal of Industrial and Business Economics,13 August 2019,https://link.springer.com/article/10.1007/s40812-019-00132-y,Industry 4.0: revolution or hype? Reassessing recent technological trends and their impact on labour,September 2019,Armanda Cetrulo,Alessandro Nuvolari,,Female,Male,Unknown,Mix,,
46,3,Journal of Industrial and Business Economics,25 July 2019,https://link.springer.com/article/10.1007/s40812-019-00125-x,A policy for a new industrial revolution,September 2019,Rafael Myro,,,Male,Unknown,Unknown,Male,"Since the beginning of the 21st century, demands for a new promotion industrial policy have markedly increased in Europe and other areas, as a means of arresting the decline in manufacturing industry’s share of total production; a trend which has continued despite the economic boom from 2002 to 2007. The offshoring of production to emerging countries, and the impressively successful industrialization process of a small number of these mainly Asian, highly populated countries (Baldwin 2016) were the main reasons for this reaction to the passive and poorly funded industrial policy followed in the countries belonging to the EU during the last 15 years of the twentieth century. Moreover, the Great Recession seriously affected industrial activity in developed countries, especially the southern members of the EU, as a result of the significant reduction in the demand for materials, equipment and durable consumer goods. This has led to new proposals for the reindustrialization of the European common space (European Commission 2014). In addition to this, the evidence that some emerging countries may have initiated a phase of premature deindustrialization (Rodrik 2016) has increased the interest in defining a new framework for industrial policy, further investigating its theoretical foundations. Finally, the emergence of industry 4.0, a product of the interaction of massive computing at increasingly lower prices with widespread and high connectivity, is leading to changes in products and the ways they are obtained. In addition, it seems to promise greater transformations in the future, which, together with the peculiar situation of weakness in the world economy in the years of recovery after the Great Recession, is forcing governments to seek radical solutions to boost their industries. In the following pages, a brief reflection on the role of industrial policy in the face of this technological challenge is offered, defining its fundamental objectives and selecting its key instruments.",1
46,3,Journal of Industrial and Business Economics,12 June 2019,https://link.springer.com/article/10.1007/s40812-019-00122-0,"A fourth industrial revolution? Digital transformation, labor and work organization: a view from Spain",September 2019,Francisco-Javier Braña,,,Unknown,Unknown,Unknown,Unknown,,
46,4,Journal of Industrial and Business Economics,12 August 2019,https://link.springer.com/article/10.1007/s40812-019-00131-z,Patent policy regulation and public health,December 2019,Anna Rita Bennato,Monica Giulietti,,Female,Female,Unknown,Female,"International policies to protect intellectual property rights (IPRs) have seen important changes during the past two decades. Rules on patents, copyrights, trademarks, and other forms of IPRs have become a standard component of international trade agreements. In 1994 during the Uruguay Round, developed countries made the upgrading of IPRs protection one of their highest priorities, setting up the agreement on trade-related aspects of intellectual property rights (TRIPs). The TRIPs agreement calls on countries to enforce a minimum standard of protection lasting 20 years for several categories of intellectual property, ensuring the same treatment in all subject matters without considering the peculiarities of each good, including pharmaceuticals. Historically, the issue of intellectual property rights is considered a contentious one. The primary reason for providing patent protection is to permit inventors to earn returns on their inventions, and therefore to provide an incentive for technology to advance. In the absence of some form of protection, private agents will have weak reasons to invest their resources in generating new information and technologies. On the other hand, though spurring the production of a greater variety of goods, this institution enhances the firms monopolistic power and so it leads to an inefficiently low level of output. A fundamental tension exists between the social desirability of widespread dissemination of available knowledge and the need for society to provide adequate rewards to suppliers of new inventions (e.g. see Chin and Grossman 1990; Deardorff 1992). On top of that, another important aspect of this issue lies in the possible adverse welfare effects that would be caused by extending intellectual patent protection indiscriminately to all goods across the world. Beyond the common wisdom according to which IPRs are an important factor to foster economic growth, an opposite view is argued equally vehemently (Mansfield 1985; Diwan and Rodrik 1991; Deardorff 1992; Maskus and Penubarti 1995). The main point of contention is the claim made by governments of many poor developing economies that unqualified patent protection for pharmaceuticals will result in substantially higher prices for medicines, with adverse consequences for the health and well-being of their citizens, and not only. Hence, welfare implications that derive from a strict enforcement of IPRs are complex. The simple fact that trade flows rise or fall in response to an enforcement of the law on IPRs is not sufficient to draw conclusions regarding economic welfare. Both static and dynamic effects need to be considered (Deardorff 1992; Helpman 1993). The aim of this paper is to study how the new international patent system affects social welfare by changing the availability of pharmaceutical products.Footnote 1 The extension of patent protection to pharmaceuticals yields an externality in terms of deterioration of health, both in developed and developing countries. In actual facts, the high integration of travel and transport has caused an acceleration in the transnational spread of viruses, and the control of infectious diseases has become more difficult.Footnote 2 In other words, communicable diseases which traditionally are linked to poverty, could become common also in the developed world. The recent facts about the diffusion of communicable diseases, due for example to social contagion across social networks (see Schwamm 2018), have showed us how developed countries are vulnerable to “neglected diseases”. Policies dealing with infective diseases are of great importance nowadays. Yet, only recently economists have begun to look at these questions in a formal way (Gersovitz and Hammer 2004). In the light of the above observations, our aim in the current paper is to rethink the role of patent protection in the pharmaceutical sector, studying the welfare implications according to different levels of public health. With this purpose we extend the set-up developed by Grossman and Lai (2004) by including explicitly the health sector. The main assumption is that agents derive utility from individual consumption of pharmaceutical goods, while at the same time their utility is increasing in the level of the national health. Individual utility depends on the individual health, and this in turn is influenced by personal consumption of drugs and benefits positively from the presence of a healthy environment. At the same time, it is by increasing their personal utility level that individuals contribute to an enhancement of the social health. Focusing on one of the legal tool adopted to protect IPR, we investigate how it is possible to define the optimal patent system in a world where individuals consume “good health” in the terminology of Grossman (1972, 2000). In the current paper, we define health differently from the classical approach.Footnote 3 According to the Grossman’s seminal work (1972), we assume that health is a good desired to enhance well-being and so utility, and vice versa through individual utility the health increases. Consumer’s utility is affected by individual pharmaceutical purchases, which in turn contribute to an improvement of the social welfare. In other words, a sort of positive externality is at work. Being in good health not only contributes to the wellbeing of a single individual, but it benefits also people that are surrounded by healthy subjects, from family members to colleagues at work. If everybody takes care of her own health, each individual contributes directly to an improvement of the social health, creating positive effects which are beneficial to everyone. On the contrary, being in poor health conditions triggers a set of costs which can be directly matched to the single individual affected by the problem (e.g. inability to go to work), and indirectly link to the affected subject (e.g. crowded hospitals/GPs). For example, in the very recent times we are witness of the diffusion of new waves of pandemic diseases which are causing important social costs.Footnote 4 In the current analysis we assume that individuals do not internalize this social externality when maximizing their utility. In other words, consumers ignore the existence of a positive externality which reduces the risks of diffusion of infectious diseases.Footnote 5 Since the patent policy affects the monopolistic prices, the choice of the optimal patent life must weigh its relation with possible externalities. In order to determine the optimal patent policy, we have to balance two opposite effects that influence the level of social welfare, re-examining the trade-off between static costs and dynamic benefits that was first studied by Nordhaus (1969).Footnote 6 First of all, we know that patent protection yields a positive effect, because a longer-lasting patent life increases the number of inventions (Nordhaus 1969; Grossman and Helpman 1991). More precisely, this benefit is present if we can identify an equilibrium where a larger supply of inputs are combined to produce a larger amount of patented goods. On the other hand, we cannot disregard the detrimental effects in terms of welfare from a static point of view. A longer patent length implies a longer monopolistic power, with all the ensuing inefficiencies associated with monopoly. In a closed economy, where the extent of the health externality is connected with the consumption level of pharmaceutical goods, the identification of the optimal patent protection is crucial for the social welfare, and the optimal patent length turns out to be increasing in the health externality.Footnote 7 The approach adopted in this paper is based on the concept of international surveillance of communicable diseases as a global public good, necessary to preserve global health (Kaul et al. 1999). This paper is organized as follows. In the next section we develop our first analysis of a simple model with ongoing innovation. In Sect. 3 we study the optimal patent policy in a closed economy. Through a numerical simulation, we measure the welfare effects in a country that has no trade with the rest of the world, focusing on the health sector. In Sect. 4 we consider a world economy in which there are two countries that differ with respect to both the market size, along with the capability of conducting research and development, and their health externality. Finally, we develop our conclusions in Sect. 5.",
46,4,Journal of Industrial and Business Economics,31 August 2019,https://link.springer.com/article/10.1007/s40812-019-00134-w,Regulatory capture in the US petroleum refining industry,December 2019,Robert Gmeiner,,,Male,Unknown,Unknown,Male,"The capture theory of regulation holds that regulations benefit regulated firms because the regulatory agencies are captured by the firms they regulate (Stigler 1971). Regulations create winners and losers as products of a political process. The capture theory says that concentrated and well-organized interests will tend to be the winners and reap rents from the regulatory process (Olson 1971). Owen and Braeutigam (1977) go so far as to write, “No industry offered the opportunity to be regulated should decline it. Few industries have done so.” This view that concentrated special interests are most able to capture regulators is empirically well supported. Kolko (1963) challenges the conventional wisdom that Progressive Era regulations worked to control concentrated economic interests and presents evidence that these regulations further entrenched the positions of the dominant businesses at the time. Gilens and Page (2014) complement this view with an empirical analysis showing that implemented policies have tended to benefit well-connected special interests. Before the 1980s, economic regulation in the United States tended to be specific and involve clear restrictions on prices and quantities. Such regulations were authorized by Congress and implemented by agencies such as the Interstate Commerce Commission and Civil Aeronautics Board. These agencies and their regulations have been scrapped, but have been replaced by an administrative state consisting of agencies that are not new, but have promulgated increasing amounts of regulation since the 1980s. (Meiners and Yandle 1989; Boskin 1993). This paradigm of federal regulation by agencies staffed with experts was promoted by Woodrow Wilson (1887). His idea was to ensure that regulation would be more aligned with the public interest and less susceptible to political manipulation. Wilson correctly observed that government tended to support powerful business interests and proposed independent agencies as a solution. However, agencies may suffer from the same problem. In this paper, I test Stigler’s capture hypothesis in the context of the Wilsonian regulatory paradigm. This analysis differs from earlier tests of the capture theory by looking at the cumulative effects of the administrative state after a time of consistent expansion. Agency regulation differs from direct legislative action in several ways. First, agency regulators are unelected and face different incentives than politicians. Second, the regulatory process is undemocratic in that while it has public comments, it does not rely on votes and the final decision is issued by a regulator. Third, the resulting product tends to be nuanced, technical, and hard to dismantle. Specific Congressional regulatory mandates involving agencies such as the ICC and CAB have been repealed, but the administrative state has not been scaled back. The only major effort to reform the administrative state was the National Partnership for Reinventing Government spearheaded by Vice President Al Gore in the early 1990s, but its long-term effects, if any, were scant (Moe 1994). The Code of Federal Regulations has increased in page count by approximately 80% since 1981 when President Ronald Reagan took office. This recent proliferation of federal regulations merits examining their cumulative effect. Current agency regulations do not impose the same sort of price and quantity controls as were previously used, but there may be overarching cumulative effects that benefit certain interests. In order to examine cumulative effects of federal regulation, I use RegData, a data set produced by Al-Ubaydli and McLaughlin (2015). RegData quantifies industry-specific word counts in the Code of Federal Regulation using machine learning algorithms. It is a time series measure of regulation specific to an industry. Such data can be used to answer broad questions about the cumulative effects of regulation. I use RegData to test whether there are significant cumulative effects of regulation of the US petroleum refining industry on refiners’ margins (output less input costs) and on stock prices of firms in the industry. I focus on the petroleum refining industry for multiple reasons. One is that it is a heavily regulated industry and its concentrated interests are well-known. These include vertically integrated supermajors such as ExxonMobil, Chevron, BP, and Royal Dutch Shell. Another reason is that petroleum refining is very important to the US economy. Beyond these reasons, the petroleum refining industry is ideal for this analysis because there is bidirectional variation in its RegData series, despite an economy-wide upward trend in quantities of regulation. To test the capture theory of regulation, I exploit differences between vertically integrated and nonintegrated firms. Nonintegrated firms buy oil on the market, whereas vertically integrated firms produce their own oil. Because they acquire inputs differently, the two types of firms have different cost structures in their respective profit functions. If increasing totals of regulation result in a narrower margin, nonintegrated firms are comparatively worse off in any case. If the narrowing is due to rising input costs, then vertically integrated firms are better off. This vertically integrated/nonintegrated distinction is valuable only because the largest, most powerful interests in the industry are the vertically integrated supermajors.Footnote 1 Whether or not vertical integration is a source of market power is immaterial to this analysis. It is a useful distinction only because the dichotomy of firm type already happens to coincide with differences in size. I find that increasing totals of regulation of the US petroleum refining industry have had the effect of narrowing the refining margin, which intuitively should harm nonintegrated firms. An analysis of the effects of regulation on stock prices shows beneficial effects for vertically integrated firms and harmful effects for nonintegrated firms. I also find evidence that this narrowing of the margin is mostly due to rising input costs. The capture theory of regulation is well supported in this industry in in the current paradigm of federal agency regulation.",1
46,4,Journal of Industrial and Business Economics,01 August 2019,https://link.springer.com/article/10.1007/s40812-019-00128-8,The productivity cost of power outages for manufacturing small and medium enterprises in Senegal,December 2019,Lassana Cissokho,,,Unknown,Unknown,Unknown,Unknown,,
46,4,Journal of Industrial and Business Economics,05 August 2019,https://link.springer.com/article/10.1007/s40812-019-00129-7,Determinants of PPP in infrastructure investments in MENA countries: a focus on energy,December 2019,Giuseppe Di Liddo,Alessandro Rubino,Ernesto Somma,Male,Male,Male,Male,"Infrastructures play a key role with respect to development outcomes such as job creation, market access, health and education (Straub 2008; Calderón and Servén 2010a, 2010b; Calderón et al. 2014), environmental and climate policy goals (Sovacool 2013) and economic growth (Rud 2012; Cook 2011; Arnold et al. 2008). Enhancing infrastructure is therefore a key component of the 2030 Development Agenda, explicitly mentioned in three of the seventeen Sustainable Development Goals. Owing to sustained economic development, population pressure and rapid advances in science and technology in the past century, infrastructure-financing capital has become increasingly diverse and broad. Hence, improving the quality and quantity of infrastructure capital as a vital factor of production has become an integral part of sustainable development policies, (Fay et al. 2011; Bhattacharya et al. 2012). However, the quality, quantity, and accessibility of economic infrastructure in low-income developing countries and emerging markets lag considerably behind those in advanced market economies, the gap being particularly large in the power sector (Gurara et al. 2018). At the same time, fiscal constraints, due to the global financial crisis in many economies, determine that government budgets—traditionally the major source of financing for infrastructure—are not sufficient to finance the infrastructure need in most emerging markets and in developing economies (Arezki and Ferid 2019). In order to address these shortcomings of public financing, in particular in innovative sectors, there has been a proliferation of PPPs, establishing a new way of delivering services and triggering an entire redefinition of the roles played by the public and private sectors (OECD 2014). The exact definition of PPP is not consensual and there are many models of investment co-financing varying from country to country, depending on the sectors of activities too. In this paper, we will adopt the definition used in the Private Participation in Infrastructure (PPI) database of the World BankFootnote 1 and therefore we consider PPP projects those infrastructural projects where a private company or investor is at least partially responsible for operating costs and associated business risks. PPPs play a significant role in infrastructure provision, indeed public–private investments in 2018 alone amounted to $90 billion across 335 projects and are characterized by an increased number of projects and fewer megaprojects (Saha, Hong and Nair 2019). However, notwithstanding the staggering amount of finance mobilized by PPP, nowadays the volume of PPPs remains modest with respect to infrastructure needs (Gurara et al. 2018). This deficit has motivated many governments to promote actively PPP investments in infrastructure services as an integral part of their development strategy (McKibbin and Henckel 2017). Whereas governments remain the main source of infrastructure financing in developing countries, providing around 70% of the necessary funds, the private sector is also a key source and represents 22% of the required investment volumes, well beyond the 8% provided by official development assistance programs (Somma and Rubino 2016). Figure 1 in appendix reports data about PPP investments between 1990 and 2015 in water and sewerage, transport, energy and ICT. Latin America and the Caribbean (LAC) is the leading region in attracting the greatest volume of investments in each sector while Middle East and North Africa (MENA) is the region showing the lowest level of PPP globally (Somma and Rubino 2016). Source: World Bank and PPIAF, PPI Project Database: http://ppi.worldbank.org—http://www.ppiaf.org Total PPP investments in energy, transport, ICT and water by region—1990–2015 USDm Although cumulatively more than $1520 Billion of PPP projects have been developed over time, the analysis of the determinants of PPP and in particular in emerging economies is still scarce (Hammami et al. 2006) and has not provided conclusive results. What determines private participation in infrastructure provision is still unclear, especially in the energy sector and in the MENA. Furthermore, the MENA region is currently understudied from an empirical perspective compared to other regions. We consider the peculiarity of the this region as key to understand why MENA countries fail to attract PPP investments notwithstanding their great potential. In order to provide answers to the above-mentioned questions, the present study looks at the impact of institutional quality—measured by means of six indicators of governance—on infrastructures investments in MENA countries. While PPP economic characteristics and contractual arrangements can be standardized, institutional and regulatory quality are highly country and region-specific and might shed a light on the specificity of MENA countries and their growing inability to attract PPP investments. Therefore rather than focusing on the contractual and economic characteristics of PPP instruments, we look at the wider institutional factors influencing the likelihood of performance failure in a PPP agreement (Iossa and Saussier 2018). In addition, we will focus on the energy sector because energy infrastructures show the greatest gap in the infrastructure endowment (Gurara et al. 2018) in emerging economies when compared with other type of infrastructures,Footnote 2 presenting at the same time peculiar risk profiles that increase the hurdle rate for the internal rate of return (Alloisio and Carraro 2015). The article has six sections. Besides this introduction, the second section is a literature review on the institutional determinants of PPPs. The third section frames the research question on the bases of the existing gap in the literature. In the fourth section, the data and the empirical strategy are presented and then the main results are provided in section five. Finally, the sixth section features the main results.",11
46,4,Journal of Industrial and Business Economics,04 March 2019,https://link.springer.com/article/10.1007/s40812-019-00114-0,Ownership and workforce composition: a counterfactual analysis of foreign multinationals and Italian uni-national firms,December 2019,Mariachiara Barzotto,Giancarlo Corò,Marco Mutinelli,Unknown,Male,Male,Male,"The characteristics of workers—i.e. employees’ skill level, gender, age and nationality—matter for a broad variety of individual, firm, sector, and regional outcomes; such as knowledge transfer/sharing (e.g. Blomstrom and Kokko 2003), innovation (e.g. Fassio et al. 2018; Frosch 2011), and productivity (e.g. Hyun et al. 2015). Previous research in international business and international human resource management has focused primarily on these workers’ characteristics as antecedents of innovation and productivity. The core argument is that workers’ characteristics affect innovation and the productivity level of companies and sectors, as well as regions. Less attention has been devoted to understanding if and how ownership—e.g. whether firms are affiliates of foreign multinational firms (FMNs), or uni-national firms/single domestic enterprises (NATs)—impacts on their internal workforce composition. Recent contributions have started to explore the relationship between offshoring and the composition of onshore workforce at the company level, comparing multinational enterprises with national ones (Becker et al. 2013). Yet, scant evidence is available on the effects of inward Foreign Direct Investments (FDIs) on the internal workforce composition, which is crucial for enhancing the competitiveness of companies (e.g. Frosch 2011) and, in turn, of regions and even countries (among others, Blomstrom and Kokko 2003). In particular, the relationship between ownership and the composition of a company’s internal workforce (in terms of skills, gender, age, and nationality) is overlooked. The present article aims to explore the role of ownership in companies’ employment behaviour, thus analysing if and how a company’s internal workforce is affected by the firm’s ownership status. The ownership status is qualified in terms of whether a firm under consideration is part of a foreign multinational group (FMN) or a single domestic enterprise/uni-national firm (NAT—a company that has neither been acquired in the period of analysis, nor invested abroad). More specifically, this article focuses on analysing whether internal workforce composition differs according to firm ownership, by comparing the employment choices made by FMNs with those of NATs. The article investigates how companies employ the local workforce, be they host-country nationals or foreigners, by looking at skill level, age, gender and nationality. A deeper understanding of the employment behaviour adopted by FMNs and NATs, ultimately, aims to shed light on the extent to which the nature of employment changes according to the ownership. The aim of the approach is also to further our understanding of the use of native and experienced workers; in other words, workers with ‘genomic, catalytic, organic and dynamic’ (Kasabov and Sundaram 2016: 1529) competencies embedded in local industrial heritage. This is especially crucial, firstly, in countries for which the main source of competitiveness is manufacturing (such as Italy) and, secondly, in critical temporal windows, such as during the economic downturn originating with the global financial crisis, when the manufacturing sector was put under a strain worldwide. The article contributes to the understanding of the relationship between firm ownership and the use of its workforce, in particular investigating employment choices, in terms of skill level, age, gender and nationality. It does so by exploring how firm ownership (FMN and NAT) affects a company’s internal workforce composition during the economic crisis within the context of the Italian region of Veneto, a region renowned worldwide for its manufacturing heritage. In order to compare the employment choices made by FMNs with the ones undertaken by NATs and to test how firm ownership impact on a company’s internal workforce composition, we adopt a novel database of manufacturing firms operating between 2007 and 2013 in the Italian region of Veneto. The dataset combines information on the internal workforce composition of companies along with data on their characteristics and economic performance. The region under investigation is located in north-east Italy and represents one of the leading manufacturing areas in the country and, more generally, in Europe. Descriptive statistics and counterfactual estimations have been developed to analyse the workforce composition of firms. The remainder of the article is organised as follows. Section 2 presents a literature review on the effects of ownership on the internal workforce composition of host-country companies. Section 3 describes the presence of inward FDIs and the main characteristics of the local labour market composition in world-renowned manufacturing areas in advanced economies, such as Italy and, more specifically, the Veneto region. Section 4 is dedicated to the data and methodology of this study. Section 5 discusses the results of the analysis and, finally, Sect. 6 draws conclusions.",4
46,4,Journal of Industrial and Business Economics,22 March 2019,https://link.springer.com/article/10.1007/s40812-019-00116-y,The choice between product and logistic innovation in a spatial model with income distribution,December 2019,Stefano Colombo,Luigi Filippini,,Male,Male,Unknown,Male,"In 2004, Acxiom Corporation, an advertising company, launched a new product called Chomonicx2.0, a lifestyle consumer segmentation system. In the words of the Acxiom CEO, “the fact that Acxiom is first with a segmentation system like Chomonicx2.0, is an excellent measure of Acxiom’s innovation”. In the subsequent years, Acxiom released new versions of Chomonicx2.0.Footnote 1 In the same period, the most relevant competitor of Acxiom, Valassis Communication, commissioned a consulting company to implement a radical change in its own transportation system management, in order to stop “one of the chief cause of margin erosion”.Footnote 2 Interesting to note, the two firms, even if competing in the same industry, adopted a completely different strategy: while Acxiom invested to improve the quality of its offer by creating a new service, Valassis Communication invested to re-organize the transportation logistic in order to save on transportation costs. The aim of the two strategies is the same (getting higher profits), but the approach is doubtless different. Similarly, as illustrated by Chen et al. (2018), in 2000 Airbus formally announced to commit to develop and launch the superjumbo A380. At the opposite, the competitor of Airbus, Boeing, opted for a dramatic cost-cutting strategy from its supply chain, including a reduction of the transportation costs. At a more general level, firms have to decide in their day-by-day business life which type of innovation to engage on. On one hand, firms would like to improve the quality of their products, as a higher-quality good can be sold at a higher price. This is the strategy adopted by Acxiom and Airbus in the story above. The efforts made by a firm in order to improve the quality of the product are usually labelled as “product innovation” (Bonanno and Haworth, 1998). Product innovation frequently absorbs a relevant share of the R&D activities of the firms (Cohen and Klepper 1996). Moreover, product innovation is commonly considered within the business community as a fundamental instrument to increase market share and firms’ profits. On the other hand, firms would like to reduce their costs too, as this, all else being equal, directly translates into higher profits. Nowadays, one of the most relevant components of firms’ expenditures is represented by transportation costs that constitute on average one of the top five expenditures for firmsFootnote 3 and are accounted for nearly 60% of the overall logistic costs of firms (Hesse and Rodrigue 2004). A firm that works to minimize its own transportation costs is said to engage in “logistic innovation”. In the example above, Valassis Communication and Boeing followed this strategy in order to increase its profits. Needless to say, transportation costs and logistic innovation are central issues when firms operating in a spatial environment are considered. The aim of this article is to investigate the strategic interaction between the decision to engage in product innovation and the decision to reduce transportation costs when two firms compete in a spatial framework. While product innovation has been widely investigated in its relation with process innovation (that is, the reduction of the production costs), as far as we know, it has never been studied in its relation with logistic innovation (i.e., the reduction of the transportation costs). In order to tackle this issue, we develop a spatial model with some novel characteristics. In particular, we consider a linear-city model with some common assumptions—namely, unit demand, market coverage, and spatial price discrimination—,Footnote 4 but where income distribution varies with the location of consumers. We want to capture the situation where some areas of the city are inhabited by high-income customers, whereas other areas are inhabited by low-income customers. Therefore, in our model the consumers are heterogeneous in two dimensions: they are located in different points, and have different income. Moreover, the distribution of income is related to the distribution of customers across the space. We develop a simple framework in order to capture the degree of income inequality through a unique parameter, such that all situations ranging from maximal inequality to minimal inequality can be described. We show that the strategic decision of firms about whether to invest either in product or in logistic innovation not only depend on the relative efficiency of each type of innovation, but also on the degree of income inequality across the city. In particular, we show that when income inequality is sufficiently high, a unique equilibrium emerges, which depends on the relative efficiency of product and logistic innovation. In particular, if product innovation is highly (weakly) efficient with respect to logistic innovation, both firms choose product innovation (logistic innovation), whereas if the degree of the relative efficiency is intermediate, only the firm which is located close to rich customers chooses product innovation, while the rival chooses logistic innovation. However, if income inequality is sufficiently low, we characterize a parameter set where a multiplicity of asymmetric equilibria exists: in particular, provided that the degree of product innovation efficiency is intermediate with respect to logistic innovation efficiency, it is also possible that product innovation is chosen only by the firm which is located close to poor consumers. The basic intuition behind this result is as follows. When choosing between product and logistic innovation there are two opposite forces at work. On one hand, product innovation allows setting higher prices (price effect). On the other hand, logistic innovation allows reducing the transportation costs and serving more consumers (demand effect). When income inequality is sufficiently low, the price effect is strong (weak) relative to the demand effect for the firm located close to poor (rich) consumers, thus generating the possibility for multiple asymmetric equilibria. We also extend our model to the case of a multiplant monopolist that has to decide where to engage in product innovation and where in logistic innovation. We also consider the case of a joint-venture, that is, a situation where two firms cooperatively decide about innovation, but compete on prices. We find that in the case of a multiplant monopolist, the resulting equilibrium resembles that in the case of fully competing firms, with the exception that the plant located close to poor customers is never devoted to product innovation. In the case of a joint-venture, we find that when product innovation efficiency is high with respect to logistic innovation efficiency or income inequality is high, only the firm which is close to rich customers chooses product innovation, while the rival chooses logistic innovation. On the other hand, when product innovation efficiency is low with respect to logistic innovation efficiency or income inequality is low, both firms choose product innovation. Finally, we also discuss the implications in terms of consumer surplus and welfare. We show that, with regard to consumer surplus, if income inequality is high enough, a parameter region exists where consumer surplus is maximized when the firm located close to the poor customers adopts product innovation and the firm located close to the rich customers adopts logistic innovation, thus contrasting with the equilibrium emerging in the fully competitive set-up. On the other hand, we find that the choice of a multiplant monopolist always maximizes total welfare. Our results have several implications. First, we show that income inequality impacts on the strategic choice of the firms about product and logistic innovation, by altering the incentives of the firms in pursuing quality-increasing innovation versus transportation cost reduction innovation. Second, income inequality affects the optimal choice of a multiplant monopolist and a joint-venture about where to locate product and logistic innovation. Last, our work shows that inequality might effectively serve as an incentive for the firms to differentiate their innovation strategies, with the firm located close to rich consumers specializing in high-quality products and the firm located close to poor consumers specializing in efficient transportation techniques. As we consider the strategic interaction between product innovation and logistic innovation in a spatial model with income distribution, we build on different strands of literature that usually consider these three elements—product innovation, logistic innovation, and income distribution—separately. In particular, product innovation has been considered mainly with regard its interaction with process rather than logistic innovation (Bonanno and Haworth 1998; Boone 2000; Lambertini and Orsini 2000; Lin and Saggi 2002; Filippini and Martini 2010). Logistic innovation has been investigated by Dos Santos Ferreira and Thisse (1996) and Hendel and Neiva de Figueiredo (1997) without taking into consideration the strategic interaction between transportation cost reducing practices and quality improvement practices. Finally, income distribution has received consistent attention (see for example Gabszewicz and Thisse 1979; Shaked and Sutton 1982; Bacchiega and Minniti 2009; Brekke et al. 2010; Benassi et al. 2019), but, at the best of our knowledge, the impact of income distribution on the choice between product and logistic innovation has never been considered. The rest of the paper is structured as follows. In Sect. 2 we introduce the model. In Sect. 3 we derive the innovation equilibrium. In Sect. 4 we consider some extensions of the basic set-up. Section 5 considers consumer surplus and welfare implications. Section 6 concludes. The proofs are in the Appendix.",
47,1,Journal of Industrial and Business Economics,17 September 2019,https://link.springer.com/article/10.1007/s40812-019-00138-6,Cryptocurrencies: market analysis and perspectives,March 2020,Giancarlo Giudici,Alistair Milne,Dmitri Vinogradov,Male,Male,Male,Male,"Cryptocurrencies continue to draw a lot of attention from investors, entrepreneurs, regulators and the general public. Much recent public discussions of cryptocurrencies have been triggered by the substantial changes in their prices, claims that the market for cryptocurrencies is a bubble without any fundamental value, and also concerns about evasion of regulatory and legal oversight. These concerns have led to calls for increased regulation or even a total ban. Further debates concern inter alia: the classification of cryptocurrencies as commodities, money or something else; the potential development of cryptocurrency derivatives and of credit contracts in cryptocurrency; the use of initial coin offerings (ICO) employing cryptocurrency technology to finance start-up initiatives; and the issue of digital currencies by central banks employing cryptocurrency technologies. These discussions often shed more heat than light. There is as yet little clearly established scientific knowledge about the markets for cryptocurrencies and their impact on economies, businesses and people. This special issue of the Journal of Industrial and Business Economics aims at contributing to fill this gap. The collection of papers in the special issue offers six distinct perspectives on cryptocurrencies, written from both traditional and behavioural viewpoints and addressing both financial questions and broader issues of the relationship of cryptocurrencies to socio-economic development and sustainability. Here in this introduction we set the stage by defining and discussing the main concepts and issues addressed in the papers collected in this special issue and previewing their individual contributions. Cryptocurrencies are digital financial assets, for which records and transfers of ownership are guaranteed by a cryptographic technology rather than a bank or other trusted third party. They can be viewed as financial assets because they bear some value (discussed below) for cryptocurrency holders, even though they represent no matching liability of any other party and are not backed by any physical asset of value (such as gold, for example, or the equipment stock of an enterprise).Footnote 1 As the word cryptocurrency, and the other terminology employing ‘coin’, ‘wallets’ in the original whitepaper proposing the supporting technology for Bitcoin (Nakamoto 2008) all suggest, the original developers consciously attempted to develop a digital transfer mechanism that corresponded to direct transfer of physical cash used for payments or other financial assets—such as a precious metals and ‘bearer bonds’—that like cash also change hands through physical transfer. What about the arrangements used for financial assets recorded in digital form (such as bank deposits, equities or bonds but not bearer bonds or bank notes)? Ownership arrangements for these assets depend on the information system maintained by a financial institution (commercial bank, custodian bank, fund manager) determining who is entitled to any income or other rights it offers and has the right of sale or transfer. Originally these systems were paper based, but since the 1960s they have utilised first mainframe and more recently computer systems.Footnote 2 If there is a shortcoming in their information system, for example a breach of security that leads to theft or loss or failure to carry out an instruction for transfer, then the financial institution is legally responsible for compensating the owner of the asset. In the case of cryptocurrencies, it is the supporting software that both verifies ownership and executes transfers.Footnote 3 There is no requirement for a ‘trusted third party’.Footnote 4 This approach though requires a complete historical record of previous cryptocurrency transfers, tracing back each holding of cryptocurrency to its initial creation. This historical record is based on a “blockchain”, a linking of records (“blocks”) to each other in such a way that each new block contains information about the previous blocks in the growing list (“chain”) of digital records. So that every participant in the cryptocurrency network sees the same transaction history, a new block is accepted by agreement across the entire network. The applications of this technology are not necessarily finance-related; it can be applied to any form of record-keeping; however if the block refers to a financial transaction then each transaction in the blockchain, by definition, includes information about previous transactions, and thus verifies the ownership of the financial asset being transferred. Falsifying ownership, i.e. counterfeiting (which, one could imagine, is easy, as digital objects can be easily duplicated by copying), is impossible because one would have to alter preceding records in the whole chain. Since records are kept in the network of many users’ computers, a “distributed ledger”, this is rather unthinkable. There is a substantial computer science literature on the supporting cryptocurrency technologies, including on the security of public key cryptography, efficient search tools for finding transactions on the blockchain, and the ‘consensus’ mechanisms used to establish agreement on ledger contents across the network.Footnote 5 Commentators expect new more efficient approaches will replace the mechanisms currently used in Bitcoin and other cryptocurrencies.Footnote 6 This though would not affect our definition of cryptocurrencies (as an asset and some technology which verifies ownership of the asset), which is independent of any particular technological implementation.Footnote 7 Cryptocurrencies can be seen as part of a broader class of financial assets, “cryptoassets” with similar peer-to-peer digital transfers of value, without involving third party institutions for transaction certification purposes. What distinguishes cryptocurrencies from other cryptoassets? This depends on their purpose, i.e. whether they are issued only for transfer or whether they also fulfil other functions. Within the overall category of cryptoassets, we can follow the distinctions drawn in recent regulatory reports, distinguishing two further sub-categories of cryptoassets, on top of cryptocurrencies:Footnote 8 Cryptocurrencies: an asset on a blockchain that can be exchanged or transferred between network participants and hence used as a means of payment—but offers no other benefits. Within cryptocurrencies it is then possible to distinguish those whose quantity is fixed and price market determined (floating cryptocurrencies) and those where a supporting arrangement, software or institutional, alters the supply in order to maintain a fixed price against other assets (stable coins, for example Tether or the planned Facebook Libra). Crypto securities: an asset on a blockchain that, in addition, offers the prospect of future payments, for example a share of profits. Crypto utility assets: an asset on a blockchain that, in addition, can be redeemed for or give access to some pre-specified products or services. A further distinguishing feature of crypto securities and crypto utility assets is that they are issued through a public sale (in so called initial coin offerings or ICOs). ICOs have been a substantial source of funding for technology orientated start-up companies using blockchain based business models. These classifications of cryptoassets are critical for global regulators, since they need to determine whether a particular cryptoasset should be regulated as an e-money, as a security or as some other form of financial instrument, especially in relation to potential concerns about investor protection in ICOs.Footnote 9 Floating cryptocurrencies account for the very large majority of the cryptoasset market capitalisation (Tether, a stablecoin, and Bitfinex’s UNUS SED LEO, a utility coin, are in the top 12 cryptoassets by market capitalisation, all the rest are floating cryptocurrencies). Table 1 summarises the market share of leading cryptocurrencies at the time of writing. What is the value of cryptocurrencies? On the one hand, cryptocurrencies should be able to ease financial transactions through elimination of the intermediaries, reduction of transaction costs, accessibility to everyone connected to the Internet, greater privacy and security (see, e.g., discussions in Böhme et al. 2015; Richter et al., 2015).Footnote 10 On the other hand, the real economic value transferred in the transactions of freely floating cryptocurrencies such as Bitcoin’s BTC and Ethereum’s Ether remains unclear. Despite the exhaustive and unfalsifiable record of all previous transactions held cryptographically, as in the Bitcoin blockchain, the information only refers to nominal numbers, i.e. the amount of cryptocurrency units transferred. One can, however, get an idea of the market value of cryptocurrencies by looking at their exchange rates against existing fiat currencies. This is possible thanks to cryptocurrency exchanges, which provide a nearly continuous price record for all actively traded cryptocurrencies. Although the resulting exchange rates are highly volatile, they reveal that cryptocurrencies have a non-zero value for those prepared to pay fiat currency in order to purchase them. What drives this value in the absence of a backing asset or an issuer’s liability? Some advocate it is the cost of “mining” (energy and time spent on computational efforts required to complete formation of a new block in the chain, and rewarded by a newly issued cryptocurrency unit), however the cost borne by one member of the network does not justify the value of the new cryptocurrency unit for other members of the network (see also Dwyer 2015, who argues the cost of mining is sunk and as such should be disregarded in the market value analysis). Others claim their market value is driven by the speculative bubble; yet, strictly speaking, the bubble is manifested in upward price deviations from the fundamental value (see, e.g., Siegel 2003, for a review of definitions), hence the bubble explanation is only partial and raises further questions about what drives investors’ beliefs that feed their demand and thus support the bubble. If it is the ease and the speed of transactions, then new transaction technologies and fund transfer systems that greatly improved in the recent decade (such as Transferwise and similar systems) should have wiped out a big chunk of the cryptocurrency value, yet this does not seem to be the case. A possible answer may lie in the features that distinguish cryptocurrencies from other assets and payment systems. Privacy, or rather anonymity, is a prominent distinctive feature popping up in most discussions of cryptocurrencies. The value of a cryptocurrency is then effectively a measure of how much users value anonymity of their transactions. While anonymity may be attractive for illegal activities (and some research reviewed below suggests cryptocurrencies are often used for these purposes), one cannot rule out users may simply wish more privacy, trying to avoid the “Big Brother” effect of traditional transactions. Of course, there may be other factors, for example, fashion (users want to use the technology others are talking about), hi-tech appeal (the desire to use the most modern technology) or curiosity (the desire to try something new), among others, but these phenomena appear shorter-lived than the allure of anonymity. A key development in the rise of cryptocurrencies and other cryptoassets has been the emergence of cryptoexchanges where anyone can open accounts and trade cryptoassets both against each other and against fiat currencies. In a survey by Hileman and Rauchs (2017), the US dollar, the Euro and the British Pound are currently most widely traded against cryptocurrencies, while the importance of the Chinese Renminbi (CNY) significantly diminished after the tightening of the regulation by the People’s Bank of China; about three-quarters of large exchanges provide trading support for two or more cryptocurrencies. Above, we highlighted that cryptoexchanges provide extensive cryptocurrency pricing and trading information in the public domain. The emergence of these exchanges has created an entire ‘ecosystem’ of services and participants, seeking to provide liquidity, exploit price discrepancies for profit and to support investment by both retail and professional investors. Academic interest in cryptocurrencies started to soar in 2014 (see Fig. 1): the Scopus database lists 127 publications containing the word ‘Bitcoin’ in the title or abstract or keywords and 24 containing ‘cryptocurrency’ or ‘cryptocurrencies’ in 2014. In 2017 and especially in 2018 the number of publications grew fast, and in 2019 the trend is continuing. Interestingly, academic work focuses much more on the Bitcoin than on the more general topic of cryptocurrencies, although in 2018 and in 2019 the gap narrowed. It appears that—apart from the Bitcoin frenzy—there is a growing attention to the general phenomenon of cryptocurrencies. However, focusing only on the ‘Economics, Econometrics & Finance’ and ‘Business, Management & Accounting’ sections of Scopus reveals that the interest in the topic surged a few years laterFootnote 11, although the number of publications is still rather low: in 2018 there were just over 100 titles on the topic in the above fields. The remaining contributions come from the ‘Computer Science’, ‘Engineering’ and ‘Mathematics’ disciplines. Publications listed on the Scopus database containing ‘Cryptocurrency/ies’ and ‘Bitcoin’ in the title or abstract or keywords. The graph reports the number of publications tracked by the Scopus database (http://www.scopus.com) accessed on August 10, 2019 containing the words “Cryptocurrency/ies” or “Bitcoin” in the title or abstract or keywords. The subsample ECON refers to the category Economics, Econometrics & Finance while the subsample BUS refers to Business, Management & Accounting This special issue of the Journal of Industrial & Business Economics offers a multifaceted view on the cryptocurrency phenomenon. Contributions have been selected with the objective to extend the existing knowledge about cryptocurrencies, which themselves embody innovations and technological change, and may appear to be a lucrative form of fund raising for small businesses; extra emphasis is made on areas of the journal’s particular interest, such as environment, sustainability and social responsibility. The remainder of this editorial proceeds as follows. In Sect. 2 we describe the contributions that shed light on the relationship between cryptocurrencies and financial investments. In Sect. 3 we focus on behavioral issues, while in Sect. 4 we introduce the development of the socio-economic perspectives related to cryptocurrencies and discuss initial coin offerings as a potential source of funds for small businesses. Finally, Sect. 5 concludes discussing the research agenda for the future.",85
47,1,Journal of Industrial and Business Economics,07 September 2019,https://link.springer.com/article/10.1007/s40812-019-00136-8,A multivariate approach for the simultaneous modelling of market risk and credit risk for cryptocurrencies,March 2020,Dean Fantazzini,Stephan Zimin,,Male,Male,Unknown,Male,"Cryptocurrencies and the cryptomarket have become a popular trend in finance in the last years, as discussed by Antonopoulos (2014), Narayanan et al. (2016), Burniske and Tatar (2017), Fantazzini (2019) and Corbet et al. (2019). Bohr and Masooda (2014) and Yelowitz and Wilson (2015) showed that different people, from crypto and tech enthusiasts to investors, are interested in these new financial tools. Large market capitalizations for the most popular cryptocurrencies, increasing number of crypto fundsFootnote 1, a growing cumulative number of Initial Coin Offerings (ICOs) and, at the same time, a frequent number of hacks and frauds, make the topic of cryptocurrencies’ risk a really urgent problem, see Fantazzini et al. (2016, 2017) and references therein. Hence, the main goal of this paper is to propose a unified framework for the simultaneous modelling of credit and market risk. If we use the formal definition of cryptocurrency proposed by Lansky, (2018), then a cryptocurrency can be defined as a system that satisfies these six conditions: 1) The system does not require a central authority, its state is maintained through distributed consensus. 2) The system keeps an overview of cryptocurrency units and their ownership. 3) The system defines whether new cryptocurrency units can be created. If new cryptocurrency units can be created, the system defines the circumstances of their origin and how to determine the ownership of these new units. 4) Ownership of cryptocurrency units can be proved exclusively cryptographically. 5) The system allows transactions to be performed in which ownership of the cryptographic units is changed. A transaction statement can only be issued by an entity proving the current ownership of these units. 6) If two different instructions for changing the ownership of the same cryptographic units are simultaneously entered, the system performs at most one of them.” — Lansky (2018, p. 19) Therefore, a cryptocurrency does not have debt, and it cannot default in a classical sense. However, its price and the investors’ demand might drop dramatically because of a revealed scam, hack or other hidden problems that cannot be observed directly from the market data. Because of that, we believe that such risk is not exactly a market one and therefore we propose a definition of credit risk which is somewhat different from the classic one: credit risk for a crypto-coin is its “death”, a situation when its price drops significantly and a coin becomes illiquid. In this regard, we have to admit that there is not an unambiguous definition of a dead coin, neither in the professional literature, nor in the academic literature. However, it is worth noting that even when considered dead, some coins do still have some negligible daily trading volumes. There are two reasons for this phenomenon: the possibility to recover at least a small amount of the initial investment, or to bet on the possible revamp of the dead coin. Differently from stock companies and small and medium enterprises (SMEs), the procedure to revamp a coin is much easier and faster: it only takes a new code or, even easier, an update of the previous code where the major flaws are corrected. This is why the “death” state for a cryptocurrency can be only a temporary situation rather than a permanent one: see, for example, Sid (2018) who discussed the story of PENG, which is a coin revamp of an old abandoned project. A brief review of the history and the financial literature devoted to cryptocurrencies is reported in Appendix 1. The first contribution of the paper is a large discussion about credit risk and market risk for cryptocurrencies and how these risks can be defined with these assets. In traditional finance, credit risk is usually defined as the gains and losses on a position (or portfolio) associated with the fulfillment (or not) of contractual obligations, whereas market risk as the gains and losses on the value of a position (or portfolio) due to the changes in market prices (Basel Committee on Banking Supervision (2009) p. 6–7, Hartmann (2010), p. 697). Instead, we argue that credit risk for cryptocurrencies can be defined as the gains and losses on the value of a position of a cryptocurrency that is abandoned and considered dead but which can be potentially revived, while market risk can be described as the gains and losses on the value of a position (or portfolio) of alive cryptocurrencies, due to the movements in market prices in centralized and decentralized exchanges. The second contribution of the paper is a set of multivariate models which can be used to estimate the market risk for a portfolio of crypto-currencies by using the Value-at-Risk and the Expected Shortfall, and simultaneously to estimate also their credit risk using the Zero Price Probability (ZPP) model by Fantazzini et al. (2008), which is a methodology to compute the probabilities of default using only market prices. Recent papers by Su and Huang (2010), Li et al. (2016) and Valle et al. (2016) showed the ZPP dominance in terms of default probability estimation with respect to competing models. The third contribution of this work is the development of closed-form formulas for the ZPP in two special cases, namely the random walk with drift and a GARCH(1,1) model with normal errors, using recent results from barrier option theory by Su and Rieger (2009). Even though crypto-assets are far from being normally distributed, these closed-form formulas can provide a quick estimate of the probability of the coin death and they can give an investor at least a rough idea of the crypto-asset credit risk. The fourth contribution of the paper is a backtesting exercise using two datasets of 5 and 15 coins for market risk forecasting and a dataset of 42 coins for credit risk forecasting. For the purpose of the multivariate modelling of a portfolio of cryptocurrencies, we employ VAR-DCC and VAR-Copula-GARCH models with different specifications for the error terms. The Value-at-Risk and the Expected Shortfall for the single coins and for an equally weighted portfolio are calculated during a back-testing exercise and then evaluated with several tests. The ZPP approach is used for the estimation of the probability of default/death for the single coins and compared to classical credit scoring models (logit and probit) and to a machine learning algorithm (Random Forest). Our results show that a t-copula/skewed-t GARCH model outperform the competing models for market risk, while ZPP-based models outperform the other models for credit risk. The paper is organized as follows: the definitions of credit risk and market risk for cryptocurrencies are introducted in Sect. 2, while the methods proposed for market and credit risk modelling with cryptocurrencies are discussed in Sect. 3. Section 4 describes the empirical results, while robustness checks are discussed in Sect. 5. Section 6 briefly concludes.",14
47,1,Journal of Industrial and Business Economics,23 August 2019,https://link.springer.com/article/10.1007/s40812-019-00133-x,Disentangling the relationship between Bitcoin and market attention measures,March 2020,Gianna Figà-Talamanca,Marco Patacca,,Female,Male,Unknown,Mix,,
47,1,Journal of Industrial and Business Economics,27 August 2019,https://link.springer.com/article/10.1007/s40812-019-00135-9,The bitcoin: a sparkling bubble or price discovery?,March 2020,Imad A. Moosa,,,Male,Unknown,Unknown,Male,"The phenomenal rise of the price of bitcoin in 2017 was reversed in early 2018. The reversal triggered (or perhaps caused by) headlines such as the following: “European Central Bank wakes up to digital currency concern”; “Bitcoin, Ethereum and all other top 100 digital currencies tumble”; “Bitcoin value plunges dramatically amid global financial market drop”; “Cryptocurrency value slumps amid fears ‘bitcoin mania’ is over; and “Bitcoin price plunge: falling cryptocurrency value highlights new fears around digital money”. For some observers the bitcoin bubble has finally burst, and justifiably so. For the bitcoin enthusiasts, the plunge was a technical correction or a step in the price discovery process, with some believing that the price will rise again to reach $40,000. As of March 2019, the anticipated surge has not materialised as the price stood at just over $3700. The objective of this paper is to propose some empirical tests that can be used to find out if the bitcoin’s price behaviour represents a bubble, at least until the end of 2017. Naturally, if bitcoin was in a bubble during the period up to the end of 2017, then the price plunge of early 2018 must represent the bursting of the bubble. Very few studies have been conducted on this issue because the conventional procedures used to detect bubbles depend on some measure of fundamental value, which is determined by expected cash flows (dividends, rents, coupon payments, etc.). Even if a model is available to measure fundamental value, it may be difficult to ascertain the validity of the model and hence the accuracy of valuation. The contribution of this paper is to suggest some methodological procedures that can be used to detect bubbles without the need for a measure of fundamental value. The first procedure is based on the price volume relation—more specifically, the proposition that if volume is determined by price dynamics then a bubble is forming. The second procedure is based on the proposition that in a bubble, technical trading dominates fundamental trading. The third procedure is based on the property of explosive price behaviour, which can be identified by fitting an unobserved components model.",11
47,1,Journal of Industrial and Business Economics,20 August 2019,https://link.springer.com/article/10.1007/s40812-019-00130-0,Disposition effect and herding behavior in the cryptocurrency market,March 2020,Steven Haryanto,Athor Subroto,Maria Ulpah,Male,Unknown,Female,Mix,,
47,1,Journal of Industrial and Business Economics,21 November 2019,https://link.springer.com/article/10.1007/s40812-019-00143-9,How economic freedom reflects on the Bitcoin transaction network,March 2020,Piergiorgio Ricci,,,Male,Unknown,Unknown,Male,"Recent technological introductions have led to the development of innovative businesses (Block et al. 2017), such as the financial technology sector (fintech), which combines finance and technology to offer new financial instruments (Bjørnskov and Foss 2013). The main tools that characterize this innovative business, such as Initial Coin Offerings (ICOs) and cryptocurrencies are based on blockchain technology (Zavolokina et al. 2017). Prior research on fintech area mainly focuses on Bitcoin cryptocurrency (Satoshi and Nakamoto 2008). In particular, several authors in the scientific literature have analyzed statistical and technological aspects of its transaction network, such as the anonymity of users (Reid and Harrigan 2013) and the distribution of amounts among Bitcoin wallets (Ron and Shamir 2013) but it has rarely considered the existing relationships with socio-economic aspects at national level. At this early stage of fintech sector development, it is strategic to identify its main enabling factors and potential risks in order to allow policymakers to create institutional contexts able to facilitate the dissemination of fintech instruments and, at the same time, of limiting the related socio-economic risks. The level of national economic freedom represents an enabling factor for the economic growth of a country (Cebula and Clark 2012). Based on this assumption, it is interesting to understand the role it plays in the development of fintech sector. In particular, high levels of economic freedom at national level produce positive effects on the conduct of financial activities (Gohmann et al. 2013). The research work has been mainly driven by the following questions: is it possible to identify any relationships between the degree of economic freedom associated with a specific country and the use of fintech services within it? What drives fintech sector development at the national level? Considering that Bitcoin cryptocurrency represents the most widespread application of blockchain technology, the Bitcoin geographical transaction network has been considered as a proxy to evaluate the fintech sector development at the country level. By using social network analysis approach, this study explores how the position of each country expressed in terms of centrality measures performed on the Bitcoin geographical network responds to changes in the degree of economic freedom in the top 70 economies selected by the ranking drawn up by the International Monetary Fund (IMF). The network modeling and social network analysis approach offer great explanatory power as they enable to trace of the international trend of Bitcoin flows passing through the international wallets. The study showed that countries characterized by high levels of economic freedom, evaluated in terms of freedom to trade internationally, a restrained value of inflation and low administrative requirements can be considered suitable for fintech sector development, in fact, they present high values of social network centrality indicators performed within the geographical Bitcoin network. With reference to potential illegal conduct arising from the use of cryptocurrencies, concerning a good percentage of the total number of Bitcoin international transactions (Foley et al. 2019), the analysis also revealed high levels of network centrality indicators by those countries characterized by low controls and restrictions on capital.",5
47,1,Journal of Industrial and Business Economics,24 September 2019,https://link.springer.com/article/10.1007/s40812-019-00139-5,Sustainable development and cryptocurrencies as private money,March 2020,John Vaz,Kym Brown,,Male,Female,Unknown,Mix,,
47,2,Journal of Industrial and Business Economics,21 November 2019,https://link.springer.com/article/10.1007/s40812-019-00144-8,Financial theories of foreign direct investment: a review of literature,June 2020,S. Veeramani,Abha Shukla,Mariam Jamaleh,Unknown,Female,Female,Female,"The increased volume of capital flows in the form of foreign direct investment (FDI) since the 1950s was accompanied by the development of many important theories which attempted to explain the behavior of rising multinationals and the determinants behind that behavior. The major part of literature on FDI was based on the assumption of imperfections in goods and factor markets, which led to the birth of the internalization theory, the oligopolistic theory and the most famous OLI approach aka the eclectic paradigm. These theories emphasize that FDI happens not because of differences in the cost of capital but because certain domestic assets are better utilized under foreign control (Froot and Stein 1991). The need for an FDI theory that acknowledges imperfections in capital markets has been neglected but not completely ignored. Assuming a certain level of segmentation among capital markets during the 1970s, financial economists advanced arguments in favor of exchange rate driven FDI. They find that exchange rate movements offer an explanation to short term swings in FDI flows while traditional determinants don’t. In that spirit, an extensive literature has developed which links different exchange rate variables to FDI flows, but without a clear conclusion about the nature of their relationship. One of the important aspects of that relationship is the degree of multinationals’ exposure to exchange risk. Recent studies show that multinationals are able to reduce their exposure to exchange risk to a level lower than that of purely domestic firms by utilizing their operational hedging abilities. Barriers to the flow of portfolio capital have also led to the consideration of a diversification motivated FDI. Since individual investors do not seem to be able to reap the benefits of international diversification, the role of multinationals arises to diversify on their behalf. While earlier studies attempt to show multinationals’ risk/return properties as superior to those of domestic firm, only recently the literature has provided a direct evidence on the efficiency of multinationals in substituting for international diversification. This is mostly done in the context of the home bias phenomenon in the equity markets. The importance of a diversification motive for FDI has also been enriched with the developments in research on the advantages a multinational could reap by running an efficient internal capital market. Both theories of diversification and exchange risk are based on the efficiency of stock markets, this results in financing being less than an issue when considering foreign direct investment. Only recently and along with the growing literature on behavioral finance, few studies have considered the impact of behavioral biases of both investors and managers on FDI decisions. In contrast to traditional views of corporate finance, the supply of capital is not elastic; investor sentiment and shocks to intermediary capital could lead to systematic mispricing (Baker 2009). In that situation, overvalued firms have the chance to finance their FDI using temporarily available cheap capital, while undervalued firms may find themselves easy targets for foreign acquirers. Managers of multinationals are also more susceptible to behavioral biases considering the high level of uncertainty involved in the decision to invest abroad. Multinational managers could easily be found herding to a particular location or anchoring to a certain deal size to alleviate the complexity of their decisions. The last 2 decades have witnessed the emergence of multinationals from unconventional places in the world including small industrialized, transition and emerging economies. These multinationals are financially disadvantaged in comparison with their counterparts from conventional source countries. The literature, most notably Oxelheim et al. (2001), suggests that this new type of multinationals must have had a different internationalization path. They follow certain proactive financial strategies to break out from their segmented capital markets and reach a level-playing field with their financially endowed rivals from developed countries. These strategies include cross listing, attracting sophisticated international investors, recruiting foreign managers, enhanced credit rating plus any innovation which might result in creating a financial advantage or even eliminating a disadvantage of the kind. This paper proceeds as follows. Section 2 presents the focus of this study. In Sect. 3 we lay down the adopted methodology. In Sect. 4, we elaborate on each of the four strands of financial literature on FDI. First, we discuss the relationship between FDI and exchange rate and concentrate on a particular aspect that is the exchange risk exposure of multinationals. Second, we introduce the arguments in favor a diversification motivated FDI and how they evolved over time. Third, we bring up the behavioral aspects of FDI. Fourth, we discuss the importance of financial creativity for a particular type of multinationals. Section 5 concludes.",2
47,2,Journal of Industrial and Business Economics,01 November 2019,https://link.springer.com/article/10.1007/s40812-019-00141-x,Crypto assets: the role of ICO tokens within a well-diversified portfolio,June 2020,Saman Adhami,Dominique Guegan,,,,Unknown,Mix,,
47,2,Journal of Industrial and Business Economics,12 February 2020,https://link.springer.com/article/10.1007/s40812-020-00147-w,On the organizational design of entrepreneurial ventures: the configurations of the entrepreneurial team,June 2020,Paola Rovelli,Vincenzo Butticè,,Female,Male,Unknown,Mix,,
47,2,Journal of Industrial and Business Economics,05 March 2020,https://link.springer.com/article/10.1007/s40812-020-00151-0,Note on the excess entry theorem in the presence of network externalities,June 2020,Tsuyoshi Toshimitsu,,,Male,Unknown,Unknown,Male,"About 30 years ago, Mankiw and Whinston (1986) and Suzumura and Kiyono (1987) cast doubt on the traditional belief that free entry is desirable for social efficiency and provided the conditions under which the number of entrants in a free entry equilibrium is socially excessive, insufficient, or optimal.Footnote 1 In particular, they established the “excess entry theorem,” which focused on the welfare-improving effect of increasing competitiveness in the model of an industry with three characteristic features, i.e., economies of scale because of the existence of fixed costs, a homogeneous final product, and oligopolistic competition. Since the seminal papers of Mankiw and Whinston (1986) and Suzumura and Kiyono (1987), many studies have generalized the excess entry theorem in various ways and extended it to industrial policies. Although a large number of studies have investigated the theorem, they focused mainly on Cournot oligopolies with homogeneous goods.Footnote 2 Following the review paper by Suzumura (2012), e.g., Kagitani et al. (2016) also examined the theorem in the case of horizontally differentiated oligopoly, based on the linear demand of the Shubik and Levitan (1980) model and demonstrated that if products are substitutes, excess entry arises regardless of the mode of competition. Many companies are currently entering information and communications industries, such as telecommunications, Internet services, and E-commerce; however, they are facing severe competition in these markets, where the products and services are associated with network externalities. Thus, in this paper, we examine whether the effect of entry into such a network goods market is socially efficient or inefficient. Introducing network externalities into a Cournot oligopoly in a homogeneous product market, we reconsider the excess entry theorem. In considering the problem, we focus on consumer expectations which play an important role in a market with network externalities. Based on the definition by Hurkens and López (2014), we examine the cases of passive and responsive expectations: i.e., passive expectations imply that consumers first form expectations about network sizes and then firms compete in quantities. Finally, consumers make optimal purchasing decisions, given their expectations. These decisions then lead to the determination of actual market shares and network sizes. Thus, in equilibrium, the realized and expected network sizes are the same (Katz and Shapiro 1985). Responsive expectations imply that firms first compete in quantities and then consumers form expectations about network sizes. Finally, consumers make optimal purchasing decisions, given the quantities (or prices) and their expectations. We demonstrate as follows. In the case of passive expectations, a socially inefficient entry arises if the degree of network externalities is sufficiently large. In other words, the excess entry theorem does not hold. If so, a government should not deter entry but rather attract entry into a network goods market. However, in the case of responsive expectations, the number of firms under free entry is socially excessive, irrespective of the degree of network externalities. That is, the excess entry theorem holds. In this case, a government should restrict new entries. The remainder of this paper is organized as follows. In Sect. 2, we present a Cournot oligopoly model with network externalities. Based on the model, we derive a fulfilled Cournot equilibrium in the cases of passive and responsive expectations. In Sect. 3, we reconsider the excess entry theorem and discuss the implications of the results, which are related to the type of consumer expectations and the degree of network externalities. Finally, in Sect. 4, we summarize the results and discuss some remaining issues. We outline the excess entry theorem in the case of a general function in the “Appendix”.",5
47,2,Journal of Industrial and Business Economics,25 July 2019,https://link.springer.com/article/10.1007/s40812-019-00127-9,A two-period unionized mixed oligopoly model: public–private wage differentials and “Eurosclerosis” reconsidered,June 2020,Minas Vlassis,Polyxeni Gioti,,Male,Female,Unknown,Mix,,
47,2,Journal of Industrial and Business Economics,25 October 2019,https://link.springer.com/article/10.1007/s40812-019-00140-y,Traditional manufacturing areas and the emergence of product-service systems: the case of Italy,June 2020,Marco Bellandi,Silvia Lombardi,Erica Santini,Male,Female,Female,Mix,,
47,2,Journal of Industrial and Business Economics,12 November 2019,https://link.springer.com/article/10.1007/s40812-019-00142-w,"Imperfect patent protection, licensing, and willingness to pay for the innovation",June 2020,Carlo Capuano,Iacopo Grassi,,Male,Male,Unknown,Male,"This study aimed to analyze how imperfect patent protection affects the patent holder’s licensing decision, the firms’ willingness to pay for the innovation, and social welfare. Our analysis is motivated by the observation that, in recent years, patenting has exploded as a global phenomenon.Footnote 1 However, patenting does not mean innovating,Footnote 2 and patents may not perfectly protect an innovation. Indeed, if litigated, a court may invalidate them. Lemley and Shapiro (2005) stated: Nearly 200,000 patents are issued every year after a very limited examination process. Most issued patents turn out to have little or no commercial significance, which is one reason that only 1.5% of patents are ever litigated, and only 0.1% of patents are ever litigated to trial. Given these uncertainties, economists have increasingly recognized that a patent does not confer upon its owner the right to exclude but rather a right to try to exclude by asserting the patent in court. When a patent holder asserts its patent against an alleged infringer, the patent holder is rolling the dice. If the patent is found invalid, the property right will have evaporated (Lemley and Shapiro 2005, p. 75). The proliferation of licensing is a direct consequence of the patent-granting explosion.Footnote 3 As Spulber (2016) points out, “total worldwide royalty payments for IP (patents, trademarks, and copyrights) exceed $289 billion.” The economic literature has analyzed the impact of licensing on a firm’s incentive to innovate; in general, when a firm can license its innovation, its R&D effort is higher.Footnote 4 Furthermore, increasing R&D has positive effects on social welfare when licensing is allowed (Colombo and Filippini 2014), with some differences based on the type of licensing contract (for example, fixed fees or ad valorem royalties) (San Martin and Saracho, 2010). Some recent literature has focused on firms that license innovation without competing in the market: the so-called non-practicing entities (NPEs). Bessen and Meurer (2013) measured the costs of NPEs on practicing firms, estimating a direct cost of 29 billion in 2011; Cohen et al. (2014) showed that a firm substantially reduces its innovative activity after settling with NPEs. Some scholars have analyzed the effect of  imperfect patent protection on social welfare and innovation. Boldrin and Levine (2013) suggested that systems with relatively weak patent protection may mildly stimulate innovation, whereas systems with strong patent protection hinder innovation and usually lead to numerous socially undesirable effects (loss of consumer surplus and total welfare, high costs of legal disputes over patents, enterprises resigning from the development of patents due to holdup); Krasteva (2014) studied the effect of imperfect patent protection on R&D incentives, showing that R&D investment is maximized for an intermediate level of patent strength; Gilbert and Kristiansen (2018) analyzed the implications of a weak licensing contract, showing that it may increase the incentives for the innovation by licensees and total welfare. In the context of uncertainty  regarding to patent protection, Gans and Stern (2000) noted that the incumbent (licensee) may be able to increase its bargaining power by threatening to perform imitative R&D if bargaining breaks down.Footnote 5 In this respect, a purely strategic incentive exists for incumbents to maintain an R&D capability that has a positive effect on innovation. They focus their analysis on the case where the entrant is the innovator, and they use the Gilbert and Newbery (1982) approach to measure the incentive to innovate. Our article extends this literature by studying how imperfect patent protection modifies the licensing and entry strategies of the firms and their incentives to acquire the innovation and how this affects social welfare. We present a game where a patent holder of a cost-reducing process innovation can compete in the market with another firm. We consider two scenarios: one where the incumbent achieves the innovation and one where the (potential) entrant does. The patent holder can compete without offering the license, offering an exclusive license, or offering a sole license for its technology to the competitor.Footnote 6 The competitor can refuse the offer and imitate the patent, with a probability of winning the trial for patent infringement. This licensing game has several consequences for the firms’ R&D investment decision and social welfare.Footnote 7 Whether the innovation is coming from the incumbent or the entrant, if the level of patent protection is low enough, the patent’s holder does not license the innovation, and the non-innovating firm imitates. If the level of patent protection is high enough, the innovator licenses its innovation (with a sole or an exclusive license, depending on the case). Increasing patent protection reduces the incentive to imitate, increasing the licensing expected profit. If the patent protection is low, the patent holder prefers to play a lottery, which may grant the monopoly; if it is high, the patent holder prefers to license the innovation. In the second part of the analysis, following the approach proposed by Gilbert and Newbery (1982), we compare the two proposed scenarios, evaluating which firm (the incumbent or the potential entrant) is more willing to pay for the innovation. The first result is that, differently from Gilbert and Newbery (1982), the potential entrant is willing to pay more than the incumbent when the patent protection is high enough. In this case, the innovation comes from the entrant, which has incentive to license it to the incumbent, without entering the market, that is, acting as a non-practicing entity. Finally, we prove that imperfect patent protection has a twofold impact on the market performance. With low patent protection, the threat of imitation discourages licensing agreements. If the patent holder is the potential competitor, it enters the market; if it is the incumbent, the entrant tries to imitate the innovation. In both the cases, duopoly may emerge in equilibrium, increasing allocative and (static) productive efficiency. With high patent protection, in both cases, firms license the innovation. When the patent holder is the entrant, the license is exclusive, and the monopolistic structure is maintained. When the patent holder is the incumbent, the license is sole, and a duopoly emerges in equilibrium. Consequently, increasing the level of patent protection may decrease the expected social welfare. Our results are comparable to the findings reported in Gans and Stern (2000). In a model setting where imperfect patent protection is not considered, Gans and Stern (2000) showed that the potential entrant may be more willing to pay for innovation. Moreover, Gans and Stern (2000, p. 505) noted that “there is no particular bar to—and there are strong incentives for—a licensing solution that maintains a monopoly market structure.” We found different results when the level of patent protection is low enough. In such a case, market structure is affected by the innovation, and a duopoly emerges in equilibrium, independently on the patent holder. The article is organized as follows: Section 2 introduces the model; Sect. 3 presents the case where the incumbent achieves the innovation; Sect. 4 concentrates on the case where the entrant is the patent holder; Sect. 5 extends the model analyzing the willingness to pay for the innovation and the welfare analysis; Sect. 6 concludes.",3
47,2,Journal of Industrial and Business Economics,05 February 2020,https://link.springer.com/article/10.1007/s40812-020-00146-x,Correction to: Industry 4.0: revolution or hype? Reassessing recent technological trends and their impact on labour,June 2020,Armanda Cetrulo,Alessandro Nuvolari,,Female,Male,Unknown,Mix,,
47,3,Journal of Industrial and Business Economics,22 July 2020,https://link.springer.com/article/10.1007/s40812-020-00172-9,"International production, structural change and public policies in times of pandemics",September 2020,Andrea Coveri,Dario Guarascio,Michael Landesmann,Female,Male,Male,Mix,,
47,3,Journal of Industrial and Business Economics,21 July 2020,https://link.springer.com/article/10.1007/s40812-020-00173-8,"Unequal societies in usual times, unjust societies in pandemic ones",September 2020,G. Dosi,L. Fanti,M. E. Virgillito,Unknown,Unknown,Unknown,Unknown,,
47,3,Journal of Industrial and Business Economics,15 July 2020,https://link.springer.com/article/10.1007/s40812-020-00168-5,Employment impact of Covid-19 crisis: from short term effects to long terms prospects,September 2020,Marta Fana,Sergio Torrejón Pérez,Enrique Fernández-Macías,Female,Male,Male,Mix,,
47,3,Journal of Industrial and Business Economics,17 July 2020,https://link.springer.com/article/10.1007/s40812-020-00165-8,A fragile and divided European Union meets Covid-19: further disintegration or ‘Hamiltonian moment’?,September 2020,Giuseppe Celi,Dario Guarascio,Annamaria Simonazzi,Male,Male,Female,Mix,,
47,3,Journal of Industrial and Business Economics,10 July 2020,https://link.springer.com/article/10.1007/s40812-020-00163-w,Pandemic pushes polarisation: the Corona crisis and macroeconomic divergence in the Eurozone,September 2020,Claudius Gräbner,Philipp Heimberger,Jakob Kapeller,Male,Male,Male,Male,"The Coronavirus and the resulting lockdowns and economic restrictions are severely testing the structural resilience of European economies. On the domestic level, the imposed restrictions tend to hit economically weaker households and firms harder, causing large-scale economic hardship that might fuel public resistance against economic restrictions based on public health concerns. Hence, social divisions may undermine the resilience of European societies in terms of public health on the level of domestic economies. Likewise, preliminary evidence on the European level suggests that economically weaker nations within the Eurozone are hit harder by the Corona crisis, which may have severe repercussions for the Eurozone as a whole. While this article focuses on the latter aspect—by asking how the Corona crisis may contribute to the amplification of economic polarisation within the Eurozone—a common observation worth spelling out in both the domestic as well as in the European context is that existing social divisions limit the collective resilience of societies in public health terms. In both contexts, weaker actors are not only hit harder, but have also fewer resources and leeway to cope with the immediate consequences of the crisis. For the case of the Eurozone, the present article points out that because of the polarisation processes that started well before the Corona pandemic both the extent of existing vulnerabilities as well as the policy space to counter the crisis differ considerably across Eurozone member countries. As a consequence, the economic impacts are likely to be asymmetric and will, in the absence of coordinated policy responses, accelerate existing polarisation processes between an economically more well-off Northern and a struggling Southern Eurozone.Footnote 1 The enormous challenge of economic recovery after the Corona health crisis will be most pressing in the Southern parts of the Eurozone, which consists of Greece, Italy, Portugal and Spain. In these Southern countries, the crisis is forecast to reduce GDP growth rates more than in the Northern Eurozone countries comprising Austria, Belgium, Finland, Germany and the Netherlands (see panel A of Fig. 1; since France often takes an intermediate position, it is reported separately). Furthermore, unemployment rates in Southern countries have not only reached much higher levels in pre-Corona times as compared to the Northern Eurozone, they also seem to be more strongly affected by the advent of the Corona crisis: according to the most recent macroeconomic forecastsFootnote 2 Southern countries will suffer a relatively more pronounced increase in unemployment due to the economic downturn, aggravating the already existing differences in the Eurozone (see panel B of Fig. 1). Thus, recovery needs differ considerable across regions in Northern and Southern parts of the Eurozone (European Commission 2020a). Macroeconomic polarisation in the Eurozone. Northern Eurozone (population-weighted average): Austria, Belgium, Finland, Germany, Netherlands. Southern Eurozone (population-weighted average): Greece, Italy, Portugal, Spain At the same time, Southern Eurozone countries—above all Italy and Greece—have entered into the Corona crisis with high levels of public debt. Recent forecasts suggest that increases in fiscal deficits in the Southern periphery will be particularly severe due to increasing government spending and decreasing tax revenues, and public-debt-to-GDP ratios—which are quantitatively affected by both increasing fiscal deficits as well as decreasing GDP—will rise strongly (see panels C and D of Fig. 1). Furthermore, government revenues are expected to decline more in Southern Eurozone countries against the background of bigger losses in economic activity than in Northern Eurozone countries. At the same time, government spending is forecast to increase more in the Northern Eurozone countries, reflecting a stronger response by automatic stabilizers as well as bigger discretionary efforts to counteract the Corona crisis (see Fig. 2). Tax revenues and government spending in the Eurozone (inflation-adjusted). Northern Eurozone (population-weighted average): Austria, Belgium, Finland, Germany, Netherlands. Southern Eurozone (population-weighted average): Greece, Italy, Portugal, Spain Against this background, there is a risk that—under current institutional conditions—Italy and other Southern Eurozone countries will be able to finance only the most urgent measures, while Northern Eurozone countries with a better starting position—especially Germany, Austria or the Netherlands—have more fiscal space to support a rapid recovery once the economy is jump-started. In this view the main constraints that prohibit a quick(er) recovery of Southern countries arise from the current institutional arrangements under which fiscal space is typically correlated with macroeconomic performance (Heimberger and Kapeller 2017).Footnote 3 Available data already point to such asymmetric fiscal responses at the national level: in particular, the immediate increase in fiscal spending in Germany (in the form of additional government spending on medical equipment, short-time work, subsidies for small and medium-sized enterprises etc.) amounts to more than 10% of economic output in 2020, compared with only 0.9% for Italy, 1.1% in Spain, 2.5% in Portugal and 1.1% in Greece. But also the indirect fiscal response—the deferral of taxes and social security contributions as well as other liquidity provisions and loan guarantees—lags behind Germany in all Southern countries except Italy (see Fig. 3, based on Anderson et al. 2020). While the numbers shown in Fig. 3 are influenced by a series of qualitatively different factors—including the impact of the pandemic and the state of the public health systems—they also reflect differences in fiscal space across countries, especially in the context of direct spending undertaken by governments. This observation suggests that existing differentials in economic performance are indeed aggravated through the Corona pandemic and that the competitiveness as well as the standard of living in the Southern countries is likely to deteriorate further relative to other parts of the Eurozone. The fiscal response to the economic fallout from coronavirus: Germany vs. Southern Eurozone countries. Immediate fiscal impulse: additional government spending (such as medical resources, short-term work, subsidies for companies, public investment). Deferral: tax and social security contributions deferral. Other liquidity provisions and guarantees: Export guarantees, liquidity assistance, credit lines through national development banks This article discusses these uneven macroeconomic consequences and economic policy responses against the background of an analysis of longer-term macroeconomic divergence in the Eurozone. Previous research has shown that the underlying processes are path-dependent and relate not only to the divergence of major macroeconomic indicators, but also to the polarisation of production structures between Eurozone member countries and the associated development of divergent export-led and private-debt-led growth models (Simonazzi 2013; Botta 2014; Storm and Naastepad 2015; Celi et al. 2018; Gräbner et al. 2020b). The present paper also highlights that increased macroeconomic polarisation in pre-Corona years has fuelled political polarisation, which has become visible in recent Corona policy debates concerning the appropriate response to the macroeconomic consequences of the COVID-19 pandemic: countries such as Italy and Spain have immediately pushed for a stronger common European fiscal response, only to find their more ambitious proposals about European burden-sharing of the crisis costs turned down by Northern Eurozone countries. A more nuanced discussion about the potential for a pan-European recovery initiative only started with a considerably time lag, promoted by a change in the political stance of the German government. It will be argued below, however, that—in the absence of coordinated policy interventions—the process of economic divergence occurring in the Eurozone must be expected to accelerate further after the lockdown. Such a process would put the Eurozone as a whole at risk. With an eye to preventing the common currency area from falling part, we will discuss some elements of coordinated fiscal and industrial policy action that could contribute to countering economic polarisation in the context of the Corona crisis. Such policies could also be designed in a way that is consistent with a longer-term orientation towards achieving social and environmental sustainability.",16
47,3,Journal of Industrial and Business Economics,25 July 2020,https://link.springer.com/article/10.1007/s40812-020-00171-w,Covid-19 crisis: centrifugal vs. centripetal forces in the EU—a political-economic analysis,September 2020,Michael A. Landesmann,,,Male,Unknown,Unknown,Male,"It is clear that the current corona-crisis puts an enormous strain on cohesion of the European Union’s economic and political structures. Specific to this crisis is that it follows the financial crisis of 2008/09 which has in no way been ‘digested’ either economically or politically by the complex political-economic constellation that represents the European Union. However, the fact that this crisis did hit the EU in this relatively vulnerable state also means that it can unleash both centrifugal as well as centripetal forces in the European integration process. This will be the subject of the political-economic analysis that I will attempt in this short article.",4
47,3,Journal of Industrial and Business Economics,08 June 2020,https://link.springer.com/article/10.1007/s40812-020-00162-x,The 2020 Covid-19 pandemic and global value chains,September 2020,Roger Strange,,,Male,Unknown,Unknown,Male,"There will come a time when the 2020 Covid-19 pandemic will be (to a greater or lesser extent) under control.Footnote 1 Many people will have died in the meantime, many others will have lost friends and/or family members, and many businesses will have ceased to exist. But for those of us fortunate to survive, lockdowns, social distancing and other such measures will be memories, and our lives will return to a degree of normality. The timescale is still very uncertain at the time of writing (mid-April 2020), but the objective of this paper is to consider the medium-term impacts on firm strategies and, in particular, the configuration of firms’ global value chains (GVCs) once all pandemic-related restrictions have been removed. The Covid-19 pandemic has three essential features. First, it is a global phenomenon in that the virus has been detected in most countries around the world. About half of humanity (c4.5bn people) was under some form of containment in the initial stages of the pandemic.Footnote 2 This feature differentiates the Covid-19 pandemic from other recent virus outbreaksFootnote 3 whose health effects have been more limited and localized. As the Imperial College COVID-19 Response Team (2020: 3) comment, the ‘last time the world responded to a global emerging disease epidemic of the scale of the current Covid-19 pandemic with no access to vaccines was the 1918–19 H1N1 influenza pandemic.’ Second, the effects of the pandemic have been multi-dimensional in that it has had adverse impacts both upon public health and upon economic activity in most national economies. Furthermore, policy responses designed to address one adverse impact typically exacerbate the other: lockdowns slow the spread of the virus but harm the economy, whilst allowing people back to work benefits the economy but may lead to more infections. This differentiates the pandemic from financial crises where potential remedies were easier to conceive and match to the underlying problems. Third, the pandemic is contagious not just in the health sense but also in an economic sense, as the global economy is so inter-connected through GVCs and international movements of people, capital, goods and services. Trade in intermediate goods and services account for over 60% of total international trade Contractor (2020) reports that multinational enterprises (MNEs) were involved either as exporters or as importers, or as lead firms in GVCs, in 80 percent of all world trade (amounting to around $US 20 trillion in 2019). He further reports that the same MNE was both the exporter and the importer (i.e. simultaneously on both ends of the shipment) in approximately 40 percent of world trade. The corollary is that no country is immune to the health and economic impacts of the virus unless it is totally isolated from the rest of the world. The structure of the paper is as follows. In the next section, we briefly consider the public health impact of the pandemic, and highlight the difficult dilemma that governments worldwide face in trying to balance the health and economic effects on their countries in the short-term. Our intention is not to address this dilemma, both because we do not have the necessary data to make informed judgments and because, ultimately, the choices will be political decisions. We then outline the short-term supply and demand shocks that the pandemic has inflicted on national economies. We next summarize the benefits for firms from participation in GVCs in normal times, and the problems that ensue when people and goods cannot move freely during pandemics. Next, we address how firms might adjust their strategies once the Covid-19 pandemic has been brought under control, particularly with regard to the configuration of the GVCs in terms of the locations of the different activities and their governance. We conclude by emphasizing the geopolitical context within which the pandemic is proceeding, and the obvious (but maybe overlooked) point that the virus needs to be brought under control everywhere not just in individual countries.",78
47,3,Journal of Industrial and Business Economics,14 July 2020,https://link.springer.com/article/10.1007/s40812-020-00167-6,Supply chain contagion and the role of industrial policy,September 2020,Andrea Coveri,Claudio Cozza,Antonello Zanfei,Female,Male,Male,Mix,,
47,3,Journal of Industrial and Business Economics,06 July 2020,https://link.springer.com/article/10.1007/s40812-020-00164-9,The Italian value chain in the pandemic: the input–output impact of Covid-19 lockdown,September 2020,Raffaele Giammetti,Luca Papi,Davide Ticchi,Male,Male,Male,Male,"The outbreak of the global Covid-19 pandemic has led many countries to implement drastic social distancing rules and sectoral lockdowns. While there is already a large strand of the literature that analyzed and provided evidence on the effectiveness of social distancing interventions in delaying and reducing the spread of epidemics (see, among others, Hatchett et al. 2007; Markel et al. 2007; Bootsma and Ferguson 2007), our knowledge of the economic impact of such public health measures is instead rather limited (e.g., Wren-Lewis 2020). However, one should also recognize that there has been a large number of works in the last few months that have tried to fill this gap along many dimensions. For example, some works have analyzed the trade-off between public health and output, others have studied the effects of pandemic on sectors most affected by government restrictions and/or by the economic fallout (e.g., tourism, banks, etc.), while a number of works have focused on the optimal fiscal and monetary policies aim at reducing the output and welfare losses (see Caracciolo et al. 2020, for a review). In this paper we want to contribute to the strand of this fast-growing literature that studies the output losses generated by governments’ restrictions on economic activity. In doing this, our starting point is that any analyzes aiming at quantifying the economic impact of restrictions to production needs to take into account the interlinkages between sectors, a feature that is often mentioned in public debates but seldomly explicitly considered in economic analyses. Indeed, as Caracciolo et al. (2020, p. 10) conclude in their review: “The relevance of complementarities between sectors (e.g. input–output chains, or more simply those between the education system and parents’ capability of employability) is sometimes mentioned, but rarely taken into account.” In particular, we here investigate the main features of the Italian production network and then quantify the role of the domestic value chain in the transmission of the economic impact of Covid-19 lockdown measures. As anticipated above, the stop to many production processes caused by lockdown measures leads to a drop in input and output whose economic impact are difficult to quantify without considering the interlinkages between sectors. For this reason, we employ the techniques of complex networks analysis and input–output traditional tools that allows us to identify the sectors that are key in the complex structure of the Italian supply chain and to provide different rankings of the most ‘systemically important’ industries involved in the Covid-19 lockdown measures. The results of our analysis suggest that by stopping the production process of many key sectors, the lockdown has led to a drop in input and output that, in turn, has generated a lock of about 52% of total circulating value added, 30% of which has been locked within indirect value chains.Footnote 1 Further, by adding sectoral physical proximity indexes to the scenarios analysis, the method developed here provides a tool to guide governments in designing safe and efficient reopening policies. Our work is closely related to various recent works that have analyzed the impact of the Covid-19 pandemic on GDP. For example, the study of the OECD (2020), that aggregates industry-level shocks, estimates a potential immediate impact of shutdowns measures on GDP of around 25%. Barrot et al. (2020) estimate industry-level shocks by considering the list of essential industries in a multisector input–output framework, and find that 6 weeks of social distancing would bring GDP down by 5.6%. Similarly, Baqaee and Farhi (2020a, b) study the effects of the lockdown on GDP declines in input network economies and show how nonlinearities associated with complementarities in consumption and production amplify the effect of negative supply and demand shocks. Bonadio et al. (2020) analyze the role played by global supply chains in estimating the impact of the Covid-19 pandemic on GDP growth for 64 countries finding that cross-country variations are well-explained by differences in lockdown severity across countries. Our work differs from the above cited contributions along two dimensions. First, we focus on the Italian economy. And, second, while all these studies are based on assumptions about social distancing rules and lockdowns severity, we draw directly on the list of essential industries developed by the Italian Prime Minister’s Decree (IPMD) dated April 10, 2020. The work is organized as follows. Section 2 explores and discusses the main features of the Italian production network. Section 3 describes the model and methodology used to assess the amount of GDP locked and discusses the results, while Sect. 4 offers some concluding remarks.",34
47,3,Journal of Industrial and Business Economics,11 July 2020,https://link.springer.com/article/10.1007/s40812-020-00169-4,European SMEs amidst the COVID-19 crisis: assessing impact and policy responses,September 2020,Jill Juergensen,José Guimón,Rajneesh Narula,Female,Male,Unknown,Mix,,
47,3,Journal of Industrial and Business Economics,10 July 2020,https://link.springer.com/article/10.1007/s40812-020-00166-7,How much will the vaccine cost (if ever discovered….),September 2020,Luigi Marengo,,,Male,Unknown,Unknown,Male,"At the end of February 2020, when the Covid-19 pandemic was beginning to spread in Europe and North America, Alex M. Azar, the Trump administration's Secretary of Health and Human Services, had an official hearing before the US Congress to discuss the public health policies that the administration intended to adopt and their impact on the US budget. During the hearing, Secretary Azar, after declaring that a vaccine against covid-19 could be ready for testing in 3 months (a statement that proved grossly overoptimistic), also made an interesting statement on its likely price: “We would want to ensure that we work to make it affordable, but we can’t control that price because we need the private sector to invest”. Faced with this statement, 45 members of Congress sent a letter to President Trump the next day asking that the Department of Health not grant any exclusive license for a coronavirus vaccine or cure to a private enterprise if the research leading to these treatments was partly financed by public funds. More generally, the letter argues that granting monopoly rights on these pharmaceutical discoveries could result in excessively high sales prices, with strongly negative consequences on public health and on the budgets of households and government. If these excessive prices were actually charged—the petitioners argued—the government should intervene and take all necessary measures to ensure that treatments and vaccines be available for all at affordable prices. Secretary Azar on the one hand and the signatories of the letter on the other pose a fundamental question: in a capitalistic market system, companies are free to invest resources in research and innovation if and where they expect high returns on these investments. Therefore, as Secretary Azar claims, if we want pharmaceutical and biotech companies to invest in the search for covid-19 vaccines and treatments, we must offer them the prospect of high returns, that is, we must give them the opportunity to sell medicines and vaccines at prices substantially higher than production costs. Our economic systems have created and developed a special tool to guarantee these high returns for innovators: patents. Patents “reward” innovators with monopoly rights for a given number of years (normally 20). But this monopoly right—argue the 45 signatories of the letter—will inevitably result in high prices, reduced quantities and a redistribution of well-being to the detriment of the consumer and in favour of the producer. At this point, however, Azar would probably rebut that it is true that the monopoly power granted by patents reduces the well-being of consumers of the new medicine compared to a hypothetical situation in which this medicine was sold under a competitive regime. But, without a patent, this medicine would probably not exist at all because no company would invest in an innovation that competitors could easily imitate. Therefore, Azar would argue, the real comparison we have to make is not between a vaccine or treatment sold at either monopoly or competitive price, but between a vaccine sold at monopoly price and no vaccine at all. Monopoly and high prices would therefore be a sort of inevitable lesser evil, a price that society must pay for the treatments and vaccines that otherwise would not be invented by profit seeking firms. Who is right? Azar or the signatories of the letter? In my opinion, the signatories of the letter, for at least two orders of reasons. First, Azar raises a real problem, that is how to encourage the production of innovations by private companies in a market economy, but assumes that patents are the best solution to this problem, while I think they are not. Secondly, it takes for granted that the market is the economically and socially best way to solve a problem such as finding cures and vaccines that block or at least reduce the effects of a pandemic such as that caused by covid-19.",1
47,3,Journal of Industrial and Business Economics,11 July 2020,https://link.springer.com/article/10.1007/s40812-020-00170-x,A critique of the Indian government’s response to the COVID-19 pandemic,September 2020,Jayati Ghosh,,,Female,Unknown,Unknown,Female,"In dealing with the pandemic, many countries followed the pattern observed in Wuhan, China, involving strict lockdowns, controls on mobility, economic and social activity and requirements of “social distancing” (hereafter referred to more correctly as physical distancing). In India, a very stringent version of this strategy was adopted relatively early in the onset of the infection, and also very abruptly, with only four hours’ notice given to the population. Unfortunately, this failed to take into account the specific socio-economic contexts and characteristics of life and work for most people in India, which made the consequences of this strategy both more damaging and less effective. Around 95% of all workers in India are informal, with no legal protection vis-à-vis their employers and also little or no social protection. Around half of all workers are self-employed, usually in informal micro-enterprises. All such workers were immediately and sharply affected by the lockdown, which deprived them of paid employment without warning. Since many of these workers already operate at the margin of subsistence, this generated severe economic distress, made worse by the fact that there was very little public assistance to prevent growing destitution and hunger. This made them more vulnerable to all diseases, including COVID-19, worsening health conditions overall. Further, around one-third of the urban population and at least quarter of villagers live in extremely crowded and congested conditions, in very small dwellings with five or more people confined to one room. “Stay-at-home” policies in such contexts are unreasonable, oppressive and even counterproductive. Physical distancing norms cannot be effectively followed, especially for prolonged periods. Requirements like frequent and prolonged washing of hands with soap cannot be met where access to clean water is limited and it must be collected and stored, often through lengthy and arduous journeys made by women and girls. All these aspects became more difficult as the lockdown continued, as declining incomes forced many people to cut back on spending for even essential items. But there were no official guidelines for people in these conditions to protect themselves from the virus.",55
47,4,Journal of Industrial and Business Economics,28 March 2020,https://link.springer.com/article/10.1007/s40812-020-00152-z,How home and host country industrial policies affect investment location choice? The case of Chinese investments in the EU solar and wind industries,December 2020,Augusto Ninni,Ping Lv,Pengqi Liu,Male,,Unknown,Mix,,
47,4,Journal of Industrial and Business Economics,08 May 2020,https://link.springer.com/article/10.1007/s40812-020-00159-6,The rush for patents in the Fourth Industrial Revolution,December 2020,Mario Benassi,Elena Grinza,Francesco Rentocchini,Male,Female,Male,Mix,,
47,4,Journal of Industrial and Business Economics,11 August 2020,https://link.springer.com/article/10.1007/s40812-020-00175-6,Innovation and the network position of firms: the case of the Eskişehir–Bilecik–Kütahya ceramic cluster in Turkey,December 2020,Gökhan Önder,Cemil Ulukan,,Male,Male,Unknown,Male,"For the last 20 years, clusters have been a topic that has attracted the attention of researchers and policy makers. Clusters' emergence and evolutionary processes (Steinle and Schiele 2002; Brenner and Mühlig 2013; Ritvala and Kleymann 2012; Boschma and Fornahl 2011; Ellison and Glaeser 1999), types, analysis and identification methods (Alcacer and Zhao 2016; Argüelles et al. 2014; Titze et al. 2011; Delgado et al. 2015), and roles in firm innovation (Casanueva et al. 2013; Chiu 2009) in productivity and regional development (Niu et al. 2008; Ketels and Memedovic 2008; Martin et al. 2011) are the main themes in the economics and strategic management literature. Researchers and policy makers have also been conducting cluster mapping research to provide data for these studies and to identify clusters within the economic structure.Footnote 1 Clusters are intrinsically socio-regional entities that include firms operating in related and complementary sectors, research centers, public and regulatory institutions, media, and other value chain actors such as logistics service providers, consultancy firms and financial institutions. The multi-actor structure of clusters has contributed to the understanding of clusters from different perspectives, and as a result, a very fragmented literature has been created (Cruz and Teixeira 2010; Sedita et al. 2018; Hervas-Oliver 2015; Lazzeretti et al., 2019). When describing clusters, many studies emphasize the relationships, cooperation and competition among all these actors operating in specific geographic locations (Porter 1990, 2000; Wu et al. 2010; Konzelmann and Wilkinson 2017). Additionally, many other studies focus on the knowledge flows and knowledge sharing mechanisms in clusters (Van den Berg et al. 2001; Maskell and Lorenzen 2004). This paper examines clusters from a network-based view. Clusters, by nature, consist of different types of network structures that are intertwined. The benefits of clusters for firms and regions can also be considered the results of these network relations. Therefore, operating within a cluster is necessary but not sufficient to benefit from the value created by clusters. Many studies argue that a central position and a greater number of ties in the network structure increase the innovation performance of firms (Boschma and Wall 2005; Li et al. 2013; Fang et al.2017), but there are also studies suggesting that the roles of networks in innovation can only be complementary (Gordon and McCann 2003; Lin et al.2012). However, there has been little discussion about the network effects on innovation with regard to the newly emerging industrial clusters located in Turkey (Eraydin and Armatli-Köroğlu 2005; Sarvan et al. 2011). The aim of this paper is to explore the “network structures” of the Eskişehir-Bilecik-Kütahya ceramic product cluster through network graphs and to then estimate the effects of firms’ network positions on firm innovativeness. The paper proceeds as follows: the next section presents a literature review on network types and their innovation outcomes in clusters. The third section provides details about the methodology and data used in the paper. The fourth section includes brief information about the Turkish ceramic product sector and the Eskişehir-Bilecik-Kütahya (EBK) ceramic product cluster. In the following sections, we present the findings, conclusions and policy implications. The paper ends with suggestions for further research.",1
47,4,Journal of Industrial and Business Economics,19 May 2020,https://link.springer.com/article/10.1007/s40812-020-00160-z,Ownership and performance in the Italian stock exchange: the puzzle of family firms,December 2020,Laura Abrardi,Laura Rondi,,Female,Female,Unknown,Female,"This paper presents new evidence on the changes in corporate ownership and governance of Italian publicly listed firms in the XXI century. We focus on “family firms” tracking, from 2000 to 2017, their ownership structure, governance positions, accounting and market performance and CEO parental ties with the controlling shareholder. The main research question of this paper deals with the relationship between family firms’ ownership and performance, which we analyze by estimating first the determinants of family ownership, then the effect of family ownership on firm performance and, finally, by accounting for the self-selection component of the endogeneity in this relationship. By focusing on the ownership and control structures of a sample of firms within a single country that is subject to a given legal regime (i.e. a French Civil law system), we do not have to control for the potential that country-specific laws, financial institutions and cultures allow to owners for expropriation of non-controlling shareholders. Country specific factors indeed influence to a great extent both the choice of the family to retain the controlling stake, the size of this stake, and the decision to appoint a family CEO as well as the compensation policy (La Porta et al. 1999; Kumar and Zattoni 2013; Elston 2019). All firms in our sample face exactly the same investor protection laws and the same institutional and cultural environment but have nonetheless chosen to adopt very different governance structures and compensation policies. Italian economy is known for being characterized by a very large number of small and medium companies owned by individuals or, more typically, by “families” that often descend from the firm’s founder, and managed by members of the controlling family. We define family firms as those which are majority-controlled by individuals related by blood or marriage. In the construction of our data, we paid special attention in identifying the firm’s founder (many listed Italian firms are very old and were founded in the XIX century), the founder’s role in the firm or in the directors’ board (if still living), and the parental ties of the CEO with the controlling shareholder or the founder. Although family firms are relevant in other industrialized economies as well (Morck et al. 2005), the peculiarity of Italy is that family firms are also dominant in the public equity market, and that this dominance is stable and long-lasting over time (Abrardi and Rondi 2020). This feature makes Italy an excellent research case to analyze the effectiveness of corporate governance mechanisms in the protection of minority shareholders’ interests. For example, as reported by CONSOB (2018), the Italian authority supervising the stock exchange, in 2017, the average share of the largest shareholder was 47.7% while the aggregate share of the other “relevant” shareholders (i.e. those with an interest of at least 2% in the company, including institutional investors) was about 12%. Evidently, the typical ownership structure of Italian quoted firms does not favor the formation of block-holders large enough to threaten the controlling shareholder or to play a relevant role in monitoring the administration of the firm. Hence, it is unlikely that the “second largest” shareholder can play the monitoring role that is often invoked by the corporate governance literature. In the absence of a potentially effective “second largest” shareholder, institutional investors are often viewed as a monitoring corporate governance mechanism (Croci et al. 2012), provided they are “active”. In Italy, institutional investors have entered the equity market only recently (for example, pension funds are still very rare) and in a limited amount, but their role has been growing over time. Anecdotal evidence in the financial press suggests that institutional investors in Italy have sometimes acted as a disciplining mechanism. The database constructed for this research includes data on the presence (binary variable) and on the aggregate equity share of institutional investors from 2000 to 2017, thus allowing us both to track their evolution over time and their impact on firm performance. We are aware that the relationship between ownership and performance is ridden by endogeneity and self-selection problems that cannot be easily solved. In the case of quoted firms, in which the owner is understood to gradually release his initial stake (Pagano and Roell 1998) and perhaps to keep the firm’s control via alternative methods of separation (Bianco et al. 1999), it may be surprising to find that, in Italy, the largest shareholder persistently holds a stake above the legal majority of 50%. Indeed, as remarked by Himmelberg et al. (1999), both managerial ownership and performance are endogenously determined by exogenous (and only partly observed) changes in the firm's contracting environment. The notion of contracting environment is wide, encompassing the quality of the national governance system and of the firm’s internal governance, the incentives and the constraints to expropriation activities by the firm’s insiders, the responsiveness of the financial market (in a listed firm, a poor performance or apparent rent extraction should be punished and the poor performers should be ousted) and, ultimately, the motivations behind the choice of keeping a large controlling share in the family business and to appoint a family CEO rather than a professional manager. Our approach is at first explorative, by comparing family and non-family firms under several points of view, such as the ownership structure, the capital structure, the attractiveness towards institutional investors, the adoption of various control-enhancing mechanisms, the dividend policy, and the choice of the primary industry (e.g. R&D intensive, subject to foreign competition, etc.). All these features then contribute to determine the relationship between firm’s ownership, control and performance in the econometric analysis, controlling for the observable firm- and industry-specific component of cross-firm heterogeneity. We control for the remaining unobserved component by including firm fixed-effects and, finally, by accounting for self-selection of families in or out of firms with a treatment effects model. The econometric analysis contributes several findings. First, family ownership is more likely in manufacturing firms (vis-à-vis services), in less concentrated industries and in sectors where R&D intensity is low. We do not find consistent evidence of a performance premium of Italian family firms or family CEOs. Indeed, family firms appear to achieve superior profitability, but lower market to book ratios, thus suggesting that the stock market evaluates family ownership with a “discount”. The discount is higher when the family CEO is also Chair of the board. Interestingly, however, firm value appears negatively impacted when the high controlling share is not connected to family ownership so it is possible that the market discount would be lower if the family held the company with a lower share. We also find that the equity stake is significantly lower when the CEO is a member of the controlling family, suggesting a trade-off between ownership and control within family firms. The paper starts by referring our paper to the vast literature of family ownership and performance in Sect. 2. We then describe the dataset (Sect. 3), and the main features of Italian listed family and non-family firms’ ownership structure, corporate governance, and performance (Sect. 4). In Sect. 5, we investigate the determinants of family ownership and we analyze the relationship between firm performance and family ownership, controlling for the characteristics of the firm’s contracting environment. Finally, in Sect. 6, we discuss the results and propose further research avenues.",2
47,4,Journal of Industrial and Business Economics,24 January 2020,https://link.springer.com/article/10.1007/s40812-020-00145-y,TV watching in the new millennium: insights from Europe,December 2020,Maria Rosa Battaggion,Alessandro Vaglio,,Female,Male,Unknown,Mix,,
47,4,Journal of Industrial and Business Economics,04 April 2020,https://link.springer.com/article/10.1007/s40812-020-00155-w,A note on the estimation of competition-productivity nexus: a panel quantile approach,December 2020,Michael L. Polemis,,,Male,Unknown,Unknown,Male,"The effect of product market competition (PMC) on productivity dates back in the pioneering work of Sir John Hicks (1935) arguing that “the best of all monopoly profits is a quiet life”. Since then several theories have brought to light different arguments. Schumpeter (1943) claims that there is a positive linear relationship between market power and productivity appraising the ability of the monopolies to stimulate productivity and growth, while Arrow (1962) suggests that there is a positive nonmonotonic convex relationship between PMC and productivity pointing out that market power induces firms to protect their status quo, thus discourages them from engaging in developing costly disruptive technologies. Although there is a sizeable literature studying the link between market structure and productivity, no clear consensus has been reached by combining prior theoretical predictions with recent empirical results. As several studies argue (see, among others, Aghion et al. 2005; Amable et al. 2016), the leading Industrial Organization models, give rise to the “Schumpeterian effect” implying a negative impact of competition on innovation. This finding is also supported by empirical evidence (Aghion and Howitt 1992; Hashmi 2013). However, many studies give rise to the so called “escape competition effect”. The latter indicates that firms tend to increase the innovation activity when competition increases as a means to improve their profit margin (Bloom et al. 2011; Blundell et al. 1999; Carlin et al. 2004; Correa 2012; Correa and Ornaghi 2014; Gorodnichenko et al. 2010; Nickell 1996; Schmitz 2005; Van Reenen 2011). While economic theory provides mixed predictions about the effect of competition on innovation and productivity, empirical research suggests that such relationship follows an inverse U-shape (see for example Aghion et al. 2005; Van Reenen 2011; Marshall and Parra 2019). These studies apply several techniques which estimate the parameters of interest at the mean of the conditional distribution of the dependent variable. However, this is a strong simplification, since parameters are not only estimated at the mean of a distribution as the existing studies assume but also in other quantiles (median, first quantile, third quantile, etc.). This study aims to cover this lacuna. Another limitation of the existing studies is to pool potentially heterogeneous industries/firms as if their data were generated according to the same process (Distante et al. 2018). To overcome these problems, we employ the Method of Moments Quantile Regression (MM-QR) analysis developed in Machado and Silva (2019). By using this nonparametric approach, we are able to study the effect of competition at different quantiles of the productivity distribution function, while we also account for the presence of fixed effects just like OLS. However the quantile regression analysis has specific advantages over the traditional parametric estimation techniques (OLS, 2SLS, etc.). First, by using a quantile technique, we are able to recognize whether (or not) the impact of independent variables is consistent when the TFP growth rate is at higher or lower quantiles. It is argued that quantile regressions can provide more accurate estimates than the OLS estimates since they are less susceptible to the outliers when the sample variables are not normally distributed as in this case (Das and Dutta 2020).Footnote 1 As regards with the application of quantile regression analysis to panels, it is noteworthy that MM-QR seems to work better with non-normal distributions, while it has the advantage of being applicable to non-linear models and being computationally much simpler, especially in models with multiple endogenous variables. Lastly, this approach ensures that the estimated structural quantile functions do not cross (Machado and Silva 2019). The latter restriction, may result in efficiency gains and improved small-sample behaviour (Zhao 2000). Based on the above, the main novelties of the paper are the following. First, this study provides rigorous insights concerning the impact of market competition on productivity at the quantiles of the respective distribution. This issue is of great interest, though it has been nearly overlooked by the existing studies, since different empirical findings may occur by the examination of the causal effects in each quantile. This seems to be prominent in this case as a result of nonnormal errors in all the regression models. Second, the US manufacturing industry has witnessed significant turbulence in terms of productivity during the sample period. Thus, the relationship between the sample variables might vary under different microeconomic regimes stimulating a possible asymmetric impact on the TFP growth rate. By using the MM-QR approach we are able to capture these variations. Lastly, we employ a novel panel quantile approach (MM-QR), which involves a two-step procedure with an OLS regression in the first step. This procedure seems to be quite appropriate for the specific quantile regression model because it does not impose too much structure on the first step. Specifically, the relevant study uses a simple linear model, extended with the bias-correction split-panel jackknife procedure. The rest of this note proceeds as follows. Section 2 describes the data and the empirical methodology applied. Section 3 discusses the empirical results along with the necessary robustness checks, while Sect. 4 concludes.",5
47,4,Journal of Industrial and Business Economics,30 April 2020,https://link.springer.com/article/10.1007/s40812-020-00156-9,"Investments, export entry and export intensity in small manufacturing firms",December 2020,Stephen Esaku,,,Male,Unknown,Unknown,Male,"There is a growing body of theoretical literature examining the behavior of heterogeneous firms that trade. The Melitz (2003) model establishes that participating in foreign trade generates high export entry rates among the large and more productive firms while at the same time lowering the probability of entry for the less productive ones. Consequently, any additional exposure to trade for the whole industry would induce further intra and inter-firm reallocation of resources towards the most productive firms. Relative to the more productive firms, the less productive ones remain to serve the domestic markets and those that venture into the export markets find themselves unable to survive longer in these markets (Melitz 2003; Roberts and Tybout 1997). This is a clear indication that firm-level efficiency is an important element in the firm’s decision to enter and exit international markets. Following the intuitive study by Bernard and Jensen (1999), several empirical studies confirm this central thesis (see Haidar 2012; Serti and Tomasi 2008) and show that export participation leads to improvements in the firm’s efficiency level (Eliasson et al. 2012; Fernandes and Isgut 2015). The above arguments imply that firms must first acquire high efficiency levels before entry and improve it further thereafter (suggesting self-selection and learning-by-exporting patterns). Contrary to the above line of thinking, Eaton et al. (2008) using firm-level data study the behavior of Colombian exporters and suggest that the above narrative might not necessarily hold for firms in developing countries. They show that small manufacturing firms enter nearby foreign markets yearly vending small quantities of their products. A large number of them do not survive beyond one year, and those that survive grow and expand their sales to more distant foreign markets. This contradicts the much established strand of literature showing that prospective exporters possess all the desired features (in terms of productivity, size, capital intensity among others) years before they start to export. However, what then explains the observed export entry pattern of these small and less productive firms in the foreign markets as documented in Eaton et al. (2008)? How can we reconcile this observed pattern with the conventional wisdom of the Melitz’s (2003) model and numerous other empirical studies supporting the self-selection assumption? This study will try to show that there might be a missing link that could provide answers to this observed puzzle in the firm-level data. Recently, some authors have also argued for the presence of a missing link that provides a better justification for the observed empirical patterns among Colombian manufacturing firms. Several studies have examined the possible contribution of firm-level investments in research and development (R&D) to the probability of exporting (see Aw et al. 2011). As ably shown by Esteve-Pérez and Rodríguez (2012), firms that engage in R&D have high probability of exporting while simultaneously raising their likelihood of engaging in R&D. Similarly, Falk (2012) examines firms in Austria and finds that intensity in R&D creates a positive and statistically significant effect on the growth of sales and employment. Moreover, firms engaging in R&D increase sales as their products stimulate increased demand among potential customers. Correspondingly, Hölzl (2009) studies the R&D behavior of firms in 16 countries and finds that R&D is central to the growth and performance of high-growth SMEs in economies where technology is not distant. Accordingly, some studies have followed a different strand of literature on international trade that examines the effect of firm-level innovations on the success of export entry and participation. For example, Colombelli et al. (2016) note that export entry alone without innovation may not sustain the survival and performance of firms. Firms should engage in innovation, both process and product, to succeed in exporting and increase their chances of survival in the foreign markets. However, their study examines only innovative start-ups while neglecting firms that have attained maturity in the market. Relatedly, Rochina-Barrachina et al. (2008) study how process innovations affect the firm’s total factor productivity growth and note that the former increases productivity of firms. Moreover, these authors confirm that successful innovations foster the efficiency of firms over time. In the same vein, various empirical studies using firm-level data present similar conclusions (see Baffour et al. 2018; Caldera 2010; Hall et al. 2009; Lileeva and Trefler 2010; Segarra and Teruel 2014). Accordingly, Bustos (2011) demonstrates that the scaling down of trade barriers reinforces trade integration which in turn stimulates firms to upgrade their production technology (Alvarez and López 2005) and the quality of the product (Iacovone and Javorcik 2012) even before export market entry. Taken together, there is a growing body of literature examining the effect of R&D, innovation, and technology upgrading on exporting. Conversely, few studies examine the relationship between firm-level investment in physical capital and the behavior of exports. We define investments in physical capital as any firm-level investment in physical capital—equipment and machinery, designed to upgrade the technology of the firm. We view investments in physical capital as a form of technology upgrading that can help firms grow their productive capacity and attain economies of scale. Accordingly, researchers have begun to develop theoretical models to test the implications of investments in physical capital on firm-level export decisions. Rho and Rodrigue (2016) present a structural model to study how firm-level investments affect export dynamics among Indonesian manufacturing firms. They show that small firms that accumulate enough capital generate capacity that makes them productive over time, and survive longer in the export markets. Moreover, Rho and Rodrigue (2015) assert that new exporters have high rates of investments than non-exporters and suggest that the productivity differences and export behavior between non-exporters and exporters may be explained by differences in firm-level investment. On the empirical side, Peluffo (2016) studies the behavior of Uruguayan manufacturing firms to understand whether increased firm-level investment in total physical assets induces firms to sell products in foreign markets. Conclusions from this study reveal that investments induce increased levels of exports and export orientation among Uruguayan manufacturing firms. We follow the recent strand of trade literature examineing the implications of firm-level investment on the decision to export. More specifically, this study’s objective is to examine the effect of investments in physical capitalFootnote 1 on the probability of export market entry, export intensity, on one hand, and the effect of exporting experience on investment on the other, on small manufacturing firms. Our interest is to examine whether new firm-level investment in physical capital explains export entry patterns of small firms. If this is the case, we should expect to see small firms that invest in physical capital increase their probability of switching status, from non-exporter to exporter, better than firms that have not invested. We assess the following specific research questions: (1) Do new firm-level investments increase small firms’ probability of switching status—from non-exporter to exporter, thus explaining the entry patterns among small exporters? (2) Do small firms that invest increase the probability of increasing export intensity? (3) Does exporting experience increase small firms’ probability of investing in physical capital? While some studies have examined the impact of R&D investments, innovation and technology upgrading, this study differs from previous literature in four major ways: First, we examine the effect of firm-level investment on the probability of export entry among small firms. The uniqueness of our data allows us to explicitly study the effect of firm-level investment on the size class of firms. This is a gray area that has received limited treatment in the international trade literature. Second, relative to previous literature that examined the effect of investments on manufacturing firms in developed countries, we exclusively focus on Africa. This region has not received adequate studies on the subject matter. This study will help shed more light on the behavior of firms that engage in investments. Third, we make a novel policy contribution to the literature on effect of firm-level investments on export entry and export intensity. Most African countries would want to promote exports to earn the much needed foreign exchange. Our study will highlight policy implications of investments on the success of export entry and export intensity among small firms. To the best of our knowledge, this study is the first to analyze the impact of firm-level investment in capital on the decision to export, with focus on small firms in Africa. We use unique data that cover a relatively long period, from 1991–2002, ideal for studying the behavior of firms. Furthermore, our data includes key information on physical investments, employment, and data necessary for estimating total factor productivity. We implement our econometric analysis using propensity score matching estimation methods. We find complementary evidence suggesting that investment in physical capital may help small firms to initiate exporting and increase export intensity. This implies that as firms invest in capital, they upgrade their production technology that helps them cut-down on the variable costs of production. Recently, Ahn and McQuoid (2012) suggested that a number of new exporters face increasing marginal costs of production due to difficulty in accessing capital financing when they start to sell in foreign markets. Since finance is a binding constraint, our evidence proves that firms that invest are likely to access export markets compared to non-investing firms. Consistent with the findings of Hall et al. (2009), our results seem to suggest that new exporters may face stiff competition that requires them to invest in some form of technology upgrade to circumvent the competition. The rest of the paper is structured as follows; Sect. 2 presents the literature review. Section 3 describes the data used to conduct empirical exercises, while Sect. 4 presents the methodology. Section 5 reports and discusses the results, while Sect. 6 concludes.",7
48,1,Journal of Industrial and Business Economics,19 February 2021,https://link.springer.com/article/10.1007/s40812-020-00181-8,Introduction to the special issue: green economy and environmental policies in oligopoly markets,March 2021,Jean J. Gabszewicz,Ornella Tarola,,Male,Female,Unknown,Mix,,
48,1,Journal of Industrial and Business Economics,28 April 2020,https://link.springer.com/article/10.1007/s40812-020-00158-7,Climate change and the financial system: a note,March 2021,Anastasios Xepapadeas,,,Male,Unknown,Unknown,Male,"There is an extensive and well-documented body of scientific evidence suggesting that global warming is the result of human activities associated with the use of fossil fuels and the emissions of carbon dioxide and other greenhouse gases (GHGs). Although there are many uncertainties, the scientific consensus is that a business-as-usual scenario might have serious negative impacts on human wellbeing (see, for example, Nordhaus 2007; Stern 2008). Some potential impacts could be irreversible and accelerate the process of global warming, such as the melting of permafrost, which could release huge quantities of methane, or the loss of sea-ice in the Arctic, which will reduce earth’s albedo. Such feedbacks could lead to global warming much greater than current projections, resulting in temperatures higher than any in the past 50 million years. Under business as usual, over the next two centuries we are likely to see climate changing at a very fast rate and on a scale that the world has not experienced in recent history. Science provides indications that the probability and frequency of floods, storms, droughts and similar natural phenomena is likely to continue to grow with cumulative emissions of GHGs, and that the magnitude of some of these impacts could be irreversible and/or catastrophic. Furthermore, following the most recent IPCC report (IPCC 2018), under the business as usual scenario the change in the global average surface temperature relative to the preindustrial period—the so-called temperature anomaly—is expected to exceed the threshold of 1.5 °C around 2040. This implies that serious impacts and economic damages associated with climate change are expected to emerge in the near future. This scenario is not unlikely since carbon emissions increased during 2018 by more than 2%, reversing the slowing trend of emissions since 2010. In this context, the objective of climate change economics is to use climate science and the projected evolution of climate under the impact of anthropogenic GHG emissions in order to design economic policies which will prevent or minimize undesirable events. Economic theory considers global warming and the resulting climate change an externality. Externalities and market failure are among the most fundamental concepts that have long been associated with environmental and resource economics A classic definition, influenced by Kenneth Arrow and James Meade, is provided by Heller and Starrett (1976, p. 10), who define an externality as “a situation in which the private economy lacks sufficient incentives to create a potential market in some good and the nonexistence of this market results in losses of Pareto efficiency”. As is well known, when externalities are present, the competitive equilibrium is not Pareto optimal. A market failure takes place and policy intervention in the form of regulation is required in order to correct the externality. Climate change represents the greatest and widest-ranging market failure ever seen. The main characteristics of the climate change externality can be summarized as follows: It is global in its impacts. GHG emissions generated in a certain location have impacts which are spread across the entire planet with different geographical intensities. Reducing emissions is an extreme global public good. All nations share the benefits from reduced emissions, while the nations that reduce emissions bear the cost of reduction. This generates free riding incentives which may impede nations from reducing individual national emissions. Some of the effects are very long term and governed by nonlinear dynamics with positive nonlinear feedbacks. There is a great deal of uncertainty both in terms of scientific mechanisms and economic impacts. The effects are potentially very large, and many may be irreversible. The standard economic theory of externalities suggests that the resulting market failure can be corrected using policy instruments which mainly include Pigouvian taxes, or allocation of property rights through some kind of bargaining (the Coasian approach). In the case of climate change, although economic policy design follows these basic lines, it must take into account a very large number of economic considerations such as: estimating damages from climate change; dealing with deep uncertainty both in terms of climate science and economics; characterizing the impacts of climate change and of climate change policies on growth; formulating global policies in the absence of a supranational authority and under free-riding incentives; and addressing intragenerational and intergenerational distribution, which raises important ethical issues between rich and poor nations and between present and future generations. Economic policy for climate change, under the constraints imposed by the economic considerations described above, has been formulated in terms of carbon taxes or cap-and-trade policies (e.g., Stern 2007, chapter 14; Golosov et al. 2014). Climate change policy has therefore been predominantly fiscal policy with the main focus being on impacts on the real economy. Not much attention has been paid until relatively recently to its impacts on the financial system and the risks involved (e.g., Campiglio et al. 2018), or the implications of climate change for the conduct of monetary policy and the role of Central Banks, given that the horizon over which climate change impacts the economy has shortened (see, e.g., Couré 2018) and that the very likely impact of climate change on growth and future output paths might require more involvement of monetary policy. The purpose of this note is to briefly present the financial risks associated with climate change and the ongoing research in this area.",2
48,1,Journal of Industrial and Business Economics,02 April 2020,https://link.springer.com/article/10.1007/s40812-020-00153-y,On the long run sustainability of small jurisdictions,March 2021,Skerdilajda Zanaj,Patrice Pieretti,Benteng Zou,Unknown,Male,Unknown,Male,"It is well-known that small economies suffer from very limited capital and labor resources both in amount and in variety. Their small home market size prevents them from exploiting scale and scope economies. It is therefore not surprising that small states need to be open to international trade and capital flows (Armstrong et al. 1998). Because domestic capital is relatively scarce in small economies, attracting foreign investments is an important way to fill this gap. Indeed, it has been well documented that small states or jurisdictions tend to get more private foreign capital as a ratio of total capital formation (Streeten 1993). As shown in Han et al. (2014), the ratio of FDI flows to the gross fixed capital formation is much higher in small countries (i.e., population less than two million) than in large countries (i.e., population in excess of 30 million).Footnote 1 In addition, small countries are also more vulnerable than other countries to climate variability and natural disasters (e.g., Briguglio 1995). Accordingly, openness to trade and foreign investment must come along a path of sustainable development which is key for their long run survival. Sustainable development for small countries in this paper is intended as “development which meets the needs of the present without compromising the ability of future generations to meet their own needs” from the “Brundtland Report” (1987) [see for instance Holden et al. 2014]. Indeed, the focus of this paper is to analyze the long run economic sustainability of a small country in a globalized world. In recent years, attracting foreign capital to invest in eco business has become a trend. For instance, it is documented that more 200 eco businesses are located in Luxembourg mainly operating in the fields of renewable energy, waste management, water and environmentally sound construction. In order to support these companies in their efforts, the government offers financial support, and it also helps establishing networks of expertise to guarantee ultimately their country sustainability. Over 99% of electricity production and almost 80% of total energy production in Iceland comes from hydropower and geothermal power making meeting buildings quite naturally eco-friendly. There are many other examples: the growing use of wind farms, the introduction of energy performance certificates in the building sector as well as the efforts implemented for more electric public transport. The Icelandic government has stated an objective of making Iceland the first nation to use only renewable energy for its power in the near future according to the Icelandic National Strategy for sustainable development. Our purpose is to explore the optimal policy mix in terms of public investment and taxation for the long run feasibility of a small jurisdictions. In this paper, the term jurisdiction has to be used in a general sense. It can be a country, a local government within a federation, a region or a city having enough power to tax and to decide on local infrastructure.Footnote 2 Thus, we assume that the policy-makers of the small jurisdiction use a mix of two instruments in a dynamic optimization framework to promote the durability of their economy. More exactly, the policy-mix consists in attracting foreign capital through low taxes and/or high level of public goods, which enhance firms’ productivity. Public goods can cover a wide range of infrastructures, services and environmental regulations provided by the local government that are attractive to firms by increasing their productivity. These public goods can be intended as infrastructure needed for adaptation to climate change or mitigation of climate change. Think, for instance, of alert mechanisms to natural disasters, or projects like artificial barriers (e.g., the MOSE in Venice); plants for the treatment of waters (desalination). Consequently, capital locates according to differentials in offered public good levels and tax differentials. Hence, a jurisdiction may not reduce its attractiveness by a unilateral increase in taxes if foreign investors are compensated by more infrastructure provision. Foreign investors are ready to pay higher taxes because infrastructure become more valuable to them (e.g., Haufler 1997; Pieretti and Zanaj 2011). In this research, the small jurisdiction is small enough to consider the rest of the world’s choices as exogenously given. This does not mean that small jurisdictions cannot grow in terms of productive capacity by attracting a high volume of foreign direct investments even if its population size remains exiguous. What we actually assume is that their ability to grow bigger than large jurisdiction is limited.Footnote 3 For example, if smallness does not only rely on population size but also on a territorial criterion, the existence of limited usable land can be considered as an absorptive constraint for foreign investments.Footnote 4 Another limitation can result from the smallness of the native population since it constraints the availability of administrative and public resources which are necessary to increase infrastructure provision. Finally, a high ratio of foreign-owned firms may be perceived as a loss of economic independence and induce some resistance to new FDIs. The literature has investigated the role of jurisdictions’ size on their capacity to attract capital. Recent papers show that small economies can be attractive not only for tax reasons but also for their provision of public infrastructures (Justman et al. 2005; Zissimos and Wooders 2008; Hindriks et al. 2008; Pieretti and Zanaj 2011). This paper extends this literature by modelling the dynamics of a small economy’s strategies to attract foreign investments. More precisely, we study a small jurisdiction’s dynamic choice of taxes that are used to afford public goods that enhance firms’ productivity. Applying the Pontryagin’s maximum principle (see, for example, Boucekkine et al. 2007), we characterize the potential steady states attainable by the small economy. The dynamic interactions among jurisdictions to attract mobile factors have already been analyzed within a repeated game framework. The main issue studied by this literature is the tax coordination problem between symmetric regions (Cardarelli et al. 2002; Catenaro and Vidal 2006; Itaya et al. 2008). However, the purpose of this paper is not to model a game between jurisdictions. We rather focus on the long run decisions of a small open economy facing exogenously given tax and infrastructure choices of the rest of the world. The world is thus divided into two unequal sized regions where size refers to the magnitude of the population, which coincides with the number of capital-owners who are simultaneously entrepreneurs and workers. In our paper, we consider that only the economic size of jurisdictions can vary endogenously as a consequence of public policy, but that their territorial borders remain unchanged. Finally, our paper is related to literature on (uneven) emissions taxation of polluting firms and their corresponding relocation choices (for instance, Exbrayat et al. 2016; Sanna-Randaccio and Sestini 2012, Sanna-Randaccio et al. 2017). In particular, Sanna-Randaccio et al. (2017) tackle the so-called “reverse market asymmetry” scenario, i.e. when the small country enacts a tougher environmental policy as compared with the rest of the world, it becomes extremely likely that firms relocate abroad their productive activities. This scenario could be one of the drivers of the long run economic collapse examined in the present paper. In order to avoid such a collapse, the small jurisdiction may resort to a policy mix allowing for R&D investment subsides, public R&D investment or other public infrastructure. In other words, relying on these public infrastructure may represent a powerful centripetal force, leading the small country to economic survival. The results of the paper can be summarized as follows. First, we demonstrate that there exists one long run efficient steady state that guarantees the long run existence of the small jurisdiction. Then, we address the question of convergence to this efficient state. When capital mobility is high enough, we demonstrate that the small economy can ensure convergence toward long run efficiency by allocating an adequate share of taxes to infrastructure expenditures. This share can be chosen in a non empty set of possible values. In case of perfect capital mobility we show, as a corollary, that the efficient steady state is always attainable. Consequently, the economic fate of small jurisdictions is in their own hands when capital mobility is not too low. Then paradoxically, there is room for efficient economic policy even if small jurisdictions are highly depending on forces outside their control. When capital mobility is too low, there may exist no way to converge to long run efficiency and economic collapse can be a possible end-state. Along this divergence path, the small economy would vainly try to retain capital by tax dumping and thus drive its infrastructure expenditures to zero. Nevertheless, this scenario does not occur if the public benefit of infrastructures net of taxes provided by the rest of the world is negative. The paper is organized as follows. The next section develops a dynamic model and the optimal conditions by applying the Pontryagin’s Maximum Principle. Section 3 derives and analyses the steady states of the model. Finally, Sect. 4 concludes.",
48,1,Journal of Industrial and Business Economics,07 March 2020,https://link.springer.com/article/10.1007/s40812-020-00149-8,Environmental taxation: Pigouvian or Leviathan?,March 2021,Isabelle Cadoret,Emma Galli,Fabio Padovano,Female,Female,Male,Mix,,
48,1,Journal of Industrial and Business Economics,03 October 2020,https://link.springer.com/article/10.1007/s40812-020-00157-8,Addressing the concerns about carbon leakage in the implementation of carbon pricing policies: a focus on the issue of competitiveness,March 2021,Florian Rey,Thierry Madiès,,Male,Male,Unknown,Male,"In the recent years, the urgency to reduce carbon emissions in order to tackle climate change has been acknowledged by numerous scientific reports published by the Intergovernmental Panel on Climate Change (IPCC). This scientific consensus has led to growing public concerns about environmental issues, which in fine was reflected in many public elections. The federal elections held in October 2019 in Switzerland testified of this trend: Out of the 200 seats at the National Council, Greens gathered 28 seats (+17), becoming the fourth largest represented political party in this chamber. Reducing carbon emissions will require effective and efficient policy instruments. Among all available policy instruments, economic instruments (carbon pricing, which includes both carbon taxes and cap-and-trade mechanisms) have been recognized as the most cost-efficient tool to reduce carbon emissions (Perman et al. 2003, pp. 202–238). Despite a “near-universal agreement among economists” that carbon pricing is needed to lower carbon emissions at a reasonable cost (Parry et al. 2015, p. xxv), most countries remain reluctant to apply carbon-pricing policies. When looking at the findings from Métivier et al. (2017), it appears that only 25% of the world’s greenhouse gazes are covered by a carbon pricing mechanism. More alarming, more than 75% of emissions regulated by carbon pricing are covered by a price below ten USD. It is thus well below the price range of 40–80 USD recommended by Stiglitz and Stern (2017) in order to stay under the two degrees variation. This perfectly shows that only a few ambitious carbon pricing schemes have been implemented so far. At a time when public awareness towards climate change has grown substantially, as well as scientific evidence of its enormous costs for the mankind, it is important to understand why policy makers are still reluctant to apply strong carbon pricing policies. Since public opinion has been identified as a “key element of policy changes in democratic countries” (Drews and Van den Bergh 2015, p. 856), it therefore follows that citizens’ concerns regarding carbon taxes should also be investigated. One of the main reasons behind this slow implementation is the concern that in a world where carbon prices are applied unevenly across countries, the reduction of carbon emissions achieved in abating countries may be partly offset by an increase of emissions in non-abating countries. This issue is called carbon leakage (CL) and can be defined as “the increase in emissions in the rest of the world when a country or a region implements a climate policy, compared to a situation where no policy is implemented” (Branger and Quirion 2014a, p. 54). It can be measured as the following ratio (Barker et al. 2007, p. 6284): CL = − ΔCO2 N/ΔCO2 M with: CO2 N being the level of emissions in non-mitigating countries, and CO2 M being the level of emissions in mitigating countries. Therefore, a CL ratio of 20% would mean that 20% of the mitigation of CO2 emissions achieved in mitigating countries is undermined by an increase of emissions in non-abating countries. It is important to note that any ratio under 100% still means that the policy has globally been able to reduce CO2 emissions. We will see that asymmetric carbon prices can lead to carbon leakage through three channels: an effect on the global energy market, a competitiveness channel and technological spillover effects. We have chosen to center this article on the competitiveness channel because it seems to play a large role on people’s acceptability for carbon pricing policies. Recently, a growing part of the literature has focused on understanding which reasons affect people’s acceptability for carbon pricing policies. Carattini et al. (2017) provide an updated review of literature on the aversion of people regarding carbon tax. One of the strongest explanation which they have found is the fear for negative effects on competitiveness and employment. Stiglitz also perfectly emphasizes the importance of the question of the competitiveness channel (see definition further below) in terms of public acceptability: “Even if the quantitative effects (of carbon leakage) are limited, the political consequences of plants and jobs moving to another jurisdiction because of its lower carbon price can be significant, and undermine support for strong carbon policies” (Stiglitz and Stern 2017, p. 23). This article investigates whether those concerns for carbon leakage are justified or not, based on economic theory and empirical results. The paper is organized as follows: We first describe the mechanisms through which carbon leakage takes place across countries. Then, we present empirical evidence of the importance of carbon leakage across countries, focusing on the competitiveness channel. Finally, we discuss which policy instruments could be used by policy-makers in order to tackle carbon leakage.",4
48,1,Journal of Industrial and Business Economics,24 February 2020,https://link.springer.com/article/10.1007/s40812-020-00148-9,Transportation costs of vehicle recycling under Hotelling’s Duopoly competition,March 2021,Eiji B. Hosoda,Masashi Yamamoto,,Male,Male,Unknown,Male,"Since Hotelling’s seminal work, many papers have addressed location decision making or price competition between spatially separate firms. Studies on transactions between discontinuous markets, however, are uncommon, even among scholars who are interested in location theory. Here, we define two markets as discontinuous or remote if there is no customer area between them. A typical example is a transaction including an island or small village surrounded by mountains. In this paper, we explore the features of competition for an end-of-life product between remote markets. During our exploration, we employ Hotelling’s model instead of the famous Salop model because our aim is to analyze spatial competition strategy, not to derive the optimal number of firms engaged in spatial competition. We believe that Hotelling’s model is better suited for our purpose. Source: Japan Automobile Recycling Promotion Center (the same data are summarized in Table 1 below.) The relationship between price and distance in the transaction of end-of-life vehicles from islands in 2008. This study was originally inspired by a real-world data set summarized in Fig. 1. The data describe the relationship between price and distance when an end-of-life vehicle (hereafter, an ELV) was shipped from a small island to the Japanese mainland in 2008. At first glance, it seems that there is no specific relationship between the shipment distance and price in the figure, which would be counterintuitive. In fact, it is common in the literature on spatial location theory to specify a transportation cost function with an increasing marginal cost. However, no such relation appears in the figure. The figure is unlikely to fit the usual assumption regarding the transportation cost function. These data differ from usual shipments in that they cover an end-of-life product that is shipped from small islands. Thus, the aim of this paper is to predict whether these distinctions affect rational firm behavior in market competition. To thoroughly understand the following analysis, we summarize the Japanese automobile market and background on policy regarding ELV recycling in Japan. It is well known that Japan has several automobile manufacturers that export worldwide, and this industry has long been considered one of the pillars of the Japanese economy. In fact, Japan’s automobile stock is one of the largest in the world (behind only the US and China). The left panel in Fig. 2 displays ownership data for motor vehicles in Japan. The size of the Japanese private automobile fleet has been fairly stable at approximately 80 million vehicles over the last decade. The right panel indicates the changes in the number of motor vehicles sold each year in Japan. There was an obvious decline during the financial crisis of 2008, but sales appear to have rebounded in recent years. Source: Automobile Inspection and Registration Information Association (https://www.airia.or.jp/publish/statistics/number.html) for automobile ownership and Japan Automobile Recycling Promotion Center (https://www.jarc.or.jp/) for the generation of ELVs The number of vehicles owned (left) and the number of new vehicle registrations/end-of-life vehicles (right). The second (red) line in the right panel depicts the changes in the number of ELVs generated in each year. The number of ELVs is also stable except in 2011, when Japan experienced a large earthquake. As a result, many people who lost automobiles because of the massive Tsunami demanded used replacements as soon as possible. This demand is argued to have reduced the generation of ELVs in 2011. Most new auto sales represent the demand for replacement vehicles by existing vehicle owners. The difference between new vehicle sales and ELVs in Fig. 2 is explained by the steady export of used vehicles from Japan, which equals approximately 1 million units per year. Because most of an automobile’s weight is provided by useful metals, mainly iron, approximately 80% of automobiles are recycled without any legal requirement. However, the remaining 20%, called ‘shredder residue,’ may contain toxic substances and has often been found at illegal dumping sites. In other words, 80% of an ELV has a positive value at the time of market transaction. This 80% has a long and successful history of recycling through market mechanisms as long as the price of iron remains high enough that a recycler can afford to dispose of the remaining 20%. When the price of iron declines, the illegal dumping of ELVs increases. During the late 1990s, the price of iron was very low, and the increase in illegal disposal of ELVs attracted social attention. To address this problem, the ELV Recycling Law was partially enacted in 2002. The full-fledged introduction of the ELV Recycling Law was completed in January 2005. The law defines the roles of automobile manufacturers, importers of foreign vehicles, recycling firms and consumers in facilitating vehicle recycling. Automobile owners must pay a fee in advance for recycling vehicles. Vehicle manufacturers use this money to properly dispose of the negatively valued 20% of ELVs, namely, CFCs (chlorofluorocarbons), airbags, and shredder residue. The law limits auto manufacturers’ responsibilities for these three items because other auto parts are traditionally recycled through market mechanisms, as mentioned above. Source: Data provided by the Japan Automobile Recycling Promotion Center The number of illegally dumped vehicles per year. The implementation of this law was quite smooth because the vehicle inspection was already mandated for all motor vehicles in Japan every 2 years. The authorities could assume that even the owners of vehicles sold before 2002 would pay the recycling fee within 2 years (at the first vehicle inspection after the passage of the law). After the implementation of the law, the illegal dumping of ELVs declined dramatically. Figure 3 presents the data for the number of illegally dumped vehicles each year. Before the full implementation of the ELV Recycling Law, over 210 thousand vehicles were illegally dumped in Japan in 2003. By 2018, this figure had decreased by 97.8%. For the case on islands in 2018, there are only 323 illegally dumped cars, which is negligible compared to the 5.27 million new vehicles sold in 2018. Note further that a fund based on the recycling fees collected through the ELV Recycling Law is used to finance the cleanup or proper treatment of illegally dumped ELVs. The use of this fund has led to a sharp decline in illegal dumping. Although we must admit that there could be considerable illegal dumping that we are not aware of, it is true that the enforcement of this law has virtually eliminated illegal dumping. In the next section, we briefly review the relevant literature. We present our model in Sect. 3 and provide some theoretical results. Section 4 provides an empirical analysis and compares our results to the theoretical outcomes. We cite some policy implications in Sect. 5 to conclude our analysis.",
48,1,Journal of Industrial and Business Economics,06 April 2020,https://link.springer.com/article/10.1007/s40812-020-00154-x,Green monopoly and downward leapfrogging,March 2021,Luca Lambertini,Andrea Mantovani,Cecilia Vergari,Male,Female,Female,Mix,,
48,1,Journal of Industrial and Business Economics,03 March 2020,https://link.springer.com/article/10.1007/s40812-020-00150-1,To buy or to do it yourself? Pollution policy and environmental goods in developing countries,March 2021,Roberta Sestini,Donatella Pugliese,,Female,Female,Unknown,Female,"Rising pollution in the developing world is undoubtedly a major concern nowadays: China became the largest greenhouse gas emitter in 2005 and still remains in this position, followed by the United States and the European Union, while Brazil and India rank fifth and eighth biggest polluters, respectively (Outlook on the Global Agenda 2015, World Economic Forum). Yet, the “bottom up” approach, implemented through goals defined at the national level, that emerged during the negotiations leading to the Paris 2015 Climate Conference (COP 21), implies that climate policy will remain sub-global and uneven in the near future. In Paris agreement it was explicitly recognized that developed countries should have a leading role in reducing their domestic emissions, and that some degree of flexibility and technological and financial support should be guaranteed to developing countries. Helping these countries to cope with the impact of increasing greenhouse emissions and climate change is thus a key issue which also intersects with multiple international initiatives aimed at liberalizing trade for the so-called environmental goods (EGs).Footnote 1 In particular, the EU and other members of the World Trade Organization (WTO) aim at boosting international trade in these products and services that directly contribute to environmental protection by liberalising trade in EGs through negotiating an Environmental Goods Agreement (EGA).Footnote 2 Reducing barriers to trade in environmental goods and services has been on the global agenda since the launch of the WTO Doha Round: the rationale is that a successful outcome would create a double win, for trade and for the environment. This is because “the lower prices for abatement goods resulting from liberalizing trade in the sector will enhance environmental protection worldwide and benefit developing as well as developed countries” (Sinclair-Desgagnè 2008). However, the gain from trade liberalization in EGs accruing to developing countries is far from established. For instance De Melo and Solleder (2018) argue that current negotiations involve mainly high-income countries, with the exception of China and Costa Rica, and show that, in order to have real benefits, an increase in regulatory convergence and in the number of participants to the agreement would be required. In a similar vein, Zugravu-Soilita (2017) empirically assesses the total effects of trade flows in environmental goods, finding that “negative, indirect technique effects do not compensate the positive, direct scale-composition effects in the EGs’ net importing countries, with the total effect on pollution being harmful”. During the last two decades, due to the increasing importance of the EG sector, a growing body of theoretical literature on the relationship between environmental policy and the market for abatement goods and services has appeared. The main upshot of these studies is to analyse the consequences of imperfect competitionFootnote 3 in this market for second-best emission taxes. The issue was first tackled by David and Sinclair-Desgagnè (2005), proving that the optimal pollution tax must depart from the Pigouvian rule and be set above the marginal social cost of damage, in order to compensate for the lower level of abatement induced by higher prices of EGs. Several extensions were developed (see Canton et al. 2008; David and Sinclair-Desgagnè 2010; Perino 2010; David et al. 2011; Canton et al. 2012, among the others) mainly under the assumption of a closed economy. Framing the issue in an open economy setting, Feess and Muehlheusser (2002) first integrate the eco-industry into the theory of strategic environmental policy and challenge the conventional wisdom on ecological dumping showing that tighter environmental regulation may benefit the country where the eco-industry is located.Footnote 4 More closely related to our paper, Nimubona (2012) considers the effects of trade liberalization—leading to an exogenous reduction of EG-import tariffs—on a developing country that imports all its consumption of EGs from a monopolistic eco-industry located in a developed country. The key question addressed in this study is how lower barriers to trade in EG affect the quality of the environment in developing countries. The answer is not unequivocal, as, notwithstanding the fall in EG prices, the regulator may strategically respond by setting laxer pollution taxes with a worsening in pollution. Other related papers are Canton (2007) and Dijkstra and Mathew (2016). In the former study, in a set-up where abatement goods are supplied in two countries (say North and South) characterized by different abilities to produce them and under perfect competition among polluting firms, the role of trade liberalization in reducing pollution is questioned. The latter work considers one domestic downstream polluting firm and two upstream firms (one domestic and one foreign) and examines the impact of liberalization on the domestic upstream firm’s R&D incentive, reaching ambiguous results. Most attention, however, was devoted to revisiting the Pigouvian tax rule taking into account both the market power of the eco-industry and international rent-shifting intents. Notably, both Canton (2007) and Nimubona (2012) conclude that, in the presence of an international eco-industry, EG-importing countries are led to set lower emission taxes—with respect to a closed-economy scenario—so to shift some rent from foreign EG suppliers. The aim of our paper is to contribute to the above recalled debate. In particular, our aim is to answer to a complementary question with respect to Nimubona (2012), namely we wonder whether it is beneficial for a developing country, both for the environment and for the whole society, to fully rely on EGs produced abroad. To this end, we build a two-country model (a developed and a developing country) with two (heterogenous) polluting firms producing and selling in the developing country and competing à la Cournot. Given an environmental tax set in the developing country, the two firms may import all their consumption of the EG from a monopolistic innovator (firm M) located in the developed country. The licensed eco-technology enables firms to reduce pollution and thus expenditure on emissions tax. The less efficient firm, if not licensed, will continue to emit pollution. The more efficient firm, further to buying the EG from the foreign innovator, has the capability to engage in abatement effort. The foreign innovator sells its pollution abatement goods through a fixed-fee licensing contract and operates with zero production costs. Climate policy enacted in the developing country is assumed to be exogenous; as recognized in the policy debate, the tax rate on GHG emissions is moderate. The problem is structured as a three-stage game: at the first stage the foreign innovator announces the number of available licenses. Then the two polluting firms decide whether or not to purchase the license, or—for firm 1—to engage in abatement effort. Finally, at the third stage, firm 1 and firm 2 simultaneously choose their output and abatement levels. Our paper borrows in large part from Kim and Lee (2016), as to modelling choices. Their set-up, however, does not tackle the possibility for firms to produce in-house their abatement goods and services, is mostly framed in terms of a closed economy, and is aimed at comparing two different types of licensing contracts, fixed-fee versus auction licensing. Besides, we adopt many of the hypotheses in Nimubona (2012) about the presence of an eco-industry owned and located in the North and selling EGs in both countries markets, being these markets segmented. Also, we share with this study the attention devoted to the consequences on environmental quality in the South of easing the access to EGs produced abroad. However Nimubona (2012) does not allow for local firms engaging in abatement effort, and assumes that the consumption good is supplied in a perfectly competitive market. The novelty of our approach is clearly acknowledged by some of the most influential scholars in this strand of literature.Footnote 5 We argue that our set-up captures some relevant features of EG supply and of their consumption in developing countries. First, there is evidence of an increasing concentration in the environmental industry, pursued also by means of mergers and acquisitions.Footnote 6 Second, due also to multiple initiatives to reduce tariffs, the demand for EGs is rapidly expanding in developing countries, whilst the domestic sector is still immature.Footnote 7 Finally, as explicitly recognized in Zhang (2011), developing countries, in the face of trade liberalization, are taking different courses: some of them—for instance South Africa—are reducing tariffs to import abatement technologies at lower cost, whilst others—e.g. India, China and Ukraine—are imposing high tariffs or local content requirements to develop local productive capacities. This justifies our attention to “mixed” configurations, where some firms become licensees while others start to develop abatement technologies by their own. We find that, provided the cost asymmetry is not very pronounced and under moderate climate policy, the “mixed” configuration with one firm (the more efficient one) engaging in environmental innovation and the rival firm obtaining the license (henceforth E, L) represents an equilibrium for the developing country for a wide set of parameters values. As to the impact of climate policy enacted in the developing country on environmental quality, it is shown that a marginal increase in the tax rate may trigger a regime switching—from the L, N to the E, L equilibrium—accompanied by a fall in total emissions. Thus a tougher climate policy may become the key driver for inducing the more efficient firm to engage in production of the abatement technology, being also effective in terms of emissions reduction. Finally, the effects on aggregate welfare depend heavily on firms’ heterogeneity: only if the cost asymmetry between polluting firms in the developing country is low enough the transition to the E, L equilibrium would succeed in making the society better off. Thus our study does not support global and uniform trade liberalisation for EGs. Rather it recognizes that making cleaner technologies developed abroad more easily available might be beneficial for developing countries insofar as this spurs the adoption of tighter emissions policies and the transition to an equilibrium where the more efficient firms engage in environmental innovation. The paper unfolds as follows. Section 2 presents the model and analyses the optimal licensing strategy. Section 3 explores the effectiveness of emissions taxation under the different equilibrium configurations. Section 4 deals with some welfare implications. In Sect. 5 we discuss an extension of the model. Finally, Sect. 6 draws some conclusions.",2
48,2,Journal of Industrial and Business Economics,24 December 2020,https://link.springer.com/article/10.1007/s40812-020-00178-3,"Productive integration, economic recession and employment in Europe: an assessment based on vertically integrated sectors",June 2021,Davide Villani,Marta Fana,,Male,Female,Unknown,Mix,,
48,2,Journal of Industrial and Business Economics,13 October 2020,https://link.springer.com/article/10.1007/s40812-020-00176-5,Which firms survive in a crisis? Investigating Gibrat’s Law in Greece 2001–2014,June 2021,Christos Axioglou,Nicos Christodoulakis,,Male,Unknown,Unknown,Male,"Investigating the impact exerted by the size of firms on their turnover growth over the business cycle is of particular importance both for understanding how aggregate fluctuations propagate top-down as well as in formulating an appropriate policy response at the corporate or the Government level. If the size of firms is exerting a positive effect on turnover growth, they may opt for corporate consolidations during an upswing; in the downturn, the same company will have to downsize, or else losses may be fatal for its survival. If the size-effect on growth is negative, a more dispersed business model will emerge as the economy takes-off. However, if the downturn is deepening, as was the case with the recent global crisis, a larger company stands a better chance to survive the shrinking turnover. The size-effect on turnover growth has been vastly studied in the economic and business literature after it was conceptualized by Gibrat (1931) into the Law of proportionate effect, according to which a company’s prospects of expansion in the market are independent of its size. Ever since, a plethora of empirical studies investigated the validity and implications of the so-called Gibrat’s Law in various economies and under different circumstances, showing that they are far from being uniform or uncontested across countries, sectors and different epochs. The literature is too vast to be reviewed here: a good source of comparative analysis can be found in the extensive surveys by Audretsch et al. (2004) and Santarelli et al. (2006), and for a more recent crop in the reviews by Teruel-Carrizosa (2010), Donati (2015) or Balthrop (2020), among several others. It should, therefore, be of no surprise that the more specific effect of aggregate fluctuations on firms' growth and survival remains far from being straightforward in the existing literature. For example, Peters et al. (2014) reviews evidence supporting that firms’ productivity growth is mainly driven by their level of innovation, a process which is largely pro-cyclical and over which small and large firms show specific advantages and disadvantages. Ruiz-Fuensanda and Bellandi (2019) report a link between the dynamics of entrepreneurship and economic cycles. Fort et al. (2013) and Haltiwanger et al. (2013) report, in turn, a high degree of cyclicality in the dynamics of firms by their relative size. Sectoral differences in the creation of firms’ employment over the business cycle, due to, for example, different intensity in technology, has also been documented by Rincor-Aznar et al. (2009). Another strand of literature, reviewed by Kudlyak and Sanchez (2016), reports evidence on the business cycle propagation by small and large firms. On the other hand, Boeri and Bellman (1995) found that, in the German economy, fluctuations do not seriously affect the patterns of turnover-growth and firms’ survival. Higson et al. (2004) showed that the effect might be asymmetric, with those at the tails (center) of the cross-sectional growth distribution to be the least (mostly) affected by the cycle. A recent body of empirical literature has shed light on this asymmetry, by pointing to the decisive role of innovation in R&D on firms’ potential in a recession (Spescha and Woerter 2018). The negative effect of private U.S. firms’ deleveraging during the Great Recession is established by Dinlersoz et al. (2019), while the diminishing impact of firms’ diversification strategies in Spain over the same period is examined by Lopez-Zapata et al. (2019). The present paper investigates the relationship between a company’s size, its turnover-growth and the survival in the market during different phases of the business cycle. Each of these aspects broadens our understanding of how corporate dynamics are shaped by aggregate fluctuations, as compared to the more limited view offered by the standard approach. For example, the establishment of Gibrat’s Law in the upturn of a cycle implies that market prospects open-up to all firms, no matter their size, thus not much altering the degree of concentration in the economy. In the downturn, a rejection of Gibrat’s Law points to which firms are more likely to stay in the market or go bust after a crisis. By employing a large dataset on the corporate sector in Greece over the period 2001–2014, our aim is threefold: First, to better understand the consequences of a crisis on the very existence of business firms, not just the fluctuations in turnover. This is made possible by extending the standard framework of Gibrat’s Law into assessing the effect of firms’ size on the probability of survival in the market. Second, to distinguish the size-effect on turnover growth between the upswing and downswing phases of the business cycle, thus offering useful insights on how decisions on expansion or consolidation for the future are taken. Greece is a characteristic example of a particularly strong economic cycle, as activity first peaked in the period following the establishment of the Economic and Monetary Union (EMU) in the early 2000s, and then sunk into a recession by the global crisis. In the subsequent debt crisis of 2010, several firms went bust at a massive scale. Third, to provide extensive empirical evidence of the impact of recession across all major sectors of economic activity, including Manufacturing, Trade, Construction, and others. In this way, it is possible to observe sector-specific effects such as the market concentration rate (Dosi and Nelson 2010), or the number of young firms in each industry (Daunfeldt and Elert 2013). At the same time, it maintains comparability with previous sector-specific empirical analyses, as those by (Audretsch et al. 2004; Canarella and Miller 2018; Balthrop 2020). Our study first investigates the co-movements of the population of firms vis-à-vis Greece's GDP over both phases of the cycle and then analyses their prospects of survival in the market. During 2001–2008, Greek GDP on average was growing by 3.5% per year with new firms entering the market, and then activity dwindled by − 22% in 2009–2014 followed by several firms going bust and exiting the market. The contraction in economic activity in Greece initially was mild, but with public debt and deficits going out of control, the country soon was denied access to financial markets; for a detailed analysis of the background and the ramifications of the Greek crisis see Christodoulakis (2016). A bailout was finally granted by the European Union and the International Monetary Fund, conditional on a front-loaded austerity program that led aggregate demand to collapse. For much of the corporate sector, the crisis was existential. With the economy entering an unprecedented recession and the banking system dried-up of liquidity, several firms had to downsize and default on their loans, while others exited the market at a massive scale; see Fig. 1 and Table 1 for more details. Moreover, a lot of them relocated business elsewhere to avoid the excessive taxation imposed by the bailout requirements; by 2014, the population of firms still in the market had shrunk back at the size it had at the beginning of the examination period. To test whether and how the size of Greek firms affected their growth and ultimate survival, we employ a large unbalanced panel dataset with observations for 40,529 Greek firms over the period 2001–2014. Two equal time-spans 2001–2008 and 2009–2014 are considered to cover the periods before and after the financial crisis, respectively. Source: Hellastat database Population of operating firms (Lhs) and real GDP (Rhs), 2001–2014. Firms in thousands; GDP in billion euros at 2010 prices. Greece is currently striving to attract new foreign investment to spur growth and support companies to adapt to new production capabilities. It is, therefore, critical to know what size of firms and in which sectors are more likely to raise turnover in the upturn and stay resilient in a future downturn. To that effect, the estimation establishes a number of characteristics of the corporate sector in Greece over different phases of the business cycle that are crucial in guiding the relevant policy actions. The main findings are the following: In the upturn, firms of all sizes take advantage of the market prospects and increase turnover. However, institutional, infrastructural or other idiosyncratic impediments may cause extensive market segmentation in Greece. That makes the small and medium-scale firms to have more flexibility in augmenting sales and expanding their size as long as aggregate demand is moving on. The implication is that larger-size firms will find it more challenging to develop, thus reducing the benefit on competitiveness, technology advancements and economies of scale in general that the Greek economy could reap from a higher degree of business concentration. The low-scale advantage disappears in a downturn as the lack of liquidity and the limited access to the banking sector prevents small and medium-sized firms from adapting and surviving in the market. Larger-size firms prove to be more resilient in keeping business and retaining large parts of the employment during the crisis, thanks to their more robust networks of production and distribution, and the access to the banking sector. In contrast, medium-sized firms seem to exit the market at roughly a double rate than larger-size firms, while smaller ones at a threefold. Likewise, firms across different sectors of activity seem to behave in a similar pattern during an upswing but much different in the downturn. Companies in the Real-Estate and Construction sectors went up by a manifold during the housing bubble that preceded the global crisis in Greece and elsewhere. After 2008, however, several of them collapsed leaving mountains of non-serviced debt with the banks, precipitating a sharp deterioration in their balance sheets and the need for massive recapitalizations. In contrast, production firms in general, and of the manufacturing sector in particular, managed to resist the repercussions of the crisis and maintained their presence in the market. As the bailout adjustment program forced an 'internal devaluation' by cutting economy-wide wages, industrial firms were in a better position to exploit the opportunity and raise competitiveness. These findings should be taken into account in shaping the sectoral allocation of new investments, as the country seeks to embark on a more sustainable growth path. Carrying out the above analysis entails particular challenges on modelling and econometric estimation. Thus far, most of the traditional empirical literature on testing Gibrat’s Law, as reviewed for example by Santarelli et al. (2006) or Oke (2018), deals with missing or unbalanced data—i.e. incomplete firm-level data over time due to firms’ entering or exiting the market—by using either static or non-fully parametric methods (Mansfield 1962). These methods may either ignore the dynamic interlink between firms’ size, growth and survival or lack the necessary parametric structure needed to obtain estimates conditional on other exogenous factors (Geroski 2005). Different approaches, such as by Peric and Vitezic (2016), utilize only the ‘balanced’ subsample of their available dataset and make conjectures on the amount of sample selection bias to which their estimates are subject. Taking into account all the above, we set up a fully parametric dynamic panel-data estimation framework with the following specific innovations: Compared to Oliveira and Fortunato (2006) and other more recent dynamic approaches reviewed and classified by Oke (2018), it manages to obtain estimates that remain robust to sample selection due to the presence of a continuous exiting/entering market process (Evans 1987). To do this, we follow Wooldridge’s (2002) extension of Heckman's (1979) cross-sectional approach to panel data. In addition to the standard Probit model used in the latter approach, we perform a battery of alternative survival models’ estimates, which provide broader support for the validity of our survival probability estimates. Given that our sample period spans both phases of the economic cycle, we model time shifts in the parameters using dummy variables and estimate them over the whole sample. Such an approach improves over separate estimations in successive periods, as in Lotti et al. (2009) or Kontolaimou et al. (2017), for two reasons. On the one hand, it increases sample-size and, therefore, the estimates' precision; on the other, it does not ignore possible dynamic relationships across cross-sections that may bias results. Our framework handles 'unbalanced' firm-level data, which incorporate information not only for the subset of firms with continuous presence in the market ('balanced' data) but also for those entering / exiting the market throughout the sample period. Our results provide a significant generalization relative to studies that use balanced panel data, such as those reviewed by Peric and Vitezic (2016) or classified by Oke (2018). Moreover, using unbalanced data for dynamic panel data estimation requires proper control over both cross-section unobserved heterogeneity and sample selection; we managed to accomplish both following the methods proposed by Semykyna and Wooldridge (2013). To the best of our knowledge, implementing these methods in the context of Gibrat’s Law testing constitutes an innovative feature of the present study. Furthermore, and as a consequence of the specific application, the paper fills the gap of studying the size-effect on Greek corporations across industries and regions, more systematically and extensively. Thus far, the role of firms' size on market expansion and their survival in Greece has been examined only to a limited extent, time-wise or sector-wise. For example, Fotopoulos and Louri (2000a, b) provided a comprehensive review of empirical studies on firms’ survival in Greece since the early 1990s, while for the period 1995–2000 Vlachvei and Notta (2008) used a small sample of manufacturing and trading firms in Greece. In both studies, however, results were sensitive to the choice of variables measuring size and growth, thus not conclusive on Gibrat's Law. In their study of the Greek manufacturing sector over 1995–2001, Fotopoulos and Giotopoulos (2010) detected no indication of Gibrat's Law, while subsequent research for the service sector by Giotopoulos and Fotopoulos (2010) produced mixed results. Since they used 'balanced' panel data, their estimation did not allow to address issues such as the entry or exit of firms. The behaviour of small and medium enterprises before and after the recent financial crisis was examined in a recent study by Kontolaimou et al. (2017). However, their empirical analysis includes only surviving firms, survival effects on growth or vice versa could not be examined. The study is further narrowed by the fact that larger-size firms were left out of the sample; thus, a possible differentiation across sectors of economic activity was ignored. The sectoral analysis brings about useful sector-specific results that otherwise might have been lost at an aggregate level. Besides, it may also facilitate cross-country comparisons across them, even if their economies have an overall different sectoral composition. The rest of the paper is organized as follows: Sect. 2 describes the data and discusses some stylized effects that recession exerted on Greek firms. Section 3 describes the general econometric framework employed in the analysis, while Sects. 4 and 5 present the main findings on survival probabilities and turnover growth, respectively. Finally, Sect. 6 draws some lessons for the character of policies aiming to support business firms in the aftermath of the Greek crisis and outlines the directions of future research.",2
48,2,Journal of Industrial and Business Economics,12 February 2021,https://link.springer.com/article/10.1007/s40812-021-00182-1,"The impact of pandemics: revising the Spanish Flu in Italy in light of models’ predictions, and some lessons for the Covid-19 pandemic",June 2021,Enrico Berbenni,Stefano Colombo,,Male,Male,Unknown,Male,"The current COVID pandemic has induced several economists to try to predict the economic consequences of such a sudden global health crisis. This is an important challenge for the profession. Indeed, a correct assessment of the consequences of the pandemic could help to design the most appropriate economic measures to face the pandemic during the crisis and in the aftermath. But how predicting the economic consequences of a pandemic? This is not an easy task. Typically, economists build on classic macroeconomic models and (try to) derive clear-cut implications of the shock caused by the pandemic in terms of variables such as wages, interest rates, consumption, inflation, GDP, and others (see Jordà et al. 2020, among the others).Footnote 1 When the economic and financial crisis hit the world in 2008–2009, many scholars tried to compare that crisis with the 1929 Great Depression. Similarly, the current COVID-19 outbreak has generated great interest in the 1918 Spanish Flu. Indeed, many scholars believe that learning about the Spanish Flu short- and medium-term economic consequences might be useful to characterize the economic effects of the current coronavirus outbreak. In this paper, we also make use of the Spanish Flu pandemic, and we discuss the implications of the Spanish Flu in Italy in the light of some simple macroeconomic models, e.g., the class of models used in Jordà et al. (2020), Karlsson et al. (2014), and Boucekkine et al. (2008), among the others, which are also used to make predictions about the effect of the current COVID-19 crisis. In the present work, we focus on the historical evidence on the economic effects of the Spanish Flu epidemic in Italy, and we use this evidence to discuss the predictions on the effects of the Covid-19 pandemic. In particular, we provide some descriptive analyses about the short- and medium-term economic impact of the Spanish Flu in Italy, which seem to suggest that the economic impact of the Spanish Flu in the short- and medium-term, if any, doesn’t entirely fit the predictions of the economic theory. In contrast, in most of the cases, the economic performance in Italy under the Spanish Flu has been the opposite of what predicted by the standard macroeconomic models which are used to forecast the consequences of the current COVID-19 pandemic. One tentative explanation is that the Spanish Flu invested Italy and the rest of the world immediately after the end of the First World War, thus making very hard distinguishing the genuine effects of the pandemic from those of the war. In this sense, our considerations about the short- and medium-term impact of the Spanish Flu in Italy suggest putting the analysis of the economic consequences of any pandemic—included the current one—into the appropriate historical context in order to get a whole picture of the interconnected forces at work. The rest of the paper proceeds as follows. In Sect. 2 we describe the economic consequences of a pandemic as derived by using standard neoclassic macroeconomic models. In Sect. 3 we briefly describe the Spanish Flu pandemic, with a particular focus on Italy. In Sect. 4 we test the main predictions of the macroeconomic models introduced in Sect. 2 by considering the Italian situation in the aftermath of the Spanish Flu and providing some descriptive analysis to assess the short- and medium-term impact of the Spanish Flu. Section 5 discusses and concludes.",4
48,2,Journal of Industrial and Business Economics,25 March 2021,https://link.springer.com/article/10.1007/s40812-021-00185-y,Note on a profit-raising entry effect in a differentiated Cournot oligopoly market with network compatibility,June 2021,Tsuyoshi Toshimitsu,,,Male,Unknown,Unknown,Male,"Along with the progress that has occurred in information and communication technology, the last two decades have witnessed a proliferation of consumer electronic products and services that exhibit network externalities. In addition, the properties of such products and services are associated with compatibility (and interconnectivity), which means that the products and services interact with other product and services to enhance performance for users. Hereafter, we define a product and service market with such properties as a network product market; examples of such markets include those for smartphones, application software, and Internet services. Currently, in newly industrializing countries, such as China, Russia, and Korea, as well as in advanced countries, including the US, the EU, and Japan, companies are entering into network product markets. Thus, it is an interesting issue to examine how entry affects individual firms, the industry, and consumers. In network product markets, compatibility and standardization of products and services are important for both providers and users of such products. Compatibility and interconnectivity are characteristics of products and services that interact with other products and services to enhance performance for users (consumers). For example, Gandal (1995) empirically analyzes complementary network externalities in personal computer (PC) software markets, in which users need to exchange data files between spreadsheets and database management systems. In this paper, we consider a network product market where each firm provides application software, provided that consumers have installed a basic platform (main frame), e.g., Blu-ray, Windows, or Android operating systems, and purchased relevant hardware, e.g., a PC, PlayStation, or Xbox. We examine how entries to the market affect profits and consumer welfare depending on the strength of network externalities and compatibilities. Commonly, in a standard Cournot–Nash oligopoly model, an increase in the number of firms leads to a reduction in both an individual firm’s output and profit, and industry profits.Footnote 1 However, following the seminal paper by Rosenthal (1980), a number of studies have found contrary results, i.e., profit-raising entry effects.Footnote 2 Naylor (2002) introduces a firm-specific trade union bargaining with firms over the wage rate into a standard Cournot oligopoly and demonstrates that industry profits increase with the number of firms if the bargaining power of the unions is sufficiently large. In a vertically related market structure, i.e., upstream and downstream firms, or input and final goods markets, Matsushima (2006) demonstrates that when upstream firms compete in quantity and freely enter the input market, competition among downstream firms reduces the input price and, thus, an increase in the number of downstream firms increases the industry profits. Mukherjee (2019), who extends Matsushima (2006), shows that entry increases the profits of the incumbent in the final goods market if firms freely enter the input market and the final goods are sufficiently differentiated. Assuming R&D investments by the downstream firms in the vertical oligopoly model, and focusing on the role of the knowledge spillover effect, Wang and Lee (2015) demonstrate that an increase in the number of downstream firms can increase industry profits if the entry cost for upstream firms is lower and the knowledge spillover effect is moderate. Although these studies are based on vertical market structures, we investigate entry effects in a network product market without upstream (input) markets. Furthermore, Mukherjee and Zhao (2017) show that if the entrants behave like Stackelberg followers and the incumbents differ in their marginal costs, then entry benefits the cost-efficient incumbents, while hurting the cost-inefficient incumbents. Based on a differentiated Cournot–Nash oligopoly model, we examine how an increase in the number of firms affects outputs and profits in the case of network externalities and product compatibility, and demonstrate the condition under which a profit-raising entry effect arises. Furthermore, we investigate the effect of a profit-raising entry on consumer and social welfare and find that if the number of firms is sufficiently large, consumer surplus declines, although social welfare increases.",
48,2,Journal of Industrial and Business Economics,02 November 2020,https://link.springer.com/article/10.1007/s40812-020-00177-4,An empirical analysis of the impact of structural changes in the mobile market,June 2021,Otello Ardovino,Marco Delmastro,,Male,Male,Unknown,Male,"The present paper examines the Italian mobile phone market evolution over a three-year period (i.e. fall 2015–fall 2018). In such lapse of time the market has experienced significant structural changes: a merger between two of the main operators was approved under the condition of the entrance of a new competitor. This market evolution represents an ideal condition to verify how a change in market configuration can affect the consumer switching choices (i.e. churn rate). To this end, we compare results from a traditional consumer choice model, estimated at two different points in time using two surveys carried out before and after this structural break. Both surveys are representative of the Italian population. The empirical results show that the entry of a new operator, although the number of players has remained unaltered due to the concurrent merger between two of the main operators, has led to a change in the nature of the competitive game. Two main effects emerge. Firstly, we detect a considerable impact of a redistributive nature: the entry of a new operator reduces prices and improves both the quality and the variety of the supply, thus producing beneficial effects on consumer welfare, especially for the less wealthy cohorts of the population. Furthermore, the entry of a new operator puts pressure on incumbent companies, which react aggressively, producing a further strengthening of consumer benefits. Hence, a significant departure from the pre-merger equilibrium (i.e. a consolidated restricted oligopoly setting) was reached. Secondly, by testing the significance of some factors that are generally considered by consumer choice theory (but very rarely implemented in empirical models), we can confirm that both transaction costs and the subjective value of services are crucial elements in the decision-making process of consumers. While transaction costs are important per se, reducing the individual propensity to change in both market configurations (pre- and post-structural break), the entry of new operators, at least in the short term, encourages the switch of those consumers for whom the implied value of the service is higher. Conversely, the impact of the level of information on decision-making process appears to be complex since different types of decisions seem to require different types of information. In our case, technical information about telecommunication services does not appear to be related to the decision of consumers, which instead seems to be driven by economic motivations. The remaining part of the paper is structured as follows. Section 2 describes the general framework of firm entry and exit from the mobile market. Section 3 illustrates the main competitive changes in the Italian mobile market during the period between fall 2015 and fall 2018. Section 4 describes the methodology underlying the empirical analysis as well as the first qualitative results. Section 5 presents the results of the regression analysis. Finally, Sect. 6 concludes the paper with some brief remarks.",2
48,2,Journal of Industrial and Business Economics,27 January 2021,https://link.springer.com/article/10.1007/s40812-020-00179-2,Industry and financial market concentration,June 2021,Gerasimos T. Soldatos,,,Male,Unknown,Unknown,Male,"Bajgar et al. (2019) and Grullon et al. (2019) are among those who document a clear increase in industry concentration in Europe as well as in North America while Laeven et al. (2016) and Fernholz and Koch (2016) are some of those who record increasing concentration in the banking industry as well. OECD (2018) maintains that there is a relationship between industry and financial market concentration, pointing at the same time to the theoretical and empirical difficulties surrounding this relationship. Empirical works from the point of view of the financial sector, document that increased bank concentration does not necessarily imply increased bank market power to the extent that banks internalize the adverse effects of higher interest rates. Hence, financial inclusion is not hurt seriously though markups in banking do increase anyway (Owen and Pereira 2018; Saidi and Streitz 2019). Such, in general, empirical findings on the connection between industry and financial market concentration, have not been studied theoretically. The approach towards the relationship between the two concentration indexes has been thus far an empirical one. Yet, a theoretical formulation of this relationship could serve as a measure against which the nature of the empirical problems met when trying to detect the relationship in practice, may be assessed. This note proceeds with such a formulation by approaching market concentration from the viewpoint of the Lerner index. The reason for the use of this index is that as the World Bank explains, this is what the “new empirical industrial organization” literature would dictate. The use of Lerner indexes is a way of avoiding the challenge that the concept of market contestability poses to the predictive accuracy of concentration measures on competition.Footnote 1 That is, a theoretical discussion is put forward but in the direction suggested by empirical work. It thus notices in the next section that the price elasticities of demand in the product and loan markets are inversely related. An increase in the latter elasticity leads to bankruptcies in the financial market and higher concentration; but the subsequent decline in product elasticity of demand encourages firms to enter the output market for assured sales, ceteris paribus. The interaction between the two concentration indexes is further illustrated through a simplified ad-hoc model of a business cycle in terms of these indexes. Perfect competition is found to be an unstable equilibrium. A limit cycle state is predicted as the alternative course of affairs. Section 3 discusses the role that monetary and competition policy can have in forging the desired mix of concentration indexes. Critical in policymaking is the proposition of this paper that the causality between the Lerner indexes in banking and industry runs both ways as implied, for instance, by Margaritis and Psillaki’s (2007) empirical analysis on firm efficiency and leverage. Section 4 emphasizes that this relationship may be difficult to be tested empirically given that entry is influenced by many other factors. Section 5 concludes this paper with some remarks on the connection of these considerations with Minsky-type cyclical fluctuations, the role of the institutional environment surrounding bank-industry interaction, and the impact of international trade on concentration indexes. From the point of view of policymaking, a money supply policy influencing the price level, or an interest-rate rule influencing the cost of capital acquisition through bank borrowing, can both change the Lerner indexes in the desired direction (if any). This is important because what the Lerner index algebra implies in practice is that one expects to see more firms entering industry when bank concentration increases.",
48,2,Journal of Industrial and Business Economics,23 June 2020,https://link.springer.com/article/10.1007/s40812-020-00161-y,Litigation risks and firms innovation dynamics after the IPO,June 2021,Enrico Forti,Serena Morricone,Federico Munari,Male,Female,Male,Mix,,
48,3,Journal of Industrial and Business Economics,26 April 2021,https://link.springer.com/article/10.1007/s40812-021-00186-x,Process R&D investment and social dilemmas,September 2021,Michal Ramsza,Adam Karbowski,Tadeusz Platkowski,Male,Male,Male,Male,"R&D coopetition (cooperation in R&D between market rivals) is used in various industries, cf., e.g., Bouncken and Fredrich (2016), Cygler et al. (2018), Jakobsen (2020), to achieve technological synergies and cost reductions, among other benefits (see, e.g., Ritala and Sainio (2014) or Conti and Marini (2019)). Interestingly, R&D coopetition can also extricate firms from disadvantageous social dilemmas, as we further show in this article. The present paper contributes to the relevant innovation literature by considering R&D behavior of firms from various social dilemma viewpoints (prisoner’s dilemma, chicken game, and trust dilemma). The particularly interesting contribution of our article extends the well-known finding discussed by Amir et al. (2011). The latter authors show that under spillover levels not too high and relatively low R&D costs, Cournot firms are caught in the prisoner’s dilemma for their R&D investment decisions. As we show, this result can be developed when the R&D fixed entry costs are introduced into the analysis. To be specific, if the entry costs are greater than 70 per cent and smaller than 100 per cent of the initial marginal costs, our results qualitatively differ and extend the findings discussed by Amir et al. (2011). As we noted, economists have already investigated the R&D behavior of firms from the social dilemma perspective. However, in the previous works, this perspective was used in a rather limited way, sometimes only as a by-product of a standard economic analysis. We set out to exploit the social dilemma perspective in firms’ R&D in a broader way compared with the relevant innovation literature reviewed below. Lambertini and Rossini (1998) showed that firms may compete in undifferentiated products due to a prisoner’s dilemma generated by externalities affecting R&D in product innovation. Amir et al. (2011) considered a standard duopoly two-stage game of process R&D and quantity competition. They showed that competing firms are caught in a prisoner’s dilemma for their R&D decisions whenever technological spillovers in the industry are low and costs of conducting R&D are not too high. Such a prisoner’s dilemma underlies the creation of an R&D-avoiding cartel. Burr et al. (2013) extended the result that duopoly firms end up in a prisoner’s dilemma for their R&D decisions, whenever technological spillovers and R&D costs are relatively low. In particular, they showed that incentives faced towards R&D cartel are maximal for the case of zero spillovers, which is when the prisoner’s dilemma has the largest scope. In the present paper, we identify not only prisoner’s dilemma, but also two other fundamental social dilemmas—chicken game and trust dilemma—in firms’ R&D investment decisions. We further show that in each of the distinct dilemmas, a possibility to enter a binding R&D agreement and start R&D coopetition changes a competitive outcome to a more desirable one. In general, we show that disadvantageous social dilemmas associated with the firms’ competitive behavior are mitigated by R&D coopetition. The latter result is in line with the relevant innovation literature, where the beneficial role of R&D agreements has been already identified. For example, Conti and Marini (2019) show that interfirm R&D agreements can effectively enhance enterprise gains from the internalization of industrial knowledge spillovers. Since social dilemmas are crucial to our paper, let us briefly differentiate between the basic types of social dilemmas. In the prisoner’s dilemma game, players face two social incentives, i.e., the gain for those who exploit cooperative partners (greed), and the loss for cooperators who are exploited by non-cooperative partners (fear), see (Płatkowski 2017). A near-cousin of the prisoner’s dilemma game, trust dilemma (assurance game), cf. (Kiyonari et al. 2000), is characterized by a different social tension than prisoner’s dilemma. In the trust dilemma only fear is present. Finally, a chicken game is a social dilemma in which only greed is present. We introduce the social dilemma perspective into the broader literature on strategic behavior of firms in R&D which in turn is a straightforward continuation of the debate initiated by Schumpeter (1942) on the relationship between industry structure and incentives to undertake R&D. In the relevant following literature, cf., e.g., Spence (1984), Katz (1986), d’Aspremont and Jacquemin (1988), Kamien et al. (1992), Kamien and Zang (2000), Amir et al. (2011), Burr et al. (2013), Bourreau et al. (2016), Capuano and Grassi (2019), the behavior of firms in R&D is modeled by non-cooperative games (see also Cosandier et al. 2017 or Amir et al. 2019), in which enterprises, first, simultaneously and independently decide about their R&D investments (these decisions affect the total manufacturing costs of each enterprise), and, further, compete in the final product market according to a given (quantity or price) competition model. To be specific, our paper is directly related to works by Amir et al. (2011) and Burr et al. (2013), but we introduce the broader social dilemma perspective into the analysis. In the present paper, we identify and discuss various types of social dilemma in strategic R&D behavior of enterprises. The analysis of firms’ R&D behavior from the social dilemma perspective can be particularly useful for strategic managers and innovation policy makers. The first group can exploit the presented findings for the purposes of optimal decision making in the strategic contexts. The second group can use the discussed results to design a policy which resolves or overcomes identified social dilemmas. The article proceeds as follows. The model of firms’ behavior in R&D is presented in the next section. The following section shows the firms’ strategic games occurring under R&D competition and R&D coopetition. The last section presents and discusses obtained results, and in particular elaborates upon social dilemmas identified in strategic behavior of enterprises.",1
48,3,Journal of Industrial and Business Economics,06 January 2021,https://link.springer.com/article/10.1007/s40812-020-00180-9,Whom should I merge with? How product substitutability affects merger profitability,September 2021,Roberto Cellini,,,Male,Unknown,Unknown,Male,"Merger profitability is investigated by a large body of theoretical literature, starting from the seminal contributions of Salant et al. (1983), Deneckere and Davidson (1985) and Perry and Porter (1985). A prominent result is that merger can be individually unprofitable for merging firms, as they reduce their output production, in front of non-merging firm(s) increasing output. As shown by subsequent literature, the occurrence of this outcome, known as “the merger paradox”, is due to strategic interdependence. More precisely, the merger paradox occurs in the case in which the merging parties jointly make less profit than the sum of their pre-merger profits. The reasons of the paradox are well known: in a oligopolistic market two countervailing effects are in operation when firms decide to merge. On the one side, there is a positive effect on the merging firms’ profit, due to the elimination of rivals; on the other hand, non-merging firms have an incentive to increase production—if competition is in quantity, and reaction curves are downward sloping. The relative size of these two effects drives the final result concerning the merger profitability from the firms’ perspective. A number of factors matter, including the initial and final (pre- and post-merger) market shares, the firms’ choice variable(s), that is, the price or quantity competition (see McElroy 1993), the competition along one or more dimensions (Pinto and Sibley 2016; Brekke et al. 2017), the cost structure and the cost nature (Farrel and Shapiro 1990; Fanti and Meccheri 2012; Majumdar et al. 2019), and the product characteristics (e.g., Hsu and Wang 2010). A literature review is provided by Faulì-Oller and Sandonis (2018). What has induced the present investigation is the debate following the proposal of merger between FCA and Renault, in summer 2019. The merger proposal had very short life –as both parties, substantially, withdraw the proposal in some days. During those days, however, several observers, especially in Italy and France, cast doubts about the profitability of such a merger, in front of the fact that these firms supply very similar products in the car market: substantially, both FCA and Renault produce traditional medium cars, and they are absent from the market segment of innovative electric cars. Some months later, the merger between FCA and PSA (Peugeot-Citroen) has successfully occurred. In this case, columnists and policy-makers generally expressed positive opinions, also basing on the fact that the two groups have different points of strength. These pieces of news lead to ask whether, in a market of differentiated products, a firm finds it more profitable to merge with a more similar or more different product supplier. Though very huge, the theoretical body of economic literature on mergers overlooks this specific point, namely whether mergers are more profitable if they occur between firms providing similar or different products. To the best of my knowledge, I can mention only two theoretical papers dealing with this point specifically; they reach opposite conclusions. Ebina and Shimizu (2009) analyse a framework made by four firms, and merger incentives are shown to be stronger for firms producing closely related goods than more differentiated goods. The main interest of Ebina and Shimizu (2009) is in the occurrence of sequential mergers, and they show that either a sequence of mergers occurs, or no mergers occur in equilibrium. Hsu and Wang (2010) consider a uniform product substitution rate among varieties, and show that merging emerges to be profitable if varieties are sufficiently distant substitutes; however, that paper considers symmetric substitutability across all pairs of goods, and the more distant goods are, the more profitable the merger between two firms is. I propose here a model specifically focussed on the point of merger profitability as related to the degree of product differentiation. The present model differs from Hsu and Wang (2010), as long as substitutability between goods is not homogeneous. It differs from Ebina and Shimizu (2009), as it is simpler, more focussed on the role of the degree of product differentiation, and does not consider sequential merger. Thus, the economic reasons driving the result on merger profitability can emerge more neatly. Roughly speaking, the result from the present model is that, in a static framework, merging with a more similar product is more profitable than merging with the firm supplying more different product variety. This conclusion is more in line with the outcome from Ebina and Shimizu (2009) than Hsu and Wang (2010). The intuition rests on the fact that when two firms producing similar goods do merge, they internalise a strong negative externality on profit deriving from the competition with each other. In this case, the internalised externality is able to overcome the negative effect deriving from the fact that the non-merging firm usually finds it optimal to increase production. On the contrary, when a merger occurs between firms producing highly differentiated goods, the internalised externality is limited, and it is not able to compensate the negative effect due to the increased production of non-merging firm. The present model is static; it does not take into account product and process innovation; it omits to consider the effects of merger upon inputs’ cost; it disregards the multidimensional nature of competition; and so on. Nevertheless, it permits to understand pros and cons of merger, for a specific firm, from a market structure perspective. It shows that the merger paradox may occur, as previous models have documented. More interestingly, the present model shows that, under a wide range of sensible parameter configuration, the paradox typically occurs when strongly differentiated goods providers do merge; the paradox is harder to emerge if merger occurs between similar goods providers. Interestingly, according to the present model, the merger with Renault should have been for FCA more profitable than the merger with a firm offering more different cars, such as Peugeot. A related issue to merger profitability concerns the mergers’ anti-competitive effects. Insights from product differentiation literature have been used extensively in policy and by antitrust authorities to establish which mergers are harmful to competition (see, e.g., Ivaldi et al. 2003b). Interestingly, some parallels can be drawn between the effect of product differentiation upon the individual profitability of merger and the effect of product differentiation on market power and hence anti-competitive behaviour of firms. In general, and roughly speaking, available literature shows that a merger between firms that produce close substitutes is more likely to increase firms’ market power than a merger between firms producing more distant substitutes (see, e.g., Baker and Bresnaham 1985; Ivaldi et al. 2003b). The substantial reason is the same that makes the profit paradox less likely in the case of a merger among firms providing more similar goods: firms providing close substitutes are held to compete more fiercely each other as compared to firms providing more differentiated goods, so that the merger in the former case is more detrimental for market competition. The structure of the paper is as follows. Section 2 presents the basics of the model and find the outcome of pre-merger configuration. Sections 3 and 4 find the outcome when a merger occurs between firms supplying “slightly” differentiated varieties, and “strongly” differentiated varieties, respectively. Section 5 takes the perspective of a firm evaluating the merger with different competitors, and provides the comparison of firms’ profit. Comments and concluding remarks are gathered in Sect. 6.",2
48,3,Journal of Industrial and Business Economics,19 March 2021,https://link.springer.com/article/10.1007/s40812-021-00183-0,Single euro payment area (SEPA) and banking industry: discriminatory pricing vs. non-discriminatory pricing,September 2021,Bita Shabgard,,,Female,Unknown,Unknown,Female,"The growth of international trade, cross-border e-commerce, and migration show that cross-border retail paymentFootnote 1 is increasingly important in the last century. To illustrate, many businesses serve clients abroad and purchase goods from international suppliers; many people make online purchases from international sellers, and migrants send money to their families in their home country; government agencies purchase from international suppliers or pay international aids. Despite the importance of cross-border payment, in Europe, customers had difficulty doing so. An individual in Spain could not authorise a direct debitFootnote 2 by a German company (for instance to pay a bill, receive a salary, etc.) unless he had a bank account in Germany. A business needed to maintain different bank accounts in the European countries in which it operated in order to conform to their instructions. Another problem that customers faced was a higher cross-border transaction price than domestic one. The evidence provided by the European Commission in 2001 shows that the price of transferring €100 from Luxembourg to another European country was €9.58, from Germany was €11.93, from Spain was €20.56, and from Portugal was €31.04, while the price of domestic payment was negligible (EC’s survey IP/01/992, 2001b). The European Commission (henceforth EC), European Payment Council (henceforth EPC), and the European Central Bank (henceforth ECB) have debated that the source of customers’ problems making cross-border payment was due to the incompatibility between domestic payment systems across European countries. In particular, each European country was served by its own domestic payment systemFootnote 3 that was created under the national rules and standards. Therefore, a third-party, like a correspondent bank, was required to link domestic payment systems in order to make a cross-border payment.Footnote 4Footnote 5 To overcome these problems, the first attempt after the introduction of the physical Euro as a single (cash) payment instrument across Europe was to enforce banks to charge the same transaction prices for domestic and cross-border payments.Footnote 6 The next step was to overcome the problem of incompatibility between domestic payment systems (such as overcoming different standards, rules, and technologies across countries) and form an integrated financial market in Europe. In this regard, the EPC enforced the Single Euro Payments Area (henceforth SEPA) project towards the retail payment market in Europe that supported by the EC and the ECB.Footnote 7 Based on the definition of the ECB (2009) ‘SEPA is an area, in which consumers, companies and other economic actors will be able to make and receive payments in euro, whether within or across national borders, with the same basic conditions, rights and obligations, regardless of their location.Footnote 8,Footnote 9 The mission of the SEPA project was to apply principles in which all non-cash euro payments are treated in accordance with the same rights and obligations irrespective of their location. In particular, the intention was to harmonise national payment systems across Europe and implement uniform pricing for domestic and cross-border payments. In fact, SEPA has created one union-wide retail payment market in the euro area. Payment service providers such as banks were responsible for implementing this project since they are the main owners and users of the payment systems. As payment services play a vital role in  banks' revenues, we focus our attention on banks as a proxy for payment service providers.Footnote 10 To comply with SEPA, banks made substantial investments to adopt the common standards required by this project for credit transfers, direct debits, and payment cards.Footnote 11 Following Kemppainen (2008), we consider this cost as a fixed adjustment cost. According to the fundamental changes in the payment system as a result of compliance with SEPA, it is of great importance to analyse consequences of this policy. In this regard, the main objective of this paper is to analyse how uniform pricing with respect to harmonisation of payment systems affects competition between European banks in the retail payment market and welfare. To analyse the potential effects of SEPA, it is essential to consider the prevailing conditions prior to SEPA project. In this paper, pre-SEPA considers as a period in which payment systems are incompatible across countries and banks are allowed to discriminate between the price of domestic and cross-border payments. Post-SEPA refers to a period in which banks complied with the SEPA project and applied the uniform pricing for making domestic and cross-border payments. Given this generalised framework, we extend the duopoly model by Laffont et al. (1998a, 1998b) (henceforth LRT model) who provide a theoretical framework for the telecommunication market, to analyse competition between two asymmetric banks, in terms of transactional capital, under non-linear pricing and in the presence of economies of scale. Kokkola (2010) states that economies of scale are one of the main features of the banking industry since they allow banks to recover the high cost of investing in infrastructure. Humphrey (2009) estimates economies of scale among 11 European countries like Germany, France, U.K, Spain, Netherlands, Italy, Belgium, Sweden, Finland, Norway, and Denmark over 1987–2004. He finds that banks in the payment market are acting under economies of scale. Beccalli, Anolli, and Borell (2015) find that there are economies of scale among different banks, and they are significantly large for the largest banks. In this regard, we analyse competition between banks under presence of economies of scale. In this paper, the large bank is defined as a bank with a large transactional capital compared to a small bank with a small transactional capital. Study of competition between two asymmetric banks is motivated by the entrance of Neobanks and FinTech companies (technology firms that focus on financial products and services) into this market, which creates asymmetry between banks in terms of transactional capital. The major difference between our model and LRT model is the definition of the cost function. In the framework of LRT model, it is considered that the two networks have the same cost structures. Here, we define that the two banks have different marginal costs and assume that the large bank is cost-efficient due to a larger capital level. In this setup, we analyse impacts of SEPA on competition between banks and welfare by comparing pre-SEPA with post-SEPA phases. Given economies of scale, we start our analysis in a symmetric case where the two banks have the same level of transactional capital. Comparison between pre- and post-SEPA shows that symmetric banks could not take advantage of SEPA, but SEPA would be beneficial for customers because of the lower transaction prices. Welfare would improve in post-SEPA when increasing transaction volumes can offset the fixed adjustment cost. Then, we analyse the impacts of SEPA in the asymmetric case. The main result illustrates that the transaction pattern affects competition between banks in post-SEPA which is consistent with Leibbrandt (2010) and Schaefer (2008). We show that when the transaction pattern is dominated by the domestic market, then competition between banks is less intense in post-SEPA. We further show that consumer surplus improves in post-SEPA because banks offer a higher net surplus to customers by applying uniform pricing. The effect of SEPA on welfare depends on the amount of fixed adjustment cost. If this cost is sufficiently small, then welfare improves in post-SEPA. On further analysis, we consider the case where economies of scale are improved in post-SEPA. Under this condition, SEPA is pro-competetive. This result is in line with the EC expectation about the effect of SEPA on competition between banks. Analytical literature related to our research question is scarce, but there are several studies that attempt to enhance understanding of the opportunities and costs of SEPA for customers and banks. These studies apply different methodologies. Some of them have examined the effects of SEPA from a theoretical or empirical standpoint, while others are interview-based studies. Leibbrandt (2010) and Schaefer (2008) study effects of compatibility of Europe’s payment systems on bank competition and welfare. Leibbrandt (2010) assumes that there are two equal sized countries served by two banks, one in each country. He considers banks compete in two stages: in the first stage, they decide to comply with the compatible system, and in the second stage, they compete on prices. Results show that if the transaction patterns are dominated by domestic transactions, banks maintain the incompatible system to avoid fixed adjustment cost. In contrast, if this cost is zero, then banks make more profit with the compatible system than the incompatible one. Consumer surplus decreases since banks charge higher prices. Schaefer (2008) applies a spatial bank competition model between two banks and focuses on the cost implications. From an analytical perspective, he considers the choice of adopting SEPA by comparing two cases: ‘high fixed adjustment cost and low cost of cross-border payment’ against ‘low or zero adjustment cost and high cost of cross-border payment’. From the results, he suggests that adopting SEPA may be welfare-enhancing if the fixed adjustment cost reduces or the share of the cross-border payment is high enough to cover this cost. He also finds welfare-enhancing due to intensifying cross-border competition between banks through reducing entry barriers. Kemppainen (2008) evaluates economic effects of SEPA in a spatial competition model in the debit card market. He considers two countries covered by two incompatible payment networks in pre-SEPA and compatible in post-SEPA. The results show that SEPA causes an increase in annual fees, larger network size, and greater consumer surplus. However, these studies do not further discuss price setting in pre- and post-SEPA. The present paper contributes to this strand of literature by focusing on price setting and cost efficiency due to the harmonisation of national payment systems to analyse the effects of SEPA on competition between banks and welfare. Todorovic et al. (2017) empirically evaluate effects of SEPA on performance of banks among 17 European countries in the period 2002–2012. They find that benefits of SEPA cannot cover fixed adjustment cost in the short term, while SEPA will improve the performance of banks in the long term. Schmiedel (2007) and later on PWC group (2014) assess the economic impact of SEPA by focusing on the survey- and interview-based studies. The rest of the paper is organised as follows. The structure of models in pre- and post-SEPA are laid out in Sect. 2. Section 3 analyses competition between banks in pre- and post-SEPA. Comparisons of two phases are shown in Sect. 4. The conclusion is presented in Sect. 5. The appendix contains the proofs.",1
48,3,Journal of Industrial and Business Economics,03 August 2020,https://link.springer.com/article/10.1007/s40812-020-00174-7,Tax evasion and competition in a differentiated duopoly,September 2021,Luciano Fanti,Domenico Buccella,,Male,Male,Unknown,Male,"The level of indirect tax evasion by firms is a relevant concern in many countries. For instance, “the Member States in Europe are collecting around one half of the VAT revenue available to them” (European Commission 2013, p.8).Footnote 1 On the other hand, despite its relevance for policy-makers, the literature dealing with the issue of firms compliance is rather scanty in contrast to that studying personal tax compliance: in the recent words of Bayer and Cowell (2009, p. 1131) “the behaviour of firms is sometimes glossed over in the economic analysis of tax policy. In the analysis of tax compliance it is often omitted altogether.” The knowledge of the relationship between the firms’ market environment and their compliance behaviours represents a key question to enable policy-makers to evaluate qualitatively as well quantitatively the firms’ tax evasion, and to develop properly an effective tax-design. Few papers have investigated this subject, assuming prevalently a perfect competition environment (e.g. Virmani 1989; Cremer and Gahvari 1992, 1993, 1999; Panteghini 2000; and Hashimzade et al. 2010); however, Marrelli and Martina (1988), Goerke and Runkel (2006, 2011), Bayer and Cowell (2009) and Besfamille et al. (2009a, b) have studied imperfect competition. More specifically, few scholars have investigated the relation between the degree of market competitive pressure and firms’ tax evasion, despite intensifying competition and effort against evasion are high on the agenda of policy-makers. This paper considers the effects of greater competitive pressure on (absolute and relativeFootnote 2) tax evasion in a duopoly market. First, we consider a general functional form for demand under Cournot competition with homogeneous products to provide a better intuition what is generally going on in the model with regard to the effects of marginal costs’ changes on tax evasion. It is shown that decreasing marginal costs reduce the competition level under a sufficiently low slope elasticity. In such a case, more competition may imply either more or less tax evasion, depending on whether the demand is elastic or inelastic, respectively. Second, while the previous literature focuses on firms acting in a homogenous goods industry, we consider the most used framework with product differentiation (i.e. Dixit 1979; Singh and Vives 1984) in which firms face constant marginal costs and linear demand functions. In particular, the degree of product substitutability and marginal production costs are considered as the competition parameters, and we analyse the effects of a change in these two parameters on absolute and relative evasion. Third, the cases in which the two firms compete on prices and on quantity are considered, checking whether and how the competition mode affects the results. To the best of our knowledge, this paper fills a gap in the literature because tax evasion in a differentiated product duopoly has not been so far explored. The main findings are as follows: (1) under Bertrand competition, the effect of increasing competition on tax evasion depends on what triggers more competition (either a higher degree of product substitutability—denoted as “product market competition”—or a lower marginal production cost—denoted as “cost competition”). In particular, (1.1) the degree of differentiation may show an inverted U-shaped relationship with tax evasion, depending on the marginal cost (i.e. a higher degree of product substitutability is more likely to decrease (resp. to increase) tax evasion the higher (resp. the lower) both the marginal production cost and the pre-existing competitive pressure); and (1.2) a higher marginal production cost is more likely to decrease tax evasion the lower the initial level of the marginal cost and the higher the degree of product substitutability; (2) comparing the cases of output and price competition, it is shown that the absolute tax evasion and the relationship between relative evasion and marginal costs are (quantitatively and qualitatively, respectively) the same in both cases. By contrast, an increase in the degree of product substitutability has an univocal effect in the Cournot case (i.e. the higher the degree of product substitutability, the higher the relative evasion); (3) comparing relative evasion in the two competition modes, it is shown that relative tax evasion is larger under Bertrand (resp. Cournot) if the degree of product substitutability is sufficiently high (resp. low) and the marginal production cost is sufficiently low (resp. high). This implies that tax evasion may be higher when the competitive pressure is stronger. Public policy may affect – directly or, more often, indirectly—competition focusing on the production costs as well as on the competition mode (as widely recognized, price competition is fiercer than output competition) and the degree of product differentiation. As to the latter, the concerns for a public regulation of product differentiation have been noted since Chamberlin (1950) (e.g. Dixit and Stiglitz 1977; Salop, 1979). As regards the former, policy can affect competition through changes in production costs, for instance, “by liberalizing input markets, slashing the bureaucratic burden imposed on firms or by simplifying international trade in inputs” (Goerke and Runkel, 2011, p. 723). The scant literature on this theme has initially pointed out that the economics of tax compliance has “no a priori argument for holding that a more collusive market (as opposed to a more competitive one) should lead to a higher tax declaration, or vice versa.” (Marrelli and Martina 1988, p. 56). The papers dealing with the theme of the tax evasion/competitive pressure relationship—and thus closest to the present one—are those of Marrelli and Martina (1988) and Goerke and Runkel (2011). Both papers assume a homogeneous product and quantity competition. Noteworthy, those papers show opposite results as regards the sign of the link between competition and tax evasion. In fact, Marrelli and Martina (1988) find that the more competitive the market is, the lower is the tax evaded; moreover, this result holds not only for a symmetric duopoly but also for an asymmetric duopoly with (not too great) differences in production costs.Footnote 3 Goerke and Runkel (2011) find ambiguous effects of market competition on tax evasion. The relationship is positive (negative) if demand is inelastic (elastic): in fact, a higher degree of competition always raises tax evasion if the demand is linear or concave. Thus, Goerke and Runkel (2011) have the result of Marrelli and Martina (1988) as a special case. According to Goerke and Runkel (2011, p. 714), two main reasons can explain the appearance in some cases of a sharp difference between their findings and those of Marrelli and Martina (1988): “First, Marrelli and Martina (1988) focus on a conjectural variation parameter to model a change in competition, whereas we look at the impact of deep competition parameters. Second, Marrelli and Martina (1988) assume decreasing absolute risk aversion. The less collusive the market, the smaller are the profits and the higher the risk aversion. This provides firms with the incentive to evade less when competition becomes more intensive. To rule out such risk-driven tax evasion, we suppose firms to be risk neutral.” Another difference between the two papers is that Marrelli and Martina (1988) assume that firms decide the evasion in terms of unpaid taxes and will incur a fine (including the tax) represented by the evaded tax multiplied for a penalty rate larger than one, while Goerke and Runkel (2011) assume that firms decide the evasion in terms of undeclared sales and the penalty is a function increasing and convex in evaded revenues. However, none of these authors considers such assumptions crucial for the results achieved and, thus, their diversity cannot be ascribed to the difference in the definitions of the evasionFootnote 4 and of the penalty. Finally, Marrelli and Martina (1988) (resp. Goerke and Runkel 2011) derive their results under the assumption of linear demand and zero costs (resp. weakly concave—e.g. linear—and iso-elastic demand and constant marginal costs). Unfortunately for the policy-makers, the policy implications of the two above mentioned papers are in sharp contrast: while Marrelli and Martina (1988) suggest that governments should encourage competition, and a larger competition (either induced by the government itself or by external factors like globalization) allows reducing the effort in the fight against evasion, Goerke and Runkel (2011) argue that policy-makers should be careful in encouraging competition, unless a stronger fight to reduce evasion accompanies it. Thus, the sharp contrast between the signs of the relationship between competition and evasion emerged in the preceding literature (due to an analysis conducted with homogeneous goods) is a stimulus to reassess the issue, developing a framework with differentiated products (with the degree of product differentiation as a “deep” competition parameter), and firms competing either in prices or output. The aim of the paper is thus to shed further light on the nexus between competitive pressure and tax evasion, a requisite for a fruitful tax policy analysis of the corporate sector. Moreover, Goerke and Runkel (2011) have already analyzed marginal costs as deep competition parameter. In their model, however, the scale of the marginal costs exerts either a positive or negative effect on tax evasion, but never an U-shaped effect as we detect. On the other hand, those authors treat the number of firms as endogenous, so focusing on the entry effects of tax evasion, while the present paper considers a fixed number of firms. Therefore, the novelty of this paper is to consider the competitive role of products differentiation and, thus, whether and how the competition on the product variety affects tax evasion outcomes and modifies the established results. Developing a duopoly model with differentiated product under price as well as output competition,Footnote 5 this paper proposes a twofold contribution. On the one hand, it obtains that, in essence, the first (second) suggestion above mentioned about the relation between tax evasion and competitive pressure holds true when the existing level of competition is relatively low (high). On the other hand, it argues that the expected effect of competition changes on firms’ tax evasion depends on its source (i.e. changes in the degree of product differentiation, in production costs, in the competition modes, i.e. on quantities or prices). In fact, the firms’ main strategies to cope with the competitive pressure are cost-reducing interventions or changing the characteristics of their products to reduce their substitutability (e.g. Scherer and Ross 1990). The present paper shows that the choice of which strategy matters for the relationship between competition and tax evasion. Our results can be also read in the light of Schleifer (2004, p. 414), who recently shows “how competitive pressures lead to the spread of the censured behaviour”, and he investigates five examples of such behaviours: employment of children, corruption, excessive executive pay, corporate earnings manipulation, and involvement of universities in commercial activities. However, he abstracts from the firms’ tax compliance which would be an adequate case of censured activity and a further good candidate for testing his findings. In this respect, the paper’s suggestion as regards the possibility that competition favours unethical behaviours is mixed: for instance such a finding holds true only if the pre-existing competitive pressure is relatively high. The paper is organised as follows. Section 2 presents the model first under a general demand form with homogenous goods and quantity competition, investigating the effects of marginal costs as a measure of competitive pressure on tax evasion; then, the market equilibrium under price and quantity competition with differentiated products is characterized, showing the link between various competition changes and tax evasion and discussing policy implications. Section 3 summarizes the findings.",1
48,3,Journal of Industrial and Business Economics,04 June 2021,https://link.springer.com/article/10.1007/s40812-021-00190-1,Opportunities and challenges of the industry 4.0 in industrial companies: a survey on Moroccan firms,September 2021,Maryam Gallab,Hafida Bouloiz,Mohamed Tkiouat,Female,Female,Male,Mix,,
48,3,Journal of Industrial and Business Economics,05 May 2021,https://link.springer.com/article/10.1007/s40812-021-00189-8,Monitoring decisions in vertical integration,September 2021,Sonia  Di Giannatale,Itza T. Q. Curiel-Cabral,Jacqueline Chacón,Female,Unknown,Female,Female,"Informational frictions inside firms and the cost of monitoring activities aimed at diminishing the detrimental effect of such frictions affect their performance. For instance, geographical distance of firms from their headquarters, as a measure of difficulty to perform monitoring activities, has been found to have a negative effect on firms? survival rate and profits (Kalnins and Lafontaine 2013; Landier et al. 2009). Moreover, the empirical vertical integration literature has analyzed the impact of monitoring costs, usually measured through variables such as sales series and geographical dispersion of outlets, on the firms? choice of organizational structure; i.e., the decision of whether to vertically integrate. The results on the matter have been mixed: a positive relationship between the difficulty in measuring sales results and vertical integration has been found by Anderson (1985), John and Weitz (1988), and Carney et al. (1991); an inverse relationship between the difficulty in monitoring the retailer’s effort and vertical integration has been reported by Minkler (1990) and Brickley et al. (2003); whereas heterogeneous ownership structures emerge in Lafontaine and Shaw (2005). In this article, we propose a model to formalize a firm’s monitoring and vertical integration decisions in the same environment; that is, in the principal-agent model studied here, the principal (manufacturer) makes the decisions of whether or not to monitor the agent (retailer), and also, whether or not to vertically integrate with the retailer in a context in which the retailer has private information about the project’s outcomes, and both the manufacturer and the retailer have private information about their respective actions. From a theoretical perspective, a hidden information and hidden action model with monitoring in a vertical integration setting contributes to the design of proper incentives for truthful revelation of private information and implementation of optimal effort levels, a modelling approach that has not been used in the related literature and that has the potential of contributing with novel empirical predictions about the interplay between the monitoring and the vertical integration decisions of firms. Formally, vertical integration is defined as an organizational form in which ownership is joint and control rights are integrated (Lafontaine and Slade 2007). Vertical integration can be classified in two categories: the approach that considers the decision to integrate forward, forward into retailing, and the approach that analyzes the backward, make or buy, decision. In this paper, we will focus in the first type of vertical integration. In terms of the principal-agent model, vertical integration corresponds to a situation where the manufacturer and the retailer constitute a firm, the manufacturer assumes all the risk inherent to the production and sales process, and the retailer gets a compensation that is equivalent to the perfect information case; that is, a fixed compensation. On the other hand, vertical separation refers to situations where manufacturers employ sales representatives or franchise arrangements so that the task of selling the product to the final customers is delegated. Thus, the main difference between integration and separation is that the retailer’s compensation reflects that, in contractual arrangements originated from the decision of vertical integration, the retailer bears some of the risk associated with the operation. A relevant question that emerges in this literature is whether vertical integration eliminates the need of monitoring. One perspective states that vertical integration eliminates the moral hazard problem and this view seems reasonable when monitoring costs are low. For instance, Hennessy (1996) finds that in the food sector, in particular in the provision of inputs, quality needs not be verified because it is possible to internalize externalities caused by informational gaps when firms integrate. However, there is another perspective that states that vertical integration has no role in solving informational gaps but only in changing the bargaining powers of the parts involved (Li et al. 2016). However, even when the firms decide to vertically integrate, Joskow (2010) points out that information asymmetries and monitoring costs are still present after the integration because the only thing that varies is the degree of risk sharing between the principal and the agent. From the perspective of agency models, Lafontaine and Slade (2007) present a model with the objective of obtaining conditions under which firms decide to vertically integrate by focusing on the trade-off between incentives and insurance. Their solution establishes that the manufacturer will favor vertical integration when the profits of doing so are higher than in the case of vertical separation plus a transaction cost associated with signing the contract. In this article it is also shown that when costs of incentive provision are higher than profits, the manufacturer will offer a fixed salary, which implies that there is vertical integration. On the other hand, when the retailer absorbs risk and is a residual claimant, there is vertical separation. Regarding monitoring costs, Lafontaine and Slade (1996) develop a model in which there are both direct and indirect effort signals, and how much the manufacturer trusts the retailer defines the choice of organizational structure. For instance, Anderson and Schmittlein (1984) find that, in the industry of electronic components, the probability of vertical integration is positively related with monitoring costs because it is difficult to measure workers’ performance. So, it becomes relevant that the manufacturer is better equipped to infer the retailer’s effort through signals. However, in neither of the theoretical models mentioned in this paragraph, the effect of monitoring costs on the organizational structure choice of firms has been explicitly analyzed. The empirical literature that has studied the effects of monitoring costs on the vertical integration decision has employed signals to approximate monitoring costs, given the difficulty of directly measuring them. For instance, John and Weitz (1988) and Anderson (1985) focus on monitoring based on results or direct signals, while Carney et al. (1991) and Brickley et al. (2003) employ indirect signals of effort such as distance with respect to the monitoring center, inventory density, among others. All of the mentioned papers find significant effects of monitoring costs on the vertical integration decision; however, the direction of the impact is not necessarily uniform. Whereas Brickley and Dark (1987) find that the probability that firms use franchising, as opposed to vertical integration, increases with monitoring costs, the rest of the papers find that higher monitoring costs increase the probability of vertical integration. Also, Minkler (1990) shows that for the case of restaurants, firms tend to adopt vertical separation if the distance between the principal and the agent is such that it is very costly for the principal to verify effort. Whereas when the measure of verification cost is the years of experience with a franchise, evidence shows that the probability of vertical integration increases with time. In a recent paper, Kalnins and Lafontaine (2013) measure the impact of geographical distance on firms’ performance by using an extensive dataset of firms in the state of Texas, and find detrimental effects of geographical distance between headquarters and establishments on both the business survival rate and profits. They consider this as evidence of the importance of information asymmetries and monitoring on firms’ choice of organizational structure. However, they also find that for large businesses (50 establishments or more) geographical distance seems not to affect firms’ profits, but they also observe that these firms tend not to be franchised and use capital-intensive monitoring. That is, this last empirical fact shows that even in vertically integrated firms, monitoring might take place. When trying to model the interactions between monitoring and vertical integration decisions, there have been two distinct analysis strategies in the incentive-based vertical integration literature: the theoretical articles have focused on hidden action models without explicitly modelling a monitoring mechanism, whereas the empirical studies have focused on measuring monitoring costs as part of the variables that determine the vertical integration decision. So, in this paper we develop a hidden information and hidden action model of vertical integration to understand the simultaneous effects of monitoring costs, and two types of signals; i.e., effort and performance signals, on the optimal decisions of both the manufacturer and the retailer. The predictions that stem from our analysis that we consider of potential relevance to the empirical literature of vertical integration can be summarized as follows:  there are vertically integrated firms that undertake monitoring activities and this is the case when the net difference between fixed payments the retailer receives in the case of monitoring versus no monitoring is sufficiently low; in firms where monitoring takes place, the effort of the manufacturer (retailer) is lower (higher) than in firms that do not undertake monitoring activities; the power of the output incentives has a positive relationship with respect to the marginal productivity of the retailer’s effort, and a negative relationship with respect to the marginal productivity of the manufacturer’s effort, the retailer’s risk aversion parameter, and the variability of the output signal; the power of the effort incentives has a positive relationship with respect to the marginal productivity of the retailer’s effort, and a negative relationship with respect to the retailer’s risk aversion parameter and the variability of the effort signal; and the empirical probability of vertical integration depends on the net effect between the marginal productivity of the retailer’s effort and the transaction costs involved in administering a vertical separation arrangement. The rest of this paper is organized as follows: in Sect. 2, we develop a moral hazard model of vertical integration using a hidden information approach, with the explicit inclusion of monitoring costs. In Sect. 3, we extend the model of Sect. 2 by including hidden actions. Finally, Sect. 4 concludes.",1
48,3,Journal of Industrial and Business Economics,27 March 2021,https://link.springer.com/article/10.1007/s40812-021-00184-z,Adaptive stochastic risk estimation of firm operating profit,September 2021,Ahmet Akca,Ethem Çanakoğlu,,Male,Male,Unknown,Male,"Financial modeling of business operations in an integrated approach is crucial to take a snapshot of the company for a specific time and understand the evolution of net income for the future. A stochastic business operating model is an integrated model in which the results of business operations such as revenues, cost of sales, and other profit and loss statement items are allowed to depend on the exogenous financial market variables such as foreign exchange, interest, and inflation rates which evolve stochastically with respect to their marginal distributions. This model can be of significant value in several management applications such as risk management, as shown in Mouna and Anis (2016), corporate finance strategy formulation, and valuation processes, as described in Parlapiano et al. (2017). In this paper, such a model will be developed to estimate the risk measures, including the mean, value at risk (VaR), the conditional value at risk (CVaR), and the standard deviation of the operating profit of a business. Adaptive stochastic modeling of the business results and estimation of the risk measures on the operating profit requires three main steps: (1) develop the adaptively evolving stochastic exogenous financial market, (2) develop the adaptively evolving models to simulate business operating results to arrive at operating profit and integrate the exogenous financial market with the business operating model, (3) calculate various risk measures on the simulated operating profit. To this end, a stochastic exogenous financial market is modeled and parametrized with an instructional, holistic, and transparent approach in which foreign exchange, interest, and inflation rates evolve stochastically with respect to their adaptively built price processes. The dependency among these variables is captured through residual Student-t copula, which has been become popular in financial time series since Sklar (1973) proposed the Sklar theorem. Section 2 lists some of the examples of stochastic models developed and explains why our proposed model adds to existing literature. The second step of the stochastic business operating model is to model the business results. In our study, nine distinct revenue and five non-revenue models, including ARIMA, PCA, and PCR, are proposed to be adaptively selected for modeling the results of business operations, hence the operating profit. Stochastic programming of the business operating models has several applications in the literature. A dynamic stochastic simulation decision-support system generator is introduced in Mentzer and Gomes (1991) for managerial planning. Xu and Birge (2006) propose an integrated corporate planning model in which production and financial decisions are given simultaneously to maximize the equity value. An instructive example is given in Rubin and Patel (2017) on how stochastic modeling improves decision making with regards to invoicing options in the radiology discipline. Yun et al. (2009) try to find an optimal equity ratio for a build-operate-transfer railway project whereby they sensitize the profits of the project to several variables. One more comprehensive application of stochastic modeling is made by Han et al. (2014) in which they try to forecast the cash flows of an international construction project taking the foreign exchange, interest, and inflation rates into account. However, most of these models have a few discrete base/best/worst-case exogenous financial market scenarios, which are developed through a single or a few fixed models. In some of them, the exogenous market scenarios are obtained from outside sources, causing the business model to lack direct integration. As opposed to the discrete scenario-based models mentioned above, our proposed business operating model is allowed to be directly integrated with the simulated exogenous financial market. The final step is to estimate the risk measures from the stochastic business operating model. Out of several risk measures, we can focus on the CVaR defined as the mean of the observations that lie beyond the VaR. The reason for us to concentrate on the CVaR is due to its ability to more accurately measure the downside risk of a portfolio, as shown in Xiong and Idzorek (2011). There are several applications of risk measurement estimates in the literature through a stochastic setup. An application of the conditional copula theory in the estimation of VaR through simulation of a portfolio composed of Nasdaq and S&P500 stock indices is given in Palaro and Hotta (2006). Jondeau and Rockinger (2006) show how GARCH based copula model can be used to estimate the VaR of an international equity indices portfolio. VaR of a foreign exchange rates portfolio is estimated through a copula approach in Jaworski (2006). He and Gong (2009) simulate stock prices and estimate CVaR of several Chinese firms after controlling for aggregated market and credit risk via copulas. Various types of conditional copulas have been used in estimating the VaR of a portfolio composed of two indices in Huang et al. (2009). Vine and Factor copulas have been used in estimating the risk among various economies in Song et al. (2019). In most of these studies, however, the simulation of the asset prices and estimation of the risk measures are limited to portfolios composed of some asset classes. In our study, we propose a method to estimate the risk measures of the operating profit of a business. To the best of our knowledge, we are not aware of any study quantifying the risk measures of the operating profit of a business whose data are allowed to depend directly on a stochastically evolving financial market data whereby dependency is modeled via residual copula functions. We can list our study’s contributions to the literature more specifically as follows: (1) the studies in the exogenous financial market field are generally considered to be proprietary and, therefore, are not published in detail. However, our work gives full detail to the modeling steps. For this reason, we believe that our study is an open source for those who want to do more work in this field. (2) As illustrated in Sect. 2, foreign exchange rates are generally not included in the construction of stochastic markets. In our study, foreign exchange rates are included in the model, taking both non-normality and conditional volatility in returns into consideration. (3) In studies on stochastic markets done so far, the dependency is mostly modeled with a one-way hierarchy or Cholesky decomposition. In the one-way hierarchy method, the multidimensional dependency is not considered. In the Cholesky decomposition method, non-normal marginal distribution dependency cannot be modeled. In our study, the residual Student-t copula method is used as dependency modeling. Thus, multivariate dependencies of the modeled instruments’ non-normal marginal distributions consistent with empirical data have better been captured. (4) Our approach is adaptive in the sense that both the asset price processes in exogenous market and processes in the business operating model are selected based on the predetermined model selection criteria whose steps are (1) running validity checks, (2) running significance checks, (3) and selecting the final model by respecting the parsimony. Although we illustrate the adaptive selection feature using publicly available data of a corporate operating in Turkey, the methodology can be extended easily to other firms and economies. The paper is organized as follows: Sect. 2 starts with reviewing how the stochastic financial asset price models have evolved. How this study complements the existing work is also identified. The modeling process starts with the foreign exchange rates whose returns may exhibit non-normality and mean or volatility dependency. To account for these, skewed-t GARCH modeling is introduced. Next, Rendleman and Bartter, and Vasicek modeling of interest rates are introduced. The reasons for selecting the Vasicek model are explained, and the model setup has been introduced. The final financial variable is the inflation rate that is modeled with a regime-switching mean-reverting process. This section concludes with the consolidation of all price processes, modeling of the dependency among multivariate data with the copula, and presentation of the simulation algorithm for the exogenous financial market. Section 3 introduces revenue and non-revenue business data models and integrates these models with the stochastic financial market to calculate the operating profit. Section 4 illustrates the adaptive model selection methodology for a specific corporation and provides the estimated risk measures. Section 5 concludes the paper.",2
48,4,Journal of Industrial and Business Economics,02 August 2021,https://link.springer.com/article/10.1007/s40812-021-00192-z,The privacy paradox: a challenge to decision theory?,December 2021,Jacopo Arpetti,Marco Delmastro,,Male,Male,Unknown,Male,"Privacy is a sensitive and hot topic both in the policy and economic debate. The value creation triggered by data aggregation and analysis is at the heart of the digital economy. Such business model is made possible by the individuals’ choice to allow the collection of their data in exchange for services offered by online platforms (Delmastro & Nicita, 2019). Such business model is nothing but the essence of the whole “data-driven” economy, which is grounded on the implicit swap of data for services: an exchange that can also result into market failures besides a series of (positive and negative) externalities, as well as lock-in effects targeting consumers. So far, the literature has tried to shed some light on the decision-making processes in the field of privacy by investigating whether the adoption of related choices is dictated by rational reasoning or by some kind of impulsiveness and if such behavior may lead to market equilibria that are inefficient and do not guarantee privacy protection (for a recent survey see (Acquisti et al., 2016)). On the empirical side, the distribution of privacy concerns among population has been also investigated, although the existing literature around privacy preferences and related data disclosure choices is mainly oriented to analyzing individuals’ behavior, without looking into the context’s knowledge degree on which choices are grounded. In this context, results are still heterogeneous and inconsistent. As a matter of fact, the detected dichotomy between privacy attitudes, intentions, and actual behaviors has led to a debate over the so-called “privacy paradox” (Acquisti & Grossklags, 2005; Alessandro Acquisti et al., 2016; Norberg, Horne, & Horne, 2007; Spiekermann, Grossklags, & Berendt, 2001; Taddicken, 2014; Turow & Hennessy, 2007; Turow, King, Hoofnagle, Bleakley, & Hennessy, 2009). Is such dichotomy a real fact or a spurious result? Do people actually care about privacy? And if so, how much? In this paper we intend to answer empirically to (some of) such questions. We start with a consumer’s standard decision-making setting. When individuals provide their consent to transfer their data, they carry out a trade-off weighing, considering whether it would be convenient to provide their personal information in exchange for benefits of another nature (economic or not)( Acquisti, 2014; Dinev & Hart, 2006). However, such cost–benefit evaluations are affected by several individuals’ cognitive limitations which are sharpened by the level of uncertainty and complexity featuring the digital data-swap context, where consumers make their own decisions facing hurdles linked also to the implicit and non-monetary nature of transactions. We therefore tackle the privacy paradox going deep to the roots of the factors hampering individuals’ privacy decision making. First, we consider incomplete and asymmetric information. It is indeed very common for individuals to be unaware of the underlying data collection practices, and thus accept some services’ terms of use involving data ceding, ignoring that the same data could be treated, aggregated, and then subsequently traded to a third party. In order to analyse the individual’s privacy preferences, it should be considered that consumers conduct their assessments in an environment featured by structural uncertainty. Such environment is characterized indeed by the non-monetary nature of transactional trade-offs and by the impossibility for individuals to set a reference point to carry out proper assessments. Moreover, the uncertainty is heightened by almost imperceptible data collection practices which–if not intentionally concealed–keep consumers in the dark on how much data is collected from them, what kind of data they are actually ceding to platforms and with what consequences in terms of further uses by the same platforms (i.e., vertical integration) or by third parties (i.e., data trading). In such digital environment, personal data transactions in exchange for services–and, more in general, users’ choices on data transfers–are strongly affected by incomplete and asymmetric information (Akerlof, 1970) concerning the nature, the amount and the data gathering procedures performed by online players (Alessandro Acquisti et al., 2016). In this context, awareness of privacy risks (or alternatively their unawareness) may indeed significantly affect consumers’ privacy choices evaluation. However, privacy concerns degree, relying on the observation of «consumer’s choices in the marketplace» (taking for instance into consideration users «propensity to share data online or to use protecting technology»), «may not necessarily provide the fuller or clearer picture of what privacy is ultimately worth to individuals» (Alessandro Acquisti, 2014). If this is true, one should first proceed to detect the presence of such users’ information asymmetry (or, the opposite, data awareness) and then, as a second step, analyze the individuals’ privacy attitude. If such procedure is not applied, it is reasonable to assume that the results would be biased, as it would not have been established whether individuals’ choices on data disclosure are due to a significant degree of ignorance, rather than to a conscious decision to reveal personal data (because of a low level of privacy concern and relevant preferences in terms of privacy protection). In order to avoid such distortion, we firstly analyze information asymmetry distribution between the (Italian) population and then–as a second step and after having verified to what extent individuals are aware of transfers of their personal data–we detect which groups and to what extent are privacy-concerned. In particular, we tackle these further privacy issues by introducing in the empirical analysis measures of consumers’ rational as well as bounded decision-making and use (or not) of technical heuristics. In such sense, we propose an empirical approach, to distinguish privacy concern measurement–meaning individuals’ “idealistic” preferences with respect to their own privacy–from individuals’ data awareness, considering that individuals’ behaviour is heavily shaped by their needs (e.g., cost-benefits assessment), as well as by the context (the digital one, in this case) in which they are plunged. In such sense, we emphasize how information asymmetry affects individuals’ choices, and a misalignment between individuals’ declared high privacy and their actually poor privacy behaviour. The paper is based on an empirical investigation that we carried out in Italy at the end of 2018 on a large sample of consumers (10,004). The target was composed of all the Italian population (aged between 14–74), and the aim was to disentangle the abovementioned factors that may influence consumers’ privacy preferences, trying to reconcile attitudes, intentions, and actual behaviors. Hence, the article is organized as follows. In paragraph 2, we sum up the existing empirical evidence on consumers’ behavior in terms of privacy preferences and concern. In paragraph 3, we set out the study design, describing both the experiment’s nature (and how its setting allows bias control and comprehension) and sample’s characteristics. Paragraph 4 illustrates the results of an econometric model which takes into account the sample selection via a three-stage binary filtering (“smartphone usage”, “data awareness–information asymmetry filtering” and “privacy concern”). In paragraph 5, we provide our conclusions and propose some final remarks.",3
48,4,Journal of Industrial and Business Economics,06 May 2021,https://link.springer.com/article/10.1007/s40812-021-00188-9,The rationale for listing on equity crowdfunding: actual and expected benefits for companies,December 2021,Francesca Di Pietro,,,Female,Unknown,Unknown,Female,"Crowdfunding is becoming increasingly important in the entrepreneurial ecosystem (Block et al., 2017, 2018; Hornuf et al., 2018; Di Pietro, 2020) for making finance more accessible to early-stage entrepreneurs (see e.g., Agrawal et al., 2015; Mollick, 2014; Walthoff-Borm, Schwienbacher, et al., 2018). Equity crowdfunding can be defined as a method of financing involving the entrepreneur selling shares in the company to a group of (small) investors through an open call for funding on an Internet-based platform (Ahlers et al., 2015). It represents an alternative source of funding for start-ups and small and medium-sized enterprises (SMEs) which the regulators consider when formulating their economic development policies for small businesses (Cumming et al., 2016). In 2017, in Europe (excluding the UK) equity crowdfunding reached a significant level in terms of investment amount, with more than €200 million transactions recorded. In the UK investment amounts reached around €300 million in the same year (Ziegler et al., 2019). The growing role of early-stage financing in the form of equity crowdfunding has been matched by interest from the academic community. Research on this topic includes investigations of the factors associated to crowdfunding campaign success (e.g., Ahlers et al., 2015; Bapna, 2017; Di Pietro et al., 2020; Mahmood et al., 2019; Mochkabadi & Volkmann, 2020; Piva & Rossi-Lamastra, 2018; Signori & Vismara, 2018), and studies of entrepreneurial firms after the launch of equity crowdfunding campaigns (see Vanacker et al., 2019 for a review; Ahlstrom et al., 2018; Walthoff‐Borm et al., 2018a). Extant work focuses mainly on the supply side of crowdfunding (i.e., investor behavior) and overlooks what drives entrepreneurs’ demand for crowdfunding in general (Belleflamme et al., 2014; Gerber & Hui, 2013) and for equity crowdfunding in particular (Ahlers et al., 2015; Brown et al., 2018; Wald et al., 2019). The commonly accepted view is that crowdfunding is used when other sources of funding are not accessible (Blaseg et al., 2020; Walthoff-Born et al., 2018b). However, since the extent to which the demand for crowdfunding is a function of the inability to access traditional sources of funding is unclear, there are grounds for arguing that crowdfunding is not merely a “last resort” (Walthoff-Born et al., 2018b), and that there are cognitive factors which shape entrepreneurial decision-making (Bruton et al., 2015; Fairchild, 2011; Fraser et al., 2015), and preference for alternative funding. The importance of equity crowdfunding for boosting the entrepreneurial ecosystem in many countries (Ziegler et al., 2019) points to the need for more qualitative research (Frydrych et al., 2014) to provide a better understanding of the rationale for choosing equity crowdfunding to raise funding (Fairchild, 2011; Fraser et al., 2015). We need a better understanding of its drivers and actual and expected benefits. The question we address in this paper is: What are the actual and expected benefits for entrepreneurs listing on equity crowdfunding sites? We investigate this research question using a comprehensive qualitative study and relying on primary data collected from 38 interviews with entrepreneurs who successfully fundraised via two UK equity crowdfunding platforms, Crowdcube and Seedrs, during the three-year period 2012–2014. Our findings suggest that in addition to the financial benefits it provides, a successful fundraising campaign reduces uncertainty regarding market acceptance of the new venture’s product, and potentially demonstrates to outsiders the value of the business. Crowdfunding performance serves to confirm future market demand which acts as a quality signal to external stakeholders and facilitates access to additional financial and non-financial resources (Brown et al., 2018; Spence, 1973; Vismara, 2016a; Wald et al., 2019; Walthoff-Borm, Schwienbacher, et al., 2018). Additionally, our study illustrates that crowdfunding is seen as a “diluted” form of equity funding with fewer and less prohibitive limitations on entrepreneurial autonomy than in other forms of equity funding (Hellmann & Puri, 2002; Kaplan & Strömberg, 2003; Sapienza et al., 2003), and that entrepreneur’s choice of crowd financers is influenced by the possibility of a more informal and collaborative entrepreneur/investor relationship. This article contributes to the crowdfunding literature, and by taking a demand-side perspective (i.e., the entrepreneur perspective) on accessing finance, sheds light on the reasons for seeking equity crowdfunding. Our study also adds to the entrepreneurial finance literature by showing that entrepreneurs’ choices of different financial channels (alternative and classical) are interlinked and that horizontal agency costs are an important factor driving these choices.",8
48,4,Journal of Industrial and Business Economics,05 May 2021,https://link.springer.com/article/10.1007/s40812-021-00187-w,Forging a new alliance between economics and engineering,December 2021,Sergio Mariotti,,,Male,Unknown,Unknown,Male,"Economists and engineers have played a vital role in the evolution of our modern society. The related disciplines have intertwined with each other, leading to mutual cross-fertilization. We take a look into history to better understand this nexus and to trace its possible future in a society increasingly influenced by the cluster of organizational and market innovations induced by Artificial Intelligence (AI) technologies. In doing so, we leverage Cornish's (2004, p. 234) assertion that “futuring can be thought of as the art of converting knowledge of the past into knowledge of the future”. The history of the relationship between economics and engineering has a beginning that could be surprising to some people. In contrast to the tradition that associates modern microeconomics with Alfred Marshall, in their book Secret origins of modern microeconomics: Dupuit and the engineers, Ekelund and Hébert (1999, pp. xi; 11) argue that: “microeconomics as we now know, was developed first and foremost by engineers rather than economists”, and that “its origins were French rather than British”, as well as: “French econo-engineers, and a few kindred 'foreigners' who were drawn into their orbit, were not merely forerunners of neoclassical microeconomics: they were its inventors”. During the nineteenth century, these scholars had somehow taken over the heritage of the Renaissance man, who did not disdain to be interested in many areas of human knowledge. In other words, men who at the same time resembled the profile of William Petty (philosopher, physicist, economist, mechanical inventor) and benefited of the French engineering tradition to tackle practical problems from a theoretical and mathematical-formal point of view. Among them, the most eminent exponent was the civil engineer Jules Dupuit, considered a founding father of the neoclassical economics and, indeed, the one who shaped the very essence of the Marshallian economics. Ekelund and Hébert (1999, pp. 11; 271) claim that Dupuit’s “view of economic analysis was perfectly holistic, encompassing both the private economy and the public economy”, and that “École Nationale des Ponts et Chaussées formed a kind of intellectual cocoon from which emerged virtually the entire superstructure of modern microeconomics”. Beyond the dispute over whether or not the French “econo-engineers” formulated the neoclassical model a generation before Marshall, it is common belief that Dupuit, along with Cournot, von Thünen, and others, anticipated key aspects of the marginalist revolution, and that French econo-engineers hold a notable position in the historiography of economics thanks to their seminal development of new tools of economic analysis. More important for our reasoning is to underline the monolithic approach followed by these precursors, in which engineering and economics were inseparable. However, in accordance with the Plato-inspired Smithian theory of the division of labor (McNulty, 1975), economics and engineering have become institutionalized, deepened, articulated and increasingly separated both professionally and academically. The two professions have never lost sight of each other (Duarte and Giraud 2020; Morgan, 2003), but with growing separation and coolness, to the point of seeming even in conflict since the early twentieth century. In 1935, Hayek (1935, p. 8) stated the problem in the simplest terms: “It is probably no exaggeration to say that to most people the engineer is the person who actually does things and the economist the odious individual who sits back in his armchair and explains why the well-meaning efforts of the former are frustrated”. The Austro-British economist claimed that this was not true and the conflict absurd. However, this common thinking has lasted (Rashid, 2001) and gives us an idea of the intricate and tormented relationship between economics and engineering that has been established in a scenario made more complex by scientific advances, new technologies, and other human achievements. Indeed, the separation between the two disciplines persisted for a long time, albeit with moments of intertwining. The period of the Hayek's writing was characterized by a crucial discussion on the nexus between the two disciplines and the following years marked important changes, which we will describe in the following section in relation to three main paradigms. The epistemology of the two disciplines has also changed. This historical process has been strongly influenced both by the society's demand for solutions and interventions to address recurring economic crises and new analytical and design tools available thanks to advances in mathematical, physical, and natural sciences. Understanding these events is essential to analyze the future of the economics–engineering nexus. Our aim is not to study this evolution per se, but to understand if it is up to the challenges and opportunities posed by the cluster of innovations that will shape the future society and increase its complexity. Our thesis is that the current social, technological and economic conditions call for a new alliance à la Prigogine, i.e., an alliance that brings these two disciplines separated over time—the hard science (exact, technological) versus the soft sciences (social, historical)—to a unitary and coherent vision of reality, which results from the integration of complementary economic and engineering constructs, the protection of plurality, and the determination to understand and tackle possible tensions (Prigogine & Stengers, 1984; Wallerstein, 1991). However, while the past was written in ink, the future is written in pencil: the evolution of disciplines is by no means a deterministic process and the different possible trajectories and the social factors selecting them must be understood and discussed.",12
48,4,Journal of Industrial and Business Economics,03 October 2021,https://link.springer.com/article/10.1007/s40812-021-00194-x,Economics not engineering,December 2021,Richard Adelstein,,,Male,Unknown,Unknown,Male,"There is much to praise in Sergio Mariotti’s learned and insightful essay. It usefully reminds us of the intellectual and methodological debt owed by neoclassical microeconomics to Jules Dupuit and the nineteenth-century French engineering tradition. It elaborates an illuminating trichotomy of possible relationships between economics and engineering and, most provocatively, calls for a “transdisciplinary” integration of the two disciplines, one that would alter the epistemology and methods of each so as to “restore a unified perspective of knowledge and [put] the study of complexity in the foreground” (Mariotti, 2021, Abstract). Perhaps inevitably, this last proposal is vague and undeveloped, suggestive rather than substantive, though its elevation of technological and economic complexity to center stage in the new alliance is a major point in its favor. But despite the intellectual synergies such a marriage might bring, and the new tools for control of individual behavior and social outcomes it might offer firms and governments, any attempt to unify economics and engineering must confront a fundamental dissimilarity of purpose between, on the one hand, every science, physical and social alike, and all the engineering disciplines on the other, a distinction with critical moral and political implications that counsel us to approach this particular transdisciplinary enterprise with great caution.",1
48,4,Journal of Industrial and Business Economics,08 October 2021,https://link.springer.com/article/10.1007/s40812-021-00200-2,Facing the complexity of the economy: an opportunity for the new alliance between economics and engineering,December 2021,Silvano Cincotti,,,Male,Unknown,Unknown,Male,"Sergio Mariotti (2021) explores the relationship between economics and engineering. The strategy is, first, to start from the historical perspective on the relationships between economics and engineering and, second, to identify the paradigms for the economics–engineering nexus. The relationships between economics and engineering is a question rather than trivial. The literature offers a long history of contributions, from Hayford (1917) to Morgan (2012) and more recently Duarte and Giraud (2020) and Blockley (2020). Mariotti provides a review of the different contributions and, in addition to being enjoyable and informative, Forging a new alliance between economics and engineering offers a reconstruction of the dynamics of such interaction and of its complexity. Three paradigms are proposed: (i) economics for engineering; (ii) economics and engineering; (iii) economics as engineering. The “economics for engineering” paradigm has a long tradition and Mariotti helps us in understanding the motivations behind such paradigm and in reconstructing the successful experiments started in the US in the second half of the XIX century and then spread worldwide in global and shared experiences. The latter paradigm (i.e., “economics as engineering”) underlines the positivism of a scientific approach to economics based on the (hidden) assumption that economics is different and must be distinguished from the social sciences. Mariotti remarks that this is just one of the several metaphors than can be used to sketch the needs of solutions that permit “engineering” applications in economics and (more often) in finance. Several Nobel Prizes winners can be described as “economic engineers”: Harry Markowitz, Merton Miller, and William Sharpe won in 1990 for “pioneering work in financial economics”, Robert Merton and Myron Scholes in 1997 “for a new method to determine the value of derivatives”, Lloyd Shapley and Alvin Roth in 2012 “for the theory of stable allocations and the practice of market design” and Paul Milgrom and Robert Wilson, in 2020, for “improvements to auction theory and inventions of new auction formats.” It is worth remarking that beside these examples that have reached the degree of durable consensus as economic engineering, one can recognize the economics as engineering paradigm also in the need of theoretical models aiming at solving the central problem of policy making, but in fact leading to an excess of confidence in classical theoretical models. This open relevant questions and will be addressed in the following Sections. Finally, the paradigm “economics and engineering”, that Mariotti represents as “a meeting between peers, respecting the disciplinary singularities and the different cultures, but in a context of cross-fertilization and sometimes of interdisciplinarity” (Mariotti, 2021, Sect. 2). «How dare you?» might wonder some orthodox economists, but in fact cross-fertilization and interdisciplinarity are ordinary in science. Moreover, in Forging a new alliance between economics and engineering Mariotti prompts out that cross-fertilization, interdisciplinarity and multidisciplinarity have been going always in the relationship between economics and engineering. Furthermore, they are strengthening as a consequence of the growth of the complexity approach to science and this will be addressed in the following Sections.",2
48,4,Journal of Industrial and Business Economics,21 September 2021,https://link.springer.com/article/10.1007/s40812-021-00195-w,"Engineering, economics, Heidegger … and Mariotti: a note",December 2021,Adam Fforde,,,Male,Unknown,Unknown,Male,"Mariotti (2021) suggests that economists and engineers profitably discuss matters of possible mutual interest. I think such discussions are likely to be challenging. The two disciplines have very different methodologies, which is one problem; the bigger one is that economists are deeply confused about important elements of their own methodologies, and the differences between these and those deployed by engineers. This inhibits likely useful change. But it is possible. Time will tell. Although, so my brothers say, I am not actually an engineer by nature, I studied it, at university (BA), before moving into economics and then Vietnam studies, and I have considerable interest in methodology. In some ways this interest comes down to the puzzle: why do ‘these people’ (not my family, rather various professionals apparently well-trained in a social science, such as economics) say these things—do they really believe them? Apparently, they do; at least they are required to, professionally. I first really noticed this when, after MSc, PhD, and post-doc work (the latter two on Vietnam, where I studied in 1978/1979 and 1985/1986), I was an aid adviser based in the Swedish Embassy in Hanoi (1989–1992) and kept meeting people in good suits from bodies like the World Bank, who seemed so certain … Speaking Vietnamese and with years of experience working on the country, much of what they said could easily be shown to be rubbish. Which of course they disliked and so avoided me. This set me off on a parallel track of interest in ‘method’, or social epistemology. A student of mine when I was teaching development turned out to be a natural science major, and I remarked that social scientists seemed to think that natural science was about ‘finding the truth’, and how odd and incorrect this belief was; he agreed. To change register, I regularly lunch with a colleague, of Italian/Hungarian extraction, and we talk about this and that. Whilst he is an economist, his father was a marine engineer—ship-based, originally (in Italy) a fitter—and his maternal grandfather a watch-smith. Both, thus, were practical men, in their different ways. Last time, we were puzzling, over a rather good meal, whether his father could well be said to have had in his head an implicit ‘big model’ when he did engineering. This problem we have not yet solved … However, it reminds me of Heidegger, the great German philosopher, and his essay The Question concerning technology,Footnote 1 in which he discusses “the essence of technology” (p. 3).Footnote 2 After a lot of stuff, and before the essay finishes, we find the line of argument pushing to assert that “the essence of technology {is} to do with revealing” (Heidegger, 1977, p. 12), and, revealing what?—“This revealing gathers together in advance the aspect and the matter of ship or house (p. 13, stress added), so (with gathering momentum) “Whoever builds a house or a ship … reveals what is to be brought forth” (p. 13). And then, the denouement—“technology is a mode of revealing” (p. 13) a “kind of unconcealment” (p. 17)—therefore “Modern science’s way of representing pursues and entraps nature as a calculable coherence of forces”, “mathematic physics arose … before technology” and could only get “under way … when it could be supported by exact physical science” (pp. 21–22). This is rather fun, for it states, in its almost baroque language, the common belief that “the economist as an engineer not only searches for the fundamental laws that govern economic behavior, markets and organizations, but intervenes …” (Mariotti, 2021, Sect. 2; stress added). This would perhaps also imply that there is always an implicit model to what engineers do, an expression of ‘fundamental laws’. The most basic challenge to this very common view—of a search for fundamental laws—is the use of what could be called ‘trial and error’, or, more specifically, design that is accepted as good enough, but where important parts of the design rest upon known predictive patterns that have no underlying causal account. Put in another way, we reliably know with good enough prediction what will happen, but we do not have an analytical model, so the parameters of the specific are not plugged into a model but looked up in a relevant database. For engineering, this can be shown by empirical example, and I think it self-evident that this is in a sense a ‘proof by induction’, as the existence of one example, which is ‘quite normal’, shows the general point. There do not have to be fundamental laws, if these are causal (as any formal model is likely to be). The example is crack propagation speed. This first became widely known as an issue when early jet airlines were found to crash because their structures were found to have hidden cracks within them, caused by repeated stress patterns, and when these got too big the machine failed (it crashed). Methods were devised to search for such cracks, which helped for a while, but then knowledge was developed that solved the problem in the sense that design and construction of airplanes based on it was socially accepted (insurers would insure the relevant parties at acceptable costs). To quote (myself and an expert—not me): “… crack propagation speed, {is} a matter of crucial importance in the engineering of things like airplanes. Here, so far as I know, mathematics does not work—the regularity cannot be modelled. Rather, any modeller simply says to a computer, in effect, ‘pretend to be metal’.Footnote 3 Thus: Aircraft fuselage structure is a good example of structure that is based largely on a slow crack growth rate design. … The rate of fatigue crack propagation is determined by subjecting fatigue-cracked specimens, like the compact specimen used in fracture toughness testing, to constant-amplitude cyclic loading. The incremental increase in crack length is recorded along with the corresponding number of elapsed load cycles acquire stress intensity (K), crack length (a), and cycle count (N) data during the test (NDT Resource Center, 2013). In such contexts, one would not expect uninsurable risk (good money is made by insurance companies selling insurance to those who buy and produce airplanes). Because an analytical model entailing causality is not needed, the question of whether there is causality in this sense is moot. Causality, then, embedded in a predictive model, is not necessary for accredited members of the epistemic community responsible for airplane design to assert reliably predictive knowledge or its absence, and so obtain or deny a license to fly the aircraft” (Fforde, 2017, p. 129). This experience could be seen as a massive stroke of luck, for those benefitting from air travel: there is no a priori reason to suppose that such regularities can be found anywhere. Engineering does not need fundamental laws for it ‘to work’ (and get paid). Heidegger is also plausibly quite wrong in his assertion that modern physics predates technology. The—according to some—father of the historical study of technology, Lynn White, wrote very early in his career a study of technological change in the ‘West’ after the fall of Rome (Roman engineers were widely admired) and before the beginnings of modern science (predictive testing of deductions) with the work of Grosseteste (Crombie, 1953; Whyte, 1940). White points out what is surely obvious, which is that before any sort of ‘modern’ use of deductive predictive testing of models had started, there were some major changes in available technology easily deemed to be advances. These can be seen as based on trial and error, but more importantly they were not based upon knowledge of fundamental laws expressed in theory. And the idea of using Roman numbers to carry out formal modelling should strike terror. As White puts it: “the technical skill of classical times was not simply maintained: it was considerably improved” (White, 1940, p. 151)—but with none of the apparatus of modern scientific method. The conclusion seems robust: engineering cannot be said to be about, in essence, ‘fundamental laws’. I will not even think of attempting a discussion of the history of modern scientific method. However, I am struck by three aspects of Mariotti (2021). I start from the quote above. I note in passing two prestigious economists (Friedman, 1953; Stigler, 1947) who assert that scientific method requires generation of theory with predictive power. From this it follows that economics is not a science, and, if based upon trial and error, neither is engineering. First, what can usefully be said about a predictive criterion? I am reminded of the quote from Nevil Shute, in his book Slide rule, that “an engineer is a man who can do for ten shillings what any fool can do for a pound” (Shute, 1954, p. 66). It seems self-evident that in engineering practice there is no absolute answer to the question when and where a theory has predictive power. Whether a model is deemed to have it depends upon what is wanted: this depends upon measurement accuracy (and its reliability and cost) as well as questions like whether ‘being a metre out is important or not, or has it to be right to a cm’? This combines Lakatos’ view the data has measurement theory associated with it (to give it meaning), with a sense that meaning is not inherent in the data, but contingent (Lakatos, 1970). There is no ‘truth’ here, at all. This reinforces the point just made about trial and error. What can usefully be said about prediction, therefore, is that, if present, it is a rule that may or may not be applied within knowledge assessments to decide whether a theory is acceptable. It follows that, in its absence, criteria outside knowledge assessments (I like to call this ‘The Prince’) will choose … Economics, it is obvious, does not have a practice that requires theories to be tested by their predictive power. Second, what can be said about the issue of ‘complexity’? Mariotti does not define what he means by the word, but it seems to be a characteristic ‘of reality’—“Our aim is … to understand … the challenges and opportunities posed by the cluster of innovations that will shape the future society and increase its complexity” (Mariotti, 2021, Sect. 1). Whether it is plausible to judge that society is becoming more—or less—complex depends on many factors, but there is a distinction between such judgements and the arguments made to support them: the extent to which models should be complex or not. It is easily argued that complex situations can be modelled acceptably with a simple model, or a complex model. The question then is, how are such choices made. To quote, again (myself): “… management of complexity is … related to the management of theorisation—of model choice. Therefore, what is meant by complexity is actually how to deal with situations that are stubbornly messy, and sometimes these are not complex at all, just very hard if not impossible to model predictively. For example, a pendulum swinging to and fro can be thought of—modelled or theorised—as complex or simple. Standard Newtonian models treat the thing the weight is suspended with as a line of no thickness, and the weight of the bob as concentrated at its centre of gravity. This is a simple model. With simple assumptions such as these, and with an accuracy required to be within seconds a month, acceptably high accuracy can be obtained, as we know from grandfather clocks. In that context, the system is modelled as simple. But this does not necessarily mean that we should believe that reality is simple. If we want to do something else, perhaps requiring far greater accuracy (and here accuracy is a socially constructed term), then a more complex model could be used, perhaps far more complex, to include modelled effects of motion through the air, changes in temperature, how the suspension may bend and flex, and so forth. Then a model with third- or fourth-degree differential equations—or higher—with many variables could be used. The model may or may not give the same solutions from the same starting point, depending on its characteristics. And it may not generate a simple prediction about how the thing changes. That is how algebra can be made to work—one basis for how models are chosen. So, statements about complexity, above all, are statements about differences between models, where model selection is related to particular contexts and what people choose to do when they model them. They are about the model, not necessarily about reality. Whatever model we choose, the pendulum is the same thing” (Fforde, 2017, pp. 7–8). This argument further reinforces the point that, like empirics, getting paid for designing and building a thing that does enough of what the buyer wants it to do for them to pay the invoice (my sense of what engineers do) encourages choice over models that takes account of context. This may be wrapped in pleasant talk about truth, hopefully stopping the client from excessive examination of how it was actually done, but if so, that truth varies from context to context, customer to customer, and so, surely it is not useful for us to call it a search for ‘fundamental laws’—rather, a practice that is good enough to get paid. Complexity is then better seen as a perhaps characteristic of models, not reality.Footnote 4 From this perspective, it is understandable but probably wrong to see that the failure of economists’ models to secure predictive power can be explained as an “admission that agents are heterogeneous, rationality bounded and not perfectly informed has paved the way to a new age in economics, the ‘complexity era’” (Mariotti, 2021, Sect. 4.1). In some sense, they always were. The bob of a pendulum is never, really, such that its centre of gravity is at its geometric centre: but it can work to assume that it is (or not, depending). It is obvious that a priori arguments about the extent to which this is the case, or not, reflect deep aspects of methodology: model choice was, and remains, determined by various factors, but predictive power is not one of them. If it were, the point would be about the degree of heterogeneity (etc.) related to what the model is meant to do when it predicts. A priori, humans can be said to be simple, complex, or somewhere in between; with a predictive criterion, and something to predict, an a priori answer is silly, but also revealing. Third, what does this imply for relations between economists and engineers. As an applied researcher, I am interested in the data in Mariotti that talks to this. I would expect there to be frictions. As a citizen, it gets far more interesting (see next Section). Mainstream economists, with their experience that model choice is not done by assessing predictive power, but by ‘what the Prince likes’, have powerful investments in certain forms of modelling. For example, whilst standard theory states that if there is joint production or own consumption and so constrained optimisation cannot show how markets work, and therefore fulfill economics’ ideological responsibilities, what happens if engineers happily deploy their mathematical skills to show that, yes markets won’t work, but we can do this instead? What if the this implies a rejection of behaviourism, a push to endogenise model choice by, in effect, asking those modelled what they think? This, of course, rejects the underlying instrumental rationality hypothesis that is clarified by North (1995), discussing Simon (1986): If {instrumental rationality is assumed} … it is possible to predict the choices that will be made by a rational decision-maker entirely from a knowledge of the real world and without a knowledge of the decision-maker’s perceptions or modes of calculation (North, 1995, p. 49, stress added). In other words, the assumption is that understanding humans means treating them like non-sentient machines: their subjectivity does not matter. This may not work, in the sense of designing and building something suitable. A possible example is the use of non-market methods to secure allocative efficiency through local institutional design, unintended to reflect ‘fundamental laws’, and not expressed in that way. An example is Finnish educational design (Aho et al., 2006, p. 9), where “flexible accountability structures that place a strong emphasis on trusting schools”. My sense is that this is part of a system that is usefully seen as a means of securing allocative efficiency through non-market mechanisms: “Teachers also create their own ways to measure student progress, and learning-oriented assessment is an integral part of daily school life.” (Aho et al., 2006, pp. 9–10). And “grades are prohibited by law” (Aho et al., 2006, p. 131). They do not do this because they are unaware of the power of markets (for those parts of their economy perform well). Given this, it would seem to me that economists would face great pressure not to go along with what engineers would see as an obvious path forward. These pressures would combine methodological tensions (caused by the unfamiliar) with a deep-rooted professional tendency to like markets and the instrumental rationality hypothesis. There is very little evidence that this affection, which has never methodologically been founded (unlike engineering practice) upon predictive power, can respond well to working in contexts where, to get paid (rather than being liked by the Dean and those who control the top journals), model choice must be linked to the ability to ‘design and build and get paid by results’. I suppose my argument is that model choice is very different in the two disciplines. It seems to me that Mariotti, 2021 shows us that, in the history of economics, engineers’ creative approach to models—what might work?—led them to engage with economic theory, in model construction. But the next step—will it work?—is not one that economic theory has much experience with. So, when Mariotti (2021, Sect. 4.1) writes that “the two disciplines, although interacting with each other, do not have a shared and coherent epistemology of complexity yet”, it is his account of this historical experience that comes to mind. From an engineering standpoint (to indulge in heroic generalisation), economics is that familiar thing, empirically untested theory—lots of them about, easy to generate, more complex ones tend to be clumsy but may be useful; the next question, though, is what can be built with it. As my discussion of crack propagation theory showed, no theory is needed: it can be useful, after all the data created required a lot of work repeatedly bending bits of metal and watching what happened, but it is not necessary. From this perspective, the suspicious observer (me) hypothesises that economists’ interest in complexity largely reflects an interest in expanding the range of models that they can construct and get paid for. One of the few studies that I know of that looked at model choice by professional academic economists (Yonay & Breslau, 2006, p. 5) reported that: “The truth of economic statements is thus the product of economists’ success in enlisting the support of other economists, data, whole economies, mathematics, and other agents, rather than adherence to an established and rule-based method”. And: “What is distinctive about model-building in economics is the process that mediates between the microworld [the economic models] and the ostensible object of the research. Rather than involving scientific instruments or data-gathering procedures, this mediation is accomplished by vaguely defined but generally accepted conventions regarding the movement from reality to models” (Yonay & Breslau, 2006, pp. 33–34, stress added). Prediction, defined contingently, requires sufficient lack of vagueness for results to be felt to be sufficiently accurate. In the term ‘sufficiently’ there is powerful discipline: insurers will (one hopes) not sell insurance policies for aeroplanes based on ‘vague conventions’. But one should not forget the element of good luck in the experience that, in some cases but not all, adequate regularity in data can be found, to believe that models are predictively powerful. Thus, it seems to me that as things stand there is no “shared conceptual framework” (Mariotti, 2021, Sect. 4.2). Let me finish, before exploring a shift in stance from ‘researcher’ to ‘citizen’, by considering the issue of the instrumental rationality hypothesis. Santos (2009, p. 71) goes to the heart of the matter: “Experimenting in the social sciences is informed by the trade-off between the exercise of control over the actions of the experimental participants and the potential to provide understanding about human behavior. Control is a requirement of the experimental method to produce pertinent and intelligible results for scientific inquiry. But the more control is exercised the more the experimental results are the outcome of economists’ actions. Economic experiments must therefore achieve a difficult balance. They must elicit intelligible behavior while ensuring that the actions of those taking part in the experiment are not determined by the design set-up and the rules of the experiment”. It follows from this that an experiment can conceptually be jointly owned—that is the models, the practices—by both the experimenter and the subjects of that experiment. As a practice, Santos’s insight is that the lack of control generates valuable results: the experimental trade-off is between control and human agency. As mentioned above, economists’ standard behaviourist approach, which entails the instrumental rationality hypothesis, requires that there be only one valid view of reality—that of the modeller. How then, perhaps in a discussion of how non-market methods of resource allocation can be discussed, does the economist cope with difference? As Mariotti’s historical analysis shows, engineers’ flexibility with modelling was part of their historical contribution to economics. As they had their next step—so what it can be used to build?—one can imagine an engineer sitting down with their experimental subjects (the technicians?) and discussing the value of the model. This suggests that any “new alliance between engineering and economics” (Mariotti, 2021, Sect. 5) will put economists into a difficult place: if markets cannot work (because of joint production or own consumption), and so for-profit businesses treated as unsuitable (aged care?), then will they be treated as heterodox and unsound, unable to publish in the top journals? Will their mathematical techniques, honed to explore new areas of ‘behaviourism with algebra’ give them anything to say that people want to hear? As an engineering undergraduate, confronted with economics whose teachers presented to us, proud of the algebra and rigour, I am and my friends often found it overblown: first, the maths was very simple; second, they never said what could be done with it. As Mariotti explains, many engineers nowadays are taught ‘Microeconomics 101’. These large classes, where they rub shoulders with commerce students, generate large incomes for economics departments, but in many universities these large numbers then drop off sharply as courses are offered in industrial economics, development economics, etc. And, for all the search for new areas in which to model and publish, labelled ‘complexity’, or whatever, the core elements of the curriculum taught to engineers and commerce students hardly change, despite nasty experiences such as the Asian and Global Economic Crises: “The coursework for the economics major has not changed substantially since 1980” (Allgood et al., 2015, p. 317). As somebody who is close to engineering in many ways, this is astonishing, only understandable with reference to some of the arguments above. But it is more than astonishing. One the one hand, does it suggest that engineers are lucky, in that in important areas adequate regularity can be found to support predictively powerful theory, and does that mean that economists, clearly not facing such regularities, are unlucky?",2
48,4,Journal of Industrial and Business Economics,26 September 2021,https://link.springer.com/article/10.1007/s40812-021-00199-6,"Design, systems approaches, and the engineering-economics nexus",December 2021,César García-Díaz,,,Male,Unknown,Unknown,Male,"Both economics and engineering develop knowledge aiming to improve the working of systems—be them physical or social—but with different epistemological frameworks. Economics is a social science, and economists have traditionally conceived themselves as scientists (Mazzoleni & Nelson, 2013) despite requests to adopt a more pragmatic approach (Mankiw, 2006; Roth, 2002). Economists essentially work through the discovery of regularities in social systems by formulating theoretical frameworks and their subsequent empirical testing. As Lazear (2000, p. 99) argues, “[e]conomics is not only a social science, it is a genuine science. Like the physical sciences, economics uses a methodology that produces refutable implications and tests these implications using solid statistical techniques”. In contrast, engineering fields are problem-oriented disciplines that do not develop but employ fundamental scientific principles from the physical sciences; engineers expand their knowledge through practice along the iterative nature of artifact design. The creation of artifacts that fulfill a specific objective is at the core of the engineering profession. Recalling Theodore von Kármán’s quote, “[s]cientists study the world as it is; engineers create the world that has never been.” (Chen et al., 1975, p. 340). These epistemological differences come out when assessing the quality of produced knowledge. As Marks (2012) puts it, the derivation of necessary and sufficient conditions is very important to the mainstream economists’ equilibria formalization. Engineers’ main objective is solving problems through artifact design that comply with a “targeted functionality”. Distinct approaches toward the use of research tools may exemplify such differences. For instance, when exploring properties of designed systems, engineers may use computer simulation to assess reliability levels; theoretical economists, in contrast, rarely employ computer simulation as a primary tool for theory development. One reason is that simulation models can guarantee sufficiency, but not necessity (Marks, 2012). Despite the above disagreements, engineering and economics have many commonalities. In the next paragraphs, I will briefly present these commonalities and differences by borrowing the framework set out by March (2014). March specifically comments on microeconomics and argues that the (sub-)field is featured by two distinct projects. The first project is intimately linked to intelligent choice and definitively amenable to extensions of the neoclassical sort; the second project acknowledges the fact that choice is embedded in rules, identities, institutions, and constantly evolving organizations. Recalling March and Olsen (1996, 1998), these two projects emphasize different logics of decision making: the “logic of consequences” (LoC) and the “logic of appropriateness” (LoA), respectively. They also serve as a ground to examine the potentials and difficulties of engineering and economics to forge alliances. There are obvious reasons for a linkage under a LoC framework, but not so along the other path.",2
48,4,Journal of Industrial and Business Economics,23 September 2021,https://link.springer.com/article/10.1007/s40812-021-00198-7,Prospects for a Prigogine alliance between economics and engineering,December 2021,Robert F. Hébert,,,Male,Unknown,Unknown,Male,"The historical alliance between economics and engineering has been explained and documented by numerous scholars; and has long been recognized for its vital role in addressing the challenges posed by economic and technological progress. But Sergio Mariotti doesn’t consider the traditional alliance up to the challenges (and opportunities) of the digital age, which, in its advocacy of artificial intelligence, introduces more complexity than the old alliance is presumably equipped to handle. He therefore champions a new alliance, formed in the spirit of a conjunction proposed by Russian Nobel laureate Ilya Romanovich Prigogine (1917–2003), between the natural and human sciences; the object being to “restore a unified knowledge, based on plurality, diversity and multiple perspectives.” Mariotti (2021a) sets the stage by identifying three historical paradigms that mark the current economics-engineering nexus. He recognizes: (1) economics for engineering; (2) economics and engineering; and (3) economics as engineering. He rejects all three paradigms as inadequate to deal with a future world of increased (and increasing) complexity. In a supplemental work Mariotti (2021b) described his quest and modus operandi: “I explore what this [alliance] would mean if we focus on two disciplines—economics and engineering—in the context of one complex problem: a future society increasingly influenced by the cluster of organizational and market innovations induced by Artificial Intelligence technologies.” Chief among the recognized economic challenges of the digital age is the interruption of markets by machine-learning algorithms that have the potential to effect widespread market segmentation and widespread price discrimination.",1
48,4,Journal of Industrial and Business Economics,21 September 2021,https://link.springer.com/article/10.1007/s40812-021-00196-9,Organizing and better understanding transdisciplinarity in the context of artificial intelligence expansion: a crucial role for the new alliance between economics and engineering,December 2021,Cyrille Rigolot,,,,Unknown,Unknown,Mix,,
48,4,Journal of Industrial and Business Economics,21 September 2021,https://link.springer.com/article/10.1007/s40812-021-00197-8,Alliances need autonomy,December 2021,G. M. Peter Swann,,,Unknown,Unknown,Unknown,Unknown,,
49,1,Journal of Industrial and Business Economics,17 November 2021,https://link.springer.com/article/10.1007/s40812-021-00203-z,The economics–engineering nexus: response to the commentaries,March 2022,Sergio Mariotti,,,Male,Unknown,Unknown,Male,,2
49,1,Journal of Industrial and Business Economics,04 January 2022,https://link.springer.com/article/10.1007/s40812-021-00206-w,Industry characteristics and agglomeration of heterogeneous firms,March 2022,Daguo Lv,Lingyu Zhang,Jingtao Yao,Unknown,Unknown,Unknown,Unknown,,
49,1,Journal of Industrial and Business Economics,10 January 2022,https://link.springer.com/article/10.1007/s40812-021-00204-y,Measuring adoption of industry 4.0 technologies via international trade data: insights from European countries,March 2022,Davide Castellani,Fabio Lamperti,Katiuscia Lavoratori,Male,Male,Female,Mix,,
49,1,Journal of Industrial and Business Economics,18 December 2021,https://link.springer.com/article/10.1007/s40812-021-00205-x,Per-unit versus ad-valorem royalty licensing in a Stackelberg market,March 2022,Manel Antelo,Lluís Bru,,Male,Unknown,Unknown,Male,"Empirical evidence on licensing deals indicates that innovators often transfer their patented innovations to direct competitors (Jiang & Shi, 2018) and that most licensing contracts feature positive royalties (Bousquet et al., 1998; Lim & Veugelers, 2003).Footnote 1 In this paper we discuss two additional aspects relevant for licensing arrangements. First, that often one of the firms in the industry has accumulated an advantage over time, which is reflected in its current market dominance. This competitive advantage may derive from prior technological superiority leading to a large customer base, well-known trademarks, etc.; and in the case of incumbents against start-ups, also from the fact that new firms face constraints in their production capacity, must build their own supply chain and distribution network, etc. In our analysis below, we will represent this incumbency feature through a Stackelberg game, where the more established firm will be the first-mover player in the marketplace. The second important aspect to consider is that, at a given time, any of the two firms can have the technological leadership. Hence the holder of a new technology can be either the leader in setting the output level (the most established firm) or the follower (the entrant) of the industry. There are many firms playing as leaders in their product markets and that frequently license their patented innovations to competitors. Jiang and Shi (2018) discuss the examples of Procter and Gamble and Ford. At the same time, the case of AMD in the semiconductor industry features an example of a licensor that plays as follower in the marketplace. AMD microprocessors for PCs compete with those from Intel,Footnote 2 and until recently Intel almost monopolized this market, but currently AMD has the technological leadership; as a consequence, while in 2015 Intel’s share for desktop and laptop microprocessors was at 92.4%, in 2021 AMD’s share was over 17%.Footnote 3 Notice that AMD technological advantage does not mean that it has overtaken Intel in terms of market share. Moreover, this rivalry does not exclude that AMD and Intel extensively cross-license each other’s technologies, so that both firms end up offering products of similar quality. The empirical literature does not provide clear-cut results on the relationship between firm size and innovation, and although the tendency seems to be positive, it is not necessarily linear. Therefore, it can be assumed that both a leading firm and a follower firm in setting the quantity to produce, and thus having a different size, can become the licensor of a new technology. For example, according to Acs and Audretsch (1987, 1990), the relationship between firms’ size and innovation depends on industry characteristics, so that in highly concentrated sectors with high entry barriers, large firms tend to innovate (and become licensors) more than small firms, whereas the opposite holds for less concentrated sectors reflecting emerging or growing technologies. As for the relationship between innovation and firm age (another proxy for incumbency), the evidence indicates that challengers invest more in R&D than more established firms when the goal is to enter new markets (Reinganum, 1983; Czarnitzki & Kraft, 2004); this suggests that older firms may be less R&D-intensive than their younger counterparts. Finally, Huergo and Jaumandreu (2004) find that the probability of innovating varies widely by activity, that small size per se broadly reduces the probability of innovation and that entrant firms are more likely to innovate than older firms. Summing up, both the firm that plays as leader or follower in the product market can be the one that innovates and licenses the resulting technology. Since the main motivation of innovative firms for licensing out their patents is to earn revenue, they will try to devise a licensing arrangement that ensures the maximum payoff. The empirical literature shows that contingent royalties—either per-unit royalty (non-negative uniform royalty per unit of production) or ad-valorem royalty (non-negative royalty based on a percentage of licensee sales)—are commonly included in licensing contracts (e.g., Bousquet et al., 1998; Lim & Veugelers, 2003; Trombini & Comacchio, 2012). In this respect, our work examines how the position of a firm in a market, namely its role as a leader or a follower in the Stackelberg game, affects the type of royalty chosen (per-unit or ad-valorem) when licensing a process technology. The theoretical literature shows that the rationale for using contingent royalties in licensing deals lies in factors such as demand or cost uncertainty (Bousquet et al., 1998), product differentiation, a licensee’s new product development cost (San Martín and Saracho, 2016), or the relative efficiency of the licensee compared to the licensor (Fan et al., 2018a). Two papers have analysed the choice between per-unit and ad-valorem royalties in a Cournot setting: San Martín and Saracho (2010) find that, under full information, an ad-valorem royalty is the preferred licensing contract. This is because ad-valorem royalty compared to per-unit royalty allows the licensor to relax market competition. Fan et al. (2018a) show that per-unit licensing is more profitable if the licensor is more efficient than the licensee in using the innovation, whereas ad-valorem licensing is more profitable in the reverse scenario. In this paper we describe the interaction between firms in a duopoly using the Stackelberg leadership model,Footnote 4 that we believe better reflects the characteristics we have described above rather than a Cournot duopoly, and we contribute three findings to the literature. First, the kind of royalty offered crucially depends on the licensor’s status as a leader or follower in the product market. In particular, the superiority of ad-valorem over per-unit royalty in a Cournot setting no longer holds when the licensor is the follower in the Stackelberg game. In this set-up, per-unit royalty is more collusive than ad-valorem royalty and allows the licensor to reap more license revenues. Our second finding is related to the licensing impact on social welfare. While, in a Cournot setting, licensing as compared to no licensing unequivocally hurts both consumers and society (San Martín & Saracho, 2010), in a Stackelberg environment the licensing impact on consumers and society as a whole depends on the licensor’s status in the marketplace. The diffusion of an innovation harms consumers and society only when the licensor is the market leader; if the licensor plays as a follower, the consumer surplus remains unaltered and society as a whole is better off in welfare terms after licensing because there is an increase in production efficiency.Footnote 5 Finally, we also show that the incentive to undertake innovative activities and license an innovation largely depends on both the licensor’s status in the product market and the size of the innovation: when the size of the innovation is sufficiently small, the incentive to innovate and disseminate the resulting innovation is higher for a leader licensor than for a follower licensor, while the opposite holds for a large innovation. In this respect, our paper complements Wang and Yang (2004)’s research. These authors find that in a linear Stackelberg duopoly like ours, the follower innovator is more likely to license a cost-reducing innovation to the leader than the leader innovator is to the follower, regardless of whether licensing is in the form of a fixed fee or royalty per unit of output. Under fixed-fee licensing, the follower gains more from small innovations while the leader gains more from large non-drastic innovations. Under royalty licensing, however, it is the follower in the product market who always gains more than the leader from an innovation. The rest of the paper is structured as follows. Section 2 outlines the model and Sect. 3 examines the preferred method—ad-valorem two-part or per-unit two-part contracting—for a licensor to license the innovation. Section 4 analyses the welfare impact of licensing, Sect. 5 studies the effect of the licensor’s status in the product market on the incentive to innovate, and finally, Sect. 6 concludes.",1
49,1,Journal of Industrial and Business Economics,19 November 2021,https://link.springer.com/article/10.1007/s40812-021-00202-0,Price discovery in the cryptocurrency market: evidence from institutional activity,March 2022,Bao Doan,Huy Pham,Binh Nguyen Thanh,,,,Mix,,
49,1,Journal of Industrial and Business Economics,29 January 2022,https://link.springer.com/article/10.1007/s40812-022-00208-2,Trade networks and shock transmission capacity: a new taxonomy of Italian industries,March 2022,Stefano Costa,Federico Sallusti,Claudio Vicarelli,Male,Male,Male,Male,"Trade networks represent a relevant vehicle for the transmission of economic shocks. The structure of inter-sectoral relationships contributes to determine how factors such as productivity dynamics, technological progress, increase or decrease in demand can affect the business system through trade relationships along supply-chains. Furthermore, transmission also operates at an international level, through the links connecting a country’s exporting (importing) sectors to foreign importing (exporting) industries. In the last decades, these factors have increased their importance, as the international interconnectedness across sectors has risen and the relevance of international trade has increased with growing participation in global value chains (GVCs). However, the extent and speed of transmission can be heterogeneous between and within countries: industries and firms can be more or less central in trade networks. In this context, a mismatch may emerge between sectors that are central for the international transmission of shocks and those that are central for their domestic propagation. In other words, a sector could be strongly connected with other foreign sectors, so representing a potential transmission channel for exogenous shocks; however, if it is not central within the domestic trade network, its ability to transmit shocks into the domestic economy would be weak. Conversely, a sector with little or no foreign trade links could be central within the domestic trade network; in such a case, a shock originating in that sector could spread widely and rapidly across the domestic economy but with little impact on foreign markets. This intuition is at the origin of the main contribution of this paper. We propose a new taxonomy of Italian business sectors which is based on the speed and the extent of their capacity of transmitting impulses across the domestic system. In doing so, we account for the existence of the aforementioned mismatch between sectors that are central for the international transmission of shocks and those that are central for their propagation within the Italian economy. In particular, our aim is to analyze how shocks spread across the Italian business system through domestic inter-sectoral relationships.Footnote 1 In this context, we analyze the positioning of industries within both the international and the domestic trade, in order to verify possible mismatches in the structure of the two networks. For Italy, this analysis is relevant for at least three reasons. First, the business system is extremely fragmented: in 2018, Italian firms with less than 10 workers accounted for 95% of total enterprises, 44% of total employment and nearly 30% of total value added (99.9%, 79% and 69% for SMEs, respectively). Second, according to Eurostat data, in the same year the top 50 Italian exporters accounted for about 25% of total export, a value comparable to that of top 5 exporters in France and Germany. Third, a substantial part of Italy’s manufacturing activities are deeply involved in GVCs, such as textiles, leather and footwear, metal products, chemicals, pharmaceutics, machinery, motor vehicles, other transport equipment (which, according to Istat data, in 2018, accounted together for nearly 47% of total exports). All these sources of heterogeneity may generate potential mismatches between domestic and international channels of shock transmission. The rest of this paper is organized as follows. Section 2 briefly recalls the literature of interest. Section 3 summarizes the tools and concepts of Network analysis we use along the work. Section 4, taking a country-based perspective, analyzes the position of the Italian economy within the international trade network. Section 5, taking a sector-level perspective, positions Italian industries both in international and domestic networks. Furthermore, it proposes a new taxonomy of sectors according to the speed at, and the extent to, which they propagate shocks within the Italian economic system. Section 6 concludes.",1
49,1,Journal of Industrial and Business Economics,17 February 2022,https://link.springer.com/article/10.1007/s40812-022-00210-8,"Directors’ compensation, ownership concentration and the value of the firm: evidence from an emerging market",March 2022,Chee Yoong Liew,YoungKyung Ko,Saraniah Thechina Murthy,,Unknown,Unknown,Mix,,
49,2,Journal of Industrial and Business Economics,19 February 2022,https://link.springer.com/article/10.1007/s40812-022-00209-1,'R&D and export performance: exploring heterogeneity along the export intensity distribution',June 2022,L. Benfratello,A. Bottasso,C. Piccardo,Unknown,Unknown,Unknown,Unknown,,
49,2,Journal of Industrial and Business Economics,04 May 2022,https://link.springer.com/article/10.1007/s40812-022-00217-1,"Survival determinants for Brazilian companies, 1996 to 2016",June 2022,Marisa dos Reis Azevedo Botelho,Graciele de Fátima Sousa,Ariana Cericatto da Silva,Female,Unknown,Female,Female,"Relationships between age, size, growth, and survival of firms have been analyzed in economic literature for almost a century. One of the milestones in this literature is the so-called Gibrat’s Law (1931), which postulates that growth of firms is independent of their size. A number of academic studies have tested this relationship, with ambiguous results (Daunfeldt & Elert, 2013; Esteves, 2007; Santarelli et al., 2006). The relationship between size and survival has also been investigated and a direct relationship between these two variables has been found in several studies (Anyadike-Danes & Hart, 2018). Recent studies contextualize these results by analyzing the relationships between age, growth, and survival. These studies, such as those by Haltiwanger et al. (2013), Barba Navaretti et al. (2014) and Coad et al. (2018), show that age and size are important variables to explain the growth of firms, and cannot be treated as equivalent. An important feature of young firms, not necessarily small ones, is that they make an important contribution to job creation, while older firms are responsible for an important part of the workforce, but not for the creating new jobs (Haltiwanger et al., 2013). Considering the age of companies taken in a continuous manner and in different cohorts shows, in general, high mortality rates in initial years of a company's foundation and high growth rates of a portion of survivors in subsequent periods, especially for small companies that surpassed microenterprise thresholds (Coad et al., 2018). However, factors such as innovative activities, exports, activity sectors and others greatly influence chances of survival, as shown by several recent studies that indicate, in general, considerable heterogeneity when it comes to these relationships. Although determinants of firms’ survival are of paramount importance for understanding market dynamics, few studies have been carried out recently for Brazilian industrial firms. Brazilian industry has undergone important changes since the economic liberalization in the early 1990s. In the last two decades, Brazilian industry has gone through a clear process of loss of importance in contributing to the gross domestic product. This deindustrialization process has been classified by several authors as an early process; that is, one that occurs when a country has a level of per capita income much lower than that seen in developed countries (Nassif et al., 2015; Palma, 2005). In addition, there is a process of regressive specialization in international trade, with a high and growing trade deficit for industrial goods (Nassif & Castilho, 2020), and evidence of “Dutch disease” (Bresser-Pereira, 2008). These processes have increased the structural heterogeneity that characterizes Brazilian industry, reflected in the great differences in productivity linked to sectors of activity and company sizes (Botelho et al., 2021). In turn, persistent asymmetries in productivity result mainly from inefficiencies at the managerial level, use of old technologies, employment of low-skilled labor and limitations of various orders of access to credit (Catela, 2018). In this context, the study of the survival of industrial firms and their main determinants is relevant. By accessing the most complete database on the Brazilian industry and the longest period of analysis allowed by this data, we have a comprehensive and unique picture of the survival of companies in Brazil and its main determinants. This paper addresses this theme by undertaking an analysis of survival rates of Brazilian companies from 1996 to 2016, covering a total of 43,865 companies, 29,115 of which are small. So as to elaborate the article, microdata from Annual Survey of Industry (PIA-Enterprise) and Central Register of Enterprises (CEMPRE), of the Brazilian Institute of Geography and Statistics (IBGE), were accessed. The methodology was based on the development of nonparametric and semi-parametric survival analysis models for small companies in Brazil. In the nonparametric model, the Kaplan–Meier hazard function was used, while the semiparametric model was based on Cox’s proportional model. Based on these methods, survival rate based on size of companies and survival rate by region where companies are located were analyzed, as well as survival rate by technological intensity of companies, in addition to the analysis to understand survival determinants of companies through a set of explanatory variables. Although widely used in the reference literature, studies using these methodologies and databases have not yet been carried out for Brazilian industry, which makes this work unique. The paper is structured in three sections in addition to this Introduction and Final Considerations. Section 2 is dedicated to reviewing the literature, Sect. 3 describes the models, and Sect. 4 analyzes the results.",
49,2,Journal of Industrial and Business Economics,01 March 2022,https://link.springer.com/article/10.1007/s40812-022-00212-6,The institutional and socio-technical determinants of renewable energy production in the EU: implications for policy,June 2022,Alessandro Marra,Emiliano Colantonio,,Male,Male,Unknown,Male,"Renewable energy sources are emerging as an important component with regard to meeting global energy demand. According to the International Energy Agency (IEA, 2019), the capacity for renewable power will increase by 50% in the coming years. Solar photovoltaic energy accounts for 60% of the expected growth, onshore wind accounts for 20%, and offshore wind for only 4%. The last of these is expected to triple in capacity by 2024, thanks to competitive auctions in the European Union (EU) and market expansion in China and the United States (US). In addition, bioenergy capacity is expected to increase, especially in China, India, and the EU. The increase in hydropower will decelerate, although it will still account for 10% of the growth in renewable energy (IEA, 2019). In 2020, the deployment of renewable energy for generating electricity has increased by 7%: the Covid-19 crisis has not halted its growth (IEA, 2020). In the EU, primary renewable energy production (REP) increased by 49% between 2008 and 2018. The most important source was wood and other biofuels, which accounted for more than 40%. Wind power accounted for 14% of the total, confirmed as the second main source, followed by hydro power with 11% of the total. Despite low levels of production, the output of biogas, liquid biofuels, and solar energy increased fast, accounting for a 7%, 7%, and 6%, respectively of the EU’s REP (EC, 2021). Although the installed capacity for REP is increasing, considerable differences remain within the EU. In 2018, renewable energy shares varied considerably across EU member states, ranging from 15 to 100%. Such differences are expected to continue because of the persistence of country-specific conditions, different starting points, and existing policies (IRENA, 2018). A growing literature has emerged to identify the factors that prompt renewable energy at the national level (Gan & Smith, 2011; Marques & Fuinhas, 2011). However, the understanding of its determinants remain incomplete, due to the use of different methodologies and frameworks, and to the lack of a comprehensive focus on institutional and socio-technical issues (Bourcet, 2020; Can Şener et al., 2018). First, there is limited understanding regarding the determinants of REP, even though some agreement has emerged on a couple of mechanisms: regulatory and environmental policies have a positive effect, while lobbying in favour of traditional energy sources produces a negative effect. Secondly, despite the consensus that the transition to renewable energy is a process that encompasses institutional, regulatory, technical, political, social, and cultural aspects, such issues have been rarely addressed in a comprehensive way in the literature (Andrews-Speed, 2016; Sovacool, 2009). Some energy companies still reject renewable resources because they prefer big and conventional power plants. Many consumers do not fully appreciate the benefits deriving from more renewable systems in terms of the reduction of carbon dioxide emissions. Sometimes public authorities are not able to set up the ‘right’ system or provide the ‘right’ incentives to encourage more renewable production. In such circumstances, institutional and socio-technical issues act as impediments and barriers to renewable deployment. Nonetheless, such aspects need to be adequately understood to realize how renewable energy is going to expand in the future: while technological advances are important, resistance from firms, consumers and public authorities can impede or slow the transition considerably (Smith et al., 2005). Improved understanding of such institutional and socio-technical factors will allow scholars and policymakers to provide forecasts for renewable energy that are better grounded in the socio-political context (Andrews-Speed, 2016). Today, compared to a few years ago, we appreciate to a greater extent the increased public awareness of the dangers of climate change, the desirability of using renewable energy, the growing adoption of environmental policy measures and incentive schemes to boost clean energy deployment, and the common acceptance of the green paradigm by both large multinationals and small and medium-sized enterprises. All this calls for a need to revisit the determinants of REP and a need to focus on the socio-technical aspects. The purpose of the study is to investigate the determinants of REP in different EU member states while controlling for income and energy imports, focusing on several institutional and socio-technical aspects in the form of regulatory constraints and policy stringency, private interests and lobbying, environmental pollution and public awareness, technical understanding and the level of education. We employ a panel vector autoregressive (PVAR) model in first differences to test the complex dynamic relationships among the above variables for the longest time span possible—from 1990 to 2015—for 18 EU member states. Although there are some differences, all EU member states are expected to raise the level of production from renewable sources by employing several levers. Nonetheless, the results with regard to the entire sample of countries show that environmental policy stringency does not influence REP, while income and education impact negatively. This evidence is counter-intuitive and would be surprising if we did not consider the strong heterogeneity between countries. EU member states are engaging in a transition to REP at different speeds, based on their starting point: this differs from country to country in terms of installed capacity and energy security. More specifically, countries with relatively higher energy imports and lower installed capacity are expected to act more quickly, since they face lower obstacles to initiating a more vigorous transition to renewable energy (Kahia et al., 2017). This same perspective has been adopted by the European Commission in the recent European Green Deal (EC, 2019). Accordingly, as suggested in the literature (Marques & Fuinhas, 2012), we divide the sample into two panels, using the share of energy imports to account for the different starting points. Energy imports, combining installed capacity and energy security issues, properly condenses differing policy, strategic and industrial choices, and allows us to distinguish between countries that are less active in terms of production (depending on energy imports from third countries; panel A, high energy importers) and countries that are more active (panel B, low energy importers). The remainder of the paper is organized as follows. Section 2 reviews the literature on the determinants of REP. Section 3 presents the empirical analysis, including the model specifications and the methodology used (Sect. 3.1), the testing framework (Sect. 3.2), and the results (Sects. 3.3 and 3.4). Conclusions and policy implications based on the findings are presented in Sect. 4.",3
49,2,Journal of Industrial and Business Economics,02 March 2022,https://link.springer.com/article/10.1007/s40812-022-00213-5,Upstream regulation and non-separable innovation,June 2022,Michael L. Polemis,Markos Tselekounis,,Male,Male,Unknown,Male,"In many vertically related industries, such as electricity, transportation, and telecommunications, market forces cannot achieve the desired competitive outcome mainly due to the existence of high levels of vertical integration (see, among others, Armstrong et al., 1994; Armstrong & Sappington, 2006; Besanko & Cui, 2019; Buckley, 2003; Economides, 2005; Pollitt, 2019). This justifies the intervention of sector-specific regulators to establish the conditions under which firms compete for and in the market, thus affecting the market structure, firms’ profits, and industry performance. As a result, regulation indirectly determines the intensity of competition which, in turn, affects the strategic decision-making of firms across a variety of dimensions in their pursuit to adapt to the new competitive environment. One aspect on which managers usually focus to influence the performance of their firms is the chosen innovation level. It is widely known that innovation can be seen as a differentiation factor that increases the profitability of firms by reducing production costs (process innovation) and/or improving product quality (product innovation). In addition, many innovation activities in several industries cannot be separated between process and product innovation when firms choose their optimal innovation level (Vareda, 2010). For instance, innovation activities focusing on improving the speed of data transmission usually result in services of better quality (higher broadband speeds) but also in significant cost reduction (better network management). This paper studies the regulation-innovation nexus in markets where a vertically integrated firm, who produces an essential input supplied to a duopolistic downstream sector at linear regulated prices, chooses its upstream non-separable innovation level. This framework may be relevant in vertically related industries where the formerly state-owned utilities have been mandated by national legislation to provide access to their essential upstream inputs at regulated prices. In such contexts, more strict regulation reflected by higher input prices favors the vertically integrated firms, whereas more light regulation resulting in lower input prices is related to more intense competition in the downstream market (Marino et al., 2019). Whether more or less stringent input regulation provides a vertically integrated upstream monopolist with higher incentives to invest in non-separable innovation activities constitutes the main research question of this paper. Our main finding is that the impact of input price regulation on innovation is a priori ambiguous: when the overall innovation activity results in more (less) product innovation than process innovation, less strict regulation decreases (increases) the level of innovation activity. However, the driving force of this finding seems to have significant implications for the investment strategies of technology-intense firms. In particular, we argue that this result arises because the vertically integrated firm strategically sets its innovation level to mitigate the negative impact of a decreasing regulated input price on its profit. Such mitigation is fulfilled by increasing (reducing) its innovation activity when process (product) innovation prevails. This regulation-driven strategic effect can be related to the reaction of firms when facing increasing competition that negatively affects their profit. As discussed above, the wholesale price of the upstream input can be seen as a proxy for downstream competition intensity. From this perspective, this paper is also close to the literature studying the relationship between competition and innovation in non-vertical settings where competition intensity is inversely reflected by the ability of firms to collude.Footnote 1 The vast majority of this literature finds an inverted-U-shape relationship between competition and innovation in the sample of industries analyzed, which is in line with our main finding from a dynamic perspective: when continuous input price reductions provoke a change in the prevailing type of innovation, a non-linear relationship between regulation and innovation arises. As several studies argue (see, among others, Aghion et al., 2005; Amable et al., 2016), the leading models of product differentiation and monopolistic competition give rise to the “Schumpeterian” effect (Schumpeter, 1942) implying a negative impact of competition on innovation. This theoretical finding is also supported by empirical evidence (Aghion & Howitt, 1992; Hashmi, 2013). However, most empirical studies embrace the “escape competition” effect (Arrow, 1962), according to which firms tend to increase the innovation activity when competition increases as a means to improve their profit margin (Blundell et al., 1999; Carlin et al., 2004; Correa, 2012; Correa & Ornaghi, 2014; Geroski, 1990; Gorodnichenko et al., 2010; Nickell, 1996; Schmitz, 2005; Van Reenen, 2011). Which effect dominates depends on several factors, such as the technology gap of the leader over the follower (Aghion et al., 2005; Amable et al., 2016), the size of the firms (Askenazy et al., 2013), and the intensity of financial constraints faced by firms (Petropoulos, 2017). As a result, we contribute to this literature by identifying—in the modeled framework—another factor affecting the dominating effect, namely the prevailing type of innovation: when the product (process) innovation prevails, the “Schumpeterian” (“escape competition”) effect dominates. The remainder of this paper proceeds as follows. Section 2 presents the model. Section 3 characterizes the equilibrium. Section 4 discusses the results, while Sect. 5 describes their robustness and limitations. The last section concludes, as well as draws managerial and policy implications.",
49,2,Journal of Industrial and Business Economics,07 September 2021,https://link.springer.com/article/10.1007/s40812-021-00193-y,"An evaluation of a mandatory profit-sharing reform in Peru, using quasi-experimental methods",June 2022,Edinson Tolentino,,,Unknown,Unknown,Unknown,Unknown,,
49,2,Journal of Industrial and Business Economics,03 June 2021,https://link.springer.com/article/10.1007/s40812-021-00191-0,Self-regulation under asymmetric cost information,June 2022,Ismail Saglam,,,Male,Unknown,Unknown,Male,"Self-regulation of firms has been extensively studied since the works of Erfle and McMillan (1990) and Glazer and McMillan (1992), who formalized the hypothesis that firms facing the threat of government regulation may constrain their product prices. In this paper, we ask how this behavior of firms could be affected by the presence of asymmetric information. To answer this question, we extend Glazer and McMillan’s (1992) self-regulation model to a case where a monopolistic firm threatened with regulation has private information about its cost function. Like in their model, we assume that the firm is regulated if a bill recommending regulatory action is proposed by at least one of the legislators. The legislators can calculate the expected welfare gain from regulation after they observe the output level the firm commits to produce in each period under the threat of regulation. Comparing the size of this welfare gain with the cost of drafting and promoting a bill, each legislator determines his/her probability of proposing a bill. On the other hand, taking such probabilities into consideration the firm calculates the risk of regulation and its optimal product price under this risk. Glazer and McMillan (1992) theoretically show that this price is always lower than the price the firm charges when it faces no regulatory threat. They also show that the firm exhibits this pricing behavior to optimally reduce the risk of regulation. Our main purpose is to explore whether these results of Glazer and McMillan (1992) may arise under asymmetric information, as well. Self-regulation has been a widespread practice in the past throughout the world, especially in industries of energy and natural resources (see Barton et al. 2006, for several examples). In these industries the production costs are generally privately known by firms. For instance, in electric power industries, the regulators can only imperfectly estimate the cost of building and operating a power generator (see, for more details, BEIS 2020 Report). Generally, these industries constitute a prominent example where one can find both self-regulation and the presence of asymmetric information relevant. For electric power industries, or for any self-regulated industry to that matter, one may find the presence of asymmetric information even more relevant in developing countries where underdeveloped auditing systems, among many other deficiencies, as argued by Laffont (2005), keeps regulators, especially those with limited capacity, away from gathering the relevant information about the industries under the focus of regulation and implementing direct regulatory policies. To the best of our knowledge, our research question has not been studied before. The most related works are the papers of Heyes (2005) and Denicolò (2008), who study how self-regulation can be used by firms as a signaling device in the presence of asymmetric information. More specifically, Heyes (2005) shows that firms with high compliance costs may use self-regulation to signal their costs in order to obtain less stringent regulation in the future. However, he assumes that self-regulation does not affect the probability of regulation. On the other hand, Denicolò (2008) studies a two-period duopoly to show that when the regulator has incomplete information about the production costs of the firms, the firm that has a comparative advantage in the cost of complying to the environmental standards enforced by the regulator may find it optimal to overcomply in the first period in order to signal that costs of complying are low, thereby inducing the regulator to enforce stricter standards in the second period. Denicolò (2008) assumes that the regulator cannot observe first-period prices and output of the duopolistic firms. (A self-regulated firm in his model can only signal by acquiring a superior production technology.) Thus, the probability of regulation is not directly affected by the pricing behavior of the firms like in our paper. Nevertheless, it is indirectly affected by their self-regulation. At the beginning of the first period, firms choose their technologies (either as good or bad). Observing them the government revises its prior beliefs about the production costs of the firms and using its updated beliefs decides whether to regulate the firms in the second period. One can argue that the pricing behavior of the firm threatened with regulation can unintentionally reveal its private information to the legislators, reducing our model to the symmetric information model of Glazer and McMillan (1992). This argument is based on the fact that the legislators can calculate—for each possible value of the firm’s cost information—the price that maximizes the firm’s profits when it is under the threat of regulation. Using the schedule of this price along with their observations on the prices actually charged by the threatened firm in periods prior to regulation, the legislators could estimate the private cost information of the firm. Clearly, an optimal regulatory policy based on these cost estimates would leave to the firm no informational rents, reducing our model to the complete information model of Glazer and McMillan (1992). While the above argument may seem reasonable, there are also other considerations. First, the inferences of the legislators about the production cost of the firm may not be verifiable to a third party such as a court or a regulatory authority. Second, even the legislators themselves may not be certain that their inferences are correct. If the firm anticipates that the legislators’ inferences about its cost function may be used by a regulatory authority, say to implement marginal cost pricing, the firm may increase its price prior to regulation by an unpredictable markup over its marginal cost. Once the legislators become aware of this strategic behavior, they may also realize that a regulatory program based on their possibly incorrect inferences may no longer be socially optimal. Characterization of a socially-optimal regulatory program under the verifiability of cost inferences—taking into account the optimal reaction of the threatened firm—is an interesting problem, which we leave for future research. In this paper, we consider a simpler problem, by assuming that the legislators and the regulator cannot make verifiable and trustable cost inferences using their observations about the prices set by the firm prior to regulation. Thus, the cost information of the firm will be ‘officially’ unknown to the legislators/regulator before the implementation of a regulatory program. Using this informational setting, we will replace the arbitrary regulatory policy in Glazer and McMillan (1992) with the optimal regulatory policy of Baron and Myerson (1982) devised for the case of asymmetric information, and simulate the outcome of our model to investigate whether the main results of Glazer and McMillan (1992) can arise under asymmetric information, as well. The rest of the paper is organized as follows: Sect. 2 presents our model, Sect. 3 contains our results, and Sect. 4 concludes.",1
49,2,Journal of Industrial and Business Economics,18 April 2022,https://link.springer.com/article/10.1007/s40812-022-00216-2,"The manufacturing output effects of infrastructure development, liberalization and governance: evidence from Sub-Saharan Africa",June 2022,John Bosco Nnyanzi,Susan Kavuma,Aisha Nanyiti,Male,Female,Female,Mix,,
49,3,Journal of Industrial and Business Economics,12 August 2022,https://link.springer.com/article/10.1007/s40812-022-00229-x,Industry dynamics in digital markets,September 2022,Federico Boffa,Amedeo Piolatto,Florian Schuett,Male,Male,Male,Male,"Digital technologies reduce the storage, computation, and data transmission costs (Goldfarb & Tucker, 2019). This has a tremendous impact on the organisation of economic activity and brings into prominence digital markets, i.e. places where digital technologies are used to exchange goods, services and information. Digital markets are often organised around platforms (see Gawer 2014; Kenney & Zysman, 2016), i.e. intermediaries that ‘bring together individuals and organizations so they can innovate or interact in ways not otherwise possible’ (Cusumano et al., 2019: 13). Such platforms have spread in many contexts and have transformed entire industries (Cabral et al., 2019). Digital markets are characterised, among others, by three distinctive aspects. First, digital technologies enable the emergence of new business models and new ways to create value. For instance, traditional businesses tend to generate profits by converting inputs into outputs, while platforms do so by coordinating distinct groups of agents (Evans & Schmalensee, 2016). This impacts how competitive dynamics affect digital markets, as opposed to traditional ones. Second, network effects play a primary role in digital markets (see Evans 2003; Rochet & Tirole, 2003; Armstrong, 2006): the benefits enjoyed by one side of the market (e.g. buyers) are contingent on the size and the relevant characteristics of the other sides that are active in that same market (e.g. sellers). Hence, intermediaries and participants in digital markets strive to leverage network effects, which thus shape industry dynamics in these markets. Third, digital technologies facilitate the gathering, storing, and processing of amounts of information that a decade ago would have been unthinkable. Such information can be analysed and used to increase the value of transactions (e.g. by tailoring products to customers’ needs) and strengthen one’s position in a market. The eight articles selected for this special issue extend our knowledge of the industry dynamics in digital markets by tackling the three aspects mentioned above: the creation of new business models enabled by digital technologies, the role of network effects, and the increased access to big data. In the remainder of this essay, we highlight the authors’ contributions and conclude by proposing some avenues for future research.",1
49,3,Journal of Industrial and Business Economics,04 August 2022,https://link.springer.com/article/10.1007/s40812-022-00222-4,"From Heron of Alexandria to Amazon’s Alexa: a stylized history of AI and its impact on business models, organization and work",September 2022,Lucrezia Fanti,Dario Guarascio,Massimo Moggi,Female,Male,Male,Mix,,
49,3,Journal of Industrial and Business Economics,28 July 2022,https://link.springer.com/article/10.1007/s40812-022-00226-0,"RegTech in public and private sectors: the nexus between data, technology and regulation",September 2022,Laura Grassi,Davide Lanfranchi,,Female,Male,Unknown,Mix,,
49,3,Journal of Industrial and Business Economics,22 July 2022,https://link.springer.com/article/10.1007/s40812-022-00227-z,Investigating the role of central banks in the interconnection between financial markets and cryptoassets,September 2022,Theodore Pelagidis,Eleftheria Kostika,,Male,Female,Unknown,Mix,,
49,3,Journal of Industrial and Business Economics,23 July 2022,https://link.springer.com/article/10.1007/s40812-022-00223-3,M&A and early investment decisions by digital platforms,September 2022,Zelda Brutti,Luis E. Rojas,,Female,Male,Unknown,Mix,,
49,3,Journal of Industrial and Business Economics,18 July 2022,https://link.springer.com/article/10.1007/s40812-022-00224-2,Software quality and backward compatibility in the video game industry,September 2022,Sebastian Wai,,,Male,Unknown,Unknown,Male,"The video game console industry provides a singular opportunity for examining the effects of software quality and variety on hardware sales. Unlike personal computers or mobile phones, software produced for each console is completely incompatible with all the other consoles, and each console is produced by a single firm. Further, major video games have years-long development cycles with well-publicized releases, and reviews go live across the internet as soon as they are available for sale. Combined, these factors enable us to gain a clear picture of the universe of software available for a particular console at any point in time. Contrast this with the difficulty in ascertaining the total pool of software available to a Windows 10 user. Windows software goes back decades; some of this software, produced for older versions of Windows, will still be compatible with Windows 10, while others will not. Further, the individual specifications of each Windows 10 machine are different, and some high-end software will not work on lower-end machines. None of this is the case for consoles. The very first Xbox 360 to roll off the line could run the exact same set of games as the last, despite being separated by 8 years. Hardware lifespans of this length are unheard of in personal computers and other consumer electronics. The video game console industry is an example of a platform market – a market for goods which two types of customer can use to interact. In this case, the players form one side while the software producers form the other. Caillaud and Jullien (2003) applied the term the “chicken and egg” problem to such markets, referring to the difficulty a new platform may have if neither side has reached a critical mass of adoption for the other to buy in. The benefits each side derive from one another are known as indirect network effects. For consoles, the solution arguably comes from the software side first in the form of launch titles developed in-house or under contract. This initial software pool, as well as the expectation of more to come, may be enough for some consumers to buy the console immediately. Others will wait until the quantity, as well as quality, of existing software increases. This paper’s main contribution is to measure the overall quality of the software available to each console and test its relationship with console sales. I do this using data collected from the review aggregation website Metacritic. Metacritic collects reviews for games (as well as films, television, and music) from various critics across the internet and combines them into a single score. While controversial at times, this “Metascore” has become an important quality measure in the industry. A secondary goal of this paper is to examine two other features of consoles: backward compatibility and reliability. Backward compatibility is a console’s capability of running software initially designed for an older machine produced by the same firm. For example, a major selling point of Sony’s PlayStation 2 was its ability to run any game designed for Sony’s original PlayStation. No console after that could boast such complete backward compatibility, but all of the consoles examined here featured some. Backward compatibility allows a new console to dip into an existing software pool to overcome the chicken and egg problem. Metacritic scores are again used to assess the quality of backward compatible software. As consoles represent a major purchase for many consumers, reliability may also be an important factor in driving sales. In particular, early models of the Xbox 360 were notorious for hardware failure. Reliability is also a cornerstone of the durable goods literature, as discussed by Coase (1972). In this paper, I use sales data and the aforementioned software quality data from the seventh video game console generation (Xbox 360, PlayStation 3, and Wii) to assess the significance of relationships between accumulated software quality and hardware sales. The seventh generation is an ideal choice for this study because all three competitors (Microsoft, Sony, and Nintendo) also competed in the sixth generation and employed backward compatibility to some extent. This article’s main empirical contributions, addressing gaps in the literature, are as follows. First, I introduce a novel way to measure accumulated software quality over both the lifetime of a console and in a floating one-year window. This measure is based on the aggregated review scores from Metacritic, whereas most previous studies have used the total number of releases. My approach improves upon those that do (discussed below). Second, I use this measure to estimate both the impact of current-generation software and backward compatible software from the previous generation. Prior work on backward compatibility has not used review scores to measure their quality. Third, I take an instrumental variables approach to dealing with the endogeneity of software releases, which similar work using review scores has not done (Kim et al. 2014; Song et al. 2017). Fourth, I incorporate a measurement of hardware reliability into the estimation, which has not previously been applied to this market. My results show a strong positive impact of accumulated software quality on weekly sales of consoles. I also show the effect of backward compatibility is positive, but much smaller than that of current generation software. These results generally confirm the intuition suggested by existing theory on indirect network effects and have some interesting managerial implications. Theoretical work on platform markets began with Rochet and Tirole (2003) and the aforementioned Caillaud and Jullien (2003). Much of the recent research, both theoretical and empirical, has grown out of Mark Armstrong’s (2006) Hotelling-style models. On the empirical side, there have been several important papers, most of which focus on the traditional advertiser-consumer style of platform market. These include Rysman’s (2004; 2007) studies of Yellow Pages and payment cards, Wilbur’s (2008) paper on television advertising, and Kaiser and Wright’s (2006) estimation of Armstrong’s duopoly model using competing magazines. Hałaburda et al. (2020) outline a theoretical model which suggests technology as the mechanism to break a firm’s dominance of the market. They cite smartphones as a market where radical jumps in technology, such as touchscreens, allowed entrants to seize market share. Kim et al. (2017) use a dynamic framework and a unique dataset, which they assembled, to analyze the daily deals market. Their paper assesses how LivingSocial, the entrant, has been able to compete against Groupon, the incumbent. The video game console market shares the entrant/incumbent structure that also characterizes smartphones and daily deals. Hann et al. (2016) use a platform approach to analyze the mobile internet market. Video game consoles are also a durable consumer good. Work on this topic dates back to Coase’s (1972) landmark work. Coase argues that, in the absence of a credible commitment mechanism, a monopolist in a durable good loses its market power. This is a problem known as time inconsistency. Such mechanisms include buyback policies and leasing, none of which are present in the console market. Coase also points out that an imperfectly durable good is another path to profitability. Consoles, like other consumer electronics, are not perfectly durable, though they typically last for several years—long enough such that a typical consumer would purchase only once during a console’s lifespan. While Coase focused on the monopoly Waldman (2003) states that oligopolists will face similar concerns. Oligopoly does, however, raise an interesting question about whether higher durability will be beneficial or not. If consumers are aware of a product’s durability, relative to its competitors, durability may become a selling point. In this market, the poor reliability of early Xbox 360 models was well-known. Within this literature, there has been a focus on both software and changing quality over time. Besanko and Winston (1990, 1991) discuss the pricing in such markets in a monopoly context. Their theory has software in mind, particularly games, where users can buy enhancements after the initial purchase. This phenomenon is actually far more common in the industry today than it was then. Further work by Fudenberg and Tirole (1998) addressed durable goods where quality increases in each “generation” with resale possible, both topics relevant to the console industry. Waldman (2003) argues that the periodic introduction of new products (as is the case in most electronics) and the consumers’ knowledge of this reduces a durable good’s value. Ellison and Fudenberg (2000) develop a model for software with periodic upgrades, touching on backward compatibility. Waldman (1996) develops a model in which a monopolist chooses the rate at which a good’s quality declines over time. Work by Economides (2000) on PC operating systems as a durable goods and network market is also closely related. He incorporates both backward compatibility and what he calls “forward compatibility,” where new software is designed to be compatible with old hardware. While forward compatibility is common in the PC market, it generally does not exist in consoles. Economides’ main result is a model which sees prices decreasing over time, but kept above cost – a contradiction of Coase’s conjecture. Second-hand markets and aftermarkets for accessories are another important consideration in durable goods. Second-hand markets do exist for consoles, though I do not know of data on this. Aftermarkets are not monopolized by console producers. Games could be considered an aftermarket, where first-party and third-party titles share the market. Smaller third-party producers of physical accessories also exist. Overall, the theoretical work on durable goods markets has exceeded empirical work (Waldman 2008), a gap into which this study fits. The video game industry has been the subject of numerous studies. Nair (2007) examines intertemporal price discrimination on the software side of the market (the games). Intertemporal price discrimination is also prevalent on the hardware side, as we will see. Ishihara and Ching (2017) estimate a durable goods model using data from the Japanese software market, finding consumers are forward-looking. Cennamo and Santalo (2013), Lee (2013) and Derdenger (2014) study the role of platform exclusivity in the video game industry. This refers to games being released on only a single console, often by “first-party” developers vertically connected with the platform manufacturer. Derdenger also classifies games as either “hits” or “busts"" based on sales, an approach shared by Corts and Lederman (2009) and Cennamo (2016). Other work has simply used the total number of available titles on the software side. This is the approach taken in two studies on backward compatibility in video games by Claussen et al. (2016) and Kretschmer and Claussen (2016). Clements and Ohashi (2005) do the same in their work on previous console generations. Zhu and Iansiti (2012) adjust this by using GameSpot (a game review website) reviews to only count high-quality games in their total. Papers by Kim et al. (2014) and Song et al. (2017) utilize similar review score data from Metacritic to study the video game industry. Neither of these papers use instruments for game releases and their associated quality, nor do they consider backward compatibility. This paper fills this gap by combining backward compatibility with the use of Metacritic scores to measure quality and using instrumental variables to deal with the endogeneity of software releases.",1
49,3,Journal of Industrial and Business Economics,08 July 2022,https://link.springer.com/article/10.1007/s40812-022-00221-5,A market for digital privacy: consumers’ willingness to trade personal data and money,September 2022,Anna D’Annunzio,Elena Menichelli,,Female,Female,Unknown,Female,"Currently, personal data are considered a currency that can be exchanged for online services (Malgieri & Custers, 2018; Spiekermann et al., 2015), as many online business models are based on the collection of the personal data used to generate revenue (Lim et al., 2018). Firms such as Google and Facebook offer services to users for free, collect their personal data and sell services to third parties such as advertisers based on the collected information. Recently, personal data protection has also become a business opportunity, as some firms provide solutions to control how personal data are shared and used online. For instance, some firms sell services that allow users to anonymize their online activity (e.g., the app Avira Phantom VPN). Other internet service providers offer the possibility for consumers to sell their personal data to firms (e.g., the websites Datacoup.com and www.citizenme.com). In the internet economy, consumers are used to trading privacy in exchange for money and vice versa; implicitly or explicitly, they are often asked either to accept a monetary discount in exchange for sharing their personal data or to pay a price to keep their personal data private. The understanding of the mental calculus that consumers undertake when they make this trade-off is still limited, and it is important for both private companies and policy-makers. It is interesting for firms to understand which markets for privacy exist and how privacy can be used to design business models, prices, and strategies in general. Studying this issue is also important to help policy-makers understand whether market-based solutions for online consumer privacy can be adopted. In general, it is important to prevent practices that are “deceptive or unfair to consumers” (FTC, 2006). There is thus a need to better investigate the “monetization of privacy,” defined by ENISA (2012) as the “consumer’s decision of disclosure or non-disclosure of personal data in relation to a purchase transaction”. In the present work, we intend to examine this topic to better understand consumers’ preferences and attitudes in the privacy space using survey data collected in Norway. We study consumers’ willingness to pay to protect their personal data (a concept hereafter referred to as \(WPP\)) and willingness to share personal data for a discount (referred to as \(WSD\)) for a list of personal data often exchanged on the internet. First, we analyze whether there is a difference between these concepts and whether this difference changes with the personal data under analysis. Then, we investigate possible attitudinal and demographic determinants of \(WSD\) and \(WPP\) to understand which dimensions firms can leverage to convince consumers to share data for a discount or to pay for privacy protection. In the current study, we deploy an approach complementary to those used in previous studies on the topic (see Acquisti et al., 2016; Yun et al., 2019). Several studies have estimated monetary valuations of privacy, but the results are very dependent on the context of the study. In addition, most studies have focused on either the willingness to accept payments for the disclosure of personal data or the willingness to pay for privacy. This paper proposes a different approach, estimating respondents’ propensity to share personal data in exchange for a discount and respondents’ propensity to pay to protect their private information. By using a Likert-type willingness scale and comparing these propensity levels, our aim is to understand which types of trade-offs between privacy and money consumers are willing to make depending on the personal data under analysis and abstracting from other contextual dimensions. We show that the way in which consumers’ valuation of privacy is assessed plays an important role, and the gap between the two concepts crucially depends on the personal data under analysis, as the estimated \(WPP\) can be significantly higher or lower than the estimated \(WSD\), depending on the specific type of personal data under analysis. More specifically, respondents are more willing to share in exchange for a discount those personal data that are usually a matter of public record or knowledge or that do not necessarily personally identify an individual (such as age and gender). Conversely, \(WPP\) is higher than \(WSD\) for more confidential personal data (e.g., phone call and SMS content, pictures). Hence, firms’ and policy-makers’ strategies should be contingent on the type of personal data being shared. The results show that personalization and trust play a large influencing role on \(WSD\). Regarding \(WPP\), we find that younger respondents are more willing to pay to protect their privacy, indicating a higher awareness among young people of the trade-off between privacy and money. Additionally, for more sensitive personal data, the results indicate that respondents are more willing to pay banks, financial institutions, public administrations, and mobile operators to keep their data private than they are to pay social media and internet companies. The remainder of this paper is organized as follows. First, we analyze the conceptual background to frame the research questions. Second, we present the data and the method we use. Then, we present the results and discuss their implications. Finally, we conclude by presenting the limitations of this study and opportunities for further research.",1
49,3,Journal of Industrial and Business Economics,11 May 2022,https://link.springer.com/article/10.1007/s40812-022-00218-0,"Business models, consumer data and privacy in platform markets",September 2022,Jorge Padilla,Salvatore Piccolo,Helder Vasconcelos,Male,Male,Male,Male,"Online platforms are the architects of the digital revolution. Thanks to these platforms, nowadays, consumers and sellers enjoy multiple trading solutions. In addition to meeting physically in stores, they can also trade in a virtual, impersonal, and presumably anonymous world. The reduction of search costs, the increased delivery speed, and higher market transparency are the bright side of this revolution. However, these companies also manage a massive amount of consumer data. Platforms such as Amazon, Apple, and Google, to name a few, collect, package, and disclose users’ data to third parties that use this knowledge for commercial, marketing, and, in the worst case, fraudulent purposes. The information that platforms collect covers a broad spectrum of individual data, ranging from users’ individual characteristics, such as gender, age, and location, to their browsing patterns, prior transactions, social interaction, etc. Platforms can, therefore, forecast consumers’ tastes, habits, and social preferences, and monetize this information through personalized offers.Footnote 1 Consumer data may also land in wrong ‘hands’ and be used for illegal purposes that damage consumers and their privacy (e.g., credit card and/or identity cloning). This is allegedly the major dark side of the digital revolution. A flourishing academic literature has started to investigate the interaction between data management, marketing strategies, and competition in platform markets (see, e.g., Bergemann & Bonatti, 2019; Jullien, 2012; Peitz & Reisinger, 2015, for recent surveys). However, these models are silent on the link between platforms’ business models and the accuracy of the consumer data that they collect and eventually disclose to self-interested third-party sellers. Notably, while some online businesses have mainly maintained a brokerage activity (e.g., eBay and Google) others operate under hybrid business models and have developed their own private labels to compete with third-party sellers operating through their marketplaces (e.g., Amazon and Apple). Do all these online intermediaries have the same incentives to acquire and disclose users’ personal information? If not, what are the determinants of different approaches to information and privacy management? Is the choice of business model—i.e., pure intermediation vs. hybrid platforms one of these key factors? In this paper, we study the drivers of the accuracy of the information that digital intermediaries collect and disclose. Specifically, we compare the incentives to collect demand information by a an online intermediary (platform) operating under two alternative business models: a pure-intermediation model, where it plays a matching function only by connecting a third-party seller with buyers, and a hybrid model where, in addition to its traditional middlemen role, the platform also introduces its own private label in the marketplace and competes with the third-party seller distributing through its marketplace. We argue that there is no objective presumption that pure intermediation platforms collect more or less information than hybrid platforms. Platforms’ business model is not neutral to data collection. This observation should be considered carefully by privacy authorities, especially because in the EU the GDPR is based on the ’data minimization principle.’Footnote 2 We set up a simple duopoly model with linear demand and random willingness to pay (demand intercept). The platform collects information on the demand’s random component and, coherently with recent regulatory trends imposing big tech companies transparency requirements to promote level playing field competition, is mandated to disclose this information to the sellers distributing through its marketplace (including its retail unit when present). Sellers use such information to target quality (or advertising) and prices. The crucial, and somewhat novel, assumption is that disclosing more accurate information directly harms consumers because they mind their privacy.Footnote 3 To isolate the effects of contractual frictions (e.g., double marginalization) on the platform’s information acquisition problem, in the baseline model we assume efficient contracting—i.e., the platform extracts a fixed share of the seller’s profit.Footnote 4 Within this setting, we show that the platform’s incentive to gather information in the two business models depends on the degree of substitutability between the private label (which is present only under the hybrid model) and the product of the seller distributing through it (intra-platform competition), and the distribution of the bargaining power in the negotiation with the seller. When intra-platform competition is sufficiently intense, and the platform has a strong bargaining position in the negotiation with the seller, it tends to acquire and disclose more accurate information in the pure-intermediation model than in the hybrid model, at the expense of consumers’ privacy. Otherwise, the platform collects and discloses more information when it operates a hybrid model. Gathering information has two main effects on the platform’s profit. On the one hand, regardless of its business model, the platform is willing to gather information because it allows the sellers active in its marketplace to make more accurate pricing and quality/advertising decisions, generating an extra profit that the platform (partially) extracts at the negotiation stage. On the other hand, since consumers have privacy concerns, gathering information reduces demand because (ceteris paribus) fewer consumers join the platform when they fear that online purchases endanger their privacy. The first effect described above is present under both business models. The second effect points to less information collection in the hybrid platform than in the pure intermediation model when the private label and the product of the third-party seller are close substitutes and when the bargaining power of the platform is high. This is because when those conditions hold, the hybrid platform can appropriate (via its retailing activity) a larger share of the incremental value generated by each consumer that joins the platform and, therefore, it has a larger incentive to increase demand by protecting privacy. Hence, when the intermediary is in a strong bargaining position and intra-platform competition is strong, hybrid platforms will collect and disclose less information than pure intermediation platforms. The opposite holds otherwise. Building on these insights, we then explore the determinants of the optimal business model. To begin with, we show that, when both models imply the same level of information accuracy, the platform prefers to operate as a pure intermediary when consumers perceive products as relatively close substitutes and its bargaining power is relatively strong. The intuition is as follows. For given information accuracy, introducing a private label in the marketplace is not worthwhile for the platform if products are close substitutes because competition erodes both the profit earned through its private label and those earned through the intermediation channel. This effect becomes even stronger, in relative terms, when the platform’s bargaining strength rises, because in the pure intermediation model, the platform extracts a relatively higher share of the seller’s profit. Yet, when the information policy differs in the two business models, the result is ambiguous. We find interesting cases in which the hybrid regime maximizes the platform’s profit and vice-versa. The driving forces are again intra-platform competition, which makes the hybrid model relatively less appealing, and the strength of the bargaining position of the third-party seller(s), which makes it more appealing. These findings generalize to a number of extensions which include alternative demand functions, multiple sellers and decentralized decision making within the platform. Notably, with inefficient contracting (e.g., with ad-valorem and linear per-unit fees) we find that the hybrid model always provides greater incentives to gather information under the hypothesis that the platform makes take-it-or-leave-it offers. While under ad-valorem fees this incentive unambiguously falls with competition, under linear per-unit fees we find a U-shaped relationship between the incentive to collect information and the degree of intra-platform competition. Finally, we show that the effect of the business model on consumer surplus is ambiguous, although positively biased towards the hybrid model in several cases.",1
49,3,Journal of Industrial and Business Economics,06 July 2022,https://link.springer.com/article/10.1007/s40812-022-00220-6,The microeconomics of data – a survey,September 2022,Flavio Pino,,,Male,Unknown,Unknown,Male,"The role of data in economics has seen a significant increase in importance in recent years, primarily due to the ever-increasing relevance of digital markets (Crèmer et al., 2019). The use of data is widespread across every sector, thanks to their versatility; typical uses include improving products or services’ quality and efficiency, personalisation, matching, and discriminating between different consumers groups or individuals (Goldfarb & Tucker, 2019). Moreover, the rapid growth of online platforms has raised new challenges to competition and privacy authorities, who need to assess the potential outcomes of data-driven business models correctly. Over the years, many authors have developed models to study the effects of data on various aspects, such as market structure, competition, welfare, and privacy. However, most of these works have only focused on a specific data effect in peculiar settings. This, in turn, has created a conundrum: while the impact of data is widely analysed, the specificity of most studies makes it difficult to abstract more general insights. Moreover, since little is known about the data collection and sale processes (Montes et al., 2019), the theoretical models currently outnumber the empiric papers where the analysed strategies can be observed in action. Thus, there can be a perceived detachment between the analysed models and real-world situations. Data have been described as ‘the oil of the digital era’Footnote 1, and there is widespread consensus that their use significantly influences the economy. However, data have a considerable number of applications and reviewing how all of them impact market outcomes may result in ambiguous insights that would be of little help for policymakers or for suggesting future research developments. To set a boundary on the scope of this work, I focus my attention on models where data are explicitly modelled as an input to a decision problem. This survey is thus positioned in the strand of literature commonly referred to as digital economics. This choice excludes most of the literature regarding artificial intelligence and machine learning, as that strand of literature often focuses on data-enabled technologies rather than on how different quantities (or qualities) of data affect such technologies. The reader can refer to Agrawal et al., (2019) for a broad analysis of artificial intelligence and economics and Abrardi et al., (2021) for a comprehensive survey on artificial intelligence and machine learning. Recent contributions have aimed to review the growing literature on digital economics by finding common characteristics that could help the authors abstract from the individual models. Goldfarb & Tucker (2019) organise the literature by identifying five types of cost reductions that stem from digital technologies and how they impact market outcomes. Bergemann & Bonatti (2019) instead focus on the characteristics of information products and their sale and the interaction between firms and data intermediaries. The contribution of the present review is twofold. First, the literature on digital economics has widely expanded in the last years, bringing new approaches and insights that could help direct future research and policy action. This survey aims to organise these recent additions and link them to previous research developments. Whenever available, I also present related evidence from empirical papers to better frame the insights described in the theoretical models. Second, I find that the assumptions regarding data collection are a strong driver for the models’ market outcomes, regardless of the specific data use. Thus, I organise the literature depending on how data collection is modelled. This approach allows me to extract general insights that hold across different models and assumptions. The survey is organised in three sections, each dedicated to a class of models. First, I analyse the studies where firms collect data without strategically interacting with other actors. Examples include models where firms exogenously have data from the start or when firms can acquire data by paying a marginal cost. Second, I focus on papers where firms acquire data through single or repeated interactions with consumers. In this class, firms consider the trade-off between data collection and its effect on consumer behaviour and the intertemporal effects of data acquisition (Chen, 1997; Fudenberg & Tirole, 1998). Third, I analyse models where firms can acquire data from strategic third parties, referred to as data intermediaries. These actors usually function as data collectors and aggregators, compounding different sources to better profile consumers (FTC, 2014). As data intermediaries serve multiple firms, their selling strategies consider how selling data to a given firm impacts its rivals. Thus, data intermediaries have a higher degree of internalisation of the overall data effects when compared to the first class of models. The class subdivision based on the data collection process allows me to abstract general insights within the class itself, regardless of the specific data uses. When firms collect data without strategic interactions, data have a pro-competitive effect. The increase in competition is due to firms’ overcollection and overuse of data, as they have a limited internalisation of data externalities. On the other hand, data overcollection raises privacy concerns that policymakers should consider when accounting for the effects of data. When firms obtain data from consumers, the effects of data strongly depend on firms’ symmetry. Data acquisition and use in repeated periods can exacerbate a firm’s starting advantage, potentially increasing concentration and even leading to market tipping. Moreover, firms can strategically trade or share data to limit their interaction with consumers and reduce the compensation they pay to consumers for data. This strategy is especially relevant when consumer data are correlated, as even small datasets can help firms infer information on consumers who did not disclose their data. Policymakers should thus pose particular attention to data sharing and data-driven mergers. Finally, data acquisition through intermediaries results in various outcomes that should be a concern for policymakers. Data intermediaries strategically sell their datasets to temper competition in the downstream markets to extract more profits at the expense of both firms and consumers. Moreover, the high concentration of the data intermediaries’ industry (FTC, 2014) grants them substantial market power, and competing data intermediaries can strategically coordinate their actions to temper competition between them. These insights suggest that further research is needed to assess better if and which policy interventions should be implemented to limit consumer harm in these scenarios. The survey is organised as follows: in Sect. 2, I focus on models where firms acquire data without strategic interactions with other actors. Section 3 describes works where firms acquire data from their interaction with consumers. Section 4 analyses papers where firms acquire data from data intermediaries. In each of these sections, I briefly describe the development in the theoretical models, highlighting the main differences and findings. I then abstract from the individual models to gather more general insights and policy implications that hold true for the entire class and better assess the effects of data that arise from each data acquisition method. Finally, Sect. 5 concludes.",2
49,4,Journal of Industrial and Business Economics,30 August 2022,https://link.springer.com/article/10.1007/s40812-022-00230-4,Follow the cloud! The impact of ICT on Italian provinces’ trade,December 2022,Marinella Boccia,Anna Maria Ferragina,Stefano Iandolo,Female,Female,Male,Mix,,
49,4,Journal of Industrial and Business Economics,02 September 2022,https://link.springer.com/article/10.1007/s40812-022-00231-3,The relationship between online retailing and the regional economy,December 2022,Oleg Andreev,Cong Phan The,Lesya Bozhko,Male,,Female,Mix,,
49,4,Journal of Industrial and Business Economics,31 October 2021,https://link.springer.com/article/10.1007/s40812-021-00201-1,"Joint optimization of inventory planning, maintenance policy and pricing for perishable complementary products by considering the product freshness and technology level",December 2022,Ali Salmasnia,Ali Talesh-Kazemi,Mohammad Reza Maleki,Male,Male,Male,Male,"Optimization of production run length and the selling price has been always one of the main concerns in both academic and industrial environments. However, in order to simplify the model, most of attempts in this area have considered two unrealistic assumptions. The first assumption includes the imperishability of products over the time. However, due to the specific properties of perishable products, it is vital to consider the reduction of inventory level with a fixed rate. In the second assumption, it is considered that the machine failure never happens implying the machine availability during the production cycle However, the machine may face accidental failures which justifies the necessity of maintenance policies. To control the number of component’s failures and reduce the cost, the proposed model considers two important factors. These two factors include: (1) the technology level of manufacturing process which affects the component’s reliability; and (2) non-periodic maintenance operations to keep the component’s reliability at a satisfactory level. The overall level of on-hand inventory is dependent on the values of demand and production rates. The demand rate, on the other hand, is affected by the product price and customer waiting time for the products while the value of production rate depends on the machine availability. Indeed, the inventory level depends on the product price and the type of maintenance strategy. Therefore, it is vital to take into account the interdependency among three topics of inventory planning for perishable items, product pricing and maintenance scheduling. Nevertheless, these topics have been evaluated separately. Examples include You et al. (2010), Cheng et al. (2015), Zhou et al. (2016), Bellandi et al. (2020), Shabgard (2021), and Rey and Madiès (2021). Pricing and inventory strategies were previously established by two different sectors. However, integration of pricing and inventory decisions using hybrid models have recently attracted a great attention. In this regard, considering a multi-period process, Chang et al. (2006) established an economic model by integration of pricing and inventory strategies when the demand’s pattern is random. Tsao and Sheen (2008) explored the problem of dynamic pricing and inventory planning for deteriorating items in special case of permissible delay in payments. Eilon and Mallaya (1966) considered the inventory problem of semi-perishable goods where the demand’s pattern is sensitive to the product price for the first time. Mo et al. (2009) studied an inventory policy in which the demand pattern is affected by the selling price and inventory level. Using a Markov process, Yu et al. (2017) extended a hybrid model by integration of production, pricing and substitution strategies for two high-end and low-end products. Further information regarding the integration of pricing and inventory decisions can be seen in Lin et al. (2015), Banerjee and Sharma (2010), and Tiwari et al. (2018). In general, a product is referred to as “perishable” when its value decreases over the time. For instance, medicine, fruits and vegetables, electronic appliances, and etc., are categorized as perishable products concurrently. Ghare and Schrader (1963) studied joint formulation of inventory and pricing policies when the items decay exponentially. Considering both perishable and nonperishable items, Abed (2008) presented a comprehensive model taking into account shortage, backorder and lot sale costs. He et al. (2010) provided a hybrid production-inventory planning model where multiple markets sell their perishable products. Similar to Mo et al. (2009), Hsieh et al. (2010) extended an inventory planning model where the demand rate is affected by the inventory level. Under an infinite horizon, Li et al. (2009) presented a hybrid pricing and inventory planning formulation for the perishable items. Besides, Dye (2007) considered both pricing and perishing of deteriorating products when the deterioration rate is known and continuous. Abed (2003) combined lot-sizing and pricing decisions for the perishable products under finite production by consideration of partial backordering and lost sale costs. After that, Goyal and Giri (2003) developed two hybrid production-inventory models where the demand, production and deterioration rates vary over time. Teng et al. (2007) evaluated two pricing and lot-sizing models by incorporating both the backlogging cost and the cost of lost goodwill. Furthermore, Shavandi et al. (2012) extended Abad (2003) formulation by considering multi-product production processes in which the perishable items are classified as: substitute, complementary and independent types. In complementary-type products, the consumption of one type is dependent on the consumption of the other ones. For example, tennis balls and rockets, tea and sugar, and some computer components are some examples of such products. In these cases, the purchasing amount of one type is dependent on the demand value of its complementary items. As a consequence, the optimum inventory levels and the selling price of two specific products should be determined simultaneously. In this regard, Zhang et al. (2011) considered a two-product inventory system where the demand of a minor product can be modified by the demand value of the main one. Mukhopadhyay et al. (2011) considered an exclusive duopoly market in which two different companies produce complementary products while each company has private information about market demand and decides whether to notify it to another one or not. Li (2010) considered complementary products manufactured by two separate manufacturers. He examined the impact of manufacturers' risk-aversion on the relative virtues of integrated manufacturing. Bilotkach (2010) addressed pricing and quality choices for complementary products. An influential factor in demand pattern of perishable items is the product freshness which decreases over the time. Obviously for perishable items, as the product freshness decreases, so does the demand rate. Despite this fact, most models related to perishable products neglect the relationship between the demand function and product freshness. Taking into account the relationship between the demand and product freshness originates from the work conducted by Fujiwara and Perra (1993). Bai and Kendall (2008) proposed a novel model in which the demand rate is affected by both the displayed inventory and product freshness state. Demirag et al. (2017) introduced a novel EOQ model where the product freshness, as an effective factor in customer demand, is dependent on its expiration date. One can refer to Piramuthu and Zhou (2013) and Banerjee and Agrawal (2017) for more details. In recent years, the integration of manufacturing scheduling and maintenance planning has gained a considerable amount of attention. In this regard, Wienstein and Chung (1999) proposed a three-stage formulation to integrate the maintenance and inventory decisions in a hierarchical production system. Under a single machine condition, Cassady and kutanoglu (2005) introduced a hybrid model based on preventive maintenance (PM) and production planning. A multi-item capacitated lot-sizing problem based on the combination of production scheduling and PM strategy is evaluated by Aghezzaf et al. (2007). Then, Aghezzaf and Najid (2008) used a nonlinear mixed-integer formulation to integrate the production and PM decisions. Nourelfath et al. (2010) considered a multi-state manufacturing system and narrowed their focus to integrate PM with tactical production decisions. An integrated production-inventory-maintenance model to obtain optimal lot-sizing and PM strategy has been presented by Nourelfath and Châtelet (2012). Under a multi-product manufacturing process, Mifdal et al. (2015) established a hybrid production-maintenance model. Gan et al. (2015) evaluated concurrent optimization of maintenance and manufacturing planning by controlling spare items. Taking into account a multi-unit system, Jiang et al. (2015) studied interconnection between inventory and maintenance planning under inventory deterioration. Salmasnia et al., (2017a, 2017b) formulated a hybrid mathematical model taking into account the economic production quantity, maintenance scheduling and statistical process monitoring and solved it using particle swarm optimization (PSO) algorithm. For further details regarding this topic please refer to Dellagi et al. (2017), Keizer et al. (2017), Salmasnia et al. (2018), Zahedi- Hosseini et al. (2018) and Salmasnia and Mirabadi (2017). The existing gaps in above-mentioned articles are highlighted in Table 1. In order to bridge the gap between the research-based knowledge and practice, this article proposes a hybrid mathematical formulation to optimize pricing, maintenance policy, and inventory planning decisions for perishable complementary products. For getting closer to practical applications, the proposed model takes the products freshness into consideration. Furthermore, the proposed model considers the technology level of producing perishable products which has a considerable impact on component’s failure rate. In order to optimize the total profit per unit of time, we consider the inventory level as a function of demand and production rates. Moreover, the demand rate is considered as a function of customer waiting time and product price, while the production rate is related to the machine availability. The structure of this article has organized as follows: In Sect. 2, the problem is defined and the model assumptions are presented. In Sect. 3, the proposed hybrid model based on the integration of inventory planning, pricing and maintenance scheduling for perishable complementary products is described. Section 4 explains the solution algorithm to obtain the optimal results. Section 5 contains numerical example, comparative studies, and sensitivity analysis. Finally, concluding remarks are discussed in Sect. 6.",
49,4,Journal of Industrial and Business Economics,28 March 2022,https://link.springer.com/article/10.1007/s40812-022-00214-4,Innovation and the persistence of monopoly under diseconomies of scope or scale,December 2022,Flavio Delbono,Luca Lambertini,,Male,Male,Unknown,Male,"Since the indirect debate between Schumpeter (1942) and Arrow (1962), the relationship between market structure and innovative activity has been of long-standing interest in industrial economics. Especially since the late ’70s of the past century a rapidly growing literature has tackled such relationship within a variety of models.Footnote 1 The main findings of such models are by now summarized also in advanced textbooks of Industrial Organization (see, e.g. Tirole 1988). In this paper, we focus on a small but highly influential subset of that literature, dealing with the persistence of monopoly under technological progress and the threat of entry. More precisely, we shall focus on the models by Gilbert and Newbery (1982) and Reinganum (1983). Both papers investigate a setting which may take the structure of an auction concerning a patented innovation which the inventor wants to sell, not being endowed with any productive facilities. Gilbert and Newbery (1982) consider a deterministic auction for a product innovation protected by an everlasting patent between an incumbent monopolist (firm I) and a potential entrant (firm E). They prove that firm I has a greater incentive than firm E to obtain the patent for the new product, even a sleeping patent, to preempt entry. In their own words, this is because “the monopolist will spend more on R&D than the rival if entry results in any reduction of total profits below the joint maximization level ” (p. 516, italics in the original). Reinganum (1983) models a stochastic race for a drastic innovation, showing that the entrant has a greater incentive than the incumbent to patent. She reformulates Gilbert and Newbery’s model considering “a case of cost reducing innovation in an industry with constant returns to scale” (p. 741, italics added). Notice that this is the setting which typically entered the textbook presentation of the debate on the persistence of monopoly (see, e.g., Tirole 1988, Chap. 10). Apparently, the presence of uncertaintyFootnote 2 drives the opposite conclusion reached by Reinganum with respect to Gilbert and Newbery. Such a conclusion, however, would be misleading: other assumptions of their models play a key role and should be taken into account. We shall show indeed that, in a deterministic setting, the incumbent may lose the auction for the product innovation modelled in Gilbert and Newbery (1982) and win the race for the process innovation as modeled in Reinganum (1983). In fact, the presence of diseconomies of scope may disadvantage a multiproduct incumbent and prevent it from deterring entry by a single-product duopolist. This scenario is ruled out by Gilbert and Newbery’s assumption that “the monopolist suffers no diseconomies in the production of the substitute relative to production by rival firm” (p. 516). On the other hand, by considering diseconomies of scale in the production of the homogeneous good, even without uncertainty in the R&D technology, Reinganum’s conclusion may be reversed with the incumbent monopolist winning the patent race and strenghtening its leadership. The remainder of the paper is organised as follows. In Sect. 2, we revisit Gilbert and Newbery’s model of product innovation, illustrating the consequences of diseconomies of scope. In Sect. 3, we explore the impact of decreasing returns to scale in Reinganum’s model of process innovation. Section 4 concludes.",
49,4,Journal of Industrial and Business Economics,29 October 2022,https://link.springer.com/article/10.1007/s40812-022-00234-0,"How exogenous is war? The links between economic, military, trade and industrial policies in the upsurge of Russia-Ukraine conflict",December 2022,Antonello Zanfei,,,Male,Unknown,Unknown,Male,,
49,4,Journal of Industrial and Business Economics,04 July 2022,https://link.springer.com/article/10.1007/s40812-022-00219-z,A warning from the Russian–Ukrainian war: avoiding a future that rhymes with the past,December 2022,Sergio Mariotti,,,Male,Unknown,Unknown,Male,"«History does not repeat itself, but it often rhymes» (credited to Mark Twain) The war in Ukraine goes hand in hand with other manifestations that signal the non-zero probability of a future of severe economic and political instability and of possible large-scale, if not global, conflicts. Economic protectionism and trade wars are escalating. Global and local economic crises are slowing down the world’s economy and are opening up prospects for much lower growth rates than those experienced in past decades. The emergence of new economic superpowers (China, in primis) calls into question the market and geopolitical equilibria that have been consolidated in the “Long Peace” period since after the Cold War. Global value chains (GVCs) are showing a strong reactivity to endogenous and exogenous shocks, as evidenced by the COVID-19 epidemic, thus opening the way toward possible de-globalization and regionalization processes. In ideological and political terms, nationalism and populism oppose and threaten multilateralism. Military conflicts are growing throughout the world, and now also affect industrialized regional areas, such as in Ukraine. This scenario has even made Francis Fukuyama think that not only should “the end of history” be postponed—as human history is still a long way from reaching its peak and its end, in terms of the social and cultural development of humanity—but that perhaps «the end of the end of history must be admitted».Footnote 1 These manifestations seem to be caused by the growing imbalances and instability of the global economic and political order. They interact with each other and can activate feedback loops that may lead to a very serious disease for humanity, which needs to be treated before it explodes in all its pathological effects. Understanding this complex evolving world requires a massive collective effort by the best human minds. Much more modestly, this article aims to contribute to the formation of an awareness of the phenomena closest to industrial economics and to open a future, hopefully fertile, debate on policy remedies that can be taken to cure the causes rather than the manifestations of the disease. The first section provides empirical evidence on some of the main manifestations, namely economic protectionism, the slowdown in the world’s economic growth, and the reorganization of GVCs. The second section proposes a vision of the dynamics that link these factors in determining their developments and outcomes, including military conflicts. To this end, reference will be made to recent literature, based on comparative historical analysis (CHA), which studies the dynamic causalities, historical sequences and temporal structures of world events in distant time periods, in order to draw comparative historical conclusions and lessons. The third session opens the way toward some industrial policy considerations and encourages a more in-depth debate on such considerations.",20
49,4,Journal of Industrial and Business Economics,10 August 2022,https://link.springer.com/article/10.1007/s40812-022-00228-y,"NATO defense demand, free riding, and the Russo-Ukrainian war in 2022",December 2022,Justin George,Todd Sandler,,Male,Male,Unknown,Male,"With the collapse of the Soviet Union in 1991, the future mission of the North Atlantic Treaty Organization (NATO) was in doubt because the Russian Federation (henceforth, Russia) did not appear to pose a territorial or existential threat to Europe (Gompert & Larrabee, 1997; Sandler & Hartley, 1999). In fact, Russian military expenditure (ME) fell during 1992–1999 by about 63% in constant 2019 US dollars (Stockholm International Peace Research Institute (SIPRI), 2022a). Many of NATO’s 16 allies during the 1990s reduced their share of gross domestic product (GDP) devoted to ME in order to take advantage of a “peace dividend” as government spending shifted from defense to social welfare. As a consequence, there were short- and long-run downsizing of the military-industrial complex among many NATO allies, which reduced the size of their armed forces, limited the procurement of weapon systems, and reconsidered the next generation of weapon platforms. The post-Cold War era brought nontraditional warfare involving nonstate actors (namely, terrorist groups) and a greater incidence of intrastate wars. The military arsenals or tactics of the Cold War did not necessarily serve to protect NATO members against the new threats and foes. To address reduced defense spending on weapon platforms and the concomitant smaller production runs, the post-1991 era witnessed mergers among defense firms in order to promote economies of scale and learning in a shrinking arms market (Hartley, 2014, 2017; Sandler & Hartley, 1999, Chap. 5). Competition among defense firms drove smaller firms out of business as their reduced production runs could not cover the large, fixed cost of weapon development for defense systems (e.g., fighter jets, bombers, frigates, tanks, and armored vehicles). Thus, defense manufacturers exited, either closing altogether or shifting to consumer goods. The arms manufacturers that emerged during the post-Cold War reorganization were larger and more efficient. Some of those defense producers offer a wider variety of defense systems to further reduce unit costs through economies of scope as multiple systems utilized the same fixed inputs. To maintain a post-Cold War relevance, NATO sought new missions by embracing peacekeeping operations outside of NATO territory when its interests were at stake. Such operations protected NATO’s concerns by limiting the spillover of conflicts, preserving resource supply lines, maintaining trade flows, curbing refugee inflows, and curtailing the spread of transnational terrorism. Noteworthy NATO peacekeeping missions involved Bosnia and Herzegovina (1992–2004), Kosovo (1999–present), Afghanistan (2001–2014, 2015–2021), and elsewhere (Kim & Sandler, 2020; Shimizu & Sandler, 2010). The need for NATO peacekeeping increased with the rise in intrastate conflicts after 1991 and the overburdening of UN peacekeeping (Sandler & Hartley, 1999). For continued relevance, NATO also expanded its membership between 1999 and 2020 by 87.5% from 16 to 30 allies. Two potential members – Finland and Sweden – are currently applying for membership in reaction to the Russian invasion of Ukraine in 2022 (The Economist, 2022a). NATO expansion not only serves to augment North America-European military cooperation, but also to confront an ever-nationalistic Russia under President Putin. NATO enlargement included seven countries, once members of the ex-Warsaw Pact, which confronted NATO during the Cold War. The ex-Soviet members of today’s NATO view their membership as a mean to ensure their security from a future Russian threat. Even though NATO is a defensive alliance, Putin views the alliance’s eastward expansion to the borders of Russia in the case of a number of new NATO allies (e.g., Estonia, Latvia, and Lithuania) as a threat to Russia. Ukraine’s expressed ambition to join both NATO and the EU eventually culminated in Russia’s invasion of Ukraine on 24 February 2022 with an apparent objective to install a pro-Russia puppet government (The Economist, 
2022b; Wikipedia, 2022). The primary purpose of the current study is to apply the Olson and Zeckhauser’s (1966) economic theory of alliances to uncover the pattern of ME burden sharing among NATO allies for 1991–2020 and 2000–2020, prior to the 2022 Russian invasion of Ukraine. Additionally, our burden-sharing analysis accounts for the spatial connectivity among NATO allies in terms of contiguity, inverse distance, economic trade, contiguity and US power projection, contiguity to Russia, and contiguity to Russia and Ukraine. The spatial analysis of ME among allied and adversarial countries traces back to Flores (2011), George & Sandler (2018, 2021), Goldsmith (2007), Skogstad (2016), Xiaoxin & Bo (2021), and Yesilyurt & Elhorst (2017). Our current approach shows that, despite NATO’s expansion and enhanced ME, the alliance displayed a great deal of free riding or relying on the defense spending of other NATO allies.Footnote 1 Such free riding is the hallmark of a non-unified alliance where members largely pursue their unilateral interests, including supporting their social programs over defense outlays. The large buildup in Russian real ME from 2000 to 2020 of 183.4% was not met with commensurate increases in NATO allies’ ME. Similar ME trends are difficult to compute for NATO because 11 allies joined during 2000–2020, which artificially bolsters the rise in NATO ME when accession dates are used. If we compute NATO’s ME trend for the members in 2020 excluding Iceland and North Macedonia, ME increases by just 46.9% in constant 2019 US dollars during 2000–2020. Figure 1 displays the pattern of Russian ME (solid line) and the 28 NATO allies’ ME (dashed line) in constant 2019 US dollars during 1992–2020.Footnote 2 Because of scale differences, Russian ME is measured on the left-hand vertical axis, and NATO aggregate ME is measured on the right-hand vertical axis. The buildup of Russian ME is clearly seen in Fig. 1 from 2000 on, following a marked decrease during much of the 1990s; however, 28 NATO allies’ ME trend, though non-monotonic, is slightly down after 2010. Russian and NATO military expenditure, 1992–2020 Surprisingly, NATO allies responded negatively to the increases in the ME of allies, which are either contiguous to Russia or contiguous to Russia and Ukraine during 1991–2020 or 2000–2020. The associated free riding to frontline allies’ ME suggests that, prior to the Russian 2022 invasion of Ukraine, NATO allies did not act as though Russia was a threat. That characterization is further supported by NATO allies’ negative response to Russian ME found in three of five spatial models. Despite the threat to NATO allies embodied by the Russian takeover of Crimea and its support to Russian separatist in Eastern Ukraine, we uncover little or no increase in NATO ME during 2015–2020. This post-2014 finding also suggests that NATO allies have not made much progress in their pledged increase in defense spending, agreed upon during the 2014 Wales Summit (George & Sandler, 2021; NATO, 2014). At the summit, NATO allies agreed to spend at least 2% of their GDP on defense by 2024. Moreover, allies pledged to allocate 20% of their annual defense budgets to the purchase of new weapons to address challenges posed by Russia and other potential adversaries. If those procurement pledges are eventually fulfilled, then the defense industries in the United States, France, Germany, the United Kingdom, Spain, and Italy will expand since these countries are the main weapon suppliers in NATO (SIPRI, 2021). In a counterfactual exercise, we include Finland and Sweden as though they were NATO members during 1991–2020 to gauge what impact their recently requested membership application may have on free riding if they continue their past defense spending trend. If past is prologue, then their inclusion will worsen free riding in NATO for all of the various weighting schemes. Our counterfactual result strongly suggests that both potential entrants must devote a larger share of their respective GDP to defense and that NATO allies must not free ride on these entrants’ increased ME if free riding is to be curbed in the contemplated 32-member NATO. Prior to the recent Russian invasion of Ukraine, any Russian leader studying the free-riding pattern characterizing NATO would conclude that the alliance would not have responded with a united front as has been the case thus far. In many ways, the invasion seemed to renew the NATO mission and energized more allies to allocate more GDP to ME. We will not know if this is the case until years in the future; however, recently announced ME increases by Germany, the United Kingdom, and France strongly suggest greater defense burden sharing as we will discuss in the conclusions. The remainder of the paper contains seven additional sections. Section 2 provides background on NATO, while Sect. 3 reviews the economic theory of alliances. In Sect. 4, we present the theoretical model, whose reduced-form equations give the various defense equations to be estimated for different spatial connectivity. Section 5 presents the empirical methodology and data for our estimates, followed by empirical results and robustness tests in Sect. 6. For NATO, arms producers and exporters are indicated and discussed for three five-year intervals in Sect. 7 in light of pledged increased burden sharing and Russian challenges to its defense exports. Section 8 contains concluding remarks.",12
49,4,Journal of Industrial and Business Economics,25 September 2022,https://link.springer.com/article/10.1007/s40812-022-00232-2,Too much of a good thing? Russia-EU international trade relations at times of war,December 2022,Lucia Tajoli,,,Female,Unknown,Unknown,Female,"Since the fall of the former Soviet Union, Russia’s economy, growth and international trade followed an uneven path, especially compared to other so-called “transition economies”. The transformation of a country from a centralized economic system into a market economy is far from straightforward and easy, as a large number of studies have shown.Footnote 1 In the case of Russia, a very large country lacking more than others market foundations and capitalist institutions, this has proven even more complex (Ericson, 1991, 1999). The reliance on its huge natural resources brought about the under-development of a number of manufacturing sectors, some of them formerly quite advanced. As effectively summarized by Alexeev and Weber (2013): After a steep decline during most of the 1990s, Russia’s economy was growing at almost 7 percent beginning in 1999 ….. Although the impressive economic growth since 1999 has raised the standards of living and put scores of Russians on the Forbes billionaires list, it has not solved a number of deep economic and social problems …. The country continues to suffer from low labor productivity, distorted and undiversified structure of the economy, with its heavy reliance on natural resource extraction, low life expectancy, high income inequality, and weak institutions, including pervasive corruption and poor property rights protection. (from the Introduction to The Oxford Handbook of the Russian Economy). In a sort of “Dutch disease” kind of effectFootnote 2 (Corden & Neary, 1982; Sachs & Warner, 1995, Behzadan et al., 2017), the curse of natural resources and especially fossil fuels abundance, together with the lack of many important institutions necessary for the proper functioning of a market economy have hampered economic development and growth in Russia. The reliance on fuels is spread throughout the Russian economy, and it is not only affecting manufacturing production and trade: current estimates indicate that revenues from oil exports make up 40% of Russia’s federal budget (Gordon, 2022). The fragile evolution of the Russian economy and trade is associated with the position taken by the Russian government in international affairs. Previous studies have highlighted the existence of a relationship between economic interdependence and the peaceful or belligerent attitude of countries, starting from Keynes (1919). More recently, Martin et al. (2008) and Jackson and Nei (2015) show empirically that this relationship is more multifaceted than expected. Analyzing international relations over two centuries, Copeland (2015) demonstrates the crucial role of expectations on the trade environment for countries’ relations. According to this analysis, when leaders have positive expectations of the future trade environment, they want to remain at peace in order to secure the economic benefits that enhance long-term power. When, however, these expectations turn negative, leaders are likely to fear a loss of access to raw materials and markets, giving them more incentive to initiate crises to protect their commercial interests. We argue that the weak position of Russia in international trade and the associated lack of an “economic superpower” status are among the reasons that explain Russia confrontational attitude toward its neighbors. The European Union (EU) relation with Russia also followed an irregular path, alternating moments of tighter economic integration with political crises. Often, short-term economic convenience, for example in terms of quick availability of gas and oil, has shadowed the risk inherent in the Russian market for European firms, and in the lack of diversification of suppliers. Consequently, what we observe now is a very unstable situation, with a reciprocal dependence. In this paper, we aim to show how on the one hand, the reliance on a large market for fuels has hindered the stable economic development of Russia and the expansion of its trade position, creating an uneven relationship with the EU, and potentially weakening Russia’s interest in a stable economic environment. On the other hand, this situation determined a (much weaker) dependence on Russia’s fuel in some EU sectors. This uneven situation is unlikely to foster economic development and stable trade links, while it is expected to bring about tension or possibly conflicts, as we have observed in the past months.",4
49,4,Journal of Industrial and Business Economics,12 October 2022,https://link.springer.com/article/10.1007/s40812-022-00233-1,Between a rock and a hard place: European energy policy and complexity in the wake of the Ukraine war,December 2022,Christine Sturm,,,Female,Unknown,Unknown,Female,"Over the past decades, the European Union (EU) maneuvered itself into a strong dependence on energy imports from Russia. In 2020, the EU imported, for example, more than four fifth of its fossil-fuel demand and almost its entire nuclear fuel from countries outside the Union. Among these countries, Russia was the EU’s largest supplier of fossil fuels and its second largest supplier of nuclear fuels (Eurostat, 2022a; ESA 2021). Cheap fuel imports from Russia helped European economies flourish, but also made these economies extremely vulnerable to supply disruptions, allowing the autocratic Russian government to put them at risk (Borrell, 2022). Despite early warnings [E.g., the annexation of Crimea in March 2014, the escalation of Russo-Ukrainian conflict in 2015, and the large movements of Russian troupes and military equipment near the Russo-Ukrainian border in 2021.], extensive energy security plans [E.g., the European Energy Security Strategy (EU, 2014).], vast scholarly literature about economic interdependence and increasing mistrust between Europe and Russia [E.g., Högselius (2013); Kundnani (2014); Copeland (1996); Copeland (2015); Krickovic (2015); Gustafson (2020); Sziklai et al. (2020); Groitl (2021); Tajoli (2022); Mariotti (2022).], Russian fuels were “too attractive to resist” (Högselius, 2013, p. 200), and major European economies have done little to reduce the Union’s vulnerability to Russian energy imports prior to February 2022, when Russia started an unprovoked war of aggression against Ukraine. Indeed, since 2014, when the European Commission announced the EU’s Energy Security Strategy, the Russian imports of natural gas – the commodity that bears the highest potential to harm the Union’s economies – increased by more than 50% [I.e., from 3,620 PJ in 2013 to 5,631 PJ in 2020 (Eurostat, 2022e; EU, 2014).]. In addition, calls to stop the gas-pipeline project Nord Stream 2Footnote 1 did not hinder the German, Dutch, French, and Austrian companies BASF/Wintershall, E.on/Uniper, Shell, Engie, and OMV to invest in this huge Russian pipeline project (Gazprom, 2015; Gustafson, 2020); Germany did not stop its chemical giant BASF to transfer assets relevant for the nation’s energy security to Gazprom;Footnote 2 France and Spain decided to cancel the MidCat pipeline project that would have allowed the transport of non-Russian gas from Spain to Northern Europe;Footnote 3 Germany’s Federal Antitrust Authority (Bundeskartellamt) approved three days before Russia’s invasion in Ukraine the transfer of Shell’s stakes in the PCK refinery to Rosneft;Footnote 4 and this list could be continued with several other delayed or abandoned infrastructure projects aimed at strengthening the EU’s energy security, or cases of complex entanglements between major European and Russian companies. In other words, “Europe is an empty box in terms of energy […] and never thought about a strategy for energy security,” as Claudio Descalzi – the CEO of the Italian utility ENI – said in March 2022 (Reuters, 2022a). Shocked by the war’s atrocities, Western nations condemned Russia’s aggression and decided to support Ukraine’s efforts to defend its territory (EU, 2022b). Yet this is easier said than done, because the EU finances with its energy imports the war it so vehemently condemns. In addition, each time the EU policymakers try to ban Russian energy imports, they face the moral dilemma of weighing own losses in livelihood against more war victims in Ukraine (Independent, 2022; Bianco et al., 2022). Indeed, as Hungary’s reluctance to agree on banning Russian oil imports and the skepticism of Germany and Italy regarding a gas embargo have shown, Member States that expect higher economic burdens from sanction packages against Russia, are also more likely to question and/or oppose them (Gordon, 2022; GCEE, 2022a & b; Agora Energiewende, 2022b). Beyond quantifying the EU’s dependence on Russian energy imports and explaining how this dependence came to be, this paper analyses the sanctions against Russia, their multifaceted implications, and the Union’s efforts to regain its energy sovereignty, highlighting the problems related to the accelerated deployment of renewable energies. A comparative vulnerability study between Germany and Lithuania complements this analysis, unveiling two fundamentally different ways to approach energy security issues and tackle the looming energy crisis. What will become apparent in this article is that that (1) the war in Ukraine and the policy announcements that accompany it place long lasting burdens on Western economies; (2) trade cannot be a panacea for bridging political divides; (3) no invisible hand will somehow align profit-seeking with energy security goals; (4) a country’s historical background and its geographical location play – as the German-Lithuanian case demonstrates – an essential role in crafting its security strategies and being prepared to act in crisis situations; and (5) a robust political consensus is not enough for steering modern societies away from fossil fuels, to be successful, the transition towards renewable economies also requires breakthroughs in power storage technologies, extensive investments in critical infrastructure projects, and innovations in all economic sectors.",10
50,1,Journal of Industrial and Business Economics,16 January 2023,https://link.springer.com/article/10.1007/s40812-023-00256-2,Fifty years of developing knowledge that stands the test of facts,March 2023,Francesco Silva,,,Male,Unknown,Unknown,Male,,1
50,1,Journal of Industrial and Business Economics,08 December 2022,https://link.springer.com/article/10.1007/s40812-022-00244-y,Journal of industrial and business economics—Economia e politica industriale: a historical sketch of the first fifty years,March 2023,Sergio Mariotti,,,Male,Unknown,Unknown,Male,"In the 1950 and 1960 s, the world economy grew at twice the secular rate (Toniolo, 1998). In Italy, the uniqueness of those years caused a major misalignment between the economy, on the one hand, and institutions and the state, on the other hand, which were no longer able to support the growth imperatives of industrial capitalism (De Cecco, 2007). In the late 1960s, with the so-called “hot autumn”, workers’ wage claims exploded and the foundations were laid for a new form of trade unionism. Some of the largest groups in Italian capitalism sought to initiate a dialogue with the latter and the productive classes to reform the state and dissolve obstacles to growthFootnote 1. In parallel, the exceptional increase in income had favored mass schooling, and the “1968 student movement” expressed the deep cultural and ideological unease of the new generations in the face of economic and social challenges (Salvati, 1981). In the early 1970s, the Italian question became intertwined with international dynamics. The world economy saw its linear and seemingly balanced growth come to a halt. Salient moments, such as the end of dollar convertibility and the 1973 oil crisis, accompanied the restructuring of relations among the Great Powers (Lewis, 1980). In this context of double crisis, but also of economic and social reforms, the most culturally and socially committed Italian scholars immersed themselves in the facts and sought interpretations that could help untangle the knots that were blocking growth. They tried to apply the method and models of industrial economics and policy to the Italian and European case. In 1973, under the stimulus of Sergio Vaccà, professor at the Bocconi University in Milan, the Bollettino di Economia e Politica Industriale was born, which the following year took on the title of Economia e Politica Industriale (EPI). Two years later, on the initiative of Romano Prodi, professor at the University of Trento and then Bologna, the Rivista di Economia e Politica Industriale (REPI) came into being, which, in 1980, took on the title L’Industria - Rivista di Economia e Politica Industriale, gaining access to the publishing rights of this historic journal, founded in 1886. The flourishing of these initiatives is hardly surprising, as the most influential Italian intellectuals had become aware of the need to contribute to the solution of the complex problems posed by the new economic and social phase. Perhaps it may come as a surprise that these two journals are still alive and well: the former has taken on an international dimension since 2015 and it is published in English under the title Journal of Industrial and Business Economics (JIBE), while the latter has retained its original title and publishes articles in both Italian and English. The two journals have remained firmly grounded in their history and have engaged in fair competition over time, thus creating a scientific and social space for the meeting and discussion between Italian scholars of industrial economics. However, many of JIBE-EPI’s current contributors and readers may be unaware of its origins and history. Therefore, on the occasion of its fiftieth anniversary, it seems appropriate to propose a historical sketch of its first fifty years. This should not only be seen as a grateful celebration of those who have devoted part of their intellectual efforts and time to the Journal. In our view, it is the best way to explain its identity, that is, the characteristics that have ensured JIBE-EPI’s uniqueness as a non-conformist and non-mainstream journal, open to interdisciplinary contributions complementary to industrial economics, which has remained its key pillar. We believe it is essential not to lose the memory of these events in order to keep a straight rudder in navigating today’s crowded sea of international journals, which are sometimes pure containers of articles, being blurry in their aims, methods and content. Below, the narrative will focus on JIBE-EPI, but some comparisons will be made both with its closest “rival” in the domestic market and, far more importantly today, with other international journals. Before moving forward, we must caution the reader. Although our arguments are supported by data, they can sometimes come across as “biased,“ given our emotional attachment to the Journal. The latter certainly has many strengths, but also some weaknesses, discussed in the text. But not all flaws should be blamed on the Journal on its 50th birthday!",
50,1,Journal of Industrial and Business Economics,12 January 2023,https://link.springer.com/article/10.1007/s40812-023-00258-0,Global value chains: antecedents and new perspectives,March 2023,Mirabelle Muuls,Rajneesh Narula,Antonello Zanfei,Female,Unknown,Male,Mix,,
50,1,Journal of Industrial and Business Economics,04 November 2022,https://link.springer.com/article/10.1007/s40812-022-00236-y,Can value chain integration explain the diverging economic performance within the EU?,March 2023,Agnes Kügler,Andreas Reinstaller,Klaus S. Friesenbichler,Female,Male,Male,Mix,,
50,1,Journal of Industrial and Business Economics,03 January 2023,https://link.springer.com/article/10.1007/s40812-022-00255-9,On GVC and innovation: the moderating role of policy,March 2023,Yasmine Eissa,Chahir Zaki,,Female,Unknown,Unknown,Female,"In recent years, the mounting trend of global value chains (GVC) participation has slowed due to global investments accompanied with the absence of major liberalization initiatives (World Bank, 2020). The “slowbalization” wave is further augmented by the aftermath of COVID-19 pandemic crisis witnessing deliberate decoupling to unbind the interdependence between industries and countries and therefore prevent the domino effect stirring in crises (Coveri et al., 2020). In this respect, studying the benefits of outsourcing at the country level is crucial to scrutinizing the tradeoff of “reshoring” activities. Beside the conventional theories emphasizing the gains of GVC participation in terms of trade (Baldwin, 2013; Feenstra & Hanson, 1996; Grossman & Rossi-Hansberg, 2008), trade in value added is indeed advantageous in terms of other facets. This paper analyzes the association between GVC participation and countries’ innovation. While the nexus between trade in final goods and innovation has been examined (Ackigit & Melitz, 2021; Alessandria et al., 2021; Keller, 2004), GVC participation is also likely to have a knowledge driven effect. Indeed, backward participation linkages to GVC transmit embedded foreign knowledge to destination countries that can be signaled by countries’ innovation performance. Aslam et al. (2018) argue that, between 1995 and 2003, foreign knowledge enhanced productivity growth by 0.4% and the former led to more than doubling domestic productivity in developing countries between 2004 and 2014. Undeniably, the gains of international fragmentation of production in terms of technological spillovers are still subject to empirical exploration. While our study highlights the relation between backward linkages to GVC participation and innovation, results emphasize the potential prospect for developing countries in realizing innovation driven economic growth.Footnote 1 Using the simpleFootnote 2 offshoring definition, we synthesize the gains of GVC participation in terms of innovation by empirically estimating the association between GVC knowledge spillovers and resident patent per capita. In addition, auxiliary interfering factors in the GVC learning effect are empirically explored namely business environment, institutions, trade policy, competition policy, as well as intellectual property rights’ (IPRs) agreements. Indeed, foreign knowledge spillovers are particularly central for developing countries disadvantaged in technology production. On a flipside however, the learning effect of GVC participation is constrained by prevalent mitigating conditions. First, developing countries are underprivileged with rule of law as a subfactor of institutions’ quality. Second, strengthening IPRs through Trade Related Aspects of Intellectual Property Rights (TRIPS)Footnote 3 trade agreement is argued to be biased towards higher income countries exporting technology. Third, unapt non-tariff trade costs in developing countries discourages foreign exporters of technology (UNCTAD, 2022) and consequently hinders foreign knowledge spillovers. Fourth, lax competition policy disincentivizes innovation (Goto, 2009). Against this background, disentangling the impact of the stated preconditions is crucial to pledging the learning effect of GVC participation. We contribute to the existing literature by studying the multifactorial mitigating dynamism, which is novel to the empirically reviewed nexus between GVC and innovation. Results show a positive and significant relationship between the GVC knowledge spillovers index and domestic innovation. Moreover, we show that trade policy, competition policy, and IPRs agreements constitute a pile of interfering preconditions in the nexus between GVC participation and innovation. Our results remain robust when we use alternative measures for our two variables of interest. This paper is composed of five sections structured as follows: Sect. 2 reviews the literature on GVC and innovation. Section 3 presents the econometric specification and describes the data. Section 4 is dedicated to the empirical results of the relationship between GVC knowledge spillovers and resident patent per capita in a panel of 83 countries over a time span of 30 years. Section 5 concludes and offers policy implications to the end of fostering innovation particularly in lower-middle income countries.",
50,1,Journal of Industrial and Business Economics,03 January 2023,https://link.springer.com/article/10.1007/s40812-022-00247-9,The virtues and limits of specialization in global value chains: analysis and policy implications,March 2023,Andrea Coveri,Antonello Zanfei,,Female,Male,Unknown,Mix,,
50,1,Journal of Industrial and Business Economics,28 November 2022,https://link.springer.com/article/10.1007/s40812-022-00240-2,Late industrialisation and global value chains under platform capitalism,March 2023,Wim Naudé,,,Male,Unknown,Unknown,Male,"The digital “revolution” that emerged out of the technology of World War II and grew in significance in the 1980s, first with the personal computer, then in the 1990s with the World Wide Web, and eventually in the 2000s with mobile connectivity, artificial intelligence (AI) and big data, has significantly boosted and reconfigured international trade. Most importantly, aided by declining transport costs and trade liberalisation, the digital revolution has enabled the fragmentation of international trade,Footnote 1 as reflected in the ubiquity of global value chains (GVCs). A GVCFootnote 2 can be defined as “the series of stages in the production of a product or service for sale to consumer” where “at least two stages are in different countries” (World Bank, 2020: 17). The extent of a country or region’s participation in GVCs is measured by calculating a GVC participation rate.Footnote 3 Since the 1970s, the GVC participation rate for most countries has increased significantly, and participating in and upgrading in GVCs are widely seen as being necessary for industrialisationFootnote 4 (Hauge, 2020; UNCTAD, 2013). GVCs have, however, more accurately, been a “mixed blessing” for late industrialising countries (Pahl & Timmer, 2020; Rodrik, 2018). For instance, using input–output table data on 58 economies over the period 1970 to 2008, Pahl and Timmer (2020) found that participation in GVCs helped countries to raise their productivity significantly but that there was a “negative association between GVC participation and employment growth” (p. 1685). Moreover, the fragmentation of production and trade into GVCs has reduced the industrial policy space for late industrialising countries (Hauge, 2020). The rise of digital platforms over the past decade, a manifestation of what has been termed digital platform capitalism, has raised further concerns about the costs of GVCs for late industrialising countries, which may imply that even the positive productivity and growth effects may diminish and that their policy space may further shrink (Bonina et al., 2021; Grabher & van Tuijl, 2020). According to Kenney and Zysman (2016, p. 61), digital platforms are reconfiguring “globalisation itself.” Furthermore, according to Coveri et al. (2021: 3), digital platforms represent a “data-driven evolution of the transnational corporation” that extends their power and influence—and concentration. While digital platforms as technology may, as technological advances did in the past, make participation by late industrialisation countries in world trade easier, it may also further complicate industrial policy and hence the industrialisation and catch-up growth of developing countries. The evaluation of the relative costs and benefits of digital platform capitalism for late industrialising countries is, however, hampered by insufficient research on the relationship between GVCs and digital platform capitalism. As Lundquist and Kang (2021: 179) recently concluded, “the interaction between the digital economy and GVCs is not well explored.” Loonam and O’Regan (2022: 161) found that “there remains a lack of empirical understanding of how digital platforms can enable GVC strategy.” Because of these lacunas, the development consequences of digital platforms “are not entirely understood” (Koskinen et al., 2019: 3). Particularly, “there is far more work to be done to explore the ‘dark side’ of platforms for development” (Bonina et al., 2021: 893). By focusing on the relationship between technologies such as digital platforms and GVCs and the consequences for late industrialising countries, the purpose of this paper is to contribute towards addressing this research gap. Specifically, the paper contributes to understanding the development consequences of digital platforms—and the dark side of platforms for late industrialising countries. It argues that given the relationship between GVCs and digital platforms, there are three requirements for relevant industrial policies in late industrialising countries. The argument is constructed as follows. First (in Sect. 2), a critical description is provided of the digital revolution that has resulted in the technologies allowing the rise of both GVCs and digital platforms and platform capitalism. Then, in Sect. 3 it is argued that despite these technologies, there are diminished expectations of the 4th industrial revolution. In Sect. 4 it is explained how these technologies and their use by digital platforms make late industrialisation, including participation in GVCs, harder. Finally, in Sect. 5, three requirements for relevant digital industrial policies to support industrialisation and appropriate GVC participation in late industrialising countries are presented and discussed. Section 6 concludes.",
50,1,Journal of Industrial and Business Economics,16 December 2022,https://link.springer.com/article/10.1007/s40812-022-00245-x,Slowbalisation or a “New” type of GVC participation? The role of digital services,March 2023,L. Blázquez,C. Díaz-Mora,B. González-Díaz,Unknown,Unknown,Unknown,Unknown,,
50,1,Journal of Industrial and Business Economics,21 January 2023,https://link.springer.com/article/10.1007/s40812-022-00253-x,Using the global value chain framework to analyse and tackle global environmental crises,March 2023,Valentina De Marchi,Gary Gereffi,,Female,Male,Unknown,Mix,,
50,1,Journal of Industrial and Business Economics,10 November 2022,https://link.springer.com/article/10.1007/s40812-022-00237-x,"Internalization strikes back? Global value chains, and the rising costs of effective cascading compliance",March 2023,Ari Van Assche,Rajneesh Narula,,Male,Unknown,Unknown,Male,"Over the last two decades, the concept of global value chains (GVC) has crept up on international business and business economics as a substitute for the ‘classic’ hierarchically structured multinational enterprise (MNE) that managed its cross-border activities as a unified entity. However, the MNE—taken to mean an organization that exercises ongoing and active coordination and control over its spatially distributed subsidiaries and affiliates, most often through full ownership of these operations—is not an endangered species. What we have seen, instead, is that there are economic and strategic rationales for MNEs to ‘loosen the apron strings’ by choosing not to internalize certain value adding activities, and instead rely on market- and quasi-market mechanisms to exert control over their GVCs. In the context of this paper, full internalization means direct control through equity ownership, a diametrically opposed governance alternative to purely transactional arm’s length relationships such as outsourcing. While the latter may allow for lower production costs, cross-border market imperfections continue to prevail despite the reduction in transaction costs from increasing cross-border interdependence. That is, transaction costs have fallen greatly as a result of globalization, but there still remain non-negligible costs to firms from bounded reliability and opportunism of unaffiliated actors. A hierarchical setting of the classic fully internalized MNE provides a variety of coordination and enforcement mechanisms that markets do not, the primary raison d’être of the MNE in the first instance (Buckley & Casson, 1976). The success of the GVC as an alternative mechanism to organize cross-border activities derives primarily from three things. First, the capability of MNEs to act as a meta-integrator of multiple actors within a given supply chain and coordinate these activities so as to optimise the MNE’s rent-generating potential. Second, participants in GVCs have acquired the capabilities to effectively select from a variety of alternative governance mechanisms those that minimise the net transaction costs of ongoing contracts with its immediate suppliers such that these remain lower than the fully internalised supply chain. Third, that actors within each tier of the GVC learn how to create incentives and penalties that dissuade their lower-tier suppliers from acting opportunistically, and to comply with the expectations (whether contractually or socially imposed) of its immediate customer, or those of the lead firm. The GVC as we know it today has evolved from ‘direct contract reasoning’, with each actor taking responsibility only for its immediate (tier 1) suppliers, to whom they are contractually bound (and with whom there was active coordination and control). Social and regulatory pressures—both formal and informally—have pushed lead MNEs to accept greater responsibility for the labour practices of all the actors within their associated chain (Schrempf-Stirling & Palazzo, 2016) in what is known as a ‘full-chain approach’ (Humphrey, 2014) even where there are no direct commercial links to the lead firm. However, where there is a large network of suppliers, the associated increased transaction costs of such extensive monitoring are non-trivial. What we have seen, therefore, is a preference to implement ‘cascading compliance’, coupled with a degree of re-internalization. Cascading compliance presents a keen challenge to theory as its swift adoption across a variety of industries and geographical regions suggests that quasi-internalization is increasingly becoming the preferred alternative to both spot market relations and full internalization within GVCs (Asmussen et al., 2022; Narula et al., 2019). Under cascading compliance, MNEs rely on supplier codes of conduct and audit-based monitoring systems to exert control (without ownership) over the activities of their immediate first-tier suppliers, while also dictating how first-tier suppliers engage with their own suppliers (Narula, 2019). MNEs have heavily relied on this approach to promote higher environmental, social and corporate governance (ESG) standards among its non-equity GVC partners, in line with the growing expectations of external stakeholders that MNEs should be held accountable for ESG abuses throughout their GVCs (Locke et al., 2009). From a business perspective, cascading compliance has proven successful. However, from a societal perspective, the modest and uneven improvements in social and environmental conditions in many GVCs have tempered the enthusiasm about the practice (Van Assche & Brandl, 2021). There is ample evidence that even those MNEs with the best intentions struggle to ensure that compliance cascades along the GVC. For example, only recently, Uniqlo, Skechers, and Zara have been accused of exploiting forced labor in the Chinese Uyghur community. For some, these examples suggest that there is something important missing in the cascading compliance model (LeBaron, 2020), and in quasi-internalization more generally, that prevents its proper execution. This paper considers whether the contemporaneous rise of cascading compliance and the move away from the use of traditional hierarchies through equity ownership have affected the extent to which this reduces or increases the tendency of firms within GVCs to behave opportunistically. Is cascading compliance as a governance mechanism an effective alternative to curtail corporate misconduct among suppliers? How does this compare with the ‘old school’ fully internalised multi-country MNE with wholly owned subsidiaries? Does cascading compliance require MNEs to develop a new set of transaction-based capabilities? To what extent can the MNE expect each tier of suppliers (who are boundedly reliable) to shoulder the cost burden of monitoring their own suppliers’ standard compliance? Or must the lead firm accept the higher costs of coordination and monitoring necessary to achieve effective cascading control? Providing answers to these questions is timely, as multiple jurisdictions around the globe have started to enact binding regulations requiring MNEs to report on their supply chain’s social and environmental compliance through policy instruments such as the Dodd Frank 1502 law and the Forced Labor Prevention Act in the United States, the United Kingdom’s Modern Slavery Act, and France’s Duty of Vigilance law (World Economic Forum, 2022).",2
50,1,Journal of Industrial and Business Economics,10 January 2023,https://link.springer.com/article/10.1007/s40812-022-00249-7,Digital technology-enabled governance for sustainability in global value chains: a framework and future research agenda,March 2023,Stephanie Lu Wang,,,Female,Unknown,Unknown,Female,"Global value chains (GVCs) (i.e., the separation between different stages in the production and consumption of materials and products of value in different parts of the world) account for about 70% of global trade (OECD, 2022). GVCs are economically efficient by leveraging different comparative advantages and specializations in different countries. Meanwhile, substantial environmental, social, and governance (ESG) related controversies are also GVC-linked. For instance, 80–90% of greenhouse-gas emissions are categorized as “Scope 3” emissions or indirect emissions that occur across GVCs (Practice et al., 2021). As another example, fueled by the drive for economic efficiency, GVCs also exacerbate social sustainability challenges, such as poor working conditions and modern slavery. With more sustainability controversies brought to light, MNEs, as the lead firms in GVCs, are increasingly held accountable not only for the activities of their own companies but also for those related to their GVCs (Egels-Zandén, 2014; Kim & Davis, 2016). ﻿In other words, “stakeholders view the firm’s responsibility boundaries as stretching much further than either its ownership or control boundaries” (Narula, 2019: p. 1632). An increasing number of researchers have studied the drivers and consequences of MNEs’ adaptiveness—or lack thereof—to this stakeholder demand. Despite mixed empirical evidence, existing studies, in general, agree that social and environmental controversies can result in significant reputational erosion and financial punishment among MNEs (Kölbel et al., 2017; Wang & Li, 2019). Despite increasingly numerous standardized social and environmental responsibility programs (Kim & Davis, 2016), studies have found third-party certifications and social audits have limited effect in reducing the sustainability risks in GVCs (Chen & Lee, 2017). Moreover, such market-based screening and monitoring instruments are not readily available in certain industries and countries. Thus, growing stakeholder demand for sustainability is forcing firms to internalize, at least partially, the governance of sustainability issues in GVCs, reducing market failure. By internalizing GVC governance activities, MNEs may develop specialized skills and dynamic capabilities that could become a source of new firm-specific advantages (FSAs) in managing stakeholder expectations (Maksimov et al., 2022; Narula et al., 2019). Recent surveys found that sustainability is already a top criterion in choosing an employer for two-thirds of potential employees younger than 34 (Practice et al., 2021). Major investment firms (including BlackRock, Vanguard, and State Street) have started translating their sustainability-oriented mindset into action. Thus, the number of MNEs that have already pledged to push their GVCs toward a greater level of environmental and social sustainability continues to increase. Although MNEs have realized why they need to take action to govern sustainability issues in the GVCs, how to implement this governance is unclear. In this article, I revisit the essence of governance mechanisms, propose a range of available digital technology-enabled governance tools and discuss how they overcome the constraints of traditional control and coordination and enable MNEs to uphold sustainability in their GVCs. I conclude with suggestions for further enriching our understanding of the quasi-internalization arguments and elaborate on ways in which digital technologies might help in the practical task of tackling sustainability.",
50,1,Journal of Industrial and Business Economics,20 October 2022,https://link.springer.com/article/10.1007/s40812-022-00235-z,Corporate misconduct in GVCs: challenges and potential avenues for MNEs,March 2023,Federica Nieri,Priscilla Rodriguez,Luciano Ciravegna,Female,Female,Male,Mix,,
50,1,Journal of Industrial and Business Economics,02 January 2023,https://link.springer.com/article/10.1007/s40812-022-00250-0,The dark side of the cascading compliance model in global value chains,March 2023,Vivek Soundararajan,,,Male,Unknown,Unknown,Male,"Private governance mechanisms such as social auditing, codes, sustainability standards, and certifications have emerged as tools essential to improve working conditions in global value chains (GVCs). However, while these mechanisms have influenced tier-one suppliers, they have failed to affect lower-tier ones (Anner, 2020; Gold et al., 2015; Soundararajan & Brammer, 2018; Villena & Gioia, 2018). As a result, it is common for workers in the lower tiers of GVCs to operate under unsafe and unfair working conditions. Both research and practice have identified a range of labor abuses perpetrated in lower tiers, including physical and sexual violence, manipulation, forced overtime, the withholding of wages, debt bondage, forms of what amounts to physical imprisonment, the blacklisting of workers who speak out against mistreatment, and poor health and safety controls on production premises (Crane et al., 2019; LeBaron, 2021). Lower-tier suppliers are not in direct contractual relationships with Multinational Enterprises (MNEs), which cannot therefore impose compliance demands on them (Grimm et al., 2016). This has resulted in MNEs adopting a cascading compliance model. In this approach, MNEs shift the responsibility of monitoring to tier-one suppliers or trade intermediaries like sourcing agents, with whom they are directly related (Narula, 2019; Soundararajan et al., 2018a). These supply chain actors are expected to act as double agents in ensuring their own compliance as well as that of their suppliers (Wilhelm et al., 2016). Research has investigated ways MNEs can effectively use a cascading compliance model. For example, by means of an in-depth case study research, Wilhelm et al. (2016) identified the importance of incentives for tier-one suppliers and the existence of information symmetry between MNEs and tier-one suppliers. They also uncovered various contingency factors such as lead firm power, the resource availability of tier-one suppliers, and the internal alignment of purchasing and sustainability functions. As a result of a qualitative study on fashion GVCs, we found that sourcing agents can help lead firms cascade compliance under certain conditions, including knowledge about the relevant fields and actors, legitimacy in the relevant fields and the opinion of the parties involved, the effective translation of the expectations of each party to the other, and satisfactory incentives (Soundararajan et al., 2018a). In another study (Soundararajan & Brammer, 2018), we found evidence for the importance of framing, procedural fairness, and reciprocity. We found that lower-tier suppliers reciprocate positively when their tier-one counterparts or sourcing agents frame the compliance requirements as opportunities and offer support. In contrast, lower-tier suppliers reciprocate negatively when the compliance requirements are framed as a risk aversion mechanism and no support is offered. In the rest of the paper, I will discuss the dark side of the cascading compliance model and its origins, and suggest ways to move towards a shared responsibility model. The arguments presented in this paper are based on my extensive research on fashion GVCs, India’s largest knitwear export industrial cluster in Tamil Nadu, as well as on literature and reports.",1
50,1,Journal of Industrial and Business Economics,16 January 2023,https://link.springer.com/article/10.1007/s40812-023-00259-z,Correction to: Industry dynamics in digital markets,March 2023,Federico Boffa,Amedeo Piolatto,Florian Schuett,Male,Male,Male,Male,,
50,2,Journal of Industrial and Business Economics,12 April 2023,https://link.springer.com/article/10.1007/s40812-023-00269-x,Emerging challenges in competition policy and regulation,June 2023,Maria Rosa Battaggion,Luke Garrod,Luca Grilli,Female,Male,Male,Mix,,
50,2,Journal of Industrial and Business Economics,16 March 2023,https://link.springer.com/article/10.1007/s40812-023-00263-3,Competition policy in the new wave of global protectionism. Prospects for preserving a fdi-friendly institutional environment,June 2023,Sergio Mariotti,,,Male,Unknown,Unknown,Male,"National competition authorities (NCAs) are inherently double-faced, as their competition policy (CP) can either promote or discourage foreign direct investment (FDI) in the country (Mariotti & Marzano, 2021).Footnote 1 In line with its origins and history, the mission of NCAs is to protect competition and establish a level playing field for all investors (domestic and foreign). This translates into a favorable environment for FDI. International Business (IB) studies recognize that non-discrimination is a key factor that promotes FDI localization by removing barriers to market entry and impediments to trade and investment (Caves, 1996; Rugman & Verbeke, 1998). However, the “capture theory” (Carpenter & Moss, 2013) explains how many conditioning factors are at work that cause NCAs to deviate from their institutional role of defending free competition. They are exposed to “industry capture”, through the lobbying of vested interests; “government capture”, through the expropriation of their independence and the weakening of their functions in the name of a higher national interest (Gardbaum, 2020); “bureaucratic capture” (i.e., civil servants bias the investigation to pursue their own career goals; see Dewatripont & Tirole, 1999); and “cultural capture” (i.e., the regulator is swaned into thinking like - and doing the bidding of—the elite group that has gained dominance in promoting and controlling policy outcomes, thanks to common backgrounds, education, experience, and intermingling; see Kwak, 2013). As pointed out by the IB literature since the seminal work of Brewer (1993), competition law leaves room for enforcement discretion and may be abused as a barrier to investment and intentionally manipulated to prevent FDI (Büthe, 2014; Clougherty & Zhang, 2021; Tunali & Fidrmuc, 2015). Because of this, over time IB studies have offered contradictory and inconclusive evidence. Focusing on the last two decades and making no claim to exhaustiveness, a number of studies find that CP fosters inward FDI, as its strengthening creates a non-discriminatory business climate towards foreign competitors (e.g., Golub et al., 2013; Oliveira et al., 2001; Parakkal, 2021; Seth & Moran, 2013). Other scholars propose an opposing view, mainly arguing that CP is protectionist in intentions and/or effects, discouraging foreign ownership of firms, especially through controlling mergers and acquisitions (e.g., Aktas et al., 2007; Clougherty & Zhang, 2021; Conybeare & Kim, 2010; Serdac Ding & Erel, 2013); Zhang & Clougherty, 2022). Recent social and political trends make the contradictions associated with the two-faced nature of CP clearer and more jarring. Consider the emergence of populism as a reaction to the inequalities and imbalances caused by globalization (Rodrik, 2017; Stiglitz, 2017). When in power, populism has paved the way for the intensification of disguised protectionism. For example, Bernatt (2022) examines the relationship between populism and antitrust in Poland and Hungary. He observes that, despite being part of the European Union and thus subject to its jurisdiction in competition matters, the policies put in place have a strong nefarious impact on NCAs, especially: (i) the limitation (close to abolition) of their independence; (ii) the decrease in human and financial resources allocated to their operation; (iii) the overall reduction in the strength of enforcement, especially with regard to large domestic and state-owned firms; and (iv) the discretionary power of the government to exempt certain transactions from merger control for reasons of strategic importance to the national interest. But we are only scratching the surface. The most striking signal about the possible welding of populism and antitrust abuse comes from the United States. The “populist antitrust” movement, which in its most solid conceptual guise has taken the name New Brandeisian movement, claims for CP a task that goes beyond the consumer-welfare-standard to embrace a public-interest-standard that borders on the political domain. In the early twentieth century, the motto “big is bad” was coined to assert that competition law was designed to protect small businesses and prevent bigness, in accord with the influential thinking of Louis Brandeis.Footnote 2 The new movement, based on this original vocation, argues that to address the consequences caused by excessive concentration and bigness, the measures against monopoly platforms must be accompanied by social goals, such as redistribution, the environment, unemployment, wage growth, privacy and data security, and national strategic independence (Dorsey et al., 2019). This approach has been adopted by the Biden Administration. On July 9, 2021, the President issued a proclamation on CP, stating that there has been excessive consolidation in many economic sectors and that this consolidation has harmed workers, small businesses, and consumers and led to vast racial, income, and wealth inequality. The Executive Order stated that the central goal of the Biden Administration is to ensure that antitrust laws in the United States are vigorously enforced to address these social problems.Footnote 3 At the same time, Tim Wu and Lina Khan, two prominent New Brandesians, have taken leading roles as Special Assistant to the President for Technology and Competition Policy and as Chair of the United States Federal Trade Commission (FTC), respectively. Beyond intentions, which we do not dispute here,Footnote 4 the promotion of “socially responsible” agencies and the creation of an antitrust position in the White House have a strong potential to undermine the independence and autonomous work of FTC. As Jean Tirole points out in a recent note, while governments are typically multi-mission actors, delegating specialized missions to independent agencies allows for greater effectiveness in pursuing targeted objectives, “[but] independence comes with duties. The agencies do not choose their mission and the broad lines of what is expected from them is clearly specified: independence requires limited and mandated powers. The expansion of missions to the political domain therefore exposes agencies to the loss of independence” (Tirole, 2022, p. 8). In the international perspective, populist CP finds mutual reinforcement in the traditional skepticism exhibited by authoritarian and corporatist states toward the independence of NCAs, the latter often being bent to strategic political interests. In his recent book (“Chinese antitrust exceptionalism”), Zhang (2021) illustrates how China has turned competition law into a powerful economic weapon, and explains its strategic application during the Sino-American technology war through numerous case studies. Zhang also informs on the vast administrative discretion possessed by the Chinese government, showing how agencies can even exploit the media to pursue hostile enforcement. Going back to FDI, this orientation toward populist CP accentuates worldwide concern about biases in the selective enforcement of seemingly neutral antitrust laws in favor of national champions and other national firms, local employment and population groups. Indeed, it baits confirmation of what has already been documented in the first two decades of the new century about numerous cases of NCA capture around the world, resulting in opportunistic uses of control over mergers and acquisitions and abusive behavior by dominant firms against foreign firms (Mariniello et al., 2015). As the adversarial behavior of NCAs has tightened, the uncertainty and risk perceived by international investors have increased, leading them to engage in more cautious investment behavior. Looking at the effects of populism, Liebmann and Kunczer (2022), using a sample of 525,688 FDI observations from 2007 to 2019, find that multinational enterprises are less likely to invest in a country the more populist its government is. More generally, if they perceive signs that there might be systematic bias against foreign firms in a country, they will either not invest or enter the local market with suboptimal choices but lower sunk costs (Zhang & Clougherty, 2022). Nowadays, more than one hundred and fifty countries have CPs and NCAs. Despite the convergence of principles and legislation, NCAs still differ in their independence and accountability, scope of action, powers of investigation and sanction, and resource allocation (Clougherty, 2005; Ginsburg, 2005; OECD, 2014). Because of this chaotic inconsistency and the environmental uncertainty it generates, the liabilities of foreignness and outsidership suffered by international investors (Johanson & Vahlne, 2015) are exacerbated in their effects. Therefore, MNEs become highly sensitive to loud and clear signals that NCAs will deliver on their promise to promote a ubiquitous level playing field. In their study of a sample of 63 countries over the period 1980–2017, Mariotti and Marzano (2021) find that signals of CP effectiveness, such as pro-enforcement CP reforms, are crucial to attracting FDI. However, they also stress that this is a necessary but not sufficient condition. Especially in countries with a lower level of “generalized trust” (i.e., trust in others, corporations, and political institutions; see Aghion et al., 2010), good quality complementary institutions pertaining to the regulatory environmentFootnote 5 are necessary to make CP effective in attracting FDI. These results are in accordance with a stream of literature recognizing that a country’s high-quality institutions are meaningful in executing a legitimate and credible CP and making its enforcement more effective. Econometric studies give evidence of this (Ait Soussane & Mansouri, 2022; Borrell & Jiménez, 2008; Buccirossi et al., 2013; Krakowski, 2005; Voigt, 2009). Together, these considerations form the prelude to the issues that will be addressed in the next sections. NCAs do not operate in a vacuum, but are embedded in a national and international fabric of economic, institutional and social relations that influences their policies and practices along several dimensions (Motta, 2004). We must analyze the changes in this intricate fabric if we want to understand whether and how the institutional role of NCAs is changing relative to its founding goals, namely the efficient allocation of resources and the promotion of competion; to answer key questions such as the one suggestively formulated by Murray (2019, p. 117)—“is antitrust law the last hope for preserving a free global economy or another nail in free trade’s coffin?”; and finally to discuss the research question we put forward in the title, namely what policies should be prioritized so that CP can continue to promote worldwide a FDI-friendly institutional environmentFootnote 6. Our discussion will be carried by focusing on a macro-trend that we believe will heavily impact the world economy, namely the new wave of “global protectionism”.",
50,2,Journal of Industrial and Business Economics,22 December 2022,https://link.springer.com/article/10.1007/s40812-022-00252-y,The effect of competition policy on FDI location decision: an empirical investigation using data on inward FDI in 38 countries,June 2023,Jihad Ait Soussane,Zahra Mansouri,,Male,Female,Unknown,Mix,,
50,2,Journal of Industrial and Business Economics,19 December 2022,https://link.springer.com/article/10.1007/s40812-022-00248-8,Privacy regulation and online concentration during demand peaks: evidence from the E-commerce sector,June 2023,Lorien Sabatino,Geza Sapi,,Unknown,Unknown,Unknown,Unknown,,
50,2,Journal of Industrial and Business Economics,17 April 2023,https://link.springer.com/article/10.1007/s40812-023-00266-0,Red tape and industry dynamics: a cross-country analysis,June 2023,Chiara Tomasi,Fabio Pieri,Valentina Cecco,Female,Male,Female,Mix,,
50,2,Journal of Industrial and Business Economics,23 March 2023,https://link.springer.com/article/10.1007/s40812-023-00264-2,From energy consumers to prosumers: the role of peer effects in the diffusion of residential microgeneration technology,June 2023,Shandelle Steadman,Anna Rita Bennato,Monica Giulietti,Unknown,Female,Female,Female,"In recent years, we have been witnessing a radical change in the role played by energy consumers. Motivated by the steady increase of the energy prices, energy consumers have decided to take part in the energy transition by reconsidering the use of renewable energy sources. These sources would guarantee them independence from the national grid, by producing, consuming and storing their own energy. Various are the motivations that sparked energy consumers’ interest: policy interventions, along with socio-economic and demographic factors can play an important role, but also individual preferences and behavioural biases represent key factors in their individual decision making over their energy plans. By focusing on residential users’ adoption and diffusion of photovoltaic microgeneration technology in the United Kingdom (UK), this paper explores the incentives and determinants which spur energy consumers to invest in a new technology, thus becoming energy prosumers.Footnote 1 In particular, we focus on the role played by peer effects triggered by the presence within the same territory of community energy groups who have previously installed solar PV technology, and how they can influence the regional adoption and diffusion of microgeneration technology among households in the UK. In the UK, the initial diffusion of solar photovoltaic (PV) technology seems to have been followed by the introduction of strong policy incentives, such as the Feed-in-Tariff (FIT) scheme. The FIT introduced in April 2010, was designed by the government to promote the adoption of small-scale renewable energy technology by offering payments for electricity generated and exported by eligible systems.Footnote 2 As a result, in 2010 the UK recorded a rapid diffusion of renewable energy technology (Bunea et al., 2022): between April 2010 and April 2015, a total of 674,218 FIT eligible appliances were installed.Footnote 3 Despite this initial boost, in 2015 the FIT tariff was reduced, and one year after the number of new installations that could be accredited under these scheme was capped, yielding a drop in the installation of the number of renewable technology systems.Footnote 4 The result was more pronounced for residential adopters of solar technology. The number of solar panel installations, indeed, moved from a monthly average of 55,000 installations in November 2011, to a monthly average of 2500 in 2017 (Sovacool, 2022). In 2019, the FIT scheme was definitely closed to new applicants, and it was replaced by the Smart Export Guarantee (SEG).Footnote 5 Due to the reduction in public subsidies intended for the installation of renewable energy technology systems by residential consumers, other factors may create new incentives for adoption. For instance, the lack of financial incentives for the individual installation of a renewable energy technology could lead a household to shift their attention to community energy groups and local authorities.Footnote 6 Community energy organisations proved to be pivotal in building trust and encouraging behavioural change among members of their community,Footnote 7 In the literature, attention has recently been placed on peer effects and social contagion in the diffusion of microgeneration technology (Curtius, 2018). The British experience of removing financial incentives and the presence of well established and organised energy communities make it an interesting case study to consider which can provide useful lessons about the potential role of peer effects in other European countries that are witnessing the emergence of local energy communities. By taking into account the role and the presence of different sources of peer effects, such as the existence on the territory of community energy groups, in this paper we study how the information mechanism promotes the diffusion of microgeneration technology for the UK households at a local levels within regions. The process leading to the adoption of a new technology, and its spreading within (intra) or across (inter) households or firms may take several years and in some cases decades (Battisti, 2008). The initiatives put in place by community energy organisations may be able to catalyse peer effects by providing information, encouraging social learning and behavioural change which help to reduce uncertainty on the adoption of a new technology, which promotes its further diffusion among households within the same region. It is possible that these organisations jointly with local authorities can use their relationships with members of the same community to influence their trust towards microgeneration technology. This paper aims to contribute to the diffusion of innovation literature, focusing on the inhibitors of technology spread across final users, studying in more detail the mechanisms through which peer effects occur, and how these peer effects can encourage diffusion. We apply a spatial econometric analysis to a novel dataset and introduce variables relating to community energy organisations, accounting for local renewable energy initiatives. By doing so, we aim to determine if these organisations and initiatives can be used to spread information, catalyse peer effects and lead to an increased uptake of domestic microgeneration technology in the region. This research will therefore allow us to discover how regional domestic adoption can be supported and encouraged through local community action, and whether local initiatives can influence further diffusion of domestic microgeneration technology. The rest of the paper is organized as follows. In the next section we outline the theoretical background within which we set our hypotheses to be tested. In Sect. 3, we present our data, along with the empirical approach and chosen variables. In Sect. 4 we present the empirical analysis. Section 5 displays our results, whereas Sect. 6 discusses key results. Section 7 summarizes our findings and concludes.",
50,2,Journal of Industrial and Business Economics,02 January 2023,https://link.springer.com/article/10.1007/s40812-022-00251-z,Competitive conditions in the public procurement markets: an investigation with network analysis,June 2023,Ioannis G. Fountoukidis,Ioannis E. Antoniou,Nikos C. Varsakelis,Male,Male,Male,Male,"Public procurement is the process by which the public sector buys goods, services, or works from companies (European Commission, 2017). The OECD estimates that the public authorities spend 9.5 trillion US dollars per year for goods and services using procurement processes (Futia et al., 2017). This amount corresponds to 14% of the GDP, while the corresponding share for the EU member states is 12%. Also, public contracts might constitute a significant share of the companies’ turnover (OECD, 2007) and could leverage competitive advantages in international markets. Thus, the operation of the public procurement market attracts the interest of the public authorities (authorities hereafter) and the economic operators (companies hereafter). Competition plays a decisive role in public procurement because it enhances the efficient allocation of public spending. Good government procurement aims at the effective management of public wealth and strengthening citizens' trust. Therefore, public procurement is a tool for implementing government policies and achieving national strategic objectives and is recognized as one of the main goals of good governance (Phillips et al., 2007). The public procurement market is of particular interest because monopsonies, oligopsonies, monopolies, and oligopolies could emerge in some sectors. The bureaucracy, the diverse legal framework, and the low expertise of public authorities raise entrance barriers to non-incumbents, especially small and medium-sized enterprises (SMEs) (Ferguson, 2018). Besides, insufficient information about the competitive conditions increases the uncertainty for firms leading to possible failures in decisions related to sales and supplies (Groves et al., 2014). Authorities as buyers examine the prospective sellers and analyze the risks of a competitive process such as cost overruns, not bidding, collusions, etc. For these reasons, the industrial organization has incorporated the analysis of the public procurement markets (Laffont & Tirole, 1993). The public procurement market differs from the consumer markets because the buyers and sellers are usually few. It is, therefore, important to monitor the market from both sides, buyers, and sellers, using concentration indicators. However, even though the participants may have the same share, the distribution of contracts for each seller or buyer may not be uniform. Therefore, even though the market looks competitive, the sellers and buyers collaborate with limited partners. This could be a signal of collusion. For these reasons the agencies for the protection of competition (agency hereafter) national and international (e.g. the EU Directorate) usually put under scrutiny the operation of sectoral public procurement markets by developing new and more advanced monitoring tools. Graph theory and network analysis have been used in decision-making systems for the procurement of companies (Oh & Behdad, 2017) to identify competing or cooperating groups or potential allies (Hansen et al., 2020). Graph databases have been a significant component of a Knowledge Management System (KMS) as they offer a way to represent the knowledge management processes as scenarios by using simple queries of nodes and relationships (Zhang, 2017). Graph databases can be used as a Decision Support System (DSS) or they can be a KMS that will synergize with an existing DSS (Bader, 2015). Xiubo et al. (2016) studied how to detect procurement fraud by employing big data analytics and network analysis. Van Erven et al. (2017) used graph databases to identify corruption and fraud in Brazilian government procurement processes. Mamavi et al. (2017) examined the influence of strategic networks on public contract awarding in France and found strong and weak ties positively affect the award of public contracts. Finally, Soylu et al. (2018) created a semantic knowledge graph using multiple sources across the EU to analyze public spending and corporate data. This paper aims to contribute to the economic and decision-making literature by applying network analysis and graph database technology to monitor the sectoral public procurement markets. The network representing the interactions between authorities and companies is bipartite, undirected in unweighted and weighted form. We use the unweighted and weighted degree and entropy for each agent. These metrics show the role of each agent in the network. We compute the network and the conditional network entropies which measure the operation of the whole network. The network entropy offers information about the market competitive conditions from the authorities and seller’s point of view, and the conditional network entropy considers also the distribution of contracts across sellers or buyers for the average agent. These metrics could serve as a signal of dominant positions and preferential treatment. In the second stage, the agency could use the agents’ degrees and entropies, unweighted and weighted to investigate the possible preferential treatment or market dominance by specific agents. The algorithm could provide red signals for the sectors which exhibit non-competitive conditions either from the authorities or/and the companies. Besides, the algorithm could provide signals about specific authorities and companies. Finally, the time evolution of the metrics could also signal a path toward more competitive or non-competitive behaviors. We apply this method in the sectoral public procurement market of pacemakers using data from the “Opentender” platform which covers 33 European countries for the period 2009–2019. The network entropy and the conditional network entropy reveal that competitive conditions prevailed during the examination period. The conditional entropy as an additional tool indicates whether authorities concentrate their purchases on a few sellers even though no authority (-ies) dominates the market. A similar argument applies in the case of companies. Equal market shares for companies or authorities do not guarantee competitive conditions. If companies or authorities concentrate on a few clients or buyers it might represent a signal for collusion. Our findings indicate the participating companies, on average, signed contracts with many authorities following a more uniform distribution. Hence, collusive behavior possibly did not exist in this specific market during the examination period. Our findings show the significance of the proposed methodology to monitor the competition and analyze the role of companies and authorities in a public procurement market. The analysis confirmed that graph databases could manage large volumes of data in a market. The methodology presented is a KMS that can function as a DSS or support an existing DSS. By implementing the algorithms of this study, it is possible to analyze the competition both in a sectoral market or a group of markets. The resulting information can be functional for the decision-making of contracting authorities and companies and for national and EU competition commissions. The rest of the paper is organized as follows. Section 2 presents the methodology and the data. Section 3 presents the empirical findings and the discussion. Section 4 offers some concluding remarks.",
50,2,Journal of Industrial and Business Economics,12 December 2022,https://link.springer.com/article/10.1007/s40812-022-00246-w,How processing trade assists local industrial upgrading: input–output analysis of export processing zones in China,June 2023,Weixiao Wu,Chang Hong,,Unknown,,Unknown,Mix,,
50,2,Journal of Industrial and Business Economics,17 March 2023,https://link.springer.com/article/10.1007/s40812-023-00262-4,"Industrial policy matters: the co-evolution of economic structure, trade, and FDI in Brazil and Mexico, 2000–2015",June 2023,André Pineli,Rajneesh Narula,,Male,Unknown,Unknown,Male,"Since the 1970s, scholars and international organizations have been promoting foreign direct investment (FDI) as a catalyst for economic development. A key assumption is that FDI is a vehicle for knowledge and technology transfer. This is supposed to make the host country more productive, not only directly—that is, through the multinational enterprise (MNE) affiliate’s activities—but also indirectly because it is assumed that FDI generate positive spillovers to domestic firms. Both economics and International Business (IB) has sought to unravel the socioeconomic impact of the MNEs (van der Straaten et al., 2023). Much of the work on IB and development has noted that the presence of MNE activity per se is not a sine qua non for economic development, and indeed, the net effect can be negative, and may evolve over time due to a variety of factors. The ‘investment development path’ (IDP) has been critical in understanding the co-evolution of FDI and economic structures. Influenced by the stages of development literature that emphasised that economic development was not simply a synonym of GDP growth but entailed qualitative changes in the structures of production, employment, and consumption (Chenery, 1960; Kuznets, 1957), the IDP has described how a country’s inward and outward FDI position evolved according to its level of development (Dunning, 1981, 1988; Dunning & Narula, 1996; Narula, 1996; Narula & Dunning, 2010). The literature on IB and development explains that the development effects of a specific investment depended upon a number of factors. The extent to which MNE investments influenced the development of its host location was primarily determined by the characteristics of its affiliates operating in the country (in the form of the MNE’s ownership advantagesFootnote 1) and the characteristics of the host country, as reflected in its location advantages. Both advantages are not immutable and tend to influence each other—that is, the presence of MNEs may contribute to alter the country’s location advantages while the country’s characteristics may affect the affiliates’ advantages. The impacts on the host economy also depend on the fundamental motives that led the MNE to engage in that specific investment. Cuervo-Cazurra et al. (2015) list four broad (and non-mutually exclusive) motives that leads a firm to invest abroad: sell more, buy better (reduce costs of inputs), upgrade (increase the pool of assets that compounds the firm’s competitive advantages) or escape (from an adverse environment in the home country).Footnote 2 The development effects of any given affiliate within an MNE’s network of affiliates are not constant, and indeed change over time. The choice of the activities performed in a country is connected to an MNE’s overall strategy. To some extent, affiliates of the same MNE located in different countries may well compete against each other for tasks and functions within the overall corporate structure. Thus, the role of any given affiliate evolves over time, and may become increasingly specialised and upgraded, or downgraded. Changes in scope, scale and intensity will reflect on its economic impact to the host economy. A subsidiary, a region or a country can remain locked-in to low value adding activities, and may have a negligible multiplier effect, reflecting weak backward and forward linkages by the MNE affiliate. Although many studies have investigated the impact of FDI on host economies, fewer studies investigate the relationship between FDI and GDP growth [for a review, see Narula and Pineli (2017, 2019)]. There are fewer studies that examine the co-evolution between FDI and economic structure, with Pineli et al. (2021) and Pineli (2022) being two recent attempts to advance the literature on FDI and structural change, but in general such analyses require data that is not usually available. We seek to bridge this gap by investigating the co-evolution between FDI, economic structure and export structures in the two largest Latin American economies, Brazil and Mexico, over the period 2000–2015. For much of the twentieth century, these countries followed quite similar development strategies, focusing on inward-oriented industrialization as a path to economic prosperity. At the end of the import-substitution era, and accelerated by huge foreign debts, Brazil and Mexico adjusted their strategies, but in differing ways. While Brazil remained largely oriented towards its domestic market, Mexico made a radical shift towards an export-led model. Despite these differences, both countries converged in (at least) one thing: the disappointing results in terms of GDP growth. In addition to the analysis of the key indicators, we briefly discuss the role played by industrial policies—or their absence—within Brazil’s and Mexico’s development strategies. We take the view that industrial policy has the objective of changing the structure of an economy in either direction, magnitude, or speed, in a way that market forces alone would not be able to achieve. Our analysis implicitly assumes that what an economy produces and exports matters for its long-term growth trajectory (Hausmann et al., 2006). Industrial policy instruments, such as infant industry protection, subsidies, tax and financial incentives, as well as performance requirements may be crucial to shift the economic structure in the direction of the desired industries. Nonetheless, industrial policy may also be the source of distortions and inefficiencies that, in the end, hamper economic growth. Therefore, industrial policy incentives must be temporary, transparent, and evaluated on measurable performance criteria defined in advance (Moreno-Brid, 2016). The paper is organized as follows: the next section presents the antecedents of the period under analysis, discussing the role and the effects of FDI during the import-substitution industrialization (ISI) phase and the following period of market-oriented reforms. The period 2000–2015 is analysed in the third section, which is followed by the concluding remarks.",
50,2,Journal of Industrial and Business Economics,06 January 2023,https://link.springer.com/article/10.1007/s40812-022-00254-w,Effects of informative advertising on the formation of market structures,June 2023,Jesús Andrés Burbano-Gómez,Mónica María Sinisterra-Rodríguez,,,Female,Unknown,Mix,,

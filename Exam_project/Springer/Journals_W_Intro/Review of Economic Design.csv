Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716611,"Economic design, adjustment processes, mechanisms, and institutions",December 1994,Leonid Hurwicz,,,Male,Unknown,Unknown,Male,,33
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716612,On the relationship between economic development and political democracy,December 1994,John E. Roemer,,,Male,Unknown,Unknown,Male,,3
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716613,2-person Bayesian implementation,December 1994,Bhaskar Dutta,Arunava Sen,,Male,Unknown,Unknown,Male,,4
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716614,Decentralized trade with bargaining and voluntary matching,December 1994,Ebbe Hendon,Birgitte Sloth,Torben Tranæs,Male,Female,Male,Mix,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716615,On the possibility of efficient bilateral trade,December 1994,Sanjeev Goyal,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716616,Reducing informational costs in endowment mechanisms,December 1994,Lu Hong,Scott E. Page,,,Male,Unknown,Mix,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716617,Characterizing neutrality in the voluntary contribution mechanism,December 1994,Tatsuyoshi Saijo,Yoshikatsu Tatamitani,,Unknown,Male,Unknown,Male,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716618,A decentralized and informationally efficient mechanism realizing fair outcomes in economies with public goods,December 1994,Jose Aizpurua,Antonio Manresa,,Male,Male,Unknown,Male,,2
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716619,Robust implementation under alternative information structures,December 1994,Luis C. Corchon,Ignacio Ortuño-Ortin,,Male,Male,Unknown,Male,,6
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716620,Nash implementation through elementary mechanisms in economic environments,December 1994,Bhaskar Dutta,Arunava Sen,Rajiv Vohra,Male,Unknown,Male,Male,,44
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716621,Implementing the nash extension bargaining solution for non-convex problems,December 1994,John P. Conley,Simon Wilkie,,Male,Male,Unknown,Male,,7
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716622,"A principal-agent model of altruistic redistribution, with some implications for fiscal federalism",December 1994,Mark Shroder,,,Male,Unknown,Unknown,Male,,1
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716623,Announcement,December 1994,,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716624,A free-rider problem with a free-riding principal,December 1994,Shasikanta Nandeibam,,,Unknown,Unknown,Unknown,Unknown,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716625,On the screening power of incentive schemes,December 1994,Fredrik Andersson,,,Male,Unknown,Unknown,Male,,1
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716626,Exchange-proofness or divorce-proofness? Stability in one-sided matching markets,December 1994,José Alcalde,,,Male,Unknown,Unknown,Male,,21
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716627,Condorcet efficiency of positional voting rules with single-peaked preferences,December 1994,Dominique Lepelley,,,,Unknown,Unknown,Mix,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716628,A mechanism implementing the proportional solution,December 1994,Sang-Chul Suh,,,Male,Unknown,Unknown,Male,,6
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716629,Implementation of social optimum in oligopoly,December 1994,Mark Gradstein,,,Male,Unknown,Unknown,Male,,2
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716630,Implementation and information in teams,December 1994,Tomas Sjöström,,,Male,Unknown,Unknown,Male,,7
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716631,Implementation by demand mechanisms,December 1994,Tomas Sjöström,,,Male,Unknown,Unknown,Male,,17
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716632,Strategic participation and the median voter result,December 1994,David Sunding,,,Male,Unknown,Unknown,Male,,
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716633,Strategy-proofness in many-to-one matching problems,December 1994,Tayfun Sönmez,,,Male,Unknown,Unknown,Male,,13
1.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02716634,Announcement,December 1994,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499123,On economies of scope in communication,December 1996,Thomas Marschak,,,Male,Unknown,Unknown,Male,,3
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499124,Mechanisms that efficiently verify the optimality of a proposed action,December 1996,Takashi Ishikida,Thomas Marschak,,Male,Male,Unknown,Male,,2
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499125,Capabilities and utilities,December 1996,Carmen Herrero,,,Female,Unknown,Unknown,Female,,18
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499126,A continuous double implementation of the constrained Walras equilibrium,December 1996,Bezalel Peleg,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499127,Optimal tax auditing when some individuals need not file,December 1996,Inés Macho-Stadler,J. David Pérez-Castrillo,,Female,Unknown,Unknown,Female,,1
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499128,"The annualKoç University Prize for the best paper of the year 1995 inEconomic Design, Volume 1",December 1996,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499129,Acknowledgement,December 1996,,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499130,Envy-minimizing unemployment benefits,December 1996,Christian Arnsperger,David de la Croix,,Male,Male,Unknown,Male,,1
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499131,Replicating Walrasian equilibria using markets for membership in labor-managed firms,December 1996,Gregory K. Dow,,,Male,Unknown,Unknown,Male,,18
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499132,The allocation of a shared resource within an organization,December 1996,John O. Ledyard,Charles Noussair,David Porter,Male,Male,Male,Male,,8
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499133,On incentive compatibility and budget balancedness in public decision making,December 1996,Jeroen Suijs,,,Male,Unknown,Unknown,Male,,41
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499134,On endogenous economic regulation,December 1996,Stanley Reiter,,,Male,Unknown,Unknown,Male,,4
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499135,Public good provision and the Smith Process,December 1996,Steffen Ziss,,,Male,Unknown,Unknown,Male,,1
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499136,Instability in the labor market for researchers,December 1996,J. David Pérez-Castrillo,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499137,Common value auctions with independent types,December 1996,Fernando Branco,,,Male,Unknown,Unknown,Male,,5
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499138,Double implementation of the Lindahl equilibrium by a continuous mechanism,December 1996,Bezalel Peleg,,,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499139,Double implementation of the ratio correspondence by a market mechanism,December 1996,Luis Corchon,Simon Wilkie,,Male,Male,Unknown,Male,,21
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499140,Profit sharing regulation and repeated bargaining with a shut-down option,December 1996,Michele Moretto,Gianpaolo Rossini,,Female,Male,Unknown,Mix,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499141,Investment timing and efficiency in incomplete contracts,December 1996,James A. Dearden,Dorothy E. Klotz,,Male,Female,Unknown,Mix,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499142,Nonlinear pricing in spatial oligopoly,December 1996,Jonathan H. Hamilton,Jacques-François Thisse,,Male,Unknown,Unknown,Male,,11
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499143,Two versions of the tragedy of the commons,December 1996,Hervé Moulin,Alison Watts,,Male,Female,Unknown,Mix,,
2.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/BF02499144,Acknowledgement,December 1996,,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050001,Optimal deterministic contracting mechanisms for principal-agent problems with moral hazard and adverse selection,November 1997,Frank H. Page Jr.,,,Male,Unknown,Unknown,Male,,4
3.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013686,The scheduling and organization of periodic associative computation: Essential networks,November 1997,Timothy Van Zandt,,,Male,Unknown,Unknown,Male,,7
3.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013685,Stackelberg leadership and transfers in private provision of public goods,November 1997,Wolfgang Buchholz,Kai A. Konrad,Kjell Erik Lommerud,Male,Male,Male,Male,,11
3.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013684,Redistribution and individual characteristics,November 1997,Iñigo Iturbe-Ormaetxe,,,Unknown,Unknown,Unknown,Unknown,,
3.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013687,Nash-implementation of the weak Pareto choice rule for indecomposable environments,November 1997,Hiroaki Osana,,,Male,Unknown,Unknown,Male,,1
3.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050006,Equilibrium in a market with intermediation is Walrasian,November 1997,John Wooders,,,Male,Unknown,Unknown,Male,,2
3.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050007,The scheduling and organization of periodic associative computation: Efficient networks,March 1998,Timothy Van Zandt,,,Male,Unknown,Unknown,Male,,11
3.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050008,Equitable nature of core allocations in atomless economies,March 1998,Farhad Hüsseinov,,,Male,Unknown,Unknown,Male,,
3.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050009,Implementation of stable solutions in a restricted matching market,March 1998,Antonio Romero-Medina,,,Male,Unknown,Unknown,Male,,16
3.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050010,Organizational design with a budget constraint,March 1998,Hans Gersbach,Uwe Wehrspohn,,Male,Male,Unknown,Male,,2
3.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050011,Impossibility of Nash implementation in two-person economies,March 1998,Shinsuke Nakamura,,,Male,Unknown,Unknown,Male,,
3.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050012,An extensive form solution to the adverse selection problem in principal/multi-agent environments,March 1998,John Duggan,,,Male,Unknown,Unknown,Male,,4
3.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050013,Fair allocation in a general model with indivisible goods,July 1998,Carmen Beviá,,,Female,Unknown,Unknown,Female,,13
3.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050014,On durable goods monopolies and the Coase-Conjecture,July 1998,Werner Güth,Klaus Ritzberger,,Male,Male,Unknown,Male,,7
3.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050015,A mechanism design approach to an optimal contract under ex ante and ex post private information,July 1998,C. Choe,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050016,Correlated equilibrium as a stable standard of behavior,July 1998,Indrajit Ray,,,Unknown,Unknown,Unknown,Unknown,,
3.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050017,Implementing action profiles with sequential mechanisms,July 1998,Sandro Brusco,,,Male,Unknown,Unknown,Male,,1
3.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050018,On optimality of illegal collusion in contracts,September 1998,Ariane Lambert-Mogiliansky,,,Female,Unknown,Unknown,Female,,8
3.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050019,Managerial costs for one-shot decentralized information processing,September 1998,Kieron Meagher,Timothy Van Zandt,,Male,Male,Unknown,Male,,4
3.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050020,Maximal surplus from the pivotal mechanism: a closed form solution,September 1998,Rajat Deb,Tae Kun Seo,,Unknown,,Unknown,Mix,,
3.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050021,Process innovation and the persistence of monopoly with labour-managed firms,September 1998,Luca Lambertini,,,Male,Unknown,Unknown,Male,,3
3.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050022,Efficient venture capital financing combining debt and equity,September 1998,Leslie M. Marx,,,,Unknown,Unknown,Mix,,
4.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050023,"Decision-making under risk: Editing procedures based on correlated similarities, and preference overdetermination",February 1999,José Ramón Uriarte,,,Male,Unknown,Unknown,Male,,1
4.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050024,Monotonic extensions on economic domains,February 1999,William Thomson,,,Male,Unknown,Unknown,Male,,10
4.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050025,"Responsibility, talent, and compensation: A second-best analysis",February 1999,Walter Bossert,Marc Fleurbaey,Dirk Van de gaer,Male,Male,Male,Male,,23
4.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050026,Feasible implementation of taxation methods,February 1999,Nir Dagan,Roberto Serrano,Oscar Volij,Male,Male,Male,Male,,11
4.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050027,The effect of bid withdrawal in a multi-object auction,February 1999,David P. Porter,,,Male,Unknown,Unknown,Male,,13
4.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050028,Hierarchies and information-processing organizations,June 1999,Hao Li,,,,Unknown,Unknown,Mix,,
4.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050029,Natural and double implementation of public ownership solutions in differentiable production economies,June 1999,Naoki Yoshihara,,,Male,Unknown,Unknown,Male,,5
4.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050030,A social choice function implementable via backward induction with values in the ultimate uncovered set,June 1999,Peter J. Coughlan,Michel Le Breton,,Male,Male,Unknown,Male,,3
4.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050031,Implementation of multi-agent incentive contracts with the principal's renegotiation offer,June 1999,Hiroshi Osano,,,Male,Unknown,Unknown,Male,,3
4.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050032,A note on van Damme's mechanism,June 1999,Elisabeth Naeve-Steinweg,,,Female,Unknown,Unknown,Female,,7
4.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050033,Optimal regulation of technical progress in natural monopolies with asymmetric information,September 1999,Uwe Cantner,Thomas Kuhn,,Male,Male,Unknown,Male,,2
4.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050034,A characterization of the existenceof optimal dominant strategy mechanisms,September 1999,Liqun Liu,Guoqiang Tian,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050035,A non-cooperative interpretation of bargaining sets,September 1999,Ezra Einy,David Wettstein,,Male,Male,Unknown,Male,,5
4.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050036,Multiproject team assignments,September 1999,Katerina Sherstyuk,,,Female,Unknown,Unknown,Female,,1
4.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050037,Coalitional manipulations in a bankruptcy problem,September 1999,M. Angeles de Frutos,,,Unknown,Unknown,Unknown,Unknown,,
4.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050038,Competition among mechanism designers in a common value environment,September 1999,Michael Peters,,,Male,Unknown,Unknown,Male,,6
4.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050039,Organizational restructuring in response to changes in information-processing technology,November 1999,Jacek Cukrowski,Andrzej Baniak,,Male,Male,Unknown,Male,,7
4.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050040,A profit-center game with incomplete information,November 1999,Tatsuro Ichiishi,Roy Radner,,Unknown,Male,Unknown,Male,,9
4.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050041,Solutions for cooperative games with r alternatives,November 1999,M.J. Albizuri,J.C. Santos,J.M. Zarzuelo,Unknown,Unknown,Unknown,Unknown,,
4.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050042,Taxation in an economy with private provision of public goods,November 1999,Johann K. Brunner,Josef Falkinger,,Male,Male,Unknown,Male,,9
4.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050043,Single-peakedness and coalition-proofness,November 1999,Bezalel Peleg,Peter Sudhölter,,Unknown,Male,Unknown,Male,,8
4.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050044,Sprumont's characterization of the uniform rule when all single-peaked preferences are admissible,November 1999,John A. Weymark,,,Male,Unknown,Unknown,Male,,6
5.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050045,Full pooling in multi-period contracting with adverse selection and noncommitment,March 2000,Georges Dionne,Claude Fluet,,Male,,Unknown,Mix,,
5.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050046,On the generalized principal-agent problem: Decomposition and existence results,March 2000,Peter S. Faynzilberg,Praveen Kumar,,Male,,Unknown,Mix,,
5.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050047,On efficient trading mechanisms between one seller and n buyers,March 2000,Domenico Menicucci,,,Male,Unknown,Unknown,Male,,
5.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050048,Auctions with endogenous participation,March 2000,Flavio M. Menezes,Paulo K. Monteiro,,Male,Male,Unknown,Male,,77
5.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580050049,Strategyproof multiple assignment using quotas,March 2000,Szilvia Pápai,,,Female,Unknown,Unknown,Female,,17
5.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00011166,Special Issue on the Formation of Groups,June 2000,Bhaskar Dutta,Matthew O. Jackson,,Male,Male,Unknown,Male,,
5.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000015,Incentive compatible reward schemes for labour-managed firms,June 2000,Salvador Barberà,Bhaskar Dutta,,Male,Male,Unknown,Male,,4
5.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000018,Buyers' and sellers' cartels on markets with indivisible goods,June 2000,Francis Bloch,Sayantan Ghosal,,Male,Unknown,Unknown,Male,,5
5.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000016,Coalition formation in general NTU games,June 2000,Anke Gerber,,,Female,Unknown,Unknown,Female,,6
5.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000017,Project evaluation and organizational form,June 2000,Thomas Gehrig,Pierre Regibeau,Kate Rockett,Male,Male,Female,Mix,,
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000014,Editorial Special Issue on the Formation of Groups,September 2000,,,,Unknown,Unknown,Unknown,Unknown,,
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000019,A strategic analysis of network reliability,September 2000,Venkatesh Bala,Sanjeev Goyal,,Male,Male,Unknown,Male,,73
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013689,Network formation with sequential demands,September 2000,Sergio Currarini,Massimo Morelli,,Male,Male,Unknown,Male,,54
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013688,The stability and efficiency of directed communication networks,September 2000,Bhaskar Dutta,Matthew O. Jackson,,Male,Male,Unknown,Male,,55
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013690,Spatial social networks,September 2000,Cathleen Johnson,Robert P. Gilles,,Female,Male,Unknown,Mix,,
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013691,Competition for goods in buyer-seller networks,September 2000,Rachel E. Kranton,Deborah F. Minehart,,Female,Female,Unknown,Female,,27
5.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013692,Network formation models with costs for establishing links,September 2000,Marco Slikker,Anne van den Nouweland,,Male,Female,Unknown,Mix,,
5.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000027,"Altruism, redistribution and social insurance",December 2000,Udo Ebert,Oskar von dem Hagen,,Male,Male,Unknown,Male,,2
5.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000026,Treasury auctions: Uniform or discriminatory?,December 2000,Ken Binmore,Joe Swierzbinski,,Male,Male,Unknown,Male,,35
5.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580000030,Abstention and political competition,December 2000,Humberto G. Llavador,,,Male,Unknown,Unknown,Male,,2
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013696,Dr. Jeffrey Scot Banks,March 2001,Richard D. McKelvey,,,Male,Unknown,Unknown,Male,,
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013693,Multiprincipals multiagents incentive design,March 2001,Rudolf Kerschbamer,Semih Koray,,Male,Male,Unknown,Male,,2
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013694,Implementation of optimal contracts under adverse selection,March 2001,Pablo Amorós,Bernardo Moreno,,Male,Male,Unknown,Male,,1
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013697,Implementation with partial verification,March 2001,Nirvikar Singh,Donald Wittman,,Unknown,Male,Unknown,Male,,11
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013698,Uniform allocation and reallocation revisited,March 2001,Bettina Klaus,,,Female,Unknown,Unknown,Female,,4
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013699,On preferences over subsets and the lattice structure of stable matchings,March 2001,Ahmet Alkan,,,Male,Unknown,Unknown,Male,,31
6.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013695,Returns to scale in one-shot information processing when hours count,March 2001,Catherine de Fontenay,Kieron J. Meagher,,Female,Male,Unknown,Mix,,
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100043,Introduction,September 2001,Tatsuro Ichiishi,Tom Marschak,,Unknown,Male,Unknown,Male,,
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013702,On characterizing the probability of survival in a large competitive economy,September 2001,Rabi N. Bhattacharya,Mukul Majumdar,,Male,,Unknown,Mix,,
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013700,Uniqueness of Arrow-Debreu and Arrow-Radner equilibrium when utilities are additively separable,September 2001,Rose-Anne Dana,,,Unknown,Unknown,Unknown,Unknown,,
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013701,"Entry, productivity, and investment",September 2001,Kenneth J. Arrow,,,Male,Unknown,Unknown,Male,,
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013703,A model of Russia's “virtual economy”,September 2001,Richard E. Ericson,Barry W. Ickes,,Male,Male,Unknown,Male,,5
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013704,Reaction to price changes and aspiration level adjustments,September 2001,Itzhak Gilboa,David Schmeidler,,Male,Male,Unknown,Male,,17
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013705,Bargaining solutions with non-standard objectives,September 2001,Peter B. Linhart,,,Male,Unknown,Unknown,Male,,6
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013706,Investment and concern for relative position,September 2001,Harold L. Cole,George J. Mailath,Andrew Postlewaite,Male,Male,Male,Male,,31
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013707,Coordination of economic activity: An example,September 2001,Stanley Reiter,,,Male,Unknown,Unknown,Male,,1
6.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00013708,"Transversals, systems of distinct representatives, mechanism design, and matching",September 2001,Leonid Hurwicz,Stanley Reiter,,Male,Male,Unknown,Male,,2
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/PL00021674,Introduction,December 2001,Tatsuro Ichiishi,Tom Marschak,,Unknown,Male,Unknown,Male,,
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100054,Roy Radner and incentive theory,December 2001,Eric S. Maskin,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100055,Sufficient conditions for Nash implementation,December 2001,Steven R. Williams,,,Male,Unknown,Unknown,Male,,
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100056,Majority rule with dollar voting,December 2001,James S. Jordan,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100057,Mediation and the Nash bargaining solution,December 2001,Charles A. Wilson,,,Male,Unknown,Unknown,Male,,7
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100058,Public spending and optimal taxes without commitment,December 2001,Jess Benhabib,Aldo Rustichini,Andrés Velasco,Male,Male,Male,Male,,6
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100059,Are “Anti-Folk Theorems” in repeated games nongeneric?,December 2001,Roger Lagunoff,Akihiko Matsui,,Male,Male,Unknown,Male,,3
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100060,Trust and social efficiencies,December 2001,Robert W. Rosenthal,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100061,Survival and the art of profit maximization,December 2001,Prajit K. Dutta,Rangarajan K. Sundaram,,Unknown,Unknown,Unknown,Unknown,,
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100036,Non-manipulable solutions in a permit sharing problem: Equivalence between non-manipulability and monotonicity,December 2001,Sang-Chul Suh,,,Male,Unknown,Unknown,Male,,1
6.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100039,First–price auctions when the ranking of valuations is common knowledge,December 2001,Michael Landsberger,Jacob Rubinstein,Shmuel Zamir,Male,Male,Male,Male,,24
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100037,A study of proportionality and robustness in economies with a commonly owned technology,June 2002,François Maniquet,,,Male,Unknown,Unknown,Male,,7
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100038,"The not-so-secret-agent: Professional monitors, hierarchies and implementation",June 2002,Sandeep Baliga,,,,Unknown,Unknown,Mix,,
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200069,Integrating the Nash program into mechanism theory,June 2002,Walter Trockel,,,Male,Unknown,Unknown,Male,,14
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200064,How to get firms to invest: A simple solution to the hold-up problem in regulation,June 2002,Hans Gersbach,,,Male,Unknown,Unknown,Male,,8
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580100040,Can we identify Walrasian allocations?,June 2002,Antonio Manresa,,,Male,Unknown,Unknown,Male,,
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200071,Achieving the first best in sequencing problems,June 2002,Manipushpak Mitra,,,Unknown,Unknown,Unknown,Unknown,,
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200072,Non-manipulability in Walrasian cost games,June 2002,Marta Faias,Emma Moreno-García,Mário Rui Páscoa,Female,Female,Male,Mix,,
7.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200076,Strategy-proof allocation when preferences are single-plateaued,June 2002,Lars Ehlers,,,Male,Unknown,Unknown,Male,,2
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200063,Stable matchings and the small core in Nash equilibrium in the college admissions problem,September 2002,Jinpeng Ma,,,Unknown,Unknown,Unknown,Unknown,,
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200067,Implementing optimal procurement auctions with exogenous quality,September 2002,Florence Naegelen,,,Female,Unknown,Unknown,Female,,12
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200070,Who else is bidding? The Pareto optimality of disclosing bidder identities,September 2002,Gopal Das Varma,,,Male,Unknown,Unknown,Male,,4
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200075,Safety regulation and monitor liability,September 2002,Eberhard Feess,Ulrich Hege,,Male,Male,Unknown,Male,,1
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200074,Constitutional implementation,September 2002,Bezalel Peleg,Eyal Winter,,Unknown,Male,Unknown,Male,,10
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200066,Proportionally adjusted marginal pricing method to share joint costs,September 2002,YunTong Wang,,,Unknown,Unknown,Unknown,Unknown,,
7.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200073,"Borda rule, Copeland method and strategic manipulation",September 2002,Pierre Favardin,Dominique Lepelley,Jérôme Serais,Male,,Male,Mix,,
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200084,Introduction,November 2002,,,,Unknown,Unknown,Unknown,Unknown,,
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200081,Ambiguity in election games,November 2002,Enriqueta Aragonès,Andrew Postlewaite,,Female,Male,Unknown,Mix,,
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200079,The dynamics of public opinion under majority rules,November 2002,Antonella Ianni,Valentina Corradi,,Female,Female,Unknown,Female,,2
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200078,Federal mandates with local agenda setters,November 2002,Jacques Crémer,Thomas R. Palfrey,,Male,Male,Unknown,Male,,2
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200083,How two-party competition treats minorities,November 2002,Jean-François Laslier,,,Unknown,Unknown,Unknown,Unknown,,
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200080,Politico-economic transition,November 2002,Per Krusell,José-Víctor Ríos-Rull,,Male,Unknown,Unknown,Male,,4
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200077,The asymptotic strategyproofness of scoring and Condorcet consistent rules,November 2002,Eyal Baharad,Zvika Neeman,,Male,Male,Unknown,Male,,6
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200085,The Majoritarian Compromise in large societies,November 2002,Arkadii Slinko,,,Male,Unknown,Unknown,Male,,2
7.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200082,Interview design for revealing preferences of policy makers,November 2002,Andranik Tangian,,,Male,Unknown,Unknown,Male,,
7.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580300091,Editorial,February 2003,Matthew Jackson,William Thomson,,Male,Male,Unknown,Male,,
7.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200065,Are serial Condorcet rules strategy-proof?,February 2003,Donald E. Campbell,Jerry S. Kelly,,Male,Male,Unknown,Male,,4
7.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580200068,Stable and efficient bargaining networks,February 2003,Antoni Calvó-Armengol,,,Male,Unknown,Unknown,Male,,7
7.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580300088,Implementation of the Walrasian correspondence by market games,February 2003,Carmen Beviá,Luis C. Corchón,Simon Wilkie,Female,Male,Male,Mix,,
7.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580300087,Implementing efficient market structure,February 2003,Veronika Grimm,Frank Riedel,Elmar Wolfstetter,Female,Male,Male,Mix,,
7.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s100580300089,RJVs in product innovation and cartel stability,February 2003,Luca Lambertini,Sougata Poddar,Dan Sasaki,Male,Unknown,Male,Male,,15
8.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0086-y,Contracting under ex post moral hazard and non-commitment,August 2003,M. Martin Boyer,,,Unknown,Unknown,Unknown,Unknown,,
8.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0092-0,The endogenous formation of cartels,August 2003,Joachim Rosenmüller,,,Male,Unknown,Unknown,Male,,
8.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0093-z,Elections and strategic positioning games,August 2003,Frank H. Page Jr,Myrna H. Wooders,,Male,Female,Unknown,Mix,,
8.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0090-2,Synergies and price trends in sequential auctions,August 2003,Flavio M. Menezes,Paulo K. Monteiro,,Male,Male,Unknown,Male,,15
8.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0094-y,He who must not be named,August 2003,Philippe Février,,,Male,Unknown,Unknown,Male,,12
8.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0098-7,Efficiency and information aggregation in auctions with costly information,October 2003,Matthew O. Jackson,,,Male,Unknown,Unknown,Male,,23
8.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0096-9,Optimal two-object auctions with synergies,October 2003,Domenico Menicucci,,,Male,Unknown,Unknown,Male,,7
8.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0099-6,Nash-equilibria in a heterogeneous oligopoly with fuzzy information,October 2003,Nils Hauenschild,Peter Stahlecker,,Male,Male,Unknown,Male,,3
8.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0095-x,Strategy-proof allocation of fixed costs,October 2003,James A. Dearden,Karl Einolf,,Male,Male,Unknown,Male,,2
8.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0097-8,Manipulation via merging and splitting in claims problems,October 2003,Biung-Ghi Ju,,,Unknown,Unknown,Unknown,Unknown,,
8.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0100-4,Solomon's Dilemma: An experimental study on dynamic implementation,October 2003,Giovanni Ponti,Anita Gantner,Robert Montgomery,Male,Female,Male,Mix,,
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0110-2,Obituary,October 2003,Semih Koray,,,Male,Unknown,Unknown,Male,,
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0101-3,Probabilistic assignments of identical indivisible objects and uniform probabilistic rules,October 2003,Lars Ehlers,Bettina Klaus,,Male,Female,Unknown,Mix,,
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0107-x,Optimal design of trade institutions,October 2003,Robert P. Gilles,Dimitrios Diamantaras,Pieter H. M. Ruys,Male,Male,Male,Male,,6
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0103-1,A reexamination of additivity of power in randomized social preference,October 2003,Shasikanta Nandeibam,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0104-0,SMEs and public procurement policy,October 2003,Pierre-Henri Morand,,,Unknown,Unknown,Unknown,Unknown,,
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0102-2,An alternative direct proof of Gibbard’s random dictatorship theorem,October 2003,Yasuhito Tanaka,,,Male,Unknown,Unknown,Male,,14
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0106-y,Who consults investment analysts?,October 2003,Jacob Paroush,,,Male,Unknown,Unknown,Male,,
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0105-z,An empirical analysis of transitivity with four scaled preferential judgment modalities,October 2003,José Luis García-Lapresta,Luis Carlos Meneses,,Male,Male,Unknown,Male,,11
8.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0109-8,Dictatorial voting operators,October 2003,Antonio Quesada,,,Male,Unknown,Unknown,Male,,1
8.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0108-9,Link bidding in laboratory networks,April 2004,Cary Deck,Cathleen Johnson,,,Female,Unknown,Mix,,
8.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0112-0,"Efficiency and truthfulness with Leontief preferences. A note on two-agent, two-good economies",April 2004,Antonio Nicoló,,,Male,Unknown,Unknown,Male,,12
8.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-003-0111-1,"Organizations and overlapping generations games: Memory, communication, and altruism",April 2004,Roger Lagunoff,Akihiko Matsui,,Male,Male,Unknown,Male,,10
8.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0113-7,Efficient provision of public goods with endogenous redistribution,April 2004,Luca Anderlini,Paolo Siconolfi,,Male,Male,Unknown,Male,,2
8.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0114-6,Bargaining power in stationary parallelogram games,April 2004,Bart Taub,Özgür Kibris,,Male,Male,Unknown,Male,,
8.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0115-5,Mechanisms for dividing labor and sharing revenue in joint ventures,April 2004,Keith Waehrer,,,Male,Unknown,Unknown,Male,,1
9.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0116-4,Up-front payment under RD rule,December 2004,Ho-Chyuan Chen,,,Unknown,Unknown,Unknown,Unknown,,
9.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0117-3,To merge or not to merge: That is the question,December 2004,Luis C. Corchón,Ramon Fauli-Oller,,Male,Male,Unknown,Male,,4
9.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0118-2,Tax implementability of fair allocations,December 2004,Yukihiro Nishimura,,,Male,Unknown,Unknown,Male,,4
9.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0119-1,Implementation and orderings of public information,December 2004,Colin M. Campbell,,,Male,Unknown,Unknown,Male,,
9.0,1.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-004-0120-8,On how size and composition of customer bases affect equilibrium in a duopoly with switching costs,December 2004,Tommy Staahl Gabrielsen,Steinar Vagstad,,Male,Male,Unknown,Male,,2
9.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0121-2,The condominium problem; auctions for substitutes,April 2005,Roberto Burguet,,,Male,Unknown,Unknown,Male,,11
9.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0122-1,Optimal money growth in a limited participation model with heterogeneous agents,April 2005,Erdem Başçí,Ismail Saglam,,Male,Male,Unknown,Male,,
9.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0123-0,Manipulation of the Walrasian mechanism in production economies with unbounded short-selling,April 2005,Laurence Kranich,,,Female,Unknown,Unknown,Female,,
9.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0125-y,Implementation of the revenue-maximizing auction by an ignorant seller,April 2005,Bernard Caillaud,Jacques Robert,,Male,Male,Unknown,Male,,11
9.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0124-z,The Pazner-Schmeidler social ordering: A defense,April 2005,Marc Fleurbaey,,,Male,Unknown,Unknown,Male,,13
9.0,2.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0126-x,"Strategy-proofness, core, and sequential trade",April 2005,Lars-Gunnar Svensson,Bo Larsson,,Unknown,Male,Unknown,Male,,10
9.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0127-9,Risky allocations from a risk-neutral informed principal,August 2005,Michela Cella,,,Female,Unknown,Unknown,Female,,2
9.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0128-8,Entry deterrence under financial intermediation with private information and hidden contracts,August 2005,Neelam Jain,Thomas D. Jeitschko,Leonard J. Mirman,Female,Male,Male,Mix,,
9.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0129-7,Externalities do not necessarily require larger message spaces for realizing pareto-efficient allocations,August 2005,Hiroaki Osana,,,Male,Unknown,Unknown,Male,,2
9.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0130-1,Nash implementation with an infinite-dimensional trade space,August 2005,Guillaume Bernis,Gaël Giraud,,Male,,Unknown,Mix,,
9.0,3.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0131-0,Manipulation of the Walrasian mechanism in production economies with unbounded short-selling,August 2005,Laurence Kranich,,,Female,Unknown,Unknown,Female,,
9.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0133-y,Equilibrium allocations of endowment-pretension games in public good economies,December 2005,M. Remzi Sanver,,,Unknown,Unknown,Unknown,Unknown,,
9.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0132-z,Probabilities of election outcomes with two parameters: The relative impact of unifying and polarizing candidates,December 2005,William V. Gehrlein,,,Male,Unknown,Unknown,Male,,8
9.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0134-x,Voluntary internalisations facing the threat of a pollution tax,December 2005,Franz Wirl,Claus Huber,,Male,Male,Unknown,Male,,4
9.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0135-9,Public goods provision: Unit-by-unit cost-sharing rules and the core,December 2005,Yan Yu,,,Male,Unknown,Unknown,Male,,1
9.0,4.0,Review of Economic Design,,https://link.springer.com/article/10.1007/s10058-005-0136-8,Strategic delegation in oligopolistic tournaments,December 2005,Matthias Kräkel,,,Male,Unknown,Unknown,Male,,10
10.0,1.0,Review of Economic Design,23 June 2006,https://link.springer.com/article/10.1007/s10058-006-0006-z,Retaliatory Equilibria in a Japanese Ascending Auction for Multiple Objects,April 2006,Gian Luigi Albano,Fabrizio Germano,Stefano Lovo,Male,Male,Male,Male,,1
10.0,1.0,Review of Economic Design,04 July 2006,https://link.springer.com/article/10.1007/s10058-006-0002-3,Implementation of competitive nonlinear pricing: tariffs with inclusive consumption,April 2006,Sissel Jensen,,,Female,Unknown,Unknown,Female,,13
10.0,1.0,Review of Economic Design,19 July 2006,https://link.springer.com/article/10.1007/s10058-006-0003-2,First-price auctions where one of the bidders’ valuations is common knowledge,April 2006,Irene Martínez-Pardina,,,Female,Unknown,Unknown,Female,,7
10.0,1.0,Review of Economic Design,23 June 2006,https://link.springer.com/article/10.1007/s10058-006-0001-4,On the Sub-optimality of Entry Fees in Auctions With Entry,April 2006,Ángel Hernando-Veciana,,,Male,Unknown,Unknown,Male,,2
10.0,1.0,Review of Economic Design,04 July 2006,https://link.springer.com/article/10.1007/s10058-006-0005-0,On the existence of an increasing symmetric equilibrium in (k + 1)-st price common value auctions,April 2006,Ilia Tsetlin,Aleksandar Saša Pekeč,,Male,Male,Unknown,Male,,
10.0,2.0,Review of Economic Design,09 August 2006,https://link.springer.com/article/10.1007/s10058-006-0004-1,New characterizations of a classical bankruptcy rule,August 2006,Juan D. Moreno-Ternero,Antonio Villar,,Male,Male,Unknown,Male,,8
10.0,2.0,Review of Economic Design,01 September 2006,https://link.springer.com/article/10.1007/s10058-006-0007-y,Selling shares to retail investors: auction vs. fixed price,August 2006,Jürgen Bierbaum,Veronika Grimm,,Male,Female,Unknown,Mix,,
10.0,2.0,Review of Economic Design,25 October 2006,https://link.springer.com/article/10.1007/s10058-006-0014-z,"Incomplete regulation, market competition and collusion",August 2006,Cécile Aubert,Jérôme Pouyet,,Female,Male,Unknown,Mix,,
10.0,2.0,Review of Economic Design,23 August 2006,https://link.springer.com/article/10.1007/s10058-006-0008-x,Seniority bias in a tournament,August 2006,Junichiro Ishida,,,Male,Unknown,Unknown,Male,,
10.0,3.0,Review of Economic Design,14 November 2006,https://link.springer.com/article/10.1007/s10058-006-0020-1,Special Issue in Memory of Murat R. Sertel,December 2006,Semih Koray,Muhamet Yıldız,,Male,Male,Unknown,Male,,
10.0,3.0,Review of Economic Design,27 September 2006,https://link.springer.com/article/10.1007/s10058-006-0012-1,Binary effectivity rules,December 2006,Hans Keiding,Bezalel Peleg,,Male,Unknown,Unknown,Male,,2
10.0,3.0,Review of Economic Design,30 September 2006,https://link.springer.com/article/10.1007/s10058-006-0013-0,Equilibria in the spatial stochastic model of voting with party activists,December 2006,Norman Schofield,,,Male,Unknown,Unknown,Male,,39
10.0,3.0,Review of Economic Design,27 September 2006,https://link.springer.com/article/10.1007/s10058-006-0010-3,"The conservative equal costs rule, the serial cost sharing rule and the pivotal mechanism: asymptotic welfare loss comparisons for the case of an excludable public project",December 2006,Rajat Deb,Laura Razzolini,Tae Kun Seo,Unknown,Female,,Mix,,
10.0,3.0,Review of Economic Design,06 October 2006,https://link.springer.com/article/10.1007/s10058-006-0009-9,Workers’ enterprises are not perverse: differential oligopoly games with sticky price,December 2006,Roberto Cellini,Luca Lambertini,,Male,Male,Unknown,Male,,3
10.0,3.0,Review of Economic Design,27 October 2006,https://link.springer.com/article/10.1007/s10058-006-0015-y,Pareto improving interventions in a general equilibrium model with private provision of public goods,December 2006,Antonio Villanacci,Ünal Zenginobuz,,Male,Male,Unknown,Male,,6
10.0,4.0,Review of Economic Design,24 October 2006,https://link.springer.com/article/10.1007/s10058-006-0018-8,Optimal mechanisms for siting noxious facilities,March 2007,Denis Lescop,,,Male,Unknown,Unknown,Male,,7
10.0,4.0,Review of Economic Design,23 November 2006,https://link.springer.com/article/10.1007/s10058-006-0019-7,Cardinality-based equality of opportunities,March 2007,Jorge Alcalde-Unzu,Ricardo Arlegi,Jorge Nieto,Male,Male,Male,Male,,
10.0,4.0,Review of Economic Design,14 March 2007,https://link.springer.com/article/10.1007/s10058-007-0023-6,Network design in games with spillovers,March 2007,Sergio Currarini,,,Male,Unknown,Unknown,Male,,7
10.0,4.0,Review of Economic Design,27 February 2007,https://link.springer.com/article/10.1007/s10058-007-0022-7,Core and equilibria in an assembled product industry,March 2007,Somdeb Lahiri,,,Unknown,Unknown,Unknown,Unknown,,
10.0,4.0,Review of Economic Design,23 February 2007,https://link.springer.com/article/10.1007/s10058-007-0021-8,On the value of information in the presence of moral hazard,March 2007,G. E. Rodriguez,,,Unknown,Unknown,Unknown,Unknown,,
11.0,1.0,Review of Economic Design,26 May 2007,https://link.springer.com/article/10.1007/s10058-007-0025-4,On the generalized principal-agent problem: a comment,June 2007,Sandrine Ollier,,,Female,Unknown,Unknown,Female,,2
11.0,1.0,Review of Economic Design,17 May 2007,https://link.springer.com/article/10.1007/s10058-007-0026-3,Network potentials,June 2007,Subhadip Chakrabarti,Robert P. Gilles,,Unknown,Male,Unknown,Male,,21
11.0,1.0,Review of Economic Design,17 May 2007,https://link.springer.com/article/10.1007/s10058-007-0024-5,Bargaining and exclusivity in a borrower–lender relationship,June 2007,Levent Koçkesen,Saltuk Ozerturk,,Male,Male,Unknown,Male,,
11.0,2.0,Review of Economic Design,06 September 2007,https://link.springer.com/article/10.1007/s10058-007-0032-5,The theory of contests: a survey,September 2007,Luis C. Corchón,,,Male,Unknown,Unknown,Male,,122
11.0,2.0,Review of Economic Design,07 August 2007,https://link.springer.com/article/10.1007/s10058-007-0031-6,Tullock and Hirshleifer: a meeting of the minds,September 2007,José Alcalde,Matthias Dahm,,Male,Male,Unknown,Male,,28
11.0,2.0,Review of Economic Design,06 September 2007,https://link.springer.com/article/10.1007/s10058-007-0030-7,Contingent payments in selection contests,September 2007,Derek J. Clark,Christian Riis,,Male,Male,Unknown,Male,,6
11.0,2.0,Review of Economic Design,31 July 2007,https://link.springer.com/article/10.1007/s10058-007-0033-4,Endogenous coalition formation in contests,September 2007,Santiago Sánchez-Pagés,,,Male,Unknown,Unknown,Male,,17
11.0,2.0,Review of Economic Design,15 August 2007,https://link.springer.com/article/10.1007/s10058-007-0029-0,The generalized Stackelberg equilibrium of the all-pay auction with complete information,September 2007,Kai A. Konrad,Wolfgang Leininger,,Male,Male,Unknown,Male,,22
11.0,3.0,Review of Economic Design,17 November 2006,https://link.springer.com/article/10.1007/s10058-006-0017-9,Some things couples always wanted to know about stable matchings (but were afraid to ask),November 2007,Bettina Klaus,Flip Klijn,Jordi Massó,Female,Male,Male,Mix,,
11.0,3.0,Review of Economic Design,10 January 2007,https://link.springer.com/article/10.1007/s10058-006-0011-2,Envy-freeness and implementation in large economies,November 2007,Matthew O. Jackson,Ilan Kremer,,Male,Male,Unknown,Male,,7
11.0,3.0,Review of Economic Design,18 November 2006,https://link.springer.com/article/10.1007/s10058-006-0016-x,The effect of spillovers on the provision of local public goods,November 2007,Francis Bloch,Unal Zenginobuz,,Male,Unknown,Unknown,Male,,31
11.0,3.0,Review of Economic Design,14 July 2007,https://link.springer.com/article/10.1007/s10058-007-0028-1,On complexity of lobbying in multiple referenda,November 2007,Robin Christian,Mike Fellows,Arkadii Slinko,,Male,Male,Mix,,
11.0,3.0,Review of Economic Design,01 September 2007,https://link.springer.com/article/10.1007/s10058-007-0027-2,On the existence of consistent rules to adjudicate conflicting claims: a constructive geometric approach,November 2007,William Thomson,,,Male,Unknown,Unknown,Male,,10
11.0,4.0,Review of Economic Design,04 January 2008,https://link.springer.com/article/10.1007/s10058-007-0035-2,Mechanism design for a solution to the tragedy of commons,February 2008,Akira Yamada,Naoki Yoshihara,,,Male,Unknown,Mix,,
11.0,4.0,Review of Economic Design,22 November 2007,https://link.springer.com/article/10.1007/s10058-007-0034-3,Repeated implementation and complexity considerations,February 2008,Hannu Vartiainen,,,Male,Unknown,Unknown,Male,,
11.0,4.0,Review of Economic Design,13 December 2007,https://link.springer.com/article/10.1007/s10058-007-0037-0,Bargaining: separately or together?,February 2008,Alp E. Atakan,,,Male,Unknown,Unknown,Male,,6
11.0,4.0,Review of Economic Design,11 December 2007,https://link.springer.com/article/10.1007/s10058-007-0038-z,Weakly fair allocations and strategy-proofness,February 2008,Tommy Andersson,Lars-Gunnar Svensson,,Male,Unknown,Unknown,Male,,2
11.0,4.0,Review of Economic Design,18 December 2007,https://link.springer.com/article/10.1007/s10058-007-0036-1,In search of efficient network structures: the needle in the haystack,February 2008,Nicolas Carayol,Pascale Roux,Murat Yıldızoğlu,Male,Female,Male,Mix,,
12.0,1.0,Review of Economic Design,11 March 2008,https://link.springer.com/article/10.1007/s10058-008-0043-x,Good neighbors are hard to find: computational complexity of network formation,April 2008,Richard Baron,Jacques Durieu,Philippe Solal,Male,Male,Male,Male,,8
12.0,1.0,Review of Economic Design,06 March 2008,https://link.springer.com/article/10.1007/s10058-008-0041-z,Price asymptotics,April 2008,Kislaya Prasad,,,Unknown,Unknown,Unknown,Unknown,,
12.0,1.0,Review of Economic Design,13 March 2008,https://link.springer.com/article/10.1007/s10058-008-0042-y,A coalitional game-theoretic model of stable government forms with umpires,April 2008,Stefano Vannucci,,,Male,Unknown,Unknown,Male,,1
12.0,1.0,Review of Economic Design,06 March 2008,https://link.springer.com/article/10.1007/s10058-008-0040-0,Optimal speculative trade among large traders,April 2008,Kfir Eliaz,Ran Spiegler,,Male,,Unknown,Mix,,
12.0,2.0,Review of Economic Design,15 February 2008,https://link.springer.com/article/10.1007/s10058-008-0039-6,"Matching, search and intermediation with two-sided heterogeneity",June 2008,Nadia Burani,,,Female,Unknown,Unknown,Female,,2
12.0,2.0,Review of Economic Design,17 May 2008,https://link.springer.com/article/10.1007/s10058-008-0045-8,Labour market recruiting with intermediaries,June 2008,Paul Schweinzer,,,Male,Unknown,Unknown,Male,,2
12.0,2.0,Review of Economic Design,17 May 2008,https://link.springer.com/article/10.1007/s10058-008-0046-7,Optimal procurement mechanisms for divisible goods with capacitated suppliers,June 2008,Garud Iyengar,Anuj Kumar,,Unknown,Unknown,Unknown,Unknown,,
12.0,3.0,Review of Economic Design,16 May 2008,https://link.springer.com/article/10.1007/s10058-008-0044-9,Optimal fees in internet auctions,September 2008,Alexander Matros,Andriy Zapechelnyuk,,Male,Male,Unknown,Male,,6
12.0,3.0,Review of Economic Design,16 July 2008,https://link.springer.com/article/10.1007/s10058-008-0048-5,Price experimentation with strategic buyers,September 2008,Oksana Loginova,Curtis R. Taylor,,Female,Male,Unknown,Mix,,
12.0,3.0,Review of Economic Design,16 July 2008,https://link.springer.com/article/10.1007/s10058-008-0047-6,Innovation contests with temporary and endogenous monopoly rents,September 2008,Frederik Schmidt,,,Male,Unknown,Unknown,Male,,3
12.0,3.0,Review of Economic Design,19 July 2008,https://link.springer.com/article/10.1007/s10058-008-0050-y,On the optimality of nonmaximal fines in the presence of corruptible law enforcers,September 2008,Gorkem Celik,Serdar Sayan,,Unknown,Male,Unknown,Male,,1
12.0,4.0,Review of Economic Design,24 July 2008,https://link.springer.com/article/10.1007/s10058-008-0049-4,Informed principal and information gathering agent,December 2008,Dongsoo Shin,Sungho Yun,,Unknown,Unknown,Unknown,Unknown,,
12.0,4.0,Review of Economic Design,15 October 2008,https://link.springer.com/article/10.1007/s10058-008-0051-x,Optimal auctions for asymmetrically budget constrained bidders,December 2008,Alexey Malakhov,Rakesh V. Vohra,,Male,Male,Unknown,Male,,15
12.0,4.0,Review of Economic Design,30 October 2008,https://link.springer.com/article/10.1007/s10058-008-0052-9,Opportunity analysis of newborn screening programs,December 2008,Carmen Herrero,Juan D. Moreno-Ternero,,Female,Male,Unknown,Mix,,
12.0,4.0,Review of Economic Design,30 October 2008,https://link.springer.com/article/10.1007/s10058-008-0060-9,Differentiable strategy-proof mechanisms for private and public goods in domains that are not necessarily large or quasi-linear,December 2008,Luis C. Corchón,José Rueda-Llano,,Male,Male,Unknown,Male,,1
12.0,4.0,Review of Economic Design,15 November 2008,https://link.springer.com/article/10.1007/s10058-008-0061-8,Coordinating under incomplete information,December 2008,Geir B. Asheim,Seung Han Yoo,,Male,,Unknown,Mix,,
13.0,1.0,Review of Economic Design,28 March 2009,https://link.springer.com/article/10.1007/s10058-009-0079-6,Introduction to the issues in honor of Leonid Hurwicz,April 2009,John Ledyard,,,Male,Unknown,Unknown,Male,,
13.0,1.0,Review of Economic Design,28 March 2009,https://link.springer.com/article/10.1007/s10058-009-0076-9,Two topics in Leo Hurwicz’s research,April 2009,Stanley Reiter,,,Male,Unknown,Unknown,Male,,2
13.0,1.0,Review of Economic Design,30 December 2008,https://link.springer.com/article/10.1007/s10058-008-0065-4,Inefficiency and complementarity in sharing games,April 2009,Dennis Courtney,Thomas Marschak,,Male,Male,Unknown,Male,,2
13.0,1.0,Review of Economic Design,31 December 2008,https://link.springer.com/article/10.1007/s10058-008-0068-1,Strategic analysis of petty corruption with an intermediary,April 2009,Ariane Lambert-Mogiliansky,Mukul Majumdar,Roy Radner,Female,,Male,Mix,,
13.0,1.0,Review of Economic Design,21 January 2009,https://link.springer.com/article/10.1007/s10058-008-0071-6,Fundamental theory of institutions: a lecture in honor of Leo Hurwicz,April 2009,Roger B. Myerson,,,Male,Unknown,Unknown,Male,,23
13.0,1.0,Review of Economic Design,26 February 2009,https://link.springer.com/article/10.1007/s10058-009-0075-x,The human side of mechanism design: a tribute to Leo Hurwicz and Jean-Jacque Laffont,April 2009,Daniel McFadden,,,Male,Unknown,Unknown,Male,,21
13.0,1.0,Review of Economic Design,23 December 2008,https://link.springer.com/article/10.1007/s10058-008-0066-3,Discrete implementation of the Groves–Ledyard mechanism,April 2009,J. Todd Swarthout,Mark Walker,,Unknown,Male,Unknown,Male,,1
13.0,1.0,Review of Economic Design,15 February 2009,https://link.springer.com/article/10.1007/s10058-009-0074-y,Communication complexity and stability of equilibria in economies and games,April 2009,J. S. Jordan,,,Unknown,Unknown,Unknown,Unknown,,
13.0,1.0,Review of Economic Design,13 November 2008,https://link.springer.com/article/10.1007/s10058-008-0059-2,"Non-existence of equilibrium in Vickrey, second-price, and English auctions",April 2009,Matthew O. Jackson,,,Male,Unknown,Unknown,Male,,20
13.0,1.0,Review of Economic Design,03 February 2009,https://link.springer.com/article/10.1007/s10058-009-0073-z,The endowment game when n = 2,April 2009,Lu Hong,,,,Unknown,Unknown,Mix,,
14.0,1.0,Review of Economic Design,16 September 2009,https://link.springer.com/article/10.1007/s10058-009-0092-9,Implementation of solutions to the problem of fair division when preferences are single-peaked,March 2010,William Thomson,,,Male,Unknown,Unknown,Male,,9
14.0,1.0,Review of Economic Design,28 November 2008,https://link.springer.com/article/10.1007/s10058-008-0062-7,On Maskin monotonicity of solution based social choice rules,March 2010,Claus-Jochen Haake,Walter Trockel,,Unknown,Male,Unknown,Male,,4
14.0,1.0,Review of Economic Design,13 August 2009,https://link.springer.com/article/10.1007/s10058-009-0090-y,Equilibrium participation in public goods allocations,March 2010,Paul J. Healy,,,Male,Unknown,Unknown,Male,,9
14.0,1.0,Review of Economic Design,01 December 2009,https://link.springer.com/article/10.1007/s10058-009-0100-0,Fundamental impossibility theorems on voluntary participation in the provision of non-excludable public goods,March 2010,Tatsuyoshi Saijo,Takehiko Yamato,,Unknown,Male,Unknown,Male,,14
14.0,1.0,Review of Economic Design,31 December 2008,https://link.springer.com/article/10.1007/s10058-008-0067-2,Optimal information transmission in organizations: search and congestion,March 2010,Àlex Arenas,Antonio Cabrales,Fernando Vega-Redondo,Unknown,Male,Male,Male,,9
14.0,1.0,Review of Economic Design,19 April 2009,https://link.springer.com/article/10.1007/s10058-009-0080-0,On aggregation and welfare analysis,March 2010,James C. Moore,,,Male,Unknown,Unknown,Male,,1
14.0,1.0,Review of Economic Design,30 September 2009,https://link.springer.com/article/10.1007/s10058-009-0095-6,Cores of many-player games; nonemptiness and equal treatment,March 2010,Myrna Wooders,,,Female,Unknown,Unknown,Female,,5
14.0,1.0,Review of Economic Design,13 August 2009,https://link.springer.com/article/10.1007/s10058-009-0088-5,Implementation of marginal cost pricing equilibrium allocations with transfers in economies with increasing returns to scale,March 2010,Guoqiang Tian,,,Unknown,Unknown,Unknown,Unknown,,
14.0,1.0,Review of Economic Design,20 December 2008,https://link.springer.com/article/10.1007/s10058-008-0064-5,Random iterates of monotone maps,March 2010,Rabi Bhattacharya,Mukul Majumdar,,Male,,Unknown,Mix,,
14.0,1.0,Review of Economic Design,20 December 2008,https://link.springer.com/article/10.1007/s10058-008-0063-6,The global LeChatelier Principle and multimarket equilibria,March 2010,George M. Lady,James P. Quirk,,Male,Male,Unknown,Male,,4
14.0,1.0,Review of Economic Design,30 December 2008,https://link.springer.com/article/10.1007/s10058-008-0069-0,The current non-status of general equilibrium theory,March 2010,Donald W. Katzner,,,Male,Unknown,Unknown,Male,,5
14.0,1.0,Review of Economic Design,19 August 2009,https://link.springer.com/article/10.1007/s10058-009-0086-7,Regional business development policy in Central and Eastern Europe: a mechanism design perspective,March 2010,Roswitha M. King,,,Female,Unknown,Unknown,Female,,1
15.0,1.0,Review of Economic Design,14 November 2009,https://link.springer.com/article/10.1007/s10058-009-0099-2,On the informed seller problem: optimal information disclosure,March 2011,Vasiliki Skreta,,,Female,Unknown,Unknown,Female,,21
15.0,1.0,Review of Economic Design,30 December 2008,https://link.springer.com/article/10.1007/s10058-008-0070-7,"Franchise bidding, regulation and investment costs",March 2011,Michel Mougeot,Florence Naegelen,,Male,Female,Unknown,Mix,,
15.0,1.0,Review of Economic Design,19 September 2009,https://link.springer.com/article/10.1007/s10058-009-0089-4,"Firms, queues, and coffee breaks: a flow model of corporate activity with delays",March 2011,Benjamin Golub,R. Preston McAfee,,Male,Unknown,Unknown,Male,,1
15.0,2.0,Review of Economic Design,22 September 2009,https://link.springer.com/article/10.1007/s10058-009-0093-8,Countervailing power? Collusion in markets with decentralized trade,June 2011,Nadia Burani,Clara Ponsati,,Female,Female,Unknown,Female,,1
15.0,2.0,Review of Economic Design,31 October 2009,https://link.springer.com/article/10.1007/s10058-009-0097-4,Buyer–seller networks with demand shocks and intermediation,June 2011,Hemant Patil,,,Male,Unknown,Unknown,Male,,1
15.0,2.0,Review of Economic Design,27 October 2010,https://link.springer.com/article/10.1007/s10058-010-0106-7,The n-person Kalai-Smorodinsky bargaining solution under pre-donations,June 2011,S. Nuray Akin,Brennan C. Platt,Murat R. Sertel,Unknown,Male,Male,Male,,4
15.0,2.0,Review of Economic Design,09 February 2011,https://link.springer.com/article/10.1007/s10058-011-0108-0,Lender learning and entry under general demand uncertainty,June 2011,Neelam Jain,Leonard J. Mirman,,Female,Male,Unknown,Mix,,
15.0,3.0,Review of Economic Design,20 June 2010,https://link.springer.com/article/10.1007/s10058-010-0104-9,Bidder collusion at first-price auctions,September 2011,Giuseppe Lopomo,Leslie M. Marx,Peng Sun,Male,,,Mix,,
15.0,3.0,Review of Economic Design,04 January 2011,https://link.springer.com/article/10.1007/s10058-010-0107-6,Optimal allocation mechanisms with single-dimensional private information,September 2011,Nicolás Figueroa,Vasiliki Skreta,,Male,Female,Unknown,Mix,,
15.0,3.0,Review of Economic Design,08 July 2011,https://link.springer.com/article/10.1007/s10058-011-0110-6,Selling goods of unknown quality: forward versus spot auctions,September 2011,Isa E. Hafalir,Hadi Yektaş,,Female,Male,Unknown,Mix,,
15.0,4.0,Review of Economic Design,12 April 2011,https://link.springer.com/article/10.1007/s10058-011-0109-z,Consistency and its converse: an introduction,December 2011,William Thomson,,,Male,Unknown,Unknown,Male,,54
15.0,4.0,Review of Economic Design,17 November 2009,https://link.springer.com/article/10.1007/s10058-009-0098-3,Agreement toward stability in matching markets,December 2011,David Cantala,,,Male,Unknown,Unknown,Male,,2
15.0,4.0,Review of Economic Design,29 April 2010,https://link.springer.com/article/10.1007/s10058-010-0103-x,A dynamic analysis of collusive networks,December 2011,Yasunori Okumura,,,Male,Unknown,Unknown,Male,,
15.0,4.0,Review of Economic Design,11 August 2010,https://link.springer.com/article/10.1007/s10058-010-0105-8,A note on strategy-proofness from the doctor side in matching with contracts,December 2011,Toyotaka Sakai,,,Unknown,Unknown,Unknown,Unknown,,
16.0,1.0,Review of Economic Design,15 September 2011,https://link.springer.com/article/10.1007/s10058-011-0111-5,Random aggregation without the Pareto principle,March 2012,Jérémy Picot,,,Male,Unknown,Unknown,Male,,2
16.0,1.0,Review of Economic Design,28 December 2011,https://link.springer.com/article/10.1007/s10058-011-0112-4,"Information complexity, punishment, and stability in two Nash efficient Lindahl mechanisms",March 2012,Matt Van Essen,,,Male,Unknown,Unknown,Male,,9
16.0,1.0,Review of Economic Design,28 January 2012,https://link.springer.com/article/10.1007/s10058-012-0113-y,The decentralization tradeoff for complementary spillovers,March 2012,Martin Gregor,Lenka Stastna,,Male,Female,Unknown,Mix,,
16.0,1.0,Review of Economic Design,15 February 2012,https://link.springer.com/article/10.1007/s10058-012-0114-x,Under-connected and over-connected networks: the role of externalities in strategic network formation,March 2012,Berno Buechel,Tim Hellmann,,Male,Male,Unknown,Male,,15
16.0,2.0,Review of Economic Design,24 June 2012,https://link.springer.com/article/10.1007/s10058-012-0128-4,Introduction to a festschrift for Andrew Schotter,September 2012,Boğaçhan Çelen,Erkut Özbay,,Unknown,Male,Unknown,Male,,
16.0,2.0,Review of Economic Design,26 May 2012,https://link.springer.com/article/10.1007/s10058-012-0120-z,"Network architecture, cooperation and punishment in public good experiments",September 2012,Jeffrey Carpenter,Shachar Kariv,Andrew Schotter,Male,Male,Male,Male,,38
16.0,2.0,Review of Economic Design,26 May 2012,https://link.springer.com/article/10.1007/s10058-012-0121-y,Fear of losing in a clock auction,September 2012,Peter Cramton,Emel Filiz-Ozbay,Pacharasut Sujarittanonta,Male,Female,Unknown,Mix,,
16.0,2.0,Review of Economic Design,26 May 2012,https://link.springer.com/article/10.1007/s10058-012-0122-x,Social learning in networks: a Quantal Response Equilibrium analysis of experimental data,September 2012,Syngjoo Choi,Douglas Gale,Shachar Kariv,Unknown,Male,Male,Male,,24
16.0,2.0,Review of Economic Design,26 May 2012,https://link.springer.com/article/10.1007/s10058-012-0123-9,Behavioral mechanism design: evidence from the modified first-price auctions,September 2012,Yusufcan Masatlioglu,Sarah Taylor,Neslihan Uler,Male,Female,Female,Mix,,
16.0,2.0,Review of Economic Design,06 June 2012,https://link.springer.com/article/10.1007/s10058-012-0124-8,A Bayesian approach to experimental analysis: trading in a laboratory financial market,September 2012,Marco Cipriani,Riccardo Costantini,Antonio Guarino,Male,Male,Male,Male,,6
16.0,2.0,Review of Economic Design,05 June 2012,https://link.springer.com/article/10.1007/s10058-012-0125-7,Heterogeneous ambiguity attitudes: a field experiment among small-scale stock investors in China,September 2012,Elizabeth Potamites,Bei Zhang,,Female,,Unknown,Mix,,
16.0,2.0,Review of Economic Design,29 May 2012,https://link.springer.com/article/10.1007/s10058-012-0126-6,A cognitive hierarchy model of learning in networks,September 2012,Syngjoo Choi,,,Unknown,Unknown,Unknown,Unknown,,
16.0,2.0,Review of Economic Design,06 June 2012,https://link.springer.com/article/10.1007/s10058-012-0127-5,An experiment of social learning with endogenous timing,September 2012,Boğaçhan Çelen,Kyle Hyndman,,Unknown,,Unknown,Mix,,
17.0,1.0,Review of Economic Design,04 July 2012,https://link.springer.com/article/10.1007/s10058-012-0129-3,Sealed bid auctions versus ascending bid auctions: an experimental study,March 2013,Christer Andersson,Ola Andersson,Tommy Andersson,Male,Male,Male,Male,,2
17.0,1.0,Review of Economic Design,05 October 2012,https://link.springer.com/article/10.1007/s10058-012-0135-5,An equity characterization of second price auctions when preferences may not be quasilinear,March 2013,Toyotaka Sakai,,,Unknown,Unknown,Unknown,Unknown,,
17.0,1.0,Review of Economic Design,18 September 2012,https://link.springer.com/article/10.1007/s10058-012-0133-7,"Arbitrage, strategic inefficiency and self-regulation",March 2013,Dimitris Voliotis,,,Male,Unknown,Unknown,Male,"The enhanced strategic and complex environment of modern financial markets necessitates the creation of a well designed system of regulatory institutions that ensures the well functioning of the financial system and strengthens the alignment of investment and risk appetites of market participants. Central banks play increasingly an enhanced role in meeting these challenges of the financial environment, by undertaking the responsibility to serve as regulators of financial markets. As a result, central banks have a dual policy role to serve, in the sense that not only do they safeguard monetary stability but also preserve financial stability. In other words, monetary authorities guarantee price stability and the sufficiency of liquidity, but also impose rules and investment barriers when necessary to sustain financial stability as regulatory authorities of the financial system. In either case, policy interventions have an unambiguous distributive effect to market participants, that many times has been criticized to be distortive enough to cause a welfare worsening. This paper draws attention to the possibility of a regulatory scheme that, in our context, ensures welfare improvement in a Pareto sense. We claim that the imposition of investment barriers can effectively make everyone better off and that the interplay of regulatory and monetary policy-making of central banks has an important role towards this result. Moreover, in an attempt to administer effectively the regulatory measures, it is necessary to strengthen the supervisory mechanisms. Toward this direction we suggest that market participants should be incentivized to commit to self-police the implementation of regulatory measures. For this task, market participants will follow some sort of “coordinated” strategy designated by the central bank and backed by a bonding monetary scheme associated to their fractional reserves. On top of that, in the context of an OLG economy, we prove that under certain conditions market participants can sustain a welfare improving outcome, without the intervention of the central bank. In fact, we claim that markets can be self-regulated. The present exercise assumes certainty and all traders in the economy can predict accurately the equilibrium prices and recall all past realizations. Nevertheless, the same model under uncertainty exhibits further interest, since it could provide a starting point for analyzing financial risks in a strategic framework, by emphasizing more on systemic risks. The mitigation of strategic risks can be achieved through the appropriate design of coordinated strategies that will “keep down” the markets when they start becoming “overheated”. There are several papers in the literature that illustrate the possibility of a Pareto improving allocation in financial markets in an OLG monetary economy. Most notably, Allen and Gale (1997) show that intertemporal non-diversifiable risks can be addressed by introducing an intermediated financial system. The same strand is followed by Freixas and Tsomocos (2004), who also advocate favorably for financial intermediaries as shock absorbers, when extrinsic uncertainty is present. In this framework, they aim to assess the performance of alternative accounting rules of firms. In contrast, this paper does not accommodate for extrinsic uncertainty and the causes of inefficiency are purely strategic. Strategic inefficiency in financial markets, however, also appears in the work of Giraud and Weyers (2004) and in Koutsougeras and Papadopoulos (2004). The first paper shows that under incomplete financial markets, imperfect competition may perform better than perfectly competitive markets, whereas the second paper shows that when market participants have strategic power, arbitrage opportunities can coexist with frictionless markets. This work is also relevant to the work of Giraud and Weyers (2004). Giraud and Weyers (2004) provide their results by making use of the folk theorem by Benoit and Krishna (1985), which requires the full-dimensionality assumption. In this paper, the main result is proved by employing the OLG folk theorem of Kandori (1992), which abandons full dimensionality by making use of the OLG term structure. Generally, the model provides a dynamic monetary economy, supplemented by a mutual central bank, which is a financial intermediary authorized to issue paper money and responsible to regulate the market. It is mutual, since the money issuance is backed by the fractional reserves of individuals of some numeraire commodity, i.e. gold. This idea basically rests upon Shubik and Tsomocos (1992), who introduce a mutual central bank with fractional reserves in an economy with a continuum of traders. They use the offer-for-sale mechanism of Dubey and Shubik (1978), where traders submit their offer and their bid at each trading post of commodities. Here, we assume an infinite number of traders divided into a finite set of types and the same offer-for-sale market mechanism applies for real markets. The consideration of a continuum of traders follows closely the Shubik and Tsomocos (1992) setup, without the symmetry of types. By this way, we naturally extend their model of the mutual central bank in a multi-period setting. However, this model does not attempt to address the same issues appearing in Shubik and Tsomocos (1992), but utilize effectively the mutual central bank with fractional reserves, not only as paper money issuer, but as a financial regulator as well. For financial markets we adopt the sell-all model of Shapley and Shubik (1977), which seems to be more suitable for a fixed supply formation. In particular, we assume two securities, one with no return and unconstrained supply that represents the storage technology, and another with positive (nominal) return that is provided in fixed supply. Securities in the model are not instruments that traders use for risk hedging. Rather, traders primarily use the securities to smooth their consumption over the time. The main toolkit of the analysis is the repeated games framework with a simple or overlapping generations term structure of players, while the equilibrium concept used is the subgame perfect equilibrium (SPE). The main result is imputed by using the OLG folk theorem of Kandori (1992), while many elements have been adopted from Benoit and Krishna (1985). Other related OLG folk theorems appearing in the literature are those of Salant (1991), which is referring to two-player games and of Smith (1992) that extends Kandori’s results, using overlaps rather than generations of traders as a basic unit of analysis. The rest of the paper goes as follows. Section 2 outlines the economy and Sect. 3 presents the market mechanism of the one-shot game with the presence of a mutual central bank. It also defines the one-shot strategic equilibrium. Section 4 presents the \(T\)-repeated game and shows that the subgame perfect equilibrium of the game guarantees no positive return, whatsoever. Section 5 introduces regulations as bonding schemes that enforce a Pareto improving outcome associated with positive return for all traders. Section 6 shifts the analysis to an overlapping economy, enabling us to achieve a positive return outcome as a self-enforcing subgame perfect equilibrium. Finally, Sect. 7 gathers our main results and offers some comments.",
17.0,1.0,Review of Economic Design,29 December 2012,https://link.springer.com/article/10.1007/s10058-012-0138-2,Nash-implementation of competitive equilibria via a bounded mechanism,March 2013,Gael Giraud,Hubert Stahn,,Male,Male,Unknown,Male,"
It is widely acknowledged that Cournotian market gamesFootnote 1, or Monopolistic competitionFootnote 2 typically induce non-competitive outcomes. So, if one tries to conceive market mechanisms whose strategic equilibrium outcomes are competitive, the most common way to proceed consists in enabling several players to quote prices (sometimes in addition to choosing quantities): this leads to games, whose seminal paradigm is of course Bertrand (1883). But it is well known that the strategic equilibrium that sustains competitive outcomes seems to rest on very vulnerable grounds: in a standard Bertrand game, the unique Nash equilibrium involves weakly dominated strategies. This means that the Nash equilibrium of such games, though it induces a Pareto-efficient outcome, will not be robust to any minimal refinement involving admissibility: as is well-known, indeed, the elimination of weakly dominated strategies is one privileged way to get strategically stable equilibria.Footnote 3
 The question we therefore ask is the following: Is it possible to construct a mechanism which is fairly decentralized, in the sense that the job of “the market manager” is not too exigent and which fully implements competitive equilibria in undominated strategies. To answer this question, our paper borrows from two strands of the literature. There is, on the one hand, the Nash implementation approach which characterizes the Nash implementable Social Choice Correspondences (SCC) by applying canonical and hence complex mechanisms in the line of Maskin (1999) and Palfrey and Srivastana (1991). On the other hand, if one follows the line of research open by Hurwicz (1979) and Schmeidler (1980), the problem is to find a “suitable” market game which implements a specific SCC, the Walrasian one. From the first strand of literature, we know (see Palfrey and Srivastana (1991)) that the set of SCC implementable in undominated Nash equilibrium may be larger than the set of SCC which are Nash implementable since the \(Q\) properties introduced by these authors is weaker then the monotonicity requirement put forward by Maskin (1999). However, Jackson (1992) notices that the power of this result is due to the fact that there are no restrictions on the mechanism which implements the SCC. In particular, it may happen that the mechanism involves (possibly hidden) integer or modulo games, e.g. an infinite sequence of mutually dominated strategies. This is why he introduced bounded mechanismsFootnote 4 for which the following holds: If a given strategy, \(s_{1}\), is (weakly) dominated by, say, \(s_{2}\), then there must exist at least one third strategy \(s_{3}\), which dominates \(s_{2}\) and is itself not weakly dominated. As a particular application of the results of Jackson et al. (1994) it can even be shown that the Walsasian correspondence is undominantly implementable via such a mechanism under rather general conditions. But in the best of our knowledge no paper provides a fairly decentralized mechanism which exhibits these properties. This conducts us to the second line of research initiated by Hurwicz (1979) and Schmeidler (1980). The object of this research is the construction of a outcome function which reflects the economic institutions of trade and which renders the Walrasian allocation. This literature (see also Postlewaite and Wettstein (1989) or Tian (1992)) essentially deals with the Nash implementation of the (constrained) Walrasian correspondence. The different players usually send price and quantity messages to “the market manager” which allocates the goods by applying “simple” or “suitable” rules. In other words, the Strategic Outcome Function (SOF) must be interpretable and should satisfy at least some desirable properties. For instance, the SOF should be individually feasible (each agent allocated bundle is in her consumption set), and balanced (the sum of the agents’ allocated bundles is equal the sum of the endowments). Tian’s mechanism (1992) which implements the constrained Walrasian correspondence satisfies these requirements and is even continuous. But he does not really introduce a mechanism which can be based on bilateral exchange since the price formation rule and the rationing mechanism are rather crude. Schmeidler (1980) and Postlewaite and Wettstein (1989) proposed a mechanisms which is, for the first, balanced but not individually feasible nor continuous and, for the second, continuous and individually feasible but only weakly balanced (the sum the allocated consumption bundles is smaller than the total endowments). Our contribution borrows from both approaches. Our main objective is to construct a mechanism which is fairly decentralized, in the sense that the job of “the market manager” is not too exigent. For instance, the potential net-trades induced by the individual price and quantity signal are deduced from pairwise interactions. The “market manager” simply (1) adjusts, when necessary, the total supply to the total demand by applying a proportional rationing rule market per market and (2) punishes the players who do not meet their budget constraint. From that point of view, our mechanism is individually feasible, weakly balanced and even balanced at equilibrium. Weak balancedness follows from the necessity to punish the agents who do not met her budget constraint. Our mechanism is however not continuous, since it is based on pairwise interactions. In fact, in the spirit of a Bertrandian game, a seller only trades with consumers quoting a higher price: a slight decrease in her own price may therefore increase the opportunity of pairwise trades independently of the amount which is traded. Our assumptions are also slightly different from those found in the literature on implementation. We essentially restrict ourself to a situation in which the endowments are strictly positive and are preferred to any allocation lying on the boundary of the consumption set. This gives us the opportunity to escape the Maskin impossibility result (1985) which claims that in general the Walrasian correspondence is not Nash implementable. By adding a local non satiation and a convexity assumption, we are then able to make sure that our mechanism implements the Walrasian equilibria. In a second step, we make sure that our mechanism is bounded. We even go a step further by showing that every action is undominated. This requires two additional assumptions: a monotonicity one which says that each good is desired by at least one agent and which gives us the opportunity to restrict the game to strictly positive prices and an assumption related to the existence of a worst element (see Jackson et al. (1994)) which says that each outcome which renders less than the initial endowments is less preferred. We nevertheless do not need continuity nor strict monotonicity. The paper proceeds as follows: the next section depicts the economic environment, and defines the mechanism. Section 3 contains some basic properties of Nash equilibria, which essentially aims at preparing the results to follow. In Sect. 4, we prove that every competitive equilibrium can be implemented by some Nash equilibrium strategy profile, while Sect. 5 is devoted to proving that, conversely, every Nash equilibrium outcome is a Walrasian equilibrium. In Sect. 6, we prove that the mechanism is bounded. Finally, the last section contains some additional comments.",
17.0,1.0,Review of Economic Design,13 December 2012,https://link.springer.com/article/10.1007/s10058-012-0139-1,The organization of expertise in the presence of communication,March 2013,Flavia Roldán,,,Female,Unknown,Unknown,Female,"Many decisions are complex and involve multiple aspects. In some cases decision-makers lack time and skill to gather, process, and summarize relevant information on which to base decisions. They may rely on information provided by specialized experts who are hired for the specific goal of offering input on given decisions. In this article, the particular way in which information is produced is determinative of its value. In particular, as Arrow pointed out (1969, p. 30), “Knowledge arises from deliberate seeking, but it also arises from observations incidental on other activities”. The goal of this article is to examine an economic framework in which an uninformed principal must elicit information from unbiased experts who must, in turn, decide whether or not to collect costly information (this is what Arrow terms “deliberate seeking”). If agents collect information, each gains access to a noisy signal about the true state of the world. Agents may communicate with each other about the signals that they have obtained, and in doing so, each will get more precise information than initially acquired (this for us is Arrow’s remark about “Knowledge\(\ldots \)also arises from observations incidental on other activities”). Our simple framework incorporates an opportunistic consideration of communication: I assume that communication opens the possibility of collusion among agents against the main interest of the principal. Because communication has conflicting consequences, one question addressed in this article is: should principals promote or impede communication among experts? If a principal allows communication among experts, what is the best way to organize agents who are in communication? In particular, when is it optimal, from a principal’s point of view, to let agents communicate with each other? These issues are studied in a multiagent-principal framework when communication among agents allows not only cooperation in favor of the principal but also collusion against her. I study and compare the principal’s net surplus under different organizational forms. First, I compute the principal’s net surplus in the no-communication case. Then I consider a situation in which the principal organizes experts in a common workplace, that is, a group of experts, and facilitates communication among them. In this case, communication has conflicting consequences. On the one hand, when agents communicate with each other, signals are more precise than in the absence of communication. This fact not only has a positive direct impact on the principal’s surplus but it also reduces, since signals are more precise, informational rents to agents. On the other hand, when agents are in communication, they are able to collude; that is, they are able to manipulate in their self interest the private information that they have received. In contrast, when the principal prevents communication among agents, she avoids the collusion problem, but in such a case, she sacrifices signal precision. We assess trade-offs involved in each work structure. To answer the question of optimal timing for communication among agents, it is important to note the following. Because positive effects of communication arise after agents have collected information, it is intuitive to say that communication should be allowed at this moment. If the principal, however, lets agents communicate with each other from the beginning, i.e., before they collect costly information, they are not only able to manipulate the information that they reveal to the principal, but also they can sign side contracts contingent on their decisions whether or not each actually gathers information. This fact, however, has a positive impact on the principal’s well-being since agents will be able to coordinate their effort choices. In contrast, when agents can only communicate with each other after exerting effort, they are not able to share their effort choices in collecting information. In such cases, each agent makes his decision without information about the other agents’ decision. Although the principal can also prevent collusion on the choice of effort in such situations, she imposes more uncertainty on agents than would be imposed if each expert had knowledge about the effort exerted by other experts. I find that complementarities between experts, that emerge in the communication stage, facilitate principals’ provision of incentives to agents who communicate with each other from the outset. In other words, the principal’s welfare increases when experts are able to observe their respective effort choices and communicate with each other about their signals. In the absence of complementarities in the signal communication phase, the principal would be better off if she could avoid communication between experts. If it is not possible to prevent communication, then she should postpone it as long as possible. The intuition behind this result is that, under complementarity effects of communication, when an agent observes that his partner does not collect information, he has incentives to fail to collect information also. In the absence of complementarities in the communication phase, when an agent observes that his partner does not collect information, he will be better off if, at least, one of them gathers information. This article is linked to three lines of research: endogenous acquisition of information, transmission of information, and organization of expertise. Literature focusing on information revelation obtains as its main result the notion that if there are no costs of supplying information, perfect information transmission requires that the decision-maker and the expert have identical preferences. In this line of research, Wolinsky (2002) obtains results close to those in this paper. The focus is on how a decision-maker can take advantage of multiple experts. Wolinsky shows that in some circumstances, allowing partial communication among experts may result in the revelation of more information than either full communication or no communication. In the aforementioned literature,Footnote 1 however, the focus is on strategic information revelation rather than on information acquisition. In contrast, in the current article, the decision-maker elicits information from multiple unbiased experts, and agents must decide whether or not to acquire information. Several authors analyze this issue in the literature. For example, Li (2001) and Szalay (2005) examine information acquisition when players have the same preferences but implementation of monetary incentives is not feasible.Footnote 2 As in the current article, Gromb and Martimort (2007) consider the design of monetary incentives and study the implications of optimal incentive contracts for the organizational design of expertise. They assume a case with a principal who bases a decision on two signals about a project’s value, and agents who can draw independent signals at a fixed cost per signal. After receiving signals, the agents recommend to either to undertake the project or not undertake it. The authors show that it is optimal to reward an agent if his recommendation is confirmed by the state or by another recommendation (conflicting reports are penalized).Footnote 3 Subsequently, the authors analyze when it is optimal, from the principal’s point of view, to have a single expert gather two signals or two experts collect one signal each. In the present paper, unlike Gromb and Martimort (2007), I assume that signal precision not only increases with effort (a fixed cost per signal) but also increases with (horizontal) communication among agents. At this point, the organization of expertise becomes crucial. With multiple agents, how should experts be organized to ensure that they refine their knowledge about the true state of the world and fully disclose their signals? Unlike Gromb and Martimort (2007), I analyze the optimal organization of communication among experts. This particular feature is close to the concept of Itoh (1993) in that the principal benefits from contracting a consolidated unit whose utility is the sum of its members’ utilities and in which employees can monitor each others’ efforts and coordinate their actions.Footnote 4
 The paper proceeds as follows. Before introducing the model, I highlight the main issues I wish to elucidate by providing an example. Section 2 presents the general setting, and Sect. 3 presents the benchmark case: the no-communication situation. Section 4 compares the principal’s surplus when communication implies both collusion among experts and synergistic effects. In such a case, I study, from the principal’s point of view, the optimality of two alternative organizations of communication: communication among agents before they decide to collect information, and after they choose whether or not to gather information. Finally, Sect. 5 offers conclusions. All proofs and details of calculations are in the Appendix. 
An example: an intelligence problem
 
Organization without Communication Consider the problem faced by the Director of Intelligence of country A. The Director has received an alert of possible sabotage against the tabloid press and must decide whether or not to impose a red alert. She hires two spies who must provide information about the likelihood of sabotage. Let us assume that the two spies work in isolation and the agents’ identities remain unknown to each other. Each spy must decide whether or not to collect intelligence data. After collecting data, each processes all available information and obtains a noisy signal about the probability of an attack occurring. For example, one of the spies might obtain information via interception of communications (telephone calls, e-mails, letters and so on). He processes data and obtains a signal, although some individuals mentioned in telephone calls or in letters cannot be declared “dangerous” because there is no proof to that effect. The other spy concentrates his investigations on information about people who have entered and left the country in the past year. Similarly, he processes data and gets a signal, but he does not find any conclusive evidence that some particular individuals being investigated are involved in a possible attack on the yellow press. The Director of Intelligence receives one signal from each agent, after which he will make a decision. If the spies supply conflicting signals, the Director penalizes them, as the state of the world is unitary (that is, sabotage or not). 
The Pros and Cons of Communication Now let us assume that the Director allows agents to communicate with each other. In such a case, they would exchange their initial knowledge and certain items of information that before might have appeared irrelevant but would now become important for the investigation. For example, they would realize that the names of some individuals mentioned in letters or telephone calls (and that, before communication, they were irrelevant for the inquiry) matched people who had entered and left the country in the last few months (and again, before communication, were impertinent for the investigation). This coincidence could provide sufficient evidence that these people were involved in planning an attack. Therefore, communication improves signal precision. Because the spies are in communication, they may coordinate their reports to show that, for example, the potential sabotage is only a rumor spread by the yellow press itself but without proof of that. The Director’s problem shifts to whether or not to allow communication between the spies. If the benefit from sharing information outweighs the potential collusion cost, it seems reasonable that both spies should work together as a single intelligence team. 
The Organization of Communication How should the spies be organized in the communication phase? At what point should the principal allow the agents to communicate with each other? 
Before If the spies’ identities are revealed from the outset, they may collude not only on their reports but also on their decision about whether or not to collect information. 
After If each agent knows his partner’s identity only after he decides whether or not to gather information, the Director avoids the possibility of collusion on effort choices, and she still takes advantage of exchange of information between the spies. In such a case, however, each spy makes his decision without knowing the other spy’s decision. In summary, the Director must decide not only whether or not she will allow communication but also the optimal time for allowing the spies to communicate with each other.",
17.0,2.0,Review of Economic Design,28 April 2013,https://link.springer.com/article/10.1007/s10058-013-0141-2,Maximally representative allocations for guaranteed delivery advertising campaigns,June 2013,R. Preston McAfee,Kishore Papineni,Sergei Vassilvitskii,Unknown,Unknown,Male,Male,"Consider an agency, publisher or network that has a set of supply pools of inventory. Examples of supply pools could include “young men on auto pages” or “unknown gender on a computer related news article”. Index campaigns by superscripts. Campaign \(j\) specifies a quantity \(Y^{j}\) and a set of eligible supply pools. A supply pool \(i\) is eligible for campaign \(j\) if \(s_i^j =1\), and \(s_i^j =0\) otherwise. (Subscripts are used to denote supply pools.) The variable \(s_i^j\) also captures the possibility that one might sell males, but deliver unknowns weighting them by the likelihood of obtaining males. Thus, if 55 % of the pool “unknown gender” are males, one can deliver males by providing “unknown genders” but only at the rate \(s_i^j=55\,\%\). The following additional notation is convenient: 
\(x_{i}\) is the volume of supply pool \(i\)
 
\(r_{i}\) is the minimum price for supply pool \(i\),Footnote 4
 
\(y_i^j\) is the amount of supply pool \(i\) that will go toward contract \(j,Y^{j}=\sum _i {s_i^j\,y_i^j}\), and 
\(V^{j}\) is the priority or value attributed to campaign \(j\). The objective we posit is to minimize the weighted squared distance of the within-supply pool market shares from representative market shares. By representative shares, we mean the values of the background population. For campaign \(j\), the representative share of inventory type \(i\) is \(\frac{s_i^j \, x_i}{\sum _k {s_k^j\,x_k}}\); this is the proportion of the total feasible inventory represented by the \(i\)th type. The squared distance is chosen primarily for tractability and we will also investigate the KL divergence (maximum entropy) as an alternate objective in a subsequent section. The squared distance has useful properties in that the statistics of mean squared error are well understood. Moreover, finding solutions to minimizing quadratics are possible using fast algorithms. It is desirable to weight the squared deviations by \(\frac{\sum _k {s_k^j x_k } }{s_i^j x_i }\), to make the solution independent of supply pool division,Footnote 5 and by \(V^{j}\) to reflect potentially differential campaign weighting. (In practice the weights depend on the total spend of the advertiser, the premium the advertiser offers over base-priced inventory, the length of the contract and the quality of the advertisements as viewed by users.) The campaign weights are \(V^{j}Y^{j}\) to reflect that a campaign of twice the size should be twice as important, or that merging two identical campaigns should not influence the solution. Thus the objective function is There are three kinds of constraints. The allocation must be feasible, meaning there is adequate inventory in each supply pool. Mathematically this requires Second, each campaign must meet its guarantee, requiring In addition, quantities are non-negative, creating a constraint Let \(p_{i}\) be the Lagrangian multiplier (shadow value) on the inventory constraint (2) and \(\alpha ^{j}\) be the multiplier on the campaign guarantee constraint (3). We will handle the non-negativity constraint manually. The Lagrangian becomes: We refer to the solution that minimizes (1) subject to (2)–(4) as the \(L^{2}\) maximally representative allocation.  There are campaign shadow values \(p^{*j}\) and inventory prices \(p_{i}\) such that the maximally representative allocation is given by: Theorem 1 shows that the demand system under maximally representative allocations is linear. Moreover, given the values \(p^{*j}\), there is a closed form for the inventory-specific prices. In particular, summing (5) over \(j\), If \(x_i >\sum _j{Max\left\{ {0,\quad \frac{Y^{j}}{V^{j}}\left( {\frac{x_i }{\sum _k{s_k^{j}x_k}}} \right) \left( {p^{*j}-{r_i }\left. /\right. {s_i^j }} \right) } \right\} }\), then \(p_{i}=r_{i}\), and otherwise \(p_{i}\) is determined by This equation solves for prices, given the list of \(p^{*j}\). The equation is piecewise linear with the number of pieces equal to the number of campaigns. Thus, a problem with potentially trillions of dimensions (pricing each supply pool) is reduced to a problem with hundreds of dimensions in practice by maximally representative allocations. The shadow prices are constrained to exceed an outside option \(r_{i}\). In essence, this solution “sells” some of the capacity in an outside market if the shadow value \(p_{i}\) would otherwise fall below \(r_{i}\). The shadow prices are denominated in dollars and thus are interpretable as actual prices to charge the buyer for that bucket of inventory. Note that \(V^{j}\) allows weighting campaign \(j\) differently from other campaigns. The value \(V^{j}\) measures how important a representative mix is for campaign \(j\). For higher values of \(V^{j}\), the importance of a representative mix is higher. The premium is beyond the minimum prices \(r_{i}\), that is, the customer paying higher values of \(\sum _{i} {(p_i -r_i )y_i^j } \) motivates a higher value of \(V^{j}\). The price of a given supply pool enters in the form \(p_i /s_i^j \) and thus “penalizes” supply pool \(i\) when the campaign can’t readily substitute; low values of \(s_i^{j}\) entail high effective prices. For example, a lipstick campaign that seeks only females might obtain inventory from a mixed gender group, but only count impressions that reach females. This counts showing impressions to the wrong group as having zero value, which is reasonable if the wrong group does not find the ads offensive.",15
17.0,2.0,Review of Economic Design,28 April 2013,https://link.springer.com/article/10.1007/s10058-013-0144-z,The Gates Hillman prediction market,June 2013,Abraham Othman,Tuomas Sandholm,,Male,Male,Unknown,Male,"Prediction markets are powerful tools for aggregating information (Berg et al. 2001; Wolfers and Zitzewitz 2004; Cowgill et al. 2009; Chen and Pennock 2010). A typical prediction market only generates a single point of interest; for instance, the probability that a certain candidate will win an election, or the percent of the vote that candidate will receive. Over more complex event spaces, however, these point estimates can be inappropriate. Consider a prediction market to estimate the US inflation rate over the next 5 years. Conceivably, market participants could be split between a very low estimate and a very high estimate. The resulting consensus of a middle value could be an accurate estimate of the expectation, but would be misleading to design policy around. Recent theoretical work has indicated that eliciting interesting distribution properties (like the element that has maximum probability) is as difficult as eliciting an entire distribution (Lambert et al. 2008). This result motivates and justifies building systems that are capable of eliciting complete probability distributions over large event spaces. In this paper, we discuss the design of a market, the Gates Hillman prediction market (GHPM) that generated a complete distribution over a fine-grained partition of possibilities while retaining the interactivity and simplicity of a traditional market. Fundamental to our design is an automated market maker (Hanson 2003; Chen and Pennock 2010) that offers three primary benefits. First, the market maker provides a rich form of liquidity because it guarantees that participants can make any self-selected trade at any time. Second, it allows instant pricing feedback to traders, rather than delayed, uncertain, potential feedback. A trader can always get actionable prices both on any potential trade she is considering and on the current values of the bets she currently holds. Third, the automated market maker obviates the need to match combinations of user-generated buy and sell orders—a problem that can be combinatorially complex (Fortnow et al. 2003; Chen et al. 2008)—making a large event space computationally feasible. Also important to the success of the GHPM was the user interface of the trading platform. Given well-documented shortcomings in human reasoning, interfaces with the same expressive power in theory can perform quite differently in practice. A large event space implies that the typical probability of any single event is small, and people have great difficulty discriminating between small probabilities (e.g., Ali 1977). To solve this problem, the GHPM used a span-based interface with ternary elicitation queries, which we discuss in Sect. 2.3. As the first test of automated market making in a large prediction market, the GHPM allowed us to discover two flaws in current automated market makers, which will help focus future design of market makers. Section 3 discusses the two flaws, spikiness and liquidity insensitivity, in detail and explores their theoretical roots. Traditional laboratory experiments are generally small due to practical constraints like subject payments, training effort, and the viable duration of an experiment. For example, Healy et al. (2010) study behavior and prices in laboratory prediction markets in detail, but their experiment only had three traders. The Gates Hillman Prediction Market involved hundreds of traders making thousands of trades, and so provides an unusually rich data set, particularly when combined with interviews with traders about the strategies they employed. Sections 4 and 5 use the data generated by the market to examine its performance and characteristics in depth.",5
17.0,2.0,Review of Economic Design,27 October 2012,https://link.springer.com/article/10.1007/s10058-012-0136-4,The “probability of a fit choice”,June 2013,Norman Schofield,,,Male,Unknown,Unknown,Male,"This paper attempts to model elections by incorporating voter judgments about candidate and leader competence. The proposed model can be linked to Madison’s understanding of the nature of the choice of Chief Magistrate (Madison 1999 [1787]). and Condorcet’s work on the so-called “Jury Theorem” (Condorcet 1994 [1785]). This aspect of Condorcet’s work has recently received renewed attention (McLennan 1998) and can be seen as a contribution to the development of a Madisonian conception of elections in representative democracies as methods of aggregation of both preferences and judgments. The Jury Theorem refers to a situation where each juror \(i\) can be assigned some probability \(p_{i}\) of selecting a “true’ option. The theorem asserts that a majority will select the true option with higher probability than any single juror, as long as the individual choices are independent. Moreover for a large electorate, this probability of choosing the true option will approach 1. Madison in Federalist X argued that a large electorate will exhibit a “high probabilty of a fit choice. “Clearly if we believe that the Jury Theorem is a valid model of social choice, then we have a justification for representative democracy, as a method to make wise choices. However, the literature on electoral competition has focused on voter preferences rather than judgments. Models of two-party competition have typically been based on the assumption that parties or candidates adopt positions in order to win, and has inferred that parties will converge to the electoral median, under deterministic voting in one dimension (Downs 1957) or to the electoral mean in stochastic models.Footnote 1 This median based one dimensional model has been applied recently by Acemoglu and Robinson (2000, 2006) in a wide ranging account of political economy, focussing on the transformation of the British polity to one that was more democratic and “inclusive”, in the nineteenth century. A standard technique in formal analysis of political economy is to demonstrate existence of a Nash equilibrium. All such equilibrium arguments using some variant of the Brouwer fixed point theorem. Section 2 of this paper considers a generalization of the fixed point theorem, namely the Ky Fan Theorem, and argues that the general equilibrium argument can be interpreted in terms of particular properties of a preference field, \(H,\) defined on the tangent space of the joint strategy space. If this field is continuous, in a certain well-defined sense, and “half open” then it will exhibit a singularity.Footnote 2 Such a singularity can be interpreted as a point that satisfies the first order condition for equilibrium. However, preference fields that are defined in terms of coalitions, under deterministic voting need not satisfy the half open open property and thus need not exhibit equilibrium. In the body of this paper I sketch out the Ky Fan Theorem and show the importance of the half open property. For social choice mechanisms based on coalitions under deterministic voting, it can be show that unless there is a collegium or oligarchy, or the dimension of the space is restricted in a particular fashion, then there need be no equilibrium. Earlier results by McKelvey (1976), Schofield (1978), McKelvey and Schofield (1987) and Saari (1997) suggest that voting models could be “chaotic” if the dimension of the space was at least two. As a result it would appear that voting will not satify the Madisonian characteristic of exhibiting a fit choice. Indeed equilibrium models of the market also depend on the half open property as, shown by Chichilnisky (1997b).The literature on the market discusses the nature of herd instinct, the way markets respond to speculative behavior and the power law that characterizes market price movements.Footnote 3 Some of these analyses are based on a version of the market equilibrium theorem. It is suggested that these models will fail the half open property and thus may not exhibit an equilibrium. Indeed they may also be chaotic. This paper suggests two different solutions to the possible non-existence of equilibrium. The first solution is to define a “general solution notion” called the heart. The heart is social choice correspondence from the space of preferences, into the space of alternatives We give examples of the heart correspondence to show how it depends on the symmetry of the distribution of preferences.The set so defined is always non empty and collapses to the equilibrium if the latter exists. The second method is to use a stochastic model of voting, rather than a model of deterministic voting. In the stochastic model each voter,\(i,\) is characterised by vector of probabilities {\(\rho _{ij}(\mathbf z)\}, \) where \(\mathbf z=( z_{1},z_{2},z_{j},\ldots ,z_{p})\) is the set of policy positions declared by the \(p\) candidates, and \(\rho _{ij}\) is the probability that \( i \) chooses candidate \(j.\) In this model we can include not only the candidate positions but also the perceptions or judgments of the voters about the qualities of the candidates. We can then simulate this model to determine the vote maximizing locations of candidates. Unlike the deterministic model of voting, we can show that this stochastic model will exhibit an equilibrium. In particular, the electoral origin will satisfy the first order condition for equilibrium.By analyzing the Hessian of each candidate’s vote share function, we can then determine whether the electoral origin does indeed satisfy the conditions for a local Nash equilibrium. If the appropriate Hessian conditions are satisfied then we can assert that the electoral origin is an attractor for the political game, In this case all vote maximizing candidates will converge to the electoral origin. We view this as a multidimensional analogue of the Downsian theorem. The stochastic model has two advantages over models of deterministic voting. Because the stochastic model is based on a multinomial model of voting, we can use survey models to estimate the effect of candidate personal qualities on voter perceptions and thus on voter judgments. We can do this by using survey data to estimate the electoral perceptions of candidate traits such as honest, moral etc.Secondly we can use Bayesian simulation techniques to determine how voters respond to economic shocks. Using simulation we can also estimate how political candidates are likely to adapt to changing electoral preferences. A recent literature has argued that activists and the resources they bring to political competition has contributed to political polarization.Footnote 4 In principal the stochastic model can be used to estimate the effect of money on the campaigns of political candidates. In turn this would allow us to validate Madison’s claim that the method of representative government would lead the “people to centre on men who possess the most attractive merit”. Since the electoral origin has the characterisic that it maximizes aggregrate voter utility we can interpret this policy position as a fit choice for the society. The estimate of the effect of money on actual outcomes, then indicates the undue influence of special interests on political choice.",1
17.0,2.0,Review of Economic Design,21 November 2012,https://link.springer.com/article/10.1007/s10058-012-0137-3,Partnership-enhancement and stability in matching problems,June 2013,Koichi Tadenuma,,,Male,Unknown,Unknown,Male,"Consider matchings between managers and workers, men and women, or professors and secretaries, and so on. Suppose that a matching is formed. Then, in the long run, preferences of agents will naturally change especially about their partners. For example, consider a firm that has several factories. The firm assigns workers to these factories. Each factory manager has preferences over the workers, while each worker has preferences over which factory he works at. Suppose that a worker is mathched to a factory manager. If the worker learns a skill that is specifically useful for production in the factory at which he is currently working, then the manager of the factory will rank him higher than before. On the other hand, if the manager makes an effort to get the working environment better, then the worker will rank the current manager higher than more of the other managers. In this paper, we focus on such “natural” changes of preferences of the agents as in the above factory example. We consider preference changes such that for each agent \(i\), her partner in the current matching is preferred to more of her potential partners than before, holding the relative rankings of the other possible partners fixed. We call such a transformation of preferences rank-enhancements of partners. This kind of transformations of preferences was introduced by Tadenuma (2011). We study how matchings should change with rank-enhancements of current partners. For that purpose, we define a matching rule to be a mapping that associates with each preference profile a matching, and formulate the following two desirable properties of matching rules in relation to rank-enhancements of partners. One property is concerning the agents whose ranks are enhanced in the preference orders of their initial partners, and the other is concerning “irrelevant” agents. First, notice that rank-enhancements of partners should be encouraged in the factory example because acquisition of factory-specific skills by the workers contributes to higher productivity of labor. Thus, if some agent is ranked higher in the preference order of his current partner, then he should not be punished at the new matching that is formed after the preference change. He should be matched to the same partner or to another agent whose rank is higher than the current one in his own preference order. We call such a property No Punishment for Rank-Enhanced Agents. This is an important property of matching rules in order to provide a proper incentive for agents to increase productivity or utilities of each pair. Second, we consider the solidarity principle, a fundamental principle in normative economics. Generally, it requires that, when some data in the problem (preferences, the amount of resources, and so on) change, all the agents in the same situation with respect to the change should be affected in the same direction: They should all be better off or they should all be worse off at the new allocation chosen by the rule. One form of this principle is solidarity under preference changes: When the preferences of some agents change, the agents whose preferences are fixed should be affected in the same direction. This version of the solidarity principle was first studied by Moulin (1987) in the context of quasi-linear binary social choice.Footnote 1 In our context of rank-enhancements of partners, however, there is another element that distinguishes the agents whose own preferences are unchanged into two types: one is the group of agents whose ranks become strictly higher in the preference orders of their current partners, and the other is the rest of the agents. As we have discussed above, the agents in the former group should not lose in the new matching after the change in preference profiles. On the other hand, the agents in the latter group are “irrelevant” agents in the sense that neither their own preferences nor their current partners’ preferences change. They may gain or lose in the new matching, but there is no reason to discriminate between the agents in the same group. Thus, we require that all these “irrelevant” agents should be affected in the same direction under rank-enhancements of partners. This property is called Solidarity under Rank-Enhancements of Partners. We examine compatibility of the above two properties with stability of matchings (Gale and Shapley 1962). A matching is stable if (i) no matched agent would rather be unmatched and (ii) there does not exist a pair of agents one from each group who would both prefer to be matched to each other than to whom they are matched. Stability is crucial for a matching to be formed and maintained by the agents. We say that a matching rule is a selection rule from stable matchings if it maps each preference profile into a stable matching for the profile. We study whether there exists a selection rules from stable matchings that satisfies No Punishment for Rank-Enhanced Agents or Solidarity under Rank-Enhancements of Partners. Unfortunately, neither of the two properties can be met with the requirement of stability. Then, we consider a further restricted class of preference changes: only one agent increases the rank of his original partner. We call such a change in preferences a single rank-enhancement of partner, and define weaker properties which apply only to this more restricted class of preference changes. It is shown that there exists a selection rule from stable matchings that satisfies No Punishment for Single Rank-Enhanced Agent. Moreover, any rule that selects from envy-minimizing stable matchings satisfies this property. Turning to the solidarity property, however, we show that Solidarity under Single Rank-Enhancement of Partner is still incompatible with stability. Tadenuma (2011) considered a somewhat stronger version of Solidarity under Single Rank-Enhancement of Partner than the current one, and showed that there exists no selection rule from envy-minimizing stable matchings that satisfies the stronger version of Solidarity. The present paper strengthens this result by showing that even without any conditions on equity of matchings, stability and the weaker version of Solidarity are incompatible. Our properties are logically related with Maskin Monotonicity (Maskin 1999), which is a necessary condition for a social choice correspondence to be implementable in Nash equilibria. It can be checked that in the present context of two-sided matchings, Maskin Monotonicity implies both No Punishment for Rank-Enhanced Agents and Solidarity under Rank-Enhancements of Partners, but the converse does not hold. Hence, as a corollary of our impossibility results, we can show that no single-valued selection rule from the stable matchings correspondence is Maskin monotonic even on the domain of “pure” matching problems in which being unmatched (or unemployed) is the worst situation for every agent (Tadenuma and Toda 1998). It should be noted that the proof of Tadenuma and Toda (1998) cannot be adapted for the theorems in the present paper because it uses transformations of preferences that are not rank-enhancements of partners. Kara and Sönmez (1996) showed that no proper subcorrespondence of the stable matchings correspondence is Maskin monotonic. But their proof relies crucially on the preference orders in which being unmatched is not necessarily worst for the agents. In fact, there exist proper, and not single-valued, subcorrespondences of the stable matchings correspondence that are Maskin monotonic on the domain of “pure” matching problems (Tadenuma and Toda 1998). Kojima and Manea (2010) introduced two properties related to Maskin Monotonicity, which they called Weak Maskin Monotonicity and Individually Rational (IR) Monotonicity, respectively, for the problem of assigning indivisible objects to agents. Because both No Punishment for Rank-Enhanced Agents and Solidarity under Rank-Enhancements of Partners are even weaker than these two properties, we obtain as corollaries of our theorems that no selection rule from the stable matchings correspondence is weakly Maskin monotonic, nor is IR monotonic, for the two-sided matching problems. These results should be contrasted with the results by Kojima and Manea (2010) for the objects assignment problems that characterized a class of selection rules from the stable assignments correspondence satisfying IR Monotonicity and Weak Maskin Monotonicity. The organization of the rest of this paper is as follows. The next section gives basic definitions and notation. Section 3 defines rank-enhancements of partners and introduces main properties of matching rules. Section 4 presents the impossibility and possibility results on matching rules satisfying No Punishment for Single Rank-Enhanced Agent with the requirement of stability. Section 5 shows incompatibility of Solidarity under Single Rank-Enhancement of Partner with requirement of stability. Section 6 clarifies the relation of our property with Maskin Monotonicity and related properties. The final section contains concluding remarks.",
17.0,3.0,Review of Economic Design,23 February 2013,https://link.springer.com/article/10.1007/s10058-013-0140-3,Relaxing IIA and the effect on individual power,September 2013,Donald E. Campbell,Jerry S. Kelly,,Male,Male,Unknown,Male,"X is a universal set of outcomes and N is the (finite) set of individuals. A weak ordering on X is a complete—hence reflexive—and transitive binary relation on X, and we let W(X) denote the set of all weak orderings on X. For \(\text{ R} \in \text{ W(X)}\) and \(\text{ Y} \subset \text{ X}\) let \(\text{ R}\vert \text{ Y}\) denote the relation \(\text{ R} \cap \text{ Y}\times \text{ Y}\) on Y, the restriction of R to Y. Let L(X) represent the set of all linear (or strong) orderings on X. (We say that \(\text{ R} \in \text{ W(X)}\) is linear if it is antisymmetric, which means that (x, y) and (y, x) cannot both belong to R unless x = y.) A profile p is a member of \(\text{ W(X)}^\mathrm{N}\), and it assigns the binary relation \(\text{ p(i)} \in \text{ W(X)}\) to individual i. A preference domain is a nonempty subset \(\mathcal P \) of \(\text{ W(X)}^\mathrm{N}\). Given a profile p, we let \(\text{ x} \succeq _{\mathrm{p(i)}} \text{ y}\) indicate that individual i either prefers x to y or is indifferent between x and y at profile p. Of course, \(\text{ x} \succ _\mathrm{p(i)} \text{ y}\) means that \(\text{ x} \succeq _\mathrm{p(i)} \text{ y}\) holds but \(\text{ y} \succeq _\mathrm{p(i)} \text{ x}\) does not. For each subset Y of X and each profile p in \(\mathcal P \) let \(\text{ p}\vert \text{ Y}\) denote the restriction of profile \(\text{ p} \in \mathcal P \) to Y. That is, \(\text{ p}\vert \text{ Y}\) represents the function \(\text{ q} \in \text{ W(Y)}^\mathrm{N}\) satisfying \(\text{ q(i)} = \text{ p(i)}\vert \text{ Y}\) for all \(\text{ i} \in \text{ N}\). A social welfare function for domain \(\mathcal P \) is a function f that specifies a binary relation f(p) on X for each \(\text{ p} \in \mathcal P \). If f assigns a binary relation f(p) to each \(\text{ p} \in \mathcal P \) (i.e., f is a social welfare function with domain \(\mathcal P \)) then \(\text{ x}\,\succeq _\mathrm{f(p)}\,\text{ y}\) means that x is weakly “preferred” to y in the social ordering f(p), and we write \(\text{ x} \succ _\mathrm{f(p)} \text{ y}\) if \(\text{ x} \succeq _\mathrm{f(p)} \text{ y}\) holds but \(\text{ y} \succeq _\mathrm{f(p)} \text{ x}\) does not. A given non-empty collection \(\Pi \) of non-empty subsets of X will be referred to as an agenda domain. There is no loss in generality in assuming that each member of \(\Pi \) has at least two members. A social choice function g on agenda domain \(\Pi \) and preference domain \(\mathcal P \) specifies a nonempty subset g(\(\uppi \),p) of \(\uppi \) for each \(\text{ p} \in \mathcal P \) and each \({\pi } \in {\Pi } \). We can view a social welfare function f as a special case of a social choice function, one for which \(\Pi \) is the collection of two-element subsets of X. We define g by writing \(\text{ g}(\{\text{ x,y}\},\text{ p}) = \{\text{ x}\}\) if \(\text{ x}\,\succ _\mathrm{f(p)}\,\text{ y}\), and \(\text{ g}(\{\text{ x,y}\},\text{ p}) = \{\text{ x,y}\}\) if \(\text{ x}\,\succeq _\mathrm{f(p)}\,\text{ y}\) and \(\text{ y}\,\succeq _\mathrm{f(p)} \text{ x}\) both hold. As usual, we say that the rule g satisfies the Pareto criterion if Pareto dominated alternatives never belong to the choice set. 
Pareto criterion
 For all \(\pi \in \Pi \) and \(\text{ p} \in \mathcal P \), if x \(\in \pi \) and \(\text{ x} \succ _\mathrm{p(i)} \text{ y}\) for all \(\text{ i} \in \text{ N}\) then \(\text{ y} \notin \text{ g}(\pi ,\text{ p})\). We do not employ independence of infeasible alternatives (IIF), but it will be used as a point of departure for our second theorem. We say that g satisfies IIF if for arbitrary \(\uppi \) the choice sets g(\(\uppi \),p) and g(\(\uppi \),q) are identical whenever p and q agree on \(\uppi \). 
Independence of infeasible alternatives (IIF) For all \(\pi \in \Pi \) and \(\text{ p},\,\text{ q} \in \mathcal P \), g(\(\uppi \),p) = g(\(\uppi \),q) if \(\text{ p}\vert \pi = \text{ q}\vert \pi \). We can think of IIA as a condition on social choice functions if we express it as follows: IIA: For all two-element subsets \(\uppi \) of \(\Pi \) we have g(\(\uppi \),p) = g(\(\uppi \),q) if \(\text{ p}\vert \pi = \text{ q}\vert \pi \). If g can be interpreted as a social welfare function then IIF is equivalent to IIA (independence of irrelevant alternatives). Our second theorem (unlike the first) assumes Arrow’s Choice Axiom, which is the choice function counterpart of transitivity. 
Arrow’s choice axiom (ACA) For all \(\pi ,\,\pi ^{\prime } \in \Pi \) and \(\text{ p} \in \mathcal P \), we have g(\(\pi ^{\prime }\),p) = g(\(\uppi \),p) \(\cap \,\pi ^{\prime }\) if \(\pi ^{\prime } \subset \pi \) and g(\(\uppi \),p) \(\cap \,\pi ^{\prime }\) is not empty. If \(\Pi \) is the set of all two-element and three-element subsets of X then g satisfies ACA if and only if for each \(\text{ p} \in \mathcal P \) there is a weak order \(\succeq \) on X such that, for all \(\pi \in \Pi ,\,\text{ g}(\pi ,\text{ p}) = \{ \text{ x} \in \pi : \text{ x} \succeq \text{ y}\) for all y \(\in \pi \)} (Arrow 1959). Therefore, if the output of a social welfare function is required to be a weak order we can view it as a social choice function on the agenda domain consisting of all two-element and three-element subsets of X, and impose ACA. If we wish to determine the social choice from some subset \(\uppi \) of X is it enough to employ information about individual preferences restricted to a subset Y of X? If so, we say that Y is sufficient for \(\uppi \). 
Sufficient set
 If g is a social choice function on domain \((\Pi , \mathcal P )\), and \(\uppi \) is a member of \(\Pi \), we say that the subset Y of X is sufficient for \(\uppi \) if for all p and q in \(\mathcal P \) such that \(\text{ p}\vert \text{ Y} = \text{ q}\vert \text{ Y}\) we have g(\(\uppi \),p) = g(\(\uppi \),q).  X is a finite set with at least three alternatives, and the domain is \(\text{ L(X)}^\mathrm{N}\). For every profile p in the domain let f(p) be the ranking on X generated by the Borda social welfare function. For every non-empty subset \(\uppi \) of X let \(\text{ g}_\mathrm{B}(\pi ,\text{ p}) = \{ \text{ x} \in \pi : \text{ x} \succeq _\mathrm{f(p)} \text{ y}\) for all \(\text{ y} \in \pi \}\). Clearly, X is the only sufficient set for f (or \(\text{ g}_\mathrm{B}\)). We can rectify this by using the local Borda rule: For each non-empty subset \(\uppi \) of X let f(\(\uppi \),p) be the ranking on \(\uppi \) generated by the Borda social welfare function when the set of alternatives employed to construct the Borda ranking is \(\uppi \). Let \(\text{ g}_\mathrm{L}(\pi ,\text{ p}) = \{ \text{ x} \in \pi : \text{ x} \succeq _{\mathrm{f}(\pi ,\mathrm{p})} \text{ y}\) for all y \(\in \pi \}\). Then \(\text{ g}_\mathrm{L}\) satisfies IIF and \(\uppi \) itself is sufficient for \(\uppi \). However, \(\text{ g}_\mathrm{L}\) does not satisfy ACA: Suppose that n = 3 and X = {x, y, z}. If p(1) = p(2) = (x, y, z) and p(3) = (y, z, x) then \(\text{ g}_\mathrm{L}\)({x, y, z}, p) = {x, y} but \(\text{ g}_\mathrm{L}\)({x, y}, p) = {x}. (Similarly, if \(\text{ g}_\mathrm{O}(\pi ,\text{ p}\)) is the set of Pareto optimal alternatives in \(\uppi \) at profile p then if p(1) = (x, y, z) and p(i) = (y, z, x) for all \(\text{ i} > 1\) we have \(\text{ g}_\mathrm{O}\)({x, y, z), p} = {x, y} but \(\text{ g}_\mathrm{O}\)({x, z}, p) = {x, z}, a violation of ACA.) \(\square \)
 In general there will be at least one sufficient set for \(\uppi \) because X itself is sufficient. There may be more that one sufficient set for a given member of \(\Pi \). In fact, any superset of a sufficient set is sufficient. However, the family of sufficient sets for a given \(\uppi \) may not contain a minimal member (by set inclusion), or there may be more than one minimal member. Therefore, it may be necessary to refer to the entire collection of sufficient sets. 
Information structure
 The information structure for social choice function g on domain (\(\Pi ,\mathcal P \)) is the function \(\Sigma _\mathrm{g}\) that for each \(\pi \in \Pi \) identifies the family \(\Sigma _\mathrm{g}(\pi )\) of sufficient sets for \(\uppi \). We will simply write \(\Sigma (\pi )\) if there is no danger of confusion. When we define a property that we might want the social choice process to satisfy, we will refer to the information structure \(\Sigma _\mathrm{g}\) for arbitrary social choice function g. But when we wish to illustrate a point with an example, we return to a familiar special case: The collection \(\Sigma _\mathrm{f}\) generated by a transitive-valued social welfare function f satisfying the Pareto criterion and non-dictatorship, and most often with preference domain \(\text{ L(X)}^\mathrm{N}\). We wish to be very general, but we also want our framework to include the classical Arrovian one and so the examples will be embedded in it. Given the social choice function g, if there is a unique minimal sufficient set Y (in terms of set inclusion) for a given member of the agenda domain \(\uppi \) then we refer to Y as the relevant set for \(\uppi \). When there is a relevant set for \(\uppi \) we denote it by \(\uppsi (\pi )\). 
Relevant set
 Given a social choice function g on domain \((\Pi , \mathcal P )\) we say that the subset Y of X is the relevant set for \(\uppi \), if Y is sufficient for \(\uppi \) and no proper subset of Y is sufficient for \(\uppi \), and if Z is sufficient for \(\uppi \) and no proper subset of Z is sufficient for \(\uppi \) then Z = Y. Given the social choice function g and arbitrary \(\uppi \) in the agenda domain for g let \(\upsigma (\pi )\) denote the intersection of the members of \(\Sigma _\mathrm{g}(\pi )\)—i.e., the intersection of the family of sets that are sufficient for \(\uppi \). If \(\upsigma (\pi )\) is itself sufficient for \(\uppi \) then \(\upsigma (\pi )\) is the relevant set for \(\uppi \). (In particular, \(\uppi \)
has a relevant set.) This claim is easy to prove: Clearly \(\upsigma (\pi )\) is a minimal sufficient set for \(\uppi \). If Y is sufficient for \(\uppi \) then \(\upsigma (\pi )\) is a subset of Y and thus there can be only one subset of X that is a minimal sufficient set for \(\uppi \). And if
\(\uppi \) has a relevant set and X is finite then \(\upsigma (\pi )\) is in fact the relevant set for \(\uppi \) as we now show: Suppose to the contrary that Y* is the relevant set for \(\uppi \) and Y* \(\ne \upsigma (\pi )\). Then there is a set Z that is sufficient for \(\uppi \) such that \(\text{ y*} \notin \text{ Z}\) for some \(\text{ y*} \in \text{ Y*}\). Because X is finite there exists a subset Z* of Z that is a minimal sufficient set for \(\uppi \). Because \(\text{ y*} \notin \text{ Z*}\) we have two distinct minimal sufficient sets, Y* and Z*. This contradicts the fact that \(\uppi \) has a relevant set. By definition, a relevant set cannot be a singleton. If {x} is sufficient for \(\uppi \) then so is the empty set—because no information is embodied in the restriction of a profile to a singleton set. When X is infinite there are well defined social welfare functions on \(\text{ L(X)}^\mathrm{N}\) for which no pair has a relevant set. (Example 2, p. 263, in Campbell and Kelly 2009.) If X is finite there are restricted domains that are consistent with social welfare functions that do not have relevant sets, as we now show.  X = {a, b, x, y} and \(\text{ n} \ge 3\). Any member of L(X) is a possible ordering for individual 1 or 2. But if p belongs to the domain then p(3) = (a, b, x, y) or p(3) = (x, y, b, a). For any profile p in the domain, if a ranks above x in p(3) then f(p) = p(1) and otherwise f(p) = p(2). Both {x, y, a} and {x, y, b} are sufficient for {x, y} but the intersection of the two sufficient sets is not sufficient. In other words, {x, y} does not have a relevant set for this social welfare function. \(\square \)
 Our next example can be viewed as a rule that incorporates a minimal departure from IIA. There is a given feasible alternative v such that {x, y, v} is the relevant set for every choice of x and y from X.  (gateau rule, from Campbell and Kelly 2000): Choose some distinguished alternative \(\text{ v} \in \mathrm{X}\) and for any profile \(\text{ p} \in \text{ L(X)}^\mathrm{N}\) let T(p) be the set of alternatives that defeat v by a majority and let \(\text{ B(p)} = \text{ X}\backslash [\text{ T(p)}\cup \{\text{ v}\}]\). Rank all of the pairs from T(p) according to p(1), person 1’s preference scheme at p, and rank all of the pairs from B(p) according to p(2). At profile p, every member of T(p) ranks above every member of {v} \(\cup \) B(p), and v ranks above every member of B(p). Therefore, social preference is transitive. The rule is non-dictatorial and satisfies the Pareto criterion. Note that {x, y, v} is the relevant set for every pair {x, y}. \(\square \)
 Note the difference between the information structure of Example 3 and that of Blau (1971), which requires {x, y, v} to be sufficient for {x, y} for every choice of \(\text{ v}\,\in \text{ X}\backslash \{\text{ x,} \text{ y}\}\). More generally, Blau assumes the existence of a positive integer \(\upbeta \ge 2\) such that every \(\upbeta \)-element superset of {x, y} is sufficient for {x, y}. He shows that this condition implies IIA on the domain \(\text{ L(X)}^\mathrm{N}\) or \(\text{ W(X)}^\mathrm{N}\). Obviously, Example 3 does not satisfy IIA. For instance, suppose that person 1 prefers x to y and person 2 prefers y to x. When everyone ranks both x and y above (resp., below) v then x will rank above (resp., below) y in the social ordering, whether or not every \(\text{ i} > 2\) has the same individual ordering of x and y in the two situations. We now introduce two restrictions on the sufficient sets. We do not assume that the Pareto criterion is satisfied, or that choice can be rationalized by an acyclic social preference relation, or that the domain belongs to a specific family. In fact, our first result does not refer to the social choice function itself. It takes as given a correspondence \(\Sigma \) which specifies for each member \(\uppi \) of \(\Pi \) a family of subsets \(\Sigma (\pi )\) of X. The result is employed in Theorem 2 by applying it to the information structure \(\Sigma _\mathrm{g}\) of a given social choice function g. If the social choice function does satisfy the Pareto criterion, and the preference domain contains \(\text{ L(X)}^\mathrm{N}\), then \(\uppi \) belongs to each of its sufficient sets. In other settings this will be assumed. 
Inclusion
 For all \(\pi \in \Pi \) and all Y \(\in \Sigma (\pi )\), we have \(\pi \subset \) Y. There are preference domains for which there exist social welfare functions that satisfy the Pareto criterion but not Inclusion, as the next example demonstrates.  X = {x, y, z}, \(\Pi \) = {{x, y}, {x, z}, {y, z}}, and \(\mathcal P = \{ \text{ p} \in \text{ L(X)}^\mathrm{N} : \text{ x}\,\succ _\mathrm{p(i)} \text{ y}\) for all \(\text{ i} \in \text{ N} \}\). Then the empty set is sufficient for {x, y} for any rule satisfying the Pareto criterion or IIA.  Although the empty set is sufficient for {x, y} for the preference domain of Example 4, the planner would not necessarily be able to determine that fact if the planner did not know the preference domain. However, to keep the scope of our investigation manageable we assume that the planner does know the preference domain, the agenda domain, and the feasible set, but does not know the current preference profile, and hence does not know how the individuals order the alternatives. Suppose that \(\pi = \{\text{ x}\}\, \cup \) A is believed to be the agenda and Y is sufficient for \(\uppi \). After individual preferences on Y have been obtained it is discovered that the actual agenda is \(\pi ^{\prime } = \{\text{ y}\}\, \cup \) A, which differs from \(\uppi \) only in that it includes y and excludes x. The idea behind our final property, Adaptability, is that if Y is large then little additional information is needed to determine the social choice from \(\pi ^{\prime }\). Specifically, it asserts that if Y includes every member of X but one, and that member does not belong to \(\uppi \), then Y is sufficient for \(\pi ^{\prime }\) if it is sufficient for \(\uppi \) and \(\pi ^{\prime }\) is a subset of Y. Before presenting the formal definition we introduce some related terminology. If A and B are subsets of X we say that B is obtained from A by replacement if we can create B from A by removing a member of A and substituting a member of \(\text{ X}\backslash \text{ A}\). And B is obtained from A by addition if we can create B by adding a member of \(\text{ X}\backslash \text{ A}\) to A. We say that A and B are adjacent if one of the sets can be obtained from the other by replacement or addition. 
Adaptability
 For any two adjacent agendas \(\uppi \) and \(\pi ^{\prime }\) in \(\Pi \) and any x \(\in \text{ X}\backslash [\pi \cup \pi ^{\prime }]\) the set \(\text{ X}\backslash \{\text{ x}\}\) is sufficient for \(\uppi \) if it is sufficient for \(\pi ^{\prime }\). Of course we would not want Adaptability to constrain a particular social choice process unless sufficient sets are the appropriate way to think about the costs of information processing in that context. At a minimum, the feasible set X would have to be large. The Borda social welfare function does not satisfy IIA, but it has an Adaptable information structure only because X is the relevant set for every pair. Our next example violates Adaptability while it treats individuals symmetrically.  X = {\(\text{ x}_{1},\,\text{ x}_{2},\,\text{ x}_{3},\,\text{ x}_{4}\)} and the preference domain of f is \(\text{ L(X)}^\mathrm{N}\). Set f(p) = p(1) unless \(\text{ x}_{1}\,\succ _{\mathrm{p}(1)}\,\text{ x}_{2}\,\succ _{\mathrm{p}(1)}\,\text{ x}_{3} \succ _{\mathrm{p}(1)}\,\text{ x}_{4}\,and\,\text{ x}_{4}\,\succ _{\mathrm{p}(2)}\,\text{ x}_{3}\,\succ _{\mathrm{p}(2)}\,\text{ x}_{2}\,\succ _{\mathrm{p}(2)}\,\text{ x}_{1}\) in which case \(\text{ x}_{4}\,\succ _\mathrm{f(p)}\,\text{ x}_{1}\,\succ _\mathrm{f(p)}\,\text{ x}_{2} \succ _\mathrm{f(p)}\,\text{ x}_{3}\). Then {\(\text{ x}_{1},\,\text{ x}_{2}\)} is the relevant set for \(\{\text{ x}_{1},\text{ x}_{2}\}\) and hence \(\{\text{ x}_{1},\,\text{ x}_{2},\,\text{ x}_{4}\}\) is sufficient for that pair. But \(\{\text{ x}_{1},\,\text{ x}_{2},\,\text{ x}_{4}\}\) is not sufficient for \(\{\text{ x}_{1},\,\text{ x}_{4}\}\) because in many cases we need to know the position of \(\text{ x}_{3}\) in the individual orderings to know whether \(\text{ x}_{1}\,\succ _\mathrm{f(p)}\,\text{ x}_{4}\), or \(\text{ x}_{4}\,\succ _\mathrm{f(p)}\,\text{ x}_{1}\). \(\square \)
 The proof of our first theorem depends on the agenda domain being connected, in the sense that arbitrary \(\uppi \) and \(\pi ^{\prime }\) can be connected by a finite sequence of members \(\Pi \) that starts at \(\uppi \) and concludes at \(\pi ^{\prime }\), and has the property that any member of the sequence is adjacent to the agenda that precedes it and to the agenda that follows it. We also require that no member of the sequence contain an alternative not belonging to \(\uppi \) or \(\pi ^{\prime }\). 
Connected agenda domain
 For any \(\pi ,\,\pi ^{\prime } \in \Pi \) there is a finite sequence \(\pi ^{1},\,\pi ^{2}, \ldots , \pi ^\mathrm{T}\) of members of \(\Pi \) such that \(\pi ^{1}=\pi ,\,\pi ^\mathrm{T}=\pi ^{\prime }\), and, for each \(\text{ t} < \text{ T}-1,\,\pi ^\mathrm{t}\) and \(\pi ^{\mathrm{t}+1}\) are adjacent and \(\text{ x} \notin \pi ^\mathrm{t}\) if \(\text{ x} \notin \pi \cup \pi ^{\prime }\). The requirement that x does not belong to any member of the sequence \(\pi ^{1},\,\pi ^{2},\ldots , \pi ^\mathrm{T}\) if x does not belong to \(\uppi \) or \(\pi ^{\prime }\) would be satisfied if one could replace the members of \(\uppi \), one at a time, with the members of \(\pi ^{\prime }\) without leaving the agenda domain. The set of all two-element subsets of X is a connected agenda domain, as is the set of all subsets of X with either two or three elements. The latter is an instance of the following type of connected agenda domain, which will play a role in our second theorem. 
Basic agenda domain
 There is a positive integer \(\ell > 1\) such that \(\Pi \) contains all subsets of cardinality \(\ell , \Pi \) contains no subset of X with less than \(\ell \) members, and if \(\Pi \) contains a subset of X with exactly \(\ell +1\) members then \(\Pi \) contains all subsets of X of cardinality \(\ell +1\). We say that \(\Pi \) is \({\varvec{\ell }}\)-based.",
17.0,3.0,Review of Economic Design,08 May 2013,https://link.springer.com/article/10.1007/s10058-013-0143-0,Information concentration in common value environments,September 2013,Vlad Mares,Mikhael Shor,,Male,Male,Unknown,Male,"Consider a seller of an indivisible item facing several potential buyers, each with some information about the object’s common value. How would a decision by a group of buyers to centralize their information and decision-making authority impact the seller’s revenue? While the question is of obvious interest to auctioneers considering allowing bidding syndicates and to policy governing mergers and collusion, these environments remain largely unmodeled. Providing a general result on the revenue impact of information concentration is the purpose of this manuscript. A growing literature is devoted to determining how access to more informative signals changes behavior in allocation problems (Matthews 1984; Persico 2000; Athey and Levin 2001; Bergemann and Valimaki 2002; Mares and Harstad 2003). Since these approaches generally define “more informative” in terms of some order over scalar random variables, they permit two buyers working together to possess anything from only a slightly more informative signal than each of them had individually to nearly perfect information about the object’s value. Another strand of literature finds that a more concentrated industry, obtained by removing a bidder and his information, is less profitable even when the auctioneer reacts with an optimal mechanism (Bulow and Klemperer 1996). However, this approach conflates the role of industry concentration with information concentration. In contrast, our approach keeps the total amount of information constant while concentrating its allocation among fewer bidders. The centralization of two buyers’ signals would simply have the joint entity with two signals. Since the new entity possesses a multidimensional signal, this calls into question the existence of equilibria in auctions (Jackson 2009) and of incentive-compatible mechanisms in general (Armstrong and Rochet 1999). Several authors have adopted models of bidding with multidimensional signals while imposing symmetry (Goeree and Offerman 2002; DeBrock and Smith 1983; Mares and Shor 2008). Yet, symmetric models are particularly ill-suited to modeling mergers or collusion as even an a priori symmetric industry will not be so following a merger. Footnote 1 Without symmetry, simple auction mechanisms need not be optimal. In this paper, we provide the auctioneer full strategic latitude in the choice of mechanisms. In private value auctions, Waehrer and Perry (2003) find that merger effects can be partially offset by strategically altering the reserve price. In our model, the object’s value is a function of \(n\) independent (but not necessarily identically distributed) signals, which are allocated among \( m\le n\) bidders. We adopt a mechanism design approach, allowing the seller to select an optimal mechanism for each allocation of signals among bidders. We require that each bidder’s vector of signals allows a scalar sufficient statistic, a condition satisfied by commonly analyzed models including additively separable and maximum or minimum value auctions (Mares and Shor 2008; Bikhchandani and Riley 1991; Bulow and Klemperer 2002; Krishna and Morgan 1997). Our central result is that a coarser partition of information among bidders always results in reduced revenue for the seller. This implies that all mergers reduce the auctioneer’s revenue, even those that make an industry more symmetric by aggregating smaller bidders. To approach the mechanism design problem in asymmetric, multidimensional signals contexts, we provide a result that significantly simplifies the problem. Whenever a scalar sufficient summary statistic exists for each player’s signals, every incentive-compatible mechanism has a revenue-equivalent incentive-compatible scalar mechanism that requires only scalar reports. We offer a constructive proof of this result in a general environment. This allows a broad class of problems to be analyzed in the Myerson (1981) framework and provides sufficient conditions for the existence of incentive-compatible mechanisms. For example, Biais et al. (2000) and Goeree and Offerman (2002) consider specific value functions for which two signals can be summarized with a scalar sufficient statistic. DeBrock and Smith (1983) employ conditional log-normal distributions of signals and use the geometric mean as a sufficient statistic of value. While the existence of scalar sufficient statistics is not a general property, it is reasonably broad to produce an interesting class of analytical examples. To illustrate, consider quasi-arithmetic means, for some increasing \(f\) and define the value function, \(V( \mathbf{x})=g\left( M_{f}(\mathbf{x})\right) \), as a function of signals \(\mathbf{x}\) for some increasing \(g\). When \(f(x)=x^{\alpha }\), we obtain the class of power means \(M_{x^\alpha }(\mathbf{x})=\left( \sum \frac{1}{n}x^\alpha _i\right) ^{1/\alpha }\) which encompasses the additive model and the interesting limit cases \(\lim _{\alpha \rightarrow \infty }M_{x^\alpha }(\mathbf{x})=\max (\mathbf{x}), \lim _{\alpha \rightarrow 0}M_{x^\alpha }(\mathbf{x})=\prod x_{i}^{1/n},\) and \( \lim _{\alpha \rightarrow -\infty }M_{x^\alpha }(\mathbf{x})=\min (\mathbf{x}).\) The minimum and maximum value functions have been studied by Mares and Harstad (2003) while mergers in the context of a symmetric additive model have been analyzed by Krishna and Morgan (1997) and Mares and Shor (2008). It is obvious that \(M_{f}(\mathbf{s})\) is a scalar sufficient statistic for any \(\mathbf{s}\), even for nonsymmetric value functions. It is also worth mentioning that while the existence of scalar sufficient statistics is broadly a sufficient condition for the existence of equilibria in certain auctions, it is by no means a necessary condition. However, generically, one can construct examples where the absence of scalar representations induces non-monotonicity of or even absence of equilibria (Jackson 2009; Reny and Zamir 2004). In what follows, we describe a model of common value environments that allows for arbitrary partitions of signals among bidders. We first present our result on the existence of optimal scalar mechanisms. This allows us to extend the definition of virtual valuations to these contexts. Then, we present our main result that coarser partitions of information in regular allocation problems decreases revenue. We demonstrate these results for a class of value functions and conclude with policy implications and suggestions for future research.",1
17.0,3.0,Review of Economic Design,21 July 2012,https://link.springer.com/article/10.1007/s10058-012-0130-x,The structure of decision schemes with cardinal preferences,September 2013,Shasikanta Nandeibam,,,Unknown,Unknown,Unknown,Unknown,,
17.0,3.0,Review of Economic Design,03 July 2013,https://link.springer.com/article/10.1007/s10058-013-0146-x,On the structure of voting systems between two alternatives,September 2013,Bonifacio Llamazares,,,Male,Unknown,Unknown,Male,"A classic problem that appears in several scientific fields consists in determining a collective preference from the preferences of a group of individuals on a set of alternatives. The simplest situation happens when the individuals have to choose between two alternatives. However, in spite of being the simplest case, it is not trivial and the analysis of voting systems (henceforth called Social Welfare Functions, SWFs) between two alternatives has generated an abundant literature among academics. One of the most outstanding works on SWFs between two alternatives was realized by Fishburn (1973). One of his results, Fishburn (1973, p. 56), can be formulated as a characterization of the anonymous, neutral and monotonic SWFs. From this result, it is possible to characterize these SWFs by means of certain increasing functions. Given that some of the SWFs most used in practice can be generated when the previous increasing functions are affine, the aim of this paper is to analyze the structure of the set formed by these functions. In this respect, in this paper we show that this set is convex and its extreme points are the functions that generate the following SWFs: simple majority, absolute majority, unanimous majority and Pareto majority. In addition to this, we show that the problem of selecting a SWF can be reduced to answering two simple questions: How many votes need an alternative for winning when the other alternative receives no vote? How many votes must an alternative gather for each vote obtained by the other alternative? The paper is organized as follows. Section 2 provides the basic concepts and notation on SWFs. Moreover, the characterization result given by Fishburn is recalled. In Sect. 3 we give the main results of the paper. We conclude in Sect. 4.",3
17.0,4.0,Review of Economic Design,09 September 2012,https://link.springer.com/article/10.1007/s10058-012-0132-8,Stable and efficient coalitional networks,December 2013,Jean-François Caulier,Ana Mauleon,Vincent Vannetelbosch,Unknown,Female,Male,Mix,,
17.0,4.0,Review of Economic Design,28 April 2013,https://link.springer.com/article/10.1007/s10058-013-0142-1,Multiple and last-minute bidding in competing internet auctions,December 2013,Marta Stryszowska,,,Female,Unknown,Unknown,Female,"eBay allows a single buyer to participate in multiple auctions selling identical goods. Given that every single auction lasts typically several days, a bidder could always submit a bid in one auction, revise it and later bid in a different auction. In such a setting, two patterns are commonly observed: multiple bidding and last-minute bidding (see Bajari and Hortacsu 2003; Roth and Ockenfels 2002, 2006). Multiple bidding takes place when a single bidder submits multiple bids in a single auction. Last-minute bidding consists of sending a bid just before the very end of the auction. Most of the existing literature on Internet auctions neglected the fact that a bidder could buy the same good in several Internet auctions and focused on the optimal bidding behavior in a single-unit setting.Footnote 1
Peters and Severinov (2006) analyzed competing Internet auctions, but took into account a different ending rule than the one used on eBay.Footnote 2
 The present paper proposes a theoretical model of competing Internet auctions with the ending rule that is actually used on eBay. The equilibrium behavior involves last-minute biddingFootnote 3 and multiple bidding, the two patterns commonly observed on eBay. Equilibrium last-minute bidding is shown to lead to potential inefficient outcomes caused by the uncertain bid transmission in the last minute of an Internet auction.Footnote 4
 The paper is structured as follows. The next section introduces the model. Section 3 presents an efficient equilibrium with multiple bidding. Section 4 demonstrates an inefficient equilibrium with last-minute bidding. Section 5 indicates possible impacts of last-minute bidding by comparing the identified inefficient equilibrium with last-minute bidding to an efficient equilibrium. Section 6 discusses the implications for the market design based on the comparison of these two equilibria. Finally, Sect. 5 concludes.",2
17.0,4.0,Review of Economic Design,17 July 2013,https://link.springer.com/article/10.1007/s10058-013-0147-9,Keyword auctions with budget-constrained bidders,December 2013,Youngwoo Koh,,,Unknown,Unknown,Unknown,Unknown,,
17.0,4.0,Review of Economic Design,19 September 2013,https://link.springer.com/article/10.1007/s10058-013-0149-7,Strategy-proofness on restricted separable domains,December 2013,Ricardo Martínez,Bernardo Moreno,,Male,Male,Unknown,Male,"Situations in which societies have to decide whether to accept or reject alternatives often occur. For example, a club may have to decide which candidates will become new members. Each candidate can be accepted or rejected separately. Another example is a collection of bills submitted for consideration by legislators in a parliament. Each single bill can be passed or not. We find similar problems in the allocation of public projects and the location of facilities in districts of a city. Considering such voting situations in a more formal way, a set of voters
\(N\) has to choose one or several alternatives among a set of candidates or objects \(K\). Voter preferences on the combinations of winning candidates are represented by orderings. A social choice function (SCF) is a mapping that, taking the preferences of all voters as inputs, results in a set of elected candidates. The attractiveness of SCFs comes from the properties they may satisfy. These properties usually refer to different SCF behaviors. The second key aspect is the domain of preferences for which an SCF is defined. In this paper we focus on the latter issue. We introduce a family of subdomains and characterize the SCFs for these subdomains that fulfill two of the most analyzed axioms in the literature. Among the several properties an SCF may satisfy, two have been intensively studied: strategy-proofness and tops-onlyness. According to the first, none of the voters can be better off by misrepresenting their preferences. According to the second property, all the information the SCF uses to compute the elected candidates is contained in the tops of the preferences. Gibbard (1973) and Satterthwaite (1975) showed that if the domain is the universal domain, then the only SCFs that are strategy-proof are the dictatorial SCFs. These results have motivated a wide body of research to delimit the set of preferences that escape this impossibility result. For example, Moulin (1980) provides characterization results for the domain of single-peaked preferences. Border and Jordan (1983) restrict their analysis to quadratic preferences in a multi-dimensional setting. Alternatively, Barberà et al. (1991) consider a different type of domain: separable preferences (the relative position of two candidates in the ordering is not affected by the position of a third candidate). These authors show that the only SCFs that satisfy strategy-proofness and tops-onlyness for separable preferences are voting-by-committee SCFs. To further this line of research, we restrict our analysis to the domain of separable preferences. In fact, we consider a family of restrictions. Each voter \(i \in N\) has a type
\(V_{i}\), which consists of a partition of the set of candidates into friends, enemies, and unbiased candidates.Footnote 1 The agents’ types are common knowledge. Associated with each partition \(V_{i}\), we have a subdomain of separable preferences, which we call the \(V_{i}\)-domain. Friends are those candidates who \(i\) always wants to be elected, and therefore will be above the empty set in any preference in the \(V_{i}\)-domain. Enemies are those candidates who \(i\) never wants to be elected, and therefore will be below the empty set in any preference in the \(V_{i}\)-domain. Unbiased candidates are the remaining candidates, and there is no information on what \(i\) thinks about them. They can be above or below the empty set in the preference relation. Note that different \(V_{i}\) types lead to different \(V_{i}\)-domains, and hence we have a family of subdomains of separable preferences. In Sect. 2, we show that the structure of these subdomains can be relevant in several concrete situations. As a special case, if \(V_{i}\) is such that all candidates are unbiased for \(i\), then the \(V_{i}\)-domain coincides with the whole domain of separable preferences. Our main goal is to characterize the SCFs that are strategy-proof and tops-only for each possible \(V_{i}\)-domain. These SCFs are the voting-by-committee SCFs introduced by Barberà et al. (1991). The remainder of the paper is structured as follows. In Sect. 2, we describe the basic model, key axioms and fundamental results in the literature. We also introduce our family of restricted separable domains and their potential for application to different frameworks. Section 3 presents our main results. In Sect. 4, we conclude with some final comments and suggest further research.",
18.0,1.0,Review of Economic Design,19 January 2014,https://link.springer.com/article/10.1007/s10058-014-0157-2,An introduction to Allan Gibbard’s oligarchy theorem paper,March 2014,John A. Weymark,,,Male,Unknown,Unknown,Male,,4
18.0,1.0,Review of Economic Design,04 February 2014,https://link.springer.com/article/10.1007/s10058-014-0158-1,Intransitive social indifference and the Arrow dilemma,March 2014,Allan F. Gibbard,,,Male,Unknown,Unknown,Male,,11
18.0,1.0,Review of Economic Design,16 November 2013,https://link.springer.com/article/10.1007/s10058-013-0152-z,Ordering sellers in sequential auctions,March 2014,Qiang Gong,Xu Tan,Yiqing Xing,,,Unknown,Mix,,
18.0,1.0,Review of Economic Design,24 October 2013,https://link.springer.com/article/10.1007/s10058-013-0153-y,Workup,March 2014,Romans Pancs,,,Male,Unknown,Unknown,Male,"In financial markets, a limit order is a trader’s public commitment to buy or sell a specified quantity at a specified price or better. One problem with the pure limit-order markets is that the use of large limit orders is discouraged by potential front-runners. A front-runner is a trader who, having observed a posted limit order to sell a large quantity, anticipates the price impact of the impending sale, sells a certain amount ahead of that sale, and then, after the price impact of the impending sale has been realized, buys back his sold amount, at depressed prices, thereby realizing a profit. One can mitigate the front-running problem by using iceberg orders (which are as limit orders except that some of the quantity remains invisible to other traders until the initially visible quantity has been executed) or expandable orders (which are as limit orders except that the initially specified quantity can be increased further after the buyer and the seller have been matched).Footnote 1 This paper provides a theoretical model of expandable orders and the accompanying quantity negotiation, called the workup. The model addresses the following questions: What determines traders’ strategies during the workup? When can expandable orders be replaced by iceberg orders? These questions are not addressed in the existing literature, which is mostly empirical. Empirically, workups have been studied by Huang et al. (2002), Boni and Leach (2004), and Dungey et al. (2009). Theoretically, iceberg orders have been studied by Anand and Weaver (2004), Esser and Mönch (2007), Aitken et al. (2001), Buti and Rindi (2009), and Moinas (2010), but these orders’ normative relationship to the workup has not been explored. The posed questions are important because the workup is ubiquitous. It is used in the U.S. and Canadian government bond markets and in over-the-counter markets.Footnote 2 Most trades in the U.S. government bond market occur in exchanges called electronic communication networks (ECNs), such as eSpeed and BrokerTec.Footnote 3 These ECNs have evolved in response to traders’ demand for anonymity and for greater privacy of their intended trades.Footnote 4 The orders are expanded often; workups leading to larger than initially specified quantities account for two thirds of the dollar volume of trade in these ECNs (Dungey et al. 2009). In over-the-counter markets, workups are often used by brokers, who assist in negotiations between the counterparties whose identities are not revealed to each other. The theoretical properties of the workup are studied in a bilateral-trade model, introduced in Sect. 2. In the model, a seller and a buyer exchange an asset for money. The per-unit transaction price is fixed. The negotiation is only about the quantity. During the workup, traders alternately increase the desired quantity by a given amount until either trader vetoes a further increase. Then the agreed-upon amount is traded.Footnote 5 Asymmetric information is of two kinds. The seller’s private information, or type, is his desired trade size, called block. The buyer’s type is whether he can front-run; if he can, he is called a front-runner; otherwise, he is called a dealer.Footnote 6
 By assumption, the dealer wishes to buy as much as he can. The front-runner does not; he does not value the asset. If the front-runner accepts quantity increases during the workup, he does so only to learn something about the seller’s desired trade. This information helps the front-runner decide whether he will profit if he front-runs. Front-running can be profitable because of the assumption that if the seller fails to sell his desired quantity within the workup, he will sell the remaining quantity outside the workup, thereby causing a price impact. The front-runner exploits this price impact by selling high (before the price impact) and buying low (after the price impact). Front-running can be unprofitable due to the fixed cost it entails. The seller prefers not being front-run, because selling outside the workup entails both a price impact and a fixed cost. Traders’ strategies in the workup are studied in Sect. 3, which solves for an equilibrium of the workup game. The focus is on a particular equilibrium (described in Theorem 1), which minimizes the front-runner’s expected payoff (as shown in Theorem 2). In this equilibrium, called the gradual-disclosure equilibrium, the negotiated quantity increases at each step of the workup by the same, minimal feasible increment. These minimal increments ensure that as many seller’s types as possible take the same action for as long as possible, thereby impairing the front-runner’s inference about the seller’s block. This impaired inference reduces the front-runner’s expected payoff, which could be justified as a desideratum in a richer model with participation costs, in which the payoff reduction might discourage the front-runner from joining the exchange, and so from incurring the socially wasteful cost of front-running and from forcing the seller to incur the socially wasteful cost of selling outside the workup. The seller’s preference for offering the smallest quantity increment at every round relies on specific equilibrium beliefs. This preference is driven by the front-runner’s belief that any deviation from the equilibrium strategy comes from the seller with the largest possible block, whom it is profitable to front-run. One particular deviation is the seller’s offer of his entire block at the first round. This deviation is equivalent to the submission of a limit sell order. The seller’s unwillingness to disclose his block at the first round is thus equivalent to unwillingness to submit a limit order for his entire block, in the presence of expandable orders.Footnote 7
\(^{,}\)
Footnote 8
 At the analyzed gradual-disclosure equilibrium, the front-runner also optimally offers the smallest possible increment at each round. He does so in order to mimic (and thus be mistaken for) the dealer and thereby keep acquiring information about the seller’s block. Theorem 3 derives the model’s testable implications by studying the dependence of the front-runner’s equilibrium strategy on the model’s parameters. For instance, as long as the front-runner participates in the workup, he stays in it for longer if front-running is costlier, if the amount he can sell when front-running is less, or if the price impact from sale is smaller. In all these cases, front-running is less profitable, and so the front-runner needs more information before deciding whether to front-run. He acquires this additional information by staying in the workup for longer to ascertain that the seller’s block is sufficiently large. Section 3 concludes by addressing a potential problem with the workup. The workup’s gradual-disclosure equilibrium (and indeed, any equilibrium) relies on the exchange’s ability to coordinate traders on specific off-equilibrium-path beliefs. Theorem 4 introduces an alternative mechanism, dubbed the button mechanism, that constrains traders to propose the smallest possible increments at every round. As a result, the workup’s gradual-disclosure equilibrium outcome is the button mechanism’s only equilibrium outcome; the exchange is no longer required to be able to coordinate traders. Section 4 explores the relationship between the workup and iceberg orders. The goal is to understand when the workup can be supplanted by iceberg orders. The approach is to use mechanism design to find the best trading mechanism, to show that this mechanism can be implemented using iceberg orders, and to observe that the assumption of the exchange’s commitment not to abuse the confidential information contained in the submitted iceberg orders is crucial for traders’ willingness to submit these orders. The workup does not rely on such commitment, and therefore may prevail in some environments. The results in Sects. 3.4 and 4 can also be interpreted normatively. In particular, if the exchange’s goal is indeed to minimize front-runners’ payoffs, the button mechanism is superior to the regular workup. By contrast, if the exchange’s goal is to maximize the sum of the traders’ payoffs, the exchange may wish to retain the regular workup and leave it to the traders to coordinate on the appropriate equilibrium. Furthermore, Sect. 4 suggests that if the exchange can commit not to abuse the private information contained in the traders’ submitted orders, then the workup can be improved upon by a static trading mechanism (Theorems 5 and 6). For the exchange to design this mechanism optimally and for traders to behave in it optimally, however, all must agree on the economic environment’s fine details. If these details are unavailable, the exchange may prefer to opt for the workup. 
Related literature Gradual disclosure in the workup is related to games of timing. When the workup has a gradual-disclosure equilibrium, the reduced-form workup game is a “simple timing game”, as discussed by Fudenberg and Tirole (1991, Section 4.5). In the workup game, the agreed-upon quantity is the counterpart of time in a timing game; a trader chooses a threshold quantity after which he rejects further increases. The payoff structure resembles a preemption game (Harris and Vickers 1985), a special case of the simple timing game. Because each trader has private information that can affect the other trader’s payoff, the workup resembles bargaining with correlated values, first studied by Evans (1989) and Vincent (1989). In the workup, however, traders negotiate about quantity, not price. Moreover, the interdependence of values emerges indirectly, through traders’ actions and outside options, not directly, through their valuations for the asset. Finally, both traders are privately informed. Some features of the equilibrium play in the workup resemble bargaining with a commitment type. Myerson (1997, Section 8.8, Theorem 8.4) analyzes a bargaining model in which the commitment type accepts any offer that is at least as good as a certain threshold offer. The commitment type is mimicked by the “regular” type, who thereby secures a payoff that is close to what the commitment type demands. (Abreu and Gul (2000), derive a related result in a model with multiple commitment types). In the workup, the dealer can be interpreted as a commitment type, whom the front-runner (the regular type) wishes to mimic in order to continue acquiring information about the seller’s block. A workup effectively “sells” to the front-runner information about the seller’s trading intention. The front-runner’s “payment” is his loss from liquidating the asset, which he does not value.  Admati and Pfleiderer (1990) study the sale of information in a rational-expectations equilibrium, whereas Hörner and Skrzypacz (2009) do so in a game-theoretic framework. The economic environment, the designer’s objectives, and the class of admissible mechanisms differentiate the present paper from Hörner and Skrzypacz (2009). Both papers find, however, that, in equilibrium, private information is released gradually. The workup is a mechanism that caters to traders with multi-unit demands. Compared to auctions with multi-unit demands (pioneered by Wilson 1979), the workup abstracts from price determination and has a special payoff structure motivated by the study of front-running. Kremer and Nyborg (2004) study complete-information common-value uniform-price auctions for divisible quantities. In their auctions, each bidder can submit only finitely many bids, each of which must specify a marginal quantity proportional to a given quantity multiple and a price proportional to a given tick. The seller’s revenue increases as the tick size decreases, because competing by undercutting each other becomes cheaper for bidders. By contrast, altering the minimal quantity increment in the workup would have no competitive ramifications, because each party to the workup faces no competition on the same side of the market. Instead, reducing the minimal quantity increment reduces the amount of information that the front-runner can glean at each round of the workup, thereby tending to reduce the front-runner’s equilibrium payoff. The model of the workup assumes minimal quantity increments but does not explain them. By contrast, Kastl (2012) proposes a model in which the discreteness of bids arises endogenously. He studies private-value uniform-price and discriminatory auctions for divisible goods, and shows that if specifying additional price-quantity pairs is costly for a bidder, the bidder restricts his bids to only a few pairs, and that the loss from this restriction is rather small. The critical feature of the workup model is the threat of front-running. Front-running is ubiquitous in practice and is of concern to both individual traders and regulators, as emphasized by Harris (1997). Front-running often emerges in models of dual trading, in which a broker-dealer observes his client’s order before trading on both his own account and on his client’s account—that is, in a dual capacity (Grossman 1989; Röell 1990; Fishman and Longstaff 1992; Pagano and Röell 1993; Danthine and Moresi 1998; Bernhardt and Taub 2008). Front-running may also emerge when a trader observes an announcement of a so-called “sunshine” trader, who broadcasts his trading intention before submitting a request to trade (Admati and Pfleiderer 1991). In either case, a trader who observes another’s trading intention and can trade first (as in the models of Fishman and Longstaff 1992; Pagano and Röell 1993; Brunnermeier and Pedersen 2005; Bernhardt and Taub 2008) may profitably front-run—in the same manner as the front-runner does in the present paper. By contrast, if all trade simultaneously, front-running is infeasible, and the knowledge of another’s trading intention may even be profitless (Rochet and Vila 1994). The closest empirical study is Boni and Leach (2004). They analyze the GovPX platform, which features expandable orders, and corroborate the rationale for the use of expandable orders that is proposed by the workup model. In their own words, they “document evidence consistent with the hypothesis that traders use expandable limit order strategies to reduce costs associated with stale limit orders and information leakage”—for instance, many trades exceed the initially quoted quantities—with the caveat that “expandable limit orders do not completely eliminate pre-trade information leakage or the potential for information free-riding”. Consistent with this evidence, the workup’s gradual-disclosure equilibrium reduces the information leakage about the seller’s block by initially concealing its size, but does not eliminate this leakage, which enables the front-runner to profit from the information learned during the workup. At odds with the evidence, however, the gradual-disclosure equilibrium has the largest feasible number of rounds, whereas Boni and Leach (2004) document just two rounds on average. This discrepancy may be specific to the GovPX’s historic data, which are based on mediation by human brokers, who are presumably slower than the algorithms in present-day ECNs.",5
18.0,1.0,Review of Economic Design,16 October 2013,https://link.springer.com/article/10.1007/s10058-013-0151-0,An equivalence of secure implementability and full implementability in truthful strategies in pure exchange economies with Leontief utility functions,March 2014,Katsuhiko Nishizaki,,,Male,Unknown,Unknown,Male,"This paper studies strategy-proof social choice functions in pure exchange economies with Leontief utility functions, where there are \(n \ge 2\) agents and \(m \ge 2\) divisible goods. Computer science literature assumes that each agent has a Leontief utility function.Footnote 1 For example, we can consider the problem of allocating such resources as central processing units, memory, and input/output resources in cloud computing systems, where each agent’s demand for the resources changes in a fixed proportion. Such practical concerns prompted this study. In pure exchange economies where there are two agents and two divisible goods, Hurwicz (1972) showed that there is no social choice function that satisfies strategy-proofness, Pareto-efficiency, and individual rationality on classical domains. After his seminal work, such negative results have been established on specific domains. The only exceptions are Leontief domains; Nicolò (2004) and Li and Xue (2012) showed that certain social choice functions do satisfy the above properties on such domains. Strategy-proof mechanisms might not work well because of the existence of Nash equilibria that induce non-optimal outcomes. This problem is solved by secure implementation (Saijo et al. 2007) which is defined as double implementation in dominant strategy equilibria and Nash equilibria.Footnote 2
Saijo et al. (2007) showed that the rectangular property (Saijo et al. 2007) that is necessary for secure implementability is generally stronger than strong non-bossiness (Ritz 1983) that is necessary for full implementability in truthful strategies (Nicolò 2004). Footnote 3 This paper shows that both properties are equivalent under the social choice function that satisfies strategy-proofness and non-wastefulness (Li and Xue 2012) in pure exchange economies with Leontief utility functions. This implies that secure implementability and full implementability in truthful strategies are equivalent under the non-wasteful social choice function in the economies. In conjunction with Li and Xue ’s (2012) results, this paper demonstrates the existence of securely implementable social choice functions with desirable properties in the economies although previous literature illustrated the difficulties in finding them.Footnote 4
 The remainder of this paper is organized as follows. Section 2 introduces the model and Sect. 3 the properties of social choice functions. Section 4 provides some preliminary results and Sect. 5 introduces the main result. Section 6 discusses the characterization of securely implementable social choice functions in two agents and two goods economies and Sect. 7 the relationship between the main result and “generalized” Leontief utility functions.",2
18.0,2.0,Review of Economic Design,21 November 2013,https://link.springer.com/article/10.1007/s10058-013-0155-9,Truth-telling and trust in sender–receiver games with intervention: an experimental study,June 2014,Mehmet Y. Gurdal,Ayca Ozdogan,Ismail Saglam,Male,Unknown,Male,Male,"In their seminal work on strategic information transmission, Crawford and Sobel (1982) showed that as the interests of an informed (the sender) and an uniformed (the receiver) individual become more aligned, more information is transmitted. While this prediction was supported by Dickhaut et al. (1995), Cai and Wang (2006) later showed that senders are more truthful and receivers are more trustful than what the theory predicts. Similar findings of excessive truth-telling are also present in a strand of experimental literature on sender–receiver games, involving the works of Gneezy (2005), Sánchez-Pagés and Vorsatz (2007); Sánchez-Pagés and Vorsatz (2009), Peeters et al. (2008) and Sutter (2009) among others. In this paper, we aim to experimentally study the robustness of excessive truth-telling phenomenon with respect to the random intervention of a truthful regulator in situations where the transfer of strategic information is under some degree of control. This modified sender–receiver game with the random intervention of a regulator is equivalent to a “behavioral game” in which the sender can be of either a strategic (standard rational) type or a behavioral (honest) type, with the probability distribution over the types being common knowledge. Sender–receiver games with behavioral types have been well studied in the theoretical literature. For example, Benabou and Laroque (1992) showed that in a repeated sender–receiver game where the state and action spaces are discrete, some senders are honest and the information of senders is noisy, a strategic sender may truthfully submit his private information for long periods of time in order to build a reputation for honesty and to manipulate a rational receiver afterwards. However, in a single-shot play of the same game, there can be no transmission of information unless the probability that the sender is honest is greater than 1/2. This behavioral game was extended by Ottaviani and Squintani (2002)—using a more general setup of Crawford and Sobel (1982) and also allowing the possibility of naive receivers (believers)—to show that an equilibrium with fully-revealing communication may arise (though in an inflated language) irrespective of the likelihood of behavioral players as long as this likelihood is positive. In such an equilibrium, the naive receiver trusts the sender whereas the strategic (sophisticated) receiver corrects the message of the sender to account for the inflation in the language. In a related work, Crawford (2003) considered an asymmetric matching-pennies game with behavioral (mortal or naive) and sophisticated players to study transmission of information about intended play. Depending on the relative likelihood of behavioral and sophisticated players, this game admits equilibria in which sophisticated senders exploit naive receivers as well as equilibria in which no player is exploited. The results of Crawford (2003) were recently extended by Landi and Colucci (2008) to a behavioral sender–receiver game, focusing on transmission of private exogenous information. It is natural to predict that the addition of intervention (or the possibility of behavioral type of senders) to a typical sender–receiver game will induce an increased level of trust among player subjects who are on the receiving side. Yet, one may also expect that strategic senders may exploit this regulated situation if they adjust their actions based on the increased trust levels, and this would oppositely lead to a fall in the trust of receivers. Thus, it is not clear per se how the overall frequencies of trust and truth-telling will be affected in the actual plays of sender–receiver games where a regulatory authority occasionally intervenes forcing the submitted messages to be truthful (or when some senders behave non-strategically). We aim to answer this question by conducting experiments for two sender–receiver games. Our Benchmark Game corresponding to the case of no intervention is identical to the sender–receiver games in Sánchez-Pagés and Vorsatz (2007) and Peeters et al. (2008). In particular, the sender observes Nature’s realization of a payoff table that could be of two equally likely types, over which the sender and the receiver have opposing interests. Each table involves two outcomes corresponding to two actions of a receiver. After Nature’s choice of a table type, the sender submits a message, consisting of the type of the actual payoff table, to the receiver who is entirely uninformed about Nature’s choice. Because of this informational asymmetry, the sender can choose to lie whenever she finds it optimal. After observing the message of the sender, the receiver takes an action by trusting or distrusting the sender, and consequently the payoffs of the two players are determined by the actual state chosen by Nature and the action taken by the receiver. In the alternative game, namely the Regulated Game, the sequence of actions are the same as in the Benchmark Game, yet there is now a regulator which truthfully submits to the receiver Nature’s choice of payoff table with commonly known probability \(\alpha \in (0,1/2)\).Footnote 1 Thus, a message about Nature’s choice can be submitted by a strategic sender only with probability \(1-\alpha \in (1/2,1)\). On the other hand, the receiver only observes the message as in the Benchmark Game and is unaware whether an intervention occurred or not. A real life example for the games we consider can be the conflict between a taxi driver and a customer. Suppose there are two alternatives routes to a given destination and depending on traffic conditions, one route takes shorter than the other. (In particular, Route A takes shorter in State A and Route B takes shorter in State B). The driver, who knows the state of the traffic, would prefer the route that will take a longer time, whereas the customer would prefer the other one.Footnote 2 Since the material interests of the driver and the customer are in conflict, all communication by the driver would be inferred as cheap-talk when individuals are rational and this corresponds to our Benchmark Game. On the other hand, when a certain fraction of drivers face high enough cost of lying in this situation, this will correspond to our Regulated (or Behavioral) Game. Another example can be the conflict between the government and a local health official (on fixed salary) who is responsible for screening citizens and applying certain treatments to those who show signs of a particular disease during the screening. If the area is known to have a low number of potential patients, then the government would prefer a low number of patients to be screened and vice versa when the number of potential patients is high. The official, who is aware of the local conditions and reports these conditions to the government, has opposite preferences since he would prefer to screen patients who are more likely to turn out to be healthy and hence do not require further treatment. This case is similar to our Benchmark Game whereas an extension with a certain probability of intervention (as random checks on the local official) by an independent inspector of the government is similar to the Regulated Game.Footnote 3
 Behavior predicted in all sequential equilibria of both the Benchmark and the Regulated Game implies that receivers never receive any relevant information. In the Benchmark Game the sender achieves this by submitting an untruthful message with probability one-half (due to the symmetric construction of the constant-sum payoff tables with respect to players and actions). In the Regulated Game, a strategic sender can submit message only with probability \(1-\alpha \); therefore, she can achieve the non-informativeness of the message that the receiver will observe, by lying with probability \(0.5/(1-\alpha )\) whenever she submits any message. The receiver, anticipating that any communication he receives is only cheap-talk, chooses in both games each of his two actions with probability one-half so as to maximize his expected payoffs given the prior probabilities on the states chosen by Nature.Footnote 4
 We conduct our experiments in the Regulated Game when senders are behavioral with probability 0.3. The sequential equilibrium predicts both truth-telling and trust with probability 1/2 for the Benchmark Game whereas truth-telling of strategic senders with probability 2/7 (28.6 %) and trust with probability 1/2 for the Regulated Game (see the next section for the details). However, our results show that in the Benchmark Game the mean value of the percentage of truthful messages per sender is 55.5 % while the mean value of the percentage of trusted messages per receiver is 53.75 %. The observed excessive truth-telling and excessive trust are much higher for the Regulated Game. Excluding instances of intervention, we find that the mean value of the percentage of truthful messages per sender is 42 % (in contrast to the prediction of 28.6 %). The overall frequency of truthful messages (due to both deliberate truth-telling and intervention) the receivers get in the Regulated Game is 59.7 %, clearly a case against the theoretical prediction of no information transmission by the two types of senders on average. This is, even more strikingly, despite the fact that the mean value of the percentage of trusted messages per receiver in the Regulated game is 61.48 %. A major question in cheap-talk experiments is the identification of the motives behind the overcommunication of senders. The previous research provides alternative answers to this question. For example, Gneezy (2005) shows that in a sender–receiver game where the preferences are conflictive but only the sender knows the payoff structure, the probability of lying is higher, the higher is the resulting gain to the sender or the lower is the resulting loss to the receiver. Cai and Wang (2006) show that overcommunication phenomenon observed in the experimental data can be attributed to the presence of sender subjects with low levels of sophistication or noisy behavior. (Hurkens and Kartik (2009), p. 180) argue that the behavior observed in Gneezy (2005) is consistent with the hypothesis that “either a person will never lie, or a person will lie whenever she prefers the outcome obtained by lying over the outcome obtained by telling the truth”. Using a similar framework to Gneezy (2005), Sutter (2009) shows that some senders exhibit sophisticated deception by being truthful under the expectation that the receiver will not follow their true message. In an alternative model of information transmission, Sánchez-Pagés and Vorsatz (2007) find that the overcommunication can arise in situations where the receiver can voluntarily incur a monetary cost to punish the sender after having trusted a dishonest message. Sánchez-Pagés and Vorsatz (2009) further show that when the sender is also allowed to choose a costly option of remaining silent, excessive truth-telling can be attributed to lying aversion. Recently, Peeters et al. (2013) find that in sender–receiver games with sanctioning opportunities, subjects who sanction in the role of the receiver are more likely to tell the truth excessively in the role of the sender. Following the analysis of Peeters et al. (2013), we show that sender subjects in our experiments have intrinsic motives for excessive truth-telling. We reach this finding using a model with boundedly rational agents, namely a logit-AQRE model where senders have a non-monetary cost of lying.Footnote 5 We assume \(\alpha \) portion of the senders have high enough cost of lying; equivalently, with \(\alpha \) probability a truthful regulator intervenes. This model predicts excessive truth-telling and excessive trust with intervention as well as without intervention. Moreover, the model predicts that equilibrium trust of the receivers is strictly increasing in \(\alpha \); whereas the equilibrium truth telling by the strategic senders is strictly decreasing in \(\alpha \). However, the effect of intervention on the overall truth-telling frequency (compared to the case without intervention) is ambiguous. This is because, on the one hand, the strategic senders tell the truth less often as \(\alpha \) increases (but still more than the predictions of the sequential equilibrium because of cost of lying), but on the other hand, the receivers observe truth-telling by the regulator with probability \(\alpha \). All of our experimental findings are consistent with the theoretical predictions of the logit-AQRE model, supporting the conjectured relation between excessive truth-telling and the cost of lying. Additionally, in our estimations, we allow the cost of lying incurred by the sender to differ between the Benchmark Game and the Regulated Game. The maximum likelihood estimates of the parameters of logit-AQRE model show that the cost of lying is higher and consequently the expected utility of the receiver is higher in the Regulated Game. The rest of the paper is organized as follows: In Sect. 2 we introduce the model and theoretical predictions, in Sects. 3 and 4 we present the experimental design and the hypotheses. We report our experimental results in Sect. 5, and finally we conclude in Sect. 6.",2
18.0,2.0,Review of Economic Design,20 June 2013,https://link.springer.com/article/10.1007/s10058-013-0145-y,Partnership markets with adverse selection,June 2014,Gregory K. Dow,,,Male,Unknown,Unknown,Male,"In the above passage from The Wealth of Nations, appearing just before his famous diatribe against the joint stock company, Adam Smith distinguishes a partnership from a corporation by highlighting the differing procedures through which members are replaced. This distinction still applies. Someone who owns shares in Microsoft can sell those shares on a stock exchange, transferring voting rights to another party in the process, without first obtaining permission from other shareholders. By contrast, professional partnerships in law or medicine almost never allow departing members to choose their own successors. In the English common law tradition, systematized by the Partnership Act of 1890, partnerships have no legal identity distinct from that of their individual members. In the absence of an explicit contrary agreement no one can be introduced as a partner without the consent of all other members of the firm (Prime and Scanlan 1995: 178), and if anyone retires from a partnership at will, this dissolves the firm (Prime and Scanlan 1995: 276). Partnership agreements that entitle an outgoing member to choose a successor, although legally possible, are rarely adopted in practice (Banks 1990: 229). Parallel default rules on turnover among partners have been legislated in Australia and New Zealand (Graw 1996), Canada (VanDuzer 1997: ch. 2), and France and Germany (Banks 1990: 21–25). Under the Uniform Partnership Act (UPA), the legislation governing partnerships in almost all US states, until recently the firm was required to dissolve upon withdrawal by any individual (Hynes 1995; Hillman 1995). Indeed, the UPA did not allow partners to waive their right to bring about dissolution. Revisions in the 1990s sought to provide more stability by substituting an ‘entity’ theory of partnership under which a business unit can survive the departure of individual members. But the default procedure is still for the partnership to dissolve upon withdrawal by a single partner. This ensures that continuing members hold veto rights over the choice of a successor. Similar policies are found in worker cooperatives or labor-managed firms. Such firms seldom treat a membership position as the private property of an individual worker or permit members to sell their positions in the firm to outsiders. One well-known exception involves the plywood cooperatives of Oregon and Washington (Craig and Pencavel 1992; Pencavel and Craig 1994; Pencavel 2002). But even here, a majority of worker-owners must give prior approval before a membership position can be sold to a new worker. The most obvious reason why partners might insist upon unanimous consent, or at least a majority vote, before accepting a new colleague is that they are concerned with the quality of the candidate. Applicants will normally vary in their skill, judgment, motivation, risk tolerance, and collegiality among other things. For enterprises in which net income is shared, decision-making is joint, and personal liability is unlimited, it is unsurprising that incumbents pay considerable attention to the characteristics of new applicants. To grasp why free transferability of partnership positions is rare, an academic economist need only imagine a system in which colleagues can sell their professorships to the highest bidder. Allowing membership transactions to take place on an open market often imposes a negative externality on the partners who stay behind, because the departing member finds it attractive to sell out to a low-quality successor. If those remaining behind would not have made the same decision, the joint payoff of the parties is reduced. In principle this problem could be eliminated by having continuing members bribe departing members to recruit their successors differently, but if it is costly to negotiate an agreement of this sort then partners will prefer a system in which insiders retain control over vacancies. Such arrangements are less necessary when individuals contribute cash or property to an enterprise, rather than labor services, on the reasonable assumption that informational asymmetries matter less for non-human inputs. A key case in point is the use of limited partnerships in the real estate, oil and gas, cable TV, and equipment leasing industries. An active secondary market for such partnership positions has developed in the US since the 1980s (Wollack and Donaldson 1992; Denning and Shastri 1993; Allen 1995; Barber 1996). Limited liability partners do not engage in operational activities, lack voting rights, and are viewed by the tax authorities as ‘passive’ investors. Adverse selection issues thus appear negligible as compared with firms in which the partners supply professional skills or participate in business decisions. Conversely, although closely held corporations enjoy limited liability, share transactions often require prior approval from the other owners due to the prominent role of each individual shareholder in managerial decision-making. There is empirical evidence that adverse selection problems do arise for professional partnerships. Spurr (1987) argues that there are large quality differences among lawyers even after the signalling effects of grades and school quality are taken into account, and that data on law firms support an adverse selection view of the promotion process. O’Flaherty and Siow (1995) find that screening during the associate phase of a law career is relatively imprecise and that mistakes are costly: they estimate that about half of the people promoted to partner by large New York law firms are unqualified and should leave. Landers et al. (1996) present survey evidence for law firms which indicates that promotion decisions are responsive to hours worked in a manner consistent with adverse selection, but not with predictions from agency theory. The framework developed here can be viewed as a special case within a larger class of models where property rights over vacancies are assigned to continuing partners in order to avoid the externality problems associated with replacement workers. However, there are several reasons to focus more specifically on adverse selection. First, such models provide detailed predictions about the nature of market equilibrium (separation vs. pooling, for example), and these details are important in determining whether or not property rights over vacancies affect the joint payoff of the partners. Second, as mentioned above, the evidence suggests that adverse selection in fact arises for partnerships. Finally, the adverse selection approach has interesting welfare implications: for instance, some equilibria always reduce total surplus if the existing allocation of agents to firms is not already surplus-minimizing. The study of partnership markets is motivated in part by a large literature on labor-managed firms (LMFs). Sertel (1982) showed that the perverse comparative static results attributed to LMFs by earlier authors vanish when there is a market on which membership rights in the firm can be traded (for related analyses, see Dow 1986, 1996). Sertel (2002) subsequently incorporated the principles of partnership markets into a much more general approach to coalition formation using the concept of the ‘Rechtsstaat’. The formal model developed here extends Sertel’s ideas about partnership markets in a different direction by introducing adverse selection into the formation of two-person coalitions. Dow (2003, ch. 7) argues that adverse selection in such markets for firm membership may be one important reason for the empirical rarity of LMFs in contemporary economies. Section 2 lays out the basic model. I assume there are two types of worker, high productivity (type-a) and low productivity (type-b). Partnerships consist of two workers who share profits equally. Vacancies are filled by having incumbents announce prices for admission to their firms. Section 3 characterizes price equilibria for given fractions of type-a and type-b workers on each side of the market. Three equilibria can arise: a separating equilibrium in which high- and low-quality incumbents announce different prices for entry to the firm; a pooling equilibrium in which incumbents of both types choose the same price and unattached agents of both types apply for membership; and another pooling equilibrium in which all incumbents announce the same price but only low-quality types apply. Section 4 identifies conditions under which control over vacancies by continuing workers dominates control by departing workers. Departing workers sell out to whoever will pay the most, and due to adverse selection this is a low-quality agent. If the continuing worker would have done the same, as is sometimes true, property rights are irrelevant. But if this incumbent would have chosen a low entry price to attract high-quality applicants, it is necessary to bribe the departing worker to make the same choice. Such side payments are likely to entail bargaining and enforcement costs, which can be avoided by committing the firm in advance to a system where continuing workers are responsible for filling vacancies. Section 5 shows that market transactions always reduce total surplus when there is a separating equilibrium, or a pooling equilibrium in which only low-quality types apply. From the standpoint of surplus maximization, in either of these cases it would be preferable to shut down the market and preserve the existing allocation of workers to firms, whatever it may be. When there is a pooling equilibrium in which both types apply, market sorting may either raise or lower total surplus relative to the initial allocation. Section 6 relaxes some assumptions and discusses ways in which adverse selection can be alleviated in practice. Proofs are provided in an appendix.",1
18.0,2.0,Review of Economic Design,27 October 2013,https://link.springer.com/article/10.1007/s10058-013-0154-x,Speculative partnership dissolution with auctions,June 2014,Ludwig Ensthaler,Thomas Giebe,Jianpei Li,Male,Male,Unknown,Male,"The economics literature on partnership dissolution under incomplete information was started by Cramton et al. (1987) (henceforth CGK). The now standard model introduced in CGK and the many works following it assume that a partner’s valuation of the jointly owned asset is unchanged by dissolution. Thus, a partner who owns half of the firm values sole ownership of the firm exactly twice as much as he values the status quo (with the partners present). Hence, the presence of the partners does not affect the valuation of the asset and it is economically efficient to break up the partnership rather than to continue if the partners have different values for sole ownership. This is of course a valid model for non-producing partnerships where partners are merely co-owners of a good, e.g., siblings who jointly inherit a house. But for most business partnerships, the presence of partners matters for the value of the partnership and its impact on dissolution should be taken into account. Partnerships that jointly produce goods or services typically form because partners benefit from complementary skills (see, e.g., Farrell and Scotchmer 1988).Footnote 1 If such partnerships are dissolved and one of the partners is to take sole ownership, the other partners usually leave the company and the complementarities are lost, making dissolution potentially inefficient.Footnote 2 Thus, when judging the efficiency of a partnership dissolution mechanism it seems natural to ask whether it encourages break-up only in those cases where continuation is not desirable. In other words, an efficient dissolution mechanism should not only ensure that the sole ownership of the partnership asset goes to the partner that values it the most, but moreover prevent break-up if economically undesirable. Furthermore, the literature generally takes the dissolution decision as given. This is the case if a partnership breaks up because it reaches its fixed term according to the partnership agreement or one of the partners passes away. Mostly, a partnership is dissolved only if some partner has called for a dissolution. Thus, it is natural to enrich the model by assuming that dissolution does not happen spontaneously, but must be triggered by a decision of the partners. If partners hold private information about their valuation of sole ownership, proposing dissolution may be a signal of that private information. The \(k+1\)-price auctions are a well-studied and widely used class of dissolution mechanisms. They are appealing because of their simplicity and ease of implementation.Footnote 3 In the business world, the auction is also referred to as the “Mexican Shoot-out Clause”, “Dutch auction”, or as a variant of the most popular “Texas Shoot-out”.Footnote 4 Auctions have been used as a dissolution mechanism in, e.g., the breakup of the 3Com and Huawei joint venture in 2007 (see Ming et al. 2007) and the termination of the Chagoyansk Joint Venture between Peter Hambro Mining Plc, Rio Tinto Mining and Exploration Limited.Footnote 5 Jade trading in Hetian (Xinjiang, China) usually takes place via \(k+1\)-price auctions. Traders who are interested in an item of jade buy it jointly from the jade panner. Afterwards, they hold an ascending price auction among themselves to determine the sole owner of the item (see Saimaiti 2011). Applied to a two-player partnership, the two partners simultaneously submit sealed bids in \(k+1\)-price auctions and the partner who submitted the larger bid buys the other partner’s share of the partnership. The transaction price for the whole asset is a convex combination of the low and high bid: \(kb_L+(1-k)b_H\) with \(k\in [0,1]\). For \(k=0\) the auction is known as a winner’s bid auction (WBA), while for \(k=1\) it is a loser’s bid auction (LBA). A very influential result in CGK is that the set of efficiently dissolvable partnerships is centered around the equal-share partnership. These partnerships can be efficiently dissolved with \(k+1\)-price auctions in a symmetric independent private values framework. We study two-player equal-share partnerships, which are the easiest to dissolve efficiently in the CGK framework.Footnote 6 We extend the model by adding a proposal stage where partners decide whether to enter a \(k+1\) price auction in order to dissolve the partnership. We do this in order to account for the fact that, in real life, partnerships do not end spontaneously in most cases. Instead, at least one of the partners needs to make a deliberate decision to call for dissolution. This extended model allows for continuation as well as inefficient breakup of the partnership.Footnote 7 Moreover we introduce complementarities by assuming a continuation value which is different from the private values for sole ownership. We show that, in this extended model, standard \(k+1\)-price auctions cannot achieve ex post efficiency. In our model, we assume a constant and commonly known continuation value. We thus assume that partners agree on the future payoffs of the partnership. This model is already rich enough to demonstrate an intuitive source of inefficiencies in partnership dissolution: Calling for dissolution signals private information and, thus, gives an incentive to exploit this signal. We conjecture that this fundamental property will carry over to more general models of the continuation value. For instance, one could express the continuation value of each partner as a function of the partners’ private information. Adding the proposal stage induces a signaling game: A partner’s decision to call for dissolution may reveal private information. This affects the other partner’s beliefs and, thus, bidding behavior at the auction stage. Naturally, a partner who has a high value for sole ownership would call for dissolution and bid aggressively in the auction in order to gain sole ownership. Anticipating this, a low-value partner might have an incentive to signal a high valuation by calling for dissolution, speculating that the other partner has a high valuation of sole ownership and will, thus, bid aggressively and pay a large price. In equilibrium this incentive is taken into account and it results in inefficient dissolution. Given this rather gloomy result on standard \(k+1\)-price auctions, we turn to several natural and easily implementable mechanisms for possible remedies. We find that modification of the proposal stage by allowing a partner to veto or requiring consent for a dissolution to occur does not restore efficiency. However, efficiency can be restored if one modifies the auction design such that dissolution occurs if and only if it is efficient. We demonstrate that a WBA with a reserve price equal to the continuation value achieves this goal. The paper proceeds as follows. First, we give a brief overview of the related literature. In Sect. 2 we introduce the model and the game. In Sect. 3 we derive the main result. A discussion on alternative mechanisms is in Sect. 4. Section 5 provides a conclusion. All proofs are in the “Appendix”. Our paper contributes to the partnership dissolution literature and the literature that analyzes \(k+1\)-price auctions. In particular, we add to the branch of the literature that models the dissolution of a partnership as the last stage of a sequential interaction. The formal analysis of the partnership dissolution problem in an incomplete information setting started with CGK. They show that there exist ex-post efficient, incentive compatible, individually rational and budget balanced dissolution mechanisms under the symmetric independent private values framework if and only if the ownership rights are sufficiently symmetric. Fieseler et al. (2003) extend the analysis to interdependent valuations. They find that in contrast to a private values setting, it might be impossible to find a distribution of ownership rights such that the partnership can be dissolved efficiently. Jehiel and Pauzner (2006) derive efficient, incentive-compatible, individually rational, and budget-balanced mechanisms for the case that valuations only depend on private information of one of the partners.Footnote 8
Chien (2007) characterizes the second-best mechanisms when it is impossible to achieve ex post efficiency. Galavotti et al. (2011) study ex post individually rational, efficient partnership dissolution in a setting with interdependent valuations.Footnote 9
 A number of papers have studied the performance of \(k+1\)-price auctions in the partnership dissolution environment.Footnote 10 CGK first analyzed the \(k+1\)-price auctions, and arrived at the conclusion that these auctions implement ex post efficiency in a symmetric private values framework if the partners hold sufficiently equal shares of the partnership. McAfee (1992) analyzes the performance of the WBA and the LBA in a two-player equal-share partnership assuming that bidders are risk averse (including the risk-neutrality case) and there is an option to sell the asset to a third party. He finds that the WBA is efficient, whereas the LBA not. Engelbrecht-Wiggans (1994) solves for the equilibrium of the WBA and LBA and compares their expected equilibrium prices in an interdependent environment. Bulow et al. (1999) analyze the WBA and the LBA in a common values model with uniform distribution of types in a take-over game. They show that a toehold in the target firm helps a buyer win an auction, sometimes very cheaply. 
Minehart and Neeman (1999) examine the performance of the WBA when coordination is important for the success of a project. de Frutos (2000) provides results on the existence and uniqueness of pure strategy equilibria in the WBA and the LBA for the case in which partners’ valuations are asymmetrically distributed. Kittsteiner (2003) analyses the general \(k+1\)-price auctions under interdependent valuations and proves the uniqueness of the equilibrium for these auctions. He also shows that \(k+1\)-price auctions remain efficient if the partners are forced to participate in the dissolution. Morgan (2004) studies fairness of dissolution mechanisms in a common values framework, including WBA and LBA. Lengwiler and Wolfstetter (2005) establish revenue equivalence between the WBA and LBA in the symmetric independent private values framework, assuming the existence of an auctioneer who collects a fixed fraction of the auction revenue. Athanassoglou et al. (2010) analyze auctions with per-unit price that is a function of the two highest bids when agents have unequal endowments.Footnote 11
Li et al. (2013) analyze the WBA and the LBA in a common-values framework with proprietary information. Wasser (2013) looks at \(k+1\)-price auctions under asymmetric valuations and ownership shares. Both strands of contributions, the one that focuses on the attainability of efficiency and the one that focuses on the equilibrium and properties of \(k+1\)-price auctions in different settings, have largely followed the tradition of CGK by assuming that dissolution is exogenously given and that the partners have the same value for the partnership as for sole ownership. Thus the impact of a loss of complementarity on the efficiency of a breakup has been neglected. We take this missing factor into account by adding a proposal stage to the standard dissolution game. Moreover, the above mechanism design literature following CGK mostly models partnership dissolution as a one-shot interaction. This ignores feedback effects that a dissolution mechanism may have on, e.g., investment incentives, effort choices, and, ultimately, partnership formation, or the effect of pre-dissolution information transmission on the outcome of dissolution. There are several contributions that, similar to our paper, model a sequential interaction. Ferreira et al. (2013) analyze the interaction between initial share allocation and ex post restructuring incentive and show that efficiency in control contests might be a motivation for the prevalence of separation of ownership and control in the real world. de Frutos and Kittsteiner (2008) study a two-stage partnership dissolution game where dissolution with buy/sell clauses is preceded by a stage where partners bid for the right to propose a price. Li and Wolfstetter (2010) examine the interaction of investment incentives and dissolution incentives under buy/sell clauses. Comino et al. (2010) show that non-contractible effort incentives can be increased if there is no pre-agreed asset allocation clause in the partnership contract.Footnote 12
 Finally, experimental work on partnership dissolution has been booming recently. Brooks et al. (2010) analyze and test experimentally the question why Texas Shootouts are prevalent in real-world contracts, but are rarely triggered. Kittsteiner et al. (2012) test the performance of the WBA and the buy/sell clause in the independent private values setting. Qin and Zhang (2013) investigate bidders’ behavior in clock auctions to dissolve two-player partnerships. Yu and Chmura (2013) analyze the effect of ambiguity attitudes in experiments where partnerships are dissolved with auctions.",2
18.0,2.0,Review of Economic Design,26 September 2013,https://link.springer.com/article/10.1007/s10058-013-0150-1,When do stable roommate matchings exist? A review,June 2014,Jens Gudmundsson,,,Male,Unknown,Unknown,Male,"When Gale and Shapley (1962) introduced the roommate problem, they did so merely to highlight an important difference to the two-sided marriage problem: some instances do not have stable outcomes. The essentials of the roommate problem is a set of agents, allowed to pair up in couples or stay single, together with individual-specific preferences over partners. A stable matching is a subdivision of the agents into pairs and singles such that no agent prefers being single to being matched with his partner, and no two agents prefer one another over their respective partners. The following example not only illustrates the absence of a stable outcome, but also sheds light on the cyclical preference structure that is central throughout the paper. Suppose there are three agents, 1, 2 and 3, with preferences as follows. Agent 1 prefers 2 to 3; agent 2 prefers 3 to 1; and agent 3 prefers 1 to 2. Each of them rather finds a partner than remains single. Suppose now agents 1 and 2 match while agent 3 stays single. Then agent 2 prefers 3 to 1, and agent 3 prefers 2 to being single. Hence, the matching is not stable: agents 2 and 3 can mutually benefit by deviating from it. By symmetry, the argument extends to the other matchings. Hence, there exists no stable matching. We will consider a more general model than the one originally put forth by Gale and Shapley (1962). First, agents may remain single. We can therefore analyse models with an odd number of agents. Second, agents need not prefer every potential partner to being single.Footnote 1 Third, an agent may be indifferent between two different agents, in contrast to the case where each agent strictly orders any two different partners. These changes capture, for example, that people tend to have limited networks. If an agent only is interested in collaborating with his friends, he can group all the others at the bottom of his preference list, below remaining single. Our first contribution is to compare some preferences restrictions that grant the existence of stable matchings.Footnote 2 These include “no odd rings” by Chung (2000), “stable partitions” by Tan (1991), and a generalized form of “\(\alpha \)-reducible” preferences introduced by Alcalde (1995).Footnote 3 Our second contribution is to examine preferences that have no so called “weak cycles”. The main result shows that such preferences are equivalent to those satisfying a generalization of the “symmetric utilities hypothesis” (Rodrigues-Neto 2007) and to those of roommate problems with “globally ranked pairs” (Abraham et al. 2008). The remainder of this paper is organized as follows. We introduce the model in the next section. In Sect. 3, we examine different restrictions on preferences that admit stable matchings. Preferences without weak cycles are investigated in Sect. 4.",8
18.0,3.0,Review of Economic Design,28 June 2014,https://link.springer.com/article/10.1007/s10058-014-0160-7,A mechanism design approach to allocating central government funds among regional development agencies,September 2014,Özgür Kıbrıs,İpek Gürsel Tapkı,,Male,Female,Unknown,Mix,,
18.0,3.0,Review of Economic Design,07 September 2013,https://link.springer.com/article/10.1007/s10058-013-0148-8,Risk-sharing networks and farsighted stability,September 2014,Gilles Grandjean,,,Male,Unknown,Unknown,Male,"In this paper, we study the formation of risk-sharing networks. There are regions in developing countries where the access to a formal insurance market is limited. Some villages lack for instance institutions that can enforce contracts or repayments of loans. Economic fluctuations, due to climate shocks, crop pests, illness or funeral expenditures are important in those low income areas. Informal risk-sharing appears to be one of the prominent strategy used to cope with these shocks (see the survey of Alderman and Paxson 1994). That is, households in need receive help from others, in the form of free loans or transfers. A growing empirical literature (see Fafchamps 1992; Grimmard 1997; Fafchamps and Lund 2003; Weerdt and Dercon 2006) has shown that a fully efficient risk-pooling equilibrium is not reached: risk-sharing does not take place within exogenous group such as the village, but rather within networks involving agents having common characteristics (neighborhood, professional or religious affiliation, kinship, etc). Most of the theoretical papers on informal risk-sharing in developing countries assume that binding agreements cannot be enforced. In this context, if the risk occurs only once, the fortunate agent has no incentive to transfer money ex-post. However, this effect disappears in a dynamic setting where multiple shocks are expected to occur since agents who transfer money today may expect to be reciprocated at a future date. This literature offers a theoretical argument to explain the observed lack of complete income pooling at the village level by analyzing the transfer schemes which are such that each agent is willing to conform to the agreement once the uncertain income shock occurs.Footnote 1
 
Platteau (2000) has argued that agents involved in a risk-sharing relationship may be committed to the agreement even if the institutional context does not provide the tools to enforce contracts, because the social norm imposes it. Bramoullé and Kranton (2007a) have developed a model, where agents establish their informal insurance relationships endogenously, assuming that linked pairs can commit to share equally their income. They have considered agents who are ex ante homogeneous, but differ through their position in the network.Footnote 2 They have shown that the efficient risk-sharing networks are such that each agent is indirectly connected to the others, involving the maximal level of insurance in the population, and that networks formed by myopic agents connect fewer individuals than the efficient ones. They have thus provided another theoretical explanation of the observation that informal insurance does not occur at the village level. Empirical studies support the idea that mutual risk-sharing agreements are formed endogenously. For instance, Rosenzweig and Stark (1989) have observed that marriages between households from different regions in India occur to diversify geographically the risks. Grimmard (1997) has found evidence of transfers and migrations in Côte d’Ivoire supporting this idea. Comola (2007) has observed that the structure of the network, that is the social position of an agent with respect to the others, is critical to understand the choice of risk-sharing partners. Based on data on the village Nyakatoke in Tanzania, she has found that “not only the characteristics of direct friends, but also the characteristics of indirect contacts are taken into account when a link is created”. This paper analyzes which pattern of insurance relationships emerges in the long run when agents are farsighted, rather than myopic, in the sense that they are able to forecast how other agents would react to their choice of partners. In his survey of models of network formation, Jackson (2005) has mentioned that farsightedness is an important consideration in some appropriate context. He has stated that “in large networks it might be that players have very little ability to forecast how the network might change in reaction to the addition or deletion of a link. In such situations the myopic solutions are quite reasonable. However, if players have very good information about how others might react to changes in the network, then these are things that one wants to allow for either in the specification of the game or in the definition of the stability concept”. To our knowledge, no existing work has attempted to establish the level of sophistication of agents when they create their informal network in rural areas of developing countries. However, we believe that the key ingredients mentioned by Jackson (2005) for farsightedness to matter are present in this framework: our focus is on small communities of agents having common characteristics, and have thus good information about each other. Agents in the model of Bramoullé and Kranton (2007a) are strategic as they establish links with other agents, anticipating that these connections might be profitable in the future if they face negative income shocks. In this paper, we assume that agents are a bit more strategic. In addition to forming connections in anticipation of likely future negative shocks, they also realize that their choice of partners may determine others’ choices of partners. Such anticipation is consistent with Comola (2007)’s observation that the full architecture of bilateral agreements determines the incentives for a pair of agents to establish a partnership. We adopt the notion of pairwise farsightedly stable set due to Herings et al. (2009) to determine which networks are formed by farsighted agents.Footnote 3 We find that for small costs of establishing and maintaining a partnership, farsighted agents may form efficient networks that involve full income pooling while myopic agents form networks connecting fewer individuals. Two mechanisms explain this result: (i) Farsighted agents belonging to small groups may decide to create new partnerships that are not directly profitable to them, because they realize that other partners will further join this bigger and more attractive group. In other words, the farsightedness of the agents may solve a coordination problem. (ii) Farsighted agents may refrain from deleting costly links if they belong to a big group, as they understand that this may induce others to rearrange their partnerships in a way that deters the myopic incentives to delete the link at first. We have already mentioned that empirical studies have revealed that risk-sharing occurs among agents having common characteristics. Farsightedness may be a factor rationalizing this observation. The paper is organized as follows. In Sect. 2 we introduce some notations and definitions for networks, and we present the model of risk-sharing networks of Bramoullé and Kranton (2007a). In Sect. 3, we investigate the formation of risk-sharing networks when agents are myopic. Section 4 provides a characterization of the pairwise farsightedly stable set of risk-sharing networks. In Sect. 5, we study the robustness of our results the choice of the farsighted stability notion, by introducing two closely related notions of farsighted stability: the von Neumann-Morgenstern pairwise farsighted stable sets and the pairwise consistent sets. In Sect. 6, we analyze more in detail the formation of risk-sharing networks when agents have a quadratic utility function. In Sect. 7, we conclude.",3
18.0,3.0,Review of Economic Design,08 June 2014,https://link.springer.com/article/10.1007/s10058-014-0159-0,Knapsack cost sharing,September 2014,Andreas Darmann,Christian Klamler,,Male,Male,Unknown,Male,"Cost allocation in combinatorial optimization problems has been intensively discussed in recent years (see Moulin 2013 for a summary). The major focus has been on the minimum cost spanning tree problem, the earliest and most widely investigated cost sharing problem in this area (e.g., Bird 1976; Bogomolnaia and Moulin 2010; Dutta and Kar 2004). There the interest lies mainly in the fair division of the cost of creating a network of minimum total cost in which each agent is connected directly or indirectly to a source. A second emphasis has been on scheduling and queuing problems, i.e., on the problem of optimally processing jobs of different lengths or weights on a single server (e.g., Chun 2006; Maniquet 2003; Moulin 2008). The above problem of finding minimum cost spanning trees has a major advantage among combinatorial optimization problems. Its optimal solution can be found in polynomial time. Only then, i.e., in the case of finding such an optimal solution “quickly”, does it seem to make sense to talk about fairly sharing the costs, because otherwise any changes to the setting could make it impossible to find the new cost allocation in reasonable time (see also Moulin 2013). Among the combinatorial optimization problems, the knapsack problem is concerned with efficiently filling a weight-restricted knapsack with items from a set of items with possibly different weights and profits. Efficiency in that respect means maximizing some profit function based on the items’ profits. In case of indivisible items, this problem is typically \(\mathsf NP\)-hard (Karp 1972). One exception is the continuous knapsack problem in which the items are divisible and therefore the solution could contain a certain fraction of one item.Footnote 1
 The fair division setting used in this paper can be summarized as follows: we start with a certain knapsack (a capacity, time interval, budget constraint, etc.) and a set of items over which individuals have binary preferences. Each of the items has a (possibly different) weight (or length, or cost). First, the knapsack is filled by maximizing social welfare defined by the sum of approvals of items in the knapsack.Footnote 2 Second, the cost of the knapsack (or maintaining the capacity, or using the time) is to be fairly divided among the individuals. The paper provides two characterization results of continuous knapsack cost sharing rules based on various axioms. Major axioms used in the results are the dummy property and merge-and-split proofness. The dummy property (see, e.g., Shapley 1953) requires that individuals that do not approve of any item in the knapsack should not carry any of its cost. On the other hand, merge-and-split proofness ensures that the individuals have no incentive to split or merge identities. The latter axiom is in general pushing towards rules based on proportionality concepts (see, e.g., Moulin 2002), something that will become visible also in our results. The first characterization result in the paper will use the two axioms and add a few independence properties to obtain a family of essentially proportional rules based on the individuals’ approvals of items in the knapsack. The second characterization result uses only the additional property of composition up (Thomson 2003) to characterize a proportional rule based on individuals’ approvals but also the items’ weights and the number of approvals for each item. Our results could also be seen as extensions of usual cost sharing problems such as the bankruptcy problem (Bergantiños and Sánchez 2002; Thomson 2003) where costs or claims play a major role in determining a fair cost allocation. However, in our setting there would be additional constraints (such as the weight capacity of the knapsack) that have to be taken into account in the cost allocation. Claims in our framework are the approvals or disapprovals of certain items by individuals (Brams and Fishburn 1983), with social welfare of a set of items (the filled knapsack) being simply defined by the total number of approvals for the single items in the set (Brams et al. 2007). To summarize, in principle we are concerned with sharing the cost of a selected set of non-rival items that provides different utilities or payoffs to the individuals. Cost allocation aspects in such a binary knapsack problem have been considered before by Dror (1990) and certain rules such as the Shapley value or the equal charge method have been suggested. In this paper we want to go one step further and provide characterization results for (a family of) continuous knapsack cost sharing rules. Possible applications of such cost-sharing methods could be found whenever a group of agents tries to assign cost shares of a joint and fixed budget used to finance various different activities out of a larger set of activities with different costs. Consider, e.g., the agricultural budget of the EU and a set of policies (with different costs) to be potentially financed by this given budget. Among those available policies, each country might approve of some and disapprove of others. First, the discussed cost-sharing methods would help in determining a socially optimal set of policies to be implemented. Moreover, independent of direct net financial benefits or costs, the methods provide an alternative approach to assign cost-shares of the joint budget to the individual countries based on aspects of fairness. The paper is structured as follows: the next section establishes the formal framework, defines the continuous knapsack problem, and introduces reasonable properties of continuous knapsack cost sharing rules. Section 3 first introduces and characterizes a whole family of such rules and then focuses on a particular rule of which a characterization result is provided. Section 4 shows the independence of the used axioms and the final section concludes the paper.",2
18.0,4.0,Review of Economic Design,06 July 2014,https://link.springer.com/article/10.1007/s10058-014-0162-5,What drives failure to maximize payoffs in the lab? A test of the inequality aversion hypothesis,December 2014,Nicolas Jacquemet,Adam Zylbersztejn,,Male,Male,Unknown,Male,"The game presented in Fig. 1 illustrates the behavioral challenge posed by relying on the assumptions behind subgame perfectness. In this game, introduced by Rosenthal (1981), the first mover either decides alone on the outcome of the game (action \(R\)) or relies on the second mover (\(L\)). In the latter case, the second mover only has to decide whether to maximize both players’ payoffs (\(r\)) or not (\(l\)). Although the threat of non-maximization is non-credible and action \(l\) is ruled out by subgame perfectness, the first mover may still lose a substantial amount should the second mover fail to maximize payoffs. This makes the Pareto-dominated solution—in which the first mover plays \(L\)—a plausible case in real-life implementations. This puzzling behavioral outcome, originally introduced as a theoretical conjecture by Rosenthal, is robustly highlighted by all lab experiments using both sequential-move and simultaneous-move versions of the game.Footnote 1 Suboptimal outcomes arise due to two different driving forces: first, as expected by Rosenthal, subjects who play as first movers are very frequently reluctant to rely on second movers; second, surprisingly enough, an important share of second movers do indeed turn out to be unreliable and fail to maximize both players’ payoffs. The Rosenthal (1981) game. Note
\(0<X<1.000.000\). The value of \(Y\) is irrelevant from the strategic perspective. We assume \(0<Y<1\) so that the outcomes are Pareto-rankable Such evidence has long been seen as a pathological example of people’s inability to use subgame perfectness. This interpretation however relies on the assumption of standard preferences that only include personal payoffs—so that the common knowledge of the payoff scheme provides complete information about all players’ preferences. The aim of this paper is to investigate another possible explanation for these puzzling behaviors, namely inequality aversion (Fehr and Schmidt 1999)—which may rationalize suboptimal outcomes in all the payoff structures implemented to date in lab experiments.Footnote 2 Our investigation is based on two research questions: do unequal payoffs generated by the dominant outcome (i) make second movers unwilling to maximize both players’ payoffs, and (ii) make first movers reluctant to rely on second movers that are actually reliable? We implement six variations of the payoff structure of the game, all sharing the main strategic dimensions of the original game. The Baseline treatments are compared to Egalitarian treatments which restore equality between players’ payoffs in the Pareto-dominant outcome. We also assess the robustness of our results to enhanced saliency of the decisions in the game, and provide evidence on the cross-cultural robustness by collecting data in France and in Poland. Our findings are threefold. First, our data unambiguously reject the inequality aversion hypothesis, since neither the behavior of the first movers nor the behavior of the second movers changes due to equalized payoffs. Second, we observe that payoff maximization among second movers is almost universal only once all the Nash equilibria in the game involve the same inequality of payoffs. This suggests that player Bs in other treatments—where player As’ relative standing in the non-cooperative equilibrium is more favourable than in the cooperative one—might act inefficiently so as to penalize their partners for enjoying an undeserved procedural advantage. This points to a totally different kind of other-regarding preferences, related to procedural justice.Footnote 3 Our third finding is that first movers do not take into account this transition in second movers’ behavior, as if they did not expect social preferences to have any effect on their partners’ actions. We conclude the paper with a discussion of the avenues that remain open to explain such a robust inefficiency observed in the lab.",7
18.0,4.0,Review of Economic Design,08 August 2014,https://link.springer.com/article/10.1007/s10058-014-0164-3,"Common preference, non-consequential features, and collective decision making",December 2014,Susumu Cato,,,Male,Unknown,Unknown,Male,"
Welfarist consequentialism is dominant in normative economics and modern moral theories.Footnote 1 It claims that welfare information from outcomes is crucial for evaluations on situations. An opposing position is called non-consequentialism.Footnote 2 According to this, there exist many important considerations such as, duties, rights, friendships, procedural justice, and freedom of choice, which are ignored by welfarist consequentialism. Recently, many researchers have investigated the roles of consequential and non-consequential values in social decision situations. While some authors emphasize the necessity of welfarist consequentialism (Kaplow and Shavell 2001, 2002),Footnote 3 others focus on the importance of non-welfaristic/non-consequential social decisions and procedural justice (Gravel 1994; Suzumura 1999; Suzumura and Xu 2001, 2004; Gotoh et al. 2005; Sakai and Shimoji 2006; Yoshihara 2008; Suzumura and Yoshihara 2008). The purpose of this paper is to examine the role of common preferences on non-consequential features (common non-consequential values) in social decision situations. We study an extended framework of the social choice theory that incorporates not only consequential considerations but also non-consequential considerations. Our framework is a modified version of the seminal works of Kelsey (1987), Suzumura (1999), and Suzumura and Xu (2001, 2004). We characterize a social ordering function that gives unequivocal priority to non-consequential values. We give three characterizations of the non-consequential social ordering function. Our first characterization has five features as follows: Each individual has an extended preference over the extended alternatives; The society consists of consequentialists and non-consequentialists; There exists at least one non-consequentialist; A social ordering function maps each profile of individual extended preferences to a social extended preference. A social ordering function satisfies the Pareto principle, the independence of irrelevant alternatives, and non-dictatorship. Under (F.1)–(F.5), a social ordering gives unequivocal priority to common non-consequential values. (F.1) is the fundamental feature of our framework. In general, a choice situation is described as follows: an outcome \(x\) is chosen under a circumstance \(\theta \). The circumstance includes a non-consequential information about the situation. We refer to a pair \((x,\theta )\) as an extended alternative. Each individual has his/her welfaristic ordering (consequential value) over the set of consequences, while he/she has a non-consequential value over the set of circumstances. A non-consequential value is assumed to be homogeneous among individuals. He/she has an ordering over the set of extended alternatives based on the two values. (F.2) and (F.3) state the types of extended preferences that individuals can have. As mentioned in (F.2), we focus on a society in which every individual is either a consequentialist or a non-consequentialist. A consequentialist gives unequivocal priority to welfare over a common non-consequential value. He/she considers the common non-consequential value if and only if he/she is indifferent between welfares from two choice situations. On the other hand, a non-consequentialist gives unequivocal priority to the common non-consequential value. As stated in (F.3), we require the existence of a non-consequentialist. In our framework, a non-consequentialist has a crucial role in avoiding Arrow’s impossibility theorem, and thus, we need at least one non-consequentialist.Footnote 4 (F.4) states that a social extended preference is the amalgamation of individual preferences. In our framework, non-consequential information is incorporated, and thus, a social preference represents an “ethical” judgment rather than the ranking generated by a voting method. (F.5) requires the democratic principles formulated by Arrow (1951) to be satisfied. Subsequently, we extend our results. We do not require the existence of a non-consequentialist in the second characterization, and we consider the general domain of common non-consequential values, which is the largest domain consistent with the existence of common judgments over circumstances. The standard Arrovian conditions are too demanding under this domain. However, if the conditions are slightly modified, we can show that a social ordering gives unequivocal priority to common consequential values. Moreover, we focus on the case where the space of extended alternatives is restricted and obtain the third characterization. In the first and second characterizations, it is assumed that every outcome is attainable under all circumstances. This assumption is demanding because there is an intrinsic connection between outcomes and circumstances in many examples. In our third characterization, each outcome is allowed to be unavailable under some circumstances. Then, we clarify under which conditions on the set of extended alternatives our characterization is robust. The rest of this paper is organized as follows. Section 2 introduces our framework. Sections 3 and 4 present our results. Section 5 considers the restricted space of extended alternatives. Section 6 discusses the relationship between our results and the results in the existing works. Section 7 concludes the paper. All the proofs are relegated to the Appendix.",
18.0,4.0,Review of Economic Design,17 October 2014,https://link.springer.com/article/10.1007/s10058-014-0165-2,Security bid auctions for agency contracts,December 2014,Byoung Heon Jun,Elmar G. Wolfstetter,,,Male,Unknown,Mix,,
19.0,1.0,Review of Economic Design,08 March 2015,https://link.springer.com/article/10.1007/s10058-015-0170-0,Letter from the editors,March 2015,Atila Abdulkadiroglu,Tilman Borgers,Fuhito Kojima,Male,Male,Unknown,Male,,
19.0,1.0,Review of Economic Design,18 January 2014,https://link.springer.com/article/10.1007/s10058-014-0156-3,Group stability in matching with interdependent values,March 2015,Archishman Chakraborty,Alessandro Citanna,Michael Ostrovsky,Unknown,Male,Male,Male,"In many matching markets agents have incomplete information about potential mates. Consequently, an agent’s preferences over mates may change if she obtains or infers the information held by other agents. For instance, admissions officers may be unsure about the quality of applicants to a graduate program and they may update their estimates if they learn that the applicant has been admitted to another program. Employers may face uncertainty about the skills and ability of prospective hires and they may revise their opinions upon learning the information held by other employers.Footnote 1
 Under incomplete information and interdependent values, the information obtained during the operation of the market may lead an agent, or a coalition of agents, to revise their valuations and challenge the market outcome in favor of an alternative that is preferred by the coalition. Since agents anticipate this possibility and take it into account in their behavior, this places additional constraints on what a mechanism designer can do in terms of aggregating agents’ information and deciding on allocations. It is therefore of interest to identify mechanisms under which matching markets with interdependent values are stable, i.e., immune to misrepresentation of information and to objections raised by agents after the operation of the market. Chakraborty Citanna, and Ostrovsky (2010; hereafter CCO) introduce a notion of stability for two-sided problems of students and colleges where colleges have interdependent values over applicants. Stability in CCO mimics the traditional notions of stability in marriage markets: an assignment is stable if no individual agent (a college or a student) wants to unilaterally drop its assigned partner and no unmatched college–student pair could benefit from matching with each other (and possibly dropping their assigned partners). In the classical many-to-one matching setting with private values, this notion of pairwise stability is equivalent to a (potentially more restrictive) notion of group stability, under which larger coalitions are allowed to form and rematch (Roth and Sotomayor 1990, p. 130).Footnote 2 In this paper we show that under interdependent values, pairwise stability is not sufficient to guarantee group stability. A pairwise stable matching mechanism may be vulnerable to a group of colleges and students proposing an alternative assignment that they unanimously prefer after observing their matching outcomes. CCO focus on matching markets with one-sided incomplete information, in which college qualities are known and student preferences over colleges are common knowledge. Furthermore, students have no private information about their quality and the student side of the matching market has homogeneous preferences.Footnote 3 In contrast, colleges have signals about the students. These signals could be informative about the student on different dimensions such as analytical ability, athletic talents, communication skills, or leadership potential. Colleges may have arbitrary, even opposed preferences over one or more of these dimensions. Crucially however, valuations are interdependent and a college’s preferences over different students may depend on the signals held by other colleges. We share with CCO these aspects of the environment but look for direct centralized matching mechanisms that cannot be blocked by any coalition once the matching outcome is determined. Such mechanisms must also be incentive compatible. More precisely, colleges cannot have an incentive to lie to the mechanism designer even after anticipating the possibility of subsequent rematching. We call these mechanisms group stable. We show that the set of group stable mechanisms is always nonempty (Theorem 1) by constructing a stable mechanism that we call modified serial dictatorship. Under modified serial dictatorship, colleges are ordered in a descending sequence according to the students’ common preferences. Each college is then assigned students based on the college’s evaluation of them, conditional on the college’s own private signals and on the signals of colleges above (but not below) them in the student’s rankings. With such a matching rule, no subset of college or students can infer enough information from the observed outcome to allow it to profit via a blocking coalition. Modified serial dictatorship is different from the simple serial dictatorship mechanism introduced in CCO. Under the latter, each college is assigned students based on its own signals and the matching outcomes of the better colleges, but not those colleges’ signals. For pairwise stability, either mechanism works. In fact there is a whole family of related mechanisms that result in pairwise stable matchings (see CCO, p. 97, footnote 8). For group stability, however, the choice of a particular serial dictatorship mechanism matters. While modified serial dictatorship produces a group stable outcome, simple serial dictatorship does not always do so. Simple serial dictatorship may not be group stable because it does not use all the information held by higher ranked colleges to determine the allocation of lower ranked colleges. In environments where colleges value students in different ways at least on some dimensions, this unused information may leave unrealized some gains from trade among coalitions of colleges. By sharing this unused information, these coalitions may profitably rematch among themselves. We confirm this intuition in Example 1 after defining what it means for a coalition to object to an allocation. Even under one-sided private information and homogenous student preferences, extending the analysis of stability from pairs to groups is not immediate. A suitable definition of group stability has to address three fundamental issues. The first, common also to the CCO setup, is observability, or the extent to which the status quo outcome is publicly observed at the posterior stage when coalitions may raise objections. The second is coalition formation, that is, what coalitions can be formed and what rematching protocols can be used. The third is communication, i.e., the restrictions, if any, that one imposes on information sharing among coalition members at the time they raise objections. With respect to the observability issue, we follow CCO and consider weak observability—each agent only observes its own mates and does not directly observe the mates of others at the time they may raise objections. Observing the entire matching outcome (strong observability) leads to impossibility results demonstrated in CCO and therefore it is too demanding a requirement. With respect to coalition formation, we also follow CCO and assume that any coalition can be formed if agents find it in their interest to do so, i.e., there are no re-matching frictions. Unlike CCO however, we allow coalitions larger than a college–student pair. In doing so we also need to specify the mechanisms available to blocking coalitions at the time they raise objections. We focus on a simple mechanism with the following features. First, we allow coalitions to object to the decision made by the status quo mechanism only via proposing an alternative matching (as opposed to a matching rule). Second, we model this counterproposal stage as a simultaneous voting game in which coalition partners must unanimously accept the alternative match for the objection to succeed and look at Bayesian Nash equilibria of this game. Third, we assume that agents can share private information only if they “belong to the coalition” and are made strictly better off by the counterproposal. Since agents will condition their acceptance of a counterproposal on their own private information (as well as on the information they have obtained from the initial allocation), a blocking coalition in general reveals to its members at least some of the information held by other members. The existence of a group stable mechanism depends upon the restrictions we impose on blocking coalitions. In principle, a group of agents could design a general communication mechanism to share the information each agent in the group has in an incentive compatible manner. Our restriction that a blocking coalition can only engage in unanimous voting over an alternative deterministic match, as opposed to using a general mechanism, captures situations where a general mechanism may be too costly to operate for a blocking coalition at the stage when the status quo rule has already proposed a match. These costs may be direct or take the form of constraints in designing complex mechanisms that require precise specification of randomization schemes and message games (for a similar point see Forges 1994). Whether or not these restrictions are realistic, we show that such restrictions are in fact necessary to obtain existence of group stable mechanisms (Theorem 2). Necessity gives our definitions and results a normative content and provides a tight characterization of group stability. The principal force behind this tight necessity result is the possibility of anticipated renegotiation. Anticipated renegotiation is a key consideration shared with CCO that goes beyond the twin requirements of incentive compatibility on one hand and immunity to objections given truthful information revelation on the other. Under anticipated renegotiation, agents are allowed not only to lie to the mechanism or block the observed matching outcome, but they can also lie to the mechanism anticipating a subsequent objection raised after observing the matching outcome. Immunity to anticipated renegotiation is a natural requirement that stable matching rules should satisfy in situations where agents are not only aware that the mechanism needs to aggregate information but are also aware that they can subsequently object to the proposed outcome. We conclude the paper by considering the efficiency properties of modified serial dictatorship. We show that in environments where higher-ranked colleges have higher thresholds for admitting students, modified serial dictatorship always produces an ex-post Pareto efficient matching. In this it has better efficiency properties than the simple serial dictatorship proposed by CCO. The literature on matching has usually considered private value environments (see, e.g., Roth and Sotomayor 1990, for a survey of many classical results). In a different context of positive analyses of specific matching markets with frictions, Chade (2006), Hoppe et al. (2009), and Ely and Siegel (2013) also highlight the role of information revealed by observed matchings on the operation of the market. We employ instead a mechanism design approach which bears similarities to the notion of truthful and obedient behavior in Myerson (1982). In the mechanism design vein, Neeman and Pavlov (2013) have recently looked at renegotiation-proofness in allocation problems with transfers. As mentioned above, CCO consider pairwise stability only, and this paper should be seen as an extension of the analysis carried out there. In particular, a key innovation arising in both papers is the notion of anticipated renegotiation, which has subsequently been applied in private-value settings by Kojima (2011) and Afacan (2012). This paper also relates to the literature on cooperative games, specifically, on the core under incomplete information (e.g., Wilson (1978), Vohra (1999), Forges et al. (2001); for a survey, see Forges et al. (2001)). Our analysis differs because we focus on objections at the posterior stage, and we consider the possibility of anticipated renegotiation.Footnote 4
",9
19.0,1.0,Review of Economic Design,29 June 2014,https://link.springer.com/article/10.1007/s10058-014-0161-6,Probabilistic procurement auctions,March 2015,Thomas Giebe,Paul Schweinzer,,Male,Male,Unknown,Male,"In many procurement settings winners are selected on the basis of the offered quality per unit of money, i.e., the quality–price ratio of the submitted bids.Footnote 1 In these applications, ‘quality’ can be a multi-dimensional property of the product in question, typically summarised by some score. Whereas the bidders might be fully aware of the quality they offer, the procurer is often not able to perfectly assess the offered qualities, for instance, when the procurement decision is based on preliminary designs or prototypes. Thus, with positive probability, the best offer does not get the award. Moreover, in some scenarios, the procured objects may have aspects of credence or experience goods such that the actual quality is not fully revealed (or may not be verifiable by a third party), making it impossible to make payments conditional on actual quality. Examples seem to abound in the government procurement of, for instance, long-term defense capabilities which are only developed on the basis of an award. In such a setting, bidders, offering a certain quality, face the strategic task to bid such that their expected payoff is maximised, taking into account that their offers may be misjudged by the procurer. This problem of a buyer’s imprecise evaluation of the sellers’ offers is at the core of our paper. We explore this problem by assuming that the procurer cannot perfectly rank the sellers’ offers. Instead, the procurer’s decision procedure is modelled as a general noisy or fuzzy ranking technology that determines a winner from among all offers in a sealed-bid procurement auction. Each seller’s offer consists of an object of given quality and a financial bid. The latter is the payment the seller demands in case she wins the auction and has to deliver the object in return. At the time of bidding, the object’s quality is assumed to be fixed.Footnote 2 Thus, the seller’s strategic variable is the ‘value for money’ implied by his financial bid for the given object. The procurer wants to select the best quality per unit of money among all offers. The ranking technology determines a winner on the basis of quality–price ratios, i.e., the ratio of the offered quality and the financial bid. The offer with the best actual quality–price ratio is most likely to win, but does not win with certainty. The probability of winning depends on the ranking technology used by the buyer. Our main motivation in this paper is to find a tractable way of solving this fuzzy auction assignment problem in the context of a procurement setting. In order to analyse this problem, we need to assign well-defined winning probabilities on the basis of the submitted bids. For precisely this purpose, the complete information contest literature developed the micro-founded and axiomatised generalised Tullock success function (a generalisation of) which we adopt for our model. From a theoretical point of view, the contribution of the present paper is to integrate this fuzzy assignment technology into an incomplete information auction framework. Although the problem turns out to be difficult to handle in general, we can identify a simple symmetric, pure-strategy equilibrium of our stylised procurement game, in which all sellers offer the same quality–price ratio, i.e., sellers with higher quality demand larger payments. For the cases in which a technology can be transferred to the buyer without further cost to the seller, this equilibrium requires a particular precision of the ranking technology employed by the buyer; it does not exist if the ranking is too precise or too imprecise.Footnote 3 For the case of positive marginal costs of supplying the object in terms of quality, the only requirement is that the scrutiny of tender documents through the buyer is sufficiently precise. We demonstrate that a class of winning probability assignments, which contains the generalised Tullock success function, is an example of a feasible technology in our incomplete information setting. For the Tullock case, both settings studied require precision parameters which are incompatible with the well-known ‘lottery contest.’Footnote 4 If the buyer can design the procurement auction such that these precision range requirements are satisfied, however, then the equilibria always exist. This paper combines ideas from fuzzy, or ‘imperfectly discriminating’ contests under complete information with a procurement problem that makes use of a price/quality scoring rule under incomplete information. In other words, we analyse the probabilistic assignment of winners in an auction setting. Therefore, we combine different strands of the literature as discussed in the following. The assumption of private information is standard and has been extensively analysed in both auction theory and the theory of perfectly discriminating contests (all-pay auctions). In the realm of imperfectly discriminating, ‘fuzzy’ contests, however, the literature has largely avoided the introduction of incomplete information.Footnote 5 Due to technical difficulties, general solutions remain elusive and, at present, very little is known about the case of incomplete information.Footnote 6 The literature has found closed-form solutions only for special cases, mostly standard two-player Tullock contests in which one or both players are privately informed about their (discrete) valuation of the prize or their (constant) marginal cost. Examples, among others, are Hurley and Shogren (1998b), Malueg and Yates (2004), and Schoonbeek and Winkel (2006). In Katsenos (2010), players can signal their marginal cost prior to the contest. Münster (2009b) looks at repeated contests. Wärneryd (2003, 2009) assume a common value of winning. Existence proofs as well as comparative statics (e.g., with respect to rent dissipation and aggregate effort) have been provided by Fey (2008), Prada-Sarmiento (2010), Ryvkin (2010), and Wasser (2010, 2013). Ko (2012) works backwards from the equilibrium distribution of efforts in order to shed light on the solution. Numerical strategies were found by, e.g., Hurley and Shogren (1998a). Ewerhart (2010) provides the only available analytical solution for a given (specially designed) continuous distribution. All above cited works are related to our model only in the sense that they are concerned with incomplete information contests. The obvious difference is that our private information only affects a player’s price/quality score, and thus, the probability of winning, rather than the cost of effort or the valuation of winning. For our setting, we provide a closed-form symmetric equilibrium for any number of players and an assignment function that is more general than the Tullock form.Footnote 7 Due to the simplicity of the equilibrium strategy, we can allow for general joint distributions of private information. Our paper is technically related to Arbatskaya and Mialon (2010) who study multi-armed contests, in which players choose several efforts simultaneously. Our price/quality ratios can be seen as two kinds of effort that a player brings into a contest. However, the difference is that quality in our case cannot be changed at the contest stage and, thus, is a type, rather than a strategic choice. Moreover, quality is part of our contest designer’s preferences. Our paper is clearly related to the literature on (standard) scoring auctions.Footnote 8
Che (1993), Che and Gale (2003), and Asker and Cantillon (2008) study quasilinear scoring rules, the latter for multidimensional types. We are, however, concerned with price/quality ratios, i.e., a nonlinear scoring rule. Standard first- and second-score auctions for this case are analysed by Hanazono et al. (2013). In contrast to our paper, all of the above assume that the highest score wins with certainty, and, with the exception of Che and Gale (2003), that production takes place after the auction, i.e., the solution is driven by the players’ privately known cost functions which play no role in our first case of sunk production cost. In the second case that we analyse, there is a linear cost of production in addition to that sunk development cost. Our basic assumption that the buyer cannot perfectly determine the quality of an offered good or service seems to be natural and has been made previously in a different context, for instance, by Dranove and Satterthwaite (1992). In a theoretical and empirical study of procurement, Decarolis (2010) gives another justification for our approach that the highest or best bidder does not necessarily win. Under widely used rules, high bids are eliminated if they differ too much from some weighted average of bids in order to reduce the risk of defaults and renegotiation. This is equivalent to saying that the probability of winning for the best bidder is less than one.",4
19.0,1.0,Review of Economic Design,09 August 2014,https://link.springer.com/article/10.1007/s10058-014-0163-4,Stability and strategy-proofness for college admissions with an eligibility criterion,March 2015,Azar Abizada,Siwei Chen,,Female,Unknown,Unknown,Female,"We study college admissions. There are a group of students and a group of colleges. Each student has the outside option of not attending college, and has strict preferences over colleges and this option. Each college has a certain number of seats, and has the outside option of admitting no students. It has strict preferences over the sets of students and empty class. The college admissions problem is first studied by Gale and Shapley (1962) in their seminal paper in which they proposed the now well-known deferred-acceptance algorithm. Many variants and extensions of the original model with useful applications have been studied (Knuth (1976), Roth (1984), Gustfield and Irving (1989), Roth and Sotomayor (1990), Balinski and Sönmez (1999), Roth (2002), Abdulkadiroǧlu and Sönmez (2003), Abdulkadiroǧlu et al. (2005a, b), Roth (2008); and references therein). In countries like China, France, South Korea, Turkey, Greece, Azerbaijan, Albania, etc, college admissions are processed through a central education system. The central authority administers an exam. All students must take this exam. Once the exam scores are revealed, the central education system decides which students are eligible to apply to colleges. To be fair, the eligibility criterion should respect the students’ scores. It would be unfair to declare a student eligible if he has a lower score than some other student who is declared ineligible. In some countries like China, college-specific additional exams, ethnic group status, high school performance, and identity of the attended school may affect the total score of a student in college admissions. This adjustment of scores may be different from one college to other. We can think of these adjusted scores as a base for preferences of colleges over students. Therefore, the preferences of the colleges over students can differ from one another and do not necessarily depend fully on the scores of the students from the central exam. Adding eligibility to college admissions problems is nontrivial. A simple way to determine whether a person is eligible or not is to set a cutoff score in advance, which means that only people who score more than the cutoff are declared eligible. Then, eligible students are assigned to colleges in the same way as in the classical college admissions. However, this separation between eligibility and assignment has certain issues. A too high cutoff score may lead to a wasteful outcome (many seats remain empty due to lack of eligible students). A too low cutoff score may lead to an unfair outcome (a very low score student may be admitted into a more preferred college over a very high score student, due to the fact that preferences of colleges need not to be consistent with ranking of students’ scores). Therefore, we should endogenously determine the set of eligible students, and assign eligible students to colleges at the same time in appropriate manners.Footnote 1 Thus, a rule is a systematic way to separate the students as the eligibles and the ineligibles, and to assign eligible students to colleges. We model an eligibility criterion in college admissions. We study rules that respect this eligibility criterion. The idea of defining eligibility criteria is to create a fair way of assigning students to colleges. In the classical college admissions, scores that students obtain from a central exam do not play any role. A rule would only consider the preferences of the colleges and the students. As we mentioned above, the preferences of the colleges do not necessarily depend on the scores of the students. It is possible that a student with a low score is one of the most preferred students for a college for some other reasons. In this case, this low score student may be admitted into a college while the high score student is rejected. Therefore, the primary objective of separating students as the eligibles and the ineligibles is to restore fairness with respect to rankings of scores. Given scores, colleges may simply prioritize students according to their scores, i.e. a student with a higher score has higher priority at all colleges than a student with a lower score. Perach et al. (2008) studied the eligibility idea for dormitory assignment problem. They assumed that colleges don’t have preferences and that colleges prioritize students according to their scores.Footnote 2 This model is similar to the school choice model, where priorities of all schools are the same as the ranking of students’ scores. It was then generalized by Perach and Rothblum (2010). They introduce preferences of colleges, which do not necessarily depend on the ranking of students’ scores, i.e. there might be a college that prefers a student with a lower score to a student with a higher score. Thus, the extended model adds a new dimension “score” to the classical college admissions. To study this model, an eligibility criterion needs to be introduced. We continue working on the more general model, where colleges have preferences that are not necessarily consistent with scores. One of the critical assumptions in Perach et al. (2008) and Perach and Rothblum (2010) is that students have distinct scores from the exam. In China, every year approximately 10 million students take the college admissions exam.Footnote 3 In Turkey, this number is 1.5 million.Footnote 4 It is highly likely that some students will obtain the same score in the exam. Therefore, we should not ignore this possibility. We generalize the model studied in Perach and Rothblum (2010) by allowing students to obtain the same score. For two-sided matching problems, allowing indifference in preferences result in many complications. In most of the literature, it is assumed that agents have strict preferences over agents on the opposite side. Allowing students to obtain the same score in the entrance exam causes similar difficulties. The central notion for the college admissions problem is stability. It requires that each matched pair should prefer each other to the outside options, and requires that there should be no pair of a college and a student such that the student prefers this college to his assignment and either the college prefers this student to its outside option and has an empty seat, or is assigned a less preferred student. This action that stability tries to avoid is also called “blocking”. Since our model is different from the classical college admissions problem, we need to adjust the definition of stability in a reasonable way. We define three notions of stability. All of these notions first require that eligibility should be set in such a way that no seat of a college is wasted. In other words, if a rule declares a student ineligible, then there are no empty seats in any of the colleges that are better than outside option for him, and that find this student better than leaving the seat empty. We first start with the stability notion proposed by Perach and Rothblum (2010): there should be no pair of a college and an eligible student such that the student prefers this college to his assignment and either the college prefers this student to its outside option and has an empty seat, or is assigned a less preferred student. We define a second stability notion which is a slight weakening of the first one that allows the highest score of ineligible students and the lowest score of eligible students to be the same. In the Perach and Rothblum (2010) setup, since scores of the students are distinct, these two notions of stability coincide. They define the Generalized Dormitory Assignment Algorithm (G-DorAA), which satisfies these stability requirements. Both stability notions are reasonable modifications for our model. Each of them respects eligibility. These notions rely on scores of students only when they consider whether a student can be part of a blocking pair or not, and they rely on preferences of colleges when they consider whether a blocking pair is justified or not, i.e. the student in the blocking pair may prefer the college to his assignment, but is it true that the college finds this student more desirable than some student that is assigned to it? In the environment where college admissions are processed through a central exam, it is usually difficult for colleges to justify their preferences when the preferences are not consistent with the ranking of scores. Since not every country takes into account other factors as it is the case in China, this may become a problem. We define the following stability notion which respects not only eligibility but also score rankings: no pair of a college \(K\) and an eligible student \(A\) are such that the student \(A\) prefers the college \(K\) to his assignment, and the college \(K\) is assigned either a student \(B\) with a lower score than student \(A\)’s or some other student \(C\) with the same score as student \(A\), whom college \(K\) finds less desirable than \(A\). We are also interested in incentive compatibility which requires that no student can ever benefit by misrepresenting his preference. We look for rules that satisfy our stability notions and the incentive compatibility requirement. We first show that when we allow students to have same scores, no rule that satisfies the first stability notion is incentive compatible. Next, we define three new rules. Two of these rules are reasonable modifications of the G-DorAA defined in Perach and Rothblum (2010). The similarities and differences between them and G-DorAA will be explained in details later. Our first rule satisfies the first stability notion, but is not incentive compatible. Our second rule only satisfies the second (weaker) stability notion but not the first one, and is incentive compatible. Our last rule satisfies the third stability notion, and is also incentive compatible. We discuss other properties of our rules and the results of previous literature that extend to our model. The rest of the paper is organized as follows: in Sect. 2, we define the model. In Sect. 3, we define axioms. In Sect. 4, we provide rules. In Sect. 5 we state and prove our main results.",2
19.0,1.0,Review of Economic Design,15 January 2015,https://link.springer.com/article/10.1007/s10058-015-0166-9,Accuracy in contests: players’ perspective,March 2015,Mustafa Yildirim,,,Male,Unknown,Unknown,Male,"Despite being a multi-billion dollar industry,Footnote 1 European soccer has witnessed serious refereeing errors.Footnote 2 As technology has advanced, soccer fans have grown intolerant of these errors. Yet European soccer leagues and international organizations such as UEFA and FIFA seem reluctant to introduce any advanced technology that can minimize refereeing errors at a reasonable cost and with little disruption to the games. It was only in 2013 that several major soccer leagues, including the English and Dutch, decided to adopt a goal-line technology, while others, including the Spanish and German, have agreed to follow suit in the near future. In contrast to team-based European soccer, technology adoption appears timely in individualistic sports like tennis, athletics, horse racing, etc. The objective of this paper is to offer a political theory for this discrepancy. Specifically, we examine players’ incentives to support a new technology that improves contest accuracy. We show that these incentives may substantially differ from those of a contest designer because, conceivably, players care more about winning than increasing the aggregate effort to entertain the audience. Therefore, in contests where players retain a significant say in contest design, technology adoption may be delayed. Indeed, the English Premier League only recently adopted the new goal-line technology after votes from its twenty clubs.Footnote 3
 The same may be true for other contests where accuracy improvement is feasible. Today, some educational institutions use plus/minus grading instead of pure letter grading, as the former better differentiates students and is thus believed to enhance grading accuracy. One rationale for this is the contention that plus/minus grading is superior to letter grading, its less accurate counterpart, in motivating student achievement. While those not using it may have various reasons, such as financial and administrative costs, one reason could be student resistance. In fact, an ad-hoc committee on plus/minus grading established by Eastern Kentucky University in 2003 reported that more than half of the universities in Kentucky are still not using plus/minus grading for various reasons, one being student resistance.Footnote 4 Likewise, Dixon (2004) finds that the ratio of students choosing plus/minus grading over those choosing pure letter grading is 1–2, whenever they are given a choice. Our model is a standard Tullock (1980) contest with heterogeneous players. We define contest “accuracy” as the elasticity of “production” in the Tullock contest success function because higher elasticity implies that winning depends more on the effort than on “exogenous uncertainty.” In practice, accuracy can be improved through various mechanisms, depending on the context—for instance, in sports by allowing referees to get access to a better monitoring technology and in education by implementing a new grading system with a larger number of marking categories. We assume there are two types of players: those with a high marginal cost and those with a low marginal cost. Following Dixit (1987), we call the former type “ underdogs” and the latter type “ favorites.” In the unique equilibrium, we find that while the underdog’s payoff is always decreasing in accuracy, the favorite’s payoff is ambiguous. In particular, when the initial accuracy is very low, the favorite prefers higher accuracy if the cost advantage is significant and lower accuracy otherwise. This makes sense because when the cost advantage is small, players are essentially identical and therefore compete most fiercely with little change in their equilibrium probabilities of winning. The intuition for a significant cost advantage is similar. This result fits well with the adoption of technology in European soccer, in general. The soccer leagues of England (resp. EPL) and the Netherlands (resp. Eredevisie) have recently decided to implement a goal-line technology, while those of Germany (Bundesliga) and Spain (La Liga) have delayed their decision until 2015. In agreement with our prediction, EPL and Eredevisie are more heterogenous than Bundesliga and La Liga as evidenced by Fig. 1.Footnote 5
 Heterogeneity of six european soccer leagues Our paper falls into a large collection of literature on contest design. These include the following: the choice of prizes (Glazer and Hassin 1988; Moldovanu and Sela 2001), the choice of contest success function (Dasgupta and Nti 1998; Nti 2004), the number of contestants (Baye et al. 1993; Amegashie 1999), the structure of multi-stage contests (Gradstein 1998; Gradstein and Konrad 1999; Amegashie 2000; Yildirim 2005), and the structure of information (Wärneryd 2003, 2012).Footnote 6 This literature is mainly concerned about contest design aimed at maximizing total effort.Footnote 7 More specifically, only the designer’s preferences matter for the contest design. While the designer’s preferences also matter in our setting, the main focus is on the players’ preferences. In highlighting accuracy differences across contests, our paper relates to Alcalde Pérez and Dahm (2007), Che and Gale (1997), Dasgupta and Nti (1998), Michaels (1988), Nti (1999, 2004), and Wang (2010). In these papers, the contest designer chooses accuracy, the extent to which winning depends on effort rather than exogenous uncertainty. Moreover, these papers also model accuracy as elasticity of production in a standard Tullock contest as in our paper. To these, Alcalde Pérez and Dahm (2007), Che and Gale (1997), Dasgupta and Nti (1998), Nti (1999, 2004), and Wang (2010) introduce heterogeneity, while Dasgupta and Nti (1998) and Michaels (1988) focus on homogeneous contests. Again, none of these papers investigate players’ preferences for accuracy. The remainder of the paper is organized as follows. The basic model is presented in the next section, followed by the equilibrium characterization in Sect. 3. Sections 4 and 5 provide findings regarding players and the designer who is concerned about maximizing total effort, respectively. Section 6 discusses the findings. Section 7 extends the model to a pairwise contest and screening, and Sect. 8 concludes.",3
19.0,2.0,Review of Economic Design,14 February 2015,https://link.springer.com/article/10.1007/s10058-015-0168-7,Revising claims and resisting ultimatums in bargaining problems,June 2015,Johannes Spinnewijn,Frans Spinnewyn,,Male,Male,Unknown,Male,"Negotiations often share the following two features. First, players revise initial claims in order to reach a compromise. Their ability to make revisions depends on the context of the negotiations and may differ among players. Second, concessions may be induced by the threat of an ultimate take-it-or-leave-it offer. However, negotiators discourage such uncompromising behavior by adopting a firm posture—threatening to walk away from negotiations without agreement—when facing such an ultimatum. These two features are extensively discussed in the negotiation literature (Sebenius 1992; Lewicki et al. 1994) and also appear in practical guides for negotiators, as in the defense procurement and acquisition guidelines by the US Department of Defense:Footnote 1 “Aim high” but “Give yourself room to compromise” and “Be willing to walk away from or back to negotiations”. In the bargaining literature, Harsanyi (1977) justified the solution of Nash (1950) by comparing the risk limits of players in the pursuit of their claims. A player’s risk limit is the highest probability of disagreement that he would accept in the pursuit of his claim in an ultimatum, when accepting his opponent’s claim is the alternative. The player with higher risk limit is in a weak bargaining position and is more likely to accept his opponent’s claim. Since a lower claim decreases the own risk limit and increases the opponent’s risk limit, players avoid a weak bargaining position by exhibiting restraint in the formulation of their claims. Risk limits are equalized if each player claims his payoff in the Nash solution. 
Moulin (1984) justified the solution of Kalai and Smorodinsky (1975) in an auction in which each player bids a probability of disagreement when an uncompromising opponent pursues his dictatorial outcome in an ultimatum. The player with the lower bid is given the advantage to propose any feasible utility allocation as a compromise. Hence, the competition for first-mover advantage rewards restraint in the choice of resistance probabilities against uncompromising behavior. In a maxmin equilibrium of the bidding strategies, both players commit to equal resistance probabilities which eliminates first-mover advantage. They both propose the Kalai–Smorodinsky solution in which they reduce their claims in the same proportion. In particular, this solution solves the trade-off for each player between the commitment to higher resistance in order to deter uncompromising behavior and the commitment to lower resistance in order to obtain a leadership position. In his justification of the Nash solution, Harsanyi assumed that claims cannot be revised, leaving little room to compromise. In his justification of the Kalai–Smorodinsky solution, Moulin assumed that players pursue their dictatorial outcomes in an ultimatum, excluding restraint in the formulation of claims. The two approaches motivate the analysis of a mechanism with four stages showing how avoidance of a weak bargaining position and competition for first-mover advantage interact. Players start by making claims, as in Harsanyi. In the second stage, players bid resistance probabilities, as in Moulin. Leadership is acquired by the player with the lowest bid. In the third stage, the leader proposes a compromise within the set of feasible compromises which depends on his claim but remains beyond his control in all other respects. In the final stage, the follower accepts or rejects the compromise. If he rejects, then he obtains his claim in an ultimatum unless he meets resistance to which the leader is committed by the second stage; the negotiations end in disagreement with the leader’s resistance probability. The single distinguishing feature of these games is the extent to which claims can be subsequently revised. The revision procedure defines the Pareto-efficient maximal revision of each player’s claim. The room to compromise is the gap between the maximal utility which a player can give to his opponent in the maximal revision and in the pursuit of his claim. The Nash solution and the Kalai–Smorodinsky solution are implemented in subgame-perfect equilibrium in the extreme cases excluding or admitting all revisions respectively. The main contribution of this paper is the highlighting of when and how the strategic justifications of Moulin and Harsanyi interact for intermediate revision procedures considered in the negotiation literature. The key in this interaction is the new concept of the extended Nash product of a player’s claim, which multiplies his claim with the opponent’s utility in his maximal revision. The player with the larger extended Nash product of his claim is the strong player as he needs a lower resistance probability to impose his maximal revision which avoids an ultimatum. In particular, players face a trade-off between claiming more so as to achieve more in an ultimatum and claiming less so as to obtain a strong bargaining position. This allows us to analyze how the aforementioned features in the negotiations literature play out in equilibrium. Players should not only aim high when formulating claims, but also leave sufficient room to compromise in order to obtain a strong bargaining position. The paper shows that in equilibrium there is interaction between both strategic justifications in intermediate revision procedures, with two exceptions. Players restrain their claims which makes them equally strong, as in Harsanyi, but at the same time they restrain their resistance so that their concessions stand in the same proportion to their claims, as in Moulin. We distinguish between two cases. In the first case, maximal revisions are incompatible. Competition for the strong bargaining position induces restraint in the formulation of claims, unless one player has a claim which puts him in a strong bargaining position for all claims of the other player. The strong player imposes his maximal revision for the largest of such claims in the first exception. Otherwise, at least one of the players gains by reducing his claim, which closes the gap between the maximal revisions or makes them compatible. In the second case, maximal revisions are compatible. The proportional solution in which the players’ utilities stand in equal proportion to their claims is a feasible compromise. The competition for leadership equalizes the players’ resistance probabilities by adopting Moulin’s maxmin bidding strategies. By the monotonicity of the proportional solution for strictly compatible maximal revisions, each player gains by increasing his claim, unless claims are maximal. Players agree on the Kalai–Smorodinsky solution in the second exception. Otherwise, none of the players can gain by changing his claim only when maximal revisions meet for claims with equal extended Nash products. The maximal revisions of the equilibrium claims meet in the Kalai–Smorodinsky solution for the bargaining problem with these claims as ideal points. The mechanism underlines that room to compromise is essential for a strong bargaining position, as recommended in the negotiation literature. When a negotiator is able—for a claim below his maximal claim—to increase his opponent’s payoff in his maximal revision, larger extended Nash products improve his bargaining position allowing for a better deal. A negotiator gains in equilibrium by facing fewer restrictions regarding the revisions of all claims below his maximal claim. Still, such exogenous restrictions can be important in particular contexts. For example, restrictions on revisions can be explicitly specified in the mandate given to the negotiator by his principal or arise from costs of revising initial plans. The restrictions may also arise from unfulfilled expectations raised by the initial claims or from aversion to making concessions. In these examples, one expects better agreements for negotiators who do not fear to disappoint their principals or suppress their frustration. Our analysis sheds light on this, evaluating more generally the impact of revision procedures on the bargaining outcome. The mechanism also clarifies the role of ultimatums with endogenously chosen risk of disagreement needed for imposing a compromise. This is further illustrated for unrestricted revisions in the alternating-offer game (Rubinstein 1982). In each round the responder can stop negotiations in an ultimatum and the proposer needs time to build resistance in order to deter such ultimatum for a better deal. The introduction of ultimatums moves the equilibrium outcome away from the Nash solution—the equilibrium solution of the alternating-offer game with equal waiting times—towards the Kalai–Smorodinsky solution—the equilibrium solution of the four-stage mechanism with unrestricted revisions. 
Related literature According to Nash (1953), the relevance of a solution concept is enhanced if one arrives at it from very different points of view. The Nash program, as reviewed in Thomson (2010), attempts to complement the axiomatic properties of solution concepts with non-cooperative foundation. While Harsanyi (1977), Moulin (1984), Binmore et al. (1986) and Howard (1992) implement the Nash program for a single bargaining solution, we achieve implementation for a family of solutions in subgame-perfect equilibrium, as Miyagawa (2002) and Anbarci and Boyd (2011). Miyagawa’s mechanism implements any solution that maximizes a welfare function belonging to a set of quasi-concave functions, including the Nash and Kalai–Smorodinsky solution. The second player counters the offer of the first player, but this offer is restricted to provide the same aggregate welfare as the first offer. In the mechanism of Anbarci and Boyd, compatible utility allocations are implemented in a first stage and incompatible utility allocations are implemented with equal probability in a second stage, unless there is an exogenously imposed probability of disagreement. The Kalai–Smorodinsky solution is the unique robust solution which both players demand above a threshold. There is no general robustness ranking for other solutions. We propose a mechanism with endogenously chosen probability of disagreement which occurs only off the equilibrium path and which induces restraint in the claims depending on the revision procedure. Interestingly, we find the Nash and Kalai–Smorodinsky solution for two opposite extremes. By considering intermediate revision procedures, we are able to compare and deepen our insight in Harsanyi’s and Moulin’s seminal contributions to the Nash program. 
Schelling (1956) discusses take-it-or-leave-it offers and commitments as strategy devices. Kahneman and Tversky (1995) show that loss aversion appears as concession aversion in the context of negotiations. The experimental literature shows that people accept losses by rejecting unfair outcomes in ultimatums (Camerer 2003). Punishing unfair treatment is rationalized in Fehr and Schmidt (1999). We refer to this literature to justify the commitment of accepting the loss of disagreement with positive probability in an ultimatum. The paper is organized as follows. The next section defines the bargaining problem, the four-stage mechanism and the revision procedures. Section 3 analyzes the extreme revision procedures allowing no or all revisions. Section 4 characterizes the solution for intermediate revision procedures. We provide examples of revision procedures in Sect. 5. Before concluding, the robustness of the mechanism is analyzed in Sect. 6. The complete description of the subgame-perfect equilibrium and proofs are given in “Appendix”.",2
19.0,2.0,Review of Economic Design,07 March 2015,https://link.springer.com/article/10.1007/s10058-015-0169-6,Optimal design of scoring auctions with multidimensional quality,June 2015,Takeshi Nishimura,,,Male,Unknown,Unknown,Male,"Auction rules of public and private procurement have changed from one-dimensional bidding to multidimensional bidding. In contrast with the former traditional rule in which each supplier submits only a price bid, the latter auction rule requires suppliers to offer not only price but also quality they promise to ensure. For instance, in the EU, Article 53 of Directive 2004/18/EC specifies the “Most Economically Advantageous Tender.” In the tendering procedure, procurement authorities award contracts based on various criteria such as price, aesthetic characteristics, after-sales service, delivery date, and so on. The design of multidimensional auctions is a matter of great concern to the procurement authorities, reflecting the fact that governments in OECD member countries spend on average 12 % of their GDP on public procurement (OECD 2012). Also, many providers of eProcurement solutions such as Perfect Commerce have developed auction software which supports private companies’ decisions on supplier selection based on quality attributes as well as price (see McMillan 2003). The purpose of this paper is to study the optimal design of multidimensional auctions in an environment with various quality attributes. The essential element of the multidimensional auction is a scoring rule. The rule, which evaluates suppliers’ offers and gives them scores, should be carefully designed because it considerably affects suppliers’ decisions what offers to make in an auction. There are many examples of scoring rules adopted by state departments of transportation in the United States: “A + B bidding” (Arizona, etc.), “weighted criteria” (Delaware, Idaho, Massachusetts, Oregon, Utah, Virginia, etc.), “adjusted bid” (Arizona, Maine, Michigan, North Carolina, South Carolina, South Dakota, etc.), and so on (see Molenaar and Yakowenko 2007 for the detail). For instance, the rule of “weighted criteria” puts a weight on each of price and quality attributes (e.g., delivery date, safety level) and evaluates each attribute individually. A total score of each offer is a weighted sum of subscores and a supplier with the highest total score wins a contract. In this paper, we extend the model of Che (1993) by allowing for multidimensional quality. Che shows that a scoring auction with a properly designed scoring rule implements a buyer’s optimal mechanism. Although many scoring rules used in practice apply multiple quality criteria, there is no theoretical study investigating what kind of scoring rule implements the optimal mechanism in an environment where quality is multidimensional. In such an environment, it is natural to expect that the interaction among quality attributes in the supplier’s production cost and the buyer’s valuation significantly affects the optimal form of a scoring rule. These observations motivate our analysis. The main focus of the paper is to elucidate conditions under which the optimal scoring rule can take or cannot take the familiar form of an additively separable rule such as “weighted criteria.” In this paragraph, for ease of exposition, we suppose that quality consists of two attributes. Our results show that the optimal scoring rule can be additively separable in the two attributes if the degree of cost substitutability is nonpositive (i.e., the two attributes are cost complements), and it cannot be additively separable if the degree is sufficiently high. The intuition follows from two observations. The first observation is that the buyer’s optimal mechanism requires a more efficient supplier to achieve higher levels of all quality attributes. A key assumption is that a more efficient supplier has a superior technology of increasing the levels of all quality attributes. With this assumption, a mechanism which requires a more efficient supplier to achieve lower levels of all quality attributes is not incentive compatible because an efficient supplier pretends to be inefficient so as to improve quality, or an inefficient supplier pretends to be efficient so as to lower quality, for the purpose of increasing expected profits. Some additional regularity assumptions also guarantee that it is suboptimal for the buyer to assign “unordered” profiles of quality attributes to suppliers (i.e., to assign an efficient supplier a profile of high and low attributes and an inefficient supplier a profile of low and high attributes) although such a mechanism may be incentive compatible. The second observation is that, in a scoring auction, any supplier chooses quality so as to maximize a quality score minus his production cost. Then, the point is that the additively separable scoring rule exhibits no complementarity between the two attributes, and thus, roughly speaking, the implementation of the optimal mechanism via the scoring rule requires some degree of cost complementarity between the attributes to incentivize a more efficient supplier to offer higher quality. Therefore, if the degree of cost substitutability is positive (i.e., the two attributes are cost substitutes), then it is better for the buyer to adopt a scoring rule which has some degree of complementarity between the quality attributes. After the seminal work of Che (1993), Branco (1997) extended the result to an environment where each supplier’s production cost has a common-cost component, so that his cost is correlated with the other suppliers’ costs. Che and Branco assume that both quality and each supplier’s type are one-dimensional. David et al. (2006) characterize a scoring rule which maximizes the buyer’s expected utility among rules of “weighted criteria” in an environment where quality is multidimensional but both the value and cost functions are additively separable in quality attributes. In contrast, Asker and Cantillon (2008) consider a fully general environment. They allow that both quality and each supplier’s type are multidimensional. The main results of Asker and Cantillon are the characterization of equilibrium bidding strategy and the expected utility equivalence between some formats of scoring auction. They also show that the scoring auction outperforms some other mechanisms from the buyer’s viewpoint. On the other hand, they have not investigated whether a scoring auction can implement the optimal mechanism. The likely reason is that it is extremely difficult to characterize the optimal mechanism when the supplier has multidimensional private information. Asker and Cantillon (2010), however, characterize the optimal mechanism in a specific environment. In the environment, quality is one-dimensional, each supplier’s type consists of two parameters (fixed cost and marginal cost) and each parameter is a binary random variable. They show that the scoring auction yields a performance close to that of the optimal mechanism, taking a numerical simulation approach. All of the above studies including the current one focus on quasi-linear scoring rules. Under the rule, a total score is given by a quality score minus price. A typical example of the quasi-linear rule is “weighted criteria.” Recently, Hanazono et al. (2013), Wang and Liu (2014), and Dastidar (2014) have analyzed non-quasilinear scoring rules which are nonlinear in price, and have studied the equilibrium bidding behavior. In addition to these theoretical studies, there is some experimental evidence supporting the high performance of scoring auctions for the buyer compared to that of traditional price-only auctions (Bichler 2000; Chen-Ritzo et al. 2005). Our results provide useful guidelines on how to design a scoring rule to practitioners. Additively separable scoring rules are pervasive in both public and private procurement. According to Beil and Wein (2003), most electronic request for quotation (eRFQ) software packages assume that scoring rules are additively separable in quality attributes, and a Chief Technology Officer of Frictionless Commerce Incorporated who was seeking help in designing a multiattribute mechanism felt that scoring rules which are not additively separable would be too arduous for industrial implementation. Our results show practitioners how the attractiveness of additively separable scoring rules depends on the degree of cost substitutability between quality attributes. Moreover, we will see in an example how to compute the buyer’s loss from using an additively separable scoring rule. This would help practitioners to judge whether they should adhere to additively separable scoring rules or not. This paper is organized as follows. Section 2 presents the model which generalizes that of Che (1993). Section 3 derives the equilibrium bidding strategy, and shows that the scoring auction can implement the optimal mechanism. Section 4 elucidates conditions under which the optimal scoring rule can be or cannot be additively separable in quality attributes, and investigates how it depends on the buyer’s weight parameter on suppliers’ profits and the number of suppliers. Section 5 concludes. All proofs are in the “Appendix”.",14
19.0,2.0,Review of Economic Design,21 March 2015,https://link.springer.com/article/10.1007/s10058-015-0171-z,Cournot competition under uncertainty: conservative and optimistic equilibria,June 2015,M. A. Caraballo,A. M. Mármol,E. M. Buitrago,Unknown,Unknown,Unknown,Unknown,,
19.0,2.0,Review of Economic Design,12 February 2015,https://link.springer.com/article/10.1007/s10058-015-0167-8,A characterization of the asymmetric Nash solution,June 2015,Shiran Rachmilevitch,,,Female,Unknown,Unknown,Female,"A bargaining problem is a pair consisting of a set of payoff vectors—the feasible set—and a specific point in that set, the disagreement point. The interpretation is this: \(n\) players need to choose a point (a utility allocation) from the feasible set; unanimously agreeing on \(x\) provides player \(i\) with \(x_{i}\) utils, while failing to reach an agreement leads to the implementation of the disagreement point’s payoffs. A bargaining solution is a selection—a rule that selects a payoff vector from the feasible set of every problem. 
Nash (1950) introduced four axioms: weak Pareto optimality, independence of equivalent utility representations, symmetry, and independence of irrelevant alternatives. He showed that they characterize what is now known as the Nash solution—the maximizer of the “Nash product”. Subsequently, Roth (1977) showed that strong individual rationality can replace the weak Pareto axiom in Nash’s characterization. Kalai (1977) showed that the deletion of symmetry from Roth’s axiom list results in a characterization of the asymmetric Nash solutions—the family of solutions each of which is a maximizer of a “generalized Nash product”. Anbarci and Sun (2011, henceforth AS) tightened the theorems of Nash and Roth: they showed that weak Pareto optimality can be replaced in Nash’s theorem by weakest collective rationality—an axiom which is weaker than both weak Pareto optimality and strong individual rationality. In contrast to Kalai’s generalization of Roth’s theorem, deleting symmetry from the axiom list of Nash or AS does not result in a characterization of the asymmetric Nash solutions. The contribution of the current paper is in formulating an axiom which is similar to the one of AS and showing that when it is combined with independence of irrelevant alternatives and independence of equivalent utility representations, a characterization of the asymmetric Nash solutions obtains. This axiom, moderate collective rationality, is weaker than strong individual rationality but stronger than weakest collective rationality.Footnote 1
",2
19.0,3.0,Review of Economic Design,30 May 2015,https://link.springer.com/article/10.1007/s10058-015-0174-9,Commitment without reputation: renegotiation-proof contracts under asymmetric information,September 2015,Emanuele Gerratana,Levent Koçkesen,,Male,Male,Unknown,Male,"Could an incumbent firm deter entry by contracting with third parties, such as a bank or a labor union? Could a government credibly commit to fiscal policy through a contract with a supranational body? More generally can contracts with third parties change the outcome of a game to the advantage of the contracting player? When contracts are non-renegotiable, the answer to this question is in general yes.Footnote 1 An incumbent can prevent entry by writing a contract that specifies a large enough payment to a third party if it fails to punish the entrant. A country can obtain entry into European Union or better borrowing terms by writing a contract that specifies a fine if it runs high budget deficits. Unfortunately, the very same reason that makes contracts useful as commitment devices, also makes them susceptible to renegotiation and calls their credibility into question. If entry occurs, for example, the incumbent and the third party have an incentive to renegotiate the contract because punishing the entrant reduces the total surplus available to them. In fact, this is true more generally. If renegotiation takes place without any frictions, the contracting party will always best respond to other players, nullifying the commitment power of contracts. This problem is well known and has led to well founded skepticism about the robustness of results obtained with non-renegotiable contracts. 
Dewatripont (1988) was the first to show that this skepticism may be unwarranted in certain situations. He analyzed an entry game and showed that the incumbent can deter entry even with renegotiable contracts, as long as there is asymmetric information between the incumbent and the third party at the renegotiation stage and the contracts are publicly observable. Dewatripont’s result is important because it reinstates the commitment value of contracts within a commonly used model. In this paper, we follow Dewatripont’s lead and extend his analysis beyond the entry model to general extensive form games with incomplete information. We show that his message applies more generally. As long as one can find a third party over whom the contracting party has an informational advantage, contracts can be used as commitment devices even when they are renegotiable. More specifically, our analysis achieves three main objectives. First, we can use our results to determine whether existing results on commitment through contracts are robust to renegotiation. Second, we can use our results as a guide to “design” renegotiation-proof contracts that achieve some strategic objective. Third, we can characterize the outcomes that can be supported with renegotiable contracts for both observable and unobservable contracts. In the main body of the paper we analyze a two-stage game, which we call the original game, and allow the second mover to write a contract with a neutral third party. We model renegotiation as a game form: The contracting party (the incumbent or the government), who has private information about the state of the world, can make a renegotiation offer to the (uninformed) third party (the labor union or the supranational body). This game form is different from the one chosen by Dewatripont and it simplifies the characterization of renegotiation-proof contracts.Footnote 2 We analyze the renegotiation-proof Perfect Bayesian equilibrium of the game with contracts, and this leads us to the following definition of renegotiation-proof contracts (see Definition 5): A contract is renegotiation-proof if it is optimal for the third party to reject any renegotiation offer that is found profitable by the contracting party. This can happen in equilibrium if the third party puts a high probability on a “blocking type” of the contracting party, i.e., a type which, under the renegotiated contract, would not transfer more to the third party than under the old contract. However, using this definition directly is not very easy. In Sect. 3 we present more operational characterizations of renegotiation-proof strategies, which are adaptations of some recent results in Gerratana and Koçkesen (2012) to our setting. In Sect. 4 we characterize the equilibrium outcomes of games with contracts. We allow contracts to be observable or unobservable (by other players) and renegotiable or non-renegotiable. If contracts are observable and non-renegotiable, then the contracting player obtains her Stackelberg payoff (of the original game), i.e., the best payoff that she can achieve given that she plays an incentive compatible strategy and the first mover plays a best response. If they are unobservable and non-renegotiable, then any Bayesian Nash equilibrium outcome of the original game in which the second mover plays an incentive compatible strategy can be supported. We show that the possibility of renegotiation affects the games with observable and with unobservable contracts in the same way: In both cases the only additional restriction is that the contracting player’s strategy is renegotiation-proof. In Sect. 5, we illustrate our results by applying them to a quantity competition and entry-deterrence game. This game is the canonical model in which the second mover has a strategic disadvantage and hence may benefit from commitment via third-party contracts. Furthermore, it allows us to compare our findings with those of Dewatripont (1988) (see Proposition 9 in Sect. 5) and identify the contribution of the current paper over Gerratana and Koçkesen (2012), which analyzes the same question in a different model (see Sect. 1.1). We show that, when applied to this model, renegotiation-proofness imposes a very simple restriction: The harshest credible punishment that the incumbent (follower) can inflict upon the entrant (leader) is to best respond in the worst state of the world, i.e., when the incumbent’s unit cost is highest, and flood the market in all other states. In fact, this is true in a more general class of games in which the first mover’s payoff is monotone increasing (or decreasing) in the second mover’s action. For this class of games, which includes many interesting economic environments such as sequential quantity and price competition, monopolistic screening, and ultimatum bargaining, renegotiation-proofness imposes the same simple restriction: The harshest credible punishment that player 2 can inflict upon player 1 always requires player 2 to best respond after the highest (or lowest) state of the world and to choose the lowest (highest) action for all the other states. We further discuss the intuition behind this result in Sect. 5 after analyzing the quantity competition game and refer the reader to Gerratana and Koçkesen (2013) for a complete analysis. In Sect. 6, we discuss how in results of Sect. 3 can be generalized in several dimensions. These generalizations are useful in applications where the third party is not neutral or those that cannot be modeled as two-stage games. This paper contributes to the literature on the the strategic effects of third-party contracts. The role of third-party contracts are maximal when they are both observable and non-renegotiable. In fact, there are several “folk theorem” type results for different classes of games (see Fershtman et al. 1991, Polo and Tedeschi 2000, and Katz 2006). The effects of unobservable and non-renegotiable third-party contracts are also well-understood: Nash equilibrium outcomes of a game with and without third-party contracts are identical (Katz 1991). In fact, all (and only) Nash equilibrium outcomes of the original game can be supported as a sequential equilibrium outcome of the game with unobservable and non-renegotiable contracts (Koçkesen and Ok 2004, Koçkesen 2007).Footnote 3
 The strategic role of renegotiable contracts is less understood and the pioneering contribution is provided by Dewatripont (1988). As we explained above, one contribution of our paper is to show that commitment effects exist in arbitrary two-stage games. Furthermore, we show that commitment effects exist even if contracts are unobservable and hence they exist also if we allow the contracts to be renegotiated immediately after they are signed.Footnote 4
 
Caillaud et al. (1995) analyzes a game between two principal-agent hierarchies. In the first stage of their game each principal decides whether to publicly offer a contract to the agent; in the second stage each principal offers a secret contract to the agent, which, if accepted, overwrites the public contract that might have been offered in stage 1; in the third stage each agent receives a payoff relevant information, decides whether to quit, and if he does not quit, he plays a normal form game with the other agent. Their main question is whether there exist equilibria of this game in which the principals choose not to offer a public contract in stage 1. If the answer to this question is no, then the interpretation is that contracts have commitment value. They show that contracts have commitment value if the market game stage is of Cournot type, but not if it is of Bertrand type. Moreover, when contracts have commitment value, they reduce the payoff of the contracting parties. The received message of Caillaud et al. (1995), in comparison with Dewatripont (1988), is that by allowing secret renegotiation, Caillaud et al. (1995) enhanced the realism of the model and clarified the role of strategic contracting.Footnote 5 The crucial difference between our model and Caillaud et al. is that they assume that agents play a simultaneous move game (and principals offer contracts to the agents simultaneously) whereas we focus on sequential move games. Therefore, one contribution of our paper is to show that the differences between the results of Caillaud et al. (1995) and Dewatripont (1988) do not depend on allowing secret renegotiation right after the contracts are signed, but instead depend on the fact that Dewatripont (1988) studies a sequential move game while Caillaud et al. (1995) a simultaneous move game. The current paper follows Gerratana and Koçkesen (2012) (from now on GK (2012)), which also studies the effects of renegotiation-proof third-party contracts in two-stage games. Some aspects of the analysis in the two papers are similar and use similar tools, namely theorems of the alternative. Indeed, results on renegotiation-proof strategies in Sect. 3 are exact analogs of their counterparts in GK (2012). However, the games to which these results are applied are completely different: In GK (2012), the original game is with complete information and the asymmetry of information between the contracting player (second mover) and the third party is due to the assumed inability of the third party to observe the action of the first mover. The current paper considers games in which the contracting player has private information and therefore there is no need to add another layer of asymmetric information between that player and the third party. The modeling choice in the current paper is more standard and allows us to determine whether some of the well-known results on commitment through contracts are robust to renegotiation. This is not possible in GK (2012), as it considers original games with complete information. Perhaps the best way to illustrate the differences between the current paper and GK (2012) is to consider an application that we will analyze in more detail in Sect. 5. Consider a Stackelberg competition in which firm 1 moves first by choosing an output level \(q_{1}\in Q_{1}\) and firm 2, after observing \(q_{1}\), chooses its own output level \(q_{2}\in Q_{2}\). Inverse demand function is given by \(P(q_{1},q_{2})=\max \{0,\alpha -q_{1}-q_{2}\}\), where \(\alpha >0\), and we assume \(Q_{i}\) is a rich enough finite subset of \({\mathbb {R}}_{+}\) whose largest element is \(\alpha \).Footnote 6 Cost function of firm 1 is \(C_{1}(q_{1})=cq_{1}\), where c is common knowledge, whereas the cost function of firm 2 is \(C_{2}(q_{2})=\theta q_{2}\). We assume that \(\theta \in \{\theta ^{1}, \theta ^{2},\cdots ,\theta ^{n}\}\), where \(n\ge 2\), is private information of firm 2 and \(\theta ^{1}<\theta ^{2}<\cdots <\theta ^{n}\). Firm 1 believes that the probability of \(\theta ^{i}\) is given by \(p(\theta ^{i})\) and for ease of exposition we assume that expected value of \(\theta \) is equal to c. The profit function of firm i is given by \(\pi _{i}(q_{1},q_{2},\theta )=P(q_{1},q_{2})q_{i}-C_{i}(q_{i})\) and we assume that both firms are profit maximizers. As we have already mentioned, the first difference is that, while in GK (2012) the original game is a game with complete information, i.e., firm 2’s cost function is common knowledge, in the current paper it is firm 2’s private information. Secondly, and more interestingly, this difference in modeling has important consequences with regard to the effects of renegotiation. In the class of games for which GK (2012) obtain sharp results, renegotiation is “irrelevant,” that is, the outcomes of games with unobservable and non-renegotiable contracts are robust to the introduction of renegotiation (see Corollary 1 in GK 2012). This class contains the complete information version of the quantity competition game introduced above. GK (2012) shows that in this game an outcome can be supported with unobservable third-party contracts if and only if firm 1’s profit is non-negative, its output is at least as high as the Cournot Nash equilibrium output, and the follower’s output is a best response to that, irrespective of whether the contracts are non-renegotiable or renegotiable. In particular, entry cannot be deterred with either non-renegotiable or renegotiable contracts (see Section 6.1 in GK 2012). This is not true in the current paper. Section 5 shows that any outcome in which firm 1 obtains non-negative profit and firm 2 best responds to firm 1’s quantity choice can be supported with unobservable and non-renegotiable contracts. In particular entry can be deterred. If, however, the contracts are unobservable and renegotiable, then under certain conditions (Condition (5) in Sect. 5) the lower bound on firm 1’s profit is positive, which implies that entry cannot be deterred with renegotiable contracts. In other words, renegotiation, in general, has a bite. Therefore, one contribution of our paper is to show that the “irrelevance of renegotiation” result obtained in GK (2012) is specific to games with complete information and does not come from the definition of renegotiation-proofness. Thirdly, GK (2012) considers only the case of unobservable contracts, whereas the current paper analyzes observable contracts as well. This allows us clarify precisely the distinction between observable and unobservable contracts with and without renegotiation. For example, in the quantity competition game, entry-deterrence is the unique equilibrium outcome under observable and non-renegotiable contracts, while it is the unique outcome under renegotiable contracts if and only if condition (5) in Sect. 5 is not true. If, on the other hand, contracts are unobservable, the set of equilibrium outcomes is larger. Finally, we show that it is possible to extend the main results on characterization of renegotiation-proof contracts and strategies in GK (2012) in two non-trivial directions (see Sect. 6). The first extension is to arbitrary extensive form games that satisfy an increasing differences property, examples of which include the chain store and repeated bargaining games. The second extension is to allow for non-neutral third parties, i.e., a third party who cares not only about the transfer he receives (or pays out) but also about the outcome of the original game. One could think of many situations in which this would be a more suitable assumption than a neutral third party. For example, the European Union, in its contractual relationships with Airbus, would be interested in the outcome of the competition between Airbus and Boeing.",2
19.0,3.0,Review of Economic Design,09 July 2015,https://link.springer.com/article/10.1007/s10058-015-0177-6,Dynamic mechanism design with interdependent valuations,September 2015,Swaprava Nath,Onno Zoeter,Christopher R. Dance,Unknown,Male,Male,Male,"Organizations often face the problem of executing a task for which they do not have enough resources or expertise. It may also be difficult, both logistically and economically, to acquire those resources. For example, in the area of healthcare, it has been observed that there are very few occupational health professionals and doctors and nurses in all specialities at the hospitals in the UK (Nicholson 2004). With the advances in computing and communication technologies, a natural solution to this problem is to outsource the tasks to experts outside the organization. Hiring experts beyond an organization was already in practice. However, with the advent of the Internet, this practice has extended even beyond the international boundaries, e.g., some US hospitals are outsourcing the tasks of reading and analyzing scan reports to companies in Bangalore, India (Associated-Press 2004). Gupta et al. (2008) give a detailed description of how the healthcare industry uses the outsourcing tool. The organizations where the tasks are outsourced (let us call them vendors) have quite varied efficiency levels. For tasks like healthcare, it is extremely important to hire the right set of experts. If the efficiency levels of the vendors and the difficulties of the medical tasks are observable by a central management (controller), and if the efficiency levels vary over time according to a Markov process, the problem of selecting the right set of experts reduces to a Markov decision problem (MDP), which has been well studied in the literature (Bertsekas 1995; Puterman 2014). Let us call the efficiency levels and task difficulties together as types of the tasks and resources. However, the types are usually observed privately by the vendors and hospitals (agents), who are rational and intelligent. The efficiencies of the vendors are private information of the vendors (depending on what sort of doctors they hire, or machines they use), and they might misreport this information in order to win the contract and to increase their net returns. At the same time the difficulty of the medical task is private to the hospital, and is unknown to the experts. A strategic hospital, therefore, can misreport the task difficulty to the hired experts as well. Hence, the asymmetry of information at different agents’ end transforms the problem from a completely or partially observable MDP into a dynamic game among the agents. Motivated by examples of this kind, in this paper, we analyze them using a formal mechanism design framework. We consider only cases where the solution of the problem involves monetary compensation in quasi-linear form. The reporting strategy of the agents and the decision problem of the controller is dynamic since we assume that the types of the tasks and resources are varying with time. In addition, the above problem has two characteristics, namely, interdependent values: in a selected team of agents, the valuation of an agent depends not only on her own skills but also on the skills of other selected agents, and exchange economy: a trade environment where both buyers (task owners) and sellers (resources) are present. In this paper, the theme of modeling and analysis would be centered around the settings of task outsourcing to strategic experts. We aim to have a socially efficient mechanism, and at the same time, that would demand truthfulness and voluntary participation of the agents. The above properties have been investigated separately in literature on dynamic mechanism design. Bergemann and Välimäki (2010) have proposed an efficient mechanism called the dynamic pivot mechanism, which is a generalization of the Vickrey–Clarke–Groves (VCG) mechanism (Vickrey 1961; Clarke 1971; Groves 1973) in a dynamic setting, and serves to be truthful and efficient. Athey and Segal (2007) consider a similar setting with an aim to find an efficient mechanism that is budget balanced. Cavallo et al. (2006) develop a mechanism similar to the dynamic pivot mechanism in a setting with agents whose type evolution follows a Markov process. In a later work, Cavallo et al. (2009) consider periodically inaccessible agents and dynamic private information jointly. Even though these mechanisms work for an exchange economy, they have the underlying assumption of private values, i.e., the reward experienced by an agent is a function of the allocation and her own private types. Mezzetti (2004, 2007), on the other hand, explored the other facet, namely, interdependent values, but in a static setting, and proposed a truthful mechanism. The mechanism proposed in these two papers use a two-stage mechanism, since it is impossible to design a single-stage mechanism satisfying both truthfulness and efficiency even for a static setting (Jehiel and Moldovanu 2001). However, the mechanism provides a weak truthfulness guarantee in the second stage of the game. A similar result in the setting of interdependent valuations with static types by Nath and Zoeter (2013) ensures that the truthfulness guarantee is strict. However, since both Nath and Zoeter (2013) and Mezzetti (2004) consider mechanisms that use two stages of information realization- in the first stage the types are realized and the allocation is decided, and in the second stage the valuations are realized by the agents and payments are decided - both of them require attention on how the information is revealed to the agents. In this paper, we follow an approach similar to Nath and Zoeter (2013) that guarantees strict truthfulness. However, the equilibrium concept used here is ex-post Nash because we assume agents play in an incomplete information setting, and contrast this with the mechanism of Mezzetti (2004). We also discuss how a complete information setting along with the equilibrium concept of subgame perfection plays an important role in these results. We explain this point in detail while presenting the main result of the paper. In this paper, we propose a dynamic mechanism named MDP-based Allocation and TRansfer in Interdependent-valued eXchange economies (abbreviated MATRIX), which is designed to address the class of interdependent values. It extends the results of Mezzetti (2004) to a dynamic setting, and with a certain allocation and valuation structure, serves as an efficient, truthful mechanism where agents receive non-negative payoffs by participating in it. The key feature that distinguishes our model and results from that of the existing dynamic mechanism literature is that we address the interdependent values and dynamically varying types (in an exchange economy) jointly and provide a strict ex-post incentive compatible mechanism. In Table 1, we have summarized the different paradigms of the mechanism design problem, and their corresponding solutions in the literature. Our main contributions in this paper can be summarized as follows. We propose a dynamic mechanism MATRIX, that is efficient, truthful (Theorem 1) and voluntary participatory (Theorem 2) for the agents in an interdependent-valued exchange economy. This extends the classic mechanism proposed by Mezzetti (2004) to a dynamic setting. It solves the issue of weak indifference by the agents in the second stage of the classic mechanism.  However, we will see that Theorem 1 is true with a restricted domain of subset allocation and peer-influenced valuations. These two properties were not needed to achieve a similar claim in the static setting (Nath and Zoeter 2013). We do not know if these are the minimal requirements for efficiency and truthfulness, but it is important to note that these properties in the dynamic setting do not immediately follow from its static counterpart. We discuss why the dynamic pivot mechanism (Bergemann and Välimäki 2010) does not satisfy all the properties that MATRIX satisfies (Sect. 3.2). We discuss that these results can be extended to a more general setting in Sect. 4. We also discuss that MATRIX comes at a computational cost which is the same as that of its independent value counterpart (Sect. 3.4). The rest of the paper is organized as follows. We introduce the formal model in Sect. 2, and present the main results in Sect. 3. In Sect. 4, we discuss about a generalization of the main results. We conclude the paper in Sect. 5 with some potential future works.",2
19.0,3.0,Review of Economic Design,29 May 2015,https://link.springer.com/article/10.1007/s10058-015-0173-x,Positional rules and q-Condorcet consistency,September 2015,Sébastien Courtin,Mathieu Martin,Bertrand Tchantcho,Male,Male,Male,Male,"A wide literature on voting theory is concerned with the theoretical debate between Condorcet social choice methods on one hand, and positional (or scoring) systems—voting \(\grave{a}\)
la Borda—on the other hand. The first ones consider, following Condorcet (1785), that the collective choice has to be based on majority duals between alternatives; while the second ones introduced by de Borda (1781), suggest to deduce the collective preference from a numerical evaluation taking into account the positions of the alternatives in the orders of individual preferences. There are several arguments in favor of either type of procedures, discussed in an abundant literature (see Nurmi 1987, 1999; Saari 2006 among others). The most significant contributions concerning positional rules are due to Smith (1973), Young (1974, 1975) and Saari (1994). The results presented by these three authors reveal some very natural properties that are satisfied only by positional mechanisms. They are certainly powerful arguments for adopting a positional approach. Unfortunately we know from Condorcet (1785) that the Borda rule and more generally the positional rules do not necessarily choose the Condorcet winner when it exists which is not the case of Condorcet consistent rule. A Condorcet winner is an alternative that is preferred to every other alternative by a majority of individuals: such an alternative would beat every other in majority comparisons. A Condorcet consistent rule selects the Condorcet winner when it exists. Similarly, it is well known that positional rules fail also to satisfy some weaker version of this property. There is no doubt about the importance of these results, since it provides a clear axiomatic boundary between Borda’s and Condorcet’s approaches of social choice mechanisms. It is worth noting, however, that the above-mentioned negative results have been obtained by assuming that the simple majority is used. An alternative is majority preferred to an other alternative if at least more than one half of the individuals prefer this alternative to the other one.Footnote 1
 The question we propose to tackle in this paper is then the following: to what extent does a larger majority (supra-majority) modify these negative results? To assume a larger majority is not so common in social choice theory. An important result concerning a supra-majority has been established by Ferejohn and Grether (1974). According to a given supra-majority m, an alternative is socially preferred to an other alternative if at least m individuals prefer this alternative to the other one. They give a necessary and sufficient condition on the majority needed in order to ensure that the rule associated with that majority always selects an alternative.Footnote 2 In a voting game context, they give a simple and elegant condition for which the Core is non empty.Footnote 3
 The purpose of this paper is then to give conditions such that the alternatives chosen by a rule belong to the Core. For that, we follow Baharad and Nitzan (2003) who study the relationship between the positional rules and what they call the “q-Condorcet consistency” principle. Given a supra-majority q, a rule satisfies this principle if the winner of this rule is not preferred by any other alternative by a q-majority of individuals. Unfortunately, they have results for only one special positional rule, the Borda rule. We want to extent these results by investigating more positional rules.Footnote 4
 Note that the well known positional approach takes place in a single stage process, where the winner(s) is (are) the alternative(s) with the highest score. But it can also be used in a multi-stage process of sequential eliminations, in each stage of which the alternative(s) with the least votes is (are) eliminated. We will then focus also on this kind of rules in this paper. Our main objective is then to study the conditions on the majority that ensure that positional rules (simple and sequential) satisfy the q-Condorcet consistency principle. The remainder of this work is organized as follows: Sect. 2 is a presentation of the general framework with notations and definitions. Section 3 provides a characterization of the most famous (simple and sequential) positional rules vis-à-vis the q-Condorcet consistency principle. Then, Sect. 4 provides results for general positional rules. Section 5 concludes the paper.",3
19.0,3.0,Review of Economic Design,29 May 2015,https://link.springer.com/article/10.1007/s10058-015-0175-8,Implementability and equity in production economies with unequal skills,September 2015,Kaname Miyagishima,,,Female,Unknown,Unknown,Female,"Equity and implementability are central requirements in the literature of fair allocation.Footnote 1 In a simple production model where agents have unequal skills and different preferences, we study social choice correspondences (SCCs) satisfying certain equity axioms and implementability conditions in this environment. A seminal condition of implementability is Monotonicity introduced by Maskin (1999). In general, this requirement is necessary for SCCs to be implemented in Nash equilibria by a mechanism. In production economies with unequal skills, Yamada and Yoshihara (2007) show that when the social planner cannot observe agents’ skills as well as preferences, two requirements called Independence of Unused Skills (IUS) and Supporting Price Independence (SPI)Footnote 2 are necessary and sufficient conditions to implement Pareto efficient SCCs by a mechanism with certain good properties.Footnote 3 Though Yamada and Yoshihara (2007) give examples satisfying these axioms, as long as we know, no other paper studies how large the class of Pareto efficient SCCs satisfying these two implementability conditions is. In this paper, we consider this problem. A traditional requirement of equity is No Envy introduced by Foley (1967). This axiom insists that no agent should prefer other agents’ bundles to his/her own. In production economies with unequal production skills, No Envy is incompatible with Pareto Efficiency (PE) (Pazner and Schmeidler 1974). Thus, earlier studies consider weaker requirements of equity such as No Envy only among equally skilled agents, or Equal Treatment of Equals (see Fleurbaey and Maniquet 1996, 2011). In this paper, we introduce a weaker axiom, Equal Reward for Equal Labor (EREL), which requires that if two agents with the same preferences and the same skill level contribute the same amount of labor, the agents should receive the same reward. This axiom is very weak and satisfied by all solutions introduced by Fleurbaey and Maniquet (2011, Section 3). As an auxiliary axiom, we also introduce No Discrimination (ND) (Thomson 1983), which requires that a feasible allocation that is Pareto indifferent to a socially selected allocation should also be in the solution. Then, we show that the only solution satisfying EREL, SPI, IUS, PE and ND is the equal income Walrasian (\(\textit{EI}\)) solution. Next, to broaden possibilities, we consider a weaker version of ND. We also introduce a new equity axiom, No Exploitation by Zero Effort (NEZE). This axiom requires that for two agents with the same preferences and the same skill level, if one contributes a positive amount of labor while the other does not, then the latter should not receive any reward. The intuition behind this axiom is that it would be inequitable if the latter nonworking agent received some amount of reward, because it would imply that what the working agent produced was “exploited” by the effortless agent. This axioms is weak because such exploitation only by non-working agent’s is prohibited when the two agents have the same skill level and the same preferences. Then, we show that the only SCC satisfying NEZE, SPI, IUS, PE and the weaker version of ND is the proportional solution (Roemer and Silvestre 1993). Our characterizations show how large the class of Pareto efficient and equitable SCCs satisfying SPI and IUS is. Yamada and Yoshihara (2007) argue that many Pareto efficient SCCs satisfy SPI and IUS. Our Theorem 1 shows, however, that among these SCCs, the \(\textit{EI}\) solution is the only solution satisfying EREL, which is a fairly weak equity axiom, and ND, which is natural and often used in the literature. On the other hand, if ND is weakened, there are other possibilities. Our Theorem 2 shows that, among those SCCs, the proportional solution is the only solution satisfying another type of weak equity axiom, NEZE, and the weaker version of ND. Neither of these solutions are good at compensating unequal skills. In this sense, our results show a trade-off between implementability and compensating the low skills. In the literature, there are many characterizations of the \(\textit{EI}\) solution in various environments. The most related works to ours is Fleurbaey and Maniquet (1996). In the same model as ours, they show that a SCC named Reference Welfare Equivalent Budget (RWEB) solution is the smallest (in terms of set inclusion) among SCCs satisfying Maskin Monotonicity, Pareto Efficiency, an equity axiom called Equal Welfare for Reference Preference, and an independence of technological changes called Contraction Independence. The \(\textit{EI}\) solution is an example of RWEB solution with a particular reference preference. They also show that, in the RWEB class, the \(\textit{EI}\) solution is the only solution satisfying a participation condition such that every agent should find his/her allocated bundle at least as well off as the situation of no work and no reward.Footnote 4 A main difference between their result and ours is that they obtain a partial characterization using Maskin Monotonicity, which is not sufficient to implement a SCC if skills are also not observable, while we fully characterize the \(\textit{EI}\) solution using SPI and IUS, which are necessary and sufficient conditions to implement Pareto SCCs under the environment. There are also several characterizations of the proportional solution using equity and implementability. Yoshihara (1998) shows that in production economies with multiple inputs and multiple outputs, the only solution satisfying PE, SPI, and an upper bound property called Upper Bound by Stand Alone Income is a generalized version of the proportional solution. Gaspart (1998) characterizes the solution using a requirement of bundle equality, Pareto Efficiency, and Pareto Preserving Independence, an independence of changes in preference and technology with preserving Pareto efficiency. Pareto Preserving Independence is slightly stronger than SPI. As long as we know, our result is the first to characterize the proportional solution using implementability conditions when both preferences and skills are unobservable. The remainder of this paper is as follows. In Sect. 2, notations and definitions are given. In Sect. 3, we introduce the axioms. In Sect. 4, the \(\textit{EI}\) solution and the proportional solution are defined. In Sect. 5, we present the characterization results. In Sect. 7, concluding remarks are provided. In the “Appendix”, we show the independence of the axioms in the theorems.",1
19.0,4.0,Review of Economic Design,29 May 2015,https://link.springer.com/article/10.1007/s10058-015-0172-y,Consistency of the Shapley NTU value in G-hyperplane games,December 2015,M. Á. Hinojosa,E. Romero-Palacios,J. M. Zarzuelo,Unknown,Unknown,Unknown,Unknown,,
19.0,4.0,Review of Economic Design,23 June 2015,https://link.springer.com/article/10.1007/s10058-015-0176-7,The informational basis of scoring rules,December 2015,Matías Núñez,Giacomo Valletta,,Male,Male,Unknown,Male,"The problem of defining a voting procedure can be split in two parts. The first one, the balloting problem, defines which kind of information one should ask from the voters, namely it devises the ideal shape of the balloting procedure. The second one, the aggregation problem, determines how to use such information in order to designate the winner(s), namely it singles out a desirable ballot aggregator. This paper focuses on a particular family of ballot aggregators, the scoring rules. Scoring rules are simple and widely used: voters assign a certain number of points to each of the alternatives (by mean of a certain balloting scheme) and those who obtain the highest total score (i.e. the sum of points) are the winners of the election. We concentrate in particular on their informational basis. Which information from the voters do these rules use, and which information do they ignore? Consider the following example. A group of five individuals has to choose among three alternatives \(\alpha ,\beta ,\gamma \). The balloting procedure is given and is conceived in such a way that each voter must assign 3 ballots to one of the alternatives, two ballots to another one and 1 ballot to the remaining one. Assume that the voters cast their ballots as in the following table (Table  1): How should one aggregate the ballots in order to single out a winner? In other words, which ballot aggregator should one use? One could take into account the ordinal characteristics of the set of ballots cast by each voter (as it happens with the Condorcet rule). In that case alternative \(\beta \) should be the winner. Alternatively one might decide to disregard the ordinal information contained in each set of ballots and just sum up all the ballots received by each alternative (as it is the case under the Borda rule). The alternative who obtains the highest total score is the winner. In this case \(\gamma \) should be the winner of the election. For a given family of balloting procedures we prove that scoring rules are the only ballot aggregators that (i) treat alternatives neutrally, (ii) have universal domain, (iii) respond positively to some voters changing their vote in favor of one alternative, and (iv) do not take into account the ordinal characteristics of each voter’s set of ballots. Most of the literature in social choice theory assumes, implicitly or explicitly, that when voters cast their ballot, they have to report their preference ordering.Footnote 1 However, in real-life elections, the information asked to each voter depends on the balloting procedure. For example, under plurality balloting, voters are asked to reveal only their first choice whereas under the Borda balloting they have to provide much more information. More importantly, even if two voting procedures collect the same information from the voters, trough the same balloting procedure, they might use different pieces of it in order to determine the winner. This is indeed the case for the Borda Rule and the Condorcet Rule. As shown in Example 1, these two rules could rely, in principle, on the same balloting procedure. However, when it comes to the ballot aggregator, the Borda Rule just looks at the total score received by each alternative while the Condorcet Rule hinges, for each pair of alternatives, on the relative score assigned by each voter. In order to make as clear as possible the distinction between the information collected by a voting procedure and the part of it actually used, we only focus on the normative properties of the ballot aggregator, taking as given the balloting procedure.Footnote 2 To the best of our knowledge the first paper that follows the same route is Goodin and List (2006a).Footnote 3 The authors provide an interesting generalization of May’s theoremFootnote 4 (May 1952) to many-alternative decisions provided that the balloting procedure collects only one ballot from each voter (Plurality balloting). We follow a similar line of reasoning but we move a step further by considering a wider family of balloting procedures. Indeed, depending on the balloting procedure, each voter can cast c ballots (with \(c \ge 1\)), each ballot standing for one alternative. As in Goodin and List (2006a) we start our analysis adapting to our framework the conditions proposed by May (1952). Not surprisingly, in such a framework, these conditions are not particularly restrictive: many ballot aggregators satisfy them, belonging both to the family of scoring rules and to the family of Condorcet rules. But if one strengthens the anonymity condition in such a way that the information about the combination of ballots cast by a voter is not relevant for the ballot aggregator, then we are only left with ballot aggregators based on the sum of the scores; the scoring rules. Our result has two interpretations of particular interest. First, we provide a characterization of scoring rules that relies on a condition that describes their informational basis. For a wide range of balloting procedures, if one wants a ballot aggregator that relies on such basis and satisfies May’s other conditions, then a scoring rule is the only available option. Second, our result allows one to trace the line that separates ballot aggregators that are solely based on the sum of scores from other ballot aggregators that look at at different types of information as, for example, those that are based on the pairwise comparison of the alternatives. Such a line is indeed represented by the fact that scoring rules only use a specific part of the information coming from the ballot box and they neglect the composition of the ballots cast by each voter (whereas it is a relevant piece of information, for example, for Condorcet rules). Our characterization of scoring rules in terms of their informational basis can be extended to situations where there is some normatively admissible asymmetry among the alternatives. This is what happens, for example, when society decides to provide some sort of protection to the status-quo when choosing among different social alternatives. Such protection may be granted by different means. For instance one may decide to assign weights to the total scores each alternative has collected. A good example of a qualified scoring rule is Weighted Approval voting as advocated by Massó and Vorsatz (2008). Under such a rule, voters use Approval balloting in order to cast their vote but, when aggregating the information coming from the ballot box, each alternative is assigned an exogenous weight. Again, here we move a step further by considering a more general informational environment comprising a wider family of balloting procedures. From a technical point of view, we build on the characterization of a qualified majority rule proposed by Houy (2007). The paper is organized as follows. Section 2 describes the setting. Section 3 provides a formal definition of May’s axioms in our framework. Section 4 presents a novel characterization of scoring rules. Section 5 deals with qualified scoring rules. Section 6 concludes. “Appendix 1” presents some examples of Condorcet rules defined in our framework. “Appendix 2” contains the proof of Theorem 2 and “Appendix 3” proves that the axioms used for the characterization of qualified scoring rules are independent.",3
19.0,4.0,Review of Economic Design,10 September 2015,https://link.springer.com/article/10.1007/s10058-015-0178-5,Mergers between regulated firms with unknown efficiency gains,December 2015,Raffaele Fiocco,Dongyu Guo,,Male,Unknown,Unknown,Male,"The adequate antitrust scrutiny of mergers between firms is a relevant policy issue in modern countries. The US Horizontal Merger Guidelines (HMG), revised by the Department of Justice and the Federal Trade Commission in 2010, and the EC Merger Regulation reformed in 2004 have acknowledged the relevance of cost synergies in merger control. Two major practical problems recognized by antitrust authorities and courts concern the uncertainty about the magnitude of efficiency gains before the merger and the merging firms’ privileged information about the realization of efficiency gains. Antitrust authorities emphasize the issue of uncertainty, since “efficiencies projected reasonably and in good faith by the merging firms may not be realized” (HMG, Sect. 4).Footnote 1 Moreover, as declared by Judge T. F. Hogan for the 1997 merger case of Staples and Office Depot, “the Court agrees with the defendants that where, as here, the merger has not yet been consummated, it is impossible to quantify precisely the efficiencies that it is will generate” (US District Court for the District of Columbia, Civ. No. 97-701).Footnote 2
 After the merger materializes, the merged firm can privately learn the realization of efficiency gains. In fact, “efficiencies are difficult to verify and quantify, in part because much of the information relating to efficiencies is uniquely in the possession of the merging firms” (HMG, Sect. 4). As Amir et al. (2009, p. 266) point out, “this first-to-know advantage thus emerges as a natural candidate for the fundamental asymmetry that mergers seem to trigger in favor of the merged firm”. The aim of this paper is to investigate the welfare effects of a merger between regulated firms when efficiency gains from joint production are uncertain before the merger and their realization becomes private information of the merged entity. Despite the importance of this phenomenon, mergers in regulated industries have received so far little theoretical attention. Recent decades have witnessed merger waves in industries in large part under regulatory control, such as electricity, gas, sanitation, telecommunications, transportation and water. Since enterprises with large numbers of customers and considerable assets are usually involved, the economic relevance of the consolidation process in regulated industries is definitively high. The 2004 report of Ernst & Young on worldwide mergers and acquisitions of regulated utilities underlines that “strong momentum behind merger and acquisition (M&A) activity in the global power and utilities (P&U) sector continued, driving deal value to US$38.6b—the highest third-quarter deal value since Q3 2010”. The empirical literature has investigated the impact of mergers in utility sectors on operating costs and shareholder wealth creation [e.g., see Kwoka and Pollitt (2010) and Datta et al. (2013) and the references cited therein]. Notable examples of mergers in regulated industries abound. E.ON, one of the world’s largest energy utility providers, was created in 2000 as a result of the merger between Veba (traditionally established in northern Germany) and Viag (traditionally established in southern Germany), with a deal value of about 14 billion dollars. In 2012 Duke Energy, operating in Indiana, Kentucky, Ohio and Western Carolinas, merged with Progress Energy, operating in Florida and Eastern Carolinas. This merger was realized through a transaction of about 32 billion dollars and generated the largest energy utility in the US by number of customers. On November 20, 2014 the US Federal Energy Regulatory Commission (FERC) approved the merger between Exelon, which distributes electricity to approximately 6.6 million customers in Illinois, Pennsylvania and Maryland, and Pepco, which serves the District of Columbia and the surrounding communities.Footnote 3 The Regional Bell Operating Companies (RBOCs), which provide regulated local telephone services in distinct areas of the US as a result of the 1984 divestiture of AT&T, have engaged after the enactment of the Telecommunications Act of 1996 in merger operations that have reduced their number from seven to only three.Footnote 4
 The consolidation process in regulated industries is also pervasive outside the US. The number of electric utilities in Ontario fell from over 300 in the 1990s to 74 in 2011 as a result of merger activities (Kushner and Ogwang 2014). According to the latest available (2007) report of the European Commission on mergers and acquisitions, 1002 mergers occurred in 2006 in European regulated network industries, which represent 11.6 % of all merger deals. Mergers between regulated utilities seem to be particularly popular in Italy. A2A S.p.A., the third largest Italian electricity company, was created in 2008 from the merger between AEM Milan and ASM Brescia, operating in two different areas of Lombardy. In 2012 Hera, the third largest Italian gas company based in Bologna, merged with AcegasAps Group, which serves Padua and Trieste.Footnote 5
 The empirical evidence and the stylized facts discussed so far indicate that a natural feature of mergers in regulated industries is that they involve firms operating in distinct territories. Kwoka and Pollitt (2010) emphasize this aspect in their analysis of the consolidation process in the US electricity sector, where more than 75 mergers occurred between 1994 and 2003, involving half of the customers of all investor-owned electricity companies, with a total deal value of over 300 billion dollars. To illustrate the nature of this consolidation process, Kwoka and Pollitt (2010, p. 647) refer to the merger realized in 1999 between Boston Edison, the major utility serving Boston, and Commonwealth Energy System, which operates in neighboring Cambridge (through its subsidiary Cambridge Electric) and in southern Massachusetts. Over the last decades most regulated industries have been involved in a partial liberalization process that has increased the scope for demand interconnections between regulated and unregulated firms. In the energy sector, transmission and distribution networks are typically regulated, while retail services are often open to competition. Regulated local telephone services coexist with unregulated telecommunications services, such as broadband Internet, long-distance and digital cable telephone services. Regulated railways operate in competition with unregulated long-distance buses and airlines. In big cities, regulated public utilities run railways, subways and buses, while unregulated firms supply alternative services such as car sharing or car rental. As Aubert and Pouyet (2006) emphasize, a major characteristic of current regulatory structures is that regulated and unregulated firms interact by providing differentiated products. In this paper we characterize the optimal merger policy involving regulated firms, whose task is to find a balance between the benefits of potential efficiency gains from the merger and the costs of distortions in the regulatory policy due to the aforementioned informational problems about efficiency gains. We explore this trade-off in a setting where a merger occurs between two regulated firms operating in two separate markets. The previous discussion indicates that mergers in regulated industries typically involve firms that provide the same service but operate in different regions, since they constitute local natural monopolies. This is the case of energy networks, local telephone services and local public transportation. In each market a regulated firm interacts with unregulated competitors because they provide goods that exhibit either some degree of substitutability (e.g., regulated railways and unregulated buses) or complementarity (e.g., regulated energy networks and unregulated retail services). Our purpose is to investigate the impact of the intensity of competition in the unregulated part of the market on the optimal merger policy involving regulated firms. For the sake of concreteness, we consider two standard modes of competition. Unregulated firms compete either fiercely in Bertrand fashion (as in a classical competitive fringe model), which entails zero market power, or in Cournot fashion, which is less intense and leads to higher profits. The main difference between these two modes of competition lies in the degree of toughness of product market competition or “toughness of price competition” (Sutton 1991).Footnote 6 Since competition is typically tougher (namely, the firms’ market power and associated profits decline) as the number of firms increases, our qualitative results carry over if we consider the impact of the number of firms in the unregulated part of the market on the optimal merger policy in any standard setting of (imperfect) competition. We find that, in the presence of uncertainty over post-merger costs before the merger occurs, Bertrand competition leads to a more lenient merger challenge rule than Cournot competition. To understand the rationale for this result, it is important to realize that, when post-merger costs are uncertain, an ex ante welfare-enhancing merger may eventually result in higher costs, namely, efficiency losses, driven for instance by clashes between corporate cultures (e.g., White 1987).Footnote 7 In this case, even when post-merger costs become common knowledge, the regulated production decreases because regulated activities are more inefficient. As Bertrand competition is more intense than Cournot competition, Bertrand competitors react more aggressively to changes in their demand stemming from regulated output reductions. Hence, more intense competition relaxes the condition for allowing the merger. Private information of the merged firm about the realization of post-merger cost synergies strengthens this result. As it is well established in the optimal regulation literature (e.g., Baron and Myerson 1982), a regulator prefers to tolerate some allocative inefficiency from the downward output distortion for the inefficient firm in order to limit the (socially costly) informational rents appropriated by the efficient firm. The more prompt reaction of Bertrand competitors to reductions in the regulated output with respect to Cournot competitors alleviates the allocative costs of downward regulated output distortions and softens the regulator’s incentive problem. In the second part of the paper, we show that these predictable results can be reversed if regulated firms diversify into a competitive segment of the market. In the energy, telecommunications and transportation sectors, regulated utilities often have affiliates in the liberalized part of the market where they operate. While the intensity of Bertrand competition tends to erode the firms’ profits and therefore makes the regulated firms’ diversification into the unregulated segment inconsequential, things are different under Cournot competition. In particular, when the regulated and unregulated goods are complements, competitive profits can discipline the diversified merged firm’s strategic behavior. The efficient merged firm has a weaker incentive to claim to be inefficient since a lower regulated quantity due to cost misrepresentation reduces the demand and the profits in the unregulated segment. The regulated firm’s internalization of competitive profits alleviates the regulator’s incentive problem and relaxes merger policy. In this case, the regulated firm’s diversification into a competitive segment implies that softer competition leads to a more lenient merger challenge rule. To the best of our knowledge, this paper constitutes the first attempt to shed some light on the welfare effects of informational problems about efficiency gains due to mergers between regulated firms that operate in distinct territories. In the practical and theoretical debate on merger policy in unregulated industries, mergers between firms established in different regions have usually been neglected, because merging parties do not compete with each other and therefore market concentration (typically measured by the Herfindahl–Hirschman Index) is unaffected. This paper raises a novel antitrust concern and shows that investigations of mergers between regulated firms are desirable even when they do not increase the firms’ ability to exert market power, since the informational problems driven by these mergers can be welfare detrimental. Relatedly, our analysis recommends a serious assessment of the intensity of competition in markets where merging regulated firms interact with unregulated competitors. It also provides theoretical support for the view of practitioners and policy makers that the effects of mergers between regulated firms on regulatory policies deserve adequate investigation. For instance, as emphasized in the Order issued on February 16, 2012, the Federal Energy Regulatory Commission (FERC) considers the impact on rates and regulation as a crucial factor when evaluating a proposed merger between regulated utilities.Footnote 8 Despite the stylized formulation for expositional purposes, the principles underlying our results are fairly general. Our analysis may therefore stimulate the theoretical and practical debate on antitrust and regulatory policies.",1
19.0,4.0,Review of Economic Design,08 September 2015,https://link.springer.com/article/10.1007/s10058-015-0179-4,The optimal design of rewards in contests,December 2015,Todd R. Kaplan,David Wettstein,,Male,Male,Unknown,Male,"Using contests to generate innovation has been around for hundreds of years. In the 1700s, the Longitude prize of £20,000 offered by the British Parliament induced John Harrison to invent the marine chronometer (see Sobel 1996). More recently, the Ansari X-prize was a ten-million-dollar competition created to jump-start the space tourism industry by attracting the attention of the most talented entrepreneurs and rocket experts in the world.Footnote 1 Such R&D contests are an example of a competition in which all contestants, including those that do not win any reward (prize), incur costs as a result of their efforts but only the winner gets the reward. In such contests, the designer may often offer smaller prizes for lesser achievements. In fact, while the full longitude prize was given for determining longitude within 30 nautical miles, £10,000 was given for a method for determining longitude within 60 miles, and £15,000 for a method within 40 nautical miles. Another example of smaller prizes is where Netflix offers a prize for improving their movie recommendation system.Footnote 2 This prize increases if the improvement is more than 10 %.Footnote 3
 We proceed to study such environments by formulating the contest as an all-pay auction. When the prize depends upon the result, this is equivalent to having a bid-dependent reward. Such environments have been analyzed before both positively, studying the equilibrium behavior properties and normatively, determining what are optimal contest designs. Environments with complete information have been analyzed from a positive point of view in Baye et al. (1996, 2012), Kaplan et al. (2003) and Siegel (2009, 2010), the normative point of view was analyzed in Che and Gale (2003) and Fu et al. (2012). Environments with incomplete information were studied from a positive point of view in Kaplan et al. (2002), the normative point of view was investigated in Moldovanu and Sela (2001) and Cohen et al. (2008). Similar research was carried out for rent-seeking contests, Nitzan (1994) provided a positive analysis, Franke et al. (2013) provided a normative analysis. Halac et al. (2015) performed a normative analysis of a contest environment modelled as a multi-period one-armed bandit (where the jackpot is an innovation). Konrad (2009) provides an excellent survey of equilibrium and optimal design in contests. In this paper, we provide further normative analysis for environments with complete information. We look at the optimal rewards under complete information when the designer cares about both the largest effort and the sum of the efforts by the participants. The designer wishes to maximize this expression net of the rewards paid out. We determine the designer’s optimal bid-dependent reward structure to achieve this goal as a function of costs in both symmetric and asymmetric environments. Interestingly, the solution under symmetry when the designer cares only about the highest effort produces equivalent behavior to that in Che and Gale (2003) where the firms compete by choosing both effort and price. In our paper, the solution under asymmetry is similar to that under symmetry except the rewards are firm specific. One may consider this problematic in the sense that the designer must know which firm is which and bias the contest in favor one of the firms. We address this issue by describing settings where this firm specific reward structure can be replaced by a reward (to the winner) that depends upon both of the firms’ efforts. In our setting, we consider a richer class of contests than considered by Che and Gale (2003) and as a result, in some cases, the optimal contest generates higher surplus for the designer than their solution of handicapping one firm. While in this paper we phrase the problem as designing a research contest, our analysis is applicable to many other scenarios that have such a winner-take-all form. For instance, many races offer prizes to the winners that depend upon time. Also, in a contest to receive a promotion at a company, the firm may set the salary increase with the promotion conditional on the worker’s performance. This paper would suggest how to structure these rewards. Our paper proceeds as follows. In Sect. 2, we introduce the general environment with the optimal rewards for symmetric case. Afterwards, in Sect. 3, we allow for asymmetry between firms. Finally, in Sect. 4, we present the concluding remarks.",6
20.0,1.0,Review of Economic Design,21 September 2015,https://link.springer.com/article/10.1007/s10058-015-0180-y,Groves mechanisms and communication externalities,March 2016,Efthymios Athanasiou,Santanu Dey,Giacomo Valletta,Male,Unknown,Male,Male,"Traveling by train across the border between France and Spain used to involve the inconvenience of changing trains. French and Spanish trains operate on rails of different gauge. In order to resolve this issue, Spain adopted a variable gauge system that enables its trains to access the French railway network. Swedish and Polish railway companies employ variable gauge systems on cross-border services as well. We consider a framework where an agent is associated with one of two platforms. Communication between two agents requires that they operate on a common platform. Adoption of a new platform is costly. The cost depends on the agent’s native platform. The benefit of communication depends on a subjective parameter reflecting the value the agent attaches to the interactions the new platform enables. The benefit of communication is increasing in the number of agents with whom one may interact. The term platform denotes a mode of operation, while the term communication denotes the possibility of interaction that having a platform in common affords. Refer to Fig. 1a. Agents are represented by nodes. The set of agents is partitioned in two groups. All members of each group share the same native platform. Agent j’s native platform is \(\alpha \). Platform adoption, that entails a platform-specific cost, is depicted by an arrow stemming from a node and pointing to a set of nodes. Individual j adopts platform \(\beta \). This enables her to communicate with each agent whose native platform is \(\beta \). The net benefit she derives depends on the number of agents she is able to communicate with [this is what Selten and Pool (1991) call ‘communicative benefit’] and the cost she had to face in order to adopt the new platform. Moreover, she becomes a source of value for all the agents whose native platform is \(\beta \), who are now able to communicate with her. This externality is a critical feature of the model. 
a By adopting platform \(\beta \) agent j makes possible the interaction between her and each agent in \(N^{\beta }\). b An assignment that enables all agents to communicate with each other. c Two-sided adoption: at least one agent from each platform group adopts the platform foreign to her Two questions arise naturally. First, what is the efficient adoption pattern? Second, provided the previous question is resolved, how may a policy maker implement it? The central aim of this paper is to understand how the answer to these two questions is shaped by the inherent externality present in the situations we study. The exercise of determining the efficient outcome constitutes a discrete optimization problem. We provide an algorithm that solves it and we address computational complexity issues. Figure 1b depicts a situation where all agents have a platform in common. Efficient outcomes do not necessarily entail that. Moreover, at the optimum, two-sided adoption, depicted in Fig. 1c, may occur. Implementation is associated with the study of mechanisms. These objects associate a social outcome to the various values the primitives of the model may take. A mechanism can be evaluated on the basis of its properties. The following four properties feature prominently in this paper: 
Assignment efficiency The mechanism always selects efficient outcomes. 
Strategy-proofness The mechanism induces all agents to reveal whatever private information they may hold. 
Individual rationality The mechanism does not force the participation of any agent. 
Feasibility The mechanism relies exclusively on the resources generated within the economy. It turns out that no mechanism satisfies all of the above requirements. There are, however, mechanisms that satisfy any three of them. We place particular emphasis on Assignment Efficiency and Strategy-Profness. Both these properties are shown to be inherently linked with incentives. A mechanism that violates either of them is prone to deficiencies that undermine the implementation exercise in a fundamental way. Appealing to a result due to Holmström (1979), embracing Assignment Efficiency and Strategy-Proofness entails confining our investigation to the family of Groves mechanisms (1973). The literature discussing such mechanisms does so primarily in three fairly distinct contexts. Pure public goods, excludable public goods and private goods. In this paper we study Groves mechanisms in a context of private goods that accounts for the effect of an externality. It turns out that some of the conventional wisdom on Groves mechanisms does not carry through to our model. For example, the celebrated Pivotal mechanism here fails Feasibility. The nature of the externality we capture in our model causes the Pivotal mechanism to sometimes assign positive transfers, something that is disallowed in the framework of either public or private goods. Our proposal involves two mechanisms. First, we look at individually rational Groves mechanisms. We show that such mechanisms are often in deficit and we provide sufficient conditions that determine when this is the case. We propose a mechanism that minimizes the deficit whenever it occurs. Second, we outline a methodology to design feasible Groves mechanisms. Following that, in economies comprising two agents, we single out the only feasible and symmetrical Groves mechanism that is not Pareto dominated by another strategy-proof, feasible and symmetrical mechanism. Effectively the objective we pursue is to identify the best-in-class mechanism. When Feasibility is out of the picture the criterion that isolates the best mechanism is related to the incidence of the deficit. We do not focus on the worst case scenario (as in Bailey 1997; Cavallo 2006; Guo and Conitzer 2007; Moulin 2009) or on the asymptotic behavior of the deficit (as in Deb et al. 2006; Green and Laffont 1979; McAfee 1992). Rather we propose a mechanism that runs a lower deficit than any other mechanism in each economy where the deficit presents itself. In order to isolate the best feasible Groves mechanism, we do not base our selection on sums of utilities but rather on their distribution (as in Guo and Conitzer 2008; Athanasiou 2013; Sprumont 2012). Loosely speaking, a mechanism Pareto dominates another one if the former generates, in each economy and for each agent, a higher amount of utility. This criterion turns out to be sharp enough to isolate a single feasible Groves mechanism when the discussion is confined to two-agent economies. A natural application of our findings concerns the problem of language acquisition. The literature on this topics focuses on decentralized outcomes that may arise in situations similar to the ones we explore. In their seminal contribution Selten and Pool (1991) introduce a general model of language acquisition. They show that an equilibrium of the multi-country multilingual language acquisition model exists. The characterization of an equilibrium is then studied by Church and King (1993). More recently, Ginsburgh et al. (2006) and Gabszewicz et al. (2011) study qualitative properties of such equilibria in the context of bilingual societies. In our model one may rationalize different Nash Equilibria, exhibiting both one-sided as well as two-sided learning. However, the efficient outcome does not generically come about as a Nash Equilibrium. This latter point provides the motivation for looking into the outcomes a Planner may bring about and how they compare with those that arise in the absence of intervention. Consider the following example. Peter and Mary speak English. Igor, Ivan and Natasha speak Russian. Both Peter and Mary attach significant value to communicating with Igor, Ivan and Natasha, but they face a cost of learning Russian that is prohibitive. Contrary to that, Igor, Ivan and Natasha attach no value to communicating with Peter and Mary, although for them the cost of learning English is smaller that the benefit it would create for Peter and Mary. Suppose that each individual’s strategy in this game consists of a decision on whether to learn the foreign language or not. At equilibrium no-one learns a foreign language, albeit for different reasons. However, the pattern of language acquisition that maximizes the sum of utilities involves Igor, Ivan and Natasha learning English. This example capture the nature of the externality that lies in the heart of the problem we study in this paper. Section 2 introduces the model. Section 3 discusses efficiency. Section 4 introduces the axioms and presents the impossibility. Section 5 discusses individually rational Groves mechanisms. Section 6 discusses feasible Groves mechanisms. Section 7 concludes.",2
20.0,1.0,Review of Economic Design,27 September 2015,https://link.springer.com/article/10.1007/s10058-015-0181-x,Fair compensation with different social concerns for forgiveness,March 2016,Aitor Calo-Blanco,,,Male,Unknown,Unknown,Male,"Whether individuals should or should not be deemed responsible for changes in their preferences has become an intense debated subject among economists and philosophers (e.g., Arneson 1989; Dworkin 2000, 2002; Fleurbaey 1995, 2002, 2005, 2008). Regardless of what they put the focus on (resources, opportunities or capabilities), standard egalitarian theories generally consider legitimate not to compensate those individuals who change their preferences and regret their previous choices. Rewarding those who claim to regret their choices is not a trivial issue as it may generate incentive problems, since individuals could fake regret in order to get extra resources. Moreover, some reject this principle of forgiveness because it generates problems of unfairness. Specifically, they argue that this principle defends compensating individuals for a frugality they have never practised, and everything at the expense of those who have worked hard and have actually been frugal. This would allow the ‘spendthrift’ to take advantage of the situation and have the proverbial cake and eat it too (see Dworkin 2002). 
Fleurbaey (2005, 2008) challenges this last ethical view. According to him, in the absence of any cost to others no-one would complain about helping individuals who regret their past choices. And the reason no-one would complain is because freedom would increase unambiguously if we give those regretful agents the possibility of choosing among an extended set of alternatives at no cost whatsoever. Therefore, Fleurbaey (2005, 2008) argues that there is no ethical, moral or fairness reason to make those individuals suffer the consequences of their wrong initial choices. He defends, then, that the problem is not really the fairness or ethical concerns, but the cost that the forgiveness ideal may have on others. However, since many redistribution and solidarity policies have been extensively justified for alternative scenarios, he argues that the fact that a forgiveness policy may entail a cost to others should not be a cause of concern either. He discards these cost issues by saying that it is only a matter of finding an adequate balance between the additional freedom obtained by those who benefit from the forgiveness ideal, and the decrease in the level of freedom experienced by those who have to fund it. Accordingly, it seems that the real problem that the principle of forgiveness raises is the fact that it may not be implemented due to incentive problems. More precisely, that the possibility that individuals have to strategically misrepresent their preferences in order to get extra resources may block the actual implementation of the principle.Footnote 1
Fleurbaey (2005, 2008) proposes dealing with this problem by means of designing an incentive-compatible forgiveness policy that would let individuals at liberty to choose different options. According to the choices that they can freely make, a social planner should control the ‘excessive’ level of welfare, according to a given measure, that any individual may obtain by adopting others’ lifestyle. Finally, Fleurbaey (2005, 2008) presents the implementation of an incentive-compatible scheme of taxes and subsidies that grants a fresh start to those who regret their previous choices.Footnote 2 He shows that, apart from yielding higher levels of equality, the implementation of the principle of forgiveness indeed increases freedom, as individuals can overturn the consequences of their previous choices. Together with this ideal of forgiveness, in this paper we also deal with the problem of compensating individuals who have different traits. Some of the most relevant theories of fairness and responsibility argue that inequalities in agents’ outcomes may contain elements for which those individuals are responsible, but also other elements for which they should not be deemed responsible. Those theories defend that individuals should only be compensated for outcome differences that are a result of the second group of elements (see Rawls 1971; Dworkin 1981a, b; Arneson 1989; Cohen 1989; Roemer 1998). Individual preferences are one of those elements that generate differences in the distribution of outcomes. In this paper we endorse the view, which is widely used nowadays, that considers that they are a legitimate source of inequality (e.g., Fleurbaey and Maniquet 2011). That is, we respect individuals’ preferences because they reflect their opinion about what is important and what is not, and hence we hold them responsible for the way in which they decide to live their lives. It is worth stressing that there is no full unanimity about the use of this approach (see Cohen 1989; Roemer 1998), but Fleurbaey (2008) thoroughly discusses this responsibility cut, and lays out sound arguments in favour of holding individuals responsible for their preferences and ambitions in life. He defends that what society should do is to let people exercise their freedom, but also to provide them with similar valuable alternatives. Apart from the ethical discussion of how to delimit the responsibility cut, endorsing this ideal of neutrality with respect to individual preferences entails interesting possibilities when analysing the principle of forgiveness. By assuming that individuals are responsible for their ambitions and life goals, which we can relate to their preferences, it is easy to design a ‘forgiving’ society. To do so it would only be necessary to consider that society should assess any individual’s current situation in terms of preferences, or ambitions, which are different from her initial ones. For several reasons (see Fleurbaey 2005), this is harder to do in models which exclusively focus on genuine choices, such as the equal opportunity approach.Footnote 3
 After having briefly explained these ethical principles, our aim is to formally analyse how the forgiveness ideal interacts with the aforementioned compensation problem. Interestingly enough, existing models of forgiveness include neither a full axiomatic justification of the suggested solution, nor any additional source of unfairness other than regret.Footnote 4 As we have already stated, the most relevant solution to deal with the ideal of forgiveness is laid out by Fleurbaey (2005, 2008). Considering a model in which individuals are responsible for their preferences, he analyses the issue by assuming that resource egalitarianism is the final target. In such a framework, he suggests maximising, across the entire population, the minimum value of what he calls the Equivalent Initial Share (EIS). Such a concept is the minimum amount of resources that any individual would need to buy a bundle that would provide her, according to her final or ex post preferences, with the same level of utility as her current choice. Therefore, the key factor in his model is that any individual’s current situation is compatible with her current, or ex post, preferences. However, besides including neither a full axiomatic justification of this equivalent measure nor any additional source of unfairness, Fleurbaey (2005, 2008) considers only full compensation for regret. That is, all the individual preferences, except for the most recent ones, are discarded in his analysis. He argues that, apart from the fact that the EIS corresponds entirely with the ideal of ‘a fresh start every morning’, by adopting the most generous approach towards the regretful agents it is possible to show that, even in the hardest cases, a “forgiving society is not like eating a cake and having it too”. Interestingly enough, Fleurbaey (2005, 2008) acknowledges that he adopts such a view without making any normative judgment on the relative worth of old preferences versus new ones. Additionally, he also mentions the possibility of defining other measures that could balance both current and old preferences, discounting, this way, the individuals’ level of regret and partially forcing them to bear the consequences of their past decisions. All this being said, we argue in this paper that, depending on the assumptions that one could consider, there exist some scenarios in which it is worth combining the ex ante and the ex post individual preferences to define an intermediate degree of forgiveness. In other words, society should evaluate the individuals’ current situation with a profile of preferences which is different from both the initial profile and the final one. For instance, the solution proposed by Fleurbaey (2005, 2008) hinges on the assumption that the new preferences are morally or cognitively superior to the old ones. Interestingly enough, he also points out that this may not be always the case, as it happens when one has gone through a process of addiction. In such scenarios he acknowledges that it is much more questionable to cater to the individual’s current ambitions. Apart from this superiority of the current preferences, Fleurbaey (2005, 2008) also assumes that changes of mind cause no externalities on others, except for the taxes that they may have to pay. However, when helping those who regret their initial choices entails additional effects on others that are difficult to be compensated in terms of resources, it is more difficult to defend the ideal of ‘a fresh start every morning’. Moreover, one can identify different types of regret, which can be linked to learning and informational factors, genuine changes of preferences, etc. Some of them, such as those related to the first group, generate an easy case of forgiveness. However, other sources of regret have a more difficult ethical justification, and society may, then, not be so willing to help individuals in such cases, as it happens with those sources that harm or morally damage others. Therefore, in such scenarios in which the justification of the forgiveness ideal is problematic, it is reasonable not to apply it to its full extent. A final argument in favour of considering an intermediate degree of forgiveness is that individuals may have different opinions about the kind of society they want to live in. As proposed by Fleurbaey (2005), they have to choose between “a community of egoistic self-righteous individuals who scorn those who mismanage their share of resources”, and a forgiving society in which “the values of solidarity and compassion, maybe modesty as well, are cherished by individuals”. The intermediate approach to forgiveness can be understood as a combination of these two views, which would represent the degree of that society’s concern for forgiveness. As a result of the previous discussion about the principle of forgiveness, our aim in the present paper is to construct, grounded on efficiency, robustness, and ethical principles, a social ordering function that allows us to rank all possible allocations on the basis of both responsibility and forgiveness criteria. Unlike previous models, this social ordering balances both ex ante and ex post individual preferences reducing, this way, the social value of their regret and partially forcing them to bear the consequences of their previous choices. Specifically, such a ranking pushes in the direction of reducing inequality between hypothetical reference budget sets that are constructed by combining initial and final preferences. This permits society to define to what extent it is willing to compensate individuals for their regretted choices. Moreover, the ranking we derive also allows for the social evaluation of alternative situations associated with different degrees of forgiveness. Therefore, this paper first introduces the axiomatic derivation of the concept of EIS, and then we extend such a measure to assess social situations characterised by different concerns for forgiveness. As it is standard in the social choice literature (e.g., Fleurbaey and Maniquet 2011), such an ordering is derived according to ethically appealing requirements that are defined by using only ordinal, and non-comparable, information about individual preferences. The rest of the paper is organised as follows. Section 2 presents the basic components of the model, while Sect. 3 introduces the ethical requirements that society is willing to satisfy. Section 4 characterises the social ordering function that results from those requirements. Section 5 reviews the conclusions of this study. All proofs are relegated to the Appendix.",5
20.0,1.0,Review of Economic Design,19 October 2015,https://link.springer.com/article/10.1007/s10058-015-0183-8,Inverse S-shaped probability weighting functions in first-price sealed-bid auctions,March 2016,Kerim Keskin,,,Male,Unknown,Unknown,Male,"It is often observed in first-price sealed-bid auction experiments that subjects tend to bid above the risk neutral Nash equilibrium (RNNE) predictions (see Cox et al. 1988; Kagel 1995, among others). This overbidding phenomenon has often been explained using models with risk averse bidders. However, for such an explanation to be valid, bidders should be excessively risk averse. Accordingly, it is argued that risk aversion cannot be the only factor and may well not be the most important factor behind overbidding (see Kagel and Roth 1992). Along this line, several alternative explanations have been provided: ambiguity aversion (Salo and Weber 1995), regret theory (Filiz-Ozbay and Ozbay 2007), level-k thinking (Crawford and Iriberri 2007), and loss aversion (Lange and Ratan 2010).Footnote 1
 In addition to the above studies, a number of papers suggest subjective probability weighting as an alternative explanation for overbidding. To the best of our knowledge, Cox et al. (1985) are the first to present the idea of using subjective probability weighting in first price auctions. They propose that a power probability weighting function (PWF) is observationally equivalent to a model with risk aversion. Afterwards, Goeree et al. (2002) employ this idea utilizing a functional form which is originally suggested by Prelec (1998). They estimate that the PWF should be essentially convex over the whole range if it were to explain their experimental observations. Finally, Armantier and Treich (2009b) experimentally show that bidders tend to overbid as they underestimate their winning probabilities, whereas Armantier and Treich (2009a) analytically show that a star-shaped PWFFootnote 2 can explain overbidding in first-price auctions. The above-mentioned PWFs imply the underweighting of all probabilities. Hence they are not in accordance with the PWFs commonly used in the literature (i.e., inverse S-shaped functions)(see Tversky and Kahneman 1992; Camerer and Ho 1994; Wu and Gonzalez 1996; Prelec 1998, among others).Footnote 3 In this paper we introduce inverse S-shaped PWFs into first-price sealed-bid auctions and investigate the extent to which such weighting functions explain overbidding. Our results indicate that bidders with low valuations underbid if all bidders use the same inverse S-shaped PWF. We also show that (i) there exist cases under which all bidders always underbid and (ii) if the number of participants is sufficiently low, there exists a threshold valuation such that any bidder with a valuation higher than this threshold will overbid.Footnote 4 Therefore, we conclude that inverse S-shaped PWFs provide a partial explanation for overbidding. It is worth noting that these findings are somewhat consistent with the aforementioned experimental studies since overbidding is mostly observed for bidders with high valuations, whereas the submitted bids of subjects with low valuations are close to the RNNE predictions (see Filiz-Ozbay and Ozbay 2007; Armantier and Treich 2009b, among others).Footnote 5
 This paper is structured as follows: In Sect. 2, we present the related aspects of subjective probability weighting, we introduce inverse S-shaped PWFs into first-price sealed-bid auctions, and we investigate the unique symmetric Nash equilibrium. Section 3 concludes.",5
20.0,1.0,Review of Economic Design,07 November 2015,https://link.springer.com/article/10.1007/s10058-015-0184-7,Tournaments and piece rates revisited: a theoretical and experimental study of output-dependent prize tournaments,March 2016,Werner Güth,René Levínský,Ori Weisel,Male,Male,Male,Male,"Tournament incentives, based on relative, rather than absolute, performance, have become an increasingly important component of organizational compensation systems (Orrison et al. 2004; Bothner et al. 2007). Starting with the seminal paper by Lazear and Rosen (1981), the incentive properties of (fixed prize) tournament compensation systems have repeatedly been analyzed (for an early review see McLaughlin 1988; Kräkel 2008; Gürtler and Kräkel 2010, are examples of recent studies). A general observation from this literature is that tournaments can, under certain circumstances (mainly the risk-neutrality of the agents), induce the same efforts from agents as piece rates, allowing principals to economize on measurement costs (as rank order is typically easier to measure than cardinal performance) and to allocate indivisible rewards without sacrificing production efficiency. In some situations tournaments can be even more efficient than piece rates, such that a firm employing a tournament compensation system can produce a higher output, or produce the same output at a lower cost, as compared to a firm employing a piece rate compensation system. Empirical studies on tournament compensation systems often rely on sports data (e.g., Ehrenberg and Bognanno 1990; Becker and Huselid 1992; Bothner et al. 2007; Kaplan and Garstka 2001) and increasingly on field studies from the organizational practice (Knoeber and Thurman 1994; Bandiera et al. 2005; Matsumura and Shin 2006; Casas-Arce and Martínez-Jerez 2009; Backes-Gellner and Pull 2013). The first experimental evidence on tournaments was provided by Bull et al. (1987), to be followed by a wide range of studies relying on laboratory data (e.g., Harbring and Irlenbusch 2008; Freeman and Gelber 2010). A recent, encompassing survey of experimental research on tournaments, contests and all-pay auctions is provided by Dechenaux et al. (2015). Both the empirical and experimental studies mostly support the basic predictions of tournament theory. The literature, however, has mainly focused on ‘fixed-prize’ tournament incentives where the size of the prize to be awarded is set in advance and is not influenced by employee performance or firm success. As long as firm performance can be assessed in advance with reasonable accuracy, a system of predefined tournament prizes that have to be paid out regardless of the firm’s success may not pose a severe problem. However, if firm performance is difficult to assess in advance (e.g., due to an uncertain economic environment), a predetermined tournament prize may well exceed what the firm can actually afford to pay. In contrast, tournaments with prizes that are not fixed, but rather depend—or include a component which depends—on the organization’s performance, eliminate or reduce the hazard of having to pay out a large prize when the organization is doing poorly. An additional advantage of tournaments with output-dependent prizes is that they carry a smaller risk of horizontal collusion and sabotage. In fixed prize tournaments contestants can engage in collusive behavior by jointly reducing their effort, or in sabotage by taking actions to reduce each other’s performance, knowing that the full prize will be paid out anyway (Harbring and Irlenbusch 2003; Bandiera et al. 2005; Harbring et al. 2007). If the size of the prize (positively) depends on the agents’ joint output, both collusion and sabotage are less attractive, because they lead to a smaller prize. There are a few examples of tournaments with variable, rather than fixed, prizes. In so-called Japanese bonus tournaments, for instance, the bonus an agent receives is not set in advance, but rather depends on his or her relative performance (Kräkel 2003; Endo 1984). The bonus sum to be distributed to all of the agents, however, is set in advance, and does not depend on the agents’ total absolute performance. Similarly, Cason et al. (2010, 2013) and Shupp et al. (2013) study ‘proportional-prize’ tournaments in which the prize sum is divided among the agents by their share of the total achievement. Again, however, the prize sum to be divided does not depend on agents’ total achievement, but is fixed in advance. The same is true for the ‘share contests’ analysed by Fallucchi et al. (2013) and for the compensation mechanism studied by Chowdhury et al. (2014). Chowdhury and Sheremeta (2011) and Baye et al. (2012) analyse contests where the own output and the rival’s output enter the winning agent’s payoff function. Hence, similarly to our model, agent payoff is influenced by (firm) performance. Contingent-prize R&D contests (Clark and Riis 2007) where contestants can signal their ability by choosing a combination of winning and losing prizes from a prize menu provide yet another example of variable prize contests. However, since the prize to be awarded is to be paid in full even when the R&D enterprise is not successful, the situation is quite different from the type of output-dependent prize tournaments we study. Lastly, Cohen et al. (2008) analyze all-pay auctions with variable, effort-dependent rewards. However, while the reward at stake depends on the effort of the winning agent, it is not influenced by the output of the other agent. In order to study the comparative advantages of output-dependent prize tournaments, we compare them with piece rates based on absolute performance and with fixed-prize tournaments (see Agranov and Tergiman 2013, who compare piece rates, relative piece rates and fixed-prize tournaments for a similar approach). Specifically, we allow for employee compensation to be linearly dependent on (a) a piece rate based on absolute performance, (b) a pre-determined fixed prize awarded on the basis of relative performance, and (c) an output-dependent prize which is also awarded on the basis of relative performance, but whose size depends on firm success (interpreted as the joint production of agents). We refer to the latter as ‘output-dependent prize tournaments’. A possible real world example of output-dependent prizes is an appropriately designed profit sharing scheme where employees are rewarded according to the realized profit and where the share of the profit that goes to an individual employee is based on his or her relative performance. In our case of a ‘winner-takes-all’-tournament, the best performing employee receives the full share; in practice, the share of the profit going to the employees might also be distributed among all employees according to their relative contributions (as is the case in Japanese bonus tournaments, see above). Theoretically, we rely on a cost minimization approach when analyzing the optimal combination of the three incentive types. The focus is on optimal contract design from the perspective of the principal: whatever quantity is to be produced should be produced with the lowest possible cost. Our analysis shows that output-dependent prize tournaments are more cost-effective than piece rates and fixed-prize tournament incentives, the two most studied types of incentives in the literature. We test the theoretical predictions with data from a laboratory experiment with both agent- and principal-participants. Our data qualitatively support the theoretical propositions: despite the fact that agent-participants systematically deviated from their theoretically predicted effort level, output-dependent prizes prove to be the most profitable in our experimental sessions (relative to the conventional alternatives). Principal-participants seemed to realize this, as they displayed a strong tendency in favor of output-dependent prizes when designing incentive systems. In sum, our results suggest to foster the use of output-dependent prizes in the organizational practice.",4
20.0,2.0,Review of Economic Design,12 October 2015,https://link.springer.com/article/10.1007/s10058-015-0182-9,An optimistic search equilibrium,June 2016,Dipjyoti Majumdar,Artyom Shneyerov,Huan Xie,Unknown,Male,,Mix,,
20.0,2.0,Review of Economic Design,28 October 2015,https://link.springer.com/article/10.1007/s10058-015-0185-6,Decision-making in organizations: when to delegate and whom to delegate,June 2016,Stefan Ambec,Michel Poitevin,,Male,Male,Unknown,Male,"Decentralization of decision-making is pervasive in many economic organizations. For example, shareholders typically decentralize management of the firm to professional managers. Many firms outsource part of their production process to external suppliers. A public regulator decentralizes production decisions to a regulated private enterprise. The structure of governments itself has some degree of decentralization since powers may be allocated to different levels of government or jurisdictions. What then are the arguments in favour of more or less decentralization? For a long time, economists have suggested that communication was central to the debate (Marschak 1959; Groves and Radner 1972; Radner 1983). The question was whether centralization or decentralization performs better in the presence of exogenously specified communication costs. One drawback of this early literature is that it neglected the incentives to reveal or withhold information. More recently, incentives have entered the picture, namely, incentives to provide valuable private information to decision-makers. The basic argument in favour of decentralization is that it allows economizing on communication costs since the decision is delegated to the agent who possesses the most relevant information. The cost of doing so, however, is a potential loss of control by the principal, because the agent may not have his preferences perfectly aligned with the objectives of the principal, therefore leading to agency costs. A basic tradeoff emerges: more decentralization economizes on communication costs while also leading to a loss of control by the principal. The resolution of this tradeoff potentially can lead to a theory of organizational structure. Despite this clear enough intuition, it has been a theoretical challenge to explain formally why decentralization of decision-making should take place in an organization. It can be shown that any decentralized organization can be replicated by a centralized one in which all agents report their private information to the principal, who then makes all decisions. In the centralized organization, the principal replicates what the agents would have done in the decentralized organization conditional on their reported information. When the principal is committed to act as such, agents have no interest in lying, and they therefore report truthfully their private information. This implies that centralization is at least weakly preferred to any decentralized organization. This classic result is known as the “Revelation Principle” (Gibbard 1973; Green and Laffont 1977; Myerson 1982). The central question then is: can the theory of incentives produce a tradeoff between centralization and decentralization? The Revelation Principle gives us a useful benchmark against which any theory of decentralization can be assessed. Any such theory must therefore start with relaxing some assumption(s) underlying this result. A first assumption is that agents act non-cooperatively when reporting their private information to the principal. If agents collude and coordinate their reporting strategy, they affect ex ante incentives. Decentralization may be part of the solution for avoiding collusion. Mookherjee (2006) surveys that literature. A second assumption is that players have some commitment power that enables them to sign a contract, and, once they have signed it, to obey the rules set out ex ante; namely, they are committed to act upon the reported (verifiable) information as specified in the initial contract. Beaudry and Poitevin (1995) show that the lack of commitment can reduce the efficiency of a centralized organization. In a centralized organization, an agent communicates his (verifiable) information to the principal who then takes an action based on the agent’s report, and as dictated by the signed contract. If players cannot commit, there are gains to renegotiate the contract once the information has been reported since, typically, the contract requires ex post distortions in order to extract the agent’s information and to minimize his rents. BP show that renegotiation, because it precludes ex post distortions (that are renegotiated away), results in higher rents to the agent. In this case, decentralizing the decision to the agent may avoid such rent costs to the principal. When decentralizing the decision to the agent, all communication is avoided since the agent has all relevant information to make the appropriate decision. Hence, renegotiation cannot take place.Footnote 1 BP show that decentralization dominates centralization when players cannot commit not to renegotiate. Watson (2007) makes a similar argument looking at the technological details of a mechanism. One drawback of this analysis is that it includes only one informed agent. It cannot therefore study the tradeoff between the gains of decentralization and the loss of control for the principal. To do so, one must introduce at least two informed agents.Footnote 2
 Recent articles have studied the tradeoff between centralization and decentralization. For example, Dessein (2002) and Alonso et al. (2008) have studied this issue. These papers do not include transfers as a means to elicit private information. If transfers are not allowed, the principal has difficulty extracting the agent’s information and therefore delegation may be a better instrument to make use of the private information. Dessein (2002) does include only one agent and cannot study some of the issues considered here. This paper analyzes the allocation of control over a project within an organization with a principal and two informed agents. Each agent has some expertise over one dimension of the project. The principal may retain control over the project, thus centralizing decision making. She may also delegate this control to either one of the agents. The first agent becomes a middle man in a hierarchy. This agent is responsible for contracting with the other (bottom) agent. The bottom agent has delegated decision making and has control over the project. Such hierarchical delegation has been studied by Melumad et al. (1995). Under the assumptions underlying the Revelation Principle, MMR show that centralization and the hierarchy can equivalently implement the second-best allocation. We assume that players cannot commit to the initial contract, and they may therefore renegotiate it if it seems profitable to do so. The communication costs imposed by renegotiation differ whether control is centralized with the principal or delegated to one agent. Furthermore hierarchical delegation may entail some loss of control by the principal. This loss of control depends on whether the principal can observe the contract between the middle agent and the downstream one or not. When the principal can observe the downstream contract, we show that a hierarchy is always preferred to centralization. A hierarchy minimizes communication costs by limiting the possibility for renegotiation because the decision is delegated to the downstream agent who does not need to communicate. There is no loss of control by the principal because the principal can control the interaction between the two agents in her contract with the middle one. We show that the agent with the least important information becomes the middle agent and the informationally most important agent is at the bottom with control over the project. When the principal cannot observe the downstream contract, there is a potential loss of control. First, we show that, among the hierarchical structures, the agent with the most important information becomes the middle agent and the informationally least important agent is at the bottom with control over the project. This is exactly the opposite to the case when the downstream contract is observable. When the contract is observable, the principal wants to minimize rents, which is achieved by having production distortions. Giving control to the informationally most important agent allows this. When the contract is not observable, the principal gets a fixed amount of profits. This fixed amount is maximized with an efficient production level, that is, with less distortions. Giving control to the informationally least important agent allows this. We also show that centralization can dominate any hierarchical delegation when the two agents are “informationally” similar (this notion will be formally defined). If one agent’s information is significantly more important than that of the other, it is preferable to delegate control over the project to one agent rather than centralizing it. The next section introduces the model. Section 3 solves for a benchmark case and characterizes the second-best allocation. Section 4 characterizes the centralized structure and the hierarchical delegation for the two cases of observability and non-observability of the downstream contract. Section 5 compares these structures. A discussion follows. All proofs are relegated to the “Appendix”.",2
20.0,2.0,Review of Economic Design,07 March 2016,https://link.springer.com/article/10.1007/s10058-016-0186-0,Discrimination in contests: a survey,June 2016,Yosef Mealem,Shmuel Nitzan,,Male,Male,Unknown,Male,"The numerous applications of contest theory include promotional competitions, litigation, internal labor market tournaments, rent-seeking, R&D races, political and public policy competitions and sports (Konrad 2009; Congleton et al. 2008). Contest design may involve the endogenous determination of relevant institutional characteristics by contest designers; economic and political entrepreneurs who wish to maximize the total efforts made by the contestants. These characteristics include various forms of discrimination between the contestants. In fact, evidence on discrimination is abounding in all the above mentioned applications and it can take various forms. More specifically, the various types of discrimination can be observed in the admission policy of universities that is often based on affirmative action, Fu (2006) and Franke (2012), in procurement auctions, Feess et al. (2008), in political rent-seeking contests, Epstein et al. (2013), in litigation games, Bernardo et al. (2000), in promotion competitions and bonus tournaments designed by employees and in the design of sport competitions (Szymanski 2003). In the latter context, handicapping is very common.Footnote 1 The application of our model is not limited to the social sciences. In particular, it also seems to fit disciplines in the life sciences such as ethology and biology, because the assumptions of contest resolution based on some lottery and effort maximization seem plausible.Footnote 2
 The contest designer can discriminate on the basis of moral, social or welfare considerations. Typically, such considerations are not related to his interest of increasing the contestants’ efforts, but to his attempt to correct a moral, social or economic distortion. Indeed discrimination is commonly defined as differential treatment of individuals with respect to an exogenous marker that is not related to the designer’s task and, in particular, to his objective. But in the present study we broaden the standard concept of discrimination such that it serves as a means of attaining the maximal expected effort by the contestants. In our study discrimination therefore includes differential treatment of contestants based on their identity (overt or head starts-discrimination), on the extent of their incurred efforts (covert discrimination) and on their different valuations of the contested prize (direct discrimination). These different means of affecting the contestants’ efforts are presented in more detail in Sect. 3. Discrimination motivated by the designer’s objective to maximize the contestants’ efforts in other contexts has been adopted in different contest frameworks. This is notable in bribery games, Clark and Riis (2000), Lien (1990), Hillman and Katz (1987) and Hillman (2013) where a bribe to the contest winner is conceived as a form of non-standard discrimination as long as it induce increased efforts by the contestants. In fact, sometimes the bribes even entail creation and extraction of rents that are ultimately contestable, see Hillman and Katz (1987) and Hillman (2013). In other contests, attention has been devoted to the effect of using tie-breaking rules, caps and minimum bids (Che and Gale 1998; Gavious et al. 2003; Sahuguet 2006; Cohen and Sela 2007; Szech 2015). A broader view of discrimination can also cover the control by the designer of the group of contestants and, in particular, their number, the optimal order of the contestants in a simultaneous or in a multi-stage sequential contest and the control of the rules of the game such that the designer openly or latently prefers one of the contestants assigning him a head starts, see Moldovanu and Sela (2006), Li and Yu (2012), Franke et al. (2014b) and Segev and Sela (2014a, b). Finally note that two common discriminatory methods are employed to achieve distributional goals in the context of auctions (Athey et al. 2011). One approach is to set aside a fraction of contracts for targeted contestants (firms). An alternative method is to provide bid subsidies for favored contestants. These modes of discriminatory policies are disregarded in our survey because they have been studied in the auction setting but not in the context of our all-pay auction and lottery contest setting. In light of the numerous applications of contest theory and the significance of discrimination in contest design, the objective of the current essay is to shed light on the following questions: What are the means of reducing asymmetry in contests? Under what circumstances leveling the playing field by applying discrimination increases the contestants’ efforts? Is there a reason for the contest designer to preserve some asymmetry between the contestants? What are the maximal possible efforts discrimination can induce and how can they be reached? In particular, what combination of CSF and discrimination policy induces these efforts? Are two modes of discrimination better than one? In light of the answers to the above questions, what conclusions can be drawn regarding the effectiveness of an all-pay-auction (APA) relative to that of simple or general lotteries? Our survey attempts to present a comprehensive synthesizing view of the existing results on discrimination in contests and their interrelationships assuming a simultaneous game of investment by the contestants under complete information.Footnote 3 It is intended to have a clear added value to researchers of contest theory as well as to those who are not particularly interested in contest theory, but are aware of the significance of the topic to economics and, in particular, to political economy. To introduce the modes of discrimination on which we focus, let us first present the basic contest that we study.Footnote 4
",17
20.0,3.0,Review of Economic Design,01 March 2016,https://link.springer.com/article/10.1007/s10058-016-0187-z,Creating a winner’s curse via jump bids,September 2016,David Ettinger,Fabio Michelucci,,Male,Male,Unknown,Male,"Jump bidding refers to the practice of calling a price strictly higher than the current highest standing bid in an open ascending auction.Footnote 1 The use of jump bids is widespread both in auctionsFootnote 2 and in other markets not explicitly regulated by auction rules such as corporate takeovers.Footnote 3 The existing work on the topic is typically based on signaling models in which a bidder places a costly jump bid to reveal that he has a favorable type.Footnote 4
 An alternative explanation to signaling has been introduced by Ettinger and Michelucci (2015). It is based on the somehow opposite motive that a jump bid can be used to limit the amount of information that can be aggregated by hiding the exact drop out prices of the bidders who do not match a jump bid. In that paper we considered a setting where the identity of a bidder’s opponent holding the highest ex post valuation depended on the exact drop out prices. In that context, we showed that a jump bid may reduce the expected price paid by a bidder by pooling drop out prices for which the identity of the opponents with highest ex post value differs. This paper enriches the hiding/manipulating information motives for jump bidding by looking at a setting for which the incentive to hide information comes from a different source: a winner’s curse argument. We have in mind situations in which a subset of the bidders have better information about some common value elements of the object on sale (perhaps because they are insiders/incumbents) than others (entrants), and where this informational asymmetry might disappear or narrow because of the information that can be aggregated in the open ascending auction. We show that the better informed bidder may call a price in order to prevent this information revelation process. The reason is that by preserving an informational advantage the informed bidder forces the less informed one to take into account of a potential winner’s curse. This fact dampens the expected willingness to pay of the less informed bidder and may decrease the expected price paid by the better informed one. Compared to our previous work, this paper offers the following contributions. First, it provides an alternative reason why manipulating information via a jump bid can be part of an equilibrium strategy.Footnote 5 Second, it adds to the literature two features of equilibrium jump bids. The first one is that the jump bid may fail to completely hide the value of the common value component. The second one is that the probability to jump bid might decrease with the type of the bidder who places the jump bid.Footnote 6
",22
20.0,3.0,Review of Economic Design,04 May 2016,https://link.springer.com/article/10.1007/s10058-016-0189-x,Organizational power: Should remuneration heterogeneity mirror hierarchy?,September 2016,Philipp E. Otto,Friedel Bolle,,Male,,Unknown,Mix,,
20.0,3.0,Review of Economic Design,01 June 2016,https://link.springer.com/article/10.1007/s10058-016-0190-4,Strategic behavior and social outcomes in a bottleneck queue: experimental evidence,September 2016,Jesper Breinbjerg,Alexander Sebald,Lars Peter Østerdal,Male,Male,Male,Male,"In many everyday situations people are confronted with bottlenecks and have to spend time in queues waiting to be served. Examples range from ordinary situations, such as people waiting in line for a grand opening sale, to extraordinary situations, such as American citizens applying online for health insurance during the initial release of ObamaCare. The queues created by such bottlenecks are often handled using a first-in-first-out (FIFO) procedure. Given its widespread use, a large body of literature has focused on the implications of FIFO in situations where people independently decide when to join a queue (e.g. Vickrey 1969; Glazer and Hassin 1983; Arnott et al. 1999; Hassin and Kleiner 2011 and references therein). Now, while FIFO is intuitively fair and acceptable to most people, it is not the only way, and sometimes not even the optimal way, of settling a queue (Hassin 1985). Other well-known ways to organize queue priorities include last-in-first-out (LIFO) or service-in-random-order (SIRO). Of course, as queue disciplines govern the service priority of the people waiting to be served, they influence the strategic incentives for arrival. However, given the focus of the existing literature on FIFO, not much is known about the differential incentive effects of other queue disciplines like LIFO or SIRO.Footnote 1 Our analysis advances into this gap by theoretically and experimentally exploring the differential incentive effects of these three well known (classic) queue disciplines: FIFO, LIFO and SIRO. We consider a queuing environment where impatient players decide when to arrive for service after the opening of a single bottleneck facility. The players’ objective is to be served as early as possible and spend minimal time in the queue. To ensure analytical tractability of the resulting equilibrium arrivals, we restrict our analysis only to environments of three players. In such setting, we analyze the differential incentive effects of the three aforementioned queue disciplines, the resulting social welfare, and the players’ ex-ante and ex-post fairness perceptions. To create a theoretical benchmark for our experimental analysis, we first derive pure and (symmetric) mixed strategy Nash equilibrium arrivals for all three queue disciplines in a three-player, discrete-time queuing game and compare the quantitative differences between the corresponding equilibrium welfare levels. Subsequently, we experimentally test our behavioral predictions and welfare properties. A first glance at the results: First, our theoretical and experimental analysis shows how queue disciplines affect strategic behavior and thus arrival patterns. Arrival distributions differ substantially between LIFO and the two other queue disciplines. Agents arrive significantly later under LIFO compared to FIFO and SIRO. In comparison, FIFO and SIRO lead to congruent arrival patterns. Second, expected queuing times are shortest under LIFO as participants arrive more smoothly as compared to FIFO and SIRO. Third, despite the fact that LIFO theoretically provides the highest welfare given the specific parameters used in our analysis, the welfare provided by LIFO in our experiment is not significantly different from that provided by FIFO and SIRO. Lastly, across all queue disciplines subjects’ expected payoffs are higher in our experiment compared to our theoretical equilibrium predictions. The reason why subjects successfully outperform the equilibrium in regards to expected payoff is that they reduced the expected queuing time by leveling out their arrivals. The paper is organized as follows: Sect. 2 presents related literature. Section 3 introduces the queuing game and model assumptions. Section 4 establishes and compares the equilibrium arrivals under each queue discipline. Section 5 introduces the experimental design and hypothesis for testing. Section 6 presents the experimental results. Section 7 interprets and discusses the results. The supplement contains proofs and other technical material.",11
20.0,3.0,Review of Economic Design,07 June 2016,https://link.springer.com/article/10.1007/s10058-016-0191-3,The airport problem with capacity constraints,September 2016,Youngsub Chun,Boram Park,,Unknown,Unknown,Unknown,Unknown,,
20.0,4.0,Review of Economic Design,30 August 2016,https://link.springer.com/article/10.1007/s10058-016-0192-2,Dynamic incentive contracts with termination threats,December 2016,Nadide Banu Olcay,,,Female,Unknown,Unknown,Female,"The provision of incentives is an important issue in economics. A large theoretical literature on the principal-agent model assumes that the agent’s incentives are not aligned with those of the principal (e.g., Holmstrom 1979; Grossman and Hart 1983; Gibbons and Murphy 1992). The problem of the principal is then to find the optimal contract that would induce the agent to take the right actions. This literature provides important insights into the determinants of pay, but seldom considers alternative ways to provide incentives. In reality, firms commonly provide incentives to workers through the threat of terminating the relationship if performance is not satisfactory.Footnote 1 This threat can offset the need to provide incentives through other means, such as explicit cash compensation. The use of termination threats as an incentive device is not peculiar to firm-employee relationships. Other examples include credit relationships where lenders threaten to cut off future credit lines in case of default (e.g. Stiglitz and Weiss 1983), the related problem of sovereign debt (e.g. Eaton and Gersovitz 1987), and landlord-tenant relationships where the landlord reacts to poor crops by evicting the share-cropper (Banerjee et al. 2002). In all these cases there is a trade-off between termination threats and other incentive devices: lenders terminate the relationship rather than raise the interest rate, and landlords terminate the contract rather than lowering the share of the cropper. The case of share-cropping is the example closest to my model. However, Banerjee et al. (2002) do not consider the optimal mix of alternative instruments for incentive provision, and the dynamic interaction between them, which is the main focus of my paper. 
Lazear (1979) provides a theoretical explanation for the robust empirical finding that wage profiles are upward sloping over time. He shows that a wage profile which pays less than the marginal productivity when the agent is young, but more when he gets old, solves the moral hazard problem over the agent’s employment horizon.Footnote 2 The fact that the agent’s marginal product is below the wage at later stages in the career necessitates mandatory retirement. Lazear’s arguments rely on a crucial assumption: the firm commits to a long-term contract which specifies all future wages as well as mandatory retirement date. In this paper, I study optimal contracts in a pure moral hazard model which is repeated over a finite number of periods. My model differs from Lazear (1979) by ruling out long-term commitments. The principal offers instead a sequence of short-term contracts, which specify the wage and probability of termination as a function of the quality of the output. The quality is verifiable and hence contractible, but the agent’s effort is not observed. Long-term contracts are ruled out by assumption. A short-run contract specifies that the agent is fired if the observed quality is too low. Once the agent is fired, the principal cannot rehire him. I focus on a situation where the agent’s outside option is not very good, so the termination threat can provide good incentives. Of course, termination is not costless to the principal, as she loses the expected future surplus from the relationship. The optimal short-run contract trades off the gain from providing current incentives against this future loss. In this dynamic setting, I consider the role of the termination threat, and how it interacts with explicit wage incentive. I find that the two incentive devices are substitutes and that both are used more intensely as the agent gets more senior. The agent has rational expectations: by backward induction, he realizes that if he stays in the relationship, over the rest of the horizon he will get a surplus, as long as it is the interest of the principal to pay high wages in the future (even though there is no commitment to this). If the agent expects a lot of future surplus from continuing the relationship (so a termination threat provides high-powered incentives) but the principal’s expected future surplus is not too high (so the cost of firing the agent is not too high) then the principal thinks the benefit of a termination threat exceeds the cost. In this case, the optimal short-run contract includes a termination threat. But if the principal’s expected future gain from the relationship is high, while the agent does not expect a lot of future surplus, then the cost of a termination threat exceeds the benefit, and the optimal short-run contract does not include any termination threat. 
Gibbons and Murphy (1992) derive an upward sloping wage profile in a model with limited commitment. However, their result is due to career concerns in the presence of symmetrically unknown ability of the agent. When updated beliefs about ability depend on observed performance, the worker has strong incentives to work hard when he is young. But as he gets old, his incentive to influence the employer’s belief about his ability gets weaker. Therefore, optimal wage incentives should increase. Gibbons and Murphy (1992) do not consider provision of incentives via termination threats. In my model there is no career concerns, yet the wage profile is upward sloping when the principal uses both termination threats and incentive pay. More recent studies, including Subramanian et al. (2002) and Baç and Genç (2009), have studied adverse selection models with limited commitment and termination option. With adverse selection, incentive pay and termination threats are complements: firms providing high-powered monetary incentives have more reason to fire their employees for poor performance. The reason is that the more strongly pay depends on performance, the more likely it is that poor performance is caused by low ability rather than low effort. Thus, under adverse selection, the termination threat functions as a sorting mechanism which complements monetary incentives, rather than “economizes” on them as in my pure moral hazard model. Hartzell (1998) studies incentive pay in a pure moral hazard problem with an exogenous probability of termination. He finds an inverse relationship between optimal pay and the probability of termination. In my model, the termination threat is optimally chosen as a part of the employment contract. In my model, the optimal short-run contract depends on how the future surplus is expected to be divided between the two parties. This is reminiscent of MacLeod and Malcomson (1987) and MacLeod and Malcomson (1989) who show that contracts can be made self-enforcing if the relationship generates sufficient surplus in the future.Footnote 3 However, they assume output is not verifiable, so that a contract which specifies the wage as a function of output has to be self-enforced. In contrast, I assume the quality of output is verifiable so that (short-term) performance-based contracts are legally binding. Efficiency wage models such as Shapiro and Stiglitz (1984) also show how the termination threat prevents shirking. But again, Shapiro and Stiglitz (1984) assume only the firing decision is contingent on performance, the wage is not. In reality, firms use both performance pay and termination threats, and the optimal mix of these incentive devices is studied in my model. MacLeod and Malcomson (1987, (1989) and Shapiro and Stiglitz (1984) assume the horizon is infinite and consider stationary contracts. In contrast, I assume the agent has a finite life, and the series of short-run contracts will not be stationary. This allows me to study how the optimal mix of pay-for-performance and termination threat changes over the agent’s finite life. Towards the end of his life, the expected future surplus from the relationship dwindles, and the termination threat becomes less effective. Therefore, even with short-run contracts and in the absence of “career concerns” , both the “stick” and the “carrot” are used more intensely over time. That incentive wage and termination profiles are both upward sloping in this sense is one of the main findings of my model. Authors such as Lazear have studied the case of perfect commitment to an enforceable contract that lasts the agent’s life time. In my model, a short-term enforceable contract is signed in the beginning of each period and expires at the end of the period. To get unambiguous results and the simplest analysis, I let the length of each period go to zero and consider the model in continuous time. This yields a polar opposite to the case of perfect commitment, where only (very) short-run commitment is possible. The literature on continuous time principal-agent models was initiated by Holmstrom and Milgrom (1987). Among others, Schattler and Sung (1993), Sung (1995) and Ou-Yang (2003) further develop this approach.Footnote 4 Sannikov (2008) creates a continuous time model for which the solution can be characterized by an ordinary differential equation. DeMarzo and Sannikov (2006) show how this approach can work in an application to agency costs and capital structure. Cadenillas et al. (2005) show how the problem can be solved with full information, while Cvitanić and Zhang (2006) extend the Holmstrom and Milgrom (1987) model to include adverse selection. However, these models do not consider commitment problems. A few papers have studied short-term contracts in the continuous time limit. DeMarzo and Sannikov (2006) study the optimal contract in a cash flow diversion model, but in which there is adverse selection and the termination threat is used as a sorting mechanism. My approach is more similar to Guriev and Kvasov (2005), who present a continuous time moral hazard problem where the contract is renegotiated at every point in time. None of these continuous-time principal-agent models allow termination threats, and they do not consider the issues that I address in my model. A number of articles have asked the question: will the results derived in models with long-term contracts still hold if such contracts are no longer feasible? Chiappori et al. (1994) show that short-term contracts can replicate an optimal long-term contract provided the latter is renegotiation proof. Fudenberg et al. (1990) argue that the agent’s perfect access to credit markets is key to the optimality of short-term contracts. Malcomson and Spinnewyn (1988) show that if short-term contracts can punish the agent sufficiently severely, there are no gains from full commitment; whereas for Rey and Salanie (1996) it is the absence of asymmetric information at the recontracting dates that matters. None of these results apply to my model. In my model, if the principal could commit to a long-term contract, she could use a “review strategy”: she could pay the minimum wage initially, but later on, pay a high wage if the quality was high in a sufficiently large fraction of the previous periods. When the number of periods is sufficiently large, this long-term contract would approximate the first best (Radner 1985). This would essentially be the Lazear (1979) model of perfect commitment to long-term contracts. Even if no commitment is possible, if the horizon is infinite then Radner (1985) shows that this kind of contract can be made self-enforcing and the first best can still be approximated. In my model, such “review strategies” cannot be made self-enforcing because the horizon is finite: the principal would renege on a Radner-type contract when the agent approaches the end of his finite life. Therefore, the lack of commitment means there is no possibility of approximating the first best in my model. The presentation in this paper is organized as follows. In Sect. 2, we describe the basic model in discrete time and emphasize some of the main insights. We consider the continuous time version in Sect. 3, and state our main theorem. Section 4 presents the proof of the main theorem. Section 5 contains comparative dynamics analysis. Section 6 concludes. All proofs are in the “Appendix”.",
20.0,4.0,Review of Economic Design,15 September 2016,https://link.springer.com/article/10.1007/s10058-016-0193-1,Vertical syndication-proof competitive prices in multilateral assignment markets,December 2016,O. Tejada,M. Álvarez-Mozos,,Unknown,Unknown,Unknown,Unknown,,
20.0,4.0,Review of Economic Design,20 September 2016,https://link.springer.com/article/10.1007/s10058-016-0194-0,New axioms for immediate acceptance,December 2016,Yajing Chen,,,Unknown,Unknown,Unknown,Unknown,,
21.0,1.0,Review of Economic Design,01 April 2016,https://link.springer.com/article/10.1007/s10058-016-0188-y,Mechanisms for combinatorial auctions with budget constraints,March 2017,Phuong Le,,,,Unknown,Unknown,Mix,,
21.0,1.0,Review of Economic Design,05 January 2017,https://link.springer.com/article/10.1007/s10058-016-0196-y,Ex-ante efficiency in assignments with seniority rights,March 2017,Antonio Miralles,,,Male,Unknown,Unknown,Male,"Preexisting priority rights are present in many assignment problems. For example, in school choice, a child whose parents apply for the last slot at a public school cannot typically occupy it if the parents of another child with a sibling already attending the school want that slot (the so-called sibling priority). There are many priority criteria in many different assignment problems: proximity to the school, low income, or being organ donor in “kidney exchange”. This paper focuses attention on (possibly weak) priority structures that are object-invariant (uniform), that is, independent from the object for which agents are competing. A paradigmatic example of this kind of problems is the assignment of students to college residences with seniority rights. The practical motivation question of this paper is whether there is a mechanism that respects object-invariant priorities while it attains good ex-ante efficiency properties. Is the respect for uniform priorities compatible with ex-ante Pareto-efficiency? As we will see, the answer is “generically yes”. A simple mechanism is proposed that achieves both objectives. We introduce the sequential pseudomarket (SP), an extension of Hylland and Zeckhauser’s (1979) pseudomarket.Footnote 1 In SP, ordered groups of agents (top-priority agents, second-priority agents...) are called in turns that participate in the pseudomarket for the remaining objects. A SP-equilibrium is a sequence of pseudomarket equilibria turn by turn. It is easy to see that SP encompasses a family of mechanisms whose opposite extremes are serial dictatorship and pseudomarkets without priorities.Footnote 2 Considering ordered groups as priority groups, it is also straightforward to see (Lemma 1) that any SP-equilibrium assignment respects uniform priorities in the ex-ante stability sense (Kesten and Ünver 2015). This makes SP suitable for random assignment problems with uniform weak priorities.Footnote 3
 In principle, the SP mechanism cannot guarantee that its equilibrium outcome is ex-ante efficient.Footnote 4 For that reason we propose a new, weaker notion of efficiency, namely consistent weak ex-ante efficiency (CWEE). This notion of efficiency is the result of applying the Consistency requirement (see Thomson, 2015, for a recent survey on this concept and its relevance) to the notion of weak ex-ante Pareto-efficiency of a random assignment.Footnote 5 Consistency in this context means that, after removing any set of individuals and their assigned probabilities from the economy, the weak ex-ante efficiency property of the random assignment holds in the remaining economy.Footnote 6 In a characterization result, we show that a random assignment is CWEE if and only if it can be generated by an SP-equilibrium for some partition of the set of agents into ordered groups (Theorem 1). This result contains both First and a Second Welfare Theorems for random assignment economies, when the efficiency notion is CWEE.Footnote 7
 It is easy to see that ex-ante efficiency implies CWEE, which in turns implies weak ex-ante efficiency. Converses are not true in the random assignment economies we study (see Example 1 in the main text.) But then, how far is CWEE from ex-ante efficiency? Theorem 2 brings good news: we can generically state that every CWEE (and hence any SP-equilibrium) random assignment is ex-ante Pareto-optimal.Footnote 8 This result is somewhat striking since differently priority-ranked agents face different relative prices, a fact that could have caused inefficiency on the random assignment. A second look at the problem clarifies it. Theorem 1 allows us to think of CWEE random assignments as SP-equilibrium random assignments. In a SP-equilibrium random assignment, no agent could be strictly better-off after trading assignment probabilities with lower-ranked agents. Whatever the latter agents obtained, it was zero-priced for the former agent, and she discarded it. However, it is still possible that some trades leave the former agent indifferent while benefitting lower-ranked agents. Two generically met assumptions disregard the latter concern. First, that no agent is indifferent between two object types (Assumption 1)Footnote 9 Second, a regularity condition for preferences (Assumption 2) which embeds the assumption of Bonnisseau et al. (2001) used for linear utility economies (see their Lemma 4.1) namely that there is no cycle of marginal rates of substitution of different agents that multiplied altogether yield one. Our assumption is quite technical yet it can be explained from the next question: From each possible initial assignment, is there any feasible redistribution of probabilities between two or more agents in which all of the affected agents remain indifferent? If the answer is no for all possible initial random assignments (which happens genericallyFootnote 10), the regularity condition is satisfied. Our Assumptions 1 and 2 imply an important side result (Proposition 1): each pseudomarket price equilibrium has a unique associated equilibrium assignment. And ex-ante suboptimality of a CWEE allocation (the concern in the previous paragraph) can only arise when this is not the case, as shown along the proof of Theorem 2. We remark at this point that weak ex-ante efficiency alone does not generically imply Ex-ante Efficiency. The generic property of CWEE in Theorem 2 is not the fruit of a “sandwiching effect” between weak ex-ante efficiency and ex-ante efficiency. More than thirty years after the seminal paper by Hylland and Zeckhauser, pseudomarkets are attracting increasing interest both in finite and continuum economies.Footnote 11 Examples of recent papers are Azevedo and Budish (2015) on strategy-proofness in the large that applies to pseudomarkets, or Budish et al. (2012) on pseudomarket mechanisms for multidimensional assignment. We contribute to this literature by providing a proper and simple combination between pseudomarket and serial dictatorship that performs satisfactorily in assignment problems with object-invariant priority structures. This paper is closely related with two other recent pieces of research: Miralles and Pycia (2014) and He et al. (2015). The first paper establishes a Second Welfare Theorem for random assignment economies. In virtue of this result, one could have obtained any ex-ante Pareto-efficient assignment, including one that respects uniform priorities, by fine-tuning individual incomes and then letting agents purchase probability bundles in a competitive market. The practical advantage of the approach taken in the current paper is that, instead of adapting incomes to agents’ preferences so that uniform priorities are respected, we just need to assign turns. Success in reaching an ex-ante efficient random assignment is generically guaranteed. The SP method becomes informationally less demanding, and hence easier to implement. The second paper analyses the question of how we can adapt the Pseudomarket mechanism to meet any set of (possibly weak) priority criteria. This includes uniform priorities as a special case. The suggested solution is an alteration of prices depending on priority status. For each object type there would be a critical priority level which pays the market price. Instead, higher priority levels enjoy zero price for the object, whereas lower priority levels face infinite price. The sequential pseudomarket is an example of such a mechanism for the case of uniform priorities. Sequential Pseudomarkets deserve however particular attention, since they generically guarantee agent-side ex-ante efficiency, a nice property when the other side of the market is constituted by objects. For more general priorities, only two-sided unconstrained efficiency is guaranteed, considering priorities as objects’ (weak) ordinal preferences. Section 2 presents the basic notation and definitions of the model. Section 3 introduces SP and its stability properties. Section 4 contains the Welfare Theorems linking SP with CWEE. Section 5 establishes generic equivalence between CWEE and ex-ante efficiency. Section 6 concludes. An “Appendix” contains the proof of Proposition 1 and additional analysis regarding Assumption 2.",2
21.0,1.0,Review of Economic Design,10 February 2017,https://link.springer.com/article/10.1007/s10058-017-0197-5,Priority-driven behaviors under the Boston mechanism,March 2017,David Cantala,Juan Sebastián Pereyra,,Male,Male,Unknown,Male,"Centralized school choice programs are aimed at expanding the capacity of families to choose the school their children will attend. Before the mechanism in place allocates students to schools, families express their preferences by submitting a rank ordered list of schools to a central clearinghouse, and when a school is overdemanded, priorities are used to resolve ties. The Boston mechanism is one of the most widely used procedures. It attempts to assign as many students as possible to their first choice school, and only after all such assignments have been made, it considers the assignment of students to their second choices, and so on. In contrast to the traditional residential-based assignment, the ultimate goal of school choice mechanisms, and the Boston mechanism in particular, is to improve students’ welfare by incorporating their preferences. However, under the Boston mechanism, truth-telling is rarely optimal. Previous studies have shown that families misrepresent their preferences reflecting district school bias: they declare those schools where they have high priority in a higher position than in the true preference. We investigate whether welfare gains are possible when the Boston mechanism is used. We show that when students report their preferences reflecting district school bias, the Boston mechanism leads to the priority-optimal stable matching. Thus, the Boston mechanism achieves no welfare gains over simply assigning each student to his or her neighborhood school. Equivalently, the final allocation is purely shaped by schools’ priorities. Formalizing the idea of district school bias, we focus on the safe schools of each student. We say that a school is safe for a student if her position in the priority order at the school is higher than the capacity of the school. We consider a model where students have a common ranking over the schools and then give a bonus to their safe schools in their submitted preferences. Thus, given a profile of schools’ priorities, a profile of preferences is priority-driven if individual differences in the submitted preferences may enter only through the district school bias. The assumption captures the idea of preferences reflecting district school bias and isolates the effect of this behavior. We analyze the performance of the Boston mechanism when students’ submitted preferences are priority-driven. We first prove that there is a unique stable matching which is the outcome of the Boston mechanism, and then efficient (Proposition 1). It is well known that the assignment of the Boston mechanism may not be stable under the reported preferences.Footnote 1 Our result shows that if students manipulate the Boston mechanism by submitting priority-driven preferences, the matching found by the mechanism is stable under the submitted preferences. Moreover, if each student has at most one safe school, the matching is also stable under the true preferences. Second, we show that when students submit priority-driven preferences, all the application-rejection mechanisms introduced by Chen and Kesten (2017) coincide in the allocation of students to schools (Proposition 2).Footnote 2 Moreover, when students have at most one safe school, we prove that these mechanisms also coincide with the top trading cycles mechanism (Proposition 3). Thus, existing mechanisms aimed at producing stable or efficient outcomes (relative to the submitted preferences) produce identical (and stable) outcomes when students play safe. Finally, we investigate the extent to which Proposition 1 holds when the condition of priority-driven preferences is relaxed. In particular, we keep the assumption that students increase the position of their safe schools in the submitted preferences, but we allow for heterogeneous preferences. We model students’ submitted preferences as the weighted sum of three components. The first component reflects a common ranking of schools, the second gives more utility to the safe school of the student, and the third component is an idiosyncratic shock. Priority-driven preferences correspond to the cases where only the first two components of the utility have positive weight. We show that as submitted preferences tend to be priority-driven (i.e., the weight of the third component tends to zero), the difference between the Boston matching and the priority-optimal stable matching tends to zero. Moreover, computational simulations show that we can relax the definition by allowing non trivial amounts of idiosyncratic shocks in students’ preferences, and our main result holds for almost all students. Many empirical and experimental papers have shown that schools’ priorities drive manipulation strategies. Abdulkadiroglu et al. (2006) study the assignment of seats at public schools in Boston when the Boston mechanism was in place, and find that some families submitted their preferences strategically by ranking their safe school in the first positions of the preferences. Calsamiglia and Güell (2014) conduct an empirical investigation in Barcelona, where the Boston mechanism is used. They find that many families apply to their safe schools, and more precisely, that families declare as their most preferred school the one where they have the highest priority. Chen and Sönmez (2006) conduct an experiment to analyze agents’ behavior when the Boston mechanism is used. Their findings show that two-third of the subjects misrepresent their preferences using “district school bias”: they declare the district school (where they have high priority) into a higher position than that in the true preference order. Similar evidence is also found by Pais and Pintér (2008), Chen et al. (2013), and Chen and Kesten (2013).Footnote 3
 Finally, it is worth noting that there is evidence of this type of behaviors even under strategy-proof mechanisms. Indeed, Echenique et al. (2016) experimentally study the Deferred Acceptance mechanism and find that subjects, instead of acting truthfully, “skip” down their true preferences. That is, when making a proposal decision, participants take into consideration how participants on the other side of the market perceive them. In our framework, this behavior implies that students consider their priorities when submitting their preferences.Footnote 4
",1
21.0,1.0,Review of Economic Design,27 December 2016,https://link.springer.com/article/10.1007/s10058-016-0195-z,Nash implementing social choice rules with restricted ranges,March 2017,M. Remzi Sanver,,,Unknown,Unknown,Unknown,Unknown,,
21.0,2.0,Review of Economic Design,01 June 2017,https://link.springer.com/article/10.1007/s10058-017-0201-0,(No) Foundations of dominant-strategy mechanisms: a comment on Chung and Ely (2007),June 2017,Tilman Börgers,,,Male,Unknown,Unknown,Male,"
Chung and Ely (2007) (hereafter abbreviated as “CE”) address the important question when a mechanism designer who does not want to make restrictive assumptions about agents’ beliefs about each other will find it optimal to choose a dominant strategy mechanism. While this question can be asked in the context of many applications of mechanism design, CE study the question in the specific context of an expected revenue maximizing seller who wants to sell a single object to buyers who have quasi-linear preferences and private values. CE construct a notion of optimality for mechanisms that reflects that the mechanism designer wants to make no assumptions about agents’ beliefs. In particular, importantly, the mechanism designer in CE does not assume that agents’ beliefs are derived from a common prior. CE refer to mechanisms that are optimal in their sense as having “maxmin foundations.” CE’s main result provides sufficient conditions for dominant strategy auctions to have maxmin foundations. These sufficient conditions generalize Myerson’s (1981) regularity conditions by allowing for correlated values. In this comment we propose a refinement of CE’s definition of optimality. Some mechanisms, although having maxmin foundations, violate the refinement. We show that this is in particular true for dominant strategy mechanisms, if there are at least three bidders. Thus, if our refinement of maxmin foundations is adopted, then dominant strategy mechanisms do not have foundations. The refinement rules out mechanisms for which there exist alternative mechanisms that never yield lower expected revenue than the given mechanism, and sometimes strictly higher expected revenue than the given mechanism. The argument that proves that dominant strategy mechanisms do not satisfy this refinement is simple. An alternative mechanism that shows that dominant strategy mechanisms do not satisfy the refinement can be constructed by modifying the dominant strategy mechanism such that bidders can enter into side bets with each other regarding a third bidder’s type, and charging any bidder who enters into such a side bet a fee. If bidders have subjective beliefs that are not derived from a common prior, then sometimes bidders find such side bets so profitable that they are willing to pay the fee to the auctioneer. The fact that CE’s model allows subjective beliefs that are not derived from a common prior is crucial to this note’s argument. Our argument is not valid if common priors are assumed, because Milgrom and Stokey’s theorem (1980) on the impossibility of speculative trade in the presence of a common prior rules out side bets. The absence of a common prior is important for CE’s proof of their main result, as this proof is based on the construction of a particular type space with subjective, inconsistent beliefs. CE do discuss how their results would change if only common prior type spaces were considered. We return to this issue in Sect. 4. The argument that we make here does not only apply to the auction setting of CE. It also implies, for example, that for analogous settings with public goods there are no interim welfare optimal mechanisms. In short, the agenda of robust mechanism design faces difficulties when quasi-linear preferences are assumed and beliefs that are not derived from a common prior are allowed.Footnote 1
",7
21.0,2.0,Review of Economic Design,11 February 2017,https://link.springer.com/article/10.1007/s10058-017-0198-4,The principal-agent problem with smooth ambiguity,June 2017,Christian Kellner,,,Male,Unknown,Unknown,Male,"In its most standard form the principal-agent problem is typically described in the following way. The owner of a firm, the principal, has to decide how to remunerate a manager, the agent, who is in charge of running the firm. The effort of the agent determines, to a large extent, the quantity produced by the firm, but the firm’s output is also influenced by random events that are beyond the control of the manager. The problem arises as the manager’s effort cannot be observed (or otherwise inferred) by the principal. As it is commonly assumed that the principal is risk neutral and the agent risk averse, the solution to this problem arises as the optimal trade-off between risk-sharing and incentives. Thus it should be apparent that the optimal wage scheme is sensitive to the agent’s reaction to non-deterministic payoffs. Most of the literature assumes that the agent (and the principal) are expected utility maximisers. In the context of the principal-agent problem, the expected utility framework requires the agent to treat the uncertainty about her monetary payoffs in the same way as a lottery over monetary prizes with known probabilities. The well-known Ellsberg paradox however has convincingly cast some doubt at the validity of such an assumption (Ellsberg 1961). In response, the decision theory literature has generalised the expected utility model to accommodate aversion (as well as other attitudes) towards ambiguity, which can explain the behaviour observed in the Ellsberg paradox. Specifically, ambiguity is defined as subjective uncertainty about outcome distributions. Our paper addresses the question to what extent the traditional analysis of the principal-agent problem remains valid in situations where ambiguity about the consequences of the agent’s actions prevails. Why might ambiguity be of particular interest in the principal-agent problem? In many cases where moral hazard arises in economic interactions, it seems reasonable to assume that the consequences of the agent’s actions cannot be described with great confidence by a single probability distribution. A good example may be a firm that employs a scientist to develop a new production technique. Yet, in other instances ambiguity may well be less of an issue. If the principal routinely contracts with some agent to do always essentially the same task, then it should be possible to have a precise understanding about the consequences of each of the agent’s actions. This might be true for the relationship between a firm and one of her sales agents. Thus, our approach also enables us to address the question how the optimal incentive scheme will differ between such situations. Additionally, ambiguity can vary between the actions available to the agent. Consider again a scientist: For her, exerting a lot of effort to develop an entirely new approach may have very ambiguous benefits. Exerting low effort (by merely adopting an existing technology) may still not lead to a deterministic outcome, but there may be less ambiguity about the outcome distribution. For the case of a “routine” principal-agent situation only the action which is in fact implemented in equilibrium might be well understood. The other actions available to the agents might very well be of ambiguous consequences (as they are never actually chosen).Footnote 1
 In the principal-agent problem the optimal incentive contract is, in general, inefficient. That is, if the principal could observe the action chosen by the agent she could offer a different wage scheme that makes her better off without harming the agent. As long as the optimal incentive scheme does not leave any rent to the agent, the degree of inefficiency is directly linked to the principal’s profit. Therefore, we want to find out whether profits necessarily decrease when ambiguity matters. Ambiguity may also affect other properties of the optimal incentive scheme. Wage schedules are often expected to be monotonic. This means that if an outcome realises that is better for the principal, the agent receives a higher wage as well. Even without ambiguity additional assumptions are needed to ensure the optimality of monotonic schemes. Thus we seek to show whether similar assumptions are sufficient in the presence of ambiguity, or if stronger assumptions are needed. To address these questions we adopt a recent model of decision making under ambiguity, the smooth ambiguity model (Klibanoff et al. 2005). This is a key difference between this paper and the work by Ghirardato (1994) (which predates the smooth model), who studies the principal-agent problem with a different model of ambiguity. In his paper, the agents use capacities instead of probabilities, which are evaluated using Schmeidler’s Choquet expected utility theory (Schmeidler 1989). A major advantage of using the smooth ambiguity model is that it suggests a clear distinction between ambiguity and attitude towards ambiguity, and allows each to vary separately, while in the Choquet expected utility model these two concepts cannot be separated.Footnote 2 For most parts of the paper, we assume that absolute ambiguity aversion is constant. We will show that without this assumption the individual rationality constraint might not bind in the optimal contract, so that the agent can be strictly better off than under her outside option. Lang (forthcoming) contains a result complementary to this paper for the case of infinitely many effort levels. He finds that under the smooth ambiguity model (and other second-order models) a strictly positive effort level is typically optimal while under first order models of ambiguity aversion, like the maxmin expected utility model (Gilboa and Schmeidler 1989) or the Choquet expected utility model, zero effort may be optimal in some circumstances. Lopomo et al. (2011) show that given Bewley preferences (which can be interpreted as ambiguity averse preferences), the optimal contract may be coarser due to ambiguity. Contracting under vague information is studied in Viero (2012), the effects of loss aversion are investigated in Herweg et al. (2010). Carroll (2015) find that ambiguity about available actions can lead to linear contracts. Di Tillio et al. (2014) and Bose and Renou (2014) provide rationales for designing ambiguous contracts even if there is initially no ambiguity about the relevant distributions. We restrict ourselves instead to studying standard (i.e. unambiguous) contracts in an exogeneously ambiguous setting. For the case of more than one agent, Kellner (2015) shows ambiguity aversion can make the use of tournaments more attractive. Other papers related to moral hazard and ambiguity are Mukerji (1998, 2003). For other more distantly related papers on the implications of ambiguity in economics see the survey by Mukerji and Tallon (2004). Also of interest, as another application of the smooth ambiguity model, in this case to portfolio choice, is Gollier (2011). While it is beyond the scope of this introduction to give an overview of the contributions to the principal-agent literature, it is worth mentioning that our formulation of the problem most directly corresponds to Grossman and Hart (1983) (henceforth GH), which we also use as a reference when we compare our results to the case without ambiguity aversion. Even in the absence of ambiguity, very few general results regarding properties of the optimal contract are available.Footnote 3 Hence, we will follow established practice of restricting the model to special cases, most notably the case of two actions and two outcomes, as this allows us to highlight important channels how ambiguity and ambiguity aversion matters. Regarding the shape of the optimal contract, we will build on the observation of Ghirardato (1994), who finds that, under ambiguity, non-monotonicity may arise even in the case of only two outcomes. We will however also show that in many cases the optimal wage contract will nevertheless entail a positive bonus to the agent if the better outcome realises, for instance if it is in a certain sense unambiguous that the high-cost action results more likely in a better outcome. On the other hand, we identify a new source of non-monotonicity for the case of at least three outcomes: Non-monotonicities may arise if there is more ambiguity about the probability of some outcomes than others. Then the principal might aim at reducing the ambiguity about wages by keeping them similar between the more ambiguous outcomes. We will argue using an example that this effect is not simply a curiosity, but may be quite relevant in the context of real-world incentive contracts. In addition to describing the optimal wage contract, we provide some insight on how the principal’s profits change in response to changes in the problem. In particular, as the smooth ambiguity model provides a separation between ambiguity and ambiguity aversion, we can study an increase in ambiguity and an increase in ambiguity aversion separately. We will show that these two changes may have very different effects. Replacing an ambiguity neutral agent with an ambiguity averse agent, for instance, will be bad for the principal, if she wants to implement the most ambiguous action. If the principal implements the least ambiguous action, then ambiguity aversion might actually increase profits, in marked contrast to the introduction of risk aversion in the standard model. The intuition behind this finding is that, while ambiguity aversion always makes the incentive constraint harder to satisfy, the effect on the individual rationality constraints depends on which action is the more ambiguous one. The situation is different, however, for an increase in ambiguity. Using a suitable definition of a uniform increase in ambiguity, we find that if ambiguity increases uniformly, then the principal’s payoff may increase only if actually the most ambiguous action is to be implemented. This follows since, given our specification of preferences, a payment scheme that is based on the most ambiguous action is less affected from a further increase in ambiguity than any less ambiguous payment scheme. Finally, we look at the case of more than two actions, and show that, given smooth ambiguity aversion, it may be the case that the binding incentive constraint in an optimal contract pertains to a more expensive action. Table 1 summaries the main differences between the expected utility approach, assuming ambiguity neutrality (column 1), and the model allowing for smooth ambiguity aversion (column 2). Column 3 compares our results with the Choquet model using the results in Ghirardato (1994). The remainder of this paper is organised as follows: Sect. 2 introduces the model. Section 3 discusses the properties of the optimal contract. It also motivates why we restrict the model to the case of constant absolute ambiguity aversion in the remaining parts of the paper. Section 4 discusses two properties of the optimal contract for the case of two outcomes only, assuming for the larger part that there are only two actions: First, we discuss sufficient conditions for the monotonicity of the optimal contract. Second, we discuss comparative statics in ambiguity and ambiguity aversion. Section 5 discusses first the robustness of these results to the case of more than two outcomes, and second to the case of more than two actions.",5
21.0,2.0,Review of Economic Design,01 June 2017,https://link.springer.com/article/10.1007/s10058-017-0200-1,On the welfare effects of affirmative actions in school choice,June 2017,Yun Liu,,,,Unknown,Unknown,Mix,,
21.0,3.0,Review of Economic Design,08 April 2017,https://link.springer.com/article/10.1007/s10058-017-0199-3,Pareto efficiency in the jungle,September 2017,Harold Houba,Roland Iwan Luttens,Hans-Peter Weikard,Male,Male,Unknown,Male,"While the competitive market is based on voluntary exchange, the jungle economy is characterized by coercive exchange where stronger agents can take goods from weaker agents. The analysis of the jungle economy provides a complement to the Walrasian equilibrium model with which it shares existence and welfare properties (Piccione and Rubinstein 2007, P&R hereafter). Embedded within the rich tradition of social contract theory following Hobbes and Locke, it facilitates a better understanding of the allocation of initial endowments, the exogenous primitive of the competitive equilibrium model. P&R propose a stylized model in which coercion governs the exchange of resources in the jungle. Coercion is driven by the agents’ preferences over bounded consumption sets and power relations that are described by an exogenous ranking of agents according to their strength. Weaker agents concede to stronger agents without engaging in costly conflict. The jungle economy mirrors the standard model of an exchange economy. The exogenous distribution of power in the jungle is the counterpart of the distribution of initial endowments in the market. In a jungle equilibrium, a stronger agent no longer wants to take goods from any weaker agent nor from a pile of common goods, that no other agent holds. P&R specify certain conditions on consumption sets and preferences under which a unique and Pareto efficient jungle equilibrium exists.Footnote 1 This jungle equilibrium coincides with the unique lexicographic maximum in which all of the economy’s resources are initially common goods and stronger agents take from the pile of common goods before weaker agents can take. It is tempting to conclude from P&R’s intriguing analysis that exactly the particular strength relation assumed in their paper constitutes the main driving force behind the final distribution of resources in the jungle. However, this conclusion is somewhat premature. The goal of our paper is to provide a more nuanced view on the interaction of strength, preferences and holdings behind the jungle equilibrium concept. Intentionally, we do not deviate from P&R’s strength relation throughout the paper. In our analysis we assume that initial holdings are distributed over the agents rather than being available as common goods, as is the case in P&R. More generally, a stronger agent may take from any weaker agent or from the (remaining) pile of common goods. Under P&R’s assumptions on consumption sets and preferences, initial holdings are irrelevant for lexicographic maximization. The intuition is that each single taking improves the taker’s welfare and thus only an allocation where the stronger agents are either satisfied or have acquired all goods can be an equilibrium. The initial distribution of resources among agents in jungle economies is relevant only to determine from whom a stronger agent takes. However, once we relax the assumptions of strong monotonicity and strict convexity of preferences assumed by P&R, the distribution of initial holdings matters. A single taking may not improve the taker’s welfare such that an inefficient allocation can ‘survive’ as an equilibrium. Imagine an example of a jungle economy with a strong agent and two weaker agents, where the former holds Leontief preferences over pairs of shoes and currently holds no shoes. Suppose one weaker agent holds a left shoe and the other holds a right shoe. Since getting only a left shoe or only a right shoe does not increase the strong agent’s utility, the strong agent will not take if restricted to a single taking. Therefore, the jungle is in equilibrium.Footnote 2 In such a case the jungle equilibrium does not satisfy lexicographic maximization, nor is it Pareto efficient. As this example illustrates, the jungle equilibrium concept fails to recognize that the stronger agent can gain by coercing both weaker agents even if each single taking does not improve the strong agent’s welfare. We show that if we include multiple takings into the equilibrium concept, jungle equilibria coincide with lexicographic maximization under rather weak assumptions.Footnote 3
 Furthermore, with the use of an example, we derive a continuum of equilibria in which the strongest agent holds goods in excess of her satiation point. As the strongest agent has no incentive to dispose of excess goods, she may withhold them from weaker agents, who cannot take them. This withholding of goods is Pareto inefficient and only voluntary gift giving by stronger agents can remove this inefficiency. With another example, we show that even voluntary gift giving may sometimes be insufficient to achieve Pareto efficiency. Then, voluntary trade is needed. These examples demonstrate why we believe that our analysis adds to a better understanding of the crucial assumptions underlying jungle economies. Pareto efficiency in the jungle is not a result of coercion alone. Interestingly, depending on the kind of preferences present in the jungle, voluntary gift giving and voluntary trade, behavior that is in sharp contrast to coercion, is needed to keep the jungle efficient. Thus, our conclusions diverge strikingly from P&R, who see no role for gift giving and trade in the jungle. We proceed as follows. Section 2 presents a formal account of a jungle economy with initial holdings of which P&R’s jungle economy is a special case. Section 3 investigates lexicographic maximization and provides two examples to motivate our analysis. The jungle equilibrium with multiple unilateral takings is investigated in Sect. 4. The subtle role of withholding, voluntary gift giving and voluntary trade is discussed in Sect. 5. Section 6 concludes.",3
21.0,3.0,Review of Economic Design,06 July 2017,https://link.springer.com/article/10.1007/s10058-017-0203-y,Simple versus rich language in disclosure games,September 2017,Jeanne Hagenbach,Frédéric Koessler,,Female,Male,Unknown,Mix,,
21.0,3.0,Review of Economic Design,01 August 2017,https://link.springer.com/article/10.1007/s10058-017-0204-x,Two-agent collusion-proof implementation with correlation and arbitrage,September 2017,Dawen Meng,Guoqiang Tian,Zhe Yang,Unknown,Unknown,,Mix,,
21.0,4.0,Review of Economic Design,07 July 2017,https://link.springer.com/article/10.1007/s10058-017-0202-z,Revenue comparison of discrete private-value auctions via weak dominance,December 2017,Makoto Shimoji,,,,Unknown,Unknown,Mix,,
21.0,4.0,Review of Economic Design,06 September 2017,https://link.springer.com/article/10.1007/s10058-017-0205-9,Two-stage contests with effort-dependent values of winning,December 2017,Aner Sela,,,Male,Unknown,Unknown,Male,"Multi-stage contests are situations in which agents spend resources in order to win one or more prizes. The prizes are allocated either in each of the stages or only in some of them, usually in the last one. Multi-stage contests have many different architectures. Examples include best-of-k contests (see Klumpp and Polborn 2006; Harris and Vickers 1987; Konrad and Kovenock 2009), elimination contests (see Gradstein and Konrad 1999; Groh et al. 2012; Fu and Lu 2012) and contests which are repeated a finite number of times. The analysis of multi-stage contests is quite complicated and challenging particularly when synergy exists between the various stages of the contest. Such a synergy could occur when contestants have a fixed budget of resources and have to decide how to allocate them over the stages of the contest. Amegashie et al. (2007), for example, showed that in a two-stage elimination all-pay contest, if contestants have fixed equal resources, they spend more resources in the initial rounds than in the subsequent ones. Likewise, Sela and Erez (2013) studied a dynamic contest between two contestants who compete against each other in n different stages and have heterogeneous resource budgets that decrease from a given stage to the next proportionally to the resources allocated in that stage. They showed that when the winning value is equal between the stages, the contestants’ resource allocations are weakly decreasing over the stages. Another well known dynamic contest with resource allocation is the Colonel Blotto game in which two contestants compete against each other in n different contests. Each contestant distributes a fixed amount of resource over the n contests without knowing his opponent’s resource distribution. In each contest, the contestant who allocates the higher level of resource wins where each contestant’s payoff is a function of the sum of wins across the individual contests (see, for example, Snyder 1989; Roberson 2006; Kvasov 2007; Hart 2008). The literature suggests various reasons for the occurrence of synergy in multi-stage contests other than a fixed resource budget. Ryvkin (2011), for example, studied a best-of k contest in which the contestants’ probabilities of winning in each stage depend on the contestants’ efforts in that stage as well as their efforts in the previous stages and found that agents are more likely to exert higher efforts in the later stages of the contest. Kovenock and Roberson (2009) studied a two-stage campaign resource allocation game in which the players’ difference campaign expenditures in the first stage serve as a head start advantage to the contestants in the second stage.Footnote 1 In this paper, we study a two-stage all-pay contest in which the contestants’ efforts in the first stage do not affect the contestants’ success functions in the second stage (as in Ryvkin 2011; Kovenock and Roberson 2009), but instead affect their values of winning in a later stage where a contestant’s value of winning is a function of his own type and the reward. There are numerous examples of values of winning in contests which are not necessarily fixed and where there is a relationship between the efforts made and the size of the values of winning. To illustrate, the greater the effort a student exerts for an exam at a university, the greater is his chance to achieve a higher grade. Similarly, the greater the effort a firm exerts to produce a new product, the greater is the probability that the quality of the final product will improve. Another example is patent races, where the more effort a firm invests, the earlier is the innovation time and therefore the larger is the value of winning. In one-stage contests, the effort-dependent values of winning have been shown to have a complex effect on contestants’ behavior. For example, in one-stage all-pay contests with effort-dependent values of winning under incomplete and complete information, Kaplan et al. (2002, 2003) showed that substantial qualitative changes can occur in the behavior of the contestants compared to their behavior in the same contests with constant values of winning. Cohen et al. (2008) studied all-pay contests with effort-dependent values of winning under incomplete information in which the value of winning the contest for each contestant depends on the effort-dependent reward. They showed that when the designer maximizes the contestants’ expected total effort and there is a sufficiently large number of contestants, the optimal value of winning decreases in the contestants’ effort. However, when the designer maximizes the contestants’ expected highest effort, the optimal value of winning may increase in the contestants’ effort for any number of contestants. A last example is Kaplan and Wettstein (2015) who studied the optimal effort-dependent value of winning in all-pay contests under complete information. Their results indicate that for asymmetric environment with two firms, it is optimal to set different values for each firm. In our two-stage contest, the effect of the effort-dependent value of winning on the contestants’ behavior is even more complicated than in one-stage contests since this value implies that there is synergy between the two stages. This synergy might be either positive or negative, namely, the contestants’ efforts in the first stage may either increase or decrease the contestants’ values of winning in the second stage. The reason for a positive effect is that the effort in the first stage may increase the value of winning in the second stage. This can occur, for example, in a two-stage R&D contest when a contestant acquires some knowledge and experience in the first stage which increases his value of winning in the second stage. The reason for a negative effect is that the effort in the first stage may decrease the value of winning in the second stage. This can happen when the effort exerted by a contestant in the first stage is overly high which causes fatigue which can affect the contestant’s ability, particularly his value of winning in the second stage.Footnote 2
 In our two-stage all-pay contest, in each stage, each contestant exerts an effort. The contestant who exerts the highest effort wins the contest, but, independently of success, all the contestants bear the cost of their efforts.Footnote 3 The contestants have different values of winning over the stages. The value of winning in the first stage is fixed but in the second stage it is variable with a constant marginal increasing (decreasing) rate; namely, a contestant’s value of winning in the second stage is a linear function of his effort in the first stage. We assume that contestants might be symmetric or asymmetric in the competition in the first stage but they are symmetric in the competition in the second stage. Accordingly, the contestants have either the same or different values of winning in the first stage but they have the same value function in the second one. With symmetric contestants who have the same values of winning, we show that if there is positive synergy between the stages (each contestant’s effort in the first stage increases his value of winning in the second stage) both contestants have an expected payoff of zero, but, if there is negative synergy between the stages (each contestant’s effort in the first stage decreases his value of winning in the second stage), both contestants have positive expected payoffs. This result is in contrast to the standard one-stage all-pay contest (auction) in which symmetric contestants always have an expected payoff of zero. On the other hand, with asymmetric contestants who have different fixed values of winning in the first stage, we show that if there is positive synergy between the stages, the stronger contestant (the contestant with the higher value of winning in the first stage) has a positive expected payoff while the weaker contestant has an expected payoff of zero. However, if the synergy between the stages is negative, both contestants have positive expected payoffs. Furthermore, regardless of whether the contestants are symmetric or asymmetric and whether the effort-dependent value of winning in the second stage is increasing or decreasing in the contestants’ efforts, we find that the contestants’ expected payoffs are non-decreasing in the value of the marginal increasing/decreasing rate of the effort-dependent value of winning. That is, when the synergy is either negative or positive, the higher the effect of the contestant’s effort in the first stage is on his value of winning in the second stage, the higher is his expected payoff. In particular, when the synergy is negative, lower values of winning in the second stage lead to higher expected payoffs. The reason is that when all the contestants have lower values of winning in the second stage, the contestants’ expected payoffs which are based on the difference of their values of winning in that stage, increase in the value of the marginal increasing/decreasing rate. We also compare the contestants’ expected payoffs under positive and negative synergies when the marginal increasing rate and the marginal decreasing rate of the effort-dependent values of winning have the same value. We find that if the contestants are symmetric, they have either the same expected payoff or a higher expected payoff under negative synergy than under positive synergy. However, if the contestants are asymmetric, the weaker contestant (the contestant with the lower value of winning in the first stage) has either the same expected payoff or a higher expected payoff under negative synergy than under positive synergy. On the other hand, depending on the value of the marginal increasing/decreasing rate, the stronger contestant (the contestant with the higher value in the first stage) has either a lower or a higher expected payoff under negative synergy than under positive synergy. The reason is that in the contest under positive synergy the contestants have only values of winning while in the contest under negative synergy they also have values for losing (the loser in the first stage has necessarily a positive expected payoff in the second stage) and therefore the weak contestant may have a higher expected payoff in the contest under negative synergy. Moreover, if the marginal increasing rate is relatively low such that the value of winning in the contest under positive synergy is not much larger than the value of losing in the contest under negative synergy even the strong contestant may prefer the combination of smaller values of winning and losing over a higher value of winning only. In other words, if the marginal increasing/decreasing rate is relatively low, also the strong contestant has a higher expected payoff under negative synergy than positive synergy. Last, in the contest under positive synergy, the contestants’ values of winning in the first stage is higher than in the contest under negative synergy and this implies that if contestants are symmetric then their expected effort in the first stage is higher in the contest under positive synergy than under negative synergy.Footnote 4
 In a related paper (Sela 2012), we studied sequential two-prize all-pay contests under complete information where the prizes are identical; each contestant may win more than one prize; and each contestant’s marginal values for the first and the second prize are either decreasing, constant or increasing. Under these conditions, the contestants’ strategies in the first stage affect what their prizes will be in the second stage. In contrast, the synergy between the stages in our present model does not depend on the identity of the winner in the first stage but instead on each contestant’s effort in that stage. The rest of the paper is organized as follows: In Sects. 2 and 3, we analyze the two-stage all-pay contest with positive and negative synergies. In Sect. 4, we compare the results of these two sections, and in Sect. 5 we conclude.",6
21.0,4.0,Review of Economic Design,16 September 2017,https://link.springer.com/article/10.1007/s10058-017-0206-8,Transfer of authority within hierarchies,December 2017,Pinghan Liang,,,Unknown,Unknown,Unknown,Unknown,,
21.0,4.0,Review of Economic Design,04 November 2017,https://link.springer.com/article/10.1007/s10058-017-0207-7,Learning by fund-raising,December 2017,Alvaro J. Name-Correa,,,Male,Unknown,Unknown,Male,"Charitable fund-raising is a highly professional activity. The Association for Professional Fund-raisers represents 30,000 members. Every year more than 115,000 nonprofit organizations consult these professionals for a total of 2 billion dollars (Kelly 1998). As with many services and manufacturing sectors such as military aircraft, software, pizza industry,Footnote 1 it is strongly believed that fund-raising is learned on the job. Hence, demand for those more experienced professionals rises. For instance, a recent survey by Cygnus Applied Research reveals that most successful fund-raisers remain on a job for three to six months before being recruited for another.Footnote 2 As the president of Cygnus puts it: “ Only one out of three fund-raisers experiences even a day without a job” . Professional fund-raisers also place a great value on experience as suggested by a fund-raiser’s quote: “ Fund Development Associates is the regional expert in fund-raising. No one has more direct, hands-on experience. By selecting our firm, you will have a team of professionals with more than one hundred years of combined successful fund-raising experience who have assisted hundreds of charitable organizations achieve their goals”.Footnote 3 Despite its importance, learning through solicitation experience has not been incorporated into the theory of strategic fund-raising. In this paper, we contend that direct solicitations are the source of learning for the fund-raiser. She learns from experience to become a more productive solicitor. For instance, the duration of a phone call to inform a potential donor may shorten with each additional solicitation. Our model builds on Name-Correa Alvaro and Yildirim (2013), in which an “ active” fund-raiser is added to the otherwise “ standard” model of giving in which donors consume two goods: a private and a public good, and contributions to the public good are simultaneous.Footnote 4 The fund-raiser individually informs potential donors about its charitable cause. Each solicitation is, however, costly. Unlike Name-Correa and Yildirim, we explicitly introduce a fund-raising technology with two components: a constant marginal cost, measuring minimum expenses per solicitation, and a variable cost, decreasing in the number of solicitations. The latter captures learning by fund-raising. Our first observation is that the charity ranks individuals in a descending order according to their incomes. Based on this ranking, it sequentially decides on solicitations. Absent learning, with each additional solicitation, the charity becomes more cautious about contacting more individuals due to the free-rider problem. Charity stops soliciting individuals once she detects a net free-rider, i.e. an individual providing a gift below the marginal cost. Under learning economies, however, the charity faces a well defined trade-off on each additional solicitation: on one hand, the free-rider problem intensifies, but on the other hand, the charity reduces marginal cost due to learning. Proposition 1 provides a full characterization of optimal fund-raising that resolves this trade-off. Our equilibrium characterization is general enough to understand the effect of overhead costs on optimal fund-raising. These costs may represent a fixed consulting fee, or an investment in information technology that enables the charity to conduct fund-raising more productively—e.g., Paskalev and Yildirim (2017). With this interpretation in mind, we address the problem of technology choice by the charity. We find that a large investment in fund-raising technologies is justified only if the donor base is relatively homogeneous in income. Our paper relates to the studies on strategic fund-raising to overcome zero-contribution equilibrium under non-convex production, either by securing seed money (Andreoni 1998), or by collecting small donations through time (Marx and Matthews 2000). None of these papers, however, consider endogenous costly solicitations. Other papers address learning about the public good quality in a dynamic framework. In these works learning is faster when the cumulative production of the good is larger (Bolton and Harris 1999; Yildirim 2003). On strategic fund-raising under costly solicitations, our paper relates to Rose-Ackerman 1982 and Name-Correa Alvaro and Yildirim (2013). Rose-Ackerman (1982) is the first to build a model of costly fund-raising in which donors, as in our work, are unaware of a charity until they receive a solicitation letter. She, however, does not construct donors’ responses from an equilibrium play. The closest paper to ours is Name-Correa Alvaro and Yildirim (2013). They fully incorporate fund-raising costs to determine the fund-raiser’s solicitation strategy. The charity commits to that strategy and successfully launches a fund-drive. Our work is similar to theirs; instead of attaching a cost to each donor, though, we explicitly introduce a fund-raising technology unrelated to donors’ identities. This allows us to model learning by fund-raising and to conduct comparative statics. The plan of this paper is as follows. In Sect. 2 the model is presented. In Sect. 3, we determine the optimal fund-raising strategy and show that this is robust to incomplete information. In Sect. 4.1 we parametrize learning and conduct comparative statics, in Sect. 4.2 we address technology choice by the charity. In Sect. 5 we introduce decreasing returns to scale in fund-raising. We conclude in Sect. 6.",
22.0,1.0,Review of Economic Design,18 January 2018,https://link.springer.com/article/10.1007/s10058-018-0208-1,"Decreasing average cost in private schools, existence of majority voting equilibrium, and a policy analysis for Turkey",June 2018,Muharrem Yeşilırmak,,,Male,Unknown,Unknown,Male,"All over the world, education is provided not only by governments but also by private firms. According to James (1993), the average private school enrollment across developed countries and developing countries was around \(19\%\) and \(25\%\) respectively in 1980. Moreover, a recent study by OECD (2012) reports that average private school enrollment was \(18\%\) in 2009 among the OECD countries. Standard models of mixed public–private school systems, such as Stiglitz (1974), take into consideration the decreasing average cost over enrollment in only public schools, whereas empirical studies such as Bee and Dolton (1985) and Watt (1980) find significant evidence of it for private schools as well.Footnote 1 Motivated by these, we provide a new equilibrium model of a mixed public–private school system where both public and private school face decreasing average cost over enrollment. Next, we calibrate the model to match certain statistics from the 2013 Turkish data. Using our model, we then quantitatively compare the benchmark for a mixed public–private school regime with a pure public school regime at which no private school exists. Using this comparison, we would like to understand the impact of shutting down some of the private schools in Turkey following the July 15 coup attempt. More formally, our model economy consists of a continuum of heterogeneous households with measure one, a public school and a private school. Households are heterogeneous with respect to exogenously specified income and ability of their child. They derive utility from numeraire consumption and the achievement level of their children. Achievement of a child depends on its ability and the education spending received. Total cost of the private school equals only the fixed cost, which is divided equally among the parents as in Nechyba (1999). This implies decreasing average cost over enrollment. Each parent’s share of the fixed cost equals tuition, which also equals private school spending per pupil. Moreover, public school spending per pupil is determined by equally dividing the total income tax revenue among public school students. Income tax rate is determined through majority voting in an election at which all households participate. Households are non-myopic voters in the sense that they take into consideration the effect of their vote on aggregate variables such as public school enrollment. Each household chooses either the private school or the public school. Consumption of a household choosing the public school equals simply after-tax income, whereas consumption of a household choosing the private school equals after-tax income minus tuition. Differently from previous models that ignored decreasing average cost in private schools, an increase in tax rate (or total public school spending) causes private school spending per pupil (or tuition) to increase in our model. The intuition is as follows: as tax rate goes up, public school spending per pupil also goes up, causing public school enrollment to increase and private school enrollment to decrease. This leads to an increase in the average cost per pupil in the private school because of the decreasing average cost. As a result, private school charges higher tuition and spending per pupil goes up. There is empirical evidence from U.S. data for this qualitative implication of our model.Footnote 2 As shown in Table 1, total public school spending in the U.S. increased by \(53\%\) between 1990 and 2012. This increase is accompanied by increases in public school spending per pupil, public school enrollment, and average tuition at the amounts of 27%, 2.44%, and 143% respectively. Moreover, the \(143\%\) rise in average tuition may be simply caused by a rise in median income or mean income. To assess this channel, we report both median income and mean income. As seen from Table 1, median income rose by \(0.04\%\) which is unlikely to be the reason behind the rise in average tuition. On the other hand, mean income rose by \(11\%\) which could explain some part of the rise in average tuition. We believe that \(11\%\) rise in mean income cannot solely explain the \(143\%\) rise in average tuition. In other words, if mean income is somehow held constant consistently with our model’s assumption, average tuition would still rise by a positive amount. We first study the sorting of households across schools and then numerically show the existence of majority voting equilibrium for calibrated values of parameters, a Cobb–Douglas utility and a joint lognormal distribution of income and ability. We calibrate the parameters of the model so as to minimize the distance between model implied statistics and their counterparts in 2013 Turkish data. More specifically, we are targeting enrollment rate in private schools, ratio of public education spending to GDP, normalized mean and standard deviation of math scores in university entrance exam (known as LYS exam). After the July 15 (2016) coup attempt in Turkey, the government decided to convert some of the private schools into public schools (Hürriyet 2016). The converted private schools were owned by the FETÖ group that attempted the coup. Using our calibrated model, we would like to analyze the effects of this policy on the student achievement distribution through a counterfactual experiment. In the experiment, we take the most extreme case and eliminate the option of the private school. As a result, private school students will transfer to the public school. Their achievement level will be adversely affected, since spending per pupil is lower in the public school. To minimize this effect, the parents of these students are expected to vote for higher income tax rates. As a result, income tax rate will increase and so will public school enrollment. In this case, the net effect on public spending per pupil is ambiguous. Our results imply that income tax rate increases from 2.72 to \(3\%\). Although public spending per pupil increases by \(6.1\%\), it is still below the benchmark private school tuition. Therefore, initial public school students would be positively affected whereas initial private school students will be negatively affected by this policy in terms of achievement. We find that mean achievement increases by \(0.039\%\). Moreover, the policy is expected to decrease the variance of achievement, since after the implementation of the policy all students will receive the same spending level. We find that variance of achievement declines by \(0.013\%\) with the policy change.Footnote 3
 One of the first papers that theoretically analyzed a mixed public–private school system is Stiglitz (1974). As mentioned above, the main difference between his framework and ours lies in capturing the decreasing average cost faced by private schools. There is a continuum of private schools in his model operating under constant average cost, whereas in our model average cost is decreasing with enrollment. Moreover, majority voting equilibrium may not exist in Stiglitz (1974), owing to the absence of single-peaked preferences over income tax rate. However, solutions for the nonexistence problem are provided by Glomm and Ravikumar (1998), and Epple and Romano (1996). Stiglitz’s model is at odds with the data presented in Table 1, since an increase in income tax rate (or total public school spending) causes average tuition to decline. The reasoning is as follows: as income tax rate goes up, public school enrollment goes up, and after-tax income of all households goes down. Because of this income effect, those households that still choose private schools spend less on their children’s education. As a result, average tuition decreases. In very rich theoretical models, Epple and Romano (1998), and Nechyba (1999) take into account the decreasing average cost faced by private schools. Differently from our paper, they capture peer-group effects in education production in their model and then study the effect of tuition vouchers. Unlike Epple and Romano (1998), in this study income tax rate is determined through a political process, namely majority voting. Differently from Nechyba (1999),Footnote 4 in our model, household voters are non-myopic in the sense that they consider the effect of their vote on aggregate variables such as public–private school enrollment. This paper is organized as follows. Section 2 explains the model. Section 3 studies the existence of equilibrium and also calibrates the model’s parameters. Section 4 reports the results of the counterfactual experiment and Sect. 5 concludes the paper.",2
22.0,1.0,Review of Economic Design,24 March 2018,https://link.springer.com/article/10.1007/s10058-018-0209-0,Stable cost sharing in production allocation games,June 2018,Eric Bahel,Christian Trudeau,,Male,Male,Unknown,Male,"We examine the cooperative games (with transferable costs) that arise in the context where multiple agents have distinct technologies allowing to produce homogeneous goods (e.g., autonomous regions in the same country produce electricity using fossil fuels, hydropower, nuclear power, etc.; with each region having its own technology and demand for electricity. Thus, efficient regions will produce more than their own demands in order to sell electricity to less efficient ones). We use the phrase production allocation game (PAG) to refer to any such problem. Other interesting applications (besides the production of utilities) of our model include the cases of: a multinational firm trying to allocate its production between its plants over the world; family members/colleagues/neighbors dividing tasks among themselves. Two interesting questions arise in any PAG. Firstly, one needs to determine the cost-minimizing allocation of the production. Secondly, the participants need to share that minimum cost, with notions of fairness and stability imposing restrictions on how to operate. We build on known optimality results to answer the first question, adapting algorithms to our setting and using them to answer the second question. The types of returns to scale exhibited by the technologies greatly influence how these two underlying issues may be resolved. On the production side, if technologies exhibit decreasing returns to scale (DRS), the optimal plan typically spreads production by having many producers contribute small quantities. In contrast, in the case of increasing returns to scale (IRS), it is always advantageous to centralize production. For the cost sharing issue, the procedure allowing to find sensible rules also varies depending on the returns to scale, although in both cases we are concerned with compensating agents who produce for others with less efficient technologies. We are primarily interested in core selection: any subgroup of agents should jointly pay less than what it would cost them to produce their demands themselves. The model shares similarities with the literature on cost sharing with technological cooperation, where agents put not only their demands but also their technologies in common when cooperating. In Trudeau (2009a), all production technologies are linear, and the marginal cost can decrease when more agents are cooperating. The assumption of linearity is relaxed in Bahel and Trudeau (2013). In both cases, the focus is on how we can subsidize agents that are able to increase the efficiency of the production technology while maintaining fairness, stability and consistency properties. In this non-linear setting, the non-emptiness of the core is not guaranteed. The model considered here is much more specific; and it belongs to the large family of cost sharing problems with a network structure (in these problems, one first needs to find the optimal network configuration). The network structure in the present work is simple: agents obtain goods directly or indirectly from a source, and the cost to transport goods from the source to an agent is an increasing function of the flow, while it is costless to transport goods between agents. It is thus a special case of network flow problems, where it may be costly to ship goods between agents. If all of these functions are convex, the core is always non-empty (Quant et al. 2006), a result that carries over to our PAGs under DRS. If all functions are concave (IRS), the core of a network flow problem may be empty (Trudeau 2009b). Interestingly, given the specific structure of our PAGs, we are able to prove that they always exhibit a non-empty core under IRS. A well-known member of the family of network flow problems is the shortest path problem, where we have either a single demander or linear cost functions. This is enough to guarantee a non-empty core. Rosenthal (2013) and Bahel and Trudeau (2014) study the issues of fairness and stability in shortest path problems. A related problem is the minimal cost spanning tree problem, where the cost to use a link is invariant with the flow, as long as the flow is strictly positive. Once again, this is a case where the core is always non-empty (Bird 1976). The focus of the recent literature has been on proposing core allocations that compensate agents who allow for cheaper network configurations (Bergantinos and Vidal-Puga 2007; Trudeau 2012). Cost sharing issues for other types of network problems are discussed in the reviews of Sharkey (1995) and Moulin (2013). In our study, we first look at stable allocations with homogeneous prices: all units bought/sold are traded at the same price, regardless of the characteristics of the buyer/seller. This property, which is independent from core selection, implies in particular the two properties symmetry (similar agents pay the same cost share) and no merging and splitting (agents do not benefit from merging or splitting their demands—see Sprumont 2005). Using these requirements, we define a set of stable allocations that are natural and intuitive: for a given price p, agents who consume units that they have not produced have to pay the amount p for every such unit. On the other hand, agents producing units that they do not consume are awarded the amount p for every such unit. Finally, agents who produce exactly their demand simply have to pay their stand-alone cost. We define lower and upper bounds for this price p by using the stability requirement. These definitions vary depending on the type of returns to scale. Core allocations with homogeneous prices often obtain through the market, and this allows us to connect our results to the vast literature linking the core of an economy to its general equilibrium allocations (see for instance Anderson 1992 for a review). In particular, the indivisibility (of the units) in our model induces similarities with assignment and matching problems with transferable utility, i.e., problems where one or more goods are traded in indivisible units for a divisible good (money)—see the review of Núñez and Rafels (2017). In the assignment problem (one-to-one matching) of Shapley and Shubik (1971), the set of core allocations corresponds to the set of competitive equilibrium payoff vectors. Quinzii (1984) generalizes the model by relaxing the asymmetry of preferences between buyers and sellers, notably allowing trades among the initial owners of the indivisible goods. While every core allocation cannot in general be obtained as a competitive equilibrium payoff vector, the author gives a sufficient condition for the correspondence to hold. This condition requires (i) complementary between money and the indivisible goods and (ii) monotonicity of the utility over money. Our results are most closely related to Kaneko (1976), who studies a many-to-one assignment market with transferable utility in which agents in the first group are endowed with goods, while agents in the second group are endowed with money. Members of the first group may sell units they do not consume to agents of the second group, with each buyer interested in at most one good. The author shows that, whenever we find a price such that the total quantity supplied equals the total quantity demanded, this leads to a core allocation with homogeneous prices. The assumptions made by Kaneko (1976) on the agents’ utility functions generate a model akin to our DRS case, for which Kaneko’s results for unitary demands provide the full core. Other studies related to our DRS case include Camiña (2006), Sotomayor (2007), Jaume et al. (2016) and Yokote (2016). Sotomayor (2002) considers a different many-to-one market in which sellers are workers who can each sell labor to at most one of the many firms (buyers). In that case, the set of core allocations coincides with the set of competitive equilibrium payoff vectors. This equivalence between the two sets is not preserved when we consider many-to-many markets, with (Sotomayor 1992, 2007) or without (Thompson 1981; Crawford and Knoer 1981; Sanchez-Soriano et al. 2001; Sotomayor 2002) the constraint that any given buyer–seller pair can only trade one unit. In both cases, some core allocations cannot be obtained through a competitive equilibrium. Under either IRS or the general framework (with arbitrary cost functions), our model has no counterpart in the literature on general equilibria for discrete economies—in part because the existence of general equilibria is not guaranteed. Interestingly, in the IRS case we nonetheless provide a set of stable allocations with homogeneous prices. Under the general framework, we show using examples that there may exist no stable allocation; and even when the core is nonempty, stable and homogeneous prices may not be feasible. Therefore, our analysis under this general framework focuses on sufficient conditions for the existence of stable allocations. We provide such a sufficient condition in Theorem 9. Moreover, we show in Theorem 10 that this condition is both necessary and sufficient for the existence of stable allocations with homogeneous prices in problems with unitary demands. These novel results under IRS and the general framework constitute the main contributions of the present paper. We also add to the existing literature in the case of DRS (as explained below). In some cases, the designer may want to set heterogeneous prices. First, it may be desirable to favor some buyers or producers by differentiating per unit prices. For example, under DRS, we can have agents bear more responsibility if their demands are higher. Thus, the present paper contributes to the literature on fair allocation of production externalities (see Moulin and Sprumont 2007 for a review) by examining this issue under privately-owned technologies. Another justification for the use of heterogeneous prices applies to the context of polluting goods, as the designer can choose to discourage over-consumption by charging more per unit to agents with high demands. In the DRS case with unitary demands, Kaneko (1976) provides stable allocations with heterogeneous prices for the case of a monopoly. Building on this result, we also describe the full core for the monopsony case (regardless of the level of demands), which includes many allocations with heterogeneous prices. In the IRS case with monopsony, the fact that we have a single producer (and a single buyer) allows to describe the entire core through this approach. In some particular cases (e.g., when all technologies are linear) the full core obtains regardless of the agents’ demands. The paper is structured as follows. Section 2 introduces the framework of Production Allocation Games. PAGs with decreasing returns to scale are studied in Sect. 3. In Sect. 4, we examine PAGs with increasing returns to scale. Section 5 discusses the general case, where the cost functions are completely arbitrary. Our concluding remarks are provided in Sect. 6. In “Appendix”, we formally show how to transform a PAG into a market with indivisible goods, following Kaneko (1976).",4
22.0,1.0,Review of Economic Design,05 June 2018,https://link.springer.com/article/10.1007/s10058-018-0211-6,Every member of the core is as respectful as any other,June 2018,Yasemin Dede,Semih Koray,,Female,Male,Unknown,Mix,,
22.0,1.0,Review of Economic Design,08 June 2018,https://link.springer.com/article/10.1007/s10058-018-0210-7,On the terminology of economic design: a critical assessment and some proposals,June 2018,William Thomson,,,Male,Unknown,Unknown,Male,"Why should we care about good language? Because “Mal nommer un objet, c’est ajouter au malheur de ce monde” (Camus 1944). [“Misnaming an object adds to misery in this world”] The purpose of these notes is to provide a critical examination of some of the terminology that is common in the literature on economic design and to suggest ways in which it could be improved. Mainly it is to encourage a conversation about language. Research frequently generates new concepts but terminology develops in a haphazard way; it is rarely the result of deliberate and carefully thought-out choices authors make. Also, a term from common language that indicates with no ambiguity a concept that has to be named is often difficult to find. Whether a new concept will be important in the development of a subject is not immediately clear. For a while, it will only be discussed by a small community of specialists. Unfortunately, during this period, usage of the name that designates it solidifies; as time passes, replacing it becomes increasingly difficult, even if better ones are found. The common argument against changing a term that has been used for a while is that it is too late: people know the concept under that name and you would be confusing them. For a concept that may vanish in the near future, the change may indeed not be worth the trouble. But when a subject is alive and well, adapting and improving our language can only help it develop. If we switch to a better name for an important concept, people will not be confused for long. Besides, what about people who are new to the field, or the new generations of students to whom, year after year, we teach the subject? Challenging established terminology is worth it, and worth it at any point in time. Examples of terminological messes abound in the literature that I discuss here. A striking one concerns the various families of rules that have been identified in the study of the assignment of indivisible resources, called “objects”. These rules are referred to as “dictatorships”, “priorities”, “queueing”, each of these terms being qualified by adjectives such as “sequential”, “serial”, “hierarchical”, and “lexicographic”. How can one possibly tell from their names how a “sequential priority rule” differs from a “lexicographic dictatorial rule”, say? Specialized terminology is of course familiar to workers in the field, but when we address a general audience (even when we give a lecture or a seminar in an economics department), as opposed to a conference audience, it is not likely to be known. Choosing good terms will make it easier to someone with no prior knowledge of the subject to develop an understanding of it. Another reason why we should feel free to improve terminology is that authors do not always agree on it anyway. This is true even for concepts as basic as efficiency à la Pareto. In order to distinguish between the two primary notions (when the test on an outcome is whether there is another one that everyone prefers, or simply one that everyone finds at least as desirable and at least one person prefers), most authors have picked two of the following three expressions, “strong Pareto-efficiency”, “Pareto-efficiency”, and “weak Pareto-efficiency”, and each pair has been used by someone (sometimes, expressions that do not include Pareto’s name are used). Until we read the definitions in each specific paper, we do not know which is meant. Also, terminology eventually changes. Besides, an old term may not be applicable in some new situation, and some other term has to be invented anyway. Unifying language is particularly important when surveying a literature because one of the goals then is to show how results are linked. In each specific study, there will only be few expressions designating related concepts but the terms used in different papers will often differ and it would be confusing to jump from one term to another when discussing them. Choices will have to be made. If a family of related concepts have to be introduced, it is good to have a mold, a template, to create descriptive names for them. Several such templates are proposed below. The expressions that result are sometimes long or unwieldy, but shorter ones, mnemonic labels, and even transparent abbreviations can often be found. Another guiding principle in choosing terminology is that jargon is better avoided if possible. If our work is to have an impact in the real world, we should be able to communicate our recommendations to the practitioners and non-economists in a language they can easily understand. Thus, here are some suggestions, or rather, invitations to the reader to think about certain terminological issues.",3
23.0,1.0,Review of Economic Design,24 April 2019,https://link.springer.com/article/10.1007/s10058-019-00222-2,Letter from the editors,June 2019,Onur Kesten,Moritz Meyer-ter-Vehn,Huseyin Yildirim,Male,Male,Unknown,Male,,
23.0,1.0,Review of Economic Design,02 January 2019,https://link.springer.com/article/10.1007/s10058-018-0217-0,Exchange-stability in roommate problems,June 2019,Azar Abizada,,,Female,Unknown,Unknown,Female,"We consider roommate problem, where a group of people needs to be paired in order to be assigned to certain rooms. Real life applications include assigning student pairs to dormitory rooms, assigning employees to shared offices, etc. To fix ideas, we concentrate on dormitory problem. Space constraint is a usual problem in such environments: most of the universities do not have enough dormitory rooms in order to offer an on-campus housing to all of its students, even when all accommodated students share a room. Therefore, we do not let any student live by himself.Footnote 1 We assume that all rooms are homogeneous. Therefore, each student has strict preferences only over his roommate. A matching is a partition of the students into pairs. Central notion in matching is stability. For the problem with space constraints, Alcalde (1995) defines a more suitable stability notion, namely, exchange-stability. A matching is exchange-stable, if there is no group of students that can exchange their rooms, so that each student gets the room of some other student from this group,Footnote 2 and all prefer their new roommates to the initial one. Exchange-stable matching may not always exist. We define restrictions on preferences, each of which guarantee existence of exchange-stable matching. First condition requires that given a problem, there is at least one pair of students that top rank each other, and once they are removed there is another pair that top ranks each other and so on. Our second condition requires that at least half of the students have identical preferences and they rank the others as their top ranked students. As we show in the paper, the second part of this restriction is crucial to obtain the result. Our main result shows that exchange-stable matching is guaranteed to exists on more general preference domain obtained from mixture of these two restrictions (which at extreme cases provide one of these restrictions). Note that this domain includes all the domains considered in the literature for guaranteeing existence of exchange-stable matching. The rest of the paper is organized as follows. In Sect. 2 we discuss related literature. In Sect. 3 we define the model and exchange-stability. In Sect. 4 we define different domains of preferences, state and prove our main result.",1
23.0,1.0,Review of Economic Design,22 January 2019,https://link.springer.com/article/10.1007/s10058-019-00218-y,Equilibrium refinements for the network formation game,June 2019,Rahmi İlkılıç,Hüseyin İkizler,,Male,Male,Unknown,Male,"To understand which networks can emerge when players strategically decide with whom to establish links, a model of network formation needs to specify the process through which players set up links, together with a notion for network equilibrium compatible with this process. We will analyze a normal form game of network formation due to Myerson (1991). All players simultaneously announce the links they wish to form, and a link is formed if and only if there is mutual consent for its formation. The mutual consent requirement of the Myerson game creates coordination problems. Nash equilibrium does not lead to sharp predictions. The empty network can always be supported by a Nash equilibrium, when nobody announces any link, and in general the game has a multiplicity of Nash equilibria. To address this multiplicity, pairwise-Nash equilibrium is commonly used in the literature.Footnote 1 It requires that, on top of the standard Nash equilibrium conditions, any mutually beneficial link be formed at equilibrium,Footnote 2 without specifying any process through which players might coordinate such a deviation. The aim of this paper is to redefine pairwise-Nash equilibrium as a non-cooperative refinement. If the concept can be rephrased without referring to any implicit cooperation, then its use in non-cooperative games would be justified. One thing needs to be cleared before one begins to talk about non-cooperative “equilibrium networks”. In this game, there usually exists many pure strategy equilibria that support the same network.Footnote 3 So, when we refer to the set, for example, of “Nash equilibrium networks”, we mean the set of networks for which there exists a pure strategy Nash equilibrium that leads to that network structure. Hence, the existence of one Nash equilibrium for the network qualifies it as a Nash equilibrium network. We define a new non-cooperative equilibrium, trial perfect equilibrium. In a trial perfect equilibrium players best respond to trembles of their opponents, where all best responses are given a strictly positive probability and trembles are ordered so that more costly mistakes are made with less or zero probability. Hence it is a non-cooperative equilibrium in the spirit (and an extension) of Myerson’s (1978) proper equilibrium and does not presume any coordination between players. We show that trial perfect equilibria coincide with pairwise-Nash equilibria for network formation games with link-responsive payoffs. This shows that it is unnecessary to refer to any bilateral coordination to eliminate networks where players fail to form mutually beneficial links. Link responsiveness requires that a change in the network changes the payoffs of the players whose links change. It is generically satisfied by network payoffs with some exogenous parameters (such as a constant marginal link cost). Section 2 introduces the model and describes the network formation game and the equilibrium concepts. The main result is provided in Sect. 3. Section 4 concludes with a discussion of our contribution. The proofs are in Sect. 5.",2
23.0,1.0,Review of Economic Design,28 January 2019,https://link.springer.com/article/10.1007/s10058-019-00219-x,Gender differences in competition: gender equality and cost reduction policies,June 2019,António Osório,,,Male,Unknown,Unknown,Male,"Women’s increasing market participation has changed the traditional family structure from breadwinner–homemaker to the dual-earner model. Despite the fact that men are doing more housework than ever before, women’s housework burden has not decreased proportionally to their increase in market labor (Bianchi 2000), which is a paradoxical incompatibility because the traditional gender construction of male breadwinner and female homemaker roles persists at home. Gornick and Meyers (2003) stress the social contradiction attached to these unbalanced developments: (...) if everyone is at the workplace, who will care for the children? Children are a metaphor for a domestic problem that persists in almost every dimension of the household. For instance, Presser (1994) and Bianchi et al. (2000) estimate that women perform 65–80% of all household labor (e.g., cooking, shopping, childcare, cleaning, among others). See Shelton and John (1996) for a survey. This situation overwhelms women that consequently find it difficult to compete with men in equal circumstances. In spite of the important progress achieved in the last decades, in particular regarding women’s labor market participation, gender policy is still not able to grant women the same opportunities as to men (Blau and Kahn 2016). At the same time, policies supporting men’s involvement in domestic tasks have developed modestly in comparison to market participation policies (Pascall and Lewis 2004). The objective of this paper is to investigate the implications of the unequal division of the household labor on men and women’s participation and on their effort incentives in competitive environments.Footnote 1 We compare the effects of  affirmative action and cost reduction policies. The latter gender policy is proposed in this paper and has the objective of promoting an equal division of tasks within the household and reducing the women’s share in the domestic labor, which then feedbacks into a reduction in the labor market cost of effort. In addition, we provide recommendations on how to reduce the persistent gender inequality. Affirmative action opens the possibility to gender equality by acting directly in the labor market.Footnote 2 On the other hand, cost reduction policies have the same gender equality objective, but targets non-market inequality (Pascall and Lewis 2004). While affirmative action introduces a labor market bias in favor of women, cost reduction policies remove the household bias against women. The latter policy promotes an equal division of domestic labor, which requires a change in the society traditional gender construction of male breadwinner and female homemaker roles. For each of these two policies, we measure men and women labor market participation, because it is an important indicator of gender equality and is central in any discussion on gender equality.Footnote 3 In addition, in order to understand the implications in terms of economic efficiency, we also measure men and women  total labor market effort and competition intensity. This is an important indicator because, despite the great acceptance that affirmative action has received, since the “best” candidate is not necessarily the chosen one, several authors question its adequacy in reaching the best economic outcomes (Coate and Loury 1993; Holzer and Neumark 2000, 2006; among others). Affirmative action raises concerns in terms of economic efficiency and social welfare (Holzer and Neumark 2000). Moreover, for some people, affirmative action is a form of reverse discrimination that reinforces stereotypes and goes against the idea of meritocracy. We consider a theoretical setting in which men and women with unequal domestic labor responsibilities compete in the labor market for a market prize (e.g., career, promotion, compensation, power, etc.). In this context, the present paper is the first theoretical approach that links the individuals share in the domestic labor with their competitive capacity in the labor market. We found that affirmative action can guarantee the same chances of success for men and women, but cannot guarantee the same participation, equal utility or effort without inducing inefficiency or distorting the labor market. However, we show that these objectives can potentially be achieved through cost reduction policies. The main conclusion is that if we want men and women to have the same opportunities and competitive capacity, we must solve the household problem first—while women are still holding a larger share of the domestic labor, they are in a weaker position to compete with men. We also found that moderate levels of affirmative action incentivizes men and women’s effort (and women’s participation). There exists a leveling effect that results in higher effort and competition intensity. Franke (2012) and Niederle et al. (2013) report similar results regarding effort and participation, respectively. However, we also found negative effects associated with affirmative action. If women are too much favored by affirmative action, they tend to save on costly effort in order to obtain a higher net utility, i.e., women free ride on affirmative action. Simultaneously, men’s effort decreases because it becomes less effective. This result reproduces the Sowell (2004) prediction that: Both preferred (women in our model) and non-preferred (men in our model) groups can slacken their efforts - the former because working to their fullest capacity is unnecessary and the latter because working to their fullest capacity can prove to be futile. We also found that cost reduction policies might be ineffective in raising the total labor market effort in the presence of affirmative action. This observation raises concerns about the potential complementary use of these two policy instruments because the positive incentive given to women does not compensate the reduction in men’s effort. Similarly, cost reduction policies make the use of affirmative action policies less justified and less effective. Intuitively, our results suggest that the use of affirmative action policies is only justified if there are asymmetries between men and women in the household. Therefore, once the household problem is solved there is no reason to support the use of affirmative action. This paper is organized as follows: Sect. 2 reviews the literature, Sect. 3 presents the theoretical framework, Sect. 4 defines affirmative action and cost reduction policies and the measurement instruments, Sects. 5 and 6 analyze competition between individuals of the same and opposite gender, respectively, and Sect. 7 concludes.",
23.0,1.0,Review of Economic Design,05 April 2019,https://link.springer.com/article/10.1007/s10058-019-00220-4,Between anchors and aspirations: a new family of bargaining solutions,June 2019,Emin Karagözoğlu,Kerim Keskin,Elif Özcan-Tok,Male,Male,Female,Mix,,
23.0,1.0,Review of Economic Design,11 April 2019,https://link.springer.com/article/10.1007/s10058-019-00221-3,The Borda rule and the pairwise-majority-loser revisited,June 2019,Noriaki Okamoto,Toyotaka Sakai,,Male,Unknown,Unknown,Male,"In 1770, Jean-Charles de Borda presented a paper on voting at the Academy of Sciences in Paris. His discussion began with an observation that in election, an alternative that gains a plurality of votes may be defeated by any other alternative in pairwise majority voting. In the modern terminology of social choice, Borda proved that the plurality rule is not pairwise-majority-loser consistent.Footnote 1 Borda continued his discussion by introducing a new election method now called the Borda rule, although he neither proved nor mentioned that the rule is pairwise-majority-loser consistent. From the viewpoint of modern social choice theory, this omission seems rather strange. The Marquis de Condorcet was appointed the permanent secretary of the academy in 1776. He published Borda’s paper in the academy’s proceedings of 1781 with favorable comments, which was printed in 1784 (de Borda 1784; de Condorcet 1784).Footnote 2 The following year, Condorcet published a now famous monograph on voting (de Condorcet 1985). In this monograph, he criticized Borda’s idea of reflecting ranking information of alternatives in social decision making. Condorcet reinforced his arguments by showing that all scoring rules, including the Borda rule, are not pairwise-majority-winner consistent.Footnote 3 Note that Borda’s and Condorcet’s arguments do not contradict each other because they discuss different criteria for a “loser” and a “winner”. In fact, the Borda rule is pairwise-majority-loser consistent. What about other scoring rules? The issues are somewhat complicated. Fishburn and Gehrlein (1976, Corollary 1) state that a scoring rule is pairwise-majority-loser consistent if and only if it is the Borda rule. They offer a sketch of the proof for the “if” part. However, to establish the “only if” part, they only write “Slight modifications in Smith’s proof yield the following corollary of Theorem 4”. This is not very helpful, because obviously, proving the “only if” part is much more difficult than proving the “if” part. Even worse is that Smith (1973) deals with a mathematically complicated model and does not analyze pairwise-majority-loser consistency, although he studies a similar topic. Therefore, it is hard to infer a correct proof from Smith’s argument. A purpose of this paper is to offer a full proof of Fishburn and Gehrlein’s statement; however, in fact, we do much more than that. We investigate details of the uniqueness of the Borda rule as a scoring rule that is pairwise-majority-loser consistent. In Fishburn and Gehrlein’s analysis, population is treated as a variable. They essentially state that for any non-Borda scoring rule, there exists a population for which the rule is not pairwise-majority-loser consistent. However, this allows the possibility that in a fixed-size population case, some non-Borda scoring rules are pairwise-majority-loser consistent. As the first step, we show that when there are three alternatives and six voters, all scoring rules are pairwise-majority-loser consistent. That is, the Borda rule is not a unique such rule at all in this case. In fact, in any fixed-size population case, we can find a non-Borda scoring rule that is pairwise-majority-loser consistent. However, when there are three alternatives, there exists a scoring rule that is not pairwise-majority-loser consistent for all population-sizes larger than six.Footnote 4 Our main theorem shows that for each non-Borda scoring rule, there exists a population n such that the rule is not pairwise-majority-loser consistent for all populations of size larger than n. That is, even if a non-Borda scoring rule is pairwise-majority-loser consistent in some fixed-size population case, it is not so in all large population cases. As a corollary to this theorem, we can conclude that the Borda rule is the unique scoring rule that is pairwise-majority-loser consistent for all populations. We also remark that this uniqueness can be generalized to accommodate the class of positional rules. Saari (1989, 1990) investigate various voting paradoxes and concludes that the Borda rule avoids many of them. Our analysis is in line with his conclusion, but the set of our results on the pairwise-majority-loser criterion is more detailed than his. For example, Saari’s analysis does not deal with a large n that we have mentioned above. Our arguments reinforce Saari’s findings that the Borda rule avoids many paradoxes, thereby supporting the Borda rule in the question of “Which is better between Borda and Condorcet”.Footnote 5 This paper is organized as follows. In Sect. 2 we introduce definitions. In Sect. 3 we conduct axiomatic analysis. In Sect. 4 we conclude our discussion. The proof of our main theorem is relegated to Appendix.",2
24.0,1.0,Review of Economic Design,19 May 2020,https://link.springer.com/article/10.1007/s10058-020-00232-5,Contests with insurance,June 2020,Yizhaq Minchuk,Aner Sela,,Unknown,Male,Unknown,Male,"It is quite frequent that a contest designer’s goal is to maximize the players’ productivity or, alternatively, their efforts. When players’ costs of effort are non-linear, the optimal contest form that maximizes the players’ expected total effort is not known and there are several ways to enhance the contestants’ total effort.Footnote 1 One of these ways is reimbursing of the contestants’ cost of efforts. The efficiency of reimbursement has already been considered in the auction theory literature (see, among others, Riley and Samuelson 1981, and Goeree and Offerman 2004). Reimbursement is also efficient for increasing the total effort in contests in which every contestant bears the cost of his effort whether or not he wins (see, among others, Kaplan et al. 2002; Yates 2011; Matros 2012). Reimbursement may have several forms, for example, Matros (2012) mentioned the R&D tax policy of President George W. Bush, where the winner gets tax credits that could be retained as reimbursement. Consolation prizes in contests could be interpreted as another way of reimbursement, where the role of the consolation prizes is to reimburse at least part of the cost of the contestants who do not win. For example, in sport tournaments such as Wimbledon, the world’s oldest tennis tournament, or the FIFA World Cup, the tournament for the association football world championship, or in other contests such as the Rubinstein International Piano Master Competition, all the participants win prizes such that the higher the stage the contestant reaches, the higher is the prize he obtains. Thus, the reimbursement is relative to the achievements of the contestants or, alternatively, it is relatively to their efforts. Here we take a different approach by analyzing the effect of insurance on contests.Footnote 2 In our model, the contest designer offers the contestants a complete reimbursement of their cost of effort, but the contestants have to pay him an insurance fee (premium) in order to get the right to be reimbursed. Each contestant has to decide whether or not to buy the insurance, and if he buys it, he pays the designer the insurance fee, and should he not win, his cost of effort is reimbursed by the contest designer. On the other hand, if a contestant decides not to buy the insurance, he does not pay the insurance fee, but then he bears the cost of his effort whether he wins or loses. Then, without knowing the decision of their opponents about the insurance, the contestants compete in an all-pay auction with non-linear cost functions under incomplete information about the contestants’ payoff functions. As such, our contest with insurance is actually a one-shot game in which every contestant has a two-dimensional strategy, the first is the decision of buying insurance or not, and the second is the effort choice.Footnote 3 We begin by analyzing the equilibrium in all-pay auctions in which the contestants’ values for winning are private information and show that the contestants with relatively low and high values of winning will decide not to buy insurance, while the contestants with middle values of winning will decide to buy it. The reason is that a contestant with a relatively low value of winning has a low probability to win, or, alternatively, a low expected payoff, and therefore the insurance fee is not worth the cost. Similarly, a contestant with a relatively high value of winning has a high probability to win, or, alternatively, a low probability to lose, and therefore the insurance is not profitable for him as well. The contestants with the middle values of winning, however, do buy the insurance, but in equilibrium it does not change their expected payoff with respect to the same contest without any insurance. We first consider a special case of an all-pay auction with insurance in which the insurance fee is equal to zero, namely, an all-pay auction with reimbursement only. We assume that the contest designer wishes to maximize his revenue which in this case is equal to the expected effort of the winner since the other contestants’ efforts are reimbursed and there is no insurance fee. We show that if the contestant’ effort function is convex, i.e., if additional effort becomes increasingly costly, reimbursement of the contestants’ costs of effort decreases the contest designer’s expected revenue. On the other hand, if the contestant’ effort function is concave, i.e., if additional effort becomes decreasingly costly, reimbursement of the contestants’ costs of effort increases the contest designer’s expected revenue.Footnote 4 In particular, when the cost function is linear reimbursement of the contestants’ cost does not affect the designer’s expected revenue. The above result that reimbursement may either increase (concave cost functions) or decrease (convex cost functions) the designer’s revenue in the all-pay auction does not imply that this contest form with insurance and with a positive fee is profitable for the contest designer, since the insurance fee can either increase or decrease the designer’s expected revenue. In this case with insurance, the designer’s expected revenue is a combination of the expected effort of all the contestants who do not buy the insurance, the expected effort of the winner, and the insurance fee paid by the contestants who buy the insurance. We provide, however a sufficient condition on the contestants’ cost function that the contest designer’s expected revenue in an all-pay auction with a positive insurance fee is higher than in the same contest form with insurance but without any fee. This sufficient condition is satisfied for every convex cost function of the contestants. Thus, although we show that when contestants have convex cost functions, insurance without a fee (reimbursement) is not profitable in the all-pay auction, insurance with a positive fee may be profitable. Indeed we provide an example in which contestants have convex cost functions and their total effort is larger in an all-pay auction with insurance than in the standard all-pay auction without insurance. Note that when the cost function is concave, as we already mentioned, we show that in a special case of an all-pay auction with insurance in which the insurance fee is equal to zero, we have a higher expected revenue than the standard all-pay auction.Footnote 5 Thus, even when the cost function is concave, the all-pay auction with insurance and the optimal fee, either zero or positive, is obviously more profitable for the designer than the standard all-pay auction without any insurance. Thus we actually show that with either convex or concave cost functions, the all-pay auction with insurance might be more profitable than the standard all-pay auction. We then analyze the equilibrium in all-pay auctions with mandatory insurance. When the insurance is mandatory, similarly to contest with entry fee or reservation price, the contestants with relatively low values choose not to participate, but all the contestants who decide to participate buy the insurance.Footnote 6 We compare the all-pay auctions with insurance that is either optional or mandatory and provide a sufficient condition such that with mandatory insurance and an optimal fee a higher expected revenue is achieved than with optional insurance and an optimal fee. Since we also showed that the all-pay auction in which the insurance is optimal might be more profitable than the standard all pay auction, we can conclude that in some environments the contest designer should offer insurance and then he must require that all the participants buy it. The rest of the paper is organized as follows. In Sect. 2 we introduce our all-pay auction with insurance. In Sect. 3, we analyze the equilibrium of this model. In Sect. 4, we analyze the designer’s revenue in all-pay auctions with insurance but without any fee, and in Sect. 5 we analyze the designer’s revenue in all-pay auctions with insurance and with a positive insurance fee where the insurance is either optional or mandatory. Section 6 concludes. All the proofs are in the Appendix. The literature on contest theory has suggested several different ways for enhancing the contestants’ total effort. For example, in all-pay auctions under complete information, Baye et al. (1993) showed that by excluding the strongest player (the player with the highest value of winning), the players’ total effort may increase.Footnote 7 Che and Gale (1998) demonstrated that a bid cap can increase the players’ total bid (effort). Gavious et al. (2003) showed that in all-pay auctions under incomplete information if agents have convex cost functions then effectively capping the bids is profitable for a designer facing a sufficiently large number of bidders. Moldovanu and Sela (2001) found that in such contests when cost functions are convex, several positive prizes may be optimal for a designer who wishes to maximize the expected total effort, and Cohen et al. (2008) studied the optimal effort-dependent reward in these contests and showed that it has to decrease in the contestants’ effort.Footnote 8 Our idea of using insurance in contests is another way to enhance the contestants’ total effort, but it is important to emphasize that our goal is not to find an optimal mechanism that maximizes the players’ expected total effort but to find a different practical method that is not necessarily better or worse than those mentioned above. Reimbursement in contests was introduced first by Cohen and Sela (2005) who studied two-player asymmetric Tullock contests in which the winner’s cost of effort is reimbursed and showed that in the unique internal equilibrium the weaker contestant wins with higher probability than the stronger one, and, furthermore, the expected total effort significantly increases compared to the same contest without reimbursement. Matros and Armanious (2009) studied this Tullock contest with n symmetric players and showed that the contest in which the winner’s cost is reimbursed maximizes net total revenue (total effort minus the winner’s reimbursement) while a contest in which all the players’ costs of efforts (other than the winner) are reimbursed, minimizes net total spending (total effort minus the losers’ reimbursement). Note that in the literature reimbursement was mostly studied for Tullock contests and in such contest forms as we already mentioned, Cohen and Sela (2005) and Matros (2012) showed that a reimbursement has a meaningful effect on the designer’s revenue in the Tullock contest with linear costs. Thus, by our finding, we can see a significant difference between the two main contest forms, the Tullock contest with a stochastic contest success function, on the one hand, and the all-pay auction with the deterministic contest success function on the other, in that in each of them the effect of reimbursement on the contestants’ total effort is completely different.Footnote 9 Recently, Minchuk (2018) showed that when the cost of effort is concave in all-pay auctions with reimbursement of the winner’s cost of effort, the expected net total revenue is higher than in all-pay auctions without reimbursement. Minchuk (2018) who studied reimbursement of the winner’s cost of effort in the all-pay auction while we focus on the reimbursement of the losers’ costs of efforts in the all-pay auction. Furthermore, the study we do of reimbursement plus fee (insurance) has not been yet studied in all-pay auctions nor in other forms of contest.",4
24.0,1.0,Review of Economic Design,20 March 2020,https://link.springer.com/article/10.1007/s10058-020-00229-0,Gaining advantage by winning contests,June 2020,Derek J. Clark,Tore Nilssen,Jan Yngve Sand,Male,Male,Male,Male,"Many contest situations have the features that (i) contestants meet more than once, (ii) winning in early rounds gives an advantage in later rounds, and (iii) the prize structure is such that the prize value in each stage differs. Contest designers interested in keeping up overall efforts among contestants in such situations may be concerned with win advantages potentially creating discouragement among early losers, at the same time as they would like to take advantage of the increased efficiency of early winners. In this paper, we set up a model to study such a contest situation and the contest designer’s optimum decision. In the model, there are two simple Tullock contests run in sequence among the same set of players, and the winner of the first contest has an advantage over the other players in the second one. The win advantage is such that the winner of the first contest has more productive efforts than the others in contest two, effectively biasing the probability of winning the second contest in favour of the early winner.Footnote 1 The win advantage from the early round introduces an asymmetry into the subsequent competition which was not there at the start. The effort-maximizing contest designer has available a prize fund of fixed size that she can freely allocate between the two contests. We find conditions under which the optimum for the designer is to put the whole prize fund in the second contest, so that the first contest is merely a token one where the contestants fight purely for the win advantage. We identify a tipping point for the bias parameter, below which this is the optimal policy for the designer. Giving one contestant a positive bias in the second-period contest success function will affect the best response functions of all players; the advantaged player can exert less effort than in the symmetric case and still win with a high probability, whilst the disadvantaged also reduce effort to save cost. In order to inspire effort in the biased contest, the designer must give a large prize. Hence, when the win advantage from the first contest translates into a biased contest success function in the second, the players must be sufficiently incentivized to induce effort by the promise of a high prize. If the bias is too large however, the discouragement effect cannot be overcome, and the principal will run a single symmetric contest. Our model can find application in a wide range of areas, for example research funding, sales force (and general personnel) management, elections and sports. Consider the choice facing research councils when deciding how to distribute prize funds for a research programme over several rounds. Here we would think of “effort” by competitors as resources spent building up a research team which would make a good application credible, rather than the amount of time spent writing an application. It has been argued that there is a sizeable win advantage in science, dubbed a Matthew Effect by Merton (1968). According to Gallini and Scotchmer (2002, p. 54), competing for grants is easier for those who have won previously: “[F]uture grants are contingent upon previous success. The linkage between previous success and future funding seems even more specific in the case of the National Science Council” . The principal, in this case a research council, may wish competitors to have most effort later on in order to allow the participating research teams to exploit the enhanced efficiency or productivity from the win advantage. Balancing this concern against the potential discouragement among non-winning research teams, our analysis shows that the research council would usually like to have the big research money late in the programme. Sequences of contests are also in frequent use in promotion or hiring competitions where candidates must perform different tasks, and winning gives a benefit at the next stage. In sales-force management, one finds seller-of-the-month awards and the like in order to provide motivation for the sales force. In such settings, it is not uncommon for the more successful agents to be given less administrative duties, better access to back-office resources, more training than the less successful, and better territories; see, e.g., Skiera and Albers (1998), Farrell and Hakstian (2001), and Krishnamoorthy et al. (2005). These factors may increase the successful salesperson’s efficiency in the competition. A common feature of political elections are television debates where candidates face each other and the audience gets information on their policies and other relevant attributes. Indeed, Schrott (1990) suggests that the winner of a pre-election TV debate may be seen as obtaining a win advantage in the ensuing election. In a quite different setting, students are subject to a number of tests throughout the year, with the final ranking being based on an exam in the end. In sports, the win advantage may be founded on psychological or physiological factors (Krumer 2013). Cohen-Zada et al. (2017) find a significant psychological advantage in men’s professional judo competitions; they link this to biological literature in which performance-enhancing testosterone increases following victory and falls after defeat. Evidence pointing to the presence of a win advantage in sequential competition is found in experimental studies carried out by Reeve et al. (1985) and Vansteenkiste and Deci (2003). These studies show that winners feel more competent than losers, and that winning facilitates competitive performance and contributes positively to an individual’s intrinsic motivation.Footnote 2 The study closest to ours is that of Möller (2012). Like us, he posits a sequence of contests in which the principal can choose how to distribute the prize fund across the contests. His win advantage differs from ours, though, since his contest designer can use prizes to fine-tune the amount of heterogeneity between competitors in the second contest. He posits a smooth relationship between the first-contest prize and the ability to compete in the second period, so that a designer can choose exactly which types of player compete in the second contest. Such power is often out of the scope of a contest designer. Our model captures a situation where there is a discrete advantage from winning, and where a loser gains nothing. Moreover, the size of the win advantage in our model is not related to the size of the prize on offer in the first contest; a psychological advantage of beating an opponent is, for example, not necessarily related to the immediate prize. This simplification means that our work easily extends to many players, whereas contest design in Möller (2012) would be difficult in this case. The win advantage in the work of Megidish and Sela (2014) is, on the other hand, more similar to ours. They consider a fixed contest structure in which two players compete for two identical prizes that are awarded in each of two rounds; the players’ valuations of the prizes differ initially, and the value that the winner of the first prize attaches to the second is either reduced or increased compared to the first. Hence winning may be a disadvantage or an advantage. The loser values the prize equally in both rounds. Megidish and Sela (2014) focus on the effect that budget constraints have on the intertemporal decisions of the contestants, as well as the interplay between the budget constraint and the marginal valuations. One important result is that a contest designer would prefer the two contestants to have increasing valuations (i.e. a win advantage) when the budget constraint does not bind. Our model indicates a limitation to this argument, since we find conditions under which the designer would prefer to shut down the second contest entirely. The effect of the win advantage can be enhanced or neutralized by the designer in our framework, whereas it is fixed in Megidish and Sela (2014). Furthermore, our formulation facilitates a transparent analysis of the case of n players, as well as calculation of the optimal size of the win advantage if this can be chosen by the principal. Also related is the study of Beviá and Corchón (2013), who look at the evolvement of strength in two sequential contests with identical prizes. The initial Tullock probability function (interpreted by the authors as a share) is augmented with a weight for each of two players showing  ex-ante strength. The share of the prize gained by a player in the first contest translates into a strength in the second one by an increasing transition function.Footnote 3 Conditions are presented under which (i) the weaker player in the first contest will become even weaker in the second (dubbed the avalanche effect), and (ii) the weaker player will get a lower share of the prize over time (the domino effect). Luo and Xie (2018) consider a direct extension of Beviá and Corchón (2013) by allowing the contestable prize to be endogenous in the sense that the prize on offer is reduced by the efforts of the rivals. This modification eliminates the possibility of the avalanche and domino effects. The intuition is that effort effectively costs the stronger player more since this reduces the size of the contestable prize and he expects to win a larger share of it; hence the weaker player has larger effort in the first contest which implies that he does not get weaker or a snaller share over time. Since effort influences the share of the prize in a continuous way, and the share then increases the strength continuously, Beviá and Corchón (2013) and Luo and Xie (2018) are more similar to Clark and Nilssen (2013), who consider a direct correspondence between current effort and the effort cost in a subsequent contest. Schmitt et al. (2004), Casas-Arce and Martínez-Jerez (2009), Grossmann and Dietl (2009), and Sela (2017) also discuss such dynamic effort effects, whereby early efforts create later advantages.Footnote 4 In Kovenock and Roberson (2009), it is the net effort, i.e., a winner’s effort over and above that of the other player’s effort, that creates an advantage in a later contest. Yildirim (2005) looks at a single contest in which efforts can be made over two rounds before the winner is decided; first-round efforts are observed before second-round efforts are made. More generally, our work is related to studies of dynamic battles; see the survey by Konrad (2009, ch 8). One such battle is the race, in which there is a grand prize to the player who first scores a sufficient number of wins. A related notion is the tug-of-war, where the winner of the grand prize is the one who first gets a sufficiently high lead. Early formal analyses of the race and the tug-of-war were done by Harris and Vickers (1987). A study of a race where there, as in our model, also are intermediate prizes in each round, in addition to the grand prize of the race, is Konrad and Kovenock (2009). Another variation of a dynamic battle is the elimination tournament, where the best players in an early round are the only players proceeding to the next round. Thus, in an elimination tournament, the number of players decreases over time. Although this is a setting very different to ours, similar issues are investigated. Both Rosen (1986) and Fu and Lu (2012) find, in line with our result, that it is optimal for the contest designer to put the prize mass towards the end of the tournament. Delfgaauw et al. (2015) correspondingly find, in a field experiment, that increasing late prizes leads to higher total effort. 
Mago et al. (2013) find that intermediate prizes increase efforts of both winner and loser in their theoretical and experimental analyses. This is contrary to our finding that it is optimal to shift prizes to the final round. However, in their work there is no win advantage. Krumer (2013) and Clark and Nilssen (2018, 2019, 2020) carry out analyses related to ours, where each stage contains an all-pay auction with a win advantage, whereas the present analysis is based on a Tullock contest at each stage. Klein and Schmutzler (2017) analyze a series of contests in which the effort in each period may be weighted when deciding on the final prize. They show, in line with our results, that the bulk of the prize mass should be distributed to the final round. For the most part, our analysis concerns a setting where the win advantage is exogenous while the prize distribution across contests is decided by the principal. Still, there is a clear link to the analyses of Meyer (1992), Ridlon and Shin (2013), Franke et al. (2013), and Franke et al. (2018) who discuss situations where the size of the win advantage, and therefore the asymmetry between players in the second contest, is decided upon by a contest designer. Similarly, Esteve-González (2016) shows, in a setting with repeated services procurement, that mitigation of a moral hazard problem in service provision may be achieved through introducing a bias in the second period contest based on past performance. Barbieri and Serena (2018) consider a best-of-three Tullock model in which the principal may change the bias at each stage, showing that the optimal scheme is victory dependent; this resembles the win advantage that we consider here. Their analysis concerns designing the structure of the game for a fixed prize, whereas our model also considers the optimal prize split. In line with this literature, we extend our analysis to also investigate the possibility that the principal may be able to design the competition in order to specify the exact size of the asymmetry in the second contest. We derive results for the optimal level of bias in this case for an effort-maximizing principal in the n-player framework, calculating also the proportional gain to using the optimal asymmetric dynamic contest as compared to a simple symmetric one. When the bias is an instrument under the control of the principal, one can also consider how incentives can be given to the first contest leader to exert high effort in the second contest. A research council may want the leading team to exert extra effort to become world-leading for example; this resembles the quality contest of Serena (2017), and we calculate the optimal bias set by the principal who wants maximum effort from the leader in the second contest. Here again there is an important difference between the case of two players and more. With two players, the equilibrium of the second contest is symmetric, and the principal cannot incite the leader to exert more effort by increasing the bias since the symmetric effort level achieves a greater than one half chance of winning. With more competitors, this effect is weakened, since the leader must beat many rivals, and simply having the effort of all others gains little in terms of extra probability of winning. The more rivals that must be beaten, the greater must be the bias in order to incentivize effort from the leader. The paper is organized as follows. In Sect. 2, we present the model. In Sect. 3, we present the analysis of the model and our main results on the principal’s optimum distribution of the prize fund across the contests, as well as the optimal bias when this is an instrument under the command of the principal. Section 4 concludes.",3
24.0,1.0,Review of Economic Design,02 March 2020,https://link.springer.com/article/10.1007/s10058-020-00228-1,Premium auctions in the field,June 2020,Sander Onderstal,,,Male,Unknown,Unknown,Male,"In his book Auctions and Auctioneering, marketing scholar Cassady (1967) describes a large variety of auction formats that he encountered during travels through over twenty countries across the globe. In the Netherlands, he observes the use of ‘premium auctions’, i.e., auctions where the runner-up (the highest losing bidder) obtains a premium, “called in Holland plok or plokgeld”, for driving up the price paid by the winner (p. 77). Van Bochove et al. (2016) report that various versions of premium auctions have been used since at least 1529 to sell timber, wine, spices, tea, coffee, books, art, tulips, financial securities, and real estate. Premium auctions are believed to be able to outperform standard auctions like the first-price sealed-bid auction, the Dutch auction, the English auction, and the Vickrey auction in some circumstances. The reason is that bidders compete fiercely for the premium, driving up the price paid by the winner. The additional competition may then compensate for the premium the seller has to pay to the runner-up. In this paper, we explore the potential of premium auctions as online selling mechanisms. We do so in an online field experimentFootnote 1 in which we compare the revenue-generating properties of two kinds of premium auctions with the Vickrey auction (i.e., the second-price sealed-bid auction). The two premium auctions that we study are variations of the Vickrey auction and only differ in that some bidders obtain a premium. In the (sealed-bid) Amsterdam auction, both the winner and the runner-up get a premium equal to 50% of the difference between the second-highest and third-highest bid. In the Fischer auction, the runner-up receives a premium equal to 5% of the price paid by the winner (i.e., the runner-up’s bid).Footnote 2 As far as we are aware, we are the first to study the theoretical and experimental properties of the Fischer auction. In our experiment, we sell three identical copies of a high-quality, limited-edition print using these three auction formats. We invited almost 10,000 members of a general-population panel to participate in an auction. The roughly 950 panel members that chose to enter were randomized over the three auction formats, resulting in over 300 participants per treatment. During the auction, the participants obtained no information on the number of other bidders or others’ bids. As a result, bids are arguably independent, so that we can still analyze the data in a statistically meaningful way despite the fact that we ran only one auction per treatment. More precisely, we use two methods to estimate the auctions’ revenue’s mean and variance for settings where the number of bidders equals 5–400. We use Mullin and Reiley’s (2006) recombinant estimation for low bidder numbers (up to 50) and estimates of the cumulative distributions of the bids for higher bidder numbers (between 50 and 400). Participants also answered survey questions on their background demographics (age, gender, education, and marital status), their risk attitude (à la Dohmen et al. 2011), and their bidding strategy. Our data show that the premium auctions do not outperform the Vickrey auction in terms of average revenue. We also find that the Amsterdam auction’s revenue dispersion is lower than in the other two auctions. Our paper adds to the experimental literature studying the relative performance of auction formats in the field. Lucking-Reiley (1999) tests revenue equivalence between first-price sealed-bid and Dutch auctions, and between English and Vickrey auctions. Häubl and Popkowski Leszczyc (2003), Reiley (2006), Katkar and Reiley (2006), Brown and Morgan (2009), Haruvy and Popkowski Leszczyc (2010) and Ostrovsky and Schwarz (2011) study the effect of reserve prices on auction revenue. Houser and Wooders (2005) and Brown and Morgan (2009) examine how auction ending rules affect auction revenue. Popkowski Leszczyc and Häubl (2010) study the profitability of bundle auction relative to separate-component auctions. Carpenter et al. (2008) and Haruvy and Popkowski Leszczyc (2018) compare various auction formats in terms of money raised for charity. As far as we are aware, we are the first to examine the relative revenue-generating properties of premium auctions in a field experiment. Earlier studies confirmed the intuition that premium auctions might raise more revenue than standard auctions. Milgrom (2004) shows theoretically that a premium auction may attract more entry by ‘weak’ bidders than the English auction and consequently may generate more revenue. Goeree and Offerman (2004) study ascending versions of the Amsterdam auction. They find theoretically and in the lab that two variants of the Amsterdam auction raise higher average revenue than the first-price sealed-bid auction and the English auction in a setting with strong ex ante bidder asymmetries. Hu et al. (2011a) observe in a laboratory experiment that the Amsterdam auction is less conducive to cartel formation and raises more money than the English auction and the first-price sealed-bid auction in the case of strong ex ante bidder asymmetries. However, the received literature reports mixed results for ex ante symmetric settings. According to the celebrated revenue-equivalence theorem (Myerson 1981), expected revenue is the same in premium auctions as in standard auctions in the case of risk-neutral bidders and separating equilibria. Experimental evidence in symmetric settings or settings with weak asymmetries is in line with this finding (Goeree and Offerman 2004; Hu et al. 2011a). In theoretical work, Hu et al. (2011b) find that the Amsterdam auction generates higher [lower] revenue than the English auction in the case of risk-seeking [risk-averse] bidders. Brunner et al.’s (2014) confirm this prediction in a laboratory experiment, although bids in the Amsterdam auction are less aggressive than predicted by theory. Sufficiently risk averse sellers might still prefer the Amsterdam auction over the English auction in that the lower revenue in the Amsterdam auction is compensated by a lower revenue dispersion (Hu et al. 2018). The setting in our field experiment is arguably a symmetric one in that anonymity assures that, from the viewpoint of the bidders, no subset of competing bidders could be identified as ‘strong’ or ‘weak.’ As a consequence, the results from our online field experiment confirm the external validity of the above results for ex ante symmetric settings. We conclude that in settings that lack strong ex ante bidder asymmetries, premium auctions are not particularly attractive for sellers. The Amsterdam auction might be an exception in that it might be an interesting format for strongly risk-averse sellers because of its relatively low revenue dispersion. The structure of this paper is as follows. In Sect. 2, we derive the theoretical properties of the sealed-bid Amsterdam auction and the Fischer auction and compare those with the well-known properties of the Vickrey auction. In Sect. 3, we discuss our experimental design and hypotheses. Our data analysis is in Sect. 4. Section 5 contains a short conclusion. Lengthy proofs of lemmas and propositions are relegated to Appendix A.",4
24.0,1.0,Review of Economic Design,08 April 2020,https://link.springer.com/article/10.1007/s10058-020-00230-7,A sequential bargaining protocol for land rental arrangements,June 2020,Alfredo Valencia-Toledo,Juan Vidal-Puga,,Male,Male,Unknown,Male,"Assume that there exists a single tenant that needs to negotiate with several lessors for the use of their land. Examples of such situations arise in natural resource exploitation, construction of public or private facilities, or urbanization in populated areas, and developing countries face critical challenges in management of land and natural resources (Kaye and Yahya 2012; van der Ploeg and Rohner 2012). In particular, conflicts between firms and indigenous communities have recently been arising in many countries in Latin America, Africa and Asia. Examples are numerous (Welker 2009; Sosa 2011; Arellano-Yanguas 2011; Akiwumi 2014; Tetreault 2015; Sarkar 2015; Walter and Urkidi 2017; Sarkar 2017; Nguyen et al. 2018; Fraser 2018). Another examples arise as restitution problems where two agents have rights over the land (Jaramillo et al. 2014) or as land aggregating for housing and infrastructure (Kominers and Weyl 2012). In these land conflicts, each side has their own legal right over the land. As pointed out by Valencia-Toledo and Vidal-Puga (2019) for the case of mining activities, Article 10 of the United Nations Declaration on the Rights of Indigenous People states that “indigenous communities have the right to give or withhold its consent to proposed projects that may affect the land they customarily own, occupy or otherwise use” (UN, 2007). Moreover, the mining firm has a potential investment and/or concession over those lands. Even when such a concession is not granted yet, the firm may have a high enough profit opportunity to make it possible to compensate the land owners in a fair way (Helwege 2015). The case of conflict between the mining industry and indigenous communities in Peru (Sosa 2011) is paradigmatic. The Southern Copper Corporation (SCC) is a company that has the right to exploit the underground resources in the southern Peruvian region. However, this land is customary used by local residents, who have the right to use the surface. A possible approach to solve the conflict is from a centralized point of view. A planner, for example the government, determines a fair compensation for the use of land.Footnote 1 However, the Peruvian government did not try this option and, instead, encouraged both sides to reach an agreement by themselves. Even though the protocol of negotiation is undetermined, we can still try to figure out how this negotiation may take place. We study this situation from a non-cooperative game perspective. This is the approach taken by Bergantiños and Lorenzo (2004), who modeled the negotiation process that took place between villagers and the authorities in order to construct pipelines to connect individual houses to a water dam. In order to follow a similar idea, we analyze the potential negotiation between the SCC and the local communities by taking into account a real situation that took place in Galicia, Spain. A military baseFootnote 2 was established in 1968 over the land owned by three land communities (named Salcedo, Vilaboa and Figueirido, respectively). In October 2008, due to the government decision to create a wide security perimeter around the base, these land communities engaged in a lawsuit claiming to be the rightful owners of the land. In November 2012, the Spanish Court of Justice settled in favor of the local communities. As local communities are the rightful owners of the land, the government was legally compelled to reach an agreement with them. We can identify the government as a single tenant and Salcedo, Vilaboa and Figueirido communities as lessors who negotiated on a price per unit of land and a quantity of land through a negotiation process. The final stage of this negotiation process, as reported by the local media, followed a cheap talk stage, that lasted almost 1 year, until the government made a final sequential negotiation round with each community separately. The assented protocol stated that, if a community agreed on a price, and, later on, the price increased for another community, then it will automatically increase for the former, without repeating the negotiation. Moreover, any agreement between the government and one community was conditional on reaching a minimum amount of land at the end of the process. At the end, the tenant rented 121 Ha of land which was less than total available (around 216 Ha). In this paper, we model the above protocol assuming that lessors are agents that want to increase the price, and that the single tenant is an agent who wants to decrease it. This situation has many similarities with the negotiation problem between SCC and the indigenous communities in Peru: there are one tenant and several lessors; the identities of the lessors are loose (land customary in Peru, and the possible presence of more communities in Spain); the tenant does not necessarily need all the land; and, even though negotiation protocol is undetermined, the tenant is the one who takes the initiative. In our model, the tenant sequentially negotiates with each lessor, following a pre-established ordering (which can be established in advance by the own tenant). In each stage, there exists a bargaining problem between the tenant and the lessor. We formalize the non-cooperative game in each stage as the alternating offer model defined by Binmore et al. (1986) and also used by Herrero (1989), which yields the Nash bargaining solution (Nash 1950) when the bargaining problem is convex. When the bargaining problem is not convex (as it may happen in our model), the generalized Nash bargaining solution proposed by Herrero (1989) arises in subgame perfect equilibria. We choose the Herrero solution because it has a solid non-cooperative foundation.Footnote 3 Even though it may not be unique in general, the Herrero solution is unique in bargaining problems where the product of utilities is maximized in a unique point.Footnote 4 This is the case in all the bargaining problems that arise in our model. This implies that, in each stage, agents reach an agreement given by the generalized Nash bargaining solution. The Nash bargaining solution also arises as equilibrium in many other natural two-player non-cooperative games, such as those modeled by Nash (1953), Rubinstein (1982), Binmore et al. (1986) itself, Van Damme (1986), or Papatya and Trockel (2016). There are also protocols with more than two players, but the results are either not so satisfactory, as they need refinements in the equilibrium concept, such as stationary strategies (Hart and Mas-Colell 1996, 2010; Trockel 2002), or not applicable to our setting, as they restrict to pure bargaining problems (Chae and Yang 1994; Krishna and Serrano 1996; Suh and Wen 2006; Bergantiños et al. 2007). Non-cooperative foundations of other bargaining solutions use some counter-intuitive features, such as bids on probabilities (Moulin 1984; Conley and Wilkie 1995) or the existence of a finite predetermined number of stages (Ståhl 1972). 
Matsushima and Shinohara (2015) also use the Nash solution as a way to identify the agreements in indeterminate non-cooperative settings. As opposed to this paper, they use the Nash solution in bargaining problems with more than two players. In this paper, we study two cases: In the first case, we assume that the tenant has the freedom to negotiate different prices per unit of land for each lessor (i.e. prices are lessor-dependent). We call this case as the non-uniform price case. In the second case, we assume that all the prices are updated, at the end of the process, to the highest price agreed upon. We call this case as the uniform price case. One of our results is that the uniform price case if more favorable for the tenant than the non-uniform price case, which seems to justify the fact that one of the first announces made by the government was that they would pay the same price per hectare to all communities. According to our model, this leads to the most favorable case for them. The remainder of this paper is structured as follows. In Sect. 2, we introduce the notation and the general non-cooperative protocol. In Sect. 3, we illustrate the non-cooperative protocol with an example with two lessors. In Sect. 4, we present the results for the unanimity case with a discontinuous profit function. In Sect. 5, we present the results for the non-unanimity case with two players and a discontinuous profit function. In Sect. 6, we study the non-unanimity case with two players and a continuous profit function. In Sect. 7, we present some concluding remarks.",
24.0,1.0,Review of Economic Design,25 October 2019,https://link.springer.com/article/10.1007/s10058-019-00227-x,NTU-bankruptcy problems: consistency and the relative adjustment principle,June 2020,Bas Dietzenbacher,Peter Borm,Arantza Estévez-Fernández,Male,Male,Female,Mix,,
25.0,1.0,Review of Economic Design,01 November 2020,https://link.springer.com/article/10.1007/s10058-020-00240-5,Optimal selling mechanisms with crossholdings,June 2021,Gino Loyola,,,Male,Unknown,Unknown,Male,"Many contests can in fact be considered as auctions with crossholdings, that is, auctions in which bidders have minority stakes in other bidders’ surplus. For instance, it is usual in some markets for competing firms to hold shares in one other, or for a fraction of a company’s ownership to belong to a non-controlling shareholder who also holds a controlling stake in a rival company.Footnote 1 The presence of crossholdings introduces countervailing incentives for bidders in that they get a payoff not only when they win an auction, but also when they lose. Since the losing bidder appropriates a fraction of the winning surplus, he will care about the valuation and the price paid by the winning bidder. Thus, losing transforms a bidder with crossholdings into a minority buyer, which induces him to bid less aggressively. As a result, the typical bidder’s incentives to raise his bid in order to obtain the object under auction are now counteracted by this particular incentive to lose induced by crossholdings. Previous literature has shown that this less aggressive bidding induced by crossholdings breaks down the revenue equivalence between standard auctions (Myerson 1981; Riley and Samuelson 1981), even when these ownership participations are symmetric. A seller interested in maximizing her expected revenue should therefore not be indifferent with respect to the mechanism used to assign an object, and consequently, the design of an optimal selling procedure in this situation should for her be a very relevant question. The present paper addresses this issue and characterizes the optimal selling mechanism in the presence of crossholdings. To this end, we follow the mechanism design methodology introduced by Myerson (1981) in a setup with independent private values. Our approach is thus normative, contrary to the positive perspective adopted by most prior research on auctions with crossholdings. In general, the literature compares some standard auctions in terms of expected revenues they yield. As mentioned above, its main conclusion is that the revenue equivalence principle no longer holds when bidders possess crossholdings [see Chillemi (2005) and Ettinger (2008) for private values; and Dasgupta and Tsui (2004) for private and interdependent values]. By contrast, we do not assume the existence of a particular auction format for exogenous reasons, but rather characterize what format the revenue-maximizing mechanism should have and how it could be implemented. One of the few papers that, like ours, takes a normative approach is Chillemi (2005). The author characterizes the optimal selling mechanism in the presence of crossholdings, but only when these stakes are positive and symmetric. His results show that the optimal mechanism is one in which expected revenue is increasing in the degree of common crossholdings, as in such a case the seller can extract a higher surplus from the losing bidder. The model constructed in the present work generalizes these results by allowing for two types of agents: bidders with asymmetric crossholdings and bidders without crossholdings. We show that since this ownership structure induces an asymmetric value structure, a discriminatory allocation rule against strong bidders emerges as optimal. In our framework, strong bidders are those who enjoy a stochastic comparative advantage in terms of their relevant value for the object being sold. Rather than stemming from crossholding in their rivals, however, this stochastic advantage arises from the degree in which bidders appropriate their own surplus. This insight explains why in the context of our ownership structure with three bidders the optimal allocation rule imposes a double bias. Firstly, among buyers with positive crossholdings the optimal mechanism discriminates against the bidder with the highest stake; and secondly, this mechanism discriminates to a greater degree against the bidder without crossholding. Our results on the bias against stronger bidders are analogous to that found in the literature on optimal auctions with bidders asymmetrically endowed in some dimension, especially in the context of a takeover bidding process (Povel and Singh 2006; Loyola 2012). This is the case, for instance, with Povel and Singh (2006), who characterize the optimal selling procedure that a target company should design when it faces buyers who are asymmetrically informed. As is done here, they show that optimal discrimination against the stronger bidder—the more informed one in their case—is achieved through a two-stage procedure. A similar result is obtained by Loyola (2012), who characterizes the procedure that should be used to sell a company when bidders have prior stakes in the target ownership (toeholds). In that situation, the optimal selling rule is, however, implemented through a one-stage procedure that discriminates against stronger bidders—those with a higher toehold size—by using a price scheme with extra winning charges and losing payments. In the crossholding case, we show that the revenue-maximizing allocation rule can be implemented by means of a sequential procedure with the following timing. In the first stage the seller invites the stronger bidders to participate, in a subsequent stage, in a modified first-price auction with personalized reserve prices. If they decline to participate, the object is awarded to the weakest bidder via an exclusive deal at a price which he will always accept. If, on the other hand, they do participate, the modified auction takes place with accepting bidders (which will always include the weakest bidder) and the discriminatory policy is implemented through a price-preferences scheme. A central property of the optimal mechanism is the fact that it is able to balance two opposite effects so as to maximize seller’s revenues. Since the discriminatory policy induces the stronger bidders with high values to reveal the truth, this enables the seller to extract more surplus from these bidders and thus increase her expected revenue. However, this incentive device is based on a threat, with potential costs in terms of efficiency (and thus in terms of creation of value) if it has to be carried out. Indeed, if the values of the stronger bidder(s) are not high enough to meet the more demanding requirements of the discriminatory policy, the seller will have to carry out the threat and assign the object to a weaker bidder. Since the latter’s valuation of the object may be smaller than that of the previously excluded bidder(s), the seller’s revenue could fall due to a lower ex post creation of value. Note that although it is analogous to the reserve price practice, here the negative effect on created value is less severe. This is because the eventual cost of the threat is only the sale of the object to a bidder with a value that is smaller than that of the excluded bidder but still larger than the seller’s. With a reserve price, on the other hand, the object is withdrawn from the auction and kept in the seller’s hands, which in our model is always worse in terms of created value. The optimal procedure also internalizes the fact that bidders with crossholdings obtain a share of the winning surplus even if they lose the auction. This allows the seller to extract surplus from losing bidders. Moreover, the discriminatory policy involved in the optimal mechanism is sensitive to changes in the ownership structure. Due to these two properties, a twofold and highly relevant result from the seller’s perspective is formally established: the expected revenue is increasing not only in the size of a common crossholding, but also in the degree of asymmetry of these stakes. Since putting the optimal procedure into practice may be too complicated, an alternative and simpler negotiation-based mechanism is proposed. This alternative procedure consists of a sequence of one-to-one negotiations with bidders in an attempt to extract surplus selectively and thus replicate the main property of the optimal mechanism. In a two-bidder setup, it is therefore shown that the seller’s expected revenue generated by this procedure is increasing with the asymmetry level of crossholdings and that, although suboptimal, it yields a larger revenue than both first-price and second-price auctions, two of the formats most commonly used in practice. In a broader sense, the present study is related to a growing body of literature on selling procedures when there are externalities, such procedures being either auctions (Bartling and Netzer 2016; Gatti et al. 2015; Brocas 2014, 2013; Lu 2012; Aseff and Chade 2008; Maasland and Onderstal 2007; Varma 2002; Jehiel et al. 1999) or bargaining processes (Loyola 2017; Laengle and Loyola 2012). However, this literature has not generally considered the asymmetric environment we study here, nor has it proposed the implementation of optimal mechanisms through a simple sequential negotiation procedure such as the one we propose. One exception in the received literature is Lu (2012)—perhaps the work that most closely resembles ours—, who develops a general methodology to find the optimal selling mechanism when there exist linear financial externalities among bidders. As an application of that methodology, Lu (2012) characterizes the revenue-maximizing procedure when there are asymmetric crossholdings among n bidders. Although this general model encompasses our setup, there are some important differences between the two. First, the approach they adopt for interpreting their results is not the same. Lu works with the virtual value concept, and so explains the discriminatory allocation rule in terms of the extent of information rents all the owners of the winning firm in the auction enjoy. We, on the other hand, work with the marginal revenue concept, and thus interpret the allocation rule as a discriminatory policy function whose biases arise from the stochastic comparative advantage of bidders with larger controlling stakes. Furthermore, this interpretative approach allows us to describe in detail how the optimal mechanism balances the trade-off between extraction and creation of value so as to maximize revenues. The second difference is that the more general setup adopted by Lu (2012) allows him to show that revenue is decreasing in the bidders’ controlling stakes, a result we are unable to demonstrate. Our simpler formulation of the ownership links structure does enable us, however, to establish a property Lu does not identify: that revenue is also increasing in the degree of asymmetry in crossholdings, which in our more reduced version with two bidders implies that revenue is also increasing in the degree of asymmetry in controlling stakes. The third difference is that Lu does not propose any practical mechanism to implement the discriminatory allocation rule. In contrast, we study two concrete mechanisms that implement all or some of the properties of that allocation rule, namely, an optimal auction procedure and a suboptimal negotiation procedure, respectively. Lastly, unlike Lu (2012) we explore the effects of crossholdings on the bidders’ incentives to adopt joint bidding practices like illegal rings and legal consortia. The remainder of this paper proceeds as follows. Sect. 2 constructs a model of auctions with crossholdings. Sect. 3 characterizes and discusses the properties of the optimal selling mechanism. The implementation of the optimal mechanism via auctions and negotiations is examined in Sect. 4. A discussion of joint bidding practices and robustness of our results is contained in Sect. 5. Finally, Sect. 6 presents the main conclusions. All of the proof are gathered in the “Appendix”.",
25.0,1.0,Review of Economic Design,02 January 2021,https://link.springer.com/article/10.1007/s10058-020-00242-3,All-pay auctions with private signals about opponents’ values,June 2021,Zhuoqiong Chen,,,Unknown,Unknown,Unknown,Unknown,,
25.0,1.0,Review of Economic Design,07 January 2021,https://link.springer.com/article/10.1007/s10058-020-00241-4,Decentralized college admissions under single application,June 2021,Somouaoga Bonkoungou,,,Unknown,Unknown,Unknown,Unknown,,
25.0,1.0,Review of Economic Design,20 October 2020,https://link.springer.com/article/10.1007/s10058-020-00238-z,A cumulative offer process for supply chain networks,June 2021,Juan F. Fung,Chia-Ling Hsu,,Male,Unknown,Unknown,Male,"The matching with contracts model of Hatfield and Milgrom (2005) is one of the most important contributions to the literature on two-sided matching. Hatfield and Milgrom (2005) provide a general and elegant framework that subsumes many of the previously separate specialized matching models—both with and without endogenously determined salaries, as well as more general relationships specified by a bilateral contract for each match. Moreover, it has led to various important applications in settings traditionally outside the scope of two-sided matching. The supply chain networks model of Ostrovsky (2008) is a significant extension beyond the standard “two-sided” matching framework that considers matching on a network. Ostrovsky (2008) presents a novel approach to an industrial organization topic, using the tools of matching, and the result is a mathematically elegant and intuitive model of supply chains. He extends the two-sided matching notion of a pairwise stable allocation to that of a chain stable network for the supply chain setting. Hatfield and Milgrom (2005) propose two methods to show the existence of a stable allocation in the two-sided, many-to-one matching with contracts setting: the generalized Gale-Shapely algorithm, which utilizes a fixed-point theorem, and the cumulative offer process, in which agents on one side propose and agents on the other side choose contracts from from a set of accumulated offers. The fixed-point approach in Hatfield and Milgrom (2005) generalizes Adachi (2000)’s use of Tarski’s fixed-point theorem to prove existence of stable matchings in the setting of one-to-one matching. Echenique and Oviedo (2004, (2006) extend Adachi (2000)’s fixed-point approach to the settings of many-to-one and many-to-many matching, respectively. Ostrovsky (2008)’s approach extends the fixed-point approach to the supply chain matching setting. Ostrovsky (2008) provides a natural generalization of substitutable preferences over contracts—same-side substitutability and cross-side complementarity—and shows that such preferences are sufficient for chain stable networks to exist. To prove existence, Ostrovsky (2008) introduces the T-algorithm, a fixed-point algorithm that finds a chain stable network. In this paper, we present an extension of the cumulative offer process of Hatfield and Milgrom (2005), modified for the supply chain matching setting. Our Supply-Chain Cumulative Offer Algorithm (SCCOA) is an extension of the many-to-many cumulative offer process in Fung and Hsu (2014). We assume choice functions, rather than preferences, to be the primitives of the model. As Chambers and Yenmez (2017) argue, the concepts of stability and substitutability are more natural in a choice-theoretic setting. The SCCOA provides an alternative proof for existence of chain stable networks when choice functions satisfy same-side substitutability and cross-side complementarity. Moreover, the proof provides insight into the interactions that lead to chain stability amongst different tiers in a supply chain network. Aygün and Sönmez (2013) and Aygün and Sönmez (2012b) show that Hatfield and Milgrom (2005) implicitly assume a condition called Irrelevance of Rejected Contracts (IRC) condition for the existence result.Footnote 1 Loosely speaking, the IRC condition means that rejecting a contract does not affect an agent’s choice set. Following the example in Aygün and Sönmez (2012b), we show that the IRC condition is also implicitly assumed in Ostrovsky (2008). In our proof of existence, we indicate the place where the IRC condition is used. Finally, we show that when preferences satisfy Hatfield and Kominers (2012)’s Law of Aggregate Demand and Supply, in addition to same-side substitutability and cross-side complementarity, then choice functions satisfy the IRC condition. Since Ostrovsky (2008)’s seminal contribution, matching in networks has been extended beyond supply chain matching. Hatfield and Kominers (2012) introduce bilateral contracts to generalize both the many-to-many and supply chain matching settings. Assuming preferences satisfy same-side substitutability and cross-side complementarity, which the authors term full substitutability, and acyclicity, which rules out contract chains that form cycles, the existence of a stable network is established using a fixed-point approach. Note that Hatfield and Kominers (2012)’s definition of stability implies Ostrovsky (2008)’s chain stability, though they are equivalent under full substitutability and acyclicity. Thus, our SCCOA finds a stable network in the setting of matching with bilateral contracts under full substitutability and acyclicity. Hatfield et al. (2013) generalize Hatfield and Kominers (2012) to a setting with continuous transfers and establish a connection between stable networks and competitive equilibria. Hatfield et al. (2018) show that in general trading networks, with full substitutes and Laws of Aggregate Demand and Supply (i.e., “mono-substitutable” choice functions), chain stability in this setting is equivalent to stability as defined in Hatfield et al. (2013) and Hatfield and Kominers (2012). This result subsumes the results in Hatfield and Kominers (2012) and Hatfield et al. (2013), implying that under mono-substitutability chain stability is equivalent to competitive equilibrium in general trading networks. Note that the definition of chain stability in Hatfield et al. (2018) is distinct from the definition in Ostrovsky (2008) as it is adapted to a more general setting. Fleiner et al. (2018) introduce possibly cyclic trading networks, generalizing the trading networks of Hatfield et al. (2013, (2018) and Hatfield and Kominers (2012). Fleiner et al. (2018) introduce the trail stability solution concept and show that full substitutability and the IRC condition imply that trail stability is equivalent to chain stability. The proof relies on Tarski’s fixed-point theorem. Adachi (2017) provides an alternative fixed-point algorithm to prove the existence of trail stable networks. The approach is a generalization of Ostrovsky (2008)’s T-algorithm. Cumulative offer processes have received increased attention in non-standard matching applications. Hatfield and Kojima (2010) propose the bilateral substitutes (which is weaker than substitutes) and the unilateral substitutes (which is stronger than bilateral substitutes but weaker than substitutes) conditions, and use a cumulative offer processes to show the existence of a stable allocation in many-to-one matching with contracts when choice functions satisfy bilateral substitutes. Sönmez and Switzer (2013) use a cumulative offer algorithm in Army cadet-branch matching, in which substitutable preferences do not hold. Kominers and Sönmez (2016) use a cumulative offer mechanism in matching with “slot-specific priorities,” which function as diversity constraints and impose a structure on preferences such that substitutability does not hold. Aygün and Turhan (2017) use a cumulative offer process in a school-choice setting with affirmative action constraints. In all of these settings, the absence of substitutability means fixed-point approaches do not work. Studying the properties of cumulative offer processes has also attracted recent interest. Hirata and Kasuya (2014) show that the cumulative offer process is order independent for many-to-one matching with contracts, assuming choice functions satisfy bilateral substitutes and the IRC condition. Order-independence of the cumulative offer process in turn implies many agents making offers simultaneously is equivalent to each agent making offers sequentially. Afacan (2017) shows that for many-to-one matching with contracts, the cumulative offer process respects improvements under unilateral substitutes, IRC, and the Law of Aggregate Demand. Finally, Hatfield et al. (2017) provide necessary and sufficient conditions for a stable and strategy-proof mechanism to exist in many-to-one matching with contracts and show that if a stable and strategy-proof mechanism exists, then it must be a cumulative offer mechanism. In the next section, we present Ostrovsky (2008)’s model, as well as the IRC condition. Section 3 presents the cumulative offer algorithm and states the main results. In “Appendix A”, we show that a stable allocation may not exist without the assumption of the IRC condition. In “Appendix B”, we provide an example of the operation of the Cumulative Offer Algorithm. The proofs are in “Appendix C”.",
26.0,1.0,Review of Economic Design,01 March 2022,https://link.springer.com/article/10.1007/s10058-022-00295-6,Testing alone is insufficient,March 2022,Rahul Deb,Mallesh Pai,Rakesh Vohra,Male,Unknown,Male,Male,"It is a truth universally acknowledged that an economy needing to recover from a pandemic-induced coma must be in need of inexpensive and widespread testing. We argue that this by itself is insufficient. Suppose that to reduce the spread of the virus, we must ensure that the fraction of infected individuals who are out and about is below some threshold. With unlimited testing, we could test each and every person, but what will they do with the information gleaned? Why should we expect someone who has tested positive for the virus to stay home and someone who has tested negative to go to work? If the first receives no compensation for staying home, she may choose to leave for work. The second, anticipating that infected individuals will show up for work, may choose to stay home.Footnote 1 As a result, the fraction of the population out and about will have an infection rate exceeding that in the population at large. Even if the prevalence of the infection is low, most agents may choose to stay home. This paper makes two points. First, in the absence of a cure or effective vaccine, any plan to restart the economy must combine testing with incentives. It is not enough to reward individuals for getting tested, as proposed by Levitt et al. (2020). One must shape behavior: compensate the infected for staying home and incentivize the uninfected to go out. Testing allows us to identify who should be the beneficiary of these transfers. Combining targeted testing with well-designed incentives results in better outcomes than either by themselves. Our second point is that where one tests also matters. There is a crucial difference between testing at work and testing ‘at home’. Testing at home should be interpreted as a way to test an individual without increasing exposure to others. An infected person who leaves home to be tested at work poses an infection risk to others who choose to go outside.Footnote 2 The precise mix of home and work testing depends, as we argue, on the level of infection risk agents are prepared to accept. The more willing agents are to go out, the more one should focus on random testing at home. If the reverse, one should focus on testing at work. If there were no delay between test and result, the distinction between home and work testing would be meaningless. One could ask an agent to test at home and report to work if they receive a negative result. The distinction matters as long as there is a substantial delay between taking the test and receiving a result (which is often the case in reality). With regards to COVID-19, not only do PCR test results arrive after a 2–5 day delay but there is an incubation period for the virus: a positive test result may not show up until 5-8 days after infection has occurred. Our claims are based on a simple model where individuals choose whether to stay in or go out and work. They decide given their subjective evaluation of costs and benefits. A planner has access to costly testing and can influence incentives by selecting rates of testing of those who stay at home and those who work, and contingent transfers to agents based on their choice and their test outcomes. We consider two scenarios:  A random subset of those who have elected to stay home are tested. If they test positive or are untested, they receive a subsidy to remain at home. If they test negative, they receive no subsidy. A random subset of those who have elected to go out are tested. Those who are untested or test negative receive their wage and possibly a premium. Those who test positive receive no wage. Our main finding is that when fear of infection is high, agents who are engaged in low-wage work with a high expected cost of falling ill should be tested at work only. On the other hand, those with high wages, low risk of being infected, and low cost of falling ill should generally be tested at home. The intuition is as follows. The optimal test-at-home policy involves a wage premium to those that work and a transfer to those who test positive at home. If the wage is small relative to the cost associated with becoming sick, those likely to be uninfected have a strong incentive to stay home. To incentivize the healthier individuals to work, a test-at-home policy would require a substantial wage premium that would encourage all types, including those likely to be infected, to work. An attractive transfer contingent on staying home would then be needed to dissuade these types. On the other hand, the optimal test-at-work policy involves a transfer to those at work who test negative and a transfer to those staying at home that is inversely proportional to the probability of being tested at work. The presence of a conditional transfer at work deters those likely to be infected, as they would receive nothing if the test result were positive. As a consequence, the home transfer can be lowered. When the wage is relatively high, and the cost of infection is low, testing at work is a deterrent if one can test at a high rate. When testing is sufficiently costly, such a test rate is infeasible, so testing at home is cheaper. The idea is straightforward. In the absence of widespread testing to distinguish between the infected and those who are not, we must rely on individuals to sort themselves. They are in the best position to determine the likelihood they are infected (e.g. based on private information about exposures, how rigorously they have been distancing etc.). Properly tailored transfers give them the incentive to do so. By tailoring the transfers and the testing policy to individual circumstances, one achieves the same goal at an even lower cost. The next section summarizes related literature. Section 3 describes the model and Sect. 4 contains the main findings. We reserve the discussion of our assumptions, as well as how the scheme of targeted testing and transfers might be implemented, to Sect. 5.",2
26.0,1.0,Review of Economic Design,12 May 2021,https://link.springer.com/article/10.1007/s10058-021-00252-9,Contests for catch shares,March 2022,Kyung Hwan Baik,Youngseok Park,,,Unknown,Unknown,Mix,,
26.0,1.0,Review of Economic Design,24 May 2021,https://link.springer.com/article/10.1007/s10058-021-00253-8,On preferences and taxation mechanisms in strategic bilateral exchange,March 2022,Cyrinus B. Elegbede,Ludovic A. Julien,Louis de Mesnard,Unknown,Male,Male,Male,"Tax policies can be used to mitigate distortions in resource allocation caused by imperfectly competitive behavior (Guesnerie and Laffont 1978; Myles 1989) and also for redistributive purposes (Mirrless 1971). In this paper, we explore a model in which tax policies play this dual role. The main motivation is to study, in a simple framework, the extent to which the effects and effectiveness of redistributive tax policies depend on the preferences of individuals who behave strategically in trade. To this end, we reconsider the class of noncooperative strategic bilateral exchange models with taxation introduced by Gabszewicz and Grazzini (1999, 2001). To study the implementation and the effectiveness of fiscal policies with redistribution, Gabszewicz and Grazzini (2001) consider the case of an exchange economy in which two commodities are initially held by a finite number of inside agents, the traders, while one agent, the outside agent, owns nothing. The traders behave strategically and the outside agent does not participate in trade because s/he does not have any resources. The strategic behavior of traders is implemented by embedding the finite exchange economy within a non-cooperative game in which the players are the traders, the strategies are supplies of commodities they bring to the market, and the payoffs are the utility levels they achieve. Insofar as two commodities are exchanged between traders who behave strategically  à la Cournot, and each type of trader is initially endowed with only one commodity, this strategic market game is akin to a bilateral oligopoly model. The bilateral oligopoly model was introduced by Gabszewicz and Michel (1997), and explored in Bloch and Ghosal (1997); Bloch and Ferrer (2001a, 2001b), Dickson and Hartley (2012), Amir and Bloch (2009), and Busetto et al. (2020a, 2020b). This model of strategic bilateral exchange represents a two commodity version of the strategic market game models (Shapley and Shubik 1977; Dubey and Shubik 1978; Sahi and Yao 1989; Amir and Bloch 2009).Footnote 1 In bilateral oligopoly each trader has corner endowment but wants to consume both commodities. All participants to exchange, namely the traders, behave strategically using quantities as strategies. There is a trading post to which traders may offer a fraction of their endowment of the commodity to be exchanged for the other commodity. The trading post aggregates the strategic supplies of all traders and allocates the amounts traded to each trader in proportion of her/his supply. The Cournot–Nash equilibrium is the noncooperative equilibrium concept. The bilateral oligopoly model is a natural starting point for studying the distortions generated by strategic behaviors in interrelated markets and the public policies to be implemented to restore Pareto-optimality. Within this framework of bilateral oligopoly, Gabszewicz and Grazzini (2001) study three kinds of fiscal policies to correct the market distortions caused by imperfectly competitive behaviors in markets and to collect resources for redistributive purpose. These fiscal policies consist of taxing trade and taxing endowments by subsidizing the outside agent; and taxing endowments and subsidizing the outside agent and the traders. By assuming that agents’ preferences are represented by the same Cobb–Douglas utility function, they show that the first two kinds of lump-sum taxes with transfers can only reach a second-best, whilst the third one leads to a first-best. Indeed, without transfers among insiders, such fiscal policies are not sufficiently powerful to neutralize the market power of strategic traders. A first-best analysis under endowment taxation with transfers for the linear, Cobb–Douglas, and CES utility functions is made in Gabszewicz and Grazzini (1999), who show that endowment taxation with transfers between the traders is Pareto optimal.Footnote 2 In this paper, we propose to extend Gabszewicz and Grazzini’s contributions by considering two kinds of lump-sum tax and transfers mechanisms. The first kind of fiscal policy, namely ad valorem taxation, consists in levying an uniform tax on transactions. The second kind of fiscal policy, namely endowment taxation, consists in levying an uniform tax on the endowments of traders. In both cases, the product of the tax is transferred to the outside agent in such a way s/he reaches some preassigned fixed utility level. The objectives of the paper are twofold. First, and rather than comparing the two fiscal policies with each other, we study the effect of taxes on strategic behavior when the individual decisions of traders depend on the degree of substitutability between commodities and also on the share of the consumption of these commodities in their utility. Second, we study the welfare implications of redistribution associated with these different taxation mechanisms in strategic bilateral exchange. To achieve these purposes, we consider that traders’ preferences are represented by generalized CES utility functions with different shares for the quantities of the two commodities consumed. This specification for traders’ utility function makes it possible to determine a broader set of strategic equilibria (with taxation) among which the existing equilibria constitute special cases. Thus, within this framework, our contribution to the literature is threefold. First, the non unitary coefficients on consumption and the elasticity of substitution make it possible to compute the set of strategic equilibria of the CES bilateral oligopoly model without taxation, among which the Cournot–Nash equilibrium of Bloch and Ferrer (2001b) is a special case for unit consumption shares. Correlatively, we state one result which puts forward the effect of the elasticity of substitution on equilibrium supplies, and which will be useful to understand the effectiveness of taxation mechanisms in our framework. Above all, our CES specification offers a unified computation of the various symmetric strategic equilibria under the two kinds of taxation mechanisms with transfer. In this respect, our model extends the Cobb–Douglas bilateral oligopoly model of Gabszewicz and Grazzini (2001), and it allows to deal with ad valorem taxation in the three bilateral oligopoly examples with endowment taxation of Gabszewicz and Grazzini (1999). Our second contribution concerns the effects of the two taxation mechanisms envisaged on the strategic supplies, by considering the possible values of the elasticity of substitution, and the role played by the non unitary shares on consumption. Indeed, we study the influence of the parameters of the CES utility functions on equilibrium strategies. More generally, our computations put forward the link between the effect of any fiscal policy and the local curvature of the indifference curves, through the possible values of the elasticity of substitution, in any Cournot–Nash equilibrium with taxation. We state one result which suggests that the reaction of strategic traders to variation in taxes depends on the value of the elasticity of substitution. This leads us to determine the extent to which the welfare effectiveness of tax policies with transfers depends on the elasticity of substitution. Our third contribution is reminiscent of the second welfare theorem in general equilibrium analysis but with strategic trade. It focuses on the redistributive purpose of the tax as well as on the optimality of the corresponding fiscal policy. In particular, our model highlights the importance of traders’ preferences in the optimality of fiscal policy, notably through the elasticity of substitution between commodities. Indeed, we show that the two fiscal policies cannot reach a Pareto-optimal allocation for any strictly positive finite value of the elasticity of substitution, i.e. for any bilateral oligopoly model in which commodities are neither perfect complements nor perfect substitutes. But, we also show that any fiscal policy with transfer can reach a first-best allocation when commodities are either perfect complements or perfect substitutes. Our results put forward that the preferences of agents matter for the welfare effects of fiscal policies. The paper is organized as follows. In Sect. 2 we describe the CES bilateral oligopoly model and study its properties in order to highlight the market distortions at work. Section 3 is devoted to the implementation of the taxation mechanisms with transfers and to the computation of the strategic equilibria with taxation. Section 4 deals with the effectiveness and welfare implications of the taxation mechanisms. Section 5 provides a comparison of the CES bilateral oligopoly model with taxation we use with the existing ones in the literature. In Sect. 6 we conclude.",1
26.0,1.0,Review of Economic Design,15 July 2021,https://link.springer.com/article/10.1007/s10058-021-00254-7,"Monopolistic third-degree price discrimination, welfare, and vertical market structure",March 2022,Xingtang Wang,Lin Zhang,,Unknown,Female,Unknown,Female,"Monopolistic third-degree price discrimination means that the monopolist implements different prices for the same commodity based on the different price elasticity of demand in different markets. It is quite common in the market. For example, iPhone and BMW respectively set varying retail prices in different countries around the world. Previous research has suggested that monopolistic third-degree price discrimination affects social welfare by influencing the total output (Schmalensee 1981; Varian 1985; Schwartz 1990; Ikeda and Nariu 2009) or product quality (Ikeda and Toshimitsu 2010; Nguyen 2014). When the product quality is endogenously chosen, past studies have investigated whether monopolistic third-degree price discrimination improves social welfare. For instance, Ikeda and Toshimitsu (2010) find that the move by the monopolist from uniform pricing to third-degree price discrimination increases welfare in a vertically differentiated product market where the product quality is endogenously chosen by a monopolist. Nguyen (2014) revisits the model of Ikeda and Toshimitsu (2010) where he focuses on the variable costs of quality, thereby showing that a move from uniform pricing to third-degree price discrimination always reduces welfare. Also, Klarl (2018) reveals that third-degree price discrimination will enhance welfare if a sufficiently pronounced complementarity between the willingness to pay and variable cost heterogeneity is given. Some studies have analyzed the impacts of price discrimination on input price under the vertical market structure (Economides 1998; Caprice 2006), but have not considered the inputs purchased by branches in different regions. Due to the transportation costs of raw materials, some enterprises set up different subsidiaries in different regions of raw materials and then sell their products in different regions.Footnote 1 For example, Mengniu Dairy has more than 10 production sites across China, which produce dairy products and then sell them by region. The pricing of Mengniu Dairy products is different in different regions, which means that Mengniu Dairy implements third-degree price discrimination. How does downstream third-degree price discrimination affect social welfare when the firm needs to purchase the inputs of final products? Existing studies have not addressed this problem. Drawing upon the model in previous research (Ikeda and Toshimitsu 2010), this paper assumes that the subsidiaries of monopoly located in two regions need to buy raw materials locally for producing their products.Footnote 2 In the product quality decision, this study considers two modes: fixed quality and endogenous quality.",5
26.0,1.0,Review of Economic Design,06 July 2021,https://link.springer.com/article/10.1007/s10058-021-00256-5,Optimal mechanism for land acquisition,March 2022,Soumendu Sarkar,,,Unknown,Unknown,Unknown,Unknown,,
26.0,1.0,Review of Economic Design,21 July 2021,https://link.springer.com/article/10.1007/s10058-021-00257-4,Equivalence theorem in matching with contracts,March 2022,Yusuke Iwase,,,Male,Unknown,Unknown,Male,"A matching-with-contract model refers to a market in which there are two disjoint sets of agents and each agent on one side is matched with another agent on the other side through a “contract” (Hatfield and Milgrom 2005). A typical example is the National Resident Matching Program in the United States, wherein a medical resident is matched with a hospital for practical training. In this example, a contract may represent a specific medical department. The purpose of this study is to understand the performance of the doctor-optimal stable mechanism in matching with contracts. An allocation is stable if no agent unilaterally rejects a contract allocated to the agent and no doctor-hospital pair bilaterally blocks the allocation. An allocation is doctor-optimal stable if it is stable and it dominates any other stable allocation. The doctor-optimal stable mechanism maps each preference profile to the doctor-optimal stable allocation. In this study, we assume that hospitals are just objects to be allocated to doctors, meaning that only doctors are under efficiency and incentive consideration.Footnote 1 In the setting of matching with contracts, we investigate group strategy-proofness, efficiency, Maskin monotonicity, and consistency of the doctor-optimal stable mechanism. Unfortunately, the mechanism does not satisfy any of these properties in general. Hence, we analyze which properties are more likely to be met by the doctor-optimal stable mechanism. Our main result says that, whenever there exists a doctor-optimal stable allocation for any doctors’ preference profile, the doctor-optimal stable mechanism is group strategy-proof if and only if it is efficient if and only if it is Maskin monotonic.Footnote 2 Moreover, we find that the doctor-optimal stable mechanism is consistent if and only if it is efficient under the two conditions: substitutes and the law of aggregate demand (LAD). Related literature Hatfield and Milgrom (2005) introduce the matching model with contracts. They show that under substitutes and the LAD, the doctor-optimal stable allocation exists for any doctors’ preference profile, and the doctor-optimal stable mechanism is strategy-proof. There are mainly three follow-up studies. Sakai (2011) finds that under substitutes and the LAD, the doctor-optimal stable mechanism is a unique stable rule that satisfies strategy-proofness in the class of stable mechanisms. Moreover, Hatfield and Kojima (2009) strengthen the result on incentives in Hatfield and Milgrom (2005) and show that the doctor-optimal stable mechanism is weakly group strategy-proof under the same assumptions, i.e., substitutes and the LAD. Hatfield and Kojima (2010) introduce a sufficient condition called unilateral substitutes, which is weaker than substitutes, for the guaranteed existence of the doctor-optimal stable allocation for any doctors’ preference profile. While Sakai (2011) and Hatfield and Kojima (2010), Hatfield et al. (2020) focus on the doctor-optimal stability and incentives, Hirata and Kasuya (2017) and Hatfield et al. (2020) consider (not necessarily doctor-optimal) stability and incentives. Hirata and Kasuya (2017) show that if a stable and strategy-proof mechanism exists, then it is unique and corresponds to the doctor-optimal stable mechanism. Hatfield et al. (2020) present three novel conditions to show that these conditions are necessary and sufficient for the existence of a stable and strategy-proof mechanism. Concerning the equivalence of good properties of a mechanism, Ergin (2002) shows that in matching without contracts, the Deferred Acceptance mechanism is group strategy-proof if and only if it is efficient if and only if it is consistent. Kojima and Manea (2010) extend the result of Ergin (2002) to a more general environment. In matching with contracts, Pakzad-Hurson (2020) shows that under substitutes and acceptance (stronger than the LAD), the doctor-optimal stable mechanism is group strategy-proof if and only if it is efficient. While considering a model outside our setting, Takamiya (2001) focuses on group strategy-proofness and Maskin monotonicity in housing markets and then shows the equivalence between them under some conditions on preference domains. Takamiya (2007) extends the result of Takamiya (2001) to a more general setting, which includes our setting and shows the same equivalence. Moreover, Klaus and Bochet (2013) consider an environment that covers not only indivisible goods but also divisible ones. Then, they introduce two conditions on preference domains such that strategy-proofness and Maskin monotonicity are equivalent.",
26.0,1.0,Review of Economic Design,14 August 2021,https://link.springer.com/article/10.1007/s10058-021-00258-3,On the manipulability of a class of social choice functions: plurality kth rules,March 2022,Dezső Bednay,Attila Tasnádi,Sonal Yadav,Male,Male,Female,Mix,,
26.0,2.0,Review of Economic Design,25 August 2021,https://link.springer.com/article/10.1007/s10058-021-00261-8,Negotiation statements with promise and threat,June 2022,Jin Yeub Kim,,,Female,Unknown,Unknown,Female,"Individuals, organizations, or governments (whom I will call negotiators) sometimes try to induce an agent’s cooperation when it is the key factor toward achieving their goals. For example, a researcher may need to motivate her co-author to collaborate more pro-actively on their research for achieving her goal of successful publication. Similarly, a country may need to elicit another country’s support and cooperation on some international issue to achieve her foreign policy goal. In both of these examples, cooperation occurs with no system of an explicit law enforceable on agents. In situations without enforceable contracts, negotiators making threats of punishment may help convince agents to cooperate to some extent. For example, a researcher could threaten her co-author’s reputation; a country can threaten to impose trade sanctions on another country. But would the use of such threats in negotiations be effective in fully inducing agents’ cooperation? I tackle this question and offer insight into what must constitute effective negotiating tactics to bring out cooperation. As much as the agent would suffer from punishments, he may have concerns or interests that do not perfectly align with a negotiator’s goal, generating costs of cooperating. Respecting those concerns would bring the agent’s interests into a closer alignment with the negotiator’s interests, possibly establishing full cooperation. To do so, the negotiator could give an assurance or promise that recognizes the agent’s concerns and interests. Incorporating those considerations, I develop a game-theoretic model of an interaction between the negotiator and the agent. The negotiator first decides whether or not to issue a negotiation statement that contains a threat of punishment, imposed if the agent does not cooperate with the negotiator, as well as a promise of reward for the agent in the event of the negotiator’s success in achieving her goal if the agent cooperates. The agent then decides whether to cooperate or not, without knowing the negotiator’s true level of resolve to follow through with the promise. Having provided the informal description of my model, I can describe my results as follows. Proposition 1 characterizes the equilibrium of my model and Proposition 2 identifies conditions under which, in equilibrium, cooperation occurs with positive probability. Proposition 3 provides a characterization of necessary and sufficient conditions under which the inclusion of promise increases the likelihood of cooperation in equilibrium compared to when there is no promise. Proposition 4 applies the credibility criteria for negotiation statement, developed by Myerson (1989), and provides a characterization of the conditions under which negotiation statements with both promise and threat credibly induce B’s cooperation with certainty. The results have several implications for the role of promise in negotiation statements as effective negotiating tactics to bring out full cooperation. First, a threat of punishment is not always enough, and combining it with a promise of reward for cooperating in the event of the negotiator’s successful goal achievement is essential to elicit more cooperation from the agent for certain ranges of parameter values. Second, for the ranges of parameter values where adding promise helps in increasing cooperation, the statement with both promise and threat may or may not be credible. Surprisingly, the inclusion of promise in negotiation statements may lower the chance of cooperation. This is not because the statement is not credible but because the negotiator is now less likely to use the statement in equilibrium. This paper connects to three strands of literature. First, my paper is related to a large body of the theoretical literature on, broadly defined, negotiations and bargaining under incomplete information (e.g., Baliga and Sjöström 2004, 2008, 2012; Banks 1990; Chung and Wood 2019; Fearon 1994; Fey and Ramsay 2011; Morrow 1989; Powell 1996; Schultz 1998; Wolford 2020). Most of those works consider negotiations as games with signaling and communication in which the meanings of negotiation statements are endogenously determined by the equilibrium itself. The model of my paper is motivated by and bases on the assumption of Myerson (1989) that negotiation statements have literal meanings that are exogenously defined and are understood by all players in negotiations. The use of such assumption and Myerson’s (1989) credibility criteria for negotiation statements to analyze the role of promise in a noncooperative game is the core contribution of this paper. Second, the application of my model to the security crisis on the Korean peninsula (discussed in Sect. 5) relates to the line of research on the prevention of security crises (e.g., Bas and Coe 2016; Bueno de Mesquita 2007; Dragu and Polborn 2014; Kim 2018; Nalebuff 1986; Powell 1990; Schwarz and Sonin 2008). Third, my paper complements the works that study the strategic importance of using both assurances and threats (e.g., Drezner 1999; Ishida and Kurizaki 2014; Kydd and McManus 2015; Sechser 2010). My paper is predicated on the argument of the empirical work by Pape (1997) that economic sanctions are rarely effective in achieving non-economic foreign policy goals. Also closely pertinent is the work by Cha and Kang (2018) that extensively examines the debate on foreign policies of sanctions against and engagement with North Korea. The paper is organized as follows. Section 2 introduces the model with promise and threat. Section 3 characterizes the equilibrium. Section 4 provides my main results concerning the role of promise. Section 5 discusses two examples that illustrate some implications of my analysis. Section 6 offers concluding remarks. Proofs of all results are in the “Appendix”.",
26.0,2.0,Review of Economic Design,01 September 2021,https://link.springer.com/article/10.1007/s10058-021-00262-7,Non-dictatorial public distribution rules,June 2022,Mridu Prabal Goswami,,,Unknown,Unknown,Unknown,Unknown,,
26.0,2.0,Review of Economic Design,17 September 2021,https://link.springer.com/article/10.1007/s10058-021-00264-5,Coalition-proof stable networks,June 2022,Chenghong Luo,Ana Mauleon,Vincent Vannetelbosch,Unknown,Female,Male,Mix,,
26.0,2.0,Review of Economic Design,09 September 2021,https://link.springer.com/article/10.1007/s10058-021-00265-4,Minimum coloring problems with weakly perfect graphs,June 2022,Eric Bahel,Christian Trudeau,,Male,Male,Unknown,Male,"The minimum coloring problem is a classic operations research problem. Originally, the goal was to find the minimum number of colors needed to color a map without any two adjacent countries ending up with the same color. The version we consider is defined by means of a graph: one must color the vertices of the graph in a way such that, for any edge in the graph, the two end vertices must be colored differently. Given a graph, the minimum number of colors needed to satisfy this condition is called the chromatic number. Many applications of the minimum coloring problem are relevant to the economics literature: Job scheduling: a number of agents have jobs that must be executed (by machines) at specific times. Jobs that are compatible (i.e., there is no time conflict) can be executed on the same machine. Given these demands, how many machines are needed? How should the total cost of executing all jobs be split between agents depending on their demands? Aircraft altitudes: flights that operate on routes that cross must be at different altitudes to avoid crashes. How many flight levels are needed and how do we split the burden of flying at higher altitudes? Exam scheduling: students have multiple courses, and we must schedule exams without conflicts (no student should have two exams at the same time). Some other constraints might be added, like no exams back-to-back. How many exam time slots will be needed? Radio frequencies: for electronic devices to function without interference, they must use different frequencies. How many frequencies are needed? Communication networks: if servers must be rebooted to install an update, and the provider does not want to interrupt service, in how many groups should the servers be divided in order to update them one group at a time, while keeping the network online? Law firm assignments: each legal case is assigned to a lawyer, but because of conflicts of interest, working on one case might preclude from working on another. Given a set of cases, how many lawyers will be needed? In all of these applications, making use of the exogenously given graph, one can partition the set of agents into elements, i.e., pairwise incompatible groups of agents such that there is no conflict within any given group. A different color can then be assigned to each element. To emphasize the economic applications, we often talk of tasks being processed on machines. Each element then needs to be assigned its own machine (color). Then, any number k of elements (in the partition) requires exactly k machines to be served, with the natural assumption that the more elements we have, the higher the cost of serving them —otherwise we can simply assign to the additional task/agent its own machine. While the operations research problem focuses on the question of cost minimization, the present work is rather concerned with the economic problem of splitting the minimum cost (of completing all tasks) between agents in a reasonable way. The technology, which is described by an exogenous cost function, gives the cost of simultaneously operating any number k of machines (or equivalently, the cost of serving any number k of elements in the partition of any subset of agents). Identifying each task with a distinct agent, our framework naturally induces a cooperative game with transferable cost which assigns to every coalition the minimum cost of completing all tasks within the coalition while taking into account the incompatibilities between them (as expressed by the graph). A group of pairwise incompatible agents is called a clique;Footnote 1 and this notion of clique is crucial for the analysis of minimum coloring problems. It is easy to see that the minimum number of colors needed is at least as large as the size of the largest clique, as members of a clique all need a different color. For an important subset of graphs, called perfect graphs, this (weak) inequality is in fact satisfied with equality for all coalitions: finding the chromatic number is equivalent to finding the highest possible cardinality for a clique. Among our described applications, it is known that the job scheduling problem and the aircraft altitude problem always induce perfect graphs. Many works in the literature have examined minimum coloring problems exhibiting a perfect graph (see for example Bahel and Trudeau (2019) and Okamoto (2008)). A wider class of graphs is the family of weakly perfect graphs, for which the chromatic number is equal to the size of a maximal clique for the grand coalition, but not necessarily for all subsets. We provide below an exam scheduling application exhibiting a weakly perfect graph that is not perfect. Suppose that 6 departments, labeled 1 through 6, are scheduling exams, and must pay a cost for each time slot needed. Each department offers one course, and two courses are deemed incompatible if a student is in both courses. There are five students, labeled A through E. The class rosters are \( \{A,B\}\) for the course of department 1, \(\{B,C\}\) for the course of department 2, \(\{C,D\}\) for the course of department 3, \(\{D,E\}\) for the course of department 4, \(\{A,E\}\) for the course of department 5 and \(\{A,B,C\}\) for the course of department 6. The incompatibility graph thus contains the following edges: \(\{1,2\}, \{2,3\}, \{3,4\}\), \(\{4,5\}, \{1,5\}, \{1,6\}\), \(\{2,6\}, \{3,6\}, \{5,6\}\). To see that the graph is not perfect, consider the coalition \(\left\{ 1,2,3,4,5\right\} .\) No triplet is a clique, so the highest possible cardinality for a clique is 2. However, dividing the group in 2 colors (with no adjacent vertices having the same color) is impossible since 1 would be of a different color than 2 and 5, 4 would be of the same color as 1 (since 4 and 5 are incompatible), and hence 3 would be of the same color as 2 (since 3 and 4 are incompatible), which is a contradiction because 2 and 3 are incompatible. However, this graph is weakly perfect: in the grand coalition \(\left\{ 1,2,3,4,5,6\right\} \), note that \(\left\{ 1,2,6\right\} ,\left\{ 2,3,6\right\} \) and \(\left\{ 1,5,6\right\} \) are all cliques (and there is no clique with four agents). Hence, the size of a maximal clique is 3, which is also the minimum number of colors [\(\left\{ 1,3\right\} \) in blue, \(\left\{ 2,5\right\} \) in red, and \(\left\{ 4,6\right\} \) in yellow] guaranteeing different colors for the two vertices of any given edge in the graph. As illustrated by Example 1, many applications induce weakly perfect graphs that are in general not perfect. Yet, not much is known about the cost sharing rules that one should use in such problems. Our analysis extends the understanding of cost sharing within minimum coloring problems in two directions: 1) we move from the class of perfect graphs to the class of weakly perfect graphs, and 2) we consider a technology exhibiting non-constant (possibly increasing or decreasing) returns to scale, whereas previous works have exclusively considered linear cost functions. An important set-valued solution concept in cooperative game theory is the core, i.e., the set of allocations such that no subset of agents pays more than what it would pay by doing the project on its own. Okamoto (2008) fully describes the core in the particular case of perfect graphs and linear cost functions. In the present paper, we prove (for the wider class of problems with weakly perfect graph and arbitrary cost function) that the core is non-empty if and only if the cost function satisfies the so-called  minimum average cost condition, i.e., the average cost of completing all tasks in the grand coalition cannot be greater than that of a smaller coalition. Interestingly, we prove that the minimum average cost condition is no longer sufficient to guarantee a non-empty core when one consider problems that do not induce a weakly perfect graph. This observation explains why Theorem 1 (and many other results) are stated on the domain of weakly perfect graphs. We show that the core, when it is non-empty, contains a subset of cost allocations that are each based on specific weights assigned to the respective maximal cliques: an agent’s allocation is then the sum of the weights of the maximal cliques he belongs to (Theorem 1). In the context of non-linear cost functions, we argue that it is desirable to have all elements of the partition contribute the same amount, as it is impossible to determine which element should be served first, second, etc. This is particularly constraining when we have a perfect graph: the only core allocations satisfying this property all assign no cost to an agent that does not belong to any maximal clique (Theorem 2). Next, we discuss two distinguished cost sharing solutions. Having weakly perfect graphs, and thus the equality between the minimum number of colors and the size of a maximal clique, allows to define budget-balanced cost allocations using maximal cliques. First, we define and examine the maximal cliques solution, which always picks an allocation from the subset described above, and is obtained by putting the same weight to all maximal cliques. The resulting cost sharing rule is simple and intuitive: cost shares are allocated in proportion to the number of maximal cliques an agent belongs to. We obtain a characterization by using a property that prevents strategic manipulations through merging and splitting, and two symmetry properties, one among cliques, and the other among compatible groups (Theorem ). Noting that allowing agents who do not belong to any maximal clique to free ride might not be desirable; we adapt a common axiom (Unanimity Lower Bound) requiring everybody to contribute at least a certain fraction of the total cost; and we define a solution concept satisfying this axiom. The resulting rule, dubbed Sequential Equal Contributions rule, is adapted from the airport rule (see Littlechild and Owen (1973)). In essence, the Sequential Equal Contributions rule ranks agents according to the size of the largest clique they belong to and then splits the marginal cost of adding an element equally between all agents that belong to a clique of that size. We then offer a characterization of this rule using Unanimity Lower Bound, an additivity property, a variant of the dummy property and an independence property (Theorem 4). In addition to the vast literature in operations research, Deng et al. (1999); Bietenhader and Okamoto (2006); Hamers et al. (2014) have examined how to allocate the cost in minimum coloring games, focusing on known solution concepts like the Shapley value, the nucleolus and the tau value. By contrast, we design cost sharing rules that are specific to the framework of the minimum coloring problem; and we use some natural axioms to characterize these rules. The maximal cliques rule generalizes the peak-demand solution introduced in Bahel and Trudeau (2019) for the specific case of the job scheduling problem. The sequential contributions rule is new for the minimum coloring game, but is adapted from the airport game (Littlechild and Owen 1973; Thomson 2014). The rest of the paper is organized as follows. Section 2 describes the model. In section 3, we study the core. Section 4 introduces some remarkable cost sharing rules and offers axiomatic characterizations. Independence of the properties used in these characterizations is proven in Appendix.",
26.0,2.0,Review of Economic Design,27 September 2021,https://link.springer.com/article/10.1007/s10058-021-00266-3,Reorganizing a partnership efficiently,June 2022,Eric S. Chou,Meng-Yu Liang,Cheng-Tai Wu,Male,,Unknown,Mix,,
26.0,3.0,Review of Economic Design,04 August 2022,https://link.springer.com/article/10.1007/s10058-022-00307-5,Well-designed incentive schemes: introduction to the special issue in honor of Semih Koray,September 2022,M. Remzi Sanver,,,Unknown,Unknown,Unknown,Unknown,,
26.0,3.0,Review of Economic Design,03 January 2022,https://link.springer.com/article/10.1007/s10058-021-00270-7,Mechanism design for pandemics,September 2022,Eric Maskin,,,Male,Unknown,Unknown,Male,"Before getting to mechanism design, let’s review why markets usually work so well. Suppose that there are many buyers and producers for some good. Suppose that buyer i enjoys (gross) benefit \(b_{i} (x_{i} )\) from quantity \(x_{i}\). Similarly, each producer j incurs cost \(c_{j} (y_{j} )\) to produce \(y_{j}\). Hence, society’s net social benefit is: At a social optimum, (1) is maximized subject to the constraint that supply equals demand: The solution to this constrained maximization is optimal in several senses: total production \(\sum {y_{j} }\) and total consumption \(\sum {x_{i} }\) are optimal \(y_{j}\) is optimal for each producer j \(x_{i}\) is optimal for each buyer i Achieving all three optimalities may seem complicated, but the market provides a simple solution. If p is the price at which the good can be bought and sold, then each buyer i maximizes and the first-order condition for this maximization is Similarly, each producer j maximizes with first-order condition But notice that (4) and (6) are also the first-order conditions for the problem of maximizing (1) subject to (2). And so the market outcome attains the social optimum as long as p is chosen so that (2) holds (mathematically, p is the Lagrange multiplier for (2)). How do we get the right choice of p? In a free market, p falls if supply exceeds demand and rises if demand exceeds supply. Through an equilibration process, the right price is found, and so, as claimed, markets do indeed attain the social optimum. But what If there is no pre-existing market for the good in question, as in the case of COVID test kits? We could still leave production and distribution to the free market, but there are several problems with doing so. In particular, how is a supplier to know (at least at first) how many tests kids to produce? After all, this is a new good and demand for it is uncertain. Furthermore, the supplier doesn't yet know who else will be producing test kits and how much they will produce. Under such circumstances, the supplier may be reluctant to incur the significant setup costs entailed in production until the uncertainties are resolved. Given time, the market could be expected to resolve them through the equilibration of supply and demand. But that process isn't instantaneous, and test kits are needed quickly. Furthermore, given that supply can’t be ramped up immediately, prices are likely to be high at first, which will disproportionately hurt poorer citizens and businesses (the very groups worst hit by the pandemic). And finally, the market approach ignores the public good aspect of test kits. If I buy and use a test kit, I will get some benefit—I will know whether or not I have the virus and can take proper precautions and seek treatment if I do. But much of the benefit goes to other people, who will be protected from infection if I quarantine as a result of testing positive. Since I have little incentive to take into account those other benefits. I am likely to underpurchase test kits. And the market system will result in too few kits being supplied and used.",
26.0,3.0,Review of Economic Design,06 May 2021,https://link.springer.com/article/10.1007/s10058-021-00251-w,Strategy-proof club formation with indivisible club facilities,September 2022,Bhaskar Dutta,Anirban Kar,John A. Weymark,Male,Unknown,Male,Male,"The degree of publicness of a good lies on a spectrum of possibilities ranging from the purely private to the purely public. In his theory of clubs, Buchanan (1965) addressed the problem of determining the optimal number of individuals with whom to share the consumption of a good when the benefits and costs of belonging to a sharing group—a club—depend on both the amount of the good and the number of individuals the club good is shared with. Buchanan was concerned with the problem of determining the optimal size of a single club. In contrast, Tiebout (1956) was interested in investigating the sorting of individuals into communities through locational choices so as to consume local public goods, which are goods that are non-rival in consumption within a community but provide no benefits to outsiders. Communities in Tiebout’s model can be thought of as being clubs in Buchanan’s sense.Footnote 1 In this article, we investigate the partitioning of a fixed group of individuals into clubs so as to share the benefits and costs of a single club good from which non-members can be excluded. We assume that each club is self-financing, so there is no cross-subsidization across clubs. The benefits of a club good are public to the members of a club, but are subject to congestion costs due to the negative externality that arises when a club good is shared with other people. Examples include community swimming pools and parks. As in these examples, we regard a club good as being some form of infrastructure, what we henceforth call a facility. We consider the case in which each facility is indivisible and is produced with a common fixed cost. The congestion cost experienced by an individual depends on both the number of individuals in his club and on his own characteristics. The latter is his type, which is private information. An allocation consists of a partition of the individuals into clubs and a specification of how the costs of the facilities are to be shared by club members. An allocation rule chooses a feasible allocation as a function of the types of the individuals. We consider two possibilities for the set \({\mathscr {A}}^F\) of feasible allocations, the set \({\mathscr {A}}\) of all allocations and the set \({\mathscr {A}}^E\) of allocations in which the cost of a club facility is shared equally among the club members. We are interested in determining which allocation rules, if any, are strategy-proof and satisfy one or more additional desirable properties when the congestion cost is non-decreasing in both the number of individuals a club is shared with and the value of the type parameter. Strategy-Proofness is the requirement that everybody always has an incentive to report his true type. By assuming that the size of a facility is fixed, we are able to focus on how the design of allocation rules that satisfy the properties that we consider depend on congestion effects in isolation from any facility size considerations. In addition to Strategy-Proofness, we consider four other desirable properties for an allocation rule: Cost Efficiency, Pareto Optimality on \({\mathscr {A}}^F\), Nondictatorship on \({\mathscr {A}}^F\), and Individual Rationality.Footnote 2 Cost Efficiency requires the partition of individuals into clubs to minimize the sum of the total financial cost of the club facilities and the aggregate congestion cost. Pareto Optimality on \({\mathscr {A}}^F\) requires that allocations be strictly Pareto optimal on the feasible set of allocations. Nondictatorship on \({\mathscr {A}}^F\) requires that nobody always has one of his most preferred feasible allocations chosen. Individual Rationality requires that nobody is ever worse off being assigned to a multi-member club than being in his own single-member club. If the congestion cost function is continuous and strictly increasing in the type parameter, we show that when the range \({\mathscr {A}}^F\) of an allocation rule is either the unrestricted range \({\mathscr {A}}\) or the restricted range \({\mathscr {A}}^E\), no allocation rule satisfies (i) Strategy-Proofness and Cost Efficiency or (ii) Strategy-Proofness, Pareto Optimality on \({\mathscr {A}}^F\), and Individual Rationality. However, if congestion costs can only take on two values, we show that a serial dictatorship satisfies all of our axioms if each club’s financial cost is shared equally among its members. We also present a number of examples of allocation rules with equal cost sharing (many of which are some form of serial dictatorship) and determine which of our axioms they satisfy when the congestion cost is linear in the type parameter. Finally, we introduce an allocation rule that uses iterative voting on ascending size to determine a club partition and show that it is not, in general, strategy-proof. This voting procedure adapts a voting rule for determining the membership of a single club due to Long (2019) to the problem of partitioning all of the individuals into clubs. In Sect. 2, we discuss some related literature. We present the model in Sect. 3 and the axioms in Sect. 4. Our impossibility theorems are presented in Sect. 5. We consider dichotomous costs in Sect. 6. In Sect. 7, we present our examples. The iterative voting procedure for determining a club partition is considered in Sect. 8. Finally, in Sect. 9, we offer some concluding remarks.",
26.0,3.0,Review of Economic Design,21 February 2022,https://link.springer.com/article/10.1007/s10058-022-00290-x,Equilibria on a circular market when consumers do not always buy from the closest firm,September 2022,Dominic Keehan,Dodge Cahan,Arkadii Slinko,Male,Unknown,Male,Male,"Hotelling’s (1982) classic model of spatial competition by firms on a linear market has been a cornerstone of economic theory, giving rise to countless extensions and generalizations. In many markets, a linear market is natural—e.g., the so-called Main Street model (Downs 1957; Eaton and Lipsey 1975; Graitson 1982). But markets are more varied in nature and the topology of the space where the competition takes place can vary. One such alternative space used in the classical circular city model, first introduced by Salop (1979), provides a natural representation for a wide variety of situations (such as a beltway around a city center) and has been a workhorse in the spatial competition literature ever since. The circular market has also been of interest because it allows the study of various market forces in isolation of boundary effects, which can be substantial (Aoyagi and Okabe 1993). Salop’s circular city model assumes that a customer always buys from the nearest firm. This assumption leads to the fact—exactly as in the Hotelling-Downs model—that no more than two firms can occupy the same location (Peeters et al. 2010, 2016). This contradicts the observable behaviour of firms that often cluster in larger numbers (e.g., fast food restaurants). To get a more realistic set of Nash equilibria we must relax this assumption and allow customers to patronise firms other than the closest one with some probability. This approach originated with Cox (1987) in a political candidate positioning framework, where he studied political competition under a class of electoral systems known as scoring rules. Though Cox’s setting was very different, as Myerson (1999) wrote, “there are logical similarities between political competition and market competition, and so analytical skills that have been sharpened by the study of either arena may be applied to offer new insights in the other arena.” In the economics setting, indeed, a scoring rule can be interpreted as a probability vector describing how likely customers are to shop at more distant firms. The application of this assumption to the firm interpretation has been explored primarily in the case of a linear market (Cahan and Slinko 2017, 2018; Cahan et al. 2018). To justify the implementation of Cox’s idea in the case of markets we need to look at the micro-foundation of Salop’s model which is rooted in the distances between the consumer and the firms. The consumer’s utility of purchasing from firm i is \(v - cd_i\), where v is the value of the object, \(d_i\) is the distance between the consumer and firm i and c is the cost c of travel per unit of distance. Since all firms are selling identical objects, the firm which is closest to the consumer offers him the highest utility, which explains why he purchases from that firm with probability 1. Implementing Cox’s idea does not change the micro-foundation, it only introduces an element of randomness to the model. Indeed, in our model the consumer’s utility of purchasing from firm i is \(v - c(d_i+\epsilon _i)\), where \(\epsilon _i\) is a random value. This random value may be the result of a number of external factors such as weather, traffic conditions or available transport. For example, a rush hour traffic jam will delay the consumer and is equivalent to travelling a longer distance. As a result, the vector of probabilities \((1,0,\ldots ,0)\) in Salop’s model becomes \((p_1,\ldots , p_n)\), where \(p_i\) is the probability the consumer buys from the ith distant firm. But the overall logic remains the same: the shortest distance is a paramount concern. The main pattern emerging from the literature is that “best-rewarding” voting rulesFootnote 1 that, roughly speaking, reward top ranked candidates while treating low ranked candidates similarly, encourage candidates to adopt differentiated policy platforms. On the other hand, “worst-punishing” voting rules, which treat top ranked candidates similarly while punishing poorly ranked candidates, encourage candidates to adopt similar policy positions. In the firm interpretation, this amounts to saying that: when the probability that a customer shops at more distant firms drops off quickly in the firm’s ranking, then firms locate at diverse locations; on the other hand, when there is a slow drop off in probability of patronage as firms become more distant, then firms are encouraged to agglomerate at a single location (e.g., a mall or city center). This result can be made quantitatively precise by defining Cox’s threshold of diversity which we call the Cox value or c-value. We assume that the society of customers is characterised by a vector of probabilities \(\mathbf{p}=({p}_1,\ldots ,{p}_{n})\), such that a customer buys from the ith most distant retailer with probability \(p_i\).Footnote 2 We assume \(p_1\ge p_2\ge \cdots \ge p_n\) and \(p_1>p_n\). The Cox value is defined as \(c(\mathbf{p},n)= \frac{p_1-{\bar{p}}}{p_1-p_n}\) so that best-rewarding vectors have \(c(\mathbf{p},n)\) greater than one half, worst-punishing vectors have \(c(\mathbf{p},n)\) less than one half, and intermediate vectors have \(c(\mathbf{p},n)\) equal to one half. Intuitively, having best-rewarding vector \(\mathbf{p}\) means that the market share of a particular firm is affected more by a relocation of its neighboring firms than more distant firms. On a linear market, there are broad classes of probability vectors for which equilibria do not exist, while for others agglomerative, non-convergent equilbria can be constructed (Cahan and Slinko 2017, 2018; Cahan et al. 2018). Results are often influenced, however, by the shape of the market and its boundaries (Eaton and Lipsey 1975; Aoyagi and Okabe 1993). Without the “peripheral firm advantage” of the linear model, our expectation is that equilibria in the circular model will be more numerous and exist for larger families of probability vectors. The goal of this paper, then, is to investigate under which probability vectors equilibria on a circular market exist and whether they are convergent or not. We show that for convex probability vectors—which are best-rewarding—non-convergent Nash equilibria always exist and are similar to the classical case where \(\mathbf{p}=(1,0,\ldots ,0)\). For intermediate vectors, we describe a class of symmetric probability vectors for which every profile is a Nash equilibrium. For the class of concave probability vectors—which are worst-punishing—we show the existence of convergent Nash equilibria and hypothesise that there are no non-convergent ones. Hinting at the validity of our hypothesis, we prove that under concave probability vectors we can rule out the possibility of bipositional Nash equilibria.",2
26.0,3.0,Review of Economic Design,07 September 2021,https://link.springer.com/article/10.1007/s10058-021-00263-6,Bridging bargaining theory with the regulation of a natural monopoly,September 2022,Ismail Saglam,,,Male,Unknown,Unknown,Male,"Regulation of a natural monopolist has been extensively studied under both symmetric and asymmetric information about production costs. Under symmetric information, the regulatory solution proposed by Dupuit (1844, 1952) and Hotelling (1938) suggests that the price of the product should be set at the marginal cost and the monopolist should be given a lump-sum subsidy to cover its fixed cost. If the regulatory objective makes an equitable compromise between the welfares of consumers and the monopolist, the solution under symmetric information can be optimal under asymmetric information as well, and it can be implemented, as shown by Loeb and Magat (1979), using a simple incentive scheme that delegates the output decision to the monopolist, which is also entitled—through a subsidy—to the whole economic surplus. Even though this scheme successfully induces the monopoly to choose an output level that would maximize the economic surplus, it also gives rise to an utter inequity because the surplus of consumers after the subsidy payment to the monopolist becomes zero. Because of this inequity, Loeb and Magat’s (1979) delegation scheme cannot be optimal in cases where the regulatory objective favors consumers more than the monopolist. The regulatory solution in such cases, as proposed by Baron and Myerson (1982), is (outcome equivalent to) an incentive-compatible direct-revelation mechanism that asks the monopolist to report its cost and that gives no incentive to lie. This mechanism entitles the monopolist to an information rent which can be optimally limited, but not eliminated, by the regulator to maximize the expected social welfare. The information rent depends on many factors, involving the cost parameter of the monopolist, the size of the industry demand, the beliefs of the regulator, and the weight of the monopolist’s welfare in the regulatory objective. Depending upon these factors, the monopolist’s rent can be very small or high, rendering either the monopolist or consumers, or even both, extremely upset about their regulatory payoffs. Even worse, the regulation may endanger the very existence of the industry. Under symmetric information, the monopolist—if regulated according to the first-best regulatory solution—might become reluctant to operate as it would earn no economic profit. The same reluctance might display itself, yet less strongly, also under asymmetric information if the monopolist does not find the information rent (which is ensured by the regulator to be always nonnegative) sufficiently attractive. The possibility that regulation might lead the monopolist to abandon the industry is a serious problem to face especially when the regulated product is a socially desirable good that can stimulate economic growth or that may result in new inventions and innovations. In practice, a well-known solution to this problem has been the wide use of patents, allowing inventors (potential monopolists) to obtain rents from their research and development activities. Another solution has been to leave a monopoly (a successful inventor) unregulated when there is reasonable evidence that the monopolist self-regulates its price and activities to avoid regulation.Footnote 1 Under the patent solution, the monopolist is entitled to the whole monopoly profit for a pre-determined period, after which it earns the regulated profit. Under the second solution, the monopolist escapes external regulation and earns its operating profit as long as it continues to self-regulate its price and output appropriately. Consequently, under both solutions the lifetime earning of the monopolist becomes a positive fraction of the lifetime total surplus generated in the market. In this paper, we propose a third solution to ensure the sustainability of the monopolistic industry. We basically consider the possibility where the monopolist and consumers can cooperatively bargain over the set of payoffs attainable under different regulatory objectives. The theory for cooperative bargaining was first introduced by Nash (1950), who formally defined a bargaining problem, consisting of a bargaining set and a disagreement point, in an abstract theoretical setting and proposed a solution—named after him as the Nash bargaining rule—along with a characterization result using several axioms. Nash (1950), and many economists who used his model to propose and/or axiomatize several other solutions, only dealt with symmetric (complete) information. The extension of Nash’s (1950) bargaining model to the case of asymmetric (incomplete) information is due to Harsanyi and Selten (1972), followed by Myerson (1979, 1984). Harsanyi and Selten (1972) considered a generalization of the Nash bargaining rule for two-person problems where the bargaining set is obtained by the strict equilibria of a particular choice mechanism under incomplete information, whereas Myerson (1979, 1984) considered problems where the bargaining set may be much larger than the one considered by Harsanyi and Selten (1972) as it contains payoff vectors generated by choice mechanisms satisfying Bayesian incentive-compatibility.Footnote 2 In our paper, we construct the regulatory bargaining problem under symmetric information using the model of Nash (1950) and several bargaining rules developed after (or anonymously before) him. More specifically, we construct the bargaining set under symmetric information as the convex and comprehensive hull of the utility possibility frontier of the ‘unregulated’ industry, since the regulatory solution (the marginal cost pricing rule) assumed in the literature induces in the utility plane only a single point, which is inconsistent with the sustainability of the industry and also leaves no room for bargaining. On the other hand, we construct the bargaining set under asymmetric information as the convex and comprehensive hull of the utility possibility frontier induced by the regulatory mechanism of Baron and Myerson (1982) for an admissible set of social welfare functions. For both symmetric and asymmetric information cases, we assume that the monopolist is not allowed to operate if the monopolist and consumers fail to agree in bargaining. Therefore, in both cases we choose the disagreement point in bargaining such that consumers’ payoff is always zero. On the other hand, we assume that the monopolist’s disagreement payoff is equal to zero if its fixed cost is non-sunk and equal to the negative of its fixed cost if it is sunk. At this point, we should state that our bargaining model under asymmetric information deviates, in an important aspect, from the approach in Myerson (1979, 1984), where agents bargain over the set of incentive-compatible choice mechanisms or the implied payoff allocations. In our paper, we assume that the agents (consumers and the monopolist) have already selected the general form of the incentive-compatible mechanism to be used, namely the optimal incentive-compatible regulatory mechanism proposed by Myerson (1982), but have not decided yet on the social welfare function that induces the particulars of this mechanism. Thus, agents in our model do not bargain over the set of admissible mechanisms as in Myerson (1979, 1984), they just bargain using the model of Nash (1950) over the set of social welfare functions, or more precisely over the set of implied expected payoff vectors, given a particular mechanism defined for each possible social welfare function. In this sense, the bargaining model we use under asymmetric information is a hybrid one that integrates the regulatory model of BM (1982) with the bargaining model of Nash (1950). Given the bargaining problems we have described above, we prove that the bargaining sets under symmetric information and asymmetric information (with complete ignorance) are dual to (reflections of) each other with respect to the 45-degree line passing through the origin (the common disagreement point) when the fixed cost of production is zero. Thanks to this duality, the bargaining solution under asymmetric information can be obtained from the solution under symmetric information by permuting the implied payoffs of the monopolist and consumers provided that the bargaining rule satisfies the axioms of anonymity and homogeneity.Footnote 3 We also show that under symmetric (asymmetric) information the bargaining payoffs (permuted payoffs) obtained under the Egalitarian, Nash, and Kalai–Smorodinsky rules are equivalent to the Cournot–Nash payoffs of unregulated symmetric oligopolies, involving two, three, and four firms, respectively. Moreover, we characterize two bargaining rules using, in addition to (weak or strong) Pareto optimality, several new axioms that depend only on the essentials of the regulation problem. Basically, we define these axioms only with reference to the bargaining sets observed under symmetric and asymmetric information. By doing this, we aim to exploit all the information present in the economic environment at hand, as suggested earlier by Roemer (1988). Indeed, his suggestions are consistent with a more general view than implied by our approach in this paper as his suggestions also include the direct use of a mechanism theory on an economic exchange environment instead of the use of a bargaining problem involving a utility possibility frontier (bargaining set) along with a disagreement point. Since the dimension of our economic problem is minimal (due to the singleness of the product/firm to be regulated) and since we already obtain the bargaining set and the disagreement point for our problem (and propose our characterization axioms) using all available economic information (in reference to the regulatory solution of a mechanism design approach by BM), we avoid the general criticism of Roemer (1988) to the classical bargaining theory on the ground that under many distributive mechanisms it is impossible to distinctly isolate two economic environments with distinct utility allocations if these environments correspond to the same bargaining problem (involving the same bargaining set and the disagreement point). The rest of the paper is organized as follows: In Sect. 2 we present some basic structures and in Sect. 3 we report our results. Finally, we conclude in Sect. 4.",2
26.0,3.0,Review of Economic Design,02 December 2021,https://link.springer.com/article/10.1007/s10058-021-00275-2,Protectionist demands in globalization,September 2022,Arzu Kıbrıs,Özgür Kıbrıs,Mehmet Yiğit Gürdal,Female,Male,Male,Mix,,
26.0,3.0,Review of Economic Design,20 July 2022,https://link.springer.com/article/10.1007/s10058-022-00306-6,Three public goods and lexicographic preferences: replacement principle,September 2022,Lars Ehlers,,,Male,Unknown,Unknown,Male,"We consider the problem of choosing multiple locations in an interval for an exogenously given number of identical public facilities. Each agent has a “single-peaked” preference relation over the interval and is allowed to select which public facility to use. An agent’s preference relation is “single-peaked” if up to a certain point, his “peak”, his welfare is strictly increasing, and it is strictly decreasing beyond that point. For example, a certain number of bus stops have to be located along a street. Other examples are gymnasiums, libraries, schools, telephone booths, and broadcasting news during a day. The planner faces the problem of choosing for each preference profile a list of locations.Footnote 1 An economy is completely described by the set of feasible locations, the set of agents, their preferences, and the number of public goods. A solution is a systematic way to assign to each economy and each public facility a location. Moulin (1980) introduces this problem in the special case of one good chosen from a one-dimensional continuum and considers strategic issues. The literature that follows extends his work in several directions.Footnote 2 However, until Miyagawa (1998) there was no axiomatic study of the problem of locating multiple public goods. Hotelling (1929) considers two competing businesses choosing where to locate on a street. He assumes that the businesses are identical and each individual patronizes only the one that is closest to where he lives. In Miyagawa (1998) an alternative specifies for each of the two public goods a location. Such a list is an option set and each agent compares two option sets by comparing their best elements according to his preference relation over locations. We call this extension of single-peaked preferences from the set of possible locations to the set of alternatives its max-extension. We contrast the max-extension with a different extension of preferences. The town government has to locate two public facilities, say two libraries, on a street. The libraries, though identical, only have one copy of each book. Then a certain book may have been already lent out and an individual, who wants to borrow this book, may have to drive to his second choice library. In most cases each individual visits his most preferred library, but sometimes both. Another example is the provision of telephone booths. Given two alternatives, first an agent compares the most preferred locations of each of the two alternatives, and if there is a tie, then he compares the other locations. We call this extension of single-peaked preferences the lexicographic-extension. Primarily, each agent uses the facility at his most preferred location, but he might be forced to consume the facility at his second choice location because the other facility is out of use. As a basic requirement we impose Pareto-optimality meaning that the rule chooses for each preference profile an efficient alternative. In our model, Pareto-optimality is weaker than in Miyagawa (1998). If the smallest and the greatest peak of a preference profile are distinct, then each alternative which is efficient for the max-extension is also efficient for the lexicographic-extension. We study the following notion of fairness. If the environment of an economy changes, then the welfare of all agents who are not responsible for this change are affected in the same direction: either all weakly gain or all weakly lose. As a variable parameter of an economy which may change over time, we consider preferences. Solidarity applied to such situations says that when the preference relation of an agent changes, then the welfare of all other agents are affected in the same direction. This replacement principle is called welfare-domination under preference-replacement, or simply replacement-domination. Moulin (1987) introduces replacement-domination in the context of binary choice with quasi-linear preferences. He calls it “agreement”.Footnote 3 For two pure public goods and the max-extension, Miyagawa (2001) shows that there are only two rules satisfying Pareto-optimality and replacement-domination. Ehlers (2002) shows for two public goods that each rule satisfying Pareto-optimality and replacement-domination is described by means of a continuous and single-peaked binary relation over the set of locations. For each preference profile such a rule chooses one location to be a most preferred peak in the peak profile according to the fixed single-peaked relation. The second location is indifferent to this peak according to the fixed single-peaked relation such that, if Pareto-optimality is not violated, the locations belong to opposite sides of the peak of the fixed relation. These rules are called single-peaked preference rules and are characterized by Pareto-optimality and replacement-domination. Our main result shows that for the provision of three public goods, the result of Ehlers (2002) does not extend. For three public goods and the lexicographic extension, only two rules satisfy Pareto-optimality and replacement-domination, the smallest-peak rule and the greatest-peak rule. The smallest-peak rule chooses all three locations to be the smallest reported peak. The greatest-peak rule chooses all three locations to be the greatest reported peak. By locating one more facility, the single-peaked preference rules are restricted to only two rules. This result is similar to that of Miyagawa (2001). For different models of public good economies the rules satisfying Pareto-optimality and replacement-domination have been identified. For the provision of one public good, Thomson (1993), and Ehlers and Klaus (2001) characterize the class of rules satisfying these properties on closed intervals. Each of these rules is determined by a unique point, called the target point. A target rule chooses for each preference profile the efficient location that is closest to the target point. Vohra (1999) and Klaus (2001) characterize the same class of rules on tree networks. Klaus and Protopapas (2020) characterize target correspondences for multi-valued location of one public good in an interval. For two pure public goods and the max-extension, Umezawa (2012) shows the incompatibility of Pareto-optimality and replacement-domination for tree networks. Gordon (2007a, 2007b) shows an incompatibility for the location of one public good on a cycle. Harless (2015) considers small changes for one public good economies. Another solidarity property is population-monotonicity which requires that the welfares of all agents are affected in the same direction when the population changes. For one public good, Ching and Thomson (1999) and Klaus (2001) characterize the class of target rules by Pareto-optimality and population-monotonicity. Gordon (2007a, 2007b) consider cycles and a more general framework. For the location of two public goods and the max-extension of preferences (and the lexicographic extension, respectively), Miyagawa (2001b) (and Ehlers (2003), respectively) identifies a certain class of rules and shows that these rules are characterized by Pareto-optimality and population-monotonicity. Ehlers (2001) and Heo (2012) consider other properties for the location of two public goods and the max-extension. The implications of other properties and settings of multiple public goods have been studied. Barberà and Beviá (2002, 2006) and Ju (2008) consider the location of public facilities when the agents are not free to choose the good they consume. Jackson and Nicoló (2004) and Bogomolnaia and Nicoló (2005) allow for congestion. Bochet and Gordon (2012) and Bochet et al. (2013) consider the location multiple pure goods without congestion. Reffgen and Svensson (2012) and Alcalde-Unzu and Vorsatz (2018) consider other issues. The organization of the paper is as follows. Section 2 introduces the general model and the axioms. Section 3 defines the max-extension and presents the main result of Miyagawa (2001). Section 4 shows our main result for three public goods and the lexicographic-extension. Section 5 compares our result with Ehlers (2002). The Appendix contains the proof of our main result.",
26.0,3.0,Review of Economic Design,04 August 2022,https://link.springer.com/article/10.1007/s10058-022-00305-7,To sell public or private goods,September 2022,Simon Loertscher,Leslie M. Marx,,Male,,Unknown,Mix,,
26.0,3.0,Review of Economic Design,03 September 2021,https://link.springer.com/article/10.1007/s10058-021-00260-9,When are committees of Condorcet winners Condorcet winning committees?,September 2022,Fatma Aslan,Hayrullah Dindar,Jean Lainé,Female,Male,Male,Mix,,
26.0,3.0,Review of Economic Design,14 August 2021,https://link.springer.com/article/10.1007/s10058-021-00259-2,An axiomatic re-characterization of the Kemeny rule,September 2022,Burak Can,Mohsen Pourpouneh,Ton Storcken,Male,Male,Male,Male,"Social choice rules are correspondences that assign sets of alternatives as collectively chosen outcomes to preference profiles of linear orders over a set of alternatives, while preference rules assign sets of linear orders as collectively chosen outcomes to these profiles. In each of these settings, a unanimous agreement, i.e., everyone having the same preference, would lead to a trivial outcome (a unanimous consensus alternative/preference) and any sensible choice/preference rule would behave this way. The difference in rules stem from decisions in non-trivial cases. Social choice/preference rules can behave very differently in cases where unanimity is not observed in the preference profiles. One way to still touch upon this unanimity idea is the following: the rules can still try to find the alternative/preference which is “closest” to being the unanimity winnerFootnote 1. When closeness is defined, for instance, by the Kemeny distance, we notice that Borda choice rule and Kemeny–Young (1959, 1978) preference rule are very similar, i.e., Borda (as a choice rule) chooses the alternative which is the “closest” to being the unanimity winner, while Kemeny (as a preference rule) chooses the linear order which is the “closest” to being the unanimity preference.Footnote 3 This paper shows that despite the aforementioned conceptual similarities between Borda and Kemeny, there are still important axiomatic differences between these rules. One would expect that axiomatic similarities will carry over from Borda choice rule to Kemeny preference rule. Borda choice rule is known to be consistent (Young 1974), so is the Kemeny preference rule. However, as well-known, Borda fails Maskin (1999) monotonicity and so do many other scoring rules [exceptions are characterized by Doğan and Koray (2015)]. Despite this monotonicity failure in Borda choice rule, Can and Storcken (2013) has shown a monotonic characterization of the Kemeny preference rule. This paper proves that indeed monotonicity is a very concrete feature of the Kemeny (as a preference) rule, despite it’s score-like algorithm and similarities to Borda (as a choice rule). We provide some novel and normative conditions on preference rules with which we prove a new monotonic characterization of the Kemeny preference rule. Thereafter, in Sect. 6 we re-visit some axiomatic features shared also by Borda and Kemeny (both as preference rules). The set of characterizing conditions used in this paper are being unbiased, monotone, weighed tournamental, strongly tie-breaking, and strongly gradual. Being unbiased requires that for profiles at which all possible preferences are reported the same number of times, the outcomes must equal to all possible linear orders. Being (update) monotone requires that if an update occurs in one of the agent’s preferences towards one of the outcomes, then this update should change the result in such a way that the new outcomes still contain this preference, and possibly more but no more than what was present before the update.Footnote 4 In Can et al. (2021), being tournamental is defined as “outcomes should only depend on the tournament results on pairs of alternatives in the profiles”. We use a weaker version of this condition, i.e., being weighed tournamental, which requires that outcomes depend also on the pairwise margins of these tournaments.Footnote 5Being strongly tie-breaking requires that if there’s indecisiveness over two alternatives, e.g., the outcomes comprise preferences with alternatives a over b, and also those with b over a, then any deviation in the profile concerning these alternatives should break this tie. Roughly speaking, being strongly tie-breaking captures the tie-breaking-part of positive responsiveness as introduced by May (1952). Finally, being strongly gradual requires that in case of an even number of agents, if the outcomes were decisive over two alternatives, e.g., all of them ranked a over b, then no minor deviation concerning a and b, can turn the outcomes into a decisive win for b over a. The paper is organized as follows. In Sect. 2 we formalize some basic notions and the model. The characterizing conditions are treated in Sect. 3 and in Sect. 4 we show how these translate to weighed tournament correspondences. Section 5 discusses that the Kemeny rule satisfies these conditions and that it is the only rule which does so. Section 6 discusses the independence of the conditions used and concludes the paper.",
26.0,3.0,Review of Economic Design,03 November 2021,https://link.springer.com/article/10.1007/s10058-021-00269-0,"Anonymous and neutral social choice: a unified framework for existence results, maximal domains and tie-breaking",September 2022,Onur Doğan,Ayça Ebru Giritligil,,Male,Female,Unknown,Mix,,
26.0,4.0,Review of Economic Design,07 December 2022,https://link.springer.com/article/10.1007/s10058-022-00319-1,On the axiomatic theory of bargaining: a survey of recent results,December 2022,William Thomson,,,Male,Unknown,Unknown,Male,"The model of bargaining proposed by Nash (1950) has been one of the most successful paradigms of game theory. It has been the foundation stone of an extensive theoretical literature, and the solution that Nash defined and characterized has been widely applied. There are several reasons for its enduring appeal. First and foremost, Nash provided an answer to a question that had stumped economists since at least Edgeworth (1881) concerning the outcome of bilateral negotations: where on the contract curve should the two parties be expected to settle? Alternatively, what should an arbitrator or a judge recommend to them? The simplicity of Nash’s model is certainly another reason; by abstracting from the concrete details of the decision that has to be made and of the bargaining process that the participants may engage in, Nash reached compelling generality. Also, the Nash solution, being based on marginal utilities, lends itself to easy calculations; it is tailor-made for economists, steeped in marginal analysis. More broadly, Nash’s paper has been deeply influential in demonstrating the power of the axiomatic method in the search for well-behaved solutions to cooperative games, and beyond that, it has been an important source of inspiration in the development of the field of economic design. Theoretical analyses of Nash’s model keep appearing and so do its applications. Nash (1950) has racked up over 10,000 citations on Google Scholar and its 1953 follow-up over 4000 citations. A presentation of Nash’s approach is not only an obligatory chapter in game theory textbooks but it is now introduced in all graduate microeconomics manuals. In a preface to a recent volume of essays on the Shapley value, I referred to that concept as a crown jewel of game theory; the Nash solution certainly qualifies as another one. This essay is a survey of the recent axiomatic literature spawned by Nash’s paper, an update of Thomson (1994, 2010).Footnote 1 For it to be self-contained, a number of classic results had to be restated but I strived to minimize redundancies. Also, this survey does not cover the applications of the theory. Doing so would require that specific concrete models be specified, with the danger that the reader would get lost in their idiosyncracies.",1
26.0,4.0,Review of Economic Design,03 October 2021,https://link.springer.com/article/10.1007/s10058-021-00267-2,An étude in modeling the definability of equilibrium,December 2022,Ariel Rubinstein,Kemal Yıldız,,Male,Male,Unknown,Male,"An étude is a usually short instrumental musical composition of considerable difficulty, which is designed to provide practice material for perfecting a musical skill ( Wikipedia (2020)). What follows is analogous to an étude—it is a short exercise in modeling that is designed to provide practice material for economic theorists.
 The étude builds on the object assignment model. Consider a society consisting of equal numbers of agents and objects. The objective is to uniquely assign each agent to an object. An agent has preferences over the objects and there are no externalities. The novel feature of the model is the inclusion of a language that is a set of orderings over the set of agents. We think of the orderings as potential criteria for determining whether an agent is better-suited to an object than other agent who envies him. The main idea of the paper is that in equilibrium the assignment of an agent to an object should be supported by a statement that is expressible using the language. Specifically, a statement that can support the assignment of an agent i to a particular object, where the set of candidates to be assigned to the object is I, should be “definable” in the following manner: “Agent i is the best-suited agent in I according to the ordering \(\ge _\lambda \)” (where \(\ge _\lambda \) is one of the language’s orderings).
 The approach adopted is not descriptive nor do we attempt to solve any practical economic problem. Nonetheless, the basic idea of applying different criteria in order to allocate different types of objects is a real-world phenomenon. For example, a university may allocate some seats to local students while other will be allocated according to academic abilities. Another example might be a public housing project, which assigns some apartments according to socio-economic status and others according to willingness-to- pay. The proposed solution concept is definable equilibrium (D-equilibrium). A candidate for D-equilibrium is an assignment of the agents to the objects and an attachment of a single criterion to each object. In D-equilibrium, each agent is the unique best-suited agent within the group of agents that includes himself and every agent who envies him, according to the criterion attached to his assigned object. In other words, there is no agent who both envies another agent’s assignment and is at least equally suited as him according to the criterion attached to the assigned object. Later, we present several interpretations of D-equilibrium. For some of them, we have in mind a “decentralized economy” in which a behind-the-scenes process—an “invisible hand”—attaches a criterion to each object. For other interpretations, we have in mind a central planner who justifies an assignment by declaring—possibly cynically—that the assigned agent is better-suited to the object than any other agent who prefers the object to the one he is assigned to. Of special interest is the class of dichotomous languages in which each criterion partitions the agents into those who satisfy a certain property and those who do not. Given such a language, an agent i can be singled out from a group of agents I by a statement of the following form: Agent i is the only agent in I who satisfies a certain property. In this case, a D-equilibrium is an assignment of the agents to the objects and an attachment of a property to each object, such that if an agent i envies agent j, then agent j has the property attached to the object while agent i does not. For societies with a language of strict orderings, a D-equilibrium assignment is identical to a stable assignment of a marriage problem à la Gale and Shapley (1962). Specifically, an assignment is a D-equilibrium assignment if and only if there it is a stable assignment in the associated marriage problem in which the two sides of the market are the agents and the objects, such that each agent follows his given preference relation over the objects and each object ranks agents according to its attached ordering. In what follows, we define the notion of D-equilibrium, discuss its interpretations, suggest a refinement of the notion, and present some examples. We also prove several simple propositions on its existence and efficiency. Our goal is to demonstrate an equilibrium concept in a social situation that does not involve trade, but rather requires that assigning any agent to an object can be supported by a certain type of statement expressed in a given language. This approach stems from the view that solution concepts in economic theory (whether they refer to markets, games or decision scenarios) should be expressed in the language of the participants. Our view is that the sensitivity of the outcome to the underlying language is a merit of a model. This is in line with Rubinstein (1978), Rubinstein (2000) who argues that an agent’s preference relation in an economic model should be definable in a given langauge.",1
26.0,4.0,Review of Economic Design,07 March 2022,https://link.springer.com/article/10.1007/s10058-022-00297-4,A note on roommate problems with a limited number of rooms,December 2022,Duygu Nizamogullari,İpek Özkal-Sanver,,,Female,Unknown,Mix,,
26.0,4.0,Review of Economic Design,12 January 2022,https://link.springer.com/article/10.1007/s10058-021-00280-5,Stability of an allocation of objects,December 2022,Murat Yılmaz,Özgür Yılmaz,,Male,Male,Unknown,Male,"An exchange economy of discrete resources with private endowments is when each agent owns an indivisible good (an object) and these objects are to be allocated among agents via direct mechanisms without monetary transfers. A central notion when there are private endowments is individual rationality, which requires that the assignment should be such that no agent is worse off than her endowment. Another important (stability) property of this problem is core: no coalition of agents should be able to block the assignment; that is, they should not prefer reallocating their endowments among themselves (and leaving the economy) over the assignment. But, core is in general empty in the weak preferences domain. An alternative (and weaker) notion is the bargaining set by Aumann and Maschler (1964): a blocking is justified only if there is no counter-objection to it and an allocation is in the bargaining set if there does not exist a justified blocking. We prove that any allocation obtained by the well-known Top Trading Cycles class is in the bargaining set, but not all allocations in the bargaining set can be obtained by this class. If preferences are strict, core is a singleton and it is the only solution which satisfies individual rationality, Pareto efficiency and strategy-proofness (Ma 1994; Sönmez 1999). Also, core is equivalent to the outcome of the well-known Top Trading Cycles (TTC) algorithm (Shapley and Scarf 1974), which works as follows: Each agent points to her most preferred available object (all objects are available at the beginning) and each object points to its owner. Since all agents and objects point, there is at least one cycle. The algorithm assigns to each agent in the cycle her most preferred available object (that is, the object she points at) and removes her with her assigned object. This continues until no one is left. The resulting mechanism is group strategy-proof and Pareto efficient (Roth 1982). When an agent may be endowed with multiple objects or no object, the top trading cycles rule is generalized to the hierarchical change rule, which is characterized by Pareto efficiency, group strategy-proofness and reallocation-proofness (Pápai 2000). A more general trading mechanism is trading-cycles and it is characterized by group strategy-proofness and Pareto efficiency (Pycia and Ünver 2017). While the extension of the TTC algorithm to the weak preferences domain is not trivial, such extensions satisfying individual rationality, Pareto efficiency and strategy-proofness are shown to exist (Jaramillo and Manjunath 2012; Alcalde-Unzu and Molis 2011; Saban and Sethuraman 2013). Strategy-proofness characterizes a subclass of these generalized TTC class satisfying Pareto efficiency (Saban and Sethuraman 2013). When the restrictive strict preferences assumption is removed, core can be empty (Shapley and Scarf 1974). Actually, core is non-empty only for a very special preference and endowment structure (Quint and Wako 2004). A weakening of core is weak core: blocking is allowed only if each agent in the blocking coalition is strictly better off than the assignment. The extensions of the TTC (to the weak preferences domain) are in the weak core. Our focus is on another notion, the bargaining set, which incorporates an important consideration into the process of blocking an assignment: when blocking, coalitions should consider possible counter-blockings of other coalitions. More precisely, an assignment is in the bargaining set if blocking by a coalition implies that there is another coalition blocking the assignment resulting from the initial blocking (Definition 2). This notion is formulated by Aumann and Maschler (1964) and later analyzed for different economies. In the context of a market game with a continuum of players, the bargaining set is equivalent to the set of Walrasian allocations (Mas-Colell 1989). For non-transferable utility games, the bargaining set is non-empty under certain conditions (Vohra 1991).Footnote 1 For an exchange economy with differential information and a continuum of traders, the bargaining set and the set of Radner competitive equilibrium allocations are equivalent (Einy et al. 2001). While the bargaining set notion in these works takes into account only one step of counter-objection to a blocking coalition, the consideration of a chain of counter-objections implies a more refined axiom (Dutt et al. 1989). The idea of bargaining set also inspires some works on allocation of discrete resources in school choice context in terms of relaxing stability notion, which is central to matching theory: if a student has an objection to an allocation because she claims an empty slot at a school, then there will be a counter-objection once she is assigned to that school since the priority of some other student will be violated at that school. An outcome is in the bargaining set if and only if for each objection to the outcome, there exists a counter-objection (Ehlers et al. 2014).Footnote 2 Some other works refer to bargaining set in similar ways (see Ehlers 2010; Kesten 2010; Alcade and Romero 2015). The paper is organized as follows: Sect. 2 introduces the model and the graph theoretical framework, on which the mechanisms and some of the proofs are built. Section 3.1 defines core and bargaining set notions. We state our main result in Sect. 4. All proofs are in the Appendix.",
26.0,4.0,Review of Economic Design,28 November 2022,https://link.springer.com/article/10.1007/s10058-022-00321-7,Outside options in neutral allocation of discrete resources,December 2022,Marek Pycia,M. Utku Ünver,,Male,Unknown,Unknown,Male,"Serial dictatorships have often emerged as the canonical simple mechanisms in the literature on the allocation of indivisible objects without transfers and with single-unit demands (i.e., the Hylland and Zeckhauser 1979 model). A serial dictatorship mechanism allocates objects by ordering agents, and then letting the first agent choose her most preferred object, thereafter letting the second agent choose his most preferred object among those still available, etc. Svensson (1999) explains the attractiveness of serial dictatorships by showing that they are the only neutral and group strategy-proof mechanisms. A mechanism is neutral if its outcome does not depend on the labelling of objects.Footnote 1 A mechanism is group strategy-proof if there is no group of agents that can misstate their preferences and obtain a weakly better house, and such that at least one agent in the group gets a strictly better house. Svensson restricts attention to environments in which agents have no outside options and hence no individual rationality constraints. We allow for the outside options: each agent can remain unmatched if she chooses to, i.e., participation is voluntary. Our main result establishes that the class of group strategy-proof, neutral, non-wasteful and individually-rational mechanisms consists of mechanisms we call binary serial dictatorships. Individual rationality ensures voluntary participation: no agent is assigned a house worse than her outside option. Non-wastefulness is a weak efficiency property: a mechanism is non-wasteful if there is no unassigned house that an agent prefers to be matched with rather than her assignment. The class of binary serial dictatorships generalizes serial dictatorships to the setting with outside options. A binary serial dictatorship first assigns a selected agent her most preferred outcome among all houses and her outside option; we also refer to being assigned the outside option as being unmatched. A second agent is then assigned his most preferred outcome among all not-yet-assigned houses and his outside option. In contrast to serial dictatorships, the identity of the second agent can depend on whether the first agent is matched with a house or with an outside option. The mechanism then repeats the procedure, selecting a third agent whose identity depends on whether the first and second agent were matched with houses or outside options, etc.Footnote 2 Our characterization has two corollaries. First, because binary serial dictatorships are Pareto efficient, we can conclude that binary serial dictatorships are also the class of group strategy-proof, neutral, Pareto efficient and individually-rational mechanisms. Second, in the subdomain of our preference domain in which the outside option is always ranked last by all agents—the domain that most previous axiomatic studies on house allocation used—our result implies that a mechanism is group strategy-proof, neutral, and non-wasteful if and only if it is a serial dictatorship. Serial dictatorships were introduced by Satterthwaite and Sonnenschein (1981) in private good economies and studied by Svensson (1994) in the house allocation context as a strategy-proof mechanism in absence of outside options (also see Roth 1982). In addition to Svensson (1999), Ergin (2000) characterized serial dictatorships by maintaining the neutrality requirement and replacing group strategy-proofness with monotonicity and consistency axioms. Abdulkadiroğlu and Sönmez (1998) showed that, given a fixed preference profile, each Pareto efficient outcome can be obtained by running a serial dictatorship.Footnote 3 Sönmez and Ünver (2010) studied neutrality and strategy-proofness, together with additional axioms, and allow agents to have property rights over some of the goods (see also Abdulkadiroğlu and Sönmez 1999 for this model). Pycia and Ünver (2021) showed that Arrovian efficient and strategy-proof mechanisms resemble sequential dictatorships except that in the last step of the algorithm, when there are only two goods left, two agents might be endowed with these goods and allowed to trade them; Pycia (2016) showed that a similar class of sequential-dictatorship-like mechanisms characterizes strong obvious strategy-proofness and Pareto efficiency. These papers focus on environments without outside options. The present paper contributes to the analysis of the voluntary participation in allocation of indivisible goods without transfers in the presence of outside options. The previous analyses of this issue focused on population monotonicity (Ehlers et al. 2002) and resource monotonicity (Ehlers and Klaus 2003); assumptions we do not impose. Following the initial draft of our work, others have examined outside options in related environments. Nanyang (2014) used neutrality and additional axioms to characterize sequential dictatorships. Erdil (2014) showed in a domain without transfers that non-wasteful and strategy-proof deterministic mechanisms are not dominated by strategy-proof deterministic mechanisms. In school-choice domain, Kesten and Kurino (2019) showed that with outside options there is no mechanism that Pareto-dominates the student-optimal stable school-choice mechanism. They also study maximal subdomains of preferences where such result no longer holds. In a more general setting with or without transfers, Alva and Manjunath (2019) showed that if a pair of individual rational and strategy-proof mechanisms are participation equivalent (i.e., if at every problem every agent either receives her outside option under both mechanisms or is assigned a non-outside-option outcome under both) then they should be welfare equivalent.Footnote 4
Calsamiglia et al. (2020) showed that the presence of outside options has an even bigger impact on individually-rational but non-strategy-proof mechanisms as it enables agents with better outside option to choose more risky equilibrium strategies. Some of the conceptual modeling and market design for kidney exchange hinges on the outside-option-like protections for patients and the incompatible donors they bring to the exchange: the donor brought by a patient can only be matched with other patients if the patient is matched with a compatible donor (e.g., Roth et al. 2005, 2007). The donor a patient brought to the exchange can thus be seen as the patient’s outside option.Footnote 5 While we show that Svensson’s serial dictatorship insight can be modified so as to make it valid when agents have outside options, there are many other standard mechanism design problems in which whether agents have the ability to take an outside option crucially affects the standard results. For instance, in the setting with monetary transfers and quasi-linear utilities, the impossibility of ex-post Pareto efficient and Bayesian incentive compatible bilateral trade shown by Myerson and Satterthwaite (1983) crucially depends on individual rationality. The Coasian dynamics of Gul et al. (1986) hinges on the inability of buyers to take an outside option, as shown by Board and Pycia (2014).",
26.0,4.0,Review of Economic Design,08 January 2022,https://link.springer.com/article/10.1007/s10058-021-00282-3,Computational implementation,December 2022,Mehmet Barlo,Nuh Aygün Dalkıran,,Male,Male,Unknown,Male,"Implementation theory deals with the problem of designing a mechanism such that the optimal alternatives prescribed by the designer coincide with the equilibrium outcomes of this mechanism. There has been a vast literature on implementation following (Maskin 1999, circulated since 1977), which is the first paper that identifies necessary as well as sufficient conditions for implementation when the equilibrium notion under consideration is Nash Equilibrium. Despite the vast literature on implementation and advances in computational tools, to the best of our knowledge, computational tools have not been employed in the implementation literature.Footnote 1 In this paper, we aim to fill the gap by providing computational tools for Nash implementation. The classical approach in Nash implementation (based on the seminal works Maskin 1999; Moore and Repullo 1990, and Dutta and Sen 1991) seeks to identify social choice correspondences (SCCs) defined on unrestricted domains of preferences that are attainable as Nash equilibrium outcomes of mechanisms.Footnote 2 To implement such an SCC, the planner no longer needs to acquire information about the true preference profile of the society as the mechanism the planner employs indirectly provides her with the relevant information: The SCC coincides with the set of Nash equilibrium outcomes of the mechanism at every realized state of the world. If  Nash implementation of an SCC is not achievable on unrestricted domains of preferences, it might still be possible to Nash implement this SCC on a restricted domain of preferences. In a nutshell, there are three essential components of Nash implementation: (i) a domain of feasible preferences; (ii) the optimal outcomes described by an SCC, i.e., the desired goal; (iii) a mechanism (game form) the Nash equilibria of which equal the optimal outcomes at every preference profile in the domain of feasible preferences. The main goal of the mechanism designer then can be thought of as identifying (iii) given (i) and (ii), i.e., identifying a mechanism that Nash implements a given SCC on the feasible domain of preferences. Our results in this paper are divided into two parts: In the first part, we ask what can be achieved in terms of Nash implementation by a given mechanism. As opposed to the standard approach, we identify (i) and (ii) given (iii). That is, we characterize the SCCs along with the domain of preferences they are defined on that are implementable in Nash equilibrium via a given mechanism. Our results, therefore, describe the scope of Nash implementation by a given mechanism in detail. In the second part of the paper, we turn back to the standard approach: By revisiting the standard necessity and sufficiency results, we provide computational tools that describe the scope of Nash implementation for a given SCC on a given domain of feasible preferences. In the first part of the paper, for a given mechanism, we establish that the set of attainable Nash equilibrium outcomes of this mechanism partitions the domain of preferences under which there is a Nash equilibrium of this mechanism (Theorem 1). This partition identifies the boundaries of Nash implementation under the mechanism at hand: Given a mechanism, the set of SCCs and the corresponding domain of preferences under which Nash implementation is viable are precisely those that respect the intrinsic relation described by the partition structure induced by this mechanism (Theorem 2). Our findings unfold the precise incompatibilities between the desired set of alternatives and the domain of preferences for any SCC that fails to be Nash implementable by a given mechanism. Identifying these incompatibilities empowers us to characterize the maximal domain of preferences where Nash implementation of a given SCC is attainable by the given mechanism. We demonstrate this using the Pareto efficient SCC: Given any mechanism, we delineate the maximal domain of preferences under which efficiency restricted to this domain is Nash implementable by this mechanism. In the second part of the paper, we first focus on the prominent Maskin-monotonicity condition as a necessary Nash implementation condition. Generalizing Maskin (1999)’s results on Nash implementation to behavioral domains, de Clippel (2014) defines the concept of a consistent collection of sets for a given SCC, the existence of which is equivalent to Maskin-monotonicity under rationality. The set of all consistent collections of sets of a given SCC on a given domain of preferences defines the boundaries of Nash implementation of this SCC. We exemplify that by identifying the set of all consistent collections of sets for an SCC, the designer might construct eligible mechanisms to implement the given SCC. Then, we turn to the well-known sufficiency conditions for Nash implementation and provide codes that check whether a given SCC satisfies the sufficiency conditions. In particular, when there are at least three individuals in the society, the existence of a consistent collection of sets is both necessary and sufficient for Nash implementation in an economic environment. We provide codes that check whether the domain of preferences of an SCC satisfies the economic environment assumption. Furthermore, when there are at least three individuals in the society, no-veto-power (NVP) property is sufficient for Nash implementation when the SCC under consideration has a consistent collection. We also provide codes that check for the NVP property of an SCC. The results in the second part of our paper can be generalized to behavioral domains as in de Clippel (2014). Instead of the domain of rational preferences, our results also accommodate the domain of individual choices that do not necessarily satisfy the weak axiom of revealed preferences (WARP). The organization of the rest of the paper is as follows. We present the preliminaries in Sect. 2. The first part of our results where we analyze the scope of Nash implementation by a given mechanism is in Sect. 3. The second part of our results where we provide computational tools for Nash implementation of a given SCC is in Sect. 4. Section 5 provides a brief literature review. Meanwhile, Sect. 6 concludes. Unless stated otherwise, the proofs are presented in the “Appendix”. Our Python codes are available online at http://dalkiran.bilkent.edu.tr/Python_Computational_Implementation.zip",2
26.0,4.0,Review of Economic Design,11 December 2021,https://link.springer.com/article/10.1007/s10058-021-00278-z,Socio-legal systems and implementation of the Nash solution in Debreu–Hurwicz equilibrium,December 2022,Claus-Jochen Haake,Walter Trockel,,Unknown,Male,Unknown,Male,"In the inaugural issue of [Review of] Economic Design in 1994, with Semih Koray among the associate editors, Murat Sertel as the editor-in-chief and William Thomson as the co-editor, the journal’s Honorary Editor Leonid Hurwicz contributed the first article titled “Economic Design, adjustment processes, mechanisms and institutions”. In this seminal and visionary work he explained concepts like mechanism design, institution, game form and equilibrium concepts (by Nash, Walras, Lindahl) in their historical contexts. Hurwicz stressed the fundamental importance of enforcement and procedures making institutional arrangements effective, which he termed genuine implementation. In this context he wrote (p. 12): “The problem of modeling enforcement poses some difficult problems. To begin with, there is a view that Nash equilibria are self-enforcing, by definition. What I believe is meant, is that if the genuine implementation apparatus is in place so that the rules of the game are obeyed and the outcome function is effective, then no player has an incentive unilaterally to defect. But, in general, there is nothing in a specific game form, prescribing particular strategy domains and outcome functions that would prevent players from resorting to ’illegal’ strategies, nor is there automatic assurance that outcomes specified by the outcome function will occur unless the required apparatus is in place.” Nor is it enough to expand the game form so as to prescribe the procedures governing the enforcement process, since similar objections can be raised with respect to this expanded game form. This is the so called infinite regress problem. In section III, “Are Nash equilibria self-enforcing?”, of his Nobel Lecture “But who will guard the guardians?”, Hurwicz (2008, p. 579) writes: “Now we come to a very important and closely related issue. We are asking whether a given Nash equilibrium in a specified game is enforceable or not. And one occasionally hears the claim that there can be no enforcement problem with Nash equilibria because allegedly Nash equilibria are self-enforcing. I want to stress that I am denying this claim, but I want to give the other side an opportunity to give their arguments. Their argument is that, by definition, in a Nash equilibrium, no player can profit by a unilateral departure from his or her equilibrium strategy. Furthermore, collusions are infeasible in a non-cooperative game. Hence, the argument goes, there is no need (or possibility, really) for enforcement.” And in Sect. 5, “Successful Enforcement and Implementation”, he states (p. 581): “And I conclude this section by saying that implementation is successful if the equilibrium outcomes correspond to those of the desired game, i.e., those envisaged by the legislation. Expressed in this framework, a Nash equilibrium is not self-enforcing because, while it is unprofitable to move to alternative legal strategies, it may be profitable, in the absence of enforcement, to move to illegal strategies. Similarly, Nash equilibria are not self-implementing because implementing actions are required to ensure that the true outcome is the same as the legal outcome.” Clearly, in case of multiple equilibria no single one of them could be expected to be self-enforcing. But, Hurwicz is contrasting deliberate participation in a game with enforced participation in a legally prescribed subgame. In a previous note (Trockel and Haake 2019) we had suggested a connection between Hurwicz’s dichotomy of legal vs. illegal and Debreu’s (1952) distinction between socially agreed choices and socially unaccepted actions. Inspired by those ideas of enforcement and a dichotomy of legal and illegal games within true games and based on Debreu’s (1952) concept of a social system and extensions due to Shafer and Sonnenschein (1975), Prakash and Sertel (1996) and of Koray and Yildiz (2018), we shall introduce socio-legal systems and their Debreu–Hurwicz equilibria in Sect. 2. We will then factorize such a system into a system form, representing the rules and the legal outcomes of the game, and the individual characteristics of the agents, namely their preferences and their mutual constraints on strategies by all agents resulting in (further) restrictions on feasible outcomes. In Sect. 3 we will comment on the relation between non-cooperative strategic foundation of axiomatic solutions of coalitional games and implementation of social choice rules as elaborated in Trockel (2002, 2003) [see also Howard (1992), Serrano (1997), Dagan and Serrano (1998), Bergin and Duggan (1999), Haake and Trockel (2010)]. In Sect. 4 we will illustrate our concept of a socio-legal system in a very special application to two-person bargaining games, departing from Nash’s simple demand game and arriving at an implementation of the Nash bargaining solution in Debreu–Hurwicz equilibrium.",1
26.0,4.0,Review of Economic Design,23 March 2022,https://link.springer.com/article/10.1007/s10058-022-00289-4,Ordinal Bayesian incentive compatibility in random assignment model,December 2022,Sulagna Dasgupta,Debasis Mishra,,Unknown,Unknown,Unknown,Unknown,,
26.0,4.0,Review of Economic Design,05 January 2022,https://link.springer.com/article/10.1007/s10058-021-00281-4,Mechanism design by observant and informed planners,December 2022,Shurojit Chatterji,Arunava Sen,,Unknown,Unknown,Unknown,Unknown,,
26.0,4.0,Review of Economic Design,22 November 2022,https://link.springer.com/article/10.1007/s10058-022-00323-5,Limit theorems for recursive delegation equilibria,December 2022,Semih Koray,Murat Sertel,,Male,Male,Unknown,Male,"This paper based on the pretend-but-perform notion was written more than thirty years ago. The pretend-but-perform idea was originally introduced by Sertel as the dual of incentive compatibility, which duality is perhaps best reflected by Mevlana Celaleddin-i Rumi, who said already in the 13th century: “Either appear as you are or be as you appear”. This prologue is meant to relate the paper to the relevant literature that came into being in the meantime without making any changes in the original version of the paper. Delegation has been considered from mainly two different standpoints in the literature. From the viewpoint of positive economics, it reflects an attempt to explain the rationale behind real-life deviations of firms from profit maximization, while the design approach treats delegation as a means of regulating an oligopoly to improve social welfare. A comparison between these two approaches is naturally contingent upon the equilibrium notion employed. Here we consider the Cournot equilibrium based on quantity competition in the context of a symmetric linear duopoly and show that delegation intensifies competition between firms by creating a further ground for competing for a larger market share. It is also shown in Koray and Sertel (1992) that this fact extends to a symmetric linear oligopoly of n firms, where the n firms behave under the pretend-but-perform mechanism precisely as \(n^2\) firms would do in the naked Cournot oligopoly. The main focus of this paper is to illustrate the inadequacy of delegation to explain firms’ deviations from profit maximization in a Cournot duopoly. This problem is, of course, essentially one of an empirical nature. Our consideration here, on the other hand, is based on some a priori theoretical reasons, which can be summarized as follows. The main result of the present paper is that each principal, i.e., each firm owner in the duopolistic context, has an incentive to redelegate, increasing the length of his delegation chain. Thus, if delegation is costless, then the principals are expected to increase the delegation chain length indefinitely. As delegation is costly in real life, the chain length they choose will be contingent upon delegation costs, which may or may not be equal to 1. The only reason why the managers would come to a Cournot equilibrium under the assigned maximands is that they are instructed to do so by their principals, as the managers are in fact not recipients of the maxima they are to achieve. Thus, it must be the case that the firm owners have some reason to delegate the untrue profit functions of their firms by also instructing their managers to behave as Cournot duopolists under those maximands. It is shown, however, in Koray and Sertel  (1989) that, given any pair of quantities \(q = (q_1, q_2)\) yielding nonnegative profits to both firms, there is an equilibrium pair of maximands of the owners’ delegation game inducing q as the Cournot-Nash equilibrium of the intermanagerial game. Thus, it is difficult to think of any reason why the owners should restrict the maximands they delegate to the form of a profit function, unless they are legally forced to do so. The delegation setup, on the other hand, is meaningful as a pretend-but-perform regulation under which an improvement of social welfare is achieved. In case the delegation chain is of finite length, the increase in social welfare stays under the optimal level. The optimal level is achieved in the limit, when the delegation chain length is increased indefinitely. The closest work in regulating a Cournot oligopoly is provided by Gradstein (1995)Footnote 1, which considers a standard Cournot oligopoly with n firms producing a homogeneous product. He constructs a mechanism to attain the social optimum in a Cournot oligopoly under the further restriction that each firm’s message space consists of choosing its output level, which also reduces the outcome function of the mechanism to the identity function. The informational assumptions are that the inverse demand function is known to the designer, but the firms’ cost functions are not, while all aspects of the environment are common knowledge among the firms. The standard regularity assumptions made about the inverse demand and the firms’ cost functions by Gradstein (1995) are such that they also suffice for the existence of a Cournot equilibrium. The regulator announces a balanced transfer vector as a function of the quantities chosen by the firms, which is enforceable as the output levels are observable. Gradstein (1995) shows that a balanced transfer vector inducing the social optimum exists if and only if the inverse demand function is a polynomial function of degree at most \(n- 1\), where n is the number of firms in the oligopoly. In our case the inverse demand function is assumed to be a polynomial function of degree 1, the highest degree possible for the characterization of Gradstein (1995) to apply to a duopoly. As for regulatory purposes, what our game cascades turn out to implement in the limit for a linear duopoly is achieved by him in one shot via balanced transfer vectors. In this paper, however, our main focus is to model delegation and analyze the nature of recursive delegation equilibria rather than implementation. Implementation enters the picture via the question of whether the “meta-Cournotic” equilibria arising under delegation can be considered as part of positive or regulatory theory. Ünver (1995) and Yıldırım  (1995) also follow up the pretend-but-perform idea in the context of a symmetric duopolistic differentiated goods market when delegation is costless and redelegation is possible. Ünver (1995) considers the case where quantity is the strategic variable and thus Cournot equilibria are reached by the managers on the industry floor, while Yıldırım  (1995) assumes that the managers on the industry floor compete in prices and end up with Bertrand equilibria. Their results parallel those obtained for a duopoly in which the two firms produce the same good, where redelegation drives the outcome towards the socially optimal one under quantity competition, while price competition creates forces that drive the equilibrium outcome closer to the collusive joint profit maximizing point when the delegation chain length is increased. Kraekel (2005) considers a two-stage game in a duopolistic context, where the maximand delegated by each owner to his manager is a linear combination of profits and sales, while the two managers compete in a duopolistic tournament against each other. Unlike a Cournot or Bertrand duopoly, it turns out that there exist asymmetric equilibria, at which one owner puts a positive weight on sales and the other a negative one.",
27.0,1.0,Review of Economic Design,27 January 2022,https://link.springer.com/article/10.1007/s10058-021-00279-y,A model of competitive signaling with rich message spaces,February 2023,Tomás Rodríguez Barraquer,Xu Tan,,Male,,Unknown,Mix,,
27.0,1.0,Review of Economic Design,11 February 2022,https://link.springer.com/article/10.1007/s10058-022-00288-5,A quantitative analysis of Turkish public school admission reform,February 2023,Muharrem Yeşilırmak,,,Male,Unknown,Unknown,Male,"Admission into public high schools in Turkey was based on the scores obtained in a nation-wide exam known as TEOG. In this system, student and school matching was positively assortative. In other words, highest achieving students were placed in the highest quality schools and lowest achieving students were placed in the lowest quality schools. As of Fall 2017, the admission rule is reformed so as to apply starting from Fall 2018. According to the new rule, all students would still take the exam but only top \(10\%\) of them will be admitted into the best public high schools based on scores whereas the remaining \(90\%\) will be placed into the remaining schools based on home addresses. This reform raised public concerns regarding equality of opportunity since quality of public schools in a neighborhood is positively related with the average income there. This paper quantitatively analyzes the effects of the reform on the variance of achievement (assumed to measure inequality of opportunity), mean achievement after high school, households’ welfare, and teachers’ welfare. We find that variance of achievement falls by \(17.76\%\), mean achievement falls by \(51.15\%\), households’ total welfare falls by \(0.115\%\), and teachers’ total welfare rises by \(18.1\%\). Why did the government change the admission rule? As also noted in Hürriyet  (2017) and Özen and Özenç  (2017), government officials believe that the old system put students into a horse race which led them to purchase private supplemental education and as a result inequality of opportunity occurred. Officials also stress that, in the old system, students may be placed into those schools which are far away from their homes. However, in such cases, government allowed students to transfer to a school closer to home as noted in Görmez and Coşkun  (2015). Moreover, students always had the option of enrolling in the neighborhood school by indicating the neighborhood school at the top of their preference lists submitted to the Ministry of Education. To analyze the reform, we set up an equilibrium political economy model of education in which there is a continuum of heterogeneous households and a continuum of heterogeneous public schools. Each household consists of one parent and one child. Households differ by exogenously set income and child ability whereas public schools differ by exogenously set teacher quality. Public schools’ expenditures are financed through income taxation. Income tax rate is determined through majority voting.Footnote 1 Each household determines consumption and private supplemental education expenditure by maximizing the utility function which depends on consumption and achievement. Achievement depends on ability, teacher effort, and educational spending. Students are assortatively matched with teachers (or schools) based on ability. Each teacher determines effort for each student in the classroom by maximizing the utility function. Teachers derive utility from consumption and mean classroom achievement and they derive disutility from effort. Teacher wage per unit of quality is determined endogenously at the national teacher labor market. We mathematically show existence of a unique majority voting equilibrium and characterize the pivotal voter. We calibrate the parameters of our model using Turkish data. We assume the joint distribution of income and ability and the distribution of teacher quality are both lognormal. In the calibration, we target mean and variance of the university entrance exam (called LYS) score distribution together with its percentiles and share of public high school spending out of GDP. We then test our model against those statistics (percentiles of LYS) not targeted in the calibration. After calibration, we run a computational experiment at which we exogenously change the matching rule between students and teachers. The new matching rule is such that most able \(10\%\) of students are matched assortatively with teachers based on ability and the remaining \(90\%\) of students are matched assortatively with the remaining teachers based on parental income. We assume that assortative matching based on income captures the fact that quality of public school in a neighborhood is positively related with the neighborhood mean income. We find that both variance of achievement and mean achievement falls after the reform. Intuitively, the rise in household income sorting across schools and the rise in the fraction of households purchasing private supplemental education pushes variance up whereas the fall in ability sorting across schools pulls variance down. Quantitatively, the latter force offsets the former two effects and variance falls as a result. Moreover, we find that mean achievement falls because of the fall in mean teacher effort and mean educational spending after the reform. We also find that, total households’ welfare falls after the reform. Intuitively, for any household, the fall in income tax rate imposes upward pressure on consumption and hence on utility level.Footnote 2 At the same time, mean achievement falls imposing downward pressure on the utility level. These forces eventually cause a fall in total households’ welfare. Regarding teachers’ total welfare, we find a rise after the reform. Intuitively, falling income tax rate causes a fall in teacher wages and this imposes downward pressure on the utility of any teacher together with falling mean achievement. At the same time, we find that mean teacher effort falls causing the cost of effort to fall. The fall in the cost of effort imposes upward pressure on teachers’ utilities. The net effect of these opposing forces quantitatively implies a rise in teachers’ total welfare. This paper has both theoretical and applied contributions to the existing literature. Theoretically, we extend the model used by Epple and Romano  (1996b) and Gouveia  (1997) by capturing not only income differences but also child ability differences across households. Moreover, in our model, achievement depends not only on educational spending but also on teacher effort and child ability different from the previous papers. On the applied side, to the best of our knowledge, this is the first paper that quantitatively analyzes Turkish admission reform. The reverse of Turkish reform took place in Sweden which is empirically studied by Söderström and Uusitalo  (2010) and Karbownik  (2020).Footnote 3 Another paper by Wang and Zhou  (2020) econometrically estimates the welfare effects of recent Chinese high school admission reform. Different from these studies, we study an equilibrium model which permits us to predict long-run changes in student achievement, per pupil public spending, welfare of households and teachers. The effects of teachers on education production is studied in a dynamic equilibrium political economy framework by Tamura  (2001), Viaene and Zilcha  (2009), and Hatsor  (2014). Different from these papers, our static model is silent in terms of economic growth and income inequality dynamics. These papers assume teachers and students are randomly matched and there is a unique public school quality available for a household. In our paper, the matching between students and teachers is assortative and a continuum of public school qualities are available. This difference causes the preferred tax rate of a household, in our model, to depend on its income, child’s ability, and the quality of the teacher (or public school) that the child is matched. More specifically, in Tamura  (2001), the preferred tax rate of the representative household in a district depends only on the relative district income. In Viaene and Zilcha  (2009) and Hatsor  (2014), the preferred tax rate depends only on income of the household and the economy-wide mean teacher quality. Moreover, in our paper, the utility of a teacher is modeled explicitly giving the opportunity to predict the changes in teachers’ utilities after the policy reform. There is a literature on school choice following the paper by Abdulkadiroğlu and Sönmez  (2003). In this line of research, several matching rules between students and schools are studied from a mechanism design approach. Compared to this literature, the type of matching rules we employ in this paper are very simple. Among other differences, most importantly, we have a political economy framework (namely, majority voting) in our paper different from this literature which allows us to endogenously determine the public spending per pupil. Therefore, our paper could be thought of as an attempt in unifying the literature following Epple and Romano  (1996b) and Gouveia  (1997) with the literature following Abdulkadiroğlu and Sönmez  (2003). Our paper is organized as follows. Section 2 explains the model. Section 3 studies theoretical properties of the model. We calibrate the parameters of the model in Sect. 4 so as to match certain statistics from 2016 Turkish data. Section 5 analyzes the effects of the Turkish admission reform through a computational experiment. Section 6 concludes.",
27.0,1.0,Review of Economic Design,09 February 2022,https://link.springer.com/article/10.1007/s10058-022-00291-w,Optimal interregional redistribution and local budget rules with multidimensional heterogeneity,February 2023,Darong Dai,Guoqiang Tian,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Review of Economic Design,17 November 2021,https://link.springer.com/article/10.1007/s10058-021-00272-5,Negative voting social welfare functions: a characterization,February 2023,Jac C. Heckelman,,,Male,Unknown,Unknown,Male,"One way to establish a social preference relation over a finite set of alternatives is to apply some sort of preference aggregation rule. For example, scoring rules aggregate points for each alternative based on voter rankings. Negative voting represents a simple scoring rule which utilizes information only on a voter’s least favored alternative.Footnote 1 A Social Decision Function (SDF) is a rule which identifies a non-empty set of winning alternatives whereas a Social Welfare Function (SWF) is a rule which generates a complete ordering (i.e. transitive ranking) of the alternatives. In particular, scoring SDFs select the alternative(s) with the most points and scoring SWFs rank the alternatives in descending order of their total points. Young (1974) and Smith (1973) prove simple scoring functions are the only class of SWFs to satisfy anonymity, neutrality, reinforcement, and continuity.Footnote 2 Young (1975) then slightly modified these properties to present a similar axiomatization of scoring SDFs. There has been a recent surge in characterizations of negative voting as a SDF. Building from Young’s (1975) classic characterization of scoring SDFs, Baharad and Nitzan (2005) prove negative voting is the only SDF to satisfy anonymity, neutrality, reinforcement, continuity, and minimal veto. Other characterizations include Bossert and Suzumura (2016) for anonymity, neutrality, reinforcement, individual-equality independence, single-agent monotonicity, and single-agent expansion, and Kurihara (2018) who utilized anonymity, neutrality, reinforcement, bottoms-only and averseness.Footnote 3 We supplement these axiomatic treatments by characterizing negative voting as a SWF rather than a SDF. When a majority of voters most prefer the same alternative x, a SWF satisfying the majority winner support property will always rank x alone at the top of the social ordering. We introduce the reciprocal property of majority loser opposition which requires that whenever there is an alternative ranked last by a majority it must uniquely be ranked last by the SWF. We prove negative voting is the only SWF that satisfies anonymity, neutrality, reinforcement, continuity, and majority loser opposition. As a corollary we provide an impossibility result, in that no SWF will satisfy anonymity, neutrality, reinforcement, continuity, majority loser opposition, and majority winner support.",
27.0,1.0,Review of Economic Design,09 November 2021,https://link.springer.com/article/10.1007/s10058-021-00271-6,A unified approach to strategy-proofness of the deferred-acceptance rule and the top-trading cycles rule,February 2023,Hidekazu Anno,Sui Takahashi,,Male,,Unknown,Mix,,
27.0,1.0,Review of Economic Design,27 November 2021,https://link.springer.com/article/10.1007/s10058-021-00276-1,Asymmetric price adjustment and price discovery in spot and futures markets of agricultural commodities,February 2023,Zhuo Chen,Bo Yan,Liyu Liu,,Male,Unknown,Mix,,
27.0,1.0,Review of Economic Design,04 February 2022,https://link.springer.com/article/10.1007/s10058-021-00286-z,Allocating \(\hbox {CO}_2\) emissions: a dynamic claims problem,February 2023,Eun Jeong Heo,Jinhyuk Lee,,,Unknown,Unknown,Mix,,
27.0,1.0,Review of Economic Design,06 January 2022,https://link.springer.com/article/10.1007/s10058-021-00283-2,Aiding applicants: leveling the playing field within the immediate acceptance mechanism,February 2023,Christian Basteck,Marco Mantovani,,Male,Male,Unknown,Male,"A desire for equity is among the central motivations behind many school choice programs, which extend access to good schools to otherwise ineligible students. Equity may however be compromised where students have to apply through a mechanism that is not strategy-proof so that optimal application strategies are hard to identify. Better-informed and strategically sophisticated applicants may then be at an advantage. The argument that less sophisticated applicants are disadvantaged under the widely used but manipulable Immediate Acceptance mechanism (henceforth \(I\!A\), also known as Boston Mechanism) has been at the core of many recent reforms that replaced it with strategy-proof mechanisms, in particular the deferred acceptance mechanism. For example, in 2005 Boston abandoned its old immediate acceptance mechanism after observing that “the need to strategize provides an advantage to families who have the time, resources and knowledge to conduct the necessary research” (Pathak and Sönmez 2008).Footnote 1 On the other hand, under the realistic assumption that applicants’ preferences are correlated and schools’ priorities are coarse, immediate acceptance may in equilibrium improve upon deferred acceptance according to various ex-ante efficiency and welfare criteria (Miralles 2009; Abdulkadiroğlu et al. 2011; Troyan 2012).Footnote 2 Hence, when this is likely to be the case, it would seem preferable not to abandon \(I\!A\), but to protect unsophisticated applicants within the mechanism by making it easier for them to identify optimal strategies.Footnote 3 Intuitively, \(I\!A\) is able to generate welfare gains as advantageous misrepresentation of ordinal preferences reveals information on preference intensitiesFootnote 4—only students with a sufficiently high valuation would be willing to apply at highly oversubscribed schools, while students for which a less popular school is almost as good would instead apply at the latter. To help applicants identify schools that are likely to be oversubscribed, a school council might decide to disclose information on the number of applicants at various schools in previous years. Assuming that the distribution of applicants’ preferences over the years is sufficiently stable, these figures should be informative of the expected number of applicants in the current year and hence help applicants to identify optimal strategies and settle on an equilibrium. If successful this renders everyone a best responder, thus eliminating the gap between formerly ‘sophisticated’ and ‘unsophisticated’ applicants, and in addition allows to reap the improvements in ex-ante welfare that \(I\!A\) can in theory provide. In Basteck and Mantovani (2018) we report experimental evidence confirming that—absent detailed information on previous applications—\(I\!A\) creates a gap between subjects of different cognitive ability. Subjects of higher ability fare better than their peers of lower ability: because they are less able to identify optimal strategies in \(I\!A\), the latter earn significantly less and are over-represented at the worst school, resulting in ability segregation across schools.Footnote 5 Nevertheless, \(I\!A\) is able to generate significant welfare improvements over deferred acceptance, both in equilibrium and in the data, as sufficiently many subjects are able to identify optimal strategies. Here we test experimentally whether providing information on previous applications can increase the welfare of subjects of lower cognitive ability within \(I\!A\), reduce the gap between subjects of higher and lower ability and help to avoid ability segregation across schools.Footnote 6 Since the failure to anticipate others’ application behavior is one likely source of strategic mistakes, enhanced information may benefit subjects of low ability. Yet information on others’ strategies needs to be complemented with an understanding of their consequences for acceptance probabilities at schools. Since subjects of higher cognitive ability may be better able to make use of the provided information, information provision may end up widening the gap between low and high ability subjects and could further disadvantage the former. In our experiment, we first measure participants’ cognitive ability by means of a Raven test before letting them play several school choice games under \(I\!A\). For the information treatment, participants are informed of the number of applicants that listed each school first in a previous game where the distribution of preferences was identical. We compare their choices and outcomes to those obtained in a control treatment where information about past strategies is not provided. We use two preference profiles designed to bring out two intuitive strategic manipulations—in the words of the West Zone Parents Group:Footnote 7 One school choice strategy is to find a school you like that is undersubscribed and put it as a top choice, OR, find a school that you like that is popular and put it as a first choice and find a school that is less popular for a “safe” second choice. In equilibrium under our first preference profile, applicants predominantly choose the latter manipulation (Skip-the-Middle), as their second-most preferred school will be oversubscribed and ranking it second would loose them the chance to be admitted in the second round. In our second preference profile, a majority of students should instead choose the former manipulation (Skip-the-Top), as their most preferred school is heavily oversubscribed in equilibrium. In both cases, knowing the demands of the past period may allow students to form a more precise prediction and use a more appropriate strategy in the current period. We find that on aggregate, subjects choose optimal strategies significantly more often when information is provided. While subjects of high cognitive ability are more likely to choose a best reply than their low ability peers in the absence of information, this gap between the two groups is reduced significantly in the information treatment. Moreover, as a result, ability segregation is significantly lower in the information treatment for both preference environments. Despite this, subjects of lower cognitive ability earn significantly less than their peers both with and without information and the gap in payoffs does not decrease significantly. The evidence suggests the persistence of this gap, though partly specific to the payoffs used in the experiment, is connected to a qualitative difference in the use of the additional information by subjects of different ability. Perhaps more worryingly, in one preference environment information leaves subjects in the lower tail of the distribution of cognitive abilities further behind, so that the gap in payoffs between the top and bottom 20% increases with information. Other school choice experiments have varied the information provided to subjects. Pais and Pintér (2008) vary information about others’ preferences exogenously and find that truthtelling in \(I\!A\) increases as applicants have less information available.Footnote 8
Chen and He (2021) show that providing information on others’ preferences, and hence indirectly on admission chances, reduces wasteful investment in information acquisition under \(I\!A\). Providing historical cutoff scores can likewise increase welfare where information costs are high (Hakimov et al. 2021). Another way to provide information on admission chances used in practice, consists in publishing the current number of applications at different schools allowing students to revise their own applications over the course of the application period. Such data may be provided at the discretion of individual schools as, for example, in Amsterdam (De Haan et al. 2015) or Berlin (Basteck et al. 2015) or systematically, e.g., by the Wake County Public School System in North Carolina (Dur et al. 2018). Here, using field data, Dur et al. (2018) are able to classify students as ‘sincere’ or ‘sophisticated’ based on whether they access this information repeatedly over the course of the application period and show that, under \(I\!A\), sophisticated students tend to avoid over-demanded schools and are hence more likely to receive an assignment. A potential downside of making other players’ strategies observable is reported by Guillen and Hakimov (2017) who show that revealing the use of sub-optimal strategies by opponents may lead subjects to choose dominated strategies more often. In contrast, Guillen and Hakimov (2018) demonstrate for the same mechanism that direct advice on optimal strategies can increase their use.Footnote 9 Our interest in \(I\!A\) is motivated in particular by its potential welfare improvements over deferred acceptance, which relates our work to papers that perform a welfare comparison between the two. Featherstone and Niederle (2016) consider an incomplete information environment where truth-telling is an equilibrium under \(I\!A\) and find a majority of subjects to report truthfully. As a result \(I\!A\) ’s natural advantage in satisfying stated preferences also yields higher welfare with respect to true preferences. In contrast, Chen and Sönmez (2006) find deferred acceptance to yield higher average expected payoffs in a designed preference profile and no difference in a random profile. In Chen and Kesten (2019) welfare under \(I\!A\) may be higher or lower than under deferred acceptance, depending on the environment.Footnote 10
Basteck and Mantovani (2018) compare \(I\!A\) and deferred acceptance using the same preference profiles as in this paper. Here, manipulations under \(I\!A\) should in theory reveal additional information and improve welfare. The effect is borne out in the results. Finally, recent field work using data from Barcelona (Calsamiglia et al. 2020) and Beijing (He 2015) also speaks in favor of \(I\!A\) in terms of aggregate welfare. Both papers estimate applicants’ preferences given observed applications under \(I\!A\) and compare it to the deferred acceptance outcome under the assumption of truthtelling. In contrast, De Haan et al. (2015) find deferred acceptance to yield higher aggregate welfare, using application data from Amsterdam where \(I\!A\) is used. Eliciting preferences in a survey, they compute the counterfactual outcome under deferred acceptance under the assumption of truthtelling. However, their survey also documents that many applicants hold incorrect beliefs regarding the availability of schools; for example many respondents consider schools to be oversubscribed that are in fact undersubsribed and where consistently undersubscribed in recent years. Kapor et al. (2020) also report evidence that applicants beliefs about others’ strategies are often incorrect, and show this causes welfare losses. Hence, their findings also suggest that information provision on past applications should increase welfare under \(I\!A\). The paper is organized as follows. Section 2 introduces the school choice environments, experimental design and procedures. Section 3 describes our hypotheses. Results follow in Sect. 4. Section 5 concludes.",1
27.0,1.0,Review of Economic Design,11 November 2021,https://link.springer.com/article/10.1007/s10058-021-00273-4,Prize sharing rules in collective contests: when do social norms matter?,February 2023,Dhritiman Gupta,,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Review of Economic Design,30 November 2021,https://link.springer.com/article/10.1007/s10058-021-00277-0,Correction to: Prize sharing rules in collective contests: when do social norms matter?,February 2023,Dhritiman Gupta,,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Review of Economic Design,03 January 2022,https://link.springer.com/article/10.1007/s10058-021-00274-3,An analysis of managerial delegation in a market with vertically-integrated producer owning an essential input monopolistically,February 2023,Chung-Hui Chou,,,Unknown,Unknown,Unknown,Unknown,,
27.0,1.0,Review of Economic Design,20 January 2022,https://link.springer.com/article/10.1007/s10058-022-00287-6,Correction to: An analysis of managerial delegation in a market with vertically-integrated producer owning an essential input monopolistically,February 2023,Chung-Hui Chou,,,Unknown,Unknown,Unknown,Unknown,,
27.0,2.0,Review of Economic Design,12 July 2022,https://link.springer.com/article/10.1007/s10058-022-00304-8,Robust implementation in sequential information design under supermodular payoffs and objective,June 2023,Hiroto Sato,,,Male,Unknown,Unknown,Male,"Information design explores how the designer can implement an outcome through manipulation of players’ beliefs about the state and their higher-order beliefs, which influences their behavior.Footnote 1 This paper studies a more general framework of sequential information design (introduced by Doval and Ely (2020)Footnote 2) in which the designer can construct the extensive form along with the information structure. In other words, the designer can release information not only about the state of the world, but also about past players’ moves. In static information design, several notions of implementation are studied such as partial implementation, smallest equilibrium implementation, and full implementation. Partial implementation requires that the target outcome is induced by some preferred Bayes Nash equilibrium (BNE) under the information structure. Bergemann and Morris (2016) and Taneva (2019) characterize the partially implementable outcomes by using the concept of obedience condition. Recently, a growing body of literature focuses on ""robust"" implementation against adversarial equilibrium selection. In supermodular game, Mathevet et al. (2020); Hoshino (2021); Sandmann (2020), and Morris et al. (2020) characterize the smallest equilibrium implementable outcomes. Under the monotone designer’s payoff, the smallest equilibrium is equal to the worst equilibrium under adversarial selection.Footnote 3 In addition, they analyze the more demanding notion, full implementation, which requires the target outcome to be induced by all equilibria. Since the outcome is uniquely implemented, full implementation is also robust against adversarial equilibrium selection. To the best of my knowledge, this is the first study on ""robust-approach"" for implementation in sequential information design. It shows that the optimal partially implementable outcome is fully implementable in sequential information design in binary action supermodular games with a monotone and supermodular designer’s objective function under dominant state assumption and outside option. This result indicates that the designer can uniquely, and hence robustly, implement the desired outcome without sacrificing payoff by using sequential information design. Notice that, it is not generally possible in static information design as in Morris et al. (2020). Moreover, in this setting, this optimal outcome is approximately equivalent to the optimal partially implementable outcome in static information design. Thus, even though the designer has no gain under partial implementation, under full implementation she does better when sequential information design is feasible instead of only static information design. These results provide new insight into the benefit of applying sequential information design in line with Doval and Ely (2020). They characterize the set of partially implementable outcomes in sequential information design, which is a superset of the partially implementable outcomes in static information design. In contrast, this paper shows how the designer can fully implement her optimal outcome using sequential information design. The construction behind the main result manipulates strategic uncertainty using the extensive form of the game. Additionally, the main results have economic applications in the global game literature (Morris and Shin 2003), which includes many settings such as currency attacks (Morris and Shin 1998), bank runs (Goldstein and Pauzner 2005), debt pricing (Morris and Shin 2004), investment game (Carlsson and van Damme 1993), team work production (Moriya and Yamashita 2020), and policy changes (De Mesquita 2010; Edmond 2013; Chen and Suen 2017), among others. In these settings, the designer often wants to maximize the probability of players taking a particular action, e.g. not attacking the currency, not withdrawing their deposits, investing in a project, and exerting effort. The main result of this paper proposes a way to achieve this goal robustly by providing the extensive form and the information structure that fully implements the optimal outcome distribution under partial implementation. To illustrate the ideas in this paper, I use an example of the investment game similar to Mathevet et al. (2020) and Morris et al. (2020). There are two players 1 and 2, where each player chooses action 1 and 0. Moreover, there are two states \(\theta _{1}\) and \(\theta _{0}\) which occur with the same probability. The payoffs are summarized in Fig. 1 where player 1 (resp. 2) is the row (resp. column): Base game In this base game, action 1 (resp. 0) is dominant at state \(\theta _{1}\) (resp. \(\theta _{0}\)) for each player. Under the prior, action 0 is strictly dominant for each player. Payoffs are supermodular: a player’s payoff to action 1 is 1 larger if the other player takes action 1 at each state. Additionally, payoffs are asymmetric: player 1 obtains a payoff 1 larger than player 2 from action 1 at state \(\theta _{1}\). In these setting, the designer wants to maximize the expected number of action 1 taken by players regardless of state. In static information design (where the designer can only choose the information structure), by the characterization in Bergemann and Morris (2016), the optimal equilibrium distribution over actions and states (call outcome) under partial implementation is shown in Fig. 2: Optimal partially implementable outcome in static and sequential information design However, by the characterization of Morris et al. (2020), the above outcome is not fully implementable, and thus the designer faces a trade-off between the robustness of implementation and the payoff in static information design. The optimal fully implementable outcomeFootnote 4 in static information design is essentially equivalent to the outcome in Fig. 3: Optimal fully implementable outcome in static information design To implement this, Morris et al. (2020) construct the following information structure: Construction of the information structure by Morris et al. (2020) where \(\varepsilon \), \(\delta \), and \(\eta \) are sufficiently small positive numbers such that \(\varepsilon \gg \eta \), and \(t_{1},t_{2}\in \{1,2,\ldots \}\) are players’ types. An intuition behind this information structure is as follows: first, type \(t_{i} = 1\) believes that the true state is \(\theta _{1}\) with probability close to 1 as \(\eta \approx 0\), and chooses action 1 since it is dominant at state \(\theta _{1}\). Then, given type \(t_{i}=1\) playing action 1, the optimal action of type \(t_{j}{:}{=}t_{3-i}=2\) is action 1 regardless of the actions by other types \(t_{i}\ge 3\).Footnote 5 Additionally, given type \(t_{j}=2\) playing action 1, the optimal action of type \(t_{i}=3\) is action 1 regardless of the actions by other types \(t_{j}\ge 4\). By this logic, both players of types \(t_{i},t_{j}<\infty \) choose action 1 in the unique equilibrium. In sequential information design (where the designer can construct the extensive form and the information structure), the outcome in Fig. 2 is also the optimal partially implementable outcome as shown by Lemma 1 in Sect. 2.3. Thus, the designer has no gain from sequential information design under partial implementation in this example. Note that this outcome can be induced by the extensive form representing the incomplete information game associated with the optimal information structure in static information design. Since there are no off-path information sets in this extensive form, this outcome is also induced by a self-contained BNE. Thus, there is no loss from restricting what the designer could achieve off the equilibrium path.Footnote 6 My main results will establish that the designer can approximately induce this outcome as full implementation in sequential information design by using the extensive form as shown in Fig. 5 with \(\varepsilon>\delta >0\). Thus, in contrast to static information design, the designer can fully implement the optimal partially implementable outcome in sequential information design. Even though the designer has no gain under partial implementation, under full implementation she does better when sequential information design is feasible instead of only static information design. Construction of the extensive form In Fig. 5, dashed lines represent players’ information sets, and blue arrows represent players’ behavior at the equilibrium. Furthermore, the designer’s messages and players’ actions are labeled with associated branches and information sets where the conditional probability is in square brackets. The intuition behind the construction of this extensive form is a finite variant of Rubinstein’s (1989) contagion through players’ information sets: (i) When player 1 receives message seed, he knows that the true state is \(\theta _{1}\), and hence optimally chooses action 1. This corresponds to the crazy type in Rubinstein (1989) contagion argument. (ii) When player 2 receives message 1, her optimal choice is action 1 regardless of player 1’s move in the remaining information sets. This is because if player 1 who receives message 1 chooses action 1, then the designer tells player 2 that player 1 does so with probability 1 and the true state is \(\theta _{1}\) with (conditional) probability \(\frac{2}{3-4\varepsilon }>\frac{2}{3}\).Footnote 7 Otherwise, the designer tells player 2 that the true state is \(\theta _{1}\) with (conditional) probability 1. Thus, in either case, player 2’s optimal choice is action 1 when she receives message 1. (iii) When player 1 receives message 1, he believes that the true state is \(\theta _{1}\) with probability \(\frac{10-20\delta }{17-20\delta -40\varepsilon }>\frac{10}{17}\) and action 1 is chosen by player 2 who receives message 1. Then, player 1’s optimal choice is action 1 regardless of the action of player 2 who receives message 0.Footnote 8 Note that, to obfuscate information about player 2’s move for player 1, the designer makes the more persuadable player (player 1) move before the less persuadable player (player 2) in this extensive form. (iv) When each player receives message 0, they know the true state is \(\theta _{0}\), and hence optimally choose action 0. Thus, the iterated elimination of strictly dominated strategies leads to the unique (self-contained) BNE.
 The above intuition highlights the reason why the designer can fully implement this outcome in sequential information design even though it is not fully implementable in static information design. Precisely, this comes from the difference of the constructions between this extensive form in Fig. 5 and the information structure of Morris et al. (2020) in Fig. 4. In their construction of the information structure (Fig. 4), the designer should provide probability such that, given player 1’s type \(t_{1}=1\) playing action 1, player 2 with type \(t_{2}= 2 \) has incentive of obeying to choose action 1 regardless of action by player 1’s type \(t_{1}=3\). However, in the extensive form (Fig. 5), player 2 receives message 1 (corresponding to \(t_{2}=2\)) if player 1 with message 1 (corresponding to \(t_{1}=3\)) obeys, and receives message 0 otherwise. Therefore, this variant of contagion in the extensive form involves the true sequentiality of the recommendations. A recommendation depends on information about the players’ past moves in addition to payoff-relevant states. Consequently, the designer can fully implement this outcome in sequential information design even though it is not fully implementable in static information design. Moreover, in contrast to the construction behind Morris et al. (2020) with infinitely many types, the extensive form consists of action recommendations for all players except for the first player, which is similar to the usual information design framework. Additionally, despite the asymmetry of payoffs, the optimal fully implementable outcome does not necessarily satisfy the perfect coordination property which is often discussed in the literature of robust implementation in static information design such as Inostroza and Pavan (2022) and Morris et al. (2020). The rest of this paper is organized as follows. A model and the main results are provided in Sect. 2. In Sect. 3, I discuss the assumptions behind the main results. Finally, the concluding remarks are summarized in Sect. 4. The proof of Lemma 1 and Theorem 1 is presented in Appendix A.",
27.0,2.0,Review of Economic Design,18 February 2022,https://link.springer.com/article/10.1007/s10058-022-00292-9,A simple matching domain with indifferences and a master list,June 2023,Rohan Chowdhury,,,Male,Unknown,Unknown,Male,"Consider a two-sided one to one market setting, where agents on one side of the market (say “Projects”) are grouped into indifference classes that are exogenously ranked, and agents on the other side (say “Firms”) prefer higher ranked projects. However, each firm face internal feasibility constraints that prevent her from matching with projects ranked above an individual threshold. Examples include—(1) the refugee matching problem: where benevolent hosts with a limited number of beds to spare, are willing to shelter refugee families in their homes in exchange for a monetary reward (which is increasing in family size) from the government; (2) firms competing for large-scale governmental projects over which there is generally a consensus as to which ones are more desirable, but firm-specific capacity constraints (technological, deadline, budget, etc.) prevent it from going after very top projects; (3) the slot allocation problem: where slots are arranged on a line with earlier slots ranked higher, users arrive over time and wish to be served as early as possible once they do, for instance, researchers wishing to use a supercomputer, ships wanting to load/unload cargo at a port, etc. Being indifferent between different options is a ubiquitous phenomenon in our society, and the above examples are no exception. These observations collectively inform our assumptions on the preference domain. We assume that projects are exogenously ranked (but several projects can have the same rank, thus giving rise to indifference classes), and each firm f faces a threshold \(t_f\). This firm f prefers projects of higher rank, but projects of rank above \(t_f\) are not acceptable to her. Preferences of firms thereby have a nested structure (formally presented in Sect. 2). Each project j can have arbitrary (non-strict) preferences over firms but prefers any firm to remaining unmatched. Taking preferences of both sides into account, firms must be matched to projects in a one-to-one fashion. We pursue two important design goals that are standard in the literature. The first is stability, which ensures that agents would accept the matching proposed by the designer and would not try to individually renegotiate it. To define it, we rely on the notion of strong blocking: two agents form a pair and block a matching if each agent in the pair strictly prefers the other over their current match.Footnote 1 The second is efficiency, i.e., the matching cannot be Pareto improved upon at any preference profile. Having met these two goals, in the above examples, the designer may additionally wish to improve welfare by maximizing as much as possible the size (i.e., the total number of matched pairs) of the proposed matching. In our domain with indifferences, all stable matchings have the same size (Proposition 1). To the best of our knowledge, it is the first non-trivial non-strict preference domain in the literature where this fact holds. This implies that, in the space of stable and efficient matchings, the designer is not constrained by welfare considerations regarding the matching size. The absence of this constraint has an important practical implication. This is because, finding a stable matching that maximizes size is a computationally hard problem in domains with indifferences, even under severe restrictions on preferences.Footnote 2 We propose two assignment rules, both of which always generate a stable and efficient matching for any given problem. The rules are in the spirit of the classical serial dictatorship rule, where an ordering of projects (in the queue) is used to determine the allocation of firms: the first project in the queue picks his best firm and leaves with that firm, from what remains the second project in the queue picks, from what remains the third project in the queue picks and so on.Footnote 3 The first rule, which we call the decreasing refined priority rule, adds two additional tweaks to this procedure. Firstly, the priority (queue) order over projects used by the rule is always aligned with the exogenous ranking over the indifference classes of projects: the projects in the top indifference class appear first in the queue order, followed next by projects in the second-from-top indifference class, and so on (see, Definition 2). This ensures stability. Secondly, the rule starts with the set of matchings instead of the set of firms. Following the queue order, at every step, the rule eliminates allFootnote 4 matchings where the project being assigned does not get his best firm (one of his best firms if there are many), from the set of matchings available at that step. This refinement ensures that the resulting matching is efficient, in the presence of indifferences. This is elaborated further in Example 1 and the discussion thereafter. Stability and efficiency do not however characterize the decreasing refined priority rule (Proposition 4). Nevertheless, we show that every matching that is both stable and efficient can be generated by a decreasing (non-refined) priority rule. Therefore, in our proposed domain, stable and efficient matchings are arguably hierarchical by nature: they can be thought of as an outcome of projects being arranged in a queue order, with each project receiving one of their best firms in turn from the set of remaining firms (Proposition 6). Motivated by the above observation, we propose another class of rules that we call the Pareto improved decreasing priority rule. Stability and efficiency completely characterize this class (Proposition 8). The rule works in two steps. In the first step, a matching is constructed using a queue order over projects, that is aligned with the exogenous ranking over projects (just as discussed above, thereby ensuring stability). Then in the second step, it tests for the presence of Pareto improvement (PI) cycles, a notion introduced by Erdil and Ergin (2017). The absence of PI-cycles in a stable matching guarantees that it is efficient: in our domain, a stable matching can be Pareto dominated if and only if projects can form a trading cycle, where every project and firm involved gets weakly better off, with at least one of them getting strictly better (Proposition 7). We proceed to discuss the strengthening of the stability notion to strong stability. We show that strongly stable matchings (those where there are not even weak blocking pairs where only one member of the pair strictly benefits) exist rarely, and imply a very specific structure on both preferences and the strongly stable matchings themselves. If they do exist, then at that corresponding profile, the set of all strongly stable matchings is exactly equal to the set of all matchings that can be generated by some decreasing refined priority rule (Proposition 9). In the following sub-section, we discuss the connection of this work with the existing literature. In Sect. 2 we present the formal model and the result that stable matchings have the same size. Section 3 focuses on stable and efficient matchings and presents the formal definitions for candidate assignment rules to find them. Section 4 discusses the strengthening of stability notion to strong stability. A concluding discussion follows. Some proofs are relegated to the “Appendix”. It is well known that, in a bilateral matching problem, when every agent has a strict (but arbitrary) preference over agents on the other side, the size of all stable matchings are identical, a consequence of the Lone Wolf TheoremFootnote 5 (McVitie and Wilson 1970; Gale and Sotomayor 1985). This result breaks down if we allow for general preferences (Roth and Sotomayor 1990). However, in our domain with indifferences, the equivalence of stable matchings with respect to size remains true. This paper contributes to the literature on matching with indifferences. In particular, it is closely related to the Stable Marriage problem with Ties and Incomplete Lists with a Master List (Irving et al. 2008). The term “Ties” simply means indifferences in our context; “Incomplete Lists” refer to the fact that each agent’s preference list may consist of only a subset of the members of the other side (the acceptable partners of this agent); and, an agent’s preference list contains her acceptable partners ranked precisely according to the “Master List” . In the above problem, Irving et al. note that even when preferences of both sides are derived from a master list,Footnote 6 weakly stable matchings need not have the same size, and subsequently discusses the algorithmics of finding weakly stable matchings that maximize size. In our model, where preferences of only one side are derived from a master list, while preferences of the other side are not incomplete;Footnote 7 weakly stable matchings necessarily have the same size. This result is therefore of practical significance since, in the presence of incomplete lists and ties (indifferences), finding a weakly stable matching that maximizes size is a hard problem; even under restrictions on the number and length of ties (Manlove et al. 2002), or preferences of both sides being limited to a master list (Irving et al. 2008).Footnote 8 On structural results, Manlove (2002) proves the lattice structure of the set of strongly stable matchings with ties (without allowing for incomplete lists) and notes the absence of such a structure for the set of weakly stable matchings. Our structural results, therefore, do not concern the set of weakly stable matchings. We argue instead that, every matching that is both (weakly) stable and efficient admits a hierarchical structure in a precise sense. Erdil and Ergin (2017) study a very general two-sided many-to-one matching domain with indifferences. Apart from PI-cycles that we use in this paper, the notion of PI-chains plays an important role in the algorithm they propose. The presence of PI-chains in a stable matching indicates the possibility for welfare improvement by accommodating a previously unmatched project and reshuffling existing matches, eventually leading to an increased matching size. By the size-equivalence result, such a scenario does not arise in our domain. In our more restricted setting, we are additionally able to pin down the structure of stable and efficient matchings as well as strongly stable matchings. In fact, we fully characterize the set of all stable and efficient matchings, and the set of all strongly stable matchings (whenever they exist). Other matching applications with indifferences include: the school choice problem with weak priorities (Erdil and Ergin 2008; Ehlers and Erdil 2010);Footnote 9 the housing market model where agents are initially endowed with a house (Aziz and De Keijzer 2012); kidney exchange models (Roth et al. 2005; Andersson and Kratz 2020). Unlike in our model, in these studies, welfare of only one side of the market is relevant, i.e., the efficiency notion is one-sided. Our mechanism design problem involves matching agents to each other without the possibility of resorting to money or lotteries. As a consequence, most fairness requirements are immediately out of reach. Any matching will therefore favour some agents and leave others largely unsatisfied. In such settings, it is widely observed that resulting mechanisms tend to have a hierarchical structure. Formally proving, or even formalizing this statement is however elusive. Under strict preferences some results are obtained: see, for instance, pure assignment models without transfers and exchange models in Svensson (1999), Pápai (2000), and Pycia and Ünver (2017). Under non-strict preferences, we are only aware of Svensson (1994) and Bogomolnaia et al. (2005). Most of these papers attempt to characterize the set of incentive-compatible (in a strong sense of strategy-proofness) and efficient mechanisms and demonstrate that they have a hierarchical structure. We study a more general setting in this paper. Our model can be considered as an extension of the “house allocation” model. Here the role of “houses” is played by “firms” . But contrary to that model, firms now have preferences, albeit rather homogeneous ones. When all projects have the same rank, our model is reduced to the “house allocation” model with general non-strict preferences. In this reduced model, Bogomolnaia et al. (2005) showed that the set of all stable and efficient assignments is equal to the set of all assignments obtained by some refined priority rule.Footnote 10 In our more general case, the results have a similar flavour. A decreasing refined priority rule always outputs a stable and efficient matching. Every stable and efficient matching is a result of a decreasing (non-refined) priority rule. The set of all stable and efficient matchings is equal to the set of all matchings that can be generated by some Pareto improved decreasing priority rule. If a preference profile admits strongly stable matching(s), then the set of all strongly stable matchings is equal to the set of all matchings that can be generated by some decreasing refined priority rule. Finally, the preference domain studied in this paper is also related to that of the refugee matching problem discussed by Andersson and Ehlers (2020). In their model, refugee families (projects) are also ordered according to family size giving rise to blocks containing projects of the same size. Hosts (firms) have limited beds and therefore their preferences have a nested structureFootnote 11 over these blocks similar to ours. The differences are as follows. Firstly, in our domain, hosts are necessarily indifferent between all refugee families in the same block (as a result we have indifference classes); while in their model, hosts can have general preferences over refugee families within the same block while respecting the nested structure between blocks. Secondly, contrary to our domain, their model do not elicit any preferences from refugee families whatsoever. It is important to take into account their preferences: there is growing evidence that the initial placement of refugee families greatly affects outcomes like education, job prospects, and earnings; which in turn profoundly alters their lifetime welfare, as most refugees do not move from the localities to which they are resettled for many years (Åslund and Rooth 2007; Damm 2014; Jones and Teytelboym 2017; Åslund et al. 2010; Martén et al. 2019). As described in Jones and Teytelboym (2018), ignoring preferences for refugees has even caused families seeking shelter in Finland to cancel their asylum applications.",1
27.0,2.0,Review of Economic Design,03 March 2022,https://link.springer.com/article/10.1007/s10058-022-00293-8,New results for multi-issue allocation problems and their solutions,June 2023,J. Sánchez-Pérez,,,Unknown,Unknown,Unknown,Unknown,,
27.0,2.0,Review of Economic Design,17 May 2022,https://link.springer.com/article/10.1007/s10058-022-00294-7,Seller experimentation and trade,June 2023,Peter Wagner,,,Male,Unknown,Unknown,Male,"Consider the following two-period game. In the first period, the seller of an object (“he”) faces the option of experimenting with it. If the seller decides to experiment, the probability of success depends on the type of the object, which is either “good” or “bad”. If the object is good, then the seller’s experiment is successful with probability \(\lambda _g\), and if the object is bad, then experimentation is successful with probability \(\lambda _b\), where \(\lambda _g>\lambda _b\). When the experiment is successful, the seller receives a flow-payoff \(y_S\ge 0\). If the experimentation fails, the seller incurs a loss of \(c_S> 0\). The seller’s flow-payoff from experimenting if his belief that the object is good is \(\pi \in [0,1]\), is given by If the seller does not experiment, his flow-payoff is zero. We assume that the seller has a positive valuation for a good object and a negative valuation for a bad object. \(\lambda _gy_S+(1-\lambda _g)c_S>0>\lambda _by_S+(1-\lambda _b)c_S\). In the second period, a potential buyer (“she”) observes a signal regarding the experimentation outcome in the first period and makes a take-it-or-leave-it offer to the seller. If the seller accepts, she pays the seller, and ownership is transferred to the buyer who subsequently has the option to experiment with the object. We assume that the buyer has a higher valuation for the good: if her experiment is successful, the buyer’s payoff is \(y_B\ge y_S\), and if it is a failure, the buyer has a loss \(c_B\le c_S\). If the seller rejects, no payment is made and the seller retains the option to experiment with the object a second time. For the buyer, the flow-payoff from experimenting when her belief is \(\pi \) is denoted by If the buyer does not experiment, her flow-payoff is normalized to zero. \(\Pr (\theta _B|\theta _S)\) The exact timing of the game is as follows. First, the unobservable type \(\lambda \in \{\lambda _g,\lambda _b\}\) of the object is realized, then the seller decides whether to experiment. In the next step, the outcome \(\theta _S\in \{s,f,0\}\) is realized, where \(\theta _S=s\) represents a success and \(\theta _S=f\) a failure in the event that the seller experiments, and \(\theta _S=0\) represents the event when the seller does not experiment. We assume that the signal that the seller receives about the experimentation outcome is unverifiable.Footnote 1 Subsequently, the buyer observes a signal \(\theta _B\in \{s,f,0\}\) about the true experimentation outcome. The distribution over the buyer’s signal \(\theta _B\) conditional on the experimentation outcome \(\theta _S\) is depicted in Fig. 1, where \(\alpha ^s,\alpha ^f\in (0,1)\). Based on the realization of her signal, she makes an offer to the seller which the seller either accepts or rejects. If the seller accepts the offer, the buyer makes a payment equal to her offer to the seller and then has the option to experiment in the second period. If the seller rejects, the buyer makes no payment, and the seller retains the option to experiment in the second period. The flow-value derived from experimenting with the object depends on the experimenter’s belief. The common prior belief that the object is good is \(\pi _0\). If the seller experiments and the experimentation is successful (\(\theta _S=s\)), then his updated belief in the second period is If the seller experiments and the experiment fails (\(\theta _S=f\)), then his updated belief in the second period is If the seller does not experiment (\(\theta _S=0\)), then his belief in the second period is identical to his prior belief. For future reference, we denote by \(\sigma =\pi _0\lambda _g+(1-\pi _0)\lambda _b\) the probability of success if the seller experiments. The seller’s flow-value of experimenting in the first period is \(u=u(\pi _0)\). After updating his belief, the seller’s valuation of owning the good in the second period is \(u^s=\max \{u(\pi ^s),0\}\) if he experiments successfully in the first period, \(u^f=\max \{u(\pi ^f),0\}\) if he experiments in the first period and the experiment fails, and \(u^0=\max \{u(\pi _0),0\}\) if he does not experiment. The buyer’s posterior belief about \(\theta _S\) conditional on the realization of her own signal is obtained via Bayes’ rule. If the buyer observes a success (\(\theta _B=s\)), she assigns probability one to \(\theta _S=s\), and hence her belief that the object is good is \(\pi ^s\). Similarly, if she observes a failure (\(\theta _B=f\)), she assigns probability one to the event \(\theta _S=f\) and hence her belief that the object is good is \(\pi ^f\). The buyer’s valuation of owning the good in the second period is therefore \(v^s=\max \{v(\pi ^s),0\} \) if she observes a success \((\theta _B=s)\) and \(v^f=\max \{v(\pi ^f),0\}\) if she observes a failure (\(\theta _B=f\)). If the buyer does not observe the experimentation outcome \((\theta _B=0)\) her posterior belief depends on the probability she assigns to the event that the seller experimented. In the special case in which she assigns probability one to this event, her valuation of the good is \(v^0=\max \{\sigma ^0v^s+(1-\sigma ^0)v^f,0\}\), where is the probability that a success occurred when the buyer observes \(\theta _B=0\). A strategy for the buyer is a (possibly random) offer for every signal \(\theta _B\in \{s,f,0\}\), represented by a triple \(p=(p^{s},p^f,p^0)\), where \(p^{\theta _B}\) is the buyer’s offer when observing signal \(\theta _B\). A strategy for the seller is an experimentation decision, represented by the (possibly random) variable \(e\in \{0,1\}\) that indicates whether the seller experiments (\(e=1\)) or not (\(e=0\)), together with an acceptance rule a that specifies for every history \((\theta _S,p')\) consisting of an experimentation outcome \(\theta _S\in \{s,f,0\}\) and offer \(p'\ge 0\) an acceptance decision \(a(\theta _S,p')\in \{0,1\}\), where \(a(\theta _S,p')=1\) if the seller accepts and \(a(\theta _S,p')=0\) if he rejects. The acceptance rule is independent of the seller’s experimentation decision without loss of generality, because \(\theta _S\) is a sufficient statistic for e. The net present values for the seller and the buyer at the beginning of the game are, respectively, A strategy pair (p, (e, a)) is a weak perfect Bayesian equilibrium if, given the players’ beliefs, (i) \(U_S(p,(e,a))\ge U_S(p,(e',a'))\) for \((e',a')\ne (e,a)\), (ii) \(U_B(p,(e,a))\ge U_B(p',(e,a))\) for \(p'\ne p\) and (iii) beliefs at each history are determined via Bayes’ rule when possible. I define an equilibrium to be a weak perfect Bayesian equilibrium that has the property that the buyer’s belief assigns probability one to the event \(\{\theta _S=\theta '\}\) when \(\theta _B=\theta '\) for each \(\theta '\in \{s,f\}\) at every history. This additional assumption merely ensures that informative signals remain informative, even when they are unexpected.",
27.0,2.0,Review of Economic Design,24 February 2022,https://link.springer.com/article/10.1007/s10058-022-00296-5,Other-regarding preferences and giving decision in a risky environment: experimental evidence,June 2023,Mickael Beaud,Mathieu Lefebvre,Julie Rosaz,Male,Male,Female,Mix,,
27.0,2.0,Review of Economic Design,08 March 2022,https://link.springer.com/article/10.1007/s10058-022-00298-3,Paying it forward: an experimental study on social connections and indirect reciprocity,June 2023,Pinghan Liang,Juanjuan Meng,,Unknown,Unknown,Unknown,Unknown,,
27.0,2.0,Review of Economic Design,04 April 2022,https://link.springer.com/article/10.1007/s10058-022-00299-2,Equilibrium design in an n-player quadratic game,June 2023,Trivikram Dokka,Hervé Moulin,Sonali SenGupta,Unknown,Male,Female,Mix,,
27.0,2.0,Review of Economic Design,01 April 2022,https://link.springer.com/article/10.1007/s10058-022-00300-y,Deviation from proportionality and Lorenz-domination for claims problems,June 2023,Miguel Ángel Mirás Calvo,Iago Núñez Lugilde,Estela Sánchez-Rodríguez,Male,Male,Female,Mix,,
27.0,2.0,Review of Economic Design,12 October 2021,https://link.springer.com/article/10.1007/s10058-021-00268-1,Correction to: When are committees of Condorcet winners Condorcet winning committees?,June 2023,Fatma Aslan,Hayrullah Dindar,Jean Lainé,Female,Male,Male,Mix,,
27.0,2.0,Review of Economic Design,18 January 2022,https://link.springer.com/article/10.1007/s10058-021-00284-1,Correction to: Socio-legal systems and implementation of the Nash solution in Debreu–Hurwicz equilibrium,June 2023,Claus-Jochen Haake,Walter Trockel,,Unknown,Male,Unknown,Male,"In definition 1, the expression in (iii) was incorrectly typeset. It should have read as  \(x_{i}^{*} \in {\text {argmax}}_{x_{i}:\left( x_{i}, x_{-i}^{*}\right) \in \mathscr {L}\left( \left( x_{i}, x_{-i}^{*}\right) , S\right) \cap \left( \mathscr {A}_{i}^{S}\left( x_{i}, x_{-i}^{*}\right) \times \left\{ x_{-i}^{*}\right\} \right) } \pi _{i}^{S}\left( x_{i}, x_{-i}^{*}\right) \quad (i \in N)\). The original article has been corrected.",
27.0,3.0,Review of Economic Design,18 November 2022,https://link.springer.com/article/10.1007/s10058-022-00317-3,Religious affiliations of Chinese people and prosocial behavior: evidence from field experiments,September 2023,Weiwei Xia,Xiaohan Guo,Weisen Xia,Unknown,Unknown,Unknown,Unknown,,
27.0,3.0,Review of Economic Design,19 April 2022,https://link.springer.com/article/10.1007/s10058-022-00301-x,Almost-truthful interim-biased mediation enables information exchange between agents with misaligned interests,September 2023,Dmitry Sedov,,,Male,Unknown,Unknown,Male,"Information relevant to decision-making is often distributed among agents with misaligned interests. For illustration, consider the following examples. In knowledge-intensive organizations complementary information is fragmented and held by employees who may be directly or indirectly affected by their colleagues’ choices. As another example, intelligence agencies gather incomplete material relevant for investigations, but are also involved in competition for influence and authority. In these cases, receiving information is privately desirable for the agents, while revealing it may be privately harmful. When such a conflict is present, how can mutually advantageous information exchange be organized? The present paper answers this question in a succinct model of distributed information and misaligned interests. The model features a one-stage game without monetary transfers, in which two agents obtain private signals from a finite set, then take actions and receive payoffs that depend on the combination of signals and actions. Agents lack commitment power and may have misaligned interests regarding each other’s actions, while the payoffs are assumed to be separable in actions. Depending on the exact preferences that each agent has regarding her counterpart’s actions, direct communication can be hard in this model. In fact, welfare-improving direct communication cannot be sustained in equilibrium at least in case when the payoffs are such that an action change benefiting one agent necessarily harms the other agent. However, this paper shows that even for such preferences (but also for other preferences regarding the counterpart’s actions), a special class of almost-truthful interim-biased mediation protocols can facilitate communication provided that each agent’s payoff depends substantially less on her counterpart’s action relative to her own. Agent interactions mediated by the protocols in the almost-truthful interim-biased class are structured as follows. First, agents observe private signals and make cheap-talk signal reports to the mediator. Next, the mediator sends a private message back to each agent. Then, each agent takes an action based on her private signal and the mediator’s message. Finally, the combination of actions and actual signals determines the payoffs. What does the mediator’s private message to an agent contain? Almost always, it contains the actual signal report submitted by the other agent (thus the mediation is labelled as almost-truthful) and, with a very small probability, it contains a distorted signal report. Distorted messages aim at implicitly encouraging a truthful agent to take the action that is interim-optimal given her private signal report only (thus the mediation is labelled as interim-biased). Since the two types of messages (with and without a distortion) take value in the same set, the agents cannot distinguish them with certainty. Why do such messages ensure the existence of a truth-telling equilibrium? To begin with, since the distortion probability is small, each agent is almost certain that the mediator’s message coincides with the other agent’s actual signal report and selects an action based on this belief. The mediator then exploits this belief when using distorted messages by shifting agents’ actions and affecting payoffs in a way that prevents deviations from truth-telling. Specifically, a distorted message harms an agent more when she reports untruthfully rather than truthfully. This occurs because a truthful agent is implicitly encouraged by the distorted message to take the interim-optimal action conditional on her private signal, while a deviating agent combines her private signal with the encouragement based on an untruthful report and shifts her action away from the truly interim-optimal one. The possibility of such an undesirable shift makes revealing the private signal truthfully to the mediator optimal from the perspective of selecting one’s own action. While a deviating agent may still benefit from the change in the counterpart’s action caused by the deviation, the incentives for truthful communication dominate provided that the misalignment of interests between the agents is small enough. Beyond showing that the almost-truthful interim-biased mediation allows for a truthful information exchange under certain condition, the paper includes several auxiliary results. These include developing an optimal mediation protocol for the illustrative example used in the paper and showing that the main results of the paper hold when the assumption of agents’ payoffs being separable in actions is slightly relaxed. Overall, this paper contributes to the existing literature on information exchange by developing a novel tool that facilitates communication between agents with a sufficiently small misalignment of interests. Reiterating on the discussion above, two notable features of almost-truthful interim-biased mediation are worth highlighting. First, an almost-truthful interim-biased mediator is able to punish deception by sometimes distorting the transmitted signals in a way that makes a truthful agent choose the action that’s optimal given her private information only, while shifting a deceitful agent’s action away from such an interim-optimal action. Second, the fact that an almost-truthful interim-biased mediator almost always transmits information without any distortions, ensures that it can actually shift agents’ actions in the desired direction when using the distortions. The rest of the paper is organized as follows. Section 2 briefly reviews the relevant literature. Section 3 provides an illustrative example revealing the intuition behind the main result. Section 4 presents the baseline model, shows that welfare-improving direct communication is not possible at least in the special case of agents benefiting from their counterpart mistakes, characterizes the class of almost-truthful interim-biased mediation protocols and demonstrates that such protocols enable information transmission. Section 5 concludes by summarizing the results and discussing the modest considerations for communication in organizations and for information exchange between intelligence agencies.",
27.0,3.0,Review of Economic Design,11 May 2022,https://link.springer.com/article/10.1007/s10058-022-00302-w,Compromising as an equal loss principle,September 2023,Olivier Cailloux,Beatrice Napolitano,M. Remzi Sanver,Male,Female,Unknown,Mix,,
27.0,3.0,Review of Economic Design,17 June 2022,https://link.springer.com/article/10.1007/s10058-022-00303-9,Entitlements to continued life and the evaluation of population health,September 2023,Juan D. Moreno-Ternero,Lars Peter Østerdal,,Male,Male,Unknown,Male,"Entitlements to continued life have been a major theme in the public debate, notably during the aftermath of the COVID-19 outbreak. Earlier than that, the so-called Affordable Care Act (also known as “Obamacare”) already made a strong case in favor of equal entitlements to continued life, as expressed in the following excerpt (e.g., Basu et al. 2020): 
The Secretary shall not use evidence or findings from comparative clinical effectiveness research conducted under section 1181 in determining coverage, reimbursement, or incentive programs under title XVIII in a manner that treats extending the life of an elderly, disabled, or terminally ill individual as of lower value than extending the life of an individual who is younger, nondisabled, or not terminally ill.
 Equal entitlements to continued life have also been scrutinized in academic research. One of the strongest defenders is John Harris, who, in a series of contributions that date back to the 80’s and spanned for more than 20 years, argued ethical concerns for a fundamental right to continued life to which all individuals are entitled to the same extent (e.g., Harris 1985, 1997, 1999, 2005). Harris’ arguments led to the conclusion that, even if some lives are not lived at perfect health, lives are in fact equally valuable, as long as they are valued by those living those lives. Under certain circumstances, such a conclusion has also been endorsed by part of the health economics community.Footnote 1 For instance, recent empirical evidence suggests that choices about which patient to treat are influenced more by the sizes of the gains achievable from treatment than by patients’ life expectancy or quality-of-life in absence of treatment (e.g., Shah et al. 2015). Somewhat related, the National Institute for Health and Care Excellence (NICE), the organization responsible for producing advice on the use of health technologies in the British National Health Service, indicates that it may be appropriate to recommend the use of treatments for terminal illness that offer an extension to life even if their base case cost-effectiveness estimates exceed the range normally considered acceptable (e.g., Rawlins and Culyer 2004). Another argument usually considered to defend equal entitlement to continued life is the recurrent argument within political philosophy that welfare interpersonal comparisons are incommensurate, and, therefore, that it is wrong to discriminate on the basis of health states. Nevertheless, such an argument has been contested (e.g., Singer et al. 1995; McKie et al. 1996) and debated (e.g., Grimley Evans 1997; Williams 1997). We provide in this paper a new perspective on the concept of equal entitlement to continued life, in connection with the evaluation of population health, a topic receiving increasing attention within economic design (e.g., Herrero and Moreno-Ternero 2008; Calo-Blanco 2016, 2020; Chambers and Moreno-Ternero 2019; Immorlica 2019). To do so, we consider the axiomatic approach to the evaluation of population health, introduced by Hougaard et al. (2013), and also considered by Moreno-Ternero and Østerdal (2017). In such an approach, the health of an individual in the population is defined according to the two standard dimensions (quality of life and quantity of life), but one of them (quality of life) receives a special treatment, as no restrictions are made regarding its mathematical structure. A distinguishing feature of this approach is that it does not make assumptions about individual preferences over quantity and quality of life. In doing so, we depart from the strand of the literature on population health evaluation in which the analysis relies on individual preferences on quantity and quality of life (e.g., Østerdal 2005; Harvey and Østerdal 2010), and also from the popular strand of the (health economics) literature in which the analysis relies on a generic individual health utility concept (e.g., Wagstaff 1991; Bleichrodt 1997; Dolan 1998). Thus, we circumvent basing our analysis on the concept of individual health preferences, which has faced recurrent criticisms over its conceptual foundation and elicitation procedures, particularly in the context of population health evaluation (e.g. Dolan 2000). We formalize equal entitlement to continued life as an axiom of social preferences for population health evaluations in the model described above. More precisely, we consider a cohort of individuals and aim to evaluate the effects of alternative health care policies for such a cohort, on the grounds of the resulting distributions of health (that the policies would generate for the cohort). In such a scenario, equal entitlement to continued life is formalized as the axiom stating that if two distributions of health only differ in granting an amount of extra years to one or another individual, then they are considered equally good by the social planner (as all lives are valued equally). The combination of such an axiom with some basic structural axioms characterizes the population health evaluation function that ranks distributions according to the unweighted aggregation (across agents in the population) of lifetimes in the distribution. Such a function does not include any concern whatsoever for the quality of life at which individuals in the population experience those lifetimes, which is in contrast with some traditional forms of evaluation for health distributions, such as the so-called Quality Adjusted Life Years (e.g., Pliskin et al. 1980), in short QALYs, and the so-called Healthy Years Equivalent (e.g., Mehrez and Gafni 1989), in short HYEs. In other words, under the presence of some basic structural axioms, endorsing the principle of equal entitlement to continued life in its full force drives towards dismissing morbidity concerns in the evaluation of population health. The result just described is closely related to a result in Hasman and Østerdal (2004), which establishes a general incompatibility between a specific form of the equal entitlement to continued life principle and the weak Pareto principle.Footnote 2 The main aim of the paper is to distill weaker axioms limiting the scope of equal entitlement to continued life, and explore their implications. To wit, axioms in which the extra years are not granted to arbitrary individuals, but to individuals sharing some features. As we show, some of the axioms keep driving towards lifetime utilitarianism. Some others do allow for more general population health evaluation functions, dubbed generalized lifetime utilitarianism. That is, the unweighted aggregation (across agents in the population) of lifetimes in the distribution, after being submitted to an increasing and continuous function. The idea of equal entitlement to continued life, as introduced above, prevents any form of discrimination against individuals with worse quality of life, when it comes to allocate extra life years. Now, some political philosophers have endorsed going a step ahead, arguing that justice requires that a positive discrimination in favor of the worst-off be allowed. The most extreme position is advocated by Rawls (1971), with his so-called difference principle, for whom differences in primary goods are only morally acceptable if they maximize the level of primary goods achieved by the worst-off individual. Parfit (1997) coined the term prioritarianism for the view that the worse off should be given priority over the better off, but that they need not necessarily receive the extreme priority that characterizes the difference principle. A comprehensive endorsement of the prioritarian evaluation of outcomes and policies is provided by Adler (2012). There are several ways in which the principle of prioritarianism could be formalized (e.g., Moreno-Ternero and Roemer 2008). In a welfarist setting, prioritarianism is usually characterized as a strictly concave social welfare function (e.g., Roemer 2004). In a non-welfarist setting of resource allocation, it can be formalized as an axiom of no-domination (e.g., Moreno-Ternero and Roemer 2006, 2012), which implies that less capable agents to transform resources into outcomes cannot receive less resources. In our setting, we can formalize prioritarianism by means of similar axioms to those in the non-welfarist setting of resource allocation, but regarding the hypothetical allocation of extra life years. More precisely, we can unambiguously say that an agent is worse off than another if the latter dominates the former in both quality and quantity of life. Our worst-off priority axiom formalizes the idea that extra life years should not be valued less when awarded to a worst-off agent, so defined. We also consider another weaker axiom in which the principle is restricted to (pairs of) agents at perfect health. We show that the combination of the worst-off priority axiom with some basic structural axioms characterizes concave lifetime utilitarianism. That is, the population health evaluation function referring to the unweighted aggregation (across agents in the population) of lifetimes in the distribution, after being submitted to a concave (increasing and continuous) function. Thus, as with the case of equal entitlement to continued life, morbidity concerns are excluded from the evaluation of the distribution of health. Nevertheless, mortality concerns are allowed to be included in a more general and egalitarian-oriented form. Based on the results mentioned above, one might wonder whether all population health evaluation functions including a concern for morbidity are also excluded when principles related to prioritarian value of life are imposed. It turns out that is not the case, as the family of population health evaluation functions arising upon aggregating individual HYEs, after being submitted to a concave (increasing and continuous) function, can also be characterized resorting to an axiom connected to the principle of priority. More precisely, the last result of the paper states that, if we consider the weaker worst-off priority axiom outlined above, combined with the basic structural axioms mentioned above, we characterize the family of population health evaluation functions arising upon aggregating individual HYEs, after being submitted to a concave (increasing and continuous) function. The rest of the paper is organized as follows. In Section 2, we introduce the model, the focal population health evaluation functions, and the basic structural axioms we consider for our analysis. In Section 3, we introduce the axioms of equal entitlement to continued life and explore their implications. As a result, we characterize population health evaluations that dismiss morbidity concerns. In Section 4, we move to extend the analysis to the case of prioritarian (rather than equal) entitlement to continued life. Here we also characterize population health evaluations that dismiss morbidity concerns and others that do capture them. Section 5 discusses some further insights, ranging from the tightness of our characterization results to variations and possible extensions of our model. We conclude in Sect. 6.",1
27.0,3.0,Review of Economic Design,02 September 2022,https://link.springer.com/article/10.1007/s10058-022-00310-w,Resource allocations with guaranteed awards in claims problems,September 2023,José-Manuel Giménez-Gómez,Josep E. Peris,María-José Solís-Baltodano,Unknown,Male,Unknown,Male,"The so-called claims problem reflects a situation where the agents’ claims cannot be totally honored when a resource must be distributed among them. The way of rationing this resource among the agents, taking into account their claims, is prescribed by a rule. In this context, we analyze how to distribute any increase of the endowment in terms of two general concepts: first, establishing that each agent should be guaranteed a minimum award, which is determined by a particular lower bound (respect of the lower bound); and, then, requiring that agents with equal guarantees, should be treated equally (equal treatment of equals). It is noteworthy that the concern of ensuring some minimum individual rights has figured in a large number of contexts. Specifically, the Universal Basic Income is a classical issue that has attracted much attention in the social policy literature and the political agenda during the last two decades (Noguera 2010). Additionally, the establishment of a minimum wage in the labor market, the debate about ensuring a universal minimum health coverage in the U.S. Senate, the European Structural and Investment Funds, ensuring minimum quantities in heritage laws, fishing quotas (Iñarra and Prellezo 2008; Kampas 2015), or, the negotiations of \(CO^2\) emissions, a relevant issue nowadays (Giménez-Gómez et al. 2016), are further real-life examples. From a theoretical point of view, the idea of establishing minimum guarantees on awards underlies the analysis of claims problems from its beginning (O’Neill 1982) up to the present day (Giménez-Gómez and Marco-Gil 2014). Indeed, for each problem, the formal definition of a rule already includes the requirement that awards be non-negative, which represents a lower bound on awards. In fact, there are some solutions that always ensure a strictly positive quantity to each agent (with positive claim): this is the case of the constrained equal awards rule (Maimonides 2000), the \(\alpha _{\min }\) solution (Giménez-Gómez and Peris 2014), or solutions that are defined by positive eigenvectors (Subiza et al. 2015). The impact of requiring that a rule fulfills a lower bound was first analyzed by Dominguez and Thomson (2006) and Yeh (2008). Afterwards, the recursive application of a lower bound has been analyzed in the literature, showing that (under some mild conditions) this process provides a unique rule. In particular, Dominguez (2013) and Giménez-Gómez and Marco-Gil (2014), among others, find out that some well known rules are retrieved by recursively applying lower bounds and, consequently, they provide new axiomatic characterizations of classical rules. Our present approach elaborates on these previous works but, instead of applying a lower bound recursively, we require that rules should fulfill the lower bound and some additional conditions on the distribution of the resources, which also depend on the lower bound being used. Specifically, we require that a rule (i) guarantees to each individual at least the amount determined by the particular lower bound being used (respect of the lower bound); and, (ii) fulfills axioms related to equal treatment of equals (conditional equal treatment), or related to some monotonicity behaviour (conditional resource monotonicity, conditional equal bound monotonicity, or priority). The idea behind these axioms is to compare the guaranteed awards among the agents and, on this basis, to determine the way of distributing the endowment whenever it increases. A key point in our study is the selection of a specific lower bound on which the aforementioned axioms are based. Hence, we need to choose a meaningful lower bound in the sense that it should be different from zero, whenever the claim is different from zero (quoting Dominguez 2013, words, “these lower bounds satisfy positivity""). In this regard, by focusing on three lower bounds (the fair bound Moulin 2002), securement (Moreno-Ternero and Villar 2004), and the \(\min \) bound (Dominguez 2006), our main results show how these axioms provide new characterizations of the constrained equal awards and Ibn Ezra’s rules. As we obtain new characterizations of the constrained equal awards and Ibn Ezra’s rules, we present some previous results to better understand our contribution.  Regarding the CEA rule, it is the only rule satisfying:  equal treatment of equals, invariance under claims truncation, and composition up (Dagan 1996). conditional full compensation and composition down (Villar and Herrero 2002). conditional full compensation and claims monotonicity (Yeh (2001), mimeo, cited in Thomson (2003)). respect of the securement bound, null claims consistency and composition up (Yeh 2008). respect of the fair bound, composition down and zero-consistency (Moulin 2002). As far as we know, the first characterization result of Ibn Ezra’s rule is due to (Bergantiños and Méndez-Naya 2001)  the Ibn Ezra rule is the unique rule satisfying equal treatment of equals and additivity.  These authors extend and characterize the solution for general claims problems. In Alcalde et al. (2005) this solution is also extended and its characterization uses cooperative game conditions  the generalized Ibn Ezra value is the unique bankruptcy value satisfying anonymity, transitional dummy and worth-generators composition.  They show that this extension fulfills order preservation, continuity, claims and estate monotonicity, supermodularity, and homogeneity. Finally, note that when facing a claims problem, each individual has a claim on the endowment that represents the maximum amount she can receive and, at the same time, the maximum amount she can lose. The agent’s loss is equal to the difference between her claim and her award. By focusing on losses (the so-called dual approach), a lower bound on awards provides the maximum amount that individual can lose; that is, we are considering upper bounds on losses. Analogously, a lower bound on losses provides an upper bound on awards. By analyzing the implications of the existence of lower bounds on losses, we straightforwardly obtain from the previous results characterizations of their dual rules: the constrained equal losses and the dual Ibn Ezra’s rule. The paper is organized as follows. Section 2 presents the main definitions. Section 3 introduces the axioms and Sect. 4 provides our main results. In Sect. 5 we present some comments dealing with the dual approach: upper bounds. Finally, Sect. 6 comments on the dual approach and mentions some possible future research.",
27.0,3.0,Review of Economic Design,16 December 2022,https://link.springer.com/article/10.1007/s10058-022-00313-7,The more the merrier? On the optimality of market size restrictions,September 2023,Colin von Negenborn,,,Male,Unknown,Unknown,Male,"Why do we observe markets where regulation restricts the number of firms allowed to compete, effectively creating oligopolies of a certain—at times seemingly arbitrary—size? After all, economic intuition suggests that in a market of profit-seeking firms, welfare is maximized by ensuring the highest possible degree of competition instead of limiting market entry. Even beyond the realm of economic theory, “competition is, in our system, a political and social desideratum” (McNulty 1968, p. 639). In some markets existing in practice, however, competition is limited in that only a number of firms are allowed to enter. In 2016, the Greek government cut the number of TV licenses granted to privately owned broadcasters from eight to four – only to change it again to five licenses two years later.Footnote 1 Similarly, the German football league (Bundesliga) raised the amount of licenses issued for live pay-TV broadcasts of its matches in 2016, claiming an improvement not only for fans and viewers, but also for the league’s finances.Footnote 2 In New York City, the number of taxis in operation is regulated by the Taxi and Limousine Commission, which has barely varied the amount of “medallions” since the 1930s – despite the city having grown rapidly both in population and size, and despite the socially optimal number of such medallions estimated to be about 55% higher than its current value.Footnote 3 Further examples include the number of licenses given out to telecommunication companies in spectrum auctions or to betting agencies in sport wagering. Regulators are interested in both consumer surplus and industry profits in most markets, since the latter can (partly) be extracted using e.g. license auctions.Footnote 4 Theory suggests that, unless specific market assumptions are made (which we discuss in the literature review), utmost competition is optimal: welfare is maximized by having as many firms as possible compete with one another. While there may be practical reasons to restrict the market size in specific settings (technical constraints, health concerns, etc.), from an economic perspective “the limit appears to be rather arbitrary” (Borenstein 1988, p. 357). This paper explains why such restrictions to competition may indeed be optimal for a regulator seeking to maximize total welfare. To this end, it studies a simple market of constant marginal costs. We consider a two-stage model. First, the welfare-maximizing regulator specifies a market size, thus allowing a subset of finitely many interested firms to operate in a market. She does so e.g. by issuing licenses. Second, those firms to which access is granted compete à la Cournot. Crucially, firms differ in their marginal costs, with the costs unknown to the regulator. We show that raising market size by issuing more licenses has a two-fold effect: an increase in competition and a decrease in average production efficiency. We disentangle the two and study their welfare effects to prove that a more oligopolistic market can indeed be welfare-enhancing. Intuitively, opening up the market gives rise to countervailing forces. On the one hand, it fosters competition. On the other hand, a greater market size also attracts less efficient firms: since firms with lower marginal costs generate higher profits, they enter the market first (as they place higher bids when licenses are auctioned off), while entrants arriving later produce at higher costs. To single out each effect, consider this market opening in two steps. First, add a new firm with production costs equal to the market average. This heats up competition while leaving production efficiency unchanged. As a result, prices are driven down, which is detrimental to firms but beneficial for consumers. Next, raise the entrant’s cost to the value expected by the regulator. Surprisingly, this cost increase does not necessarily harm firms as a whole: it is possible that an increase in average marginal costs, keeping the total number of firms fixed, raises total firm profits. The more efficient firms can exploit the inefficiencies of their high-cost rivals, taking over their market share. The additional firm profits overcompensate the loss for consumers caused by the increase in production costs. When more firms are admitted to a market, we therefore observe a trade-off due to the two—potentially countervailing—effects: fostering competition always increases welfare, while the simultaneous decrease in production efficiency has an ambiguous impact on welfare. The regulator has to consider both the competition and the cost effect in her choice of the market size. Identifying the welfare-maximizing market size, we first consider the benchmark scenario of homogeneous firms. Here, the standard intuition applies as utmost competition is optimal. The regulator does not impose any restrictions and allows all firms to enter. But with firms being heterogeneous, the regulator may face the aforementioned trade-off between competition and production costs, depending on market characteristics. It can now become optimal in terms of welfare to enforce an oligopoly, granting market access only to a limited number of firms. This main finding is presented as a possibility result: market constellations can be such that competition is desirable only to some extent such that the usual notion of “the more the merrier” does not apply. After establishing this result analytically, the paper then employs numerical methods to study how the optimal regulatory policy varies with the market specifications. The paper proceeds as follows: the existing literature is reviewed in Sect. 2, while Sect. 3 presents the formal model. In Sect. 4, we first analyze the market equilibrium given a fixed market size. Subsequently, we solve the regulator’s problem of choosing the optimal market size to maximize expected total welfare. Section 5 shows that the results are robust to changes to the model: we analyze both Cournot and Bertrand competition, i.e. firms setting either prices or quantities; heterogeneity is studied both in marginal costs and in quality levels; a regulator interested in auction revenue is considered. Here we also present an auction implementing entry of firms in the order of their efficiency as well as superior mechanisms. All proofs are in the Appendix. Accompanying Mathematica code is available upon request.",
27.0,3.0,Review of Economic Design,31 October 2022,https://link.springer.com/article/10.1007/s10058-022-00314-6,Designing randomized response surveys to support honest answers to stigmatizing questions,September 2023,James C. D. Fisher,Timothy J. Flannery,,Male,Male,Unknown,Male,"Respondents often evade stigmatizing questions.Footnote 1 Thus, a small but growing number of surveys use randomized response methods, which inject noise into answers to provide a degree of privacy.Footnote 2 While it is intuitive that respondents will provide non-evasive, honest answers when the noise is sufficiently large, noise also deteriorates the quality of estimates based on survey data. The interviewer should thus design the survey to balance (i) honest reporting by respondents and (ii) the accuracy of downstream estimates. Yet, the theoretical literature on randomized response methods does not solve this “survey design problem.” We study the survey design problem through a game-theoretic lens. Some respondents are stigmatized, and the interviewer’s goal is to estimate the unknown, population rate of stigmatization. We focus on Warner’s (1965) original randomized response technique (RRT), which randomly and privately provides a respondent one of two yes-no questions: “Are you stigmatized” and “Are you not stigmatized?” The randomization creates uncertainty about whether a yes answer came from a stigmatized respondent—i.e., it injects noise into responses—and so provides such respondents with a degree of privacy. Nevertheless, an honest yes answer is more likely to have come from a stigmatized respondent when the probability of the first question exceeds \(\frac{1}{2}\). Hence, the interviewer perceives that a yes-responder is more likely stigmatized than a no-responder. The interviewer is also able to estimate the population rate based on respondents’ answers. Respondents, in our game, do not like to be perceived as stigmatized. They also prefer to tell the truth. Thus, when answering a question, each weighs the benefit of an honest reply against its stigmatization-related costs and chooses honesty when it maximizes his payoff. The benefit of an honest response only depends on one’s own answer. The stigmatization-related costs depend on one’s own answer, the randomization probabilities, and the answers of others. Respondents’ stigmatization statuses are correlated by the population rate of stigmatization and so the interviewer’s perception of one respondent’s status varies with others’ answers. Taking the design of the survey as given, respondents play a simultaneous move “survey response game.” We first focus on this game and ask, “When are respondents honest?” We characterize respondents’ best responses for every situation they may encounter to derive the “honesty constraint,” i.e., the mapping between the space of preferences and survey design parameters such that the honest perfect Bayesian equilibrium exists. This constraint incorporates both the interviewer’s perceptions, i.e., posterior beliefs, and respondents’ expectations about others’ answers. The honesty constraint has two implications for survey design. To elaborate, let N be the sample size and let p be the likelihood of the question “Are you stigmatized?” in the RRT. First, the honesty constraint limits the maximal p for each N. Intuitively, a higher p reduces randomization in the RRT and makes it more likely that an honest yes answer comes from a stigmatized type. This increases the interviewer’s posterior belief that a yes-respondent is stigmatized. Hence, with a sufficiently large p, the disutility of a yes response outweighs the benefit from an honest reply and so leads to lying. Second, the honesty constraint leads the maximal value of p to increase with N. The argument is technical, but leverages two complementary forces that are rooted in honesty and Bayesian updating. Roughly, the first force is driven by an increasing N collapsing the interviewer’s conditional posterior about the rate of stigmatization, while the second force is driven by and increasing N changing the distribution of co-respondents’ potential answers. While we elaborate on these forces in the paper, their joint effect is that the set of p on which i is honest increases with N, resulting in a “strategic complementarity” between N and p. The survey response game is developed in Sect. 2 and the honesty constraint is characterized in Sect. 3. We continue the analysis by asking, “How should the survey be designed to support respondents’ honesty?” In our “survey design game,” the interviewer selects (N, p) before respondents play the survey response game. After observing respondents’ answers, he estimates the rate of stigmatization and is compensated based on the accuracy of his estimate. He faces two constraints on his selection of (N, p). First, the cost of the survey must satisfy a budget constraint. Second, the survey must be designed to support honesty for reasons of ethics, regulation, and/or practicality. Therefore, the interviewer cannot pick an (N, p) for which respondents want to lie, i.e., (N, p) satisfies the honesty constraint. For simplicity, we solve the full game via a restriction of perfect Bayesian equilibrium where respondents are honest where possible. We demonstrate existence of a unique equilibrium and we characterize the interviewer’s equilibrium strategy for “classical estimators.” Classical estimators are those whose bias and variance are decreasing in (N, p) when respondents are honest. They include, for instance, Warner’s (1965) maximum likelihood estimator, which is common in applied work. With a classical estimator, standard results from statistics show that the interviewer’s payoff is increasing in (N, p), provided respondents are honest. Yet, the maximal p is increasing in N. The interviewer thus wants a sample size of \(N^{\star }\), where \(N^{\star }\) is the largest sample he can afford given his budget, and he wants the probability of the “Are you stigmatized?” question to be \(p^{\star }\), which is the maximal p for \(N^{\star }\). The pair \((N^{\star },p^{\star })\) is the equilibrium survey design. Since \((N^{\star },p^{\star })\) satisfies the honesty constraint there are strategies for respondents and beliefs for the interviewer from the survey response game where honesty prevails. We show how to stitch these strategies, beliefs, and \((N^{\star },p^{\star })\) into an equilibrium of the survey design game. This equilibrium is unique due to the restriction. The design \((N^{\star },p^{\star })\) encapsulates the fundamental trade-off between the accuracy of the estimator, which is increasing in (N, p), and the dual needs of meeting the budget constraint and ensuring respondents’ honesty, which cap the maximal (N, p) in a non-linear fashion. The survey design game is given in Sect. 4 and the equilibrium design is derived in Sect. 5. The Online Archive contains computer code that computes the equilibrium design and supplemental results. We close the paper by exploring both technical generalizations of our games and economic extensions in Sect. 6. Section 7 gives concluding remarks. 
Related Literature
 A rich literature in statistics examines randomized response methods from a theoretical standpoint. This literature began with Warner’s (1965) introduction of the RRT and has developed along two main branches.Footnote 3 The first branch focuses on generalizations of the RRT and of the associated estimators, it includes studies like (Boruch 1971; Winkler and Franklin 1979), and successors. The second branch focuses respondents’ “privacy” under randomized response; it includes studies like (Leysieffer and Warner 1976; Ljungqvist 1993), and successors. This latter branch recognizes that respondents may lie when privacy is low and thus there is a high risk of stigmatization. Ljungqvist, in particular, introduces the survey design problem. However, neither he nor the subsequent literature either (i) develop the general, multi-respondent incentive constraints required for honest survey design or (ii) formally solve the survey design problem.Footnote 4 Within the economics literature, to our knowledge only (Blume et al. 2019) study the RRT from a theoretical standpoint.Footnote 5 They employ a dynamic psychological game with a single respondent and fully solve for the incentive constraints required for honesty. They also characterize the mixed equilibrium when these constraints are violated, and they conduct an experiment to test their predictions. They do not, however, consider multiple respondents or honest survey design. Against this backdrop, our primary contribution to these literatures is two-fold. First, building on the work of Blume et al. (2019), we develop and fully characterize the multi-respondent incentive constraints required for honesty. These constraints encode a strategic interdependency between respondents’ answers, which is absent in the prior literatures. Second, we leverage these constraints to determine the unique equilibrium survey design.Footnote 6 We close by observing that there is mixed empirical support for the RRT. On one side are studies like (Moshagen et al. 2014), which find that RRT yields accurate measurements. On the other side are studies like (Blume et al. 2019) and Umesh and Peterson Umesh and Peterson (1991) that find mixed evidence. Our results suggest that these divergent findings may be attributable to survey design.Footnote 7",
27.0,3.0,Review of Economic Design,04 November 2022,https://link.springer.com/article/10.1007/s10058-022-00316-4,The Sumo coach problem,September 2023,Daniel Rehsmann,,,Male,Unknown,Unknown,Male,"This paper analyzes the optimal allocation of stochastically dependent resource bundles to a collection of interconnected contests. Two players competitively allocate their bundles across a finite number of contests. A single contest is won in expectation by the player who submits a stochastically dominant random variable; the probabilistic distance increases in the realized resource differential. Each player’s expected payoff for the whole game is a combination of the probabilities of winning single contests. The optimal allocation of the available resources involves strategic considerations. Should a player try to narrowly win as many contests as possible, or should she “tank” some of them? Intuition suggests that an optimal allocation strategy must take each resource bundle’s strength into account and involve some randomness. We answer how to optimally allocate random variables among the set of such interconnected contests. Our main contribution is to characterize the set of Nash equilibria in all non-trivial parameter settings of the defined class of games and to prove the existence of mixed-strategy equilibria in tournaments involving three or more competitors. Although all equilibria share common characteristics, they vary in terms of pure strategy support. The characterization is closely linked to a combinatorial concept, namely to the notion of “Latin squares.”Footnote 1 Following Ferguson (2020), we define our game as a “Latin square game” and provide necessary and sufficient conditions for equilibria in all finite two-person zero-sum Latin square games. We then use generic properties of the equilibria in our game to answer questions of optimal marginal alterations of a player’s bundles in a comparative static analysis, postulating equilibrium play. The studied interaction shares some aspects with the Colonel Blotto game, introduced by Borel (1921), in which two players compete across several battlefields by partitioning a fixed number of troops among them. While a Colonel Blotto game allows for all (or all discrete) partitions of the total resources to be allocated, the interaction studied in this paper restricts the players to assign a priori fixed but random partitions to component contests. We formalize a version of Snyder’s (1989) definition of a simultaneous multi-component contest, altering the allocational restrictions accordingly. In a more applied context, Hamilton and Romano (1998) and Arad (2012) study similar problems referred to as the “Tennis Coach Problem.” We approach a similar problem but assume stochastic resource bundles that render the payoff representation entirely probabilistic. The modeled interaction seems applicable to several economic settings where the allocation of stochastic resources to a set of contested situations is the primary strategic element. Examples include organizational competitions and races between firms or electoral campaigning across several districts. The defined interaction also seems prevalent in team sports with individual matches such as Tennis, Sumo, Chess, and many others. In a broader context, our analysis can be applied to settings similar to the Colonel Blotto problem but tends to reflect less continuous assignment assumptions in situations where individuals are matched to distinct tasks. In our narrative, the described strategic elements are embedded in the context of a Sumo tournament, where team coaches assign their competitors to a set of distinct bilateral matches. A rank of a Sumo competitor reflects past performance in tournaments (see, e.g., Duggan and Levitt 2002); the probability of winning a specific match, however, depends on various factors (see, e.g., Bleekley et al. 2010). Addressing the complexity of Sumo tournaments, we thus assume that coaches are uninformed about actual strength realizations of the competitors as the assignment is chosen. We model the situation as a one-shot zero-sum game; in particular, the coaches of two teams—each of the latter endowed with idiosyncratic skill distributions—simultaneously announce their team members’ assignments to independent matches. The competitors feature commonly known ranks, and in expectation, a stronger competitor wins a match. All winning probabilities of the independent matches are then combined into an overall probability of winning the team competition, which the team coaches maximize by choosing an optimal allocation strategy. We structure the remainder of the paper as follows and start by relating our contribution to the existing literature. We introduce the model in Sects. 2 and characterize the equilibria in Sect. 3. We apply a comparative static analysis in Sect. 4 and discuss possible extensions and limitations of our model in Sect. 5. All proofs can be found in the “Appendix”. This paper mainly contributes to the literature on multi-battle contests. Such games share the characteristic that contenders meet each other on multiple battlefields, with discrete battles on each front. General overviews of multi-battle contests are provided by, e.g., Vojnović (2016) and Fu and Wu (2019). The classical literature on simultaneous multi-battle contests follows the original formulation of the Colonel Blotto game, due to Borel (1921), which features prominently in the early literature. Early contributions include, e.g., Gross and Wagner (1950), Blackett (1958) and Tukey (1949). More recent work that contributes to “Blotto-type” games examines asymmetries in resources or objectives (e.g., Roberson 2006; Kvasov 2007; Hart 2008; Avrahami and Kareev 2009; Kovenock and Roberson 2020) or alternative definitions of success (e.g., Golman and Page 2009; Kovenock and Roberson 2010, 2020). Since the present model differs in its strategic setting markedly from a Colonel Blotto game, it can be embedded more accurately in the literature on multi-battle contests between teams.Such contests are defined as “team contests with multiple pairwise battles” by Fu et al. (2015). A sequential formalization of such multi-battle contests dates back to Harris and Vickers (1987), who analyze the combination of component contests in the context of a dynamic R &D race. A simultaneous multi-battle contest was introduced by Snyder (1989), where two parties compete in parallel elections by allocating campaign resources to a set of legislative districts. In contrast to this formalization, our model restricts the allocation of resources to fixed partitions and follows the intuition of Hamilton and Romano (1998). Their basic setting as a two-player, zero-sum game, where team coaches assign their (ranked) competitors to a set of discrete tournaments, is also maintained in our contribution. This paper differs from their contribution by introducing a more general probabilistic method of payoff formalization linked to the concept of Latin squares. Furthermore, we assume that at the time of assignment, coaches are uninformed about the actual strength of each competitor. Moreover, we provide a more comprehensive characterization of equilibria in such games. Arad (2012) explores a setting similar to Hamilton and Romano (1998) in a primarily experimental study, in which coaches assign four heterogeneously skilled players to four positions. The game-setting is formalized as an n-player constant sum game, with non-probabilistic payoffs on the distinct playing slots. Equilibria and experimental behavior are analyzed from the perspective of a boundedly rational, mainly behavioral approach. A setting more closely linked to the Colonel Blotto literature is analyzed by Rinott et al. (2012), studying a sequential setup. Players allocate a finite amount of resources to individual team members, engaging in a sequence of one-on-one fights. The particular team members’ participation in such a tournament is analyzed by Fu et al. (2015). Their paper contrasts from our work since they assume that the players’ assignments to battlefields are fixed, while we exclude the team members’ effort choices from our analysis while assigning players to battlefields strategically. Adopting Ferguson (2020)’s definition of Latin square games as distinct versions of constant-sum games, some results of this paper also contribute to that literature. A necessary and sufficient condition for the existence of Nash equilibria in this model can be generalized to all two-player Latin square games. To the best of our knowledge, Ferguson (2020) is the only existing discussion of Latin square games in the literature. Finally, our comparative static analysis contributes to research on improving team performance. Strategic considerations in the training of teams remain mainly unformalized in the management, organizational, and theoretical literature. Team performance and leadership issues are studied in the context of contests by Gershkov and Schweinzer (2021). Both the psychology literature (e.g., Salas and Cannon-Bowers 2002) and research in human resource management (e.g., Campbell and Kuncel 2002) frequently contribute to the analysis of team performance. An example of a meta-study in the management literature is Salas et al. (2008).",

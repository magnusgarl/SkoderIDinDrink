Volume,Issue,Journal Name,Published Date,Link,Title,Journal Year,Author 1,Author 2,Author 3,Gender_Author 1,Gender_Author 2,Gender_Author 3,Article_Gender,Intro,Citations
1.0,1.0,Italian Economic Journal,27 February 2015,https://link.springer.com/article/10.1007/s40797-014-0006-z,Preface,March 2015,Aldo Montesano,Alessandro Roncaglia,,Male,Male,Unknown,Male,,
1.0,1.0,Italian Economic Journal,19 February 2015,https://link.springer.com/article/10.1007/s40797-015-0011-x,Risk Literacy,March 2015,Annamaria Lusardi,,,Female,Unknown,Unknown,Female,"Shifting economic policies and changes in the pension and economic landscape have forced individuals around the world to assume greater responsibility for their own financial well-being. For example, consumer credit has become much more available than in the past and individuals have to deal with substantially changed terms for credit cards, mortgages, and other borrowing vehicles. Against a backdrop of increasingly complex financial instruments, individuals must determine how much to save for retirement and how to allocate that retirement wealth. The ability to meet this expanded responsibility not only has a critical impact on individuals’ future financial security but carries important implications for the macro economy and the strength and stability of countries. How prepared are individuals to take on this greater responsibility and to process the economic information needed to make informed decisions about their current and future finances? Over the past 10 years, I have worked with central banks, treasury departments, financial regulators, and other institutions around the world to collect information to gauge financial literacy. Together with many collaborators, I have been able to show that three simple questions can be used to measure levels of financial knowledge as well as to differentiate across degrees of financial sophistication. More than 20 countries have added these three questions to their national surveys. Moreover, new surveys have been developed to measure knowledge of critically important financial concepts, such as risk and risk diversification. It is clearly important to focus on knowledge of risk because risk is a common feature of financial instruments and most decisions that deal with the future (which is inherently uncertain) contain elements of risk.Footnote 1
 Findings are sobering. Globally, only a very small percentage of individuals possess basic knowledge of the concepts that form the basis for financial decision making. This is as true in the United States and countries with well-developed financial markets as it is in developing countries (Lusardi and Mitchell 2014b). One finding stands out: in every country studied and in financial contexts, individuals display very low knowledge of risk. Strikingly, one-third of respondents state they do not know the answer to the questions that measure understanding of risk. This is important because risk literacy is an important component of financial literacy and financial illiteracy carries significant consequences. Financially knowledgeable individuals are more likely to save, to plan for future events, and to invest in the stock market; they are also less likely to engage in high-cost borrowing (Lusardi and Mitchell 2014b; Lusardi and Tufano 2009).Footnote 2 The sheer number of people who lost their home during the recent financial crisis is a painful reminder of how important financial decisions are for families and the economy. As I will discuss in more detail in this paper, knowledge of risk is a particularly powerful predictor of how competent individuals are with saving and planning decisions. The paper is organized as follows: In Sects. 2 and 3, I review the questions used for measuring financial and risk literacy. In Sect. 4, I review the evidence for Italy. In Sect. 5, I show the link between risk literacy and behavior and in Sect. 6, I discuss financial advice. In Sect. 7, I describe how we can improve risk literacy and provide concluding remarks in Sect. 8.",14
1.0,1.0,Italian Economic Journal,05 December 2014,https://link.springer.com/article/10.1007/s40797-014-0005-0,Growth and Cycles of the Italian Economy Since 1861: The New Evidence,March 2015,Fabio Clementi,Marco Gallegati,Mauro Gallegati,Male,Male,Male,Male,"Italy has a solid and long tradition of studies dealing with the issue of reconstructing national accounts in the post-Unification period. The first set of historical national accounts from 1861 to 1956, due to the National Institute for Statistics (ISTAT 1957), was revised and improved by a group of researchers of the University of Ancona under the supervision of Fuà (1969). Maddison (1991) proposed a revision of the GDP series for the period 1861–1989 by lowering the initial levels of the ISTAT-Vitali GDP series, and then Rossi et al. (1993) reconstructed the GDP series from the expenditure side for the period 1890–1990 using the new benchmark for 1911 and the new estimates provided, respectively, by the Bank of Italy (Golinelli and Monterastelli 1990; Rey 1991). Recently, Fenoaltea (2005a, b, 2006) has produced, for the period between national Unification and World War I, the first entirely new estimates of Italian aggregate GDP since ISTAT-Vitali by combining Federico (2003) series for agriculture with his own series for industries and services. Finally, Baffigi (2013) has provided new GDP series, together with supply and demand side estimates, covering the 150 years after Italy’s political Unification as a part of the “150 anni” national accounts project that includes the Bank of Italy, ISTAT and the University of Rome “Tor Vergata”—as well as academics from other institutions.Footnote 1
 The availability of historical macroeconomic time series has favoured a proliferation of studies, especially in recent years, on the nature and causes of business cycle fluctuationsFootnote 2 and—in a historical perspective—on growth in Italy.Footnote 3 As a result, in the last two decades the knowledge about the historical patterns of Italian economy over such a long time span has considerably improved. From this point of view, the availability of new data provides the researcher with the opportunity to look at old facts and interpretations in new ways, and to answer the question to what extent these new estimates provide contrasting results about growth and cycles with respect to those presented in the previous literature, especially in the pre-World War II period. In this paper we revisit the Italian economic growth and cycles in the post-Unification period using the new historical accounts presented in Baffigi (2013) for the period 1861–2011. In particular, we analyze whether the revised estimates provide new evidence as to the presence and number of structural changes in GDP growth that can suggest a different alternative interpretation of the phases of Italy’s long-run economic growth, especially with regard to the interpretation that traces back Italy’s economic development to the beginning of the nineteenth century. Specifically, following the recent literature supporting the view that macroeconomic time series, and GDP in particular, can be represented by stationary fluctuations around a (deterministic) segmented trend (e.g. Rappoport and Reichlin 1989), we test for multiple structural breaks at unknown dates in the trend function of GDP growth rate as suggested by Bai and Perron (1998, 2003). Moreover, since the new dataset also provides us with new estimates for both the supply and demand sides, this paper presents an analysis of the impact of these revisions to the overall pattern of the various supply and demand components’ contributions to real growth. As regards the analysis of the underlying cyclical component, we take into account both the classical and modern definitions of business cycle, where the first analyzes the specific patterns of individual cycles and the latter the co-movements between GDP and different key macroeconomic variables. Although most macroeconomists share the real business cycle (RBC) view that the “business cycles are all alike” (Lucas 1977), we try to go beyond this approach by integrating modern RBC analysis with a detailed description of individual phases and cycles according to the National Bureau of Economic Research (NBER) traditional approach.Footnote 4 The aim of our methodology is to provide empirical evidence on regularities and discontinuities in economic fluctuations and to reconsider the hypothesis of business cycles similarity of the predominant view. In particular, following the modern definition of business cycle,Footnote 5 after isolating the underlying cyclical component corresponding to fluctuations of approximate length between 1.5 and 8 years, we analyze the empirical regularities observed in the co-movements among different aggregative time series. Moreover, following the Burns and Mitchell (1946) classical definition of business cycle analysis, which suggests to look at the characteristics of each individual cycle and phase in terms of duration, amplitude and steepness, we also check whether the revisions alter the key features of business cycle fluctuations, including the dating of turning points. The paper is organized as follows. Section 2 describes the new dataset and provides a brief comparison with previous estimates. The long run dynamics of Italian economic growth is analyzed in Sect. 3 by identifying structural breaks in GDP growth rate, along with the contribution of the various supply and demand components to economic growth. Section 4 presents the results of the analysis of the cyclical component of Italian GDP using both the classical and modern definitions of business cycle. Finally, Sect. 5 concludes.",10
1.0,1.0,Italian Economic Journal,19 February 2015,https://link.springer.com/article/10.1007/s40797-015-0013-8,A Test of Narrow Framing and its Origin,March 2015,Luigi Guiso,,,Male,Unknown,Unknown,Male,"It has been argued recently that “narrow framing”—a tendency of people to evaluate a risky prospect in isolation rather than mixing it with the other risks they face—may be much more important in explaining decisions than has been thought so far. For instance, Barberis et al. (2006) argue that narrow framing can explain why individuals turn down small, independent gambles with a positive expected return. This, not only is at odds with standard expected utility preferences (Rabin 2000), but would be hard to reconcile even with preferences characterized by loss aversion when, realistically, individuals face other risks as well. The reason why this is so is that, as Kahneman (2003) observes, narrowly framed decisions depart far more from risk neutrality than those that are made in a more inclusive context. In fact, by focusing only on the one specific prospect, one ignores the risk-diversification properties that the prospect may have when considered together with other risks, making the specific prospect less appealing. In other words, narrow framing inhibits one from taking advantage of diversification opportunities, which by definition require joint evaluation of risky prospects. As shown by Barberis and Huang (2006), narrow framing is a critical ingredient for the behavioral approach to the equity premium puzzle; and Barberis et al. (2006) show that narrow framing can help explain one important puzzle in finance—the fact that contrary to the standard portfolio model a substantial fraction of individuals do not invest in stocks (Mankiw and Zeldes 1991; Haliassos and Bertaut 1995). Furthermore, Brown et al. (2008) show that narrow framing can in principle explain a long-time old consumption puzzle: the under-annuitization puzzle. That is the fact that very few consumers annuitize their retirement savings contrary to the prediction of standard consumption models that most should. Brown et al. (2008) also provide experimental evidence that narrow framing actually explains the puzzle.Footnote 1
 While the potential consequences of narrow framing are starting to be better understood and models that incorporate narrow framing into tractable preference specifications are beginning to appear (Barberis and Huang 2006), empirical tests of narrow framing and even more so of its origin are still lacking. In this paper I provide a test of narrow framing in a large sample of individual investors interviewed in 2007 and offer some evidence of what can lead individuals to frame choices narrowly, drawing on a few recent theories. The strategy of the test is simple and relies on the idea that the prevalence of narrowly framed decisions is an effect of “accessibility” (Kahneman 2003)—that is, of the ease with which mentally recorded information that may be relevant to the assessment of a risky prospect is recalled (Higgins 1996). My strategy consists in facing individuals with a decision concerning a small investment (or lottery) with positive expected value while randomly changing the degree of accessibility. To achieve this, sample participants were asked to think about and report the probability distribution of their future earnings. This provides information about the human capital risk they face, which they would probably find it desirable to hedge. Half of the individuals in the sample, randomly chosen, were asked to make the choice concerning the small investment before the questions about labor income expectations were asked. The other half, instead, made the decision immediately after the probability distribution of their future earnings was elicited. Hence, when deciding about the small investment, individuals in the second group have a greater accessibility to their human capital risk so their decision should be less subject to narrow framing. In fact, since the small investment is by construction independent of the human capital risk, it should provide diversification against the latter. Individuals with readier access should recognize this benefit and thus be less likely to turn down the investment. I find that in both groups, most individuals reject the small lottery, in concordance with a substantial body of experimental evidence (Kahneman and Tversky 1979; Tversky and Kahneman 1981, 1992). However, individuals that have easier access to their human capital risk are significantly less likely to do so: the difference in the rejection rate between the two groups is more than 8 %, or about 13 % of the sample rejection rate. This result is invariant to adding controls for individual demographic characteristics and attitudes such as risk aversion. In our sample, not all individuals face income uncertainty, at least over the one year horizon for which they are asked to give their earnings probability distribution. Since the diversification benefits of the small independent lottery are greater for those who actually face income uncertainty, one should find that those with earnings risk are less likely to turn down the lottery, if they decide under partial narrow framing. In fact, I find that those who face uncertain incomes are less likely to refuse. However, income uncertainty has no effect for those who were asked to decide about the lottery before reporting their human capital risk while it is negative and significant for those who have decided after their labor income risk was elicited. This result is consistent with the first group framing the decision about the lottery narrowly because when deciding they fail to bring to mind the other risks they face. The second group, instead, thanks to readier accessibility of income uncertainty, can at least partially take advantage of the diversification possibilities offered by the lottery. One important issue is what features of decision-making trigger narrowly framed choices. Kahneman (2003) argues that narrowly framed decisions are a consequence of individuals’ relying on intuitive thinking rather than reasoning (on the two cognitive processes see Stanovich and West 2000, 2002). Since intuitive thinking is immediate and highly dependent on the specific elements available (Stanovich and West 2000, 2002; Hammond 1996), individuals who rely on intuition will depend more on the most accessible elements. Hence, according to this view, an intuitive thinker will focus mostly on the specific features of the lottery and tend to assess its value in isolation from other risks whose relation to the lottery is not immediately accessible. On the contrary, individuals that rely on reasoning will be more receptive to less accessible information and be more likely to become aware of the diversification benefits offered by the small lottery. Thus their decision should be more responsive to accessibility. It has also been argued (Barberis and Huang 2006) that narrow framing could reflect regret at possibly having made a poor choice compared to a better alternative (Roese and Olson 1995). Since regret comes from comparing the consequences of a specific action with those of a verifiable alternative it leads people to focus on the outcomes of the action itself and ignore their contribution to overall wealth. Hence, regret-prone individuals should be more likely to frame the small lottery narrowly and turn it down. The UCS survey collects information on whether individuals rely mostly on intuitive thinking or on systematic reasoning when making decisions as well as on their propensity to regret. When I control for regret I find that regret-prone individuals are more likely to reject the lottery. When I sort the sample according to thinking mode, I find that the probability of turning the lottery down is lowest for those who rely mostly on reasoning and for whom the probability distribution of their future earnings is more readily accessible. For those who rely on reasoning, access to their income risk lowers the probability of turning the lottery down by as much as 12 % points, about 20 % of the sample rejection rate. By contrast, making their human capital risk more accessible to individuals who rely on intuition has no perceptible effect on the decision. This evidence is consistent with narrow framing being triggered by the decision mode and amplified by regret. Several papers report evidence that is consistent with the existence of narrow framing in experimental settings, starting with Tversky and Kahneman (1981), who show that individuals when offered to choose between two pairs of concurrent risky prospects decide by comparing single pairs, not the combined outcome of the decisions, and end up choosing a dominated combination. Read et al. (1999) survey this literature and provide various examples from different domains that are suggestive of narrow framing, including Sabini and Silver (1982), Camerer et al. (1995), Thaler et al. (1997), Thaler (2000). None of these papers, however, uses narrow framing to explain \(why\) people turn down small lotteries when they face other pre-existing risks. Most importantly, this contribution differs from previous ones because I use predictions from emerging theories to inquire into the cause of narrow framing, thus strengthening the interpretation. Most recently, Rabin and Weizsacker (2009) in a remarkable study revisit and extend (Tversky and Kahneman 1981) example and show that the tendency to choose dominated lotteries when offered a set of concurrent risky prospects is much more general than one may be led to conclude on the basis of Tversky and Kahneman (1981) and other similar examples. In what is probably the closest paper to this one, they show both in an experimental setting and using a large representative survey, that a majority of people choose dominated strategies when prospects are presented in isolation. But this disappears when the joint distribution of these prospects is shown, consistent with narrow framing driving their decisions. Importantly, they also prove theoretically that even small degrees of narrow framing can result in people making dominated choices, provided preferences depart from CARA, a definitely mild requirement (e.g., Guiso and Paiella 2008). Like Rabin and Weizsacker (2009) we find evidence of narrow framing but in a different context, that of a small lottery and pre-existing risk. Unlike them we also test for the source of narrow framing by studying the role of two potentially important drivers: regret and thinking mode. Hence, the two studies should be viewed as complementing each other. This paper is also related to a recent strand of literature in economics that inquires into the determinants of departures from the predictions of standard expected utility models. In particular the work of Frederick (2005), Benjamin et al. (2006), Kirby et al. (2005), Dohmen et al. (2011), who focus on the role of cognitive ability in explaining aversion to small, beneficial risks and high discounting. Differently from these papers we focus on narrow framing and the role played by the individual’s prevalent decision process. At a more general level, the evidence I find that narrow framing can be traced back to individuals thinking mode is related to a new strand of literature both empirical (e.g., McClure et al. 2004; Sirigu et al. 2004; Breiter et al. 2001; Butler et al. 2013) and theoretical (e.g., Fudenberg and Levine 2006; Brocas and Carrillo 2008a, b) maintaining that fundamental preference parameters usually taken as given can be traced back to the architecture of the brain and the interplay between emotion (or intuition) and reasoning. The rest of the paper is organized as follows. Section 2 sets up a simple framework to illustrate how narrow framing can explain why people turn down small lotteries and show the role of thinking mode, regret and other factors. Section 3 describes the data and the test design. Section 4 presents the main results and Sect. 5 provides evidence on the role of theory-based determinants of narrow framing. Section 6 concludes.",5
1.0,1.0,Italian Economic Journal,14 February 2015,https://link.springer.com/article/10.1007/s40797-015-0010-y,Uncertainty and International Climate Change Negotiations,March 2015,Yiyong Cai,Warwick McKibbin,,Unknown,Male,Unknown,Male,"The rapidly increasing concentration of greenhouse gases in the atmosphere due to human activity is believed by many to be a key contributing factor in climate change (Field et al. 2011). The emission of greenhouse gases is generally considered as a market failure which requires global collective actions to adress (Hoel 1991; Uzawa 2003). Governments are well aware of the argument that if emissions of greenhouse gases continue to follow recent trends, the world may be at risk of catastrophic disasters in the decades to come. Nevertheless, global efforts towards greenhouse gas mitigation keep running into delay as is seen at the numerous meetings of the UNFCCCFootnote 1 Conferences of Parties. Although the public good aspect of climate change is at the forefront of most economic analyses of climate change, it is uncertainty that is another important characteristic of the policy problem. Many of the issues surrounding climate change are historically unprecedented. This makes formulating national policy extremely difficult. It also creates problems for analysing international policy in the usual context of a game being played between countries. The standard approach to policy choice is based on maximizing expected utility, but it becomes problematic for analysing climate policy choice when policymakers do not have a well defined prior distribution over possible outcomes (Kunreuther et al. 2013). In particular, the exected utility approach has difficulty explaining the failure of international climate policy coordination. While Ulph and Ulph (1997) shows that uncertainty facilitates coordination based on the theory of expected utility, this has been challenged by the laboratory findings of Barrett and Dannenberg (2012, 2014) that collective action fails when there is large uncertainty regarding the threshold for “dangerous” climate change. Various alternatives to expected utility maximization have been suggested for analyzing climate policies under uncertainty. For example, the “Limited Degree of Confidence” criterion maximizes a weighted average of the expected utility in all possible cases and the expected utility in the worst 1 % scenarios. The “Safety First” criterion maximizes the expected utility with the constraint that the probability of reaching some lower-bound utility is bounded (see Hall et al. 2012; McInerney et al. 2012). However, these approaches still require a prior distribution over possible outcomes, and both predict even more mitigation than maximizing expected utility. Other scholars have argued in favor of the non-probabilistic “maxmin” decision rule for maximizing the economic welfare for a worst case climate scenario (see e.g., Froyn 2005; Funke and Paetz 2011). Climate change is due to the cumulative nature of carbon concentration (Ulph and Ulph 1997; Weitzman 2012). Therefore, if the consequences predicted by many scientists are to be avoided, precautious decisions need to be made now to reduce greenhouse gas emissions before the probabilities are well known. This is exemplified in the 2010 BBC Radio 4 interview of former UK Prime Minsister Tony BlairFootnote 2: It doesn’t need to be certain for us to act ... if you find out 2030 or 2040 ‘that was a real problem, we should have dealt with that’, you’re going to pay a pretty heavy price in history. Under this degree of uncertainty, policymakers are faced with making decisions while knowing that the consequences of their policy choices under the worst case scenario if realized may be dire. The “maxmin” perspective of climate uncertainty justifies strong mitigation action (Funke and Paetz 2011), and again, fails to account for the stalemate of international climate policy. The problem facing policymakers is actually even more complex than the quote of Tony Blair suggests. Apart from climate uncertainty, policymakers are also faced with economic uncertainty surrounding the costs of carbon mitigation. This form of uncertainty is reflected by former Australian Prime Minister John Howard’sFootnote 3 2006 speech to the Business Council of Australia where he stated: (Ratifying Kyoto) could have damaged the comparative advantage this country enjoyed ... I do not intend to preside over policy changes in this area that are going to rob Australia of her competitive advantage .... From a practical point of view, the “maxmin” decision rule is not useful if it does not account for the costs of carbon mitigation, which play an important role in breaking the deadlock in international negotiation. An over-cautious mitigation policy could lead to a drain of public finance. In this paper we expand the “maxmin” approach to address both climate and economic uncertainties in the making of international climate policies. We follow van den Broek et al. (2003), Jiménez-Lizárraga and Poznyak (2007, 2012) to apply the theory of robust optimal control in a multi-player game context. We assume that policymakers’ preferences over economic outputs under climate change can be represented by some utility functions. Collectivism refers to the existence of a political regime under which policymakers set their policy instruments to maximize their joint utility, the “global welfare”. In contrast, unilateralism is the status quo in which policymakers are only concerned about their individual utility. In the process of negotiation, policymakers firstly choose whether to agree on collective action, and consequently formulate policies that secure the highest payoff in the least-favorable situation. This means that there will not be any regrets of policymakers under the worst-case scenario. The idea is modelled by introducing an additional player called “nature”Footnote 4 into the game between countries. Nature strategically chooses the least-favorable combination of climatic and economic events (see e.g., Zhou et al. 1996; Basar and Bernhard 2008; Basar and Olsder 1999; Hansen and Sargent 2008; Funke and Paetz 2011). If the policymakers increase efforts to mitigate, nature will decrease the impacts of climate change but increase the costs of mitigation, leading to the policymakers’ regret of over-mitigation. In contrast, if the policymakers decrease efforts to mitigate, nature will increase the impacts of climate change but decrease the costs of mitigation, leading to the policymakers’ regret of under-mitigation. This additional player confounds the policy response of countries in the climate policy game and thus captures policymakers’ concerns about worst-case climate and economic outcomes. With a very simple model that features the interaction between economic output, greenhouse emissions and climate policies, this paper shows that, in the context of the assumed goals of policymakers, collectivism is not generally optimal. Indeed, there are cases when unilateralism is superior for both carbon mitigation and economic loss minimization. Our proposed approach provides a plausible framework for explaining several mutual contrasting theories and empirical findings in the recent literature on uncertainty and climate policy negotiation (e.g., Barrett and Dannenberg 2012, 2014; Barrett 2013; Vasconcelos et al. 2013). It also provides an analytical framework that can be applied to numerical simulations of international climate policy games.",1
1.0,1.0,Italian Economic Journal,14 February 2015,https://link.springer.com/article/10.1007/s40797-015-0012-9,Exit and Voice: Yardstick Versus Fiscal Competition Across Governments,March 2015,Massimo Bordignon,,,Male,Unknown,Unknown,Male,"In a representative democracy, elections represent the fundamental way to discipline politicians. Bad or incompetent governments are thrown out of office and this threat forces them to behave in the interests of voters. Many observers, however, would agree that the electoral mechanism alone may not be powerful enough and that additional disciplining devices on politicians may be helpful. Not surprisingly, the economists’ main contribution to this debate has been to advocate more competition across governments. As competition across firms reduces extra profits in the market, so competition across governments would reduce political rents. This general idea has taken two main forms, aptly summarized by Albert Hirschman’s famous distinction between “exit” and “voice” (Hirschman 1970). According to the former, people may escape from too greedy a government either by migrating altogether, as in the Tiebout’s tradition, or more realistically, by transferring abroad their mobile assets (Brennan and Buchanan 1980). It would be difficult to overestimate the practical influence of this idea. For example, in the debate on the fiscal institutions of the European Union, tax competition among member countries is often defended on the grounds of its disciplining effects on the hefty European governments. But there is also a second version of the same idea. Competition across governments might also improve the information set of voters (Salmon 1987). With competing governments and correlated economic environments, citizens may engage in more relative performance evaluation (also known as “yardstick competition”) across politicians, using observations about the results of governments in other regions or other countries to infer something about the quality of their own governments, so reinforcing the disciplining effects of “voice”. According to its supporters, both globalization, with its increase in correlation across national economies, and increased media coverage converge in strengthening the practical relevance of this form of disciplining device. Indeed, tax and yardstick competition may also go hand in hand; in the EU, for instance, the increased integration of markets and politics, coupled with the increased mobility of factors, have certainly worked in the direction of deepening both forms of governmental competition. Tax and yardstick competition have been separately scrutinized at large in the economic literature, both theoretically and empirically (see below). Their link, however, has not been addressed with the same attention. Does tax competition support the informational advantages of yardstick competition? When both forces are at work, which are the predictions in terms of fiscal choices, political equilibria and citizens’ welfare? Surprisingly enough, these questions have never been raised in the literature, at least not in formal analyses. The only paper that briefly touches these issues is a work by Besley and Smart (2007).Footnote 1 However, they are concerned with the effects of several general fiscal restraints on voter’s welfare and as a result, they choose a modelling strategy that does not allow them to focus specifically on the interaction between the two forms of intergovernmental competition.Footnote 2 As the topic is relevant, it is instead important to address it explicitly, in a model where both tax and yardstick competition can be introduced and their effects compared. In the model of this paper, an incumbent politician takes fiscal decisions in a first period, hoping to be re-elected at the end of this period to run for a second term. Voters are “rational ignorant”; they do not know precisely the quality of their incumbent and do not have the same information that politicians have on crucial elements that affect the functioning of the public sector. This informational asymmetry offers “bad” politicians the opportunity, under some conditions, to mimic the good type in the first period and be reelected in the second. In this framework, we first introduce tax competition, which reduces the ability of politicians to tax capital income, and then yardstick competition, assuming the existence of a second economy, correlated to the first, that allows voters to compare the fiscal choices of politicians. The main results of the paper are as follows. First, we show that the effects of the two forms of intergovernmental competition on political equilibria are generally different. Intuitively, fiscal competition works by reducing the resources a “bad” government can lay his hands on; yardstick competition works by providing the voter with more information to choose between “bad” and “good” governments. As the two mechanisms are different, it is not surprising that they may produce different results. Indeed, if there is a general tendency, this points to a conflict between the two forms of competition. Tax competition, by constraining government’s choices on some tax tools, makes the signal (the tax rates) that voters could use to select between bad and good politicians through yardstick competition less informative. In the model, this translates into a larger set of parameters that supports pooling equilibria (between good and bad governments) under tax competition. Second, concerning welfare, yardstick competition is certainly beneficial to voters, as long as it does not change the political equilibrium, while the effects of tax competition are generally ambiguous. But there is however a sense in which the two forms of government competition can be thought of as complementary. Yardstick competition tends to be beneficial to voters when bad politicians “pool” in the first period, as they are more easily found out; tax competition tends to be beneficial to voters when bad politicians “separate” in the first period, as voters are less exploited. Putting both forms of governmental competition together it might well then be that their joint effect on consumer’s welfare may turn out to be positive, despite their conflicting effects on political equilibria. Yardstick competition was introduced in economics by Salmon (1987) and first formalized by Besley and Case (1995), that also offer an empirical application to the USA. A theoretical analysis is in Bordignon et al. (2004) and an empirical application to Italian municipalities is offered by Bordignon et al. (2003). A textbook treatment is in Besley (2006). Kotsogiannins and Schwager (2008) offer a recent theoretical example, by enquiring on the effect of equalization grants on political equilibria. Revelli (2005) offers an interesting recent empirical application to local media markets in the UK. The literature on tax competition is huge. Wilson (2006) offers a recent survey. The question of whether tax competition is beneficial or not, and of its effect on political equilibria, was first raised by Wilson (1989) and Edwards and Keen (1996). A recent interesting theoretical example is Eggert and Sorenson (2008) who argue that tax competition leads to too low tax rates on capital even when politicians only wish to accumulate rents. Empirical studies on tax competition abound. Trannoy et al. (2007) offer a recent example. The rest of the paper is organized as follows. Section 2 sets up the model. Section 3 derives political equilibria, considering both the case with and without tax competition. Section 4 introduces yardstick competition. Section 5 compares the different mechanisms and derives the basic result of this paper. Section 6 discusses welfare effects. Section 7 concludes.",3
1.0,1.0,Italian Economic Journal,12 February 2015,https://link.springer.com/article/10.1007/s40797-015-0009-4,"Samuelson, Keynes and the Search for a General Theory of Economics",March 2015,Roger E. Backhouse,,,Male,Unknown,Unknown,Male,"The idea of a general theory of global applicability has a powerful appeal to economists. Maynard Keynes sought to justify his theory as the general theory of employment, interest and money, whilst in the postwar period the prestige of general equilibrium theory rested to a considerable extent on its claim to generality. Postwar debates over Keynes and “the classics” were, to a considerable extent, attempts to show that one theory was general and the other a special case, applicable only if local circumstances were right. Keynesian economics thus provides an interesting test case to discuss the the relationship between global and local approaches to economic analysis.Footnote 1
 The main subject of this paper is, however, not Keynes, but Paul Samuelson, the economist who arguably dominated American economics in the 1950 and 1960s. As far as many economists were concerned, Foundations of Economic Analysis (Samuelson 1947) virtually defined how to do economic theory rigorously: it presented the theory of constrained maximisation; it showed how to use methods of linear algebra to derive comparative statics results from such models; it explained how to formulate and analyse economic dynamics; and its appendices developed many of the mathematical techniques that economists needed to know. The ultimate in theoretical rigour might involve the mathematics of convex sets and other techniques not found in Foundations but such methods were used by only a tiny minority of economists. Published the following year, Economics: An Introductory Analysis (Samuelson 1948) was different. It focused on the so-called “Keynesian cross”, the diagram showing equality of aggregate saving and investment which even adorned the book’s cover but its appeal to students lay as much in its style and the way discussions of institutions—households, business and government—linked economic analysis to things that students could understand. While writing the book, Samuelson repeatedly apologised to his friends that the book was very “institutional”. The book virtually swept the board in elementary economics teaching in the United States, so much so that it was later claimed that all introductory economics textbooks were clones of Samuelson. The aim of this paper is to argue that the literature contains three very different conceptions of what it means to search for a general theory of economics. Failure to recognise this has caused economists to misunderstand the ideas of one of the most important figures in the discipline since the Second World War.",32
1.0,2.0,Italian Economic Journal,14 February 2015,https://link.springer.com/article/10.1007/s40797-015-0008-5,"Augusto Graziani: Theoretician, Applied Economist, Historian of Economic Thought—An Appraisal",July 2015,Lilia Costabile,,,Female,Unknown,Unknown,Female,"Augusto Graziani (Naples, 4 May 1933–Naples, 5 January 2014) was one of the leading Italian economists of his generation. He brought to economics the gifts of an analytically-oriented mind, crystal-clear logical reasoning, flawless erudition, and a passionate interest in the political and policy implications of both theoretical models and empirical analysis. He contributed to economic theory, applied economics, and the history of economic thought. After a presentation of Graziani’s life and career (Sect. 2), in this article I illustrate the coherence among his writings in these three fields by investigating the links connecting his theoretical evolution (Sect. 3), his interpretation of the development of the Italian economy (Sect. 4), and his approach to the history of economics (Sect. 5). Central to Graziani’s investigations in the three above fields was his interest in capital accumulation within the dynamic evolution of economic systems: this interest inspired his interpretation of general equilibrium theory and the controversies surrounding this approach in the 1960s, as well as, later on, his inquiries into the nature of capital and the origins of its ownership and control. This line of inquiry led him away from general equilibrium analysis, although, as we shall see, he always maintained an attitude of great respect for this theoretical approach. His theory of the monetary circuit—the arrival point of his intellectual trajectory—models the market economy as a sequential, hierarchical system in which only selected agents are able to influence strategic variables, such as the rate of capital accumulation, the levels and sectoral compositions of both output and employment, and the distribution of income. Graziani’s theoretical evolution is also reflected in, and inspired by, the simultaneous evolution of his empirical work. While his reading of post-war economic growth in open economies during the ‘miracle’ years emphasised dynamic factors such as technical change and productivity growth in response to the international integration of markets, his vision gradually became more focused on the conflicting strategies of macro-groups and their consequences on aggregate outcomes, with particular reference to the evolution of the Italian economy and its integration into Europe. Finally, both in Graziani’s theoretical writings and in his studies on the history of economic thought, references to the work of other economists also evolved from his initial concentration on general equilibrium theorists to interest in a more complex group including the founders of the theory of a ‘monetary economy of production’, the theoretical approach that he came to regard as most congenial to his own viewpoint, and to the development of which he creatively contributed. Besides illustrating the coherence among the various lines of Graziani’s inquiry, I shall also try to answer the question of whether his intellectual trajectory is characterised by continuity or discontinuity. While recognising change in his analytical framework, I argue in favour of a basic continuity in his thought. In the remaining part of this Introduction, before illustrating Graziani’s life, career and intellectual profile in the following sections, let me just very briefly mention some personal memories. Augusto—his ironic smile hovering at the corner of his mouth—once told me that commemorations provide their authors with a splendid opportunity to talk about themselves on the pretext of describing aspects of the commemorated person’s intellectual and personal relationships. He was critical, and at the same time tolerant, of human vanity. With that ironic smile in my mind, I shall keep references to my intellectual collaboration with him to the minimum.Footnote 1
 Graziani’s intellectual prestige was evident. I remember young and old economists alike, who had been lingering to chat in the corridors at a Conference held in 1978, hastening back to the conference hall when they heard that Graziani was about to speak. And he rarely disappointed his audience: whether or not the listeners accepted his propositions, which often challenged the ‘conventional wisdom’, his full control of the analytical aspects of the issues involved, his logical arguments, and his limpid style made his speeches a memorable experience, and provided the audience with invaluable food for thought. One of the greatest lessons he gave to his pupils and colleagues was disinterestedness: by which I certainly do not mean that he was a neutral or disengaged actor or observer. Rather, I mean that he never let his personal interests restrain the freedom of his thought. In other words, he was an intellectual in the noblest meaning of the word: the kind of figure that we most need today.",3
1.0,2.0,Italian Economic Journal,25 November 2014,https://link.springer.com/article/10.1007/s40797-014-0001-4,Finance-Dominated Capitalism and Income Distribution: A Kaleckian Perspective on the Case of Germany,July 2015,Eckhard Hein,Daniel Detzer,,Male,Male,Unknown,Male,"It is by now widely agreed among both heterodox and some orthodox authors that the financial and economic crises, which started in 2007, were caused by changes in income distribution over the previous decades and the emerging current account imbalances at the global and at regional (Euro area) levels, apart from malfunctioning deregulated financial markets.Footnote 1 These developments have been determined by policies aimed at deregulation and liberalisation of labour, goods and financial markets, both at the national and the international level, and the reduction of government intervention into the market economy and of government demand management. This broad policy stance may be called ‘neo-liberalism’, describing the policies implemented—to different degrees in different capitalist economies—since the late 1970s/early 1980s or later. ‘Financialisation’ or ‘finance-dominated capitalism’—we use these terms interchangeably—is interrelated and overlaps with neo-liberalism.Footnote 2 Epstein (2005a, p. 3) has presented a vague but widely accepted definition, arguing that ‘\([{\ldots }]\) financialization means the increasing role of financial motives, financial markets, financial actors and financial institutions in the operation of the domestic and international economies’. The features of financialisation or finance-dominated capitalism are wide ranging and have been described and analysed extensively and in detail by many authors. Detailed empirical case studies have been presented by, for example, the contributions in Epstein (2005b), and by Krippner (2005), Orhangazi (2008a, b), and (Palley (2008, 2013), Chapter 2) for the US, by Stockhammer (2008) for Europe, by Van Treeck (2009) and Van Treeck et al. (2007) for Germany as compared to the US, and recently and more extensively by Detzer et al. (2013) for Germany.Footnote 3
 From a macroeconomic perspective, Hein (2012) has claimed that finance-dominated capitalism can be characterised more precisely by the following elements. With regard to distribution, the dominance of finance has been conducive to a rising gross profit share, including retained profits, dividends and interest payments, and thus a falling labour income share, on the one hand, and to increasing inequality of wages and rising top management salaries, on the other hand. The major reasons for this have been falling bargaining power of trade unions, rising profit claims imposed, in particular, by increasingly powerful rentiers, and a change in the sectoral composition of the economy in favour of the financial corporate sector. Regarding investment in the capital stock, financialisation has been characterised by increasing shareholder power vis-à-vis management and workers, an increasing rate of return on equity and bonds held by rentiers, and an alignment of management with shareholder interests through short-run performance-related pay schemes such as bonuses and stock option programmes. On the one hand, this has imposed short-termism on management and has served to decrease managements’ animal spirits with respect to real investment in the capital stock and the long-run growth of the firm and to increase the preference for financial investment, which generates high profits in the short run. On the other hand, it has drained internal means of finance for real investment purposes from corporations, through increasing dividend payments and share buybacks, in order to boost stock prices and thus shareholder value. These ‘preference’ and ‘internal means of finance’ channels have each had partially negative effects on firms’ real investment in the capital stock, and hence also on the long-run growth potential of the economy to the extent that productivity growth is capital embodied. Regarding consumption, the dominance of finance has generated increasing potential for wealth-based and debt-financed consumption expenditures, thus creating the potential to compensate for the demand-depressing effects of financialisation, which were imposed on the economy via redistribution and the impact on real investment. Stock market and housing price booms have each increased notional wealth against which households were willing to borrow. Changing norms (conspicuous consumption, ‘keeping up with the Joneses’), new financial instruments (credit card debt, home equity lending), and deterioration of creditworthiness standards, triggered by debt securitisation and ‘originate and distribute’ strategies of banks, made credit increasingly available to low income, low wealth households, in particular. This allowed for consumption to rise faster than the median income in several countries and thus to stabilise aggregate demand. But it also generated increasing debt-income ratios of private households and thus increasing financial fragility. The deregulation and liberalisation of international capital markets and capital accounts have created the potential to run and finance persistent current account deficits. Some countries could therefore rely on debt-led soaring private consumption demand as the main driver of aggregate demand and GDP growth, generating and accepting concomitant rising deficits in their trade and current account balances. Other countries focussed on mercantilist export-led strategies as an alternative to generating demand in the face of redistribution at the expense of (low) labour incomes, stagnating consumption demand and weak real investment, and have hence accumulated increasing surpluses in their trade and current account balances. However, this constellation generated the problems of increasing foreign indebtedness and international financial fragility. In this paper we will focus on the first and most basic element of financialisation viewed from a macroeconomic perspective: We will provide a deeper investigation of the long-run effects of financialisation on income distribution in one of the major mercantilist export-led countries, namely Germany, before the crisis. The focus will be on the period of finance-dominated capitalism, which is supposed to have started in the early 1980s in the US and other countries, but considerably later in Germany. As analysed in more detail in Detzer et al. (2013) and Detzer (2014), the most important changes in the German financial sector which contributed to an increasing dominance of finance took place in the course of the 1990s: in 1991 the abolition of the stock exchange tax, in 1998 the legalisation of share buybacks, in 2002 the abolition of capital gains taxes for corporations, and in 2004 the legalisation of hedge funds, among others. At the same time, many of the big banks shifted their activities from traditional commercial banking towards investment banking and the German company network was increasingly dissolved. With those changes, a much more active market for corporate control emerged, along with the establishment of new financial actors, such as hedge funds and private equity funds. We would thus expect to see some effects of financialisation on income distribution starting in the early/mid 1990s. The paper builds on the more general examination of the effects of financialisation on income distribution in Hein (2014a) who provides a Kaleckian theoretical framework and a literature review on the general empirical and econometric evidence related to the channels through which financialisation should have affected functional income distribution, in particular, according to this theoretical approach. In Sect. 2 we will start with an empirical overview of different dimensions of (re-)distribution in Germany: functional distribution, personal/household distribution and finally the share and composition of top incomes. Having reviewed the empirical developments of several indicators of income distribution, we will then focus on functional distribution, because, according to Atkinson (2009), the development of functional income distribution is important for the other dimensions of distribution, as well as for the macroeconomic effects of distributional changes. In Sect. 3 we will briefly reiterate the Kaleckian analysis of the main channels of influence of neo-liberalism and finance-dominated capitalism on the tendency of the labour income share to fall, as suggested by Hein (2014a). And in Sect. 4 we will then review the empirical evidence for these channels for the case of Germany, in particular. Section 5 will summarise and conclude. The scope of this paper is thus limited to providing a Kaleckian view on the long-run effects of financialisation on income distribution in Germany before the crisis. It will neither deal with any further effects of financialisation and re-distribution on consumption, investment and the German macroeconomy as a whole, nor will the financial and economic crises and the recovery in Germany be discussed. For these issues, the interested reader is referred to Detzer et al. (2013) and Detzer and Hein (2014), for example.",7
1.0,2.0,Italian Economic Journal,12 December 2014,https://link.springer.com/article/10.1007/s40797-014-0004-1,Gender Differences in Bank Loan Access: An Empirical Analysis,July 2015,Giorgio Calcagnini,Germana Giombini,Elisa Lenti,Male,Female,Female,Mix,,
1.0,2.0,Italian Economic Journal,03 December 2014,https://link.springer.com/article/10.1007/s40797-014-0003-2,Collaboration Between Firms and Universities in Italy: The Role of a Firm’s Proximity to Top-Rated Departments,July 2015,Davide Fantino,Alessandra Mori,Diego Scalise,Male,Female,Male,Mix,,
1.0,2.0,Italian Economic Journal,29 April 2015,https://link.springer.com/article/10.1007/s40797-015-0014-7,The Role of Leverage in Firm Solvency: Evidence From Bank Loans,July 2015,Emilia Bonaccorsi di Patti,Alessio D’Ignazio,Giacinto Micucci,Female,Male,Male,Mix,,
1.0,2.0,Italian Economic Journal,12 December 2014,https://link.springer.com/article/10.1007/s40797-014-0002-3,"Assessing Primary and Lower Secondary School Efficiency Within Northern, Central and Southern Italy",July 2015,Giuseppe Di Giacomo,Aline Pennisi,,Male,Female,Unknown,Mix,,
1.0,3.0,Italian Economic Journal,29 April 2015,https://link.springer.com/article/10.1007/s40797-015-0016-5,Addendum to the Preface of the March issue 2015,November 2015,Aldo Montesano,Alessandro Roncaglia,,Male,Male,Unknown,Male,,
1.0,3.0,Italian Economic Journal,24 April 2015,https://link.springer.com/article/10.1007/s40797-015-0015-6,Likelihood Ratio Test and Information Criteria for Markov Switching Var Models: An Application to the Italian Macroeconomy,November 2015,Maddalena Cavicchioli,,,Female,Unknown,Unknown,Female,"In this paper we consider Markov switching vector autoregression (in short, MS VAR) models and deal with the problem of model specification. Despite the interest of applied financial and economic researchers in these models and the abundance of empirical works, there have been no clear guide on the issue of model selection in order to verify adequacy of the model to the data. In other context, such as regression-type models, a variety of tests is routinely performed, so that it seems natural to demand same standards for MS models. The first issue to be considered relates to estimation and filtering techniques. A fundamental contribution in this field is due to Hamilton (1990, 1993). In his work the focus is on expectation maximization (EM) algorithm. The main limitation of this procedure is the implementation in the case of models with autoregressive dynamic, which conduces to some problems. This depends on the fact that the log likelihood is typically multimodal and convergence problems might appear. Thus, any optimization algorithm, including EM, may converge towards a local maximum or even a saddle point. Furthermore, starting values can have a profound impact on which local optimum is selected. Today there are no methods which guarantee to find the maximum likelihood estimators (MLE), but the best advice available is to start the optimization algorithm from several different (possibly random) points in the parameter set. A further drawback of the EM algorithm is its rate of convergence, which is only linear in the vicinity of the MLE. Various modifications of the basic algorithm have been proposed in the literature to improve it (see, for example, Bickel et al. 1998 and references therein). However, little has been published at present on which of these modifications perform well. For practical implementation Krolzig (1997) relied on EM algorithm in conjunction with filter recursions (Baum–Lindgren–Hamilton–Kim, in short BLHK, filter, presented in Chp. 5, p.79) in order to determine smoothed probabilities and MLE of model’s parameter vector. A major improvement in estimation is due to Cavicchioli (2014b) in which explicit expressions of the MLE of MS VAR models and their corresponding limiting covariance matrices have been derived. The advantage is twofold. Firstly, the implementation does not require the EM algorithm. We need only the computation of the smoothed probabilities that can be done by iterating backward the BLHK filter, mentioned above. This reduces calculation times by as much as several orders of magnitude and overpasses the above problems arising from EM algorithm, improving computational performances. Secondly, classical test procedures could be implemented for the selection of lag order of the autoregressive polynomial as well as the linearity of some parameters of the model, assuming that standard asymptotic theory holds. However, a critical decision remains, that is, the choice of the number of regimes in the specification of the MS VAR. In fact, testing procedures on regime’s number suffer from non-standard asymptotic distributions of the likelihood ratio test statistic due to existence of nuisance parameters under the null hypothesis. In particular, when nuisance parameters are not identified under the null, the likelihood function is not locally quadratic with respect to these parameters under the null at the optimum and the corresponding scores are identically zero. As a result, the information matrix is singular under the null hypothesis and usual tests do not have an asymptotic standard distribution. Thus, we have two problems at hand. From one side the determination of the regime number, that is ultimately, the problem of linearity versus nonlinearity. From the other side, having set the number of regimes, we want to investigate parameters stability avoiding the use of EM algorithm, given the drawbacks discussed above. Concerning the first problem, the most employed methods for determining the state dimension are mainly based on either recursive complexity-penalized likelihood criteria (see, for example, Psaradakis and Spagnolo 2003, 2006; Olteanu and Rynkiewicz 2007; Rios and Rodriguez 2008) or on finite-order vector autoregressive moving average (VARMA) representations of the initial Markov switching models (see, for example, Krolzig 1997; Zhang and Stine 2001; Francq and Zakoian 2001; Cavicchioli 2014a). Furthermore, Hansen (1992), Garcia (1998), Cho and White (2007), and Carter and Steigerwald (2012) examined the distribution of the likelihood ratio statistics which is non-standard. However, their procedures can be quite involved to be implemented and, for this reason, not widely used in practice. Other likelihood ratio tests for parameter stability based on bootstrapping methods were proposed by Di Sanzo (2009). See also the class of optimal tests for the constancy of parameters in random coefficients models (including also Markov switching models) proposed recently by Carrasco et al. (2014). These autors used Bartlett-type identities for the construction of the test statistics, and bootstrap critical values. Another option is to conduct generic specification tests (precisely, for omitted autocorrelation, omitted ARCH, and omitted explanatory variables) developed by Hamilton (1996) on the hypothesis that an \(m\)-regime model accurately describes the data to avoid test indeterminancy. Finally, Huang (2014) avoids the problem of having nuisance parameters studying the special case of testing two regimes versus the alternative of one ultimate absorbing state. This paper responds to both questions. With regard to the second problem, we give simple formulae for likelihood ratio (LR) tests of various hypotheses of interest in the general case of MS VAR models to check parameters stability. Our formulae are in close forms and their implementation does not require the EM algorithm, improving computational performances. Moreover, it is shown that the maximum likelihood theory for MS VARs we develop generalizes the classical theory for linear VAR models described in Hamilton (1994), reconciling in an unified framework estimation and testing procedures for linear and non-linear VAR models. Furthermore, in response to the first problem, we give simple formulae to address the well-known problem of testing linearity versus nonlinearity or versus several states alternative with relatively simple methods which could be easily applied in practice. This relies on specifying penalized likelihood criteria using our close form expression for the likelihood which avoids burdensome and recursive likelihood computations. A simulation experiment shows the goodness of this approach. The rest of the paper is organized as follows. In Sect. 2 we introduce the MS VAR model and recall the matrix formulae for the MLE based on the results proved in Cavicchioli (2014b). Then we give an explicit formula in close form for the likelihood function of these models. In Sect. 3 we explicitly specify the likelihood ratio test in this context and propose various LR tests for several hypotheses of interest. Such tests require that the standard asymptotic theory for MLE holds and that the number of states is unaltered under the null hypothesis. Penalized likelihood criteria procedures for those hypotheses altering the number of regimes are given in Sect. 4. Section 5 is devoted to illustrate the presence of nonlinearities in Italian macroeconomic data. Finally, a brief summary and conclusion are given in Sect. 6.",1
1.0,3.0,Italian Economic Journal,23 July 2015,https://link.springer.com/article/10.1007/s40797-015-0020-9,How Good are Out of Sample Forecasting Tests on DSGE Models?,November 2015,Patrick Minford,Yongdeng Xu,Peng Zhou,Male,Unknown,,Mix,,
1.0,3.0,Italian Economic Journal,14 July 2015,https://link.springer.com/article/10.1007/s40797-015-0018-3,Five Crossroads on the Way to Basic Income. An Italian Tour,November 2015,Ugo Colombino,,,Male,Unknown,Unknown,Male,"In this paper, we explore the feasibility and the optimal features of a universal policy of income support in Italy. Abundant analyses and empirical evidence produced during the last two decades have documented the deficiencies, with respect to both efficiency and equity goals, of the Italian income support policies.Footnote 1 Three critical points concern the contingent interventions, e.g. unemployment benefits and Cassa Integrazione: (a) they are limited to certain occupational sectors and types of contract, thus generating social exclusion and processes of the insider–outsider type, (b) they are more aimed at preserving the job rather than the worker’s income and opportunities, thus discouraging the labour reallocation from unprofitable jobs to more promising ones, (c) some interventions go through a bargaining process involving firms, unions and local or central authorities, thus adding more sources of potential inequities. As to the structural anti-poverty interventions (e.g. Assegno Sociale, Assegno per il Nucleo Familiare, Social Card etc.), they are mainly aimed at supporting elderly or disabled people or low-income households with mean-tested/conditional/categorical benefits. Embodied in the personal income taxation system there are tax credits and child benefits that can also be classified as anti-poverty policies. It has been observed, however, that the design of the mean-tested tax credits and child benefits create distortions and bad incentives for labour market participations of married women (Colonna and Marcassa 2012). None of the above policies is universal: for example, Cassa Integrazione and Assegno per il Nucleo Familiare are limited to wage employees. A serious attempt to rationalize the income support policies in Italy took place in the second half of the 90s. In 1997 a governmental commission (Commissione Onofri) recommended the introduction of a universal minimum income mechanism both to contrast poverty and to favour the mobility of labour between firms and across occupations, as a crucial element for a new general design of the Italian welfare state (Onofri 1997). In 1998, Reddito Minimo di Inserimento (RMI)—a limited form of minimum income support—was introduced in a number of municipalities in order to test its organizational feasibility. However, in 2001 the Government decided to put an end to the RMI “experiment”. Meanwhile a partial constitutional reform had transferred the responsibility for social assistance from the central government to the regional authorities. This process, together with the unfavourable macroeconomic international conditions and a very high public debt, during the following decades discouraged further attempts to consider minimum income policies as a universal and nation-wide institution, despite the recommendations on the part of the European Community.Footnote 2 Since 2008, however, the “Great Recession” has put much stress on the current policies, confirming their shortcomings and stimulating a debate on the need for a reform. In 2014, 52 % of the employed under 25 has a precarious job (26.5 % in 2000). From 2008 to 2014, the unemployment rate increased from 6.7 to 13.0 %. In the same period, the relative (absolute) poverty in Italy increased from 11.3 (4.6) to 12.6 (7.9). The Italian system has been labelled Flex-insecurity (Berton et al. 2009), since it appears to suffer from the deficiencies of a deregulated—and yet inefficient—labour market and from the lack of a universal safety net. In 2012 and 2015 the Italian Parliament introduced changes in the income support policies for the unemployed. The reforms make some modest moves in the direction of universalism: the overall system, however, is still largely based on categorical principles.Footnote 3
 The Italian scenario is part of a wider picture. Besides the effects of the “Great Recession”, high unemployment rates and job insecurity are more fundamentally a byproduct of automation and globalization. Along with large potential gains, these processes also bring massive reallocations of activities, jobs and labour. There is evidence that the gains from automation and globalization end up in just a few hands and are likely smaller than they might otherwise be unless efficient redistribution mechanisms are implemented (Spence 2011; Standing 2012; Sachs and Kotlikoff 2012; Cowen 2013; Krugman 2013; Hughes 2014; Marchant et al. 2014; Brynjolfsson and McAfee 2014). The increasing need for redistribution stemming from globalization and technological progress is also the focus of the so-called “compensation thesis” (Rodrik 1998; Brady et al. 2005). Failing to design efficient means of redistributing the benefits may prevent some of the potential benefits from materializing. There are several types of income redistribution and maintenance policies, and the terminology used when discussing them is sometimes confusing. Guaranteed minimum income or minimum income guarantee policies envisage transfers that guarantee a minimum level of income. The transfers may be subject to some selection criterion (for example, only single mothers under age 25) or condition (such as means testing or work requirements). If there are no selection criteria, the policy is universal, and if there are no conditions, it is unconditional. The negative income tax guarantees a minimum level of income, but the size of the transfer depends on the person’s own income (means testing). In some implementations, negative income tax–like mechanisms might include a work requirement (such as requiring a minimum number of hours of work). There are also non means-tested transfers that are subject to behavioral conditions, such as sending children to school: these are referred to as conditional cash transfers. Unconditional basic income envisages unconditional transfers. When unconditional basic income is also universal (e.g. given to every citizen), it is sometimes called citizen income. The literature on welfare systems has suggested various typologies. Esping-Andersen (1990) addresses the general structure of welfare systems and defines three ideal types (liberal, corporatist and social-democratic). Italy’s welfare, together with Germany’s and France’s, is classified as corporatist, i.e. a system mainly based on contributory mechanisms and occupational–professional categorizations. However, both France and Germany, differently from Italy, do have a universal income support policy. Frazer and Marlier (2009) analyze more specifically the income support mechanisms and classify the European countries into four groups based on the prevalence of universalistic vs. categorical policies, the eligibility criteria and the generosity of the benefits. Based on analogous attributes, Jansova and Venturini (2009) define seven ideal types as fuzzy sets and compute scores that measure the degree to which the European countries belong to alternative types. The latter two analyses show that Italy—although belonging to the same corporatist type—adopt a very different income support policy with respect to France and Germany where, besides standard unemployment insurance, universal (means-tested) income support mechanisms are active since long. However, despite large differences in institutions, political constraints, combinations of monetary and in-kind benefits and levels of generosity, it is fair to say that, up to the early 90s, the social assistance policies of most industrialized countries were close to a more or less explicit means-tested guaranteed minimum income, with a very high implicit benefit reduction rate (the rate at which benefits are withdrawn as the recipient’s own earnings increase). During the last two decades, automation, globalization and their implications of the reallocation of jobs and skills, inflated the number of people in need of assistance and, in turn, the volume of social expenditure. The ageing of the population and the relative restriction of the labour force in developed countries made even more difficult the sustainability of the current social policies. Poverty-trap, stigmatization and marginalization problems associated with means-tested policies emerged as well. Many countries responded by moving toward less protection, and/or greater selectivity, and/or more sophisticated means-testing and eligibility conditions: reducing guarantees, increasing work incentives (through tax credits, wage subsidies, and behavioral requirements as a condition for receiving benefits), and narrowing the segment of the population qualifying for income support. While such policies have been partially successful for managing short-run income support programs and moderating poverty trap effects, it remains to be seen whether they meet the goal of implementing efficient mechanisms of global redistribution. Economists, social scientists and politicians divide into two positions: some (the majority) tend to support the direction that has so far prevailed in practice; others suggest universal and unconditional policies as the true solution (e.g. Standing 2011; Atkinson 2015). This paper aims at providing an empirical contribution to the debate, with focus on Italy. While in most European countries universal income support is—to some extent—already implemented, and the critical issue is how to overcome the deficiency of the classical welfare policies by moving either toward more sophisticated means-testing and behavioural conditioning or towards unconditional mechanisms, the specificity of Italy (and Greece) is the absence of a universal income support policy. We present an exercise in designing a universal income support mechanism that replaces the actual categorical policies and maximizes a given social welfare function subject to a public budget constraint. In principle, the formulation of the problem is provided by optimal taxation theory (e.g. Saez 2001, 2002). However, instead of looking for an analytical solution we adopt a computational–empirical approach. Namely, we use a microeconometric model and a social welfare methodology in order to explore and evaluate various alternatives mechanisms.Footnote 4 Adopting a microeconometric model that accounts for behavioural responses is appropriate since the crucial issues, when evaluating income support policies, concern the incentives upon household choices and their interaction with efficiency and distributional effects. In Sect. 2, we illustrate, with references to the relevant theoretical and empirical literature, the alternative possible attributes of a mechanism of income support along five dimensions. This analysis produces 30 alternative reforms that are described in detail in Sect. 3 and in the Appendix. In order to evaluate the reforms, we adopt a procedure that requires many steps. First, in Sect. 4, we develop a microeconometric model of household labour supply, based on a constrained utility maximization framework. Second, the model permits to simulate the new labour supply choices of household members when facing the new budget constraints induced by the alternative reforms. Therefore, for each alternative reform and for each household, we produce relevant results such as hours worked, income, taxes paid, benefit received etc. and ultimately the new attained utility level. The simulation is performed subject to two constraints: market equilibrium and fiscal neutrality (Sect. 5.2). More precisely, we endogenously identify the reform’s parameters that guarantee to satisfy the constraints. Third, the final evaluation is obtained by aggregating the (money-metric and interpersonally comparable) utility levels into four alternative social welfare functions: the pure utilitarian, the Gini Social Welfare (GSW) and two variants of the GSW augmented with poverty indexes (Sect. 5). Section 6 presents and comments the behavioural, fiscal and social welfare results. We also performs an analysis that allows us to identify the contributions of the reforms’ attributes on social welfare and its efficiency and equity components. Section 7 contains the conclusions.",7
1.0,3.0,Italian Economic Journal,16 July 2015,https://link.springer.com/article/10.1007/s40797-015-0019-2,Welfare Analysis of Tax and Expenditure Reform,November 2015,Alberto Pench,,,Male,Unknown,Unknown,Male,"Tax reform literature addressed a number of issues of theoretical and practical relevance along different lines of research: some works concerned single consumer economies thus focusing on efficiency issues while others introduced distributional considerations employing social welfare functions. From a practical point of view debated issues were the non optimality of an equiproportional taxation and the information requirements needed to implement a Pareto improving tax reform starting from an arbitrarily given tax structure. To cite only some works within the above mentioned streams of research the reader is referred to the works of Diewert (1978), Atkinson and Stiglitz (1980), Corlett and Hague (1953–1954), Yitzhaki and Thirsk (1990), Hatta (1977, 1986) and Ahmad and Stern (1984). Most of these papers neglected the use of tax revenue to finance public expenditure since they considered the substitution of one or more taxes with another or others or a compensating change in lump sum transfers. Notable exceptions were Guesnerie (1977, 1995) which explicitly considered a compensating change in public expenditure: a feature of such contributions is that the author focused on the problem of existence and on theoretical analysis of alternative tax and public good equilibria and explicitly denied any interest in canonical formulae for Pareto improving tax reforms expressed in terms of elasticities or other parameters. Other papers dealing with tax and expenditure reforms are found within the vast literature on marginal cost of public funds: Schöb (1994) and Slemrod and Yitzhaki (2001), to cite only two and, more recently, the exhaustive work by Anderson and Martin (2011) to which the reader is referred for detailed references. Within such a vast literature it is hard to find a gap to be filled and the present paper can be considered a reformulation and extension of some of the results presented in Anderson and Martin (2011) with a deeper emphasis on the side of public expenditure: the aim, in the tradition of tax reform literature, is to find conditions for a Pareto improving tax and expenditure reform in terms of complementarity and substitutability relations among private and public goods and labour but also to point out the characteristics public expenditure categories should possess to achieve such a result. In this respect the paper might be helpful in the difficult choice by the policy maker about where to concentrate expenditure cuts. The paper includes two sections which present, respectively, a basic version of the model and its generalization with several categories of public expenditure; a final section summarizes the results and briefly comments upon them.",
1.0,3.0,Italian Economic Journal,11 June 2015,https://link.springer.com/article/10.1007/s40797-015-0017-4,Public Sector Contribution To Competitiveness,November 2015,Vincenzo Patrizii,Giuliano Resce,,Male,Male,Unknown,Male,"Competitiveness is seen as a key indicator for Country’s economic potential. Conventional measures tend to concentrate either on the private sector, or to treat the economy as a peculiar large private company. Either way, factors cost and goods final prices play a major role in any measure of competitiveness.Footnote 1 A significant exception is provided by the Global Competitiveness Index which, by means of a large variety of indices, aims at including a wide set of factors, not directly observable in market operations, such as, for instances, people’s perception of obstacles in running business activities and in the functioning of institutions, public and private (World Economic Forum 2012).Footnote 2
 Here we follow this more general approach to competitiveness as it provides a framework in which to pose the question of how the Public Sector affects competitiveness. We depart, however, from the Global Competitiveness Index way of including public institutions due to its heavy reliance on “subjective” measures. Our contribution goes in the direction of devising a more objective way to link the working of public institutions to the Country’s economic performance. We follow the idea, put forward by Krugman (1994), that although competitiveness could have many and even opposing meanings they all have to be grounded on the concept of productivity. Indeed, World Economic Forum (2008), defines national competitiveness as a set of factors, policies and institutions that determine the Country’s productivity level. With productivity in mind the question of measuring Public Sector’s contribution comes to be very close to the old question of getting a measure for the “real” value of public expenditure. It is well known that National Accounts assume for public expenditure a one to one productivity in terms of final services and, therefore, uniform throughout the Country. Partial relaxation of this assumption is what characterizes this paper.Footnote 3
 Public expenditure can better be seen as the financial side of Public Sector provision of services in the economy, its “real” value is not the same throughout the Country as it varies significantly across areas: The North–South growth problem is a well-known and lasting example.Footnote 4 We need, therefore, measurements of Public Sector contribution to be area-specific, that is to be decomposable by area. In addition, taking also into account the quest for efficiency improvements in public expenditure, the overall measure of Public Sector productivity has to be decomposable in terms of policies’ objectives (type of public services). To make such measurements amenable, the first problem is that of isolating final services provided by the Public Sector, their cost and the government tiers responsible for provision. Once data are made available we estimate services’ productivity, and proceed to aggregate it at local (Provinces) level.Footnote 5 This provides the basic tool to arrive at an efficiency adjusted measure of public expenditure: a measure of its productivity. Its geographical distribution provides indicators of how Public Sector’s policies affect the Country’s overall and local productivity. Comparing the index across services and tiers of government helps identifying areas and causes of lack in productivity. The paper develops as follows: Sect. 2 describes the way public services are provided in Italy according to layers of government; Sect. 3 and the Appendix document the organisational aspects of provision and illustrate the different measures of outputs needed to account for the multidimensional nature of public services; Sect. 4 presents the basics of Data Envelopment Analysis (Dea) and the characteristics of a Slack Based Measure (Sbm) of efficiency integrated with Principal Component Analysis in order to make the multidimensional characteristic of the data manageable; Sect. 5 presents and discusses the results and Sect. 6 summarises and concludes.",12
1.0,3.0,Italian Economic Journal,24 July 2015,https://link.springer.com/article/10.1007/s40797-015-0021-8,The Great Beauty: Public Subsidies in the Italian Movie Industry,November 2015,G. Meloni,D. Paolini,M. Pulina,Unknown,Unknown,Unknown,Unknown,,
2.0,1.0,Italian Economic Journal,10 November 2015,https://link.springer.com/article/10.1007/s40797-015-0024-5,Note on Sustainability Prices When Sectoral Growth Rates Differ,March 2016,Carlo D’Adda,,,Male,Unknown,Unknown,Male,,1
2.0,1.0,Italian Economic Journal,14 September 2015,https://link.springer.com/article/10.1007/s40797-015-0023-6,Correlations Between Macroeconomic Cycles in the US and UK: What Can a Frequency Domain Analysis Tell Us?,March 2016,Patrick M. Crowley,Andrew Hughes Hallett,,Male,Male,Unknown,Male,"The contribution in this paper is to further analyse fluctuations in the components of US and UK growth and the interactions between the components of US and UK growth in the time–frequency domain, using multiresolution decomposition analysis and cross-correlation analysis, thereby extending the contribution of Crowley and Hughes Hallett (2015). These interactions, to date, have not been fully explored in the time–frequency domain with the full range of data that we now have available, although probably the initial attempt to study macroeconomic data using time–frequency analysis is Gallegati and Gallegati (2007). The approach presented here can illuminate the changing relationships between GDP components at different time horizons that conventional time series analysis has difficulty identifying. The time–frequency domain offers economists a different perspective from time-series analysis for analysing economic data. The number of contributions in economics that use frequency domain analysis is still small, and yet a number of important advances have been made in time–frequency domain methods which have not fully filtered into the mainstream economics literature. While time–frequency domain techniques are not yet part of the standard toolbox for analysis of time series in economics, these techniques are standard in other disciplines such as engineering, acoustics, neurological sciences, physics, geology and environmental sciences. We begin by noting the correlations for the US (1947 to 2015Q1) and the UK (1956–2015Q1) between the growth rates of the main components in aggregate demand in real GDP, namely personal consumption expenditures, private investment, government expenditures (both current and capital) and export of goods and services, are not that high and are often not significant and sometimes do not have the expected signs (Tables 1, 2; Crowley and Hughes Hallett 2014a). Based on static statistics, few of the correlations are significant (—using a Fisher test, 5 % significance levels are 0.119 for the US and 0.127 for the UK). Of course, this exercise ignores two important considerations: (i) that (possibly variable) lead or lag relationships exist and/or (ii) that (again possibly variable) cycle relationships at different frequencies might be significant between the constituent components of GDP, or within the cyclical composition of GDP, even if not in GDP as a whole. Clearly simple correlation coefficients are not going to reveal these relationships, and therefore more appropriate time–frequency domain tools are required to uncover these hidden factors. To address these considerations we use discrete wavelet analysis to analyze the shifting (time-varying) linear relationship between the components of GDP, both in the US and UK economies, in terms of amplitude (power) in each cycle, and for inter-component correlations and lead-lag relationships.",6
2.0,1.0,Italian Economic Journal,23 November 2015,https://link.springer.com/article/10.1007/s40797-015-0025-4,Firm Subsidies and the Innovation Output: What Can We Learn by Looking at Multiple Investment Inputs?,March 2016,Marco Cosconati,Alessandro Sembenelli,,Male,Male,Unknown,Male,"The importance of innovation for companies and—ultimately—for aggregate economic growth is now understood by policy makers.Footnote 1 In fact, innovation helps companies increase their productivity levels, enter new markets or stave off competition. It is now also commonly accepted that innovation comes in many different forms, ranging from a new product arising from R&D (product innovation) to efforts to incorporate innovative production equipment (process innovation), use new workplace practices (organizational innovation) or create new marketing concepts (marketing innovation).Footnote 2 At a policy level the diversity of innovation causes difficulties in understanding the process as a whole and–ultimately–in designing appropriate innovation policies and monitoring their effectiveness. In particular, most incentive schemes implicitly aimed at stimulating innovation output are designed with the main purpose of addressing market failures, such as externalities, imperfect information or coordination problems. Those are likely to affect specific factors, including R&D and fixed investment, which enter the innovation process as inputs. Obviously, this approach has solid economic foundations and, indeed, there is general consensus among economists that market mechanisms fail to provide the socially optimal level of R&D spending, basically because private firms are not able to fully capture all the profits arising from the results of their R&D activity. Government intervention in this area is thus justified from an economic point of view by the market failure aspect of R&D: because the social returns to private R&D are often higher than the private returns, some research projects would benefit society but would be privately unprofitable. By lowering the cost to the firm, a subsidy can make these projects profitable as well. A somewhat related economic argument might also apply to fixed investment since the existence of financial constraints in some disadvantaged areas can lead to a sub-optimal capital accumulation level which in turn could be corrected by the implementation of appropriate incentive schemes. Providing convincing evidence on the direct effect of public subsidies on R&D and fixed investment is an important issue since both types of firm activities are found in the literature to be major determinants of firm innovation activities and ultimately of a country’s growth prospects.Footnote 3 Still, this evidence is not conclusive since it is the effect of subsidies on innovation outputs–as opposed to innovation inputs–what matters the most. The ability of incentive schemes to allocate funds to the highest return projects should clearly be at the center of the literature, but there is still little direct evidence on this issue. More specifically, we do not know much about the effect of public subsidies on the pace of technological progress, although the role of intermediaries–including public bodies–in selecting entrepreneurs with the best chances of introducing new products or processes is a key mechanism through which GDP growth is affected. In this paper we contribute to this infant literature by providing empirical evidence on the impact of state aids on innovation through the increased incentives to invest in R&D and fixed investment. By doing so we allow for the possibility that process innovation–and to a lesser extent other forms of innovation as well– are introduced into the firm through gross investment in plants and equipments.Footnote 4 Also, by considering the two inputs jointly, we explicitly recognize the role that R&D investment may have in making possible and facilitating the absorption of innovations embodied in new capital goods purchased by the firm. To this end it is required, as it is not done in the existing literature, that we model both the relationships between each type of innovation output and innovation inputs and the relationships between each innovation input and (a combination of) state aid types.Footnote 5 We do so by using various waves of a rich survey on innovation at the firm level gathered by Unicredit’s Research Department for a large number of Italian firms.Footnote 6 This survey contains detailed categorical information on the introduction of process, product and organizational innovation. Moreover, it contains quantitative information on inputs of the innovation process at the firm level, such as R&D spending and fixed investment, and on the way they are financed. In particular, we know whether a firm has benefitted from grants, tax credits or soft loans to finance R&D or fixed investment, and also the contribution of each type of aid on total financing. Available information preclude us from focussing on specific incentive programs as it is usually done in the so-called “program evaluation” literature. Since one of the objectives of this paper is indeed to throw some light on how firms exploit multiple state aid opportunities and on how this affects innovation outputs this is not a limitation. From a methodological point of view our empirical approach requires the estimation of three sets of equations, each posing difficult econometric challenges. Compared to the vast majority of existing literature, an important advantage of the data set we use is that we have repeated cross-sections which allow us, in principle, to control for the firm specific and time invariant component of the error term and to avoid—at least partly—the standard endogeneity problems brought about by the non-observability of managerial quality. An additional econometric problem is that the identification of causal effects without making independency assumptions between firm level unobserved heterogeneity and the covariates requires lack of correlation between the regressors and the idiosyncratic error term at all leads and lags. This strict exogeneity assumption rules out the possibility that current values of some of the explanatory variables are correlated with present and past idiosyncratic errors. This is unlikely to be the case in the present context since, for instance, a firm level technology shock might be correlated both with R&D and fixed investment and with the probability of applying for and obtaining a public subsidy. Given the structure of our database, and indeed of most existing data-sets, there is no way of addressing this second concern in a fully satisfactory way.Footnote 7 We address these issues by estimating an “ expected subsidy” variable as a function of firm’s observable characteristics. According to our main identifying assumption such a variable is exogenous in the R&D and fixed investment equations.Footnote 8
 This paper is organized as follows. Section 2 provides a brief and selective review of existing literature and highlights the main novelties of our work. In Sect. 3, we describe the data set we use for our investigation and we present some empirical regularities on public subsidies, R&D and fixed investment, and innovation in our sample. Section 4 is the core of our paper where both the methodological approach and the empirical analysis are presented. Section 5 concludes the paper by summarizing the results and describing the additional questions that could be addressed in future research.",8
2.0,1.0,Italian Economic Journal,04 August 2015,https://link.springer.com/article/10.1007/s40797-015-0022-7,Comparing the Efficiency of Italian Public and Private Universities (2007–2011): An Empirical Analysis,March 2016,Tommaso Agasisti,Luisa Ricca,,Male,Female,Unknown,Mix,,
2.0,1.0,Italian Economic Journal,07 January 2016,https://link.springer.com/article/10.1007/s40797-015-0026-3,Bargaining Agenda and Entry in a Unionised Model with Network Effects,March 2016,Luciano Fanti,Domenico Buccella,,Male,Male,Unknown,Male,"Network industries are among the fastest-developing sectors of advanced modern economics. Typical examples of network goods are telephone and software: it is natural to observe that the utility of a particular consumer from using a telephone or a piece of software increases with the number of other telephone or software users. The large-scale expansions of mobile devices such as smartphones and tablets exemplify the increasing significance of those industries in day-to-day life.Footnote 1
 In general, network goods are products in which the utility derived by one consumer/user increases with the number of other consumers/users of those goods, that is, the total sales of the goods enhance the welfare of each consumer (Katz and Shapiro 1985; Amir and Lazzati 2011). In addition, the number of other consumers/users of the product may directly affect the demand for the network goods also because it may speak for the product quality and availability of after-sale services for long-lasting consumers. Therefore, the consumers’ expectations about the total sales of the goods may in principle be affected by different mechanisms of output decisions and different production costs and thus by different labour market institutions. Indeed, a central issue of labour market institutions in advanced economies is the selection of the bargaining agenda between firms and unions. Because of the interconnections between labour and product markets, the subject takes on considerable importance both for labour economics and industrial organization. The unionised firm literature presents the classical result that, when firms bargain only over wage and choose employment, i.e. according to the right-to-manage (RTM) model (e.g. Nickell and Andrews 1983), profits are higher than when they bargain also over employment, either simultaneously as in the efficient bargaining (EB) model (e.g. McDonald and Solow 1981) or sequentially as in the sequential efficient bargaining (SEB) model (Manning 1987a, b). 
Dowrick (1990) first analysed the issue of the more profitable negotiation agenda in the context of unionised industries; this author has found “that profits under the Right-to-Manage model exceed those under Efficient Bargaining” (Naylor 2003, p. 59). Moreover, the negotiation outcomes of the RTM and SEB agendas have also been compared, and the conventional result of the established literature argues that “under unionized monopoly, the firm will prefer to keep employment off the bargaining agenda, whatever the degree of union influence over employment. In other words, the Right-to-Manage outcome generates higher profits than either the efficient or sequential bargains, for a given level of union influence over the wage” (Naylor 2003, p. 61). This is true particularly regarding a unionised monopoly in which strategic competitive effects are absent and thus wage costs (depending on the specific alternative bargaining arrangements) can never be used as a strategic device. Here, we first consider the monopoly case in which the network effects on the preference of the agenda cannot be obfuscated, with regard to the firm, by indirect strategic competitive effects.Footnote 2 Second, we analyse the case of monopoly with the threat of entry in which the network effects influence the preference of the agenda also through the strategic effects due to potential competition in both the product and labour markets. The effects of firms’ strategic interaction and market entry on preferences over the bargaining agenda have been recently investigated by Bughin (1999), Vannini and Bughin (2000), Buccella (2011), (Fanti 2014, 2015) and Fanti and Buccella (2015). Moreover, it is natural to think that the choice of bargaining agenda may exert some influence on the behaviour of incumbents and entrants, although the investigation of this theme is rather scant. Nonetheless, the link between the presence of labour unions and the market structure and, consequently, the market entry, is rather relevant both on empirical (e.g. Chappell et al. 1992)Footnote 3 and theoretical grounds. In fact, from a theoretical perspective, a few articles have dealt with the presence of labour unions and entry such as Dewatripont (1987, 1988a, b), Ishiguro and Shirai (1998), Pal and Saha (2008), Mukherjee and Wang (2013). However, none of them have studied the choice of the bargaining agenda as an entry deterrence tool. Exceptions are Bughin (1999), Buccella (2011) and Fanti and Buccella (2015), in which the result that RTM dominates EB can be considered to be conventional in the pure monopoly, while in a monopoly with threat of entry, EB is predominant as a Nash equilibrium. However, despite their relevance, all of those authors abstract from the possibility that goods in unionised industries present positive consumption externalities. Indeed, the presence of network effects is not innocuous. For instance, a recent growing body of literature has shown that network externalities may alter many established results of the industrial organization literature obtained basically by assuming non-network goods, especially in relation to oligopolies with managerial delegation (e.g. Hoernig 2012; Bhattacharjee and Pal 2014; Chirco and Scrimitore 2013). The paper aims to consider the impact of the network effects on the preferences over the bargaining agenda first in the pure monopoly case (i.e., in the short run in the absence of firm competition) and then, in the long run, on the monopoly with the threat of entry when the strategic effects of potential competition are taken into account. In particular, the present paper attempts to answer the following questions: does the conventional result, that under pure monopoly a firm always prefers RTM to EB and SEB (with a corresponding conflict of interests between firm and union), hold true in the presence of network effects? What happens if there is the threat of market entry in the industry? May the incumbent still strategically select the EB agenda to deter entry? What are the consequences in terms of social welfare? The answers to these questions reveal that network effects may alter the established results of the previous literature. As regards monopoly, our novel findings show that network effects matter. In fact, in contrast to the established result that a monopolist always prefers RTM, a SEB agreement is preferred, provided that the network effects are sufficiently intense (and the union’s bargaining power is not too high). Because the union and consumers always prefer the SEB agenda, the presence of network effects may solve the traditional conflict of interests between parties and achieve a Pareto-superior societal outcome. We identify the following three effects of the network externalities related to the analysis of entry. First, when these externalities are sufficiently strong, the entry deterrence effect is always weakened, regardless of the bargaining agenda and the size of the fixed costs the entrant has to face. Second, as network externalities intensify, the entry deterrent agenda shifts from EB/SEB (in the case of low/high fixed costs of entry) to RTM. Third, for sufficiently high union bargaining power, a no univocal role of the network effect emerges. In fact, on the one hand, the presence of network effects allows the exploitation of the bargaining agenda for medium values of the network intensity; on the other hand, both relatively low and high values of the network effect facilitate the elimination of the agenda as a barrier to entry. With regard to social welfare, the main results are as follows. The duopoly under the SEB agenda is always the most preferred bargaining institution from the society viewpoint. Nonetheless, the possibility that the bargaining agenda can be used as an entry deterrence tool impedes the achievement of the first-best social welfare. In this case, the role of the network effect is extremely various. In fact, if the intensity of the network is extremely high, the achievement of the desirable social welfare outcome is possible. However, if the network effect is of medium intensity, the entry is more likely to be deterred under the RTM agenda, which leads to the unwelcome result of reaching the least social welfare level. Moreover, a number of testable hypotheses emerge from our analysis. In industries with sizable network externalities: (1) without the threat of entry, monopolists should prevalently choose an efficient bargaining arrangement, and (2) in the case of threat of entry: (2.1) when unions are strong, more competitive market structures should be prevalent both for low and high network effects; on the other hand, if the network effects are sufficiently strong but not too strong, a monopolistic structure should be relatively more often present, irrespective both of the size of the fixed costs and the type of bargaining agenda; (2.2) when unions are not strong, a monopolistic structure should be less common when network effects are very strong; and (2.3) the RTM should be the predominant bargaining agenda, especially when the fixed costs of entry are high. The remainder of the paper is organised as follows. Section 2 presents the basic monopoly-union bargaining model. Section 3 analyses the issue of potential entry and discusses the welfare implications. Finally, the last section summarises the main results and suggests directions for further research on the subject.",11
2.0,1.0,Italian Economic Journal,15 February 2016,https://link.springer.com/article/10.1007/s40797-016-0027-x,Can a Natural Economy Operate in Macroeconomy? A Caution for Deviation from Natural Economy,March 2016,Koji Akimoto,,,Male,Unknown,Unknown,Male,"This study examines whether a natural economy can operate in the macroeconomy. The fundamental problem issue of this paper is that a fundamental cause of economic crises, from which the current capitalist economy is suffering, lies in a large deviation from a natural economy. This leads us to ask if a mechanism exists in the macroeconomy through which the natural economy operates. An overview of the core issue needed to discuss a natural economy will first be offered. Two remarkable trends in the capitalist economy since the 1980s have been the rapid expansion of the world’s financial assets and the corresponding rapid GDP increase in high asset areas.Footnote 1 The expansion of money should be considered as destructive to the economy’s balance (see Akimoto 2014). From an economic theory viewpoint, this destruction is considered to be a deviation from macroeconomic equilibrium and therefore a deviation from a natural economy. However the natural economy is not a synonym for macroeconomic equilibrium. Nonetheless, a natural economy,—despite differences in definitions between various viewpoints, involves an economic equilibrium. Therefore, taking a contraposition, deviation from such an equilibrium can be considered a deviation from a natural economy. Akimoto (2014) discusses the high risks entailed in the deviation from natural economy and concluded that human cannot control the economy artificially ignoring natural economy Thus, one of the important problems remaining to be investigated is whether a mechanism by which the natural economy operates does in fact exists in the macroeconomy. At this stage, we should explore how the natural economy functions. As the economist Smith (1776) described, a market price fluctuates around the natural price. This fluctuation has two important implications. First, a natural economy constitutes the economy’s basic structure. Therefore, studying the natural economy means analyzing the basic laws that underpin and control the economy. Second, any large deviation from the natural economy entails high risk. Such risks therefore provide an important alarm for controlling the economy without rigorous rules. We cannot artificially control the economy by ignoring the natural economy. When we study the natural economy by Smith (1776), we should also analyze natural interest rates. After Smith, many economists tried to define a natural interest rate from macroeconomic viewpoint. First, we should highlight the definition by Wicksell (1898). Wicksell defined a natural interest rate as being neutral for a price level established in the real market. More precisely, it is the rate at which demand is equal to supply in the real market, making a capital market unnecessary. As is well known, this definition affected Keynes (1930). In his book, A Treatise on Money (1930), Keynes constructed the fundamental equation whereby a natural interest rate was defined as a rate that made investment equal to savings. At the
natural interest rate, the price level is equal to the monetary income per output paid for the production factors. However, a situation wherein investment equals savings implies dependences on the shapes of both investment and saving functions. For instance, if an innovation is expected to occur, the expectation of this innovation will change the investment function and the natural interest rate will increase. In this case, no information is given with respect to developments in the production process. However, it is absolutely certain that the natural interest rate depends upon the structure of the production process. 
Pasinetti (1981) proposed a different approach to that of Wicksell. Using an approach based on the theory of labour, Pasinetti concentrated on the structures of the production process and innovation. First, he began with the condition of full employment of labour and capital stock, using a vertically integrated analysis defined by the multi-sector model. Second, he introduced the concept of natural rate of profit and constructed a natural economy. Finally, he introduced financial assets to thenatural economy and proved the emergence of a rate of interest. After these preparations, Pasinetti defined the own-rate of interest for each commodity and rigorously analyzed the relation between the nominal rate of interest and real rates of interest (standard real rate of interest). After defining the concept of interest rates at each stage, he finally reached the definition of the natural rate of interest.Footnote 2
 From the above discussion, we see that the natural interest rates defined by Wicksell and Pasinetti are both representative and based on a macroeconomic equilibrium. Among these, we adopt the definition by Pasinetti because it depends on the structure of the production process. Therefore, the concept of natural economy in this study depends on definitions of both Smith (1776) and Pasinetti (1981). Although Pasinetti proved the existence of a natural economy, he did not demonstrate a mechanism through which a natural economy operates. Therefore, the aim of this paper is to show that this mechanism exists in a macroeconomy and to demonstrate the reasons that the mechanism holds. That is, if only one condition is not satisfied, the whole natural economy would collapse. To keep the macroeconomic viewpoint, we adopt the fundamental equation of KaldorFootnote 3 who is one of the most important descendent of Keyes. In addition, we introduce Riccardian price system. Riccardo is a descendent of Adam Smith. The main results we would like to propose are following two remarks. There exists a Nash equilibrium. The Nash equilibrium corresponds to the natural economy defined by Pasinetti (1981) and Smith (1776) constructs an balanced economic growth. Therefore, there exists a mechanism by which the natural economy can operate and capitalist economy can survive. The above conclusions demonstrate that we should take off the factors which obstruct the conditions under which the Nash equilibrium exists.",
2.0,2.0,Italian Economic Journal,22 February 2016,https://link.springer.com/article/10.1007/s40797-016-0028-9,The Drivers of Italy’s Investment Slump During the Double Recession,July 2016,Fabio Busetti,Claire Giordano,Giordano Zevi,Male,Female,Male,Mix,,
2.0,2.0,Italian Economic Journal,25 February 2016,https://link.springer.com/article/10.1007/s40797-016-0030-2,An Evaluation of the Policies on Repayment of Government’s Trade Debt in Italy,July 2016,Leandro D’Aurizio,Domenico Depalo,,Male,Male,Unknown,Male,"There is no official estimate of the total amount of trade debt of the General Government (GG) in Italy. According to estimates released by the Bank of Italy in its Annual Report, it was about 4.8 % of GDP in 2013, down from the peak of about 6 % in 2012.Footnote 1 Based on the data released for the Excessive Deficit Procedure (EDP) notification, Italian GG’s trade debt is the largest in the Euro Area both in absolute terms and as a share of GDP: relative to GDP, the second largest share is recorded in Portugal (2.2 %), followed by Finland, France (both at 2.1 %) and Germany (1 %). Moreover, according to a survey conducted by Intrum Justitia, the average payment delay (in addition to contractual agreements) of the Italian Government is 90 days, compared to 4 days in Finland, 10 in Germany, 19 in France, about 70 in Portugal; in Greece is recorded the largest delay, well above 100 days. There are many reasons for such trade debt accumulation, mainly occurring at local-government levels. Many municipalities faced severe financial constraints, mainly due to the Internal Stability Pact they had to comply with. This pact imposed severe limits to the timely reimbursement of local Governments creditors (Chiades and Mengotto 2013). In addition, at all government levels, until 2012 an implicit incentive for not paying on time suppliers was that trade debts were outside the Maastricht debt’s definition and had consequently very low priority. Only in July 2012 Eurostat slightly changed the rules so that credits released without recourse to financial intermediaries became part of the debt definition for the EDP notification. On the side of the firms, there was a growing concern that GG’s repayment delays could worsen their economic prospects. This worry was aired by Confindustria (the main organization representing Italian manufacturing and services companies) in January 2012 with the launch of a program called “Progetto Confindustria”. It asked for the payment of at least 60 billion of GG’s trade debt, under the rationale that it would have made liquidity conditions less binding and investment more appealing for the firms having commercial relations with the GG. A further second-order effect should have benefited the firms without trade relations with the GG, that dealt with GG’s suppliers. The Italian government devoted an increasing attention to the issue. Since the end of 2011, in order to alleviate financial problems of the firms and contribute to restore economic growth, the Parliament approved several laws to speed up the payment of Government’s outlays and relax the liquidity conditions for the firms. In 2012 several laws provided money for the repayment of existing debt and simplified the bureaucratic procedures of payments. Indeed, in the Italian legislation, the GG can pay a bill only under strict formal conditions and, as a general rule, a credit can circulate in the financial market only if the debtor issues a “certification” to recognize its debt. Whilst in the previous years the release of the certification was a formal (but heterogeneous) procedure, in 2012 a decree law (Ministerial Decree, 22-05-2012) standardized and simplified the process. However, standardizing the certification process was not enough to reduce the outstanding debt. Therefore some laws enacted in 2013 provided resources (25 billion available in 2013 and 16 billion in 2014) to public bodies for the payment of outstanding commercial debts overdue at the end of 2012. The policy maker had the double aim of repaying existing commercial debts and adopting a new approach based on timely debt repayments, that could foster a climate of mutual trust between GG and its suppliers.Footnote 2
 According to Government’s estimates, the repayment policy of 2013 was expected to increase the GDP by 0.2 % in 2013, 0.7 % in 2014 and 0.3 % in 2015. The estimates by the Bank of Italy and Confindustria were more cautious, but nonetheless projected a GDP growth of 0.5–0.7 % in 2 years and 1 % in 3 years, respectively. The projections strongly depended on the speed of reimbursements and on firms’ utilization of the repayments. In particular, the effect on GDP growth would have increased with the share of repayment used for new investment. Understanding whether these policies relaxed the financial conditions for the firms having commercial relations with GG is crucial to shed light on the performance of the whole Italian economy, the third largest in the Euro Area. We therefore evaluate the impact of the reimbursements on the Italian companies, by using a yearly survey conducted by the Bank of Italy on a representative sample of Italian firms with at least 20 employees, merged with financial information provided by banks to the Bank of Italy in its role of authority in charge of bank supervision. With the available data, we can measure the effects on firms’ performances until the end of 2013. Because the primary aim of the policies was to provide firms with funds to relax their liquidity constraints, we look at their effects on some financial variables. We find that receiving money had a significant positive impact on firms’ financial position, reducing their needs to assign credits to financial intermediaries and the probability of default on part (or all) of their debts. To some extent, also firms not receiving money might have benefited from the policies, through accessing the factoring market to sell their trade credits, without increases of the interest rate charged for the service. We provide detailed evidence that the timing and the structure of the policies examined rule out any bias in our results caused, for example, by sample selection: indeed, the policies were completely unexpected and the target credits were due much time before policies were introduced. This paper is organized as follows: Sect. 2 provides some details on the GG’s trade debt legislation. Section 3 illustrates the modeling techniques we applied to estimate the policy effects. The data and some descriptive statistics are presented in Sects. 4 and 5, respectively. Section 6 shows the results. Section 7 provides some conclusive remarks.",1
2.0,2.0,Italian Economic Journal,02 March 2016,https://link.springer.com/article/10.1007/s40797-016-0031-1,Subsidizing New Technology Adoption in a Stackelberg Duopoly: Cases of Substitutes and Complements,July 2016,Masahiko Hattori,Yasuhito Tanaka,,Male,Male,Unknown,Male,"Firms’ adoption of new technology is very important for economic growth. However, it may be insufficient in less competitive industries from the social welfare point of view. In this case, a government subsidy is necessary. We present an analysis of firms’ adoption of new technology and government subsidization policy in a Stackelberg duopoly with differentiated goods. The technology itself is free, but each firm must expend a fixed set-up cost, such as training employees. We analyze the following three-stage gameFootnote 1. First stage: The government determines the subsidy for each firm. Second stage: The leader decides whether to adopt the new technology and then determines its output. Third stage: The follower decides whether to adopt the new technology and then determines its output. At the sub-game perfect equilibria, the number of adopting firms decreases from three to zero as the set-up costs increase. Social welfare is defined as the sum of consumer surplus and firms’ profits, which is equal to consumer utility minus production costs, including the new technology set-up costs. Subsidies are financed by lump-sum taxes on consumers, which are not related to the goods produced by firms. Excluding income effects, these taxes do not affect demand for the goods, and are offset by subsidies. There are several cases for optimal policies depending on the set-up costs and whether the goods produced are substitutes or complements. In particular, we highlight the following cases: Social welfare is maximized when only the Stackelberg leader adopts the new technology, but no firm adopts the new technology without a subsidy. Then, the government should subsidize only the leader, which is a discriminatory policy. (Case 5 of Theorem 1 and Case 3-(1)-ii of Theorem 2) Social welfare is maximized when both firms adopt the new technology, but only the leader adopts the new technology without a subsidy. Then, the government should subsidize only the follower. This policy is not discriminatory because adoption is the dominant strategy for the leader. (Case 2 of Theorem 1) Social welfare is maximized when both firms adopt the new technology, but no firm adopts the new technology without a subsidy. Since adopting a new technology is the best response for the follower when the leader adopts the new technology, the government should subsidize only the leader. This policy is not discriminatory because the follower adopts the new technology without a subsidy. (Case 2-(1)-ii and Case 2-(2)-iii of Theorem 2) Social welfare is maximized when both firms adopt the new technology, but only the follower adopts the new technology without a subsidy. The government should subsidize only the leader. This policy is not discriminatory because adopting a new technology is the dominant strategy for the follower. (Case 2-(2)-ii of Theorem 2) Theorem 1 describes the case where goods are substitutes, and Theorem 2 that where goods are complements. Our model is, at least mathematically, equivalent to a model of technology license with a fixed license feeFootnote 2. In Sect. 2, we review related literature and present the model in Sect. 3. We analyze the optimal subsidy policy when goods are substitutes and when they are complements in Sects. 4 and 5, respectively.",11
2.0,2.0,Italian Economic Journal,21 March 2016,https://link.springer.com/article/10.1007/s40797-016-0032-0,"The Shadow Banking System in the Euro Area: Definitions, Key Features and the Funding of Firms",July 2016,Fabrizio Malatesta,Sergio Masciantonio,Andrea Zaghini,Male,Male,Female,Mix,,
2.0,2.0,Italian Economic Journal,25 April 2016,https://link.springer.com/article/10.1007/s40797-016-0033-z,Local University Supply and Distance: A Welfare Analysis with Centralized and Decentralized Tuition Fees,July 2016,Elias Carroni,Berardino Cesi,Dimitri Paolini,Male,Male,Male,Male,"The welfare effect of a change in the number of universities has been widely investigated both theoretically and empirically. There exists strong empirical evidence of the necessity of widening higher education participation by means of an increasing number of universities.Footnote 1 Mobility costs and the peer group effect at university (as measured by the average ability of the students) have been found to be the main determinants of the individuals’ sorting behavior and of the welfare effect resulting from a change in the number of universities.Footnote 2 Recently, Cesi and Paolini (2014) showed that the introduction of a new university induces a welfare improvement that is stronger when the university system is symmetric (that is, universities have the same average student ability). As a matter of fact, mobility costs and the peer group effect are not the only determinants of individual behavior. Indeed, the choices of individuals and, consequently, the welfare implications of widening university participation through the change in the number of universities may crucially depend crucially on the tuition fees. Following Cesi and Paolini (2014) who use students’ average ability as a proxy for the peer group effect, we study the welfare effect from introducing a new university in a system of endogenous tuition fees. We study two alternative systems. The main difference between the two is in the determination of the tuition fee. In a centralized system, a social planner determines the tuition fee that maximizes social welfare.Footnote 3 In the decentralized system, each university sets its own tuition fee. In the decentralized system, the universities set the tuition fee that maximizes the total revenue from the collected fees: the optimal fee comes as result of a trade off between peer group and number of enrolments. Indeed, less able individuals refrain from enrolling in response to an increase of the fee. This will increase the peer group but it will result in a lower total number of students. In the centralized system, we find that the welfare is maximized when the social planner sets zero fees. An increase of the fee, in fact, would have two different effects. Firstly, it will boost the average ability, as less able individuals abstain from enrolling. On the other hand, given that some individuals prefer to remain unskilled, university participation will be reduced. Consequently, we will have an ambiguous effect on the total amount of collected fees, since a higher per-student fee is paid by less individuals. We show that, passing from a positive to a zero fee, the welfare gains from more participation always offset the losses from lower peer group effect. Thus, the social planner maximizes welfare simply by maximizing university participation. A natural corollary of this result is that a two-university system induces a higher welfare than a one-university system for any mobility cost.Footnote 4 In other words, if a free-of-charge university is opened in each city and individuals do not pay a fee and save on the transportation cost, while enjoining the same average ability at the home university. This result is in line with the finding of Cesi and Paolini (2014) that widening participation through the introduction of a new university is welfare improving. In the decentralized system, results become richer. With a one-university system, the optimal fee will be such that high-ability individuals from both cities go to university, provided that the mobility cost is sufficiently low. As the mobility cost gets higher, the monopolistic university can collect a higher amount of fee-revenue by setting a fee such that only local individuals (who face no mobility cost) find it convenient to go to university. In the duopoly case, each university attracts local individuals setting the fee that maximizes the fee-revenue. Which system between a two-university or a one-university decentralized system depends on the mobility cost. When the mobility cost is low, one university is socially desirable compared to the two-university system. This is because a monopolistic university sets a lower the fee in order to attract also non-local individuals, and this widens participation and increases both individuals welfare and total amount of fees collected. When, instead, the mobility cost gets higher, the monopolistic university only attracts local individuals, thus participation is higher (in our model double) in the two-university system. Our results suggest that a centralized system with a free-of-charge university is preferred to the decentralized one, since when the choice of the fee is decentralized to universities, the latter maximize the total amount of fee without concerns on social welfare. Our paper aims to contribute to the literature on students’ sorting behavior at university (Del Rey 2001; De Fraja and Iossa 2002; Del Rey and Wauthy 2006; Gautier and Wauthy 2007; Poyago-Theotoky and Tampieri 2014). In particular, we study the role of peer group effect by integrating an endogenous tuition fee in Cesi and Paolini (2014). Unlike Gautier and Wauthy (2007), whose main focus is the study of the “tension between teaching and research”, our focus is on the impact of the decentralization, the fees scheme and the peer group on students’ choice and welfare. The remainder of this paper is divided as follows. After having presented the main ingredients of the model in Sect. 2, we provide the analysis of the centralized system (Sect. 3) and of the decentralized system (Sect. 4). In Sect. , we present some final discussion and we draw the conclusions.",
2.0,2.0,Italian Economic Journal,09 May 2016,https://link.springer.com/article/10.1007/s40797-016-0034-y,Slutsky Revisited: A New Decomposition of the Price Effect,July 2016,Kazuyuki Sasakura,,,Male,Unknown,Unknown,Male,"The very first mission of demand theory was, is, and will be to analytically answer the question of how the demand for a good responds to variations in its own price (the price effect). Since Pareto (1892) discovered, and both Slutsky (1915) and Hicks and Allen (1934a, b) gave an elegant formulation, it is only the Slutsky equation that has been universally used for such an analysis. The Slutsky equation teaches us, quite correctly, that the price effect can be decomposed into the substitution effect and the income effect (the Slutsky decomposition). It has been the most fundamental tool not only for pure demand theory but also for wide applications, microeconomic or macroeconomic. It is no exaggeration to say that without it economists could have been only half through their works. The contribution of the Slutsky equation to economics is literally immeasurable.Footnote 1
 On the other hand, current workhorses in demand theory are demand functions derived from the Cobb–Douglas utility function and the CES utility function which is a generalized version of the Cobb–Douglas. It is known that in such cases the income effect is always negative. In other words all goods are normal ones. Since the substitution effect is negative in general, the Slutsky equation repeats a matter of course in a sense, i.e., a negative substitution effect and a negative income effect lead to a negative price effect. This statement itself is very helpful and no one can deny it. But now it may be too ordinary to grab academic researchers. Then, is there an alternative to it? In this paper I propose another way to decompose the price effect. According to it, the price effect is composed of the “ratio effect” and the “unit-elasticity effect.” The “ratio effect” is positive (negative) if the expenditure spent on a good under consideration increases (decreases) when its own price goes up, and it can be divided further into the familiar substitution effect and the “transfer effect” which reflects the income effect of other goods. The “unit-elasticity effect” indicates how the demand for the good changes in response to a rise in its own price if the expenditure for the good remains unchanged, i.e., if its price elasticity of demand is unity. The “unit-elasticity effect” thus defined is always negative. The usefulness of the new decomposition is the most understandable in the Cobb–Douglas and the CES cases. Particularly in the Cobb–Douglas case, as will be seen below, the “ratio effect” vanishes because a negative substitution effect and a positive “transfer effect” just cancel out. The new decomposition would be applicable to other various problems. For example, not a few economists may be interested in the case where the ratio effect is large enough to govern the price effect, i.e., the case of a Giffen good. It never occurs in the Cobb–Douglas or in the CES case. The new method can also be applied to the analysis of such a good. This paper is organized as follows. Reviewing the Slutsky equation, Sect. 2 presents a new equation which decomposes the price effect into the ratio effect and the unit-elasticity effect. Applications of the two equations to examples often used in economics are given, too. Section 3 explains the new equation graphically for a better understanding. The new method is applies to the case of a Giffen good in Sect. 4, the cross-price case in Sect. 5, and a case where a consumer holds initial endowments in Sect. 6. Section 7 concludes the paper.",3
2.0,3.0,Italian Economic Journal,11 July 2016,https://link.springer.com/article/10.1007/s40797-016-0037-8,Perceived Social Position and Objective Inequality: Do They Move Together? Evidence from Europe and the United States,November 2016,Chiara Assunta Ricci,,,Female,Unknown,Unknown,Female,"According to the sociological literature, the analysis of social classes should take into account multiple dimensions and factors, such as income, wealth, relations of production, lifestyle, education and occupation. Furthermore, many different authors (Hodge and Treiman 1968; Jackman and Jackman 1973; Wright and Singelmann 1982; Savage 2015) emphasise the role of individuals’ perceptions of their position in society in their analysis of social classes. These sociologists argue that no study of social class can be comprehensive enough if it does not take into account a person’s sense of self, which may not coincide completely with objective reality and may influence individuals’ behaviour and choices. By contrast, the economic literature often ignores many of these factors and opts for analyses based on statistically measurable characteristics, such as income and consumption. Despite the wide acceptance of the conceptualisations of class offered by the sociological theory, in their empirical works economists tend to consider only relative definitions and use the term “class” to refer to specific strata of the income distribution. This practice raises issues concerning the lack of both sound theoretical assumptions and stable criteria to define and operationalise the theoretical concept of “class” and in particular the “middle class”. For example, comparing the results from different empirical studies can be difficult when the definition of the classes depends on the whole sample considered. Despite these methodological issues, evidence from many studies lets us evaluate the role of specific elements representing the “real” socio-economic aspects of individuals’ life, while the role of perception is often neglected. In particular, these studies demonstrate that the evolution of living standards of different groups across society can depend on real income growth over time, wealth and debts to finance consumption (Atkinson and Brandolini 2013), insecurity and vulnerability in income due to greater risks of unemployment and volatility in earnings (Torche and López-Calva 2013; Krugman 2014; Ricci 2016). The channels through which these aspects may have effects on individuals’ choices are difficult to disentangle, and the purpose of this paper is not to go into this issue. In fact, the starting point of this study is the consideration that social class can be understood as both a subjective and an objective (at least in economic terms) phenomenon. Furthermore, this paper aims to analyze how these two dimensions evolve. In particular, we are interested in analysing whether changes in the degree of inequality within specific groups in terms of objective data are associated with similar changes in the perceptions of the members of the same groups by controlling variations in individual characteristics. The hypothesis is that people could perceive to be similar to (different from) other members of their group when objective socio-economic data show an increasing (decreasing) distance among them. The empirical analysis refers to the changes that occurred in income distribution, socio-economic characteristics and subjective perception of social position within society in six different countries: Germany, Italy, Poland, Norway, the United Kingdom and the United States. First, we analyse how the income distribution has evolved during the period 1994–2010 on the basis of objective reality, by exploring income inequalities across the whole population and considering different population subgroups. To this aim we use the comparative distributional data sources available in the Luxembourg Income Study. Then self perceptions are introduced, by analysing the changes occurred in subjective perceptions of social position and their determinants. The analysis aims at finding out whether these changes are due to a variation in individual characteristics or other unobservable factors. The reported values of people’s social self perception from the International Social Survey Program (ISSP) are considered to investigate the main drivers of the inequality in 1992 and 2009. The purpose of this analysis is to evaluate the different impact of covariates on people’s social perception. The analysis also aims to find out to what extent the shape of the distribution of people into different classes depends on specific individual features. The interaction between people’s perception and objective data on their socio-economic characteristics will permit to make some considerations on the role of perception in people’s real behaviour and choices. The paper is organised as follows. In the next section a review of the literature on perception of people’s position in society and its determinants is provided. In Sect. 3, data and methodological choices are briefly presented. Then, empirical results are discussed (Sect. 4). Finally, Sect. 5 draws some conclusions on the relationship between subjective perceptions of personal position in society and measured objective inequality.",4
2.0,3.0,Italian Economic Journal,12 September 2016,https://link.springer.com/article/10.1007/s40797-016-0041-z,Data Versus Survey-based Normalisation in a Multidimensional Analysis of Social Inclusion,November 2016,Ludovico Carrino,,,Male,Unknown,Unknown,Male,"Although there is quite a consensus on the need for broadening the scope of the analysis of Well-being beyond the monetary dimension (see, e.g., the influential report by Stiglitz et al. 2010), there is not equal agreement on how such an ambitious task should be operationalized. It is well known that subjectivity and arbitrariness exist with respect to the choice of the dimensions to be included in the composite index, the normalisation of the variables, and the characterisation of the aggregation function (see, e.g., Ravallion 2012a; Decancq and Lugo 2013; Martinetti and von Jacobi 2012).Footnote 1 The socio-economic literature highlighted that no unanimous method exists to perform such choices, pointing out numerous theoretical issues (Stiglitz et al. 2010; Ravallion 2011, 2012a; Klugman et al. 2011; Maggino and Nuvolati 2012; Decancq and Lugo 2013), testing empirical robustness (Kasparian and Rolland 2012; Lefebvre et al. 2010; Saisana et al. 2005; Ravallion 2012b). Yet, although there may be no “absolute cure” for multidimensional evaluations, a good practice could consist in enhancing methodological transparency (Sen and Anand 1997). While the major focus of the recent literature has been devoted to the choice of the dimensions’ weights, few studies have concentrated on the role played by normalisation in influencing the final results (Lefebvre et al. 2010; Saisana et al. 2005). Our contribution highlights that, in fact, normalisation is a crucial stage where an “early” implicit weighting takes place, which can strongly affect the overall results of the multidimensional analysis. We show that, since no golden rules exist on how a normalisation function should be selected and characterised, different strategies, all acceptable a-priori, can lead to very different weighting structures and, ultimately, opposite results for the composite indicator. Therefore, the unavoidable arbitrariness regarding the choice of the normalisation function, as well as its methodological justification, should be made as transparent as possible. To illustrate these points, in this paper we will build a composite measure of Social Inclusion for 63 European administrative regions from 2004 to 2012, using data from EUROSTAT. The aggregation framework is a CES function (constant elasticity of substitution), and the selection of variables follows the relevant literature on this topic (stemming from Atkinson et al. 2002).Footnote 2
 In this analysis, we adopt a baseline linear aggregation model where the normalised components have equal weights and we look at what happens to the aggregate measure of Social Inclusion when the sole normalisation function changes. In particular, we apply the widely used data-driven min–max normalisation strategy, whose parameters depend on the available data. This data-driven function generates implicit trade-offs (between the index’ components) and shadow prices with weak economic justification. We also propose a novel strategy, an expert-based min–max function, whose parameters are grounded on the responses to a survey conducted on a population of 150 professors of Economics or Management at the Ca’ Foscari University of Venice. Our results indicate that, even within a simple-average framework, changing the normalisation function substantially affects the relative relevance of each component of the aggregate measure. As a consequence, significant differences emerge in the levels and rankings of regional Social Inclusion in Europe, leading to very different policy implications. The data-driven strategy softens the heterogeneities within and between European countries by putting a substantial weight on the longevity variable rather than on educational and economic statuses. As a result, the European regional distribution of Inclusion appears to be uni-modal around the mean. Conversely, the expert-based normalisation emphasises the unemployment and the school-dropouts variables, and returns a bi-modal distribution of Social Inclusion. We, thus, discuss how the different premises of the two strategies characterise the interpretation of the results: the data-driven approach allows for a positive interpretation of the index, while the survey-driven approach allows for a normative one. In other words, if the index’ intrinsic trade-offs are grounded on statistical terms, its results should be interpreted accordingly. The remaining of the paper is organized as follows. Section 2 briefly describes the concept of Social Inclusion and the data. Section 3 sets a standard framework for multidimensional aggregation and details the baseline model. Section 4 introduces the normalisation strategies, while Sect. 5 discuss the implicit trade-offs resulting from applying the aforementioned normalisation functions on the baseline model. Section 6 details the results of the Social Inclusion indices, Sect. 6.1 concludes.",5
2.0,3.0,Italian Economic Journal,23 August 2016,https://link.springer.com/article/10.1007/s40797-016-0040-0,Constrained Network Formation,November 2016,Pietro Battiston,,,Male,Unknown,Unknown,Male,"In the last 20 years, the theory of networks has been recognised an important role in explaining the formation and functioning of social and economic settings in which relationships among agents are of fundamental importance. In particular, several models of network formation were developed targeting the mechanisms by which some characteristics of nodes (typically, the cost of creating/keeping alive a link, compared to the utility received from becoming—directly or indirectly—connected to some other nodes) endogenously determine the structure of a network. A stream of literature, starting from the seminal work of Bala and Goyal (2000), has developed focusing on a noncooperative approach, where the choice of adding a link between two nodes is made independently by only one of them, which bears all the cost—although other nodes potentially benefit from such link. Based on this framework, a definition of stability can be given, typically based on the concept of pairwise Nash equilibrium (such as in Galeotti 2006 and Haller et al. 2007), or some refinement of it (for instance Dutta and Mutuswami 1997 consider coalition choices, while the concept of “far-sightedly stable networks” formulated by Herings et al. 2009 is based on attributing nodes a longer horizon of strategical reasoning). The aforementioned studies share the implicit assumption that links can be added and destroyed freely (though at some cost). Even experimental works on endogenous network formation have usually been based on the assumption that participants can at any point in time—or at least repeatedly—decide to create/break a link (Goeree et al. 2009; Kirchsteiger et al. 2016). This is a natural starting point for several reasons: links in many real world networks (e.g. computer networks, social relationships...) are indeed at least potentially volatile, the data available to researchers often describe some inherently volatile flow (e.g. trade, influence, information) over them, and even considering networks which are typically characterised by a stratification of links over time (such as connections in Internet social networks, or the network of roads between cities), most databases available to researchers are snapshots of networks at given points in time, sacrificing information on their temporal evolution. However, there are several contexts in which the process of network formation is profoundly shaped by constraints, and in which the assumption that links can be freely created is at odds both with reality and with data available to researchers. Constraints may have different origins: they can for instance be related to time (e.g. networks where nodes are scientific papers, patents or other kinds of timestamped objects), space (e.g. planarity), and rivalry (e.g. cross-ownership networks, nodes affected by capacity limits): two specific examples will be analysed in more detail in Sect. 3. Only recently some form of constrained growth was formalised in the context of strategic network formation by Haller (2012). His study provides interesting conclusions concerning networks which grow around an exogenously fixed subset of links, shown to potentially change drastically the existence, numerosity, stability and efficiency of stable configurations. An interesting insight is that such backbone infrastructures, that is, sets of links which are guaranteed to exist ex ante and independently from individual incentives, and which hence forbid nodes from playing their individual best replies, can actually cause global welfare improvements. The present study generalises the approach to the analysis of repeated addition of nodes and/or links, under positive and negative constraints. Differently from the work of Haller, the set of guaranteed/forbidden links will not necessarily be exogenously given, but can come instead from the previous iteration of the network formation process. This results in a rich framework, which can be specialised according to the characteristics of the network under analysis.",1
2.0,3.0,Italian Economic Journal,28 July 2016,https://link.springer.com/article/10.1007/s40797-016-0038-7,Optimal Fiscal and Monetary Policies Under Limited Asset Market Participation,November 2016,Lorenzo Menna,,,Male,Unknown,Unknown,Male,"The standard normative result in New Keynesian models characterized by price stickiness is that monetary policy can replicate the flexible price allocation by completely stabilizing inflation (Blanchard and Galì 2007), which renders the role of fiscal policy of secondary importance. Under medium scale DSGE models with nominal and real rigidities, monetary policy remains the main tool for business cycle stabilization; while optimal fiscal policy is passive (Schmitt-Grohe and Uribe 2007). Standard New Keynesian DSGE models rest on the representative agent assumption which is only valid as long as everybody participates to financial markets and marginal rates of substitution are equalized among agents. Such assumptions are at odds with the data, as shown by a growing body of literature (see, for instance, Vissing-Jørgensen 2002). A convenient device to introduce heterogeneity in a standard DSGE model has been used in a second strand of New Keynesian literature which, following a seminal contribution by Mankiw (2000), emphasizes the role of rule-of-thumb (RT henceforth) consumers who do not participate to financial markets and therefore cannot save or borrow. Galì et al. (2004) as well as Furlanetto and Seneca (2009) show that this form of limited asset market participation (LAMP henceforth) can rationalize the empirically observed response of aggregate consumption to public spending shocks. In Furlanetto and Seneca (2012), the LAMP hypothesis helps account for recent empirical evidence on productivity shocks. The evidence for limited participation to asset markets is overwhelming. As Table 1 shows, in the United States only a small minority of households participate in the stock (15.1  %) and bond market (1.6  % for common bonds and 12  % for savings bonds). Other types of assets are all held by less than the 50  % of households, apart from retirement accounts (50.4  %) and transaction accounts (92.5  %), the latter being a form of non-interest bearing money.Footnote 1 Other evidence suggests that a small minority of households holds the greatest part of wealth in several countries. Alvaredo et al. (2013), for instance, show that the top 1  % of wealth holders owns 35  % of total wealth in the US and 20–25  % in Europe. The great majority of the population has either little wealth (usually in the form of housing) or no wealth at all. The fact that a substantial share of the population in several countries holds very small amounts of liquid wealth has two classes of consequences that are of interest for policy-makers. First, many households do not receive any income in the form of profits, dividends, rents and interests. Inequality in capital income can in principle be important from an optimal taxation perspective. Second, such households may have difficulties smoothing consumption over time. In fact, poor households are also often borrowing-constrained. Between 40 and 60  % of respondents to the Survey of Household Economics and Decisionmaking (2014) conducted by the FED in 2013 said that they would be unable to cover three months of their expenditures by either using their savings or by borrowing.Footnote 2 This fact has important implications for counter-cyclical fiscal policy, as it invalidates the Ricardian equivalence principle, and opens up the possibility of using government policies to help households insure against idyosincratic shocks. There are several explanations for LAMP, ranging from fixed asset market participation costs and agency costs (Bo Sun 2011) to rational inattention (Rachedi 2016). Introducing RT consumers in an otherwise standard DSGE model allows to study these issues in a relatively simple framework. The present paper considers a fully fledged DSGE model with capital, nominal rigidities, real rigidities and RT consumers, in which the planner has access to both monetary and fiscal instruments; and uses it to assess how such instruments should be used when redistributive issues are important. In practice, the Ramsey planner chooses the optimal inflation rate and the optimal capital, consumption and labor tax rate, with the objective of maximizing a social welfare function, given by a weighted sum of asset market participants (ricardian agents, henceforth) and RT consumers utility functions. The share of RT consumers is set to 40  %, a rather conservative value, given that participation to the stock and bond market is much lower. However, values close to 0.4 arise in several empirical estimations of the share of RT consumers (see for instance Mankiw 2000; Galì et al. 2004). In particular, I contribute to two strands of literature. The first strand of literature to which this paper is related studies optimal taxation in the steady state equilibrium, when government spending is exogenously given. Seminal contributions in the standard representative agent neoclassical framework are (Chamley 1981, 1986), Chari et al. (1994) and Coleman (2000), among others. They find that optimal policy requires a zero capital tax in the long run. Coleman (2000), in particular, shows that it is optimal to tax consumption and subsidize labor as long as public expenditures include transfers and the consumption tax base is higher than the labor tax base, which is verified in both the model and the data. Schmitt-Grohé and Uribe (2006) analyze the optimal tax scheme in a model with monopolistic competition, price stickiness and monetary transaction costs, but without consumption taxes. They find that a capital subsidy becomes optimal, and the latter is higher if it is possible to tax monopolistic profits separately from capital income. One of the few papers that considers a heterogenous agent economy in this strand of literature is Judd (1985), which takes into account a model with workers (who do not hold any wealth) and capitalists and finds that the optimal capital tax remains zero notwithstanding the possibility to use it to redistribute income. Indeed, taxing capital reduces the long run capital level, depresses real wages and ends up reducing workers’ income. In this paper I find that the optimality of a zero or even negative capital tax is robust to the introduction of LAMP. However, the framework considered here allows to obtain novel results for what concerns the relative importance of labor and consumption taxes. First of all, I find that under the representative agent assumption, the result of Coleman (2000) that consumption taxes are preferable to labor taxes is not robust when firms are subject to fixed production costs. In the presence of such costs, it is indeed the case that the Ramsey planner prefers to set the labor tax rate higher than the consumption tax rate. However, the consumption tax is again preferable to the labor tax once LAMP is introduced in the model. The intuition for this result is that while the labor tax burden disproportionately falls on RT consumers whose sole source of income is labor, consumption taxes allow to indirectly tax profits, which only accrue to Ricardian agents. As the latter own the whole wealth of the economy and earn higher incomes, consumption taxes serve the purpose of redistribution. Differently from the capital income tax, the consumption tax does not distort the consumption-investment decision in the steady stateFootnote 3 and is less distortionary. The second strand of literature to which this paper contributes investigates the optimal monetary and fiscal stabilization policies along the business cycle, which are usually studied in a New Keynesian framework in the presence of nominal rigidities. As already mentioned, the seminal contribution of Schmitt-Grohe and Uribe (2007) finds that the bulk of business cycle stabilization should be implemented through monetary policy and fiscal policy should be passive. In particular, monetary policy should almost completely stabilize inflation along the business cycle and fiscal policy should limit itself to avoid explosive public debt paths. The robustness of this result under LAMP has been debated in the literature. Ascari et al. (2011) consider a model without an explicit role for fiscal policy and find that if both prices and wages are sticky, monetary policy should simply stabilize inflation fluctuations even in the presence of RT consumers. Motta and Tirelli (2012) find that introducing both LAMP and consumption habits makes fiscal activisms optimal. In particular, while monetary policy should respond to inflation fluctuations even more strongly than under the representative agent model, fiscal policy should aim at stabilizing nominal income growth. Such a policy combination is able to reduce fluctuations in income distribution, which in the presence of consumption habits are particularly costly in terms of welfare. However, this literature usually considers stylized models that abstract from capital accumulation. Furthermore, the fiscal instruments at the disposal of the planner are often limited in number. Motta and Tirelli (2012), for instance, assume that the policy-maker can access non-distortionary lump-sum taxes. The present paper shows that optimal fiscal policy aims at reducing fluctuations in income distribution even in the absence of consumption habits, once one allows for capital accumulation. In addition, I show how an optimal combination of distortionary taxes can be used to achieve this result. To understand why the addition of capital accumulation by itself makes fiscal activism important, consider the response of the economy to a productivity shock. Productivity shocks increase overall income, but redistribute it from RT consumers to ricardian agents, making the former relatively worse off. Fluctuations in income distribution along the business cycle are inefficient as the first best allocation requires that all idyosincratic risks are insured away and the marginal utility ratio between agents is kept constant.Footnote 4 Such inefficiency is the composition of two different effects. First of all, price stickiness tends to cause increases in the mark up of prices over marginal costs, which pushes up monopolistic profits and reduces demand for labor and capital. This first effect can be cured with monetary policy: mark-ups can in fact be completely stabilized if inflation is kept constant at zero. On the other hand, the fact that all capital is in the hands of ricardian agents implies that the increase in the return to capital that follows productivity shocks is completely appropriated by them, which tends to augment the ratio between the income of the wealthiest agents and that of the poorest. Such an effect can not be confronted by monetary policy, which, on the contrary, tends to increase it by stimulating demand for capital through price stabilization. Fiscal policy can instead play a role. The planner temporarily borrows funds from ricardian agents to finance a reduction of the labor tax. This allows to sustain the consumption of RT consumers and stabilize the marginal utility ratio. The welfare gains from such a policy critically depend on the curvature of the utility function. The more concave the utility function is, the higher are the costs linked to income inequality in the steady state and the higher are the costs of its fluctuations along the business cycle. The optimal policy almost eliminates swings in the distribution of income for a KPR utility function with the curvature parameter set at five. In the log-utility case instead, income distribution is not completely stabilised. Notice, however, that the latter case is probably the less relevant from an empirical point of view. A large literature, beginning with Mehra and Prescott (1985), shows that log-utility, while very common in the macroeconomic literature, is inadequate for fitting asset prices and that much higher curvatures of the utility function must be considered to explain financial data. This suggests that households are much more averse to consumption fluctuations than what is predicted by the log-utility case. The rest of the paper is organized as follows. In Sect. 2 present the model and the Ramsey problem. Section 3 presents the calibration of the deep parameters of the model. Sections 4 and 5 discuss respectively the optimal deterministic steady state and the optimal Ramsey dynamics. Section 6 concludes.",2
2.0,3.0,Italian Economic Journal,23 June 2016,https://link.springer.com/article/10.1007/s40797-016-0036-9,Rating Performance and Bank Business Models: Is There a Change with the 2007–2009 Crisis?,November 2016,Vincenzo D’Apice,Giovanni Ferri,Punziana Lacitignola,Male,Male,Unknown,Male,"The financial crisis started in 2007 highlighted that transformations in bank business models played a role in lowering lending standards and heightening systemic risk (see, for example, Altunbas et al. 2011; Berndt and Gupta 2009; Bord and Santos 2012; Mian and Sufi 2009; Stiglitz 2010; Financial Services Authority 2009; D’Apice and Ferri 2010). Specifically, many banks moved away from traditional business—gathering deposits to make loans to be held to maturity on their balance sheets, Originate to Hold (OtH) model—to a new market-attuned business model (Originate to Distribute, OtD), where they make loans to be securitized on financial markets. The extent to which banks moved away from their traditional business model may be gauged as a new source of risk (Gennaioli et al. 2012), which the rating agencies might possibly take into account (Salvador et al. 2014a). Starting from this idea, we investigate whether, after the outbreak of the 2007–2009 crisis, bank ratings became more related to the bank business model. To do so, focusing on the three major agencies (Fitch, Moody’s and S&P’s), we use the level of bank ratings between 2006 and 2009 and an index of how closely each bank kept to the traditional business model.Footnote 1
 Specifically, we take two slightly different but complementary approaches via cross-section and panel estimations. In the cross-section, we analyze whether the banks that were more traditional before the crisis had a better rating performance with respect to the other banks between 2006 and 2009.Footnote 2 In a way, in fact, the perception that these banks were less exposed to the financial-market-related risks at the core of the crisis could have persistent favorable effects on their ratings. In the panel estimations, we examine the relationship with the ratings by letting the bank business model change.Footnote 3 Various banks, in fact, decided to change their business model in response to the crisis and this could also affect the way the rating agencies evaluated them. Overall, both approaches show that banks with a higher share of traditional income over total income exhibited better rating performance in our sample of 241 listed banks from 39 countries. Thus, even though the experience of the crisis suggests that regulation ought to factor in each bank’s business model in assessing its risk exposure (Caprio et al. 2014), market forces—in this case the rating agencies—may have imparted themselves the right incentives to banks. In fact, better ratings favored those banks adhering more closely to the traditional business model. Our results could be seen at odds with other studies on the relationship between bank business model and risk, finding that also more diversified banks have been more stable in the recent crisis (see, e.g., Altunbas et al. 2011; Köhler 2015). However, our study differs in two main aspects. First, we use the bank rating as measure of risk, whereas they use the z-score (Köhler 2015) or bank distress proxies (Altunbas et al. 2011). While bank distress regards extreme events only and the z-score captures, by construction, the distance to default as a risk-return combination, rating changes seem more general. A rating change may, in fact, denote a bank undergoing non-extreme events or its performance being affected by items—e.g., liquidity—disregarded by the z-score. Second, as suggested by De Young and Torna (2013), we consider fees and commissions related to traditional operations as traditional income; whereas they include this source of revenue in the non-interest income (Altunbas et al. 2011; Köhler 2015). In addition, a further differentiation of our paper descends from the fact that we take only listed banks from 39 countries, while they use also unlisted banks (Köhler 2015) or focus only on European banks (Altunbas et al. 2011; Köhler 2015). On one hand, considering listed banks only ensures that there will likely be at least two ratings for each bank. On the other, however, this choice reduces the number of banks in our sample. In the rest of the paper, Sect. 2 draws a survey of the literature. Section 3 outlines the data used. Section 4 reports the methodology, discusses the main results of our econometric analysis and describes various robustness checks. Finally, Sect. 5 concludes and debates the implications of our findings.",6
2.0,3.0,Italian Economic Journal,11 August 2016,https://link.springer.com/article/10.1007/s40797-016-0039-6,The Role of Borrower-Cosigner Kinship Relations on Loan Default,November 2016,Lucia dalla Pellegrina,Antonio Scollo,,Female,Male,Unknown,Mix,,
2.0,3.0,Italian Economic Journal,06 June 2016,https://link.springer.com/article/10.1007/s40797-016-0035-x,Do Firms Benefit from University Research? Evidence from Italy,November 2016,Paola Cardamone,Valeria Pupo,Fernanda Ricotta,Female,Female,Female,Female,"The purpose of this paper is to investigate the role played by positive externalities produced by universities in the productivity of local firms. Our main hypothesis is that, regardless of formal university-firm relationships, firms can benefit from knowledge ‘spillovers’ from nearby universities, which can be used to improve their productivity either directly or indirectly via innovation output. Such spillovers can arise through informal information transmission via the local personal networks of university and industry professionals, participation in conferences and presentations, or other kinds of knowledge transmissions from academia to industry (Varga 2000). The role of university research in the stimulation of technological innovation and higher productivity is due to its nature as a public good, which determines positive externalities to the private sector (Anselin et al. 1997). Firms access external knowledge at a cost that is lower than the cost of producing this value internally or of acquiring it externally (Harhoff 2000). The cost of transferring such knowledge is a function of physical distance and gives rise to localised externalities (Siegel et al. 2003). According to Adams (2002), the open science of universities draws firms to locate near academic institutions: firms go to local universities to obtain information that is current and not proprietary. Yusuf (2008) explains that the closer one gets to the knowledge frontier, the larger the human factor in the transmission process and, consequently, the importance of networking and circulation of knowledge workers. In a nutshell, the presence of a university may contribute to the creation of a social and cultural climate that promotes networks of formal and informal interaction, allowing the resulting spillovers to be more easily absorbed by local firms. The first conceptualizations of this process were provided by Arrow (1962) and Nelson et al. (1959), and further refined by Griliches (1979), Nelson (1982), Cohen and Levinthal (1989). From an empirical point of view, however, there are few studies that explicitly analyse the impact of academic research, and consider its impact on innovation (among others, see Acs et al. 1992; Anselin et al. 1997; Audretsch et al. 2012; Del Barrio-Castro et al. 2005; Jaffe 1989; Leten et al. 2011; Piergiovanni et al. 1997), or on regional economic growth (Carree et al. 2014; Duch et al. 2011; Goldstein and Drucker 2006). Unlike previous studies, we assess the effect of the university research on firm total factor productivity (TFP). This variable, generally regarded to be the best metric of economic efficiency, is crucial especially for those countries, such as Italy, which suffer from low productivity (ISTAT 2007a; OECD 2007; Van Ark et al. 2007). Yet, few studies have investigated the impact of university knowledge on firm productivity, and all have considered formal R&D co-operation (Arvanitis et al. 2008; Belderbos et al. 2004; Harris et al. 2011; Medda et al. 2005), and are thus incomparable with our analysis. More similar to our contribution is the study by Audretsch and Lehmann (2005) who, assessing the effect of knowledge generated by universities on the rate of growth of firms in terms of employees, provided the “missing link” between the literature on firm growth and that studying university spillovers. To the best of our knowledge, however, ours is the first paper which specifically addresses the effect of university R&D spillovers on firm productivity. We measure university research by using the total research spending by the university. As a productivity measure, we estimate TFP at firm level by employing the Levinsohn and Petrin (2003) approach and the UniCredit-Capitalia (2008). Firm data are combined with indicators of university research in the province where a firm is located. In so doing, we rely on previous research results which stress that a good deal of knowledge is embodied in people and organisations, and spatially clustered (Jaffe 1989; Audretsch and Feldman 1996; Anselin et al. 1997). We also evaluate if firm size, sector and geographical location influence the relationship between local universities and firm productivity: in particular, whether university research effects hold uniformly across regions, whether local university spillovers could be specific to certain industries, and whether firm size matters. Results show that university R&D does not seem to improve firm productivity. However, if we consider geographical location and sector, we find that universities have a positive effect on the performance of firms located in the North of Italy and those operating in the specialised supplier sector. Several robustness checks are performed, confirming the significant role played by universities especially in the North of Italy. The paper is organized as follows. The following section describes data and indicators employed. We then illustrate methodology and results. The final section concludes.",3
3.0,1.0,Italian Economic Journal,28 November 2016,https://link.springer.com/article/10.1007/s40797-016-0044-9,Individual Mismatch and Aggregate Overeducation: Evidence from a Quasi-Natural Experiment,March 2017,Patrizia Ordine,Giuseppe Rose,,Female,Male,Unknown,Mix,,
3.0,1.0,Italian Economic Journal,28 October 2016,https://link.springer.com/article/10.1007/s40797-016-0043-x,Are Consumers More Willing to Invest in Luck During Recessions?,March 2017,Sara Capacci,Emanuela Randon,Antonello Eugenio Scorcu,Female,Female,Male,Mix,,
3.0,1.0,Italian Economic Journal,05 December 2016,https://link.springer.com/article/10.1007/s40797-016-0045-8,"Youth Labour-Market Performance, Institutions and Vet Systems: A Cross-Country Analysis",March 2017,Floro Ernesto Caroleo,Elvira Ciociano,Sergio Destefanis,Male,Female,Male,Mix,,
3.0,1.0,Italian Economic Journal,23 December 2016,https://link.springer.com/article/10.1007/s40797-016-0046-7,"Institutional Wage Setting, Distinctive Competencies and Wage Premia",March 2017,Riccardo Leoni,Paola Gritti,,Male,Female,Unknown,Mix,,
3.0,1.0,Italian Economic Journal,24 October 2016,https://link.springer.com/article/10.1007/s40797-016-0042-y,On the Efficiency of Italian Universities: A Comment,March 2017,Carlo D’Ippoliti,Giulia Zacchia,,Male,Female,Unknown,Mix,,
3.0,2.0,Italian Economic Journal,11 May 2017,https://link.springer.com/article/10.1007/s40797-017-0054-2,"Income Disparities, Population and Migration Flows Over the Twenty First Century",July 2017,Frédéric Docquier,Joël Machado,,Male,Male,Unknown,Male,,4
3.0,2.0,Italian Economic Journal,20 March 2017,https://link.springer.com/article/10.1007/s40797-017-0050-6,"Voter Turnout in Italian Municipal Elections, 2002–2013",July 2017,Federico Revelli,,,Male,Unknown,Unknown,Male,,3
3.0,2.0,Italian Economic Journal,10 April 2017,https://link.springer.com/article/10.1007/s40797-017-0053-3,An Analysis of the Determinants of Over-Education Among Italian Ph.D Graduates,July 2017,Barbara Ermini,Luca Papi,Francesca Scaturro,Female,Male,Female,Mix,,
3.0,2.0,Italian Economic Journal,06 June 2017,https://link.springer.com/article/10.1007/s40797-017-0057-z,The Italian Labor Market Reform: An Evaluation of the Jobs Act Using the Prometeia DSGE Model,July 2017,Michele Catalano,Emilia Pezzolla,,Female,Female,Unknown,Female,,4
3.0,2.0,Italian Economic Journal,07 June 2017,https://link.springer.com/article/10.1007/s40797-017-0056-0,Tax Structure and Economic Growth: A Panel Cointegrated VAR Analysis,July 2017,Silvestro Di Sanzo,Mariano Bella,Giovanni Graziano,Male,Male,Male,Male,,6
3.0,2.0,Italian Economic Journal,23 March 2017,https://link.springer.com/article/10.1007/s40797-017-0051-5,Is the Mediterranean the New Rio Grande? A Comment,July 2017,Simone Bertoli,,,Female,Unknown,Unknown,Female,,2
3.0,3.0,Italian Economic Journal,06 November 2017,https://link.springer.com/article/10.1007/s40797-017-0065-z,Agent-Based Macroeconomics and Classical Political Economy: Some Italian Roots,November 2017,Giovanni Dosi,Andrea Roventini,,Male,Female,Unknown,Mix,,
3.0,3.0,Italian Economic Journal,10 July 2017,https://link.springer.com/article/10.1007/s40797-017-0060-4,An Agent Based Macroeconomic Model with Social Classes and Endogenous Crises,November 2017,Alberto Russo,,,Male,Unknown,Unknown,Male,,4
3.0,3.0,Italian Economic Journal,20 March 2017,https://link.springer.com/article/10.1007/s40797-017-0049-z,Innovation Dynamics and Industry Structure Under Different Technological Spaces,November 2017,Alessandro Caiani,,,Male,Unknown,Unknown,Male,,10
3.0,3.0,Italian Economic Journal,30 March 2017,https://link.springer.com/article/10.1007/s40797-017-0052-4,Informative Contagion Dynamics in a Multilayer Network Model of Financial Markets,November 2017,Alessio Emanuele Biondo,Alessandro Pluchino,Andrea Rapisarda,Male,Male,Female,Mix,,
3.0,3.0,Italian Economic Journal,31 May 2017,https://link.springer.com/article/10.1007/s40797-017-0055-1,"Inequality, Redistributive Policies and Multiplier Dynamics in an Agent-based Model with Credit Rationing",November 2017,Elisa Palagi,Mauro Napoletano,Jean-Luc Gaffard,Female,Male,Male,Mix,,
3.0,3.0,Italian Economic Journal,09 June 2017,https://link.springer.com/article/10.1007/s40797-017-0058-y,Business Cycle in a Macromodel with Oligopoly and Agents’ Heterogeneity: An Agent-Based Approach,November 2017,Marco Mazzoli,Matteo Morini,Pietro Terna,Male,Male,Male,Male,,
3.0,3.0,Italian Economic Journal,06 November 2017,https://link.springer.com/article/10.1007/s40797-017-0064-0,The Crisis of Economic Theory and the Complexity View: A Note,November 2017,Domenico Delli Gatti,,,Male,Unknown,Unknown,Male,,1
4.0,1.0,Italian Economic Journal,03 April 2018,https://link.springer.com/article/10.1007/s40797-018-0072-8,Back on Track? A Macro–Micro Narrative of Italian Exports,March 2018,Matteo Bugamelli,Silvia Fabiani,Andrea Linarello,Male,Female,Female,Mix,,
4.0,1.0,Italian Economic Journal,03 January 2018,https://link.springer.com/article/10.1007/s40797-017-0069-8,The Gender Wage Gap Among College Graduates in Italy,March 2018,Daniela Piazzalunga,,,Female,Unknown,Unknown,Female,,17
4.0,1.0,Italian Economic Journal,03 January 2018,https://link.springer.com/article/10.1007/s40797-017-0070-2,Play Versus Strategy Method: Behavior and the Role of Emotions in the Ultimatum Game,March 2018,Chiara Nardi,,,Female,Unknown,Unknown,Female,,1
4.0,1.0,Italian Economic Journal,09 January 2017,https://link.springer.com/article/10.1007/s40797-016-0047-6,Non-Price Competitiveness and Financial Drivers of Exports: Evidences from Italian Regions,March 2018,Bernardina Algieri,Antonio Aquino,Lidia Mannarino,Female,Male,Female,Mix,,
4.0,1.0,Italian Economic Journal,20 February 2017,https://link.springer.com/article/10.1007/s40797-017-0048-0,License and Entry Strategies for an Outside Innovator Under Duopoly,March 2018,Masahiko Hattori,Yasuhito Tanaka,,Male,Male,Unknown,Male,,4
4.0,1.0,Italian Economic Journal,25 September 2017,https://link.springer.com/article/10.1007/s40797-017-0061-3,A Dual Characterization of Pareto Optimality,March 2018,Aldo Montesano,,,Male,Unknown,Unknown,Male,,
4.0,1.0,Italian Economic Journal,17 October 2017,https://link.springer.com/article/10.1007/s40797-017-0063-1,Piero Sraffa’s Use of the History of Economic Thought in the Cambridge Lectures,March 2018,Attilio Trezzini,,,Male,Unknown,Unknown,Male,,2
4.0,2.0,Italian Economic Journal,14 December 2017,https://link.springer.com/article/10.1007/s40797-017-0068-9,Does Random Selection of Selectors Improve the Quality of Selected Candidates? An Investigation in the Italian Academia,July 2018,Daniele Checchi,Silvia De Poli,Enrico Rettore,Female,Female,Male,Mix,,
4.0,2.0,Italian Economic Journal,23 October 2017,https://link.springer.com/article/10.1007/s40797-017-0062-2,The Effects of R&D Subsidies to Small and Medium-Sized Enterprises. Evidence from a Regional Program,July 2018,Marco Mariani,Fabrizia Mealli,,Male,Female,Unknown,Mix,,
4.0,2.0,Italian Economic Journal,24 July 2017,https://link.springer.com/article/10.1007/s40797-017-0059-x,"Justice: Greater Access, Lower Costs",July 2018,Margherita Saraceno,,,Female,Unknown,Unknown,Female,,2
4.0,2.0,Italian Economic Journal,16 November 2017,https://link.springer.com/article/10.1007/s40797-017-0066-y,Growth and Inequality in an Experimental AK Model,July 2018,Ferruccio Ponzano,Roberto Ricciuti,,Male,Male,Unknown,Male,,1
4.0,2.0,Italian Economic Journal,30 November 2017,https://link.springer.com/article/10.1007/s40797-017-0067-x,Subsidy Policy and Elderly Labor,July 2018,Yusuke Miyake,Masaya Yasuoka,,Male,Male,Unknown,Male,,
4.0,2.0,Italian Economic Journal,17 April 2018,https://link.springer.com/article/10.1007/s40797-018-0074-6,Corporate Social Responsibility and Managerial Bonus Systems,July 2018,Luciano Fanti,Domenico Buccella,,Male,Male,Unknown,Male,"The separation between ownership and control as well as managers’ behaviours departing from the pure profit maximization are long lasting stylised facts, especially in large companies (e.g. Berle and Means 1932). As known, the managerial economics and agency theory have first attempted to explain the reasons of the observed separation between ownership and control (e.g. Baumol 1958; Holmström 1982). However, in the second half of the 1980s, Vickers (1985), Fershtman and Judd (1987) and Sklivas (1987) (VFJS for short) have shown, by resorting to a modern game-theoretic approach, that the rationale for the separation between ownership and control might be essentially based on strategic reasons: hiring managers instructed to behave more aggressively in the market, forcing rivals to reduce output, raises market share and profits. Furthermore, while the original VFJS’s approach deals with a managerial contract based on a combination of profits and sales or revenues (D), another incentive scheme that accounts for the relative performance of firms (RP) has been considered, motivated by empirical research outcomes (Gibbons and Murphy 1990;Footnote 1 Salas Fumás 1992; Miller and Pazgal 2002).Footnote 2 However, the rather paradoxical result of both types of managerial contract is that, although it is always convenient for an owner to conduct managers towards non-profit-maximising behaviours, if also owners in rival firms hire managers to delegate output decisions, the final results are such that, at equilibrium, firms end up in a prisoner’s dilemma with relatively lower profits. However, both approaches (D and RP) traditionally analyse profit-seeking firms, while in recent years the number of firms adopting corporate social responsibility (CSR) behaviours is rapidly increasing, and the debate on firms’ social responsibility has also more and more frequently involved the academic literature (e.g. Baron 2001, 2009; Baron and Diermeier 2007; Jensen 2001; Goering 2007, 2008; Lambertini and Tampieri 2010; Benabou and Tirole 2010; Besley and Ghatak 2010). Indeed, according to KPMG, CSR behaviours are undoubtedly a mainstream business practice across the world: 73% of 4500 companies surveyed in 45 countries in 2015 have reported the completion of CSR activities, with an increase of 2% points since 2013 and 9% points since 2011. Notably, in 2015, 92% among the world’s largest 250 companies, in which ownership and control are clearly separated, records CRS activities (KPMG 2011, 2013, 2015). The Reputation Institute (2016) CSR survey confirms that the world’s top ten companies with the best CSR reputations are worldwide, widely known, large managerial companies; in fact, the first ten places in the global ranking are as follows: (1) Rolex; (2) The Walt Disney Company; (3) Google; (4) BMW; (5) Daimler; (6) Lego; (7) Microsoft; (8) Canon; (9) Sony; (10) Apple. Therefore, it is natural to ask if the established results of the managerial literature also hold true under the modern CSR behaviour by competing firms. This paper attempt to carry out this task. The preceding literature studying the issue of managerial delegation with firms of CSR-type is rather scant. Exceptions are, to the best of our knowledge, four recent articles: Goering (2007), Kopel and Brand (2012), Manasakis et al. (2014) and Fanti and Buccella (2017). However, those papers model CSR following completely different approaches. Indeed, the first two and the latter papers belong to the branch of the literature on CSR which measures the firm’s social interests through the consideration of the consumer surplus in the firm’s objective function (while the consumers’ demand is not affected by the firms’ social engagement) to be maximised in the competition on product market, which leads by itself to an increased quantity and reduced profits. On the contrary, the third paper belongs to the branch which assumes that the firm’s social concerns are well-valued by consumers which increase their demandFootnote 3: this approach leads not only to a larger quantity by itself, but also to higher prices and, ultimately (provided that costs to produce CSR-type goods are not prohibitively high) higher profits. The current paper belongs to the former above-mentioned branch, motivated by the fact that, in the words of Kopel (2015, 560) “consumers (beside shareholders) emerge as the most important stakeholder group when it comes to having an impact on a firm’s approach to sustainability.” Therefore, for the sake of comparison with the present paper, we briefly review only the three above-mentioned papers which share with the present one the representation of the social concern of the firm through an objective which is a combination of profits and consumer surplus. Goering (2007) examines a mixed duopoly in which a private non-profit firm (that is, a firm with CSR features) competes with a private profit-maximising, and only the non-profit firm’s stakeholders may hire their managers (while the other firm is assumed to be without managers). Kopel and Brand (2012) amend Goering’s model showing that if one firm is of CSR-type and the other firm is of profit-seeking type, then at the sub-game Nash equilibrium (SPNE) both firms choose to delegate; thus, the exogenous Goering’s assumption that only the firm of CSR-type hires managers is not robust. In particular, Kopel and Brand (2012) study the endogenous determination of the choice whether to hire managers in an exogenously assumed mixed duopoly. Fanti and Buccella (2017) assume as exogenously given the presence of sales delegation in both duopolistic firms and, differently from the other papers above mentioned, develop a game for determining whether both firms, only one firm or neither firms choose CSR rules at equilibrium. They show that at the SPNE both firms’ owners follow CSR rules, so attributing a sound game-theoretic fundament to this seemingly unprofitable behaviour. Moreover, rather counter-intuitively, they show that, at the equilibrium, an increase in the firms’ “social concern” may actually decrease consumer surplus and increase firms’ profitability. The latter finding leads the authors to conclude that “owners can find beneficial to delegate to managers engaged in CSR activities to improve the firms’ profitability.” Those contributions, however, do not consider the full spectrum of common manager’s bonus schemes that, in the standard profit-seeking firms context, are denoted as “sales delegation” (D), “relative profits” (RP) and “pure profits” (PM). However, when firms are of CSR-type, the component common to all these schemes is no longer the pure profit but the objective function of the “socially concerned” firm, that is, the pure profit plus a share of the consumer’s welfare. For the sake of simplicity we keep the same notation with regard to the first two bonus schemes, while we denote the third one—which is based on only the CSR objective function—as “pure CSR” (PCSR). Moreover, neither they study the endogenous choice whether to hire managers in a duopoly in which both firms are of CSR-type, nor they compare their findings with those of the standard managerial delegation literature.Footnote 4
 The aim of this paper is to extend the managerial delegation literature with firms of CSR-type, by investigating whether and how the firms’ CSR behaviours affect the established results of such a literature, taking into account different managerial contracts schemes. We find that the SPNE is given by the common choice of the RP scheme, whereas the CSR firm’s objective function would be highest under the P choice (and lowest under the D choice). Therefore, the well-known prisoner’s dilemma nature of the managerial delegation game also holds when firms adopt CSR behaviours. We contribute to the managerial delegation literature in three essential ways. First, we consider firms of CSR-type. Second, we explicitly explore models in which firms may differ in terms of their managerial remuneration policies. Third, we focus on an expanded analysis of three different managerial remuneration policies. The paper proceeds as follows. Section 2 introduces the model. Section 3 develops the model under common managers’ contracts, while Sect. 4 studies the cases of types of managers’ contract different between the two firms. Section 5, exploiting the models in Sects. 3 and 4, develops the game in which owners can freely choose in a simultaneous and independent way their managerial bonus system, by looking at the emergence of Nash equilibria in pure strategies and discusses the main results with their implications. Section 6 outlines the conclusions.",3
4.0,3.0,Italian Economic Journal,19 September 2018,https://link.springer.com/article/10.1007/s40797-018-0076-4,Patterns of Convergence (Divergence) in the Euro Area: Profitability Versus Cost and Price Indicators,November 2018,Monica Amici,Emmanuele Bobbio,Roberto Torrini,Female,Male,Male,Mix,,
4.0,3.0,Italian Economic Journal,28 October 2018,https://link.springer.com/article/10.1007/s40797-018-0077-3,"The Road to Permanent Work in Italy: “It’s Getting Dark, Too Dark to See”",November 2018,Maria Giovanna Bosco,Elisa Valeriani,,Female,Female,Unknown,Female,,6
4.0,3.0,Italian Economic Journal,09 May 2018,https://link.springer.com/article/10.1007/s40797-018-0075-5,Italy’s Price Competitiveness: An Empirical Assessment Through Export Elasticities,November 2018,Walter Paternesi Meloni,,,Male,Unknown,Unknown,Male,,4
4.0,3.0,Italian Economic Journal,08 March 2018,https://link.springer.com/article/10.1007/s40797-018-0071-9,The Italian CCB Reform and Usury Credit Risk: A Quantitative Analysis,November 2018,Raffaella Barone,,,Female,Unknown,Unknown,Female,,1
4.0,3.0,Italian Economic Journal,29 October 2018,https://link.springer.com/article/10.1007/s40797-018-0078-2,The Italian Districts in the Global Value Chains,November 2018,Silvia Sopranzetti,,,Female,Unknown,Unknown,Female,,6
4.0,3.0,Italian Economic Journal,31 October 2018,https://link.springer.com/article/10.1007/s40797-018-0080-8,BMI and Employment: Is There an Overweight Premium?,November 2018,Paolo Nicola Barbieri,,,Male,Unknown,Unknown,Male,,4
4.0,3.0,Italian Economic Journal,15 March 2018,https://link.springer.com/article/10.1007/s40797-018-0073-7,The Zero Lower Bound and the Asymmetric Efficacy of Monetary Policy: A View from the History of Economic Ideas,November 2018,Giancarlo Bertocco,Andrea Kalajzić,,Male,Female,Unknown,Mix,,
4.0,3.0,Italian Economic Journal,22 October 2018,https://link.springer.com/article/10.1007/s40797-018-0079-1,Internal Balance and International Competitiveness: Sports Leagues Decision Models,November 2018,Michele Bisceglia,Assunta Gabriella Caputi,Vincenzo Pacelli,Female,Female,Male,Mix,,
5.0,1.0,Italian Economic Journal,02 February 2019,https://link.springer.com/article/10.1007/s40797-019-00085-0,Regional Development Theories and Formalised Economic Approaches: An Evolving Relationship,March 2019,Roberta Capello,,,Female,Unknown,Unknown,Female,,6
5.0,1.0,Italian Economic Journal,20 November 2018,https://link.springer.com/article/10.1007/s40797-018-0081-7,Should I Stay or Should I Go? Firms’ Mobility Across Banks in the Aftermath of the Financial Crisis,March 2019,Davide Arnaudo,Giacinto Micucci,Paola Rossi,Male,Male,Female,Mix,,
5.0,1.0,Italian Economic Journal,17 January 2019,https://link.springer.com/article/10.1007/s40797-018-00082-9,Does the Project Design Matter for the Performance of Infrastructure Execution? An Assessment for Italy,March 2019,Marina Cavalieri,Rossana Cristaudo,Calogero Guccio,Female,Female,Male,Mix,,
5.0,1.0,Italian Economic Journal,18 February 2019,https://link.springer.com/article/10.1007/s40797-019-00083-2,Is Long-Term Non-employment a Lifetime Disease?,March 2019,Bruno Contini,Roberto Quaranta,,Male,Male,Unknown,Male,,3
5.0,1.0,Italian Economic Journal,08 March 2019,https://link.springer.com/article/10.1007/s40797-019-00087-y,Inefficiency in Childcare Production: Evidence from Italian Microdata,March 2019,Luigi Brighi,Paolo Silvestri,,Male,Male,Unknown,Male,,
5.0,1.0,Italian Economic Journal,25 January 2019,https://link.springer.com/article/10.1007/s40797-019-00084-1,Dynamics of Political Budget Cycle,March 2019,Ganesh Manjhi,Meeta Keswani Mehra,,Male,Female,Unknown,Mix,,
5.0,1.0,Italian Economic Journal,04 March 2019,https://link.springer.com/article/10.1007/s40797-019-00088-x,Correction to: Dynamics of Political Budget Cycle,March 2019,Ganesh Manjhi,Meeta Keswani Mehra,,Male,Female,Unknown,Mix,,
5.0,1.0,Italian Economic Journal,09 February 2019,https://link.springer.com/article/10.1007/s40797-019-00086-z,Labour Supply and Welfare Effects of Disability Insurance: A Survey,March 2019,Chiara Dal Bianco,,,Female,Unknown,Unknown,Female,,2
5.0,2.0,Italian Economic Journal,23 May 2019,https://link.springer.com/article/10.1007/s40797-019-00095-y,"Local Development, Urban Economies and Aggregate Growth",July 2019,Antonio Accetturo,Andrea Lamorgese,Paolo Sestito,Male,Female,Male,Mix,,
5.0,2.0,Italian Economic Journal,08 June 2019,https://link.springer.com/article/10.1007/s40797-019-00097-w,Historical Origins and Developments of Italian Cities,July 2019,Antonio Accetturo,Sauro Mocetti,,Male,Male,Unknown,Male,"The distribution of the population over space, across cities—the so-called urban network—plays a crucial role for the economic growth of a country (Castells-Quintana 2017), especially in more advanced economies (Frick and Rogriguez-Pose 2018). The Italian history of urbanization is—at the same time—both ancient and peculiar. Italy is a country that has long been characterized as an urban civilization (Michaels and Rauch 2018); in the late Roman empire in Italy there were 2.5 cities or villages per 1000 km2 against 1.2 in France, 1.0 in Spain, and 0.5 in Germany and England.Footnote 1 The Italian cities, although in sharp decline in the early Middle Ages, have regained considerable economic importance since before the Renaissance. According to Bairoch et al. (1988) and Malanima (1998, 2005) at the beginning of the 14th century the average European urbanization rate was 9.5%, less than half of that of Italy; in that period, on the European continent, only Flanders, Brabant and Holland had comparable urbanization rates. However, in the following centuries, several economic and social shocks (e.g. epidemics, trade displacement away from the Mediterranean, political fragmentation, and the industrial revolution) made the Italian context less favorable for urbanization; Italian cities lost momentum and, as a consequence, their relevance for the national economy has become much more limited in comparison with other advanced economies. Today, the share of the population living in an urban area with at least 500,000 inhabitants (31%) is smaller than in France (41%), Germany (40%), Spain (38%) and the UK (41%). The aim of the present paper is to review and discuss the historical origins and subsequent development of the Italian urban system. We focus, in particular, on two main questions: why did the Italian urban system evolve in this way? Why did the main urban areas give such a (relatively) limited contribution to the national economy? The answers to these questions provided in the present paper are necessarily selective. We first describe the formation and the evolution of the Italian urban network in an historical perspective by examining the role of geography and historical shocks in explaining these patterns. Then we show more recent trends on the evolution of cities, focusing in particular in the second half of the 20th Century when, structural change away from agriculture made the Italian economy much more sensitive to agglomeration economies. Finally, we discuss the role of congestion costs (housing costs in particular) in explaining more recent patterns. Our results show that historical shocks played a relevant role in setting the Italian urban network in the middle ages and in modern era. Political fragmentation and constant military threats contributed to the creation of a polycentric urban system in the North and two parasitical urban centers (Naples and Palermo) in the South. As a result, in the second half of the XIX century (when both fragmentation and military threats were over), history made the Italian urban system unfit to accommodate the large economic transformations that were characterizing other urban areas in Europe. This feature was particularly evident at the start of the Italian industrialization process: Italian urban areas attracted rural population (rural–urban migrations are a typical engine of early industrialization processes) in the 1950s’ and the 1960s’ but stopped growing from 1970s’; this suggests that Italian large agglomerations quickly reached their ceilings. Moreover, Italian cities—though smaller—are as congested as other European urban areas with possible negative consequences on the aggregate growth of the country. The paper is organized as follows. Section 2 presents some issues linked to the definition of an urban area. Sections 3 and 4 presents an historical account of the relevance of Italian cities and its geographical and historical determinants. Section 5 shows some recent patterns in the dynamics of urban areas. Section 6 discusses the relevance of congestion costs. Section 7 concludes and presents some policy implications.",7
5.0,2.0,Italian Economic Journal,28 June 2019,https://link.springer.com/article/10.1007/s40797-019-00101-3,Stylized Facts on Italian Cities,July 2019,Andrea Lamorgese,Andrea Petrella,,Female,Female,Unknown,Female,,2
5.0,2.0,Italian Economic Journal,20 June 2019,https://link.springer.com/article/10.1007/s40797-019-00099-8,The Wage Premium in Italian Cities,July 2019,Andrea R. Lamorgese,Elisabetta Olivieri,Marco Paccagnella,Female,Female,Male,Mix,,
5.0,2.0,Italian Economic Journal,24 May 2019,https://link.springer.com/article/10.1007/s40797-019-00094-z,The House Price Gradient: Evidence from Italian Cities,July 2019,Elisabetta Manzoli,Sauro Mocetti,,Female,Male,Unknown,Mix,,
5.0,2.0,Italian Economic Journal,21 June 2019,https://link.springer.com/article/10.1007/s40797-019-00098-9,Human Capital Differentials Across Urban and Rural Areas in Italy. The Role of Migrations,July 2019,Rosario Maria Ballatore,Vincenzo Mariani,,Male,Male,Unknown,Male,,7
5.0,3.0,Italian Economic Journal,06 May 2019,https://link.springer.com/article/10.1007/s40797-019-00090-3,Heterogeneous Firms and the North–South Divide in Italy,October 2019,Armando Rungi,Francesco Biancalani,,Male,Male,Unknown,Male,,13
5.0,3.0,Italian Economic Journal,28 May 2019,https://link.springer.com/article/10.1007/s40797-019-00096-x,The Demand and Supply for Popular Culture: Evidence from Italian Circuses,October 2019,Concetta Castiglione,Roberto Zanola,,Female,Male,Unknown,Mix,,
5.0,3.0,Italian Economic Journal,03 August 2019,https://link.springer.com/article/10.1007/s40797-019-00103-1,Does the Same FDI Fit All? How Competition and Affiliates Characteristics Affect Parents’ Productivity,October 2019,Giorgia Giovannetti,Enrico Marvasi,Giorgio Ricchiuti,Female,Male,Male,Mix,,
5.0,3.0,Italian Economic Journal,15 May 2019,https://link.springer.com/article/10.1007/s40797-019-00092-1,Policy Lag and Sustained Growth,October 2019,Shunsuke Shinagawa,Eiji Tsuzuki,,Male,Male,Unknown,Male,,3
5.0,3.0,Italian Economic Journal,06 May 2019,https://link.springer.com/article/10.1007/s40797-019-00091-2,State Owned Enterprises (SOEs) and Non Transparent Trade Policies,October 2019,Gianpaolo Rossini,,,Male,Unknown,Unknown,Male,,
5.0,3.0,Italian Economic Journal,23 March 2019,https://link.springer.com/article/10.1007/s40797-019-00089-w,Vaccination as a trade-off between risks,October 2019,David Crainich,Louis Eeckhoudt,Mario Menegatti,Male,Male,Male,Male,,8
5.0,3.0,Italian Economic Journal,04 July 2019,https://link.springer.com/article/10.1007/s40797-019-00102-2,Note on Multi-Sectoral Growth and Sustainability Pricing in a Leontief-Type Model with Technical Progress,October 2019,Carlo D’Adda,,,Male,Unknown,Unknown,Male,,1
6.0,1.0,Italian Economic Journal,08 February 2020,https://link.springer.com/article/10.1007/s40797-020-00121-4,Globalization Cycles,March 2020,Maurice Obstfeld,,,Male,Unknown,Unknown,Male,,6
6.0,1.0,Italian Economic Journal,04 October 2019,https://link.springer.com/article/10.1007/s40797-019-00113-z,Efficiency-Wage Competition: What Happens as the Number of Players Increases?,March 2020,Marco Guerrazzi,,,Male,Unknown,Unknown,Male,"Some versions of the efficiency-wage theory grounded on adverse-selection recognize that firms may stand in a situation comparable to monopsony in the labour market, in the sense that employers might be able to set employment as well as their wage offer aiming at maximizing profits (e.g. Solow 1979). As argued by a number of scholars, however, in that theoretical setting a special kind of competition could hold instead among productive firms (cf. Hahn 1987; Summers 1988; van de Klundert 1989; Jellal and Wolff 2003). Specifically, even if the labour market experiences a persistent excess of supply, it can happen that firms will avoid wage cuts not only because this would lower their profitability, but also because lower wages would enhance the productivity of their output competitors. In previous works, such a strategic relationship for the optimal wage behaviour of firms has been dubbed as efficiency-wage competition, i.e., a situation in which the profit-maximizing wage bid of a given productive unit depends on the wage bids put forward by all the other firms operating in the economy (cf. Guerrazzi 2013; Guerrazzi and Sodini 2018). The existing literature on efficiency-wage competition focuses only on 2-firm—or 2-sector—economies and until now nothing has been said about the consequences triggered by an increase in the number of players. In this paper, I aim at filling that gap by exploring what happens in that theoretical framework as the number of competing firms increases. In detail, drawing on Guerrazzi (2013), I study the optimal wage behaviour of firms when all the productive units find profitable to bid and pay the same wage and there are no labour-supply constraints so that some individuals might actually experience involuntary unemployment. Moreover, I analyse the dynamic behaviour of wages in the neighbourhood of the symmetric Nash equilibrium and I explore the features of the full employment allocation. The findings of this theoretical exploration reveal that the actual shape of the effort function is crucial in determining key features of the model economy. In this regard, the efficiency-wage literature usually has exploited two distinct analytical formulations for the link between wages and workers’ productivity, i.e., sigmoid and concave effort functions (cf. Stiglitz 1973, 1976; Hahn 1987). As I will show below, the choice of one specific formulation leads to completely different outcomes with respect to the other option. Specifically, when workers are endowed with a concave (sigmoid) effort function, the wage behaviour of firms follows a collusive (competitive) pattern. Moreover, assuming that firms adjust their current wage bid on the basis of the lagged bids put forward by their competitors, the adoption of a concave (sigmoid) effort function implies that the symmetric Nash equilibrium is locally unstable (stable) and the speed of divergence (convergence) is an increasing function of the number of productive units engaged in the efficiency-wage competition process. Furthermore, with a concave (sigmoid) effort function, the full employment of workers that are willing to work at the prevailing wage is characterized by a degree of labour exploitation that increases (decreases) together with the number of firms required to sustain that allocation. Considering efficiency-wage economies with adverse selection, the theoretical findings outlined above may provide some guidance in assessing what type of effort function is the best alternative for a certain economy. At the same time, these results may have intriguing implications for the existence and persistence of involuntary unemployment and for the implementation of policies aimed at increasing the level of employment. This paper is arranged as follows. Section 2 reviews some of the related literature. Section 3 introduces the theoretical framework. Section 4 explores how wages, effort, employment and profits prevailing in a symmetric Nash equilibrium depend on the number of competing firms. Section 5 analyses the dynamics of wages in the neighbourhood of the Nash equilibrium. Section 6 addresses the issue of labour exploitation when there is no involuntary unemployment. Finally, Sect. 7 concludes by offering some theoretical and policy implications for fighting unemployment in efficiency-wage economies.",
6.0,1.0,Italian Economic Journal,12 October 2019,https://link.springer.com/article/10.1007/s40797-019-00115-x,Trends in Women’s Employment and Poverty Rates in OECD Countries: A Kitagawa–Blinder–Oaxaca Decomposition,March 2020,Rense Nieuwenhuis,Wim Van Lancker,Bea Cantillon,Male,Male,Female,Mix,,
6.0,1.0,Italian Economic Journal,17 September 2019,https://link.springer.com/article/10.1007/s40797-019-00108-w,Women in the Top of the Income Distribution: What Can We Learn From LIS-Data?,March 2020,Roman Bobilev,Anne Boschini,Jesper Roine,Male,Female,Male,Mix,,
6.0,1.0,Italian Economic Journal,24 October 2019,https://link.springer.com/article/10.1007/s40797-019-00116-w,Deep and Extreme Child Poverty in Rich and Poor Nations: Lessons from Atkinson for the Fight Against Child Poverty,March 2020,Yixia Cai,Timothy Smeeding,,Unknown,Male,Unknown,Male,"Child poverty has drawn increasing attention from social scientists and policy makers in the past decades. Nations should be judged on how they treat their children, especially those facing the uninsurable risk of being born to parents who are unable or unwilling to support them. A number of studies from developed countries illustrate the negative effects of children living in poverty, especially deep poverty (incomes less than half of the poverty line) on their future development. These include chronic health and psychological problems as well as poor educational attainment compared to their middle class and affluent peers in rich countries (Almond et al. 2018; Magnuson and Votruba-Drzal 2008; Rainwater and Smeeding 2003; Smeeding and Thévenot 2016). In some less-developed but still rapidly growing nations, extreme child poverty (incomes per person below $2.00 per day) is still a major issue. Although some doubt the goal will be reached, governments around the world have committed to a new set of sustainable development goals (SDGs) that include ending extreme poverty for everyone and everywhere by 2030 (Gertz and Kharas 2018; World Bank 2018b). Usually, to examine the extremely disadvantaged, the World Bank conducts studies using spotty microdata and methods that are not used to measure poverty in rich countries, but new efforts on shared prosperity and poverty reduction at higher income and poverty line levels have improved these estimates while generating new challenges for policy and poverty (World Bank 2018b). In addition, the family size adjustments that were used in earlier reports assume no economies of scale in household consumption. We will also overcome this obstacle in the paper. Deep and extreme poverty issues have recently surfaced in very rich but unequal nations such as the US. Nobel laureate Angus Deaton (2018) points to high levels of extreme disadvantage in the US, citing a stunning UN report on US poverty by Alston (2017), United Nations Special Rapporteur on extreme poverty and human rights. Alston found very poor child conditions such as ringworm and toothless children due to dental decay in his examination of poverty in various areas in the US. Deaton goes on to compare poverty in rich and poor countries, using a method invented by Allen (2017); Deaton claims that “there are 5.3 million Americans who are absolutely poor by global standards. This is a small number compared with the one for India, for example, but it is more than in Sierra Leone (3.2 million) or Nepal (2.5 million), about the same as in Senegal (5.3 million) and only one-third less than in Angola (7.4 million).” Approximately half of the people in deep poverty are children (Stevens 2019). Hence, rich nations are becoming aware of extreme poverty in their midst (see also Edin and Shaefer 2015). As major emerging nations experience income growth, they begin to look more like rich Western nations, especially in cities and urban areas and even in remote rural areas. In this paper, we examine child poverty in a set of emerging nations: Brazil, China, India and the Republic of South Africa (RSA), and in another set of large and rich English-speaking nations: Australia, Canada, Ireland, the United Kingdom (UK) and the United States (US). Taken together, these countries include about a third of the world’s population and more than 25% of all children. The aim of our paper is to consistently measure relative (“deep”) and absolute (“extreme”) child poverty in a more global context, using a set of rich and poor nations, and to think about what could be done to alleviate these conditions. Three different institutions (including the market-driven aspect, private redistribution through inter-household transfers, and traditional public redistribution through cash transfers and tax systems) are also recognized, especially because of their differences across nations. The Luxembourg Income Study (LIS) (2019) allows us to consistently examine differences in child poverty using multiple poverty measures, multiple periods and a collection of income measures in a fully flexible and comparable way. The paper is very much in line with Tony Atkinson’s concerns for poor children as expressed in his recent research on global poverty with the World Bank, in his work with rich countries such as LIS and his prescriptions for ending poverty among children in all nations (Atkinson 2015, 2016). We say this knowing that Tony would be aghast at the depth of child poverty that we explore here, far below the 60% of national median poverty standards that he repeatedly defended and upheld (e.g. Atkinson 2015). We structured the paper as follows. Below we look at the major current child poverty issues in poor and rich nations. The data and methods are presented in the third section, which is followed by results on child poverty levels and trends in nine countries of interest using both relative and absolute measures from 2002 to the most current year. We also analyze the influence of the different components of household income resources on deep-child-poverty rates in order to examine the role of market and redistributive effects, which materialize through transfers and child benefits, on poverty reduction. We close the paper with a discussion of the further implications for effective interventions that improve children’s life chances in a local context.",2
6.0,1.0,Italian Economic Journal,28 September 2019,https://link.springer.com/article/10.1007/s40797-019-00112-0,Perspectives on Poverty in Europe. Following in Tony Atkinson’s Footsteps,March 2020,Stephen P. Jenkins,,,Male,Unknown,Unknown,Male,"This paper provides a number of perspectives on poverty in Europe, drawing heavily on the work of Tony Atkinson. The paper’s title refers to Europe rather than a set of specific countries. The distinction is important: the discourse on poverty within European countries has taken an increasingly Europe-level dimension as the integration process has deepened. This is reflected in the ways in which poverty is now conceptualised, measured, and monitored. The goal of this paper is to illustrate these developments, referring to the historical experience, while also providing empirical evidence about poverty trends using several indicators, together with remarks about the direction of anti-poverty policy in an era of austerity and greater questioning of the roles of EU-level versus national institutions and initiatives. In the course of the paper, I pay homage to Tony Atkinson, a true European and internationalist who was dedicated to reducing poverty everywhere. Tony contributed definitive studies of income distribution, poverty and social inclusion in Europe over several decades, as I shall first briefly explain. For extensive overviews of Tony’s work, including many other topics, see inter alia Aaberge et al. (2017) and Jenkins (2017). Consider, for example, Tony’s 1992 study with John Micklewright of the distribution of income in Eastern Europe before the Iron Curtain disappeared. This is distinguished by its originality (few had examined this topic before) and by its careful assemblage of data from a range of sources. Alongside material about earnings inequality are chapters on the distribution of household income, issues of measurement, and discussion of poverty and the safety net. Poverty in Europe (1998), based on Tony’s 1990 Yrjö Jahnsson Lectures, is an extensive review of how to measure poverty in Europe and assess differences across years and countries, the economics of poverty, and the political economy of anti-poverty policy in Europe. His 2002 and 2007 books, joint with Bea Cantillon, Eric Marlier, and Brian Nolan, on Social Indicators and The EU and Social Inclusion are landmark studies in the theory and practice of social indicators. This research forms the basis of the extensive set of indicators used by the EU for more than a decade to monitor social inclusion and now institutionalised in their Statistics on Income and Living Conditions (EU-SILC), the source for most of the estimates presented later in this paper. Tony and colleagues’ work develops a clear set of principles related to measurement validity and data quality, also acknowledging the practical needs of policy-makers and citizens for transparent and timely information. The team’s work did not stop with their initial analysis (2002) but also considered at length implementation challenges and how to take the EU social inclusion process further forward (the 2007 volume). This continued with the Net-SILC projects that Tony led with Eric Marlier. Their 2010 and 2017 edited volumes (the latter co-edited with Ann-Catherine Guio) are extensive collections considering the strengths and weaknesses of EU-SILC, as well as the potential for modifications and extensions to it. Tony’s 2010 Macerata Lecture, although an unpublished working paper, stands out as a valuable review of the progress being made to reduce poverty and increase social inclusion in Europe, written at the time the EU-2020 indicators were being decided upon. Finally, I refer to Tony’s 2017 report for the World Bank and his last book (2019). Monitoring Global Poverty brings to a world stage many of the issues Tony had considered in the European context, including going beyond defining poverty only in monetary terms to include additional indicators, the relationships between globally harmonized and national estimates, and careful attention to data requirements and to data quality. Measuring Poverty Around the World continues these themes but does so in broader fashion. Rich countries, including those in Europe, are integrated into the conceptual discussion alongside middle- and low-income countries. The recommendation of Tony’s 2017 report to derive genuinely global poverty estimates (rather than the previous practice of having estimates covering middle- and low-income countries only) is taken up again and illustrated in his 2019 book. The World Bank is now implementing the recommendation. There are four enduring themes in all of Tony’s work that are also reflected in his work on poverty in Europe. First, Tony always believed that the topic of income distribution is an integral part of economics. Thus policy about poverty cannot and should not be seen separately from economic policy. For example, he has written that ‘[o]ne important argument in favour of a poverty target is that it would place anti-poverty policy on the same footing as macro-economic policy’ (1998: 151), and: We cannot consider anti-poverty policy in isolation from other policies. The scope for financing income maintenance depends on macro-economic policy choices, on levels of government spending, and on rates of inflation. The use of transfer payments or other instruments of anti-poverty policy … in turn have implications for economic policy. Social and economic policy are interdependent. This may appear obvious, but it remains the case that social policy is often placed in a separate compartment. (Atkinson 1998: 151.) Second, Tony emphasised the importance of EU-level institutions and actions for making progress against poverty in each Member State. The EU context provided leadership, ‘contextualised benchmarking’ and mutual learning across countries. Third, Tony believed strongly in the importance of thinking clearly about measurement principles but not in isolation—how these relate to policy formulation and monitoring are key and integral issues as well. And that leads to the fourth theme: that it is essential for poverty analysis and poverty to get the statistical infrastructure right, including data. Tony also brought two distinctive personal traits to all his work on poverty analysis and poverty. He was an internationalist. Alongside his deep concern and knowledge of the British situation, he was a Europhile, very knowledgeable about Europe and he served various European organisations (for example he was a member of the Conseil d’Analyse Economique advising the French Prime Minister 1997–2001, and of the European Statistical Governance Advisory Board 2009–2011). Global perspectives have always been there too, not only in his final publications. It was in 1998, not 2017, that Tony wrote: ‘as we think about developments in Europe, we should not lose sight of the objective of eliminating world poverty, which in my view has precedence’ (1998: 152). Tony’s second trait is his progressive and optimistic mind-set when thinking about anti-poverty policy at times when it appears that little progress is being made. In what follows I elaborate on the Atkinsonian themes that I have drawn attention so far, very much following in the footsteps of a giant. I address four topics. Section 2 shows how our capacities to monitor in Europe have improved substantially over recent decades. Section 3 illustrates how progress on EU poverty reduction has been disappointing and reviews analysis by Tony and others considering the reasons for this. In Sect. 4, I discuss some conceptual and measurement issues with empirical illustrations relating to how we should measure ‘poverty’ and specify policy targets. In the final section, I consider the future direction of EU-level anti-poverty actions in the light of the earlier discussion, ending by contrasting Tony’s optimistic approach with the pessimistic perspectives which are perhaps more prevalent.",14
6.0,1.0,Italian Economic Journal,13 August 2019,https://link.springer.com/article/10.1007/s40797-019-00106-y,Internal Migration in Italy: The Role of Migration Networks,March 2020,Romano Piras,,,Male,Unknown,Unknown,Male,,7
6.0,1.0,Italian Economic Journal,09 December 2019,https://link.springer.com/article/10.1007/s40797-019-00119-7,The Dynamics of Wealth Concentration: Thoughts on Tony Atkinson’s Contributions,March 2020,Salvatore Morelli,,,Male,Unknown,Unknown,Male,"The study of wealth distribution in economics is central to explain the evolution of households’ wellbeing, and to assess the functioning of our macroeconomy as well as the inclusive nature of our societies. “Wealth takes on new significance for the prosperity of households” as remarked by Mario Draghi in 2007 as Governor of the Bank o Italy. The remarks, made during the conference “The Luxembourg Wealth Study: Enhancing Comparative Research on Household Finance”, suggested that the shift of focus from income to wealth may be the direct result of the ongoing transformations of most advanced economies which are witnessing ageing population, more volatile and insecure job markets, as well as a reduced scope for social safety nets. Unequal distribution of wealth also begets unequal command over productive resources (or ‘control over industries’ as remarked in Atkinson 1972). The resulting unbalance of powers can distort, in turn, the way our economy and democracy work, providing compelling instrumental justifications to focus on wealth distribution. Wealthy elites, for instance, may be “subverting legal, political and regulatory institutions to work in their favour”, further increasing their level of wealth (Glaeser et al. 2003). It is therefore not surprising that the study of wealth distribution and concentration is “at the heart of the broad research field of economic inequality” (Cowell and Van Kerm 2015). The growing recognition of the role of aggregate household balance sheets and its distribution is also witnessed by growing investments in macro and micro data capacity carried by leading central banks, statistical offices, and independent research institutions around the world over the past decades. Since 2000, the Bank of Italy publishes quarterly data on the stocks and flows of assets and liabilities classified by institutional sectors (e.g. including the household sector and the profit institutions serving households). The Luxembourg Wealth Study (LWS) was launched in 2003 to enhance cross-country comparability of micro data on wealth distribution. After a decade, the European Central Bank released in 2013 the first Eurosystem’s Household Finance and Consumption Survey (HFCS), collecting household-level data on households’ finances and consumption. More recently, as witnessed by the World Inequality Database (WID), the empirical and theoretical research turned the spotlight on the shape and the evolution of the upper tail of the wealth distribution. In doing so one can preserve the focus on a substantial share of total wealth in the economy despite the tiny share of the population involved (e.g. 0.01%, 1, or 5%), for wealth is extremely concentrated. Estimating the size distribution of wealth among individuals and households and understanding its main determinants remain crucial steps to guide policy interventions. Although the main ingredients of wealth accumulation are clear, there is no agreement yet on what is actually driving the observed rise in wealth concentration in advanced economies. Is most of the wealth accumulation driven by self-made or inherited wealth? And what part of the change in wealth holding is due to revaluation of existing stocks of wealth? These fundamental issues may have strong political implications, and as such, generate heated controversies and are being strongly debated with new theoretical and empirical analyses. Anthony Atkinson devoted considerable amount of effort to these issues, making fundamental contributions to the field. This note attempts to link some of Atkinson’s main insights to the current developments of the literature.",
6.0,2.0,Italian Economic Journal,23 June 2020,https://link.springer.com/article/10.1007/s40797-020-00130-3,The Credibility Revolution in the Empirical Analysis of Crime,July 2020,Paolo Pinotti,,,Male,Unknown,Unknown,Male,,8
6.0,2.0,Italian Economic Journal,24 February 2020,https://link.springer.com/article/10.1007/s40797-020-00124-1,"Law Enforcement, Social Control and Organized Crime: Evidence from Local Government Dismissals in Italy",July 2020,Federico Cingano,Marco Tonello,,Male,Male,Unknown,Male,"The presence of organized crime is detrimental for local economic outcomes as growth and productivity, the allocation of public spending, corruption, let alone public safety and law offenses (Pinotti 2015; Barone and Narciso 2015). And yet there is limited empirical evidence on the effectiveness of policies aimed at fighting organized crime, especially in the territories where its presence is pervasive.Footnote 1 In this paper we estimate the consequences for crime of an Italian policy imposing the dismissal of local administrations suspected of Mafia infiltration (a measure we label local government dismissals—LGDs). Introduced in the early 1990s as an unconventional policy tool to combat the sharp increase in the activity of organized crime in the South of Italy, LGDs have since then been implemented in dozens of cases (Priolo 2004). Are LGDs effective in lowering crime? The measure implies the dismissed administration is replaced by an external commission which governs the municipality until new elections, typically after around 2 years. Hence, it induces a sharp (though possibly temporary) increase in law enforcement in territories where the presence of the state is perceived as extremely weak. Being largely an administrative act, however, LGDs do not imply the strengthening of formal deterrence, as increased police deployment or financial resources allocated to public order. In addressing the above question we therefore adopt a broad perspective looking at alternative potential channels. The first is that, by breaking its ties with local politicians, dismissals have a direct effect on the presence of mafia and its local criminal activity. Alternatively, LGDs might more generally affect local crime by inducing the perception of greater enforcement if—as posited by influential theories since Sah (1991)—perceptions matter for deterrence. Quantifying the consequences of LGDs on crime is challenging given that Mafia-infiltrated municipalities may feature peculiar patterns of criminal offenses. For example, if dismissals occur as a consequence of high levels of crime then simple comparisons with other municipalities would likely over-emphasize their effectiveness. Absent suitable instruments, our analysis leverages on newly available time series of municipality level data on a detailed list of criminal offenses. This allows us to exploit within-municipality variation, comparing changes in crime rates around the intervention net of time-invariant differences in crime between municipalities. Because dismissals might still occur as a consequence of spikes in crime, we will extend the analysis to the immediate neighborhood of the dismissed municipalities, often excluding the latter from the sample. Identification arises from contrasting changes in crime in (neighbors of) a dismissed municipality with contemporaneous changes in (neighbors of) municipalities that are not dismissed in the same year. Finally, we exploit the fact that reverse causality should be more of a concern for mafia-related crime (homicide, threat, extortion, arson, drug-trafficking, usury, etc.) and examine them separately from petty crime, including sexual offenses and thefts. Preliminary inspections of the data reassuringly show that in treated areas the patterns of petty crime are fairly stable prior to the intervention and do not anticipate the dismissal. They do drop in its aftermath, though. We find that LGDs are associated with a significant and persistent fall in minor (petty) crimes, whose incidence is estimated to decrease by 9% on impact, and by 12.4% in the years following the dismissal. The fall in crime is detected irrespective of the dismissed municipality being included in the sample, and is robust to several sensitivity checks. Its geographical scope seems limited though, as dismissals do not affect crime rates beyond the municipality’s immediate neighbors. Our findings imply social gains from lower petty crimes ranging between 100 and 230 k Euro per dismissal, depending on the specification and discount rate. By contrast, dismissals do not prove effective in reducing mafia-related (power and enterprise syndicate) offenses, implying little direct effects of the policy on the activity of organized crime. Such heterogeneous responses suggest the fall in minor offenses should not be ascribed to a ‘lay-low’strategy, whereas local crime syndicates command a generalized reduction of criminal activities following the intervention. With formal enforcement unlikely to be a relevant determinant either, we tentatively explore the role of perceived deterrence, as described for example by Lochner (2007). With LGDs, individual perceptions may change as a consequence of direct social learning (e.g. if friends, relatives, or affiliated to the same criminal organizations are caught), of indirect social learning (e.g. if some case determines a higher media pressure), or following changes in the surrounding environment (e.g. a greater urban order). Our results are consistent with forms of perception-based deterrence spurred by the signaling value of the policy or media attention. Moreover, LGDs look more effective where the endowment of civic capital is higher, consistently with social control theories (Keizer et al. 2008). A large body of research studies the origins of Mafia-type organizations, their functioning and internal organization (Buonanno et al. 2015; Dimico et al. 2017; Bandiera 2003; Mastrobuoni 2015; Campaniello et al. 2016). Fewer works attempted to assess their economic consequences (Pinotti 2015; Barone and Narciso 2015; Daniele and Marani 2011; Mirenda et al. 2019), and very little is known as to the effectiveness of policies attempting to increase law enforcement in areas where organized crime is pervasive. We contribute suggesting that enforcement policies can successfully reduce the incidence of (minor) crime in Mafia-dominated regions, and trace the effect to perception-based mechanisms, whose effectiveness is not much investigated relative to formal enforcement. Other recent work focused on LGDs but evaluated their consequences on public spending (Acconcia et al. 2014; Galletta 2017; Cataldo and Mastrorocco 2019) and the quality of local politicians (Daniele and Geys 2015), rather than crime rates. More generally, this work contributes to the empirical literature evaluating the effects of increasing enforcement on local crime (Machin and Marie 2011; Di Tella and Schargrodsky 2004), although the specific policy considered does not imply an increase in actual deterrence (police deployment or additional transfers). By studying the consequences of LGD beyond the borders of the dismissed municipality, it also relates to empirical works on the cross-border effects of law enforcement (Gonzalez-Navarro 2013; Dube et al. 2013; Bronars and Lott 1998). The paper is organized as follows. Section 2 presents the institutional setting. Section 3 describes the dataset, the variables construction and presents some descriptive statistics. Section 4 explains the empirical strategy, while Sect. 5 discusses the main results, presents the robustness checks, and provides an attempt to size the social benefits from the implementation of the policy. Section 6 discusses the possible mechanisms. Section 7 concludes and derives policy implications.",3
6.0,2.0,Italian Economic Journal,02 March 2020,https://link.springer.com/article/10.1007/s40797-020-00125-0,Partners in Crime: Evidence from Recidivating Inmates,July 2020,Giovanni Mastrobuoni,Pierre Rialland,,Male,Male,Unknown,Male,,2
6.0,2.0,Italian Economic Journal,19 April 2020,https://link.springer.com/article/10.1007/s40797-020-00128-x,The Economic Impact of Organized Crime Infiltration in the Legal Economy: Evidence from the Judicial Administration of Organized Crime Firms,July 2020,Francesca Calamunci,Francesco Drago,,Female,Male,Unknown,Mix,,
6.0,2.0,Italian Economic Journal,24 February 2020,https://link.springer.com/article/10.1007/s40797-020-00122-3,Looking at ‘Crying Wolf’ from a Different Perspective: An Attempt at Detecting Banks Under- and Over-Reporting of Suspicious Transactions,July 2020,Mario Gara,Claudio Pauselli,,Male,Male,Unknown,Male,,4
6.0,2.0,Italian Economic Journal,06 March 2020,https://link.springer.com/article/10.1007/s40797-020-00126-z,Blowing in the Wind: The Infiltration of Sicilian Mafia in the Wind Power Business,July 2020,Valeria Virginia Checchi,Michele Polo,,Female,Female,Unknown,Female,,2
6.0,2.0,Italian Economic Journal,24 February 2020,https://link.springer.com/article/10.1007/s40797-020-00123-2,Lost in Corruption. Evidence from EU Funding to Southern Italy,July 2020,Ilaria De Angelis,Guido de Blasio,Lucia Rizzica,Female,Male,Female,Mix,,
6.0,3.0,Italian Economic Journal,31 August 2019,https://link.springer.com/article/10.1007/s40797-019-00105-z,Completing the Economic and Monetary Union: Wisdom Come Late?,November 2020,Francesco Spadafora,,,Male,Unknown,Unknown,Male,,
6.0,3.0,Italian Economic Journal,25 July 2019,https://link.springer.com/article/10.1007/s40797-019-00104-0,The Real Effects of Endogenous Defaults on the Interbank Market,November 2020,Massimo Minesso Ferrari,,,Male,Unknown,Unknown,Male,,
6.0,3.0,Italian Economic Journal,14 September 2019,https://link.springer.com/article/10.1007/s40797-019-00109-9,Evaluating the Impact of the US–China Trade War on Euro Area Economies: A Tale of Global Value Chains,November 2020,Ilaria Fusacchia,,,Female,Unknown,Unknown,Female,,4
6.0,3.0,Italian Economic Journal,29 June 2019,https://link.springer.com/article/10.1007/s40797-019-00100-4,Time Allocation and Snacks and Sugar Sweetened Beverages Taxation,November 2020,Alberto Pench,,,Male,Unknown,Unknown,Male,,
6.0,3.0,Italian Economic Journal,13 September 2019,https://link.springer.com/article/10.1007/s40797-019-00110-2,Present Value Models and the Behaviour of European Financial Markets,November 2020,Gian Maria Tomat,,,Male,Unknown,Unknown,Male,,
6.0,3.0,Italian Economic Journal,28 September 2019,https://link.springer.com/article/10.1007/s40797-019-00111-1,Does Institutional Quality Matter for Infrastructure Provision? A Non-parametric Analysis for Italian Municipalities,November 2020,Marina Cavalieri,Calogero Guccio,Ilde Rizzo,Female,Male,Female,Mix,,
6.0,3.0,Italian Economic Journal,24 September 2019,https://link.springer.com/article/10.1007/s40797-019-00107-x,Governor Baffi’s View on the Italian Great Inflation,November 2020,Elena Seghezza,,,Female,Unknown,Unknown,Female,,
7.0,1.0,Italian Economic Journal,26 February 2020,https://link.springer.com/article/10.1007/s40797-020-00120-5,When History Leaves a Mark: A New Measure of Roman Roads,March 2021,Vania Licio,,,Female,Unknown,Unknown,Female,,3
7.0,1.0,Italian Economic Journal,16 June 2020,https://link.springer.com/article/10.1007/s40797-020-00132-1,Welfare and Convergence Speed in the Ramsey Model Under Two Classes of Gorman Preferences,March 2021,Wilson da Cruz Vieira,Alberto Bucci,Simone Marsiglio,Male,Male,Female,Mix,,
7.0,1.0,Italian Economic Journal,07 October 2020,https://link.springer.com/article/10.1007/s40797-020-00135-y,"The Productivity Gap Among Major European Countries, USA and Japan",March 2021,Giorgio Calcagnini,Germana Giombini,Giuseppe Travaglini,Male,Female,Male,Mix,,
7.0,1.0,Italian Economic Journal,21 July 2020,https://link.springer.com/article/10.1007/s40797-020-00133-0,Recent Trends in Economic Activity and TFP in Italy with a Focus on Embodied Technical Progress,March 2021,Alessandro Mistretta,Francesco Zollino,,Male,Male,Unknown,Male,,
7.0,1.0,Italian Economic Journal,13 March 2020,https://link.springer.com/article/10.1007/s40797-020-00127-y,The Multisector Applied Computable General Equilibrium Model for Italian Economy (MACGEM-IT),March 2021,Claudio Socci,Francesco Felici,Renato Loiero,Male,Male,Male,Male,,1
7.0,1.0,Italian Economic Journal,16 June 2020,https://link.springer.com/article/10.1007/s40797-020-00131-2,ITER: A Quarterly Indicator of Regional Economic Activity in Italy,March 2021,Valter Di Giacinto,Libero Monteforte,Tiziano Ropele,Male,Male,Male,Male,,
7.0,1.0,Italian Economic Journal,10 December 2019,https://link.springer.com/article/10.1007/s40797-019-00118-8,Regional Support for the National Government: Joint Effects of Minimum Income Schemes in Italy,March 2021,Giovanni Gallo,,,Male,Unknown,Unknown,Male,,4
7.0,2.0,Italian Economic Journal,11 March 2021,https://link.springer.com/article/10.1007/s40797-021-00151-6,From Flasks to Fine Glasses: Recent Trends in Wine Economics,July 2021,Anna Carbone,,,Female,Unknown,Unknown,Female,"The wine industry has changed dramatically in the last decades with an acceleration of change since the turn of the century onwards (Bargain et al. 2018; Unwin 2005). On the demand side, things have immensely changed as wine, once included in the everyday diet as a basic source of energy especially for peasants, farmers and working-class people, is now a hedonic good basically consumed in free time and in social occasions and with strong status symbol implications (Charters and Pettigrew 2008; Thach 2012). Wine is no more a slightly differentiated good often traded as a bulk commodity; wine is luxury; wine is cool and fashion. Especially in high income countries and/or higher income population groups, attributes related to culture, traditions, emotions, tourism and discovery, experience of novelty, self-identity, signalling of social status and so forth, contribute to motivations for purchase and consumption in addition to sensory features (Carlsen and Boksberger 2015; Mouret et al. 2013; Thach, 2012). Everywhere market segmentation is very pronounced. These aspects of wine demand are partly associated to the entry in the market of consumers living in countries where wine was not a traditional alcoholic beverage and it is now somehow regarded as a new exotic and trendy good connected to the general idea of the French savoir vivre and/or to the Italian style. In these countries per capita consumption of wine is rapidly increasing. Differently, in traditional producing/consuming countries the globalization wave leads consumers to drink less wine and more beer and spirits than in previous times. Here, per capita consumption follows a pronounced negative long run trend, like in Italy where figures dropped from more than 100 kg in the sixties to less than 40 kg in present years (Pomarici et al. 2021; Carbone et al. 2019; Lesschaeve 2007). Furthermore, nowadays wine is no more only bought to be consumed: as in some cases its value increases with time, wine can be bought and kept just as other kind of investments and happens to be a somehow close substitute, not of beer and spirits as one may think, but rather as an alternative to art masterpieces or to other financial assets in portfolio diversification strategies (Masset and Henderson 2010). The supply side also changed under the pressure of different forces. Many efforts have been devoted to a general quality upgrade and product differentiation in order to better meet demand. These have led to one of the most complex agri-food industries and one of the most sophisticated agri-food products. Production technology, though not much, has also evolved both in the vineyard and in the winery so that the whole process has changed. Cultivated grapes changed, with a substantial reduction in the number of varieties and the diffusion of a small number of ubiquity vines spread up all over. The wine itself has changed, not only due to the different grapes used for making wine but also thanks to a greater attention to quality and diversification as in the case of sustainable, organic, biodinamic, ethic attributes, just to make a few examples. Grape production remains highly fragmented, especially in some countries, however, grape growers and wine producers did change from small households only partially market-oriented to larger specialized companies better embedded in (often global) value chains. The downstream stages of the value chain underwent a significant concentration process. The retail sector increased its average size and internal complexity and it has now in its hands the governance of some relevant upstream stages of the value chain (Folwell and Volanti 2003; Gwynne 2008). Furthermore, new actors entered in the scene such as wine technicians, oenologist, wine experts and wine journalists and communicators, and so on. These are all basically related to process management, quality improvements, process and product certification, communication and promotion (Hommerberg 2011). The experience and/or credence nature of wine attributes that are increasingly more relevant are at the basis of the strong need for providing information and quality guarantees along the chains as well as to the final consumer (Sherman and Tuten 2011). New countries entered the industry also on the supply side. These are generally referred to as the New Wine World (NWW)—a group of countries scattered in different continents—as opposed to the Old Wine World (OWW) that refers to European producer countries (Bargain et al. 2018). There has also been a general shift towards former cooler places warmed-up by climate change. Figure 1 shows the map of world major wine traders. Countries in red are net importers while countries in green are net exporters; among these the European ones are OWW countries while the remaining ones plus USA (in red because, despite being an important world exporter is a net wine importer) form the NWW group. The world geography of wine trade Thus, the new and wider geography of the wine industry relates to both demand and supply. This leads to a significant globalisation of the wine industry as a multifaceted phenomenon that encompasses a variety of features among which it is worth to recall: (i) demand globalisation and the diffusion of the so-called international grape varieties, also accompanied by the opposite trend of rediscovery of traditional local varieties; (ii) the “migration” of wine experts from OWW to NWW for disseminating traditional know-how and informal knowledge (Giuliani et al. 2011); (iii) foreign direct investments (together with other financial operations) of large companies (both within the sector and from other sectors) seeking at diversifying their portfolio, their production base, locations and/or activities (Giuliani et al. 2011); (iv) increased and re-shaped wine trade flows (Unwin 2005). As for trade flows, these changed, under the influence of the changes in supply and demand, in many regards and especially in quantities, values, diversification, and countries involved, including an increase of intra-industry trade (Carbone et al. 2021). The rising interest around wine is also witnessed by the vast number of magazines, journals, TV shows, blogs, fairs and prizes that flourished all around the world. This growing interest of the public is paired by the greater attention that specialists and scientists in different fields devote to wine. This is true also in the field of economics and has led to many new specialized journals as well as to many articles about wine being published in general economics reviews just as the Special Issue hosted by the Italian Economic Journal (Vol. 7, Issue 2, 2021). The topics covered by this large stream of literature are manifold and form a heterogeneous body of contributions with respect to the underling theoretical paradigm and methodological approaches adopted. In the following pages I present a review of recent developments in wine markets as seen through the main contributions of the wine economic literature. The review focuses on some of the main strands of the recent literature developed by wine economists with no ambition of being exhaustive as this would be far beyond the scope and length of this article.",3
7.0,2.0,Italian Economic Journal,05 January 2021,https://link.springer.com/article/10.1007/s40797-020-00139-8,The Sophistication of International Wine Trade: A New Import Measure,July 2021,Anna Carbone,Federica Demaria,Roberto Henke,Female,Female,Male,Mix,,
7.0,2.0,Italian Economic Journal,08 May 2021,https://link.springer.com/article/10.1007/s40797-021-00145-4,Reflections on the Political Economy of European Wine Appellations,July 2021,Julian M. Alston,Davide Gaeta,,Male,Male,Unknown,Male,"Around the world, wine markets are in a state of flux: producers are adjusting to shifting consumer demand for wine, global production is close to an all-time high, and we have once again entered a bust phase in the perennial boom-bust cycle—hard times for European wine producers, exacerbated by Brexit, President Trump’s tariffs, and now the COVID-19 pandemic. This evolving market context is one of several factors that has been putting pressure on Europe’s wine policy, which has sought for decades to manage the balance of supply and demand and quality of European wine, with only partial success. Today’s European wine policy is centered on a system of appellations, implemented as geographical indications (GIs) for wine that entail significant technological regulations—restricting the varieties that may be grown, while imposing maximum yields per hectare and other rules regarding grape production and winemaking practice.Footnote 1 This system was built over many decades on foundations laid in France in the early 1900s. The Appellation d’Origine Contrôlée (AOC) system was introduced in France in 1935 to establish standards for wine production systems—imposing regulations on both wine products and the processes used to produce them within defined locations—and thereby to create and enhance collective reputations based on regions (Haeck et al. 2019; Meloni et al. 2019). These regions and their typical wines were to be distinguished from one another by their unique combinations of terroir, varieties, standards, and production methods. The idea spread. In today’s global wine market as many as 1239 different wine appellations exist; 57 in Bordeaux alone (Livat et al. 2018; IOV 2019). Similar policies were introduced by other European wine-producing countries as they joined the European Community (EC), and the appellation rules were harmonized under the aegis of an expanding European Union (EU) throughout various reforms of the Common Agricultural Policy (CAP), most recently in 2013. The creation of the AOC system in France (and its counterparts in other countries) may have yielded net social benefits by solving a “lemons” problem, as argued by Mérel et al. (2019), or through its didactic effects in educating poorly informed wine producers and consumers, or both. Over time, however, the cost–benefit balance has changed. Restrictions on producer choices over what to produce and how to produce it impose social costs that inevitably increase over time (Becker 1983). Exacerbating this general phenomenon, regulations on European wine production have been held largely immutable over decades during which technological and market possibilities for producers have changed immensely. And new demands for adaptive responses to climate change can only make matters worse. Meanwhile, improvements in marketing and communications technology, and the rise of alternative sources of information, have diminished the potential consumer and producer benefits from quality signals provided by wine appellations embodying information about production processes. And, with globalization, the world market has been substantially restructured, including a rise in New World wine market shares that may be partly attributed to the constraints imposed by the EU policies, and which changes the implications of those constraints. The policies have evolved, too, but perhaps too slowly. The appellation system has operated in conjunction with a battery of supply management policies. Since the early 1980s, for many years the main concern for European wine policymakers was the problem of overproduction of undifferentiated table wines. However, as we explain below, this problem of excess capacity has now been transmitted to higher quality wines, produced within the system of GIs. These designations were introduced to provide a guarantee of excellence, to protect the consumer against risk of counterfeiting, and to serve as a symbol of the region of production for promoting the positive externalities from collective reputation. They may still do all these things, but the linkages between appellation rules and wine quality signals are becoming weaker, partly because the world has changed considerably since those rules were introduced. Some producers are finding it in their interest to opt out. The objective of this paper is to explore the European system of appellations for wine, its management, the way in which it is regulated, its governance mechanisms, and the consequences for wine producers and consumers. In this context, we raise questions about the use of technological regulation as an economic policy tool. Economic arguments would favor instruments that are closely targeted to the economic purpose and that allow market transactions to reveal opportunity costs; ideas that have been neglected by EU wine policy. Meloni et al. (2019) provide a taxonomy of wine regulations which they use to compare the EU and the other main producing countries. They note that “The EU is not only the largest global wine-producing region and the main importer and exporter of wine, but also the most regulated wine market” (Meloni et al., p. 622). As the authors highlight, Europe stands out from other producing countries for the size of its producer subsidies and the strength of its vineyard regulations seeking to increase the quality and restrict the quantity of wine produced. These vineyard regulations are the focus of the present paper, in particular the appellation rules and supply management policies. These policies are complex, involving diverse instruments devised and implemented by the EU, by Member States, and by sub-national organizations acting separately and together. Our analysis takes into account the realities of imperfect enforcement and economizing responses by producers and others.Footnote 2 The paper proceeds as follows. First, in Sect. 2 we introduce the appellation rules that are the centerpiece of European wine policy and our main focus. Next, Sect. 3 presents a brief history of European policies as they evolved to address a persistent problem of surplus production of lower quality wine. Section 4 documents the resulting shifts in structure of the industry over the decades. Section 5 documents the consequences of various supply management policies, which has been a major emphasis of EU involvement, and a major public expense. Then we shift to the economics of appellations. Section 6 delves into GIs as collective brands. Section 7 analyzes firm-level economics of technological regulations as part of the appellation system, and Sect. 8 extends this analysis to the context of the market as a whole. Finally, Sect. 9 presents a synthesis of findings and concludes the article.",3
7.0,2.0,Italian Economic Journal,19 March 2021,https://link.springer.com/article/10.1007/s40797-021-00144-5,"The Italian Wine Sector: Evolution, Structure, Competitiveness and Future Challenges of an Enduring Leader",July 2021,Eugenio Pomarici,Alessandro Corsi,Roberta Sardone,Male,Male,Female,Mix,,
7.0,2.0,Italian Economic Journal,07 February 2021,https://link.springer.com/article/10.1007/s40797-020-00137-w,A SAM-Based Analysis of the Economic Impact of Frauds in the Italian Wine Value Chain,July 2021,Donato Romano,Benedetto Rocchi,Veronica Manganiello,Male,Male,Female,Mix,,
7.0,2.0,Italian Economic Journal,07 January 2021,https://link.springer.com/article/10.1007/s40797-020-00138-9,Indicators of Individual Wine Reputation for Friuli Venezia Giulia,July 2021,Guenter Schamel,Anna Ros,,Unknown,Female,Unknown,Female,"Many authors have applied hedonic pricing models to wines from different regions of the world to understand which characteristics affect prices and to what extent. Even though hedonic models have been applied to Italian wine regions including Friuli Venezia Giulia (FVG), we add to the literature by estimating a unique ordered logit model using Gambero Rosso wine quality evaluations. Friulian wine is known for its great varietal diversity, quality and its importance, both economically and culturally. According to Corubolo (2013), the production and consumption of wine represent an aspect deeply related to the local culture. Moreover, the focus on quality improvements enabled the wines of the region to remain competitive and to successfully enter the international market. Furthermore, FVG is mainly renowned for its wide range of autochthonous white grape varieties and it is therefore referred to as a unique white wine region in Italy. Given the cultural and economic importance of the wine sector for the region, the development of a hedonic pricing model will help to better understand and assess the impact of different characteristics on the wine price. In this paper, we analyse a data set of wines from FVG which received a Gambero Rosso Award for 2016. The Gambero Rosso wine guide is commonly recognized as the most important wine guide in Italy. It lists high quality wines for a given calendar year in four award categories (one black glass, two black glasses, two red glasses, three red glasses) and their prices in eight categories. It provides information on wine varieties (e.g. Friulano, Picolit, Ribolla Gialla, Malvasia, Chardonnay, Pinot Grigio, and Sauvignon, as well as other varietals and blends), colour (white, red, rosè), age (from 2 to 12 years), and the provincial origin (Trieste, Udine, Pordenone and Gorizia). In addition, we add any past award levels received in 2014 and 2015 to the data set, serving as a reputation indicator. The nature of the data set and how prices are reported lends the application of limited dependent variable models which are not commonly used in hedonic pricing models for wine. Our main objective is to understand whether Gambero Rosso awards and reputation effects have a significant impact on wine prices, correcting for other quality indicators such as variety, wine colour, age and provincial origin. In the following section, we present a literature review of selected hedonic pricing models for wine leading to the unique feature of the model developed in this application. Then, we introduce the methodology, the source of the data with summary statistics on the dependent and the independent variables and the hedonic model. Finally, the estimated ordered logit model is illustrated, followed by the interpretation of the results obtained and some concluding remarks.",1
7.0,2.0,Italian Economic Journal,02 April 2021,https://link.springer.com/article/10.1007/s40797-021-00146-3,Consumer Taxes on Alcohol: Is the Wine Sector a Niche Within the Alcoholic Beverages?,July 2021,Paola Corsinovi,,,Female,Unknown,Unknown,Female,"Taxes placed on alcoholic beverages vary considerably by the type of product, by country and, in several cases, within countries. Historically, these taxes were introduced with the primary purpose of raising national revenues. Nowadays, governments may introduce taxes on alcohol, alongside other regulations, for a variety of reasons. These include economic motivations (to enhance revenues, to protect domestic markets from foreign competition, to raise producer product prices, to protect domestic producers against ""dumping"" by foreign companies or governments etc.), social motivations (to regulate consumption and reduce negative consequences for both the person drinking and third parties, including unemployment, family problems, violence, crime, etc.) and political motivations (national interests, lobbying pressures and unrelated trade disputes such as the recent tariffs authorized by WTO, imposed on EU food products in the US market)Footnote 1 (Anderson 2020; Anderson et al. 2018; Meloni et al. 2019). Prof. Kym Anderson in his paper ""Consumer Taxes on Alcohol: An International Comparison over Time"" (2020) provides an exhaustive description and estimation of tax rates from 2008 to 2018 imposed on alcoholic products like wine, beer and spirits, for a wide range of high-and middle-income countries. His publication represents the most updated and comprehensive research available on this topic. The author offers several interesting findings, from highlighting the complexity surrounding the taxation of alcoholic products to, in particular, the heterogeneity in tax rates among countries. The research is based on an analysis of taxes as the ad valorem consumer tax equivalent (CTE) in US dollars per unit of alcohol, and as a percentage of the wholesale pre-tax price at representative price points and alcohol percentages. In addition, the author takes into consideration the various Value-Added (VAT) and Good and Services (GST) tax rate across countries. Data are collected from the European Commission, OECD, World Bank and WTO and national government sources. The methodology is addressed by comparing tax rates and other instruments across countries, and for varying beverages types (pages 43–47). Additionally, the author stipulates that finding and designing optimal policies to reduce the negative health and social consequences of the harmful use of alcohol is far from simple. This note pays tribute to the great contribution made by Prof. Anderson, with a specific focus on the EU wine sector. This short text is far from exhaustive but provides a starting block for a more in-depth analysis into this complex issue.",4
7.0,3.0,Italian Economic Journal,15 January 2021,https://link.springer.com/article/10.1007/s40797-020-00140-1,Calculating a Giffen Good,November 2021,Kazuyuki Sasakura,,,Male,Unknown,Unknown,Male,,
7.0,3.0,Italian Economic Journal,30 October 2019,https://link.springer.com/article/10.1007/s40797-019-00117-9,Merger Waves Through Market Leadership,November 2021,Walter Ferrarese,,,Male,Unknown,Unknown,Male,,
7.0,3.0,Italian Economic Journal,13 August 2020,https://link.springer.com/article/10.1007/s40797-020-00134-z,Is Anything Predictable in Market-Based Surprises?,November 2021,Luca Brugnolini,Antonello D’Agostino,Alex Tagliabracci,Male,Male,Male,Male,,
7.0,3.0,Italian Economic Journal,24 December 2020,https://link.springer.com/article/10.1007/s40797-020-00136-x,Asset Specificity and the Secondary Market for Productive Assets,November 2021,Simone Boccaletti,,,Female,Unknown,Unknown,Female,"The ability to pledge collateral significantly raises the firms’ financial capacity. When a borrower cannot repay its debt obligations, the creditor has the opportunity to seize and liquidate collateralized assets in order to recover a partial amount of the original credit. Liquidation values depend on the degree of specificity and on the presence of a secondary market (Shleifer and Vishny 1992). Asset specificity is the ability to reuse a productive asset by alternative users and/or to alternative uses with the least sacrifice in terms of productivity (Williamson 1988, 1991). Therefore, the degree of specificity is a key determinant of the resale value of an asset in the secondary market: if it can be redeployed at a small cost, then its resale value is high. Hence, assets with different degree of asset specificity are associated to different resale values. The recent empirical literature has highlighted how firms that use more redeployable assets obtain better financing conditions (e.g., Benmelech et al. 2005; Benmelech and Bergman 2008, 2009). Even if the concept of asset specificity could seem abstract, it can be applied to any financial contract in which a borrower pledges collateral. Assets that are most frequently used in this type of contract are real estates, inventories and standardized machines and equipment, which are easily redeployable. By contrast, examples of less redeployable assets are firm-specific machinery and patents. Moreover, while pledging collateral can help any firm to raise external finance, pledging assets with low redeployability could be difficult for small, informationally opaque and shallow-pocket firms, which usually rely on banks and trade creditors (Norden and van Kampen 2013). This paper builds on the fact that borrowers have the possibility to choose (at least to some extent) the specificity of their productive assets. A classical example is the airline industry, in which the redeployability of aircrafts used by airlines affects the cost of external financing (Benmelech and Bergman 2008, 2009). Since airlines tend to use a limited number of aircraft types, the secondary market for an aircraft is mainly populated by companies already operating with that model. Suppose that a firm operates with an aircraft model that is not commonly used in the industry. If the firm goes bankrupt, the resale value of its aircrafts might be low for two reasons. First, the number of potential buyer that can efficiently redeploy those assets is very low. Second, a firm which is not using that type of aircraft has to pay large redeployability costs in order to include that model in its activities (e.g., pilot and staff training). Here, the choice of the degree of specialization is determined by which aircrafts and how many different models to use, and it might give rise to the redeployability problem. In this paper I study the relation between the secondary market for productive assets and firms’ financing conditions by focusing on the ex-ante choice of asset specificity. I develop a theoretical model in which an entrepreneurFootnote 1 may run a risky project, and may invest in asset specialization before applying for a loan.Footnote 2 In a first-best scenario, where the entrepreneur is not financially constrained, she will choose the maximum degree of asset specificity (provided that the marginal benefit of the investment overcomes the marginal cost). However, when she is financially constrained, she has to pledge her productive asset as collateral in order to get funds to run the project. Here, the degree of asset specificity affects the repayment to lenders through the secondary market, i.e. the auction that takes place when a creditor seizes an asset from a borrower in financial distress. In this auction, the main determinant of liquidation values is the degree of asset specificity: on one hand, asset specialization increases productivity and the return of the project when the firm is active; on the other hand, it reduces the asset’s redeployability and hence its recovery value when the firm is in financial distress. I will refer to this statement as the specialization trade-off. In addition, there is substitutability between investing in asset specificity and the amount invested in the project: if the borrower does not invest in asset specificity, she devotes her initial cash in the project, lowering the amount of debt needed from the capital market. If instead she invests in asset specificity, then she needs additional capital in order to cover the cost of the project. I will refer to this trade-off as the substitutability trade-off. The paper explains how the substitutability trade-off and the specialization trade-off play a crucial role in the determination of specific investments. I show that the number of potential buyers in the secondary market and redeployability costs are the main determinants of firms’ ex-ante choice of asset specificity: if the number of potential buyers is small, then liquidation values are low and credit constrained firms do not invest in asset specificity. If the number of potential buyers is large, then liquidation values are high, allowing firms to specialize their assets since they find better financial conditions. Moreover, redeployability costs have a direct effect on the resale value of the asset: the higher the redeployability costs, the lower the asset’s resale value. If redeployability costs are low, then by investing in asset specificity, firms face both favourable credit conditions and higher project returns in the non-distress case. If instead redeployability costs are high, then the investment in asset specificity deteriorates firms’ financing conditions. In this case a firm does not invest in asset specificity when the increase in the cost of credit overcomes the increment in the expected return of the project. This paper contributes to the existing theoretical literature on financial contracting by providing a simple and elastic model in which investments in asset specificity, financial conditions and the dynamics of the secondary market for productive assets are interconnected and endogenously determined in equilibrium. The paper derives the conditions under which asset specialization becomes optimal, and shows that, overall, financially constraints reduce firms’ incentive to invest in asset specificity. Those findings may provide some guidance in assessing the collateral channel, particularly in those industries where specific investments are used to build competitive advantage. The remainder of the paper is organized as follows. Section 2 provides an overview of the main related literature. Section 3 introduces the theoretical model (the benchmark, the secondary market and the financial contract). Section 4 provides the results (for both self-financing and financially constrained firms). Section 5 discusses the analysis and the main hypothesis behind the theoretical framework. Section 6 concludes. All proofs for the theoretical model are in Appendix B.",
7.0,3.0,Italian Economic Journal,11 March 2021,https://link.springer.com/article/10.1007/s40797-021-00150-7,"Bribes, Lobbying and Industrial Structure",November 2021,Roy Cerqueti,Raffaella Coppier,Gustavo Piga,Male,Female,Male,Mix,,
7.0,3.0,Italian Economic Journal,14 October 2019,https://link.springer.com/article/10.1007/s40797-019-00114-y,Insider Trading and the Market Abuse Directive: Are Voluntary and Mandatory Takeover Bids Different?,November 2021,Riccardo Ferretti,Pierpaolo Pattitoni,Roberto Patuelli,Male,Male,Male,Male,,
7.0,3.0,Italian Economic Journal,18 May 2020,https://link.springer.com/article/10.1007/s40797-020-00129-w,To Bribe or Not to Bribe? An Experimental Analysis of Corruption,November 2021,Massimo Finocchiaro Castro,,,Male,Unknown,Unknown,Male,,1
7.0,3.0,Italian Economic Journal,22 March 2021,https://link.springer.com/article/10.1007/s40797-021-00142-7,Public Opinion Views on Immigrants’ Contribution to the Local Economy: the Role of TV Exposure,November 2021,Leonardo Becchetti,Berkan Acar,,Male,Male,Unknown,Male,,1
8.0,1.0,Italian Economic Journal,08 July 2021,https://link.springer.com/article/10.1007/s40797-021-00162-3,Trouble Underground: Demand Shocks and the Labor Supply Behavior of New York City Taxi Drivers,March 2022,Alessandro Saia,,,Male,Unknown,Unknown,Male,"In recent years, the extensive body of literature within economics that focuses on labor supply decisions has once again become central to the academic debate. Instead of investigating the labor supply behavior of standard workers,Footnote 1 several influential papers have been devoted to the analysis of a specific category of workers: New York City taxi drivers. Taxi drivers have proven to be ideal when investigating labor supply decisions. On the one hand, they operate in settings where they are faced with temporary changes in their earning opportunities (in other words, the income effects on the labor supply are likely to be negligible). On the other hand, they are free to decide whether to work, and to what degree, and this is also easy to measure. In this paper, we exploit positive changes in labor demand driven by subway disruptions to quantify the relevance of behavioral biases in the labor supply decisions of New York City taxi drivers. By creating more favorable demand conditions, subway disruptions have a positive impact on drivers’ potential earnings. When subway disruptions commence, commuters will be forced to rely on alternative forms of transportations and, due to the higher the number of commuters on the street looking for a cab, it will be easier for drivers to find a new passenger. To the best of our knowledge, this is the first work to exploit exogenous and unanticipated demand shocks to gauge the importance of behavioral on drivers’ labor supply behavior of taxi drivers. Using web crawling and text recognition techniques, we collected high-frequency information on subway disruptions from different Online resources. In the empirical analysis, we show that subway disruptions largely affect drivers’ labor supply behavior. On average, when underground breakdowns are observed the probability of a driver of ending her shift is around 15% smaller, even when accounting for driver fixed-effects, hour fixed effects and calendar day fixed-effects. While this finding is in line with the prediction of the standard model of labor supply (i.e., labor supply increases in response to short, positive changes in earnings opportunities), this result doesn’t necessarily rule out the presence of targeting behavior. To assess the extent to which drivers’ responses are affected by income-targeting behavior, we investigate whether they respond differently to demand shocks once they have reached their target. Two main results emerge. On the one hand, our estimates show that drivers’ responses to positive changes in labor demand are large and economically relevant both when they are below and when they are above their income goal. On the other hand, they also provide clear evidence of the relevance of behavioral biases in labor supply decisions of taxi drivers: when earning opportunities are temporarily higher, the fact of surpassing the target significantly reduces drivers’ labor supply. Drivers’ responses to demand shocks, while always positive and statistically significant, are around 40\(\%\) smaller when drivers are above their income target. We complement these findings by investigating how drivers’ response varies as a function of distance to their target. Results show that drivers’ response increases when they are close to the target: the effect of subway disruptions on drivers’ stopping behavior is stronger when they are closer to the target. As soon as drivers move away from the target the effect of an increase in taxi demand on stopping probability, while being always negative, decreases in the magnitude. Overall, results presented in this paper are in line with the predictions of a model of labor supply where both the standard component and behavioral elements coexist. While drivers’ behavior seems largely consistent with the prediction of a standard model of labor supply, the large difference between below-target and above-target responses suggests that targeting behavior nevertheless plays a non-secondary role in drivers’ decisions.",
8.0,1.0,Italian Economic Journal,26 February 2021,https://link.springer.com/article/10.1007/s40797-021-00143-6,"Fertility, Inequality and Income Growth",March 2022,Masaya Shintani,Masaya Yasuoka,,Male,Male,Unknown,Male,,
8.0,1.0,Italian Economic Journal,08 June 2021,https://link.springer.com/article/10.1007/s40797-021-00157-0,Intergenerational Upward (Im)mobility and Political Support of Public Education Spending,March 2022,Debora Di Gioacchino,Laura Sabani,Stefano Usai,Female,Female,Male,Mix,,
8.0,1.0,Italian Economic Journal,01 June 2021,https://link.springer.com/article/10.1007/s40797-021-00154-3,Labour Market Regulation and Youth Unemployment in the EU-28,March 2022,Giorgio Liotti,,,Male,Unknown,Unknown,Male,"The research concerning the reasons for the strong rise in (youth) unemployment—mainly in the Western European countries—has been at the centre of social, political and economic debate since the end of the 1980s (Lazear 1990; Layard et al. 1991; Pissarides and McCaster 1990). The hypothesis suggested by neoclassical labour market theory is that the higher unemployment was due mainly to the presence of more stringent rules of the labour market, which represented an obstacle to achieving full employment (Blanchard and Summers 1986; OECD 1994; Lazear 1990). It is worth pointing out that during the 1990s and the early years of the 2000s, the neoclassical hypothesis also found confirmation in several econometric papers (for more details see, among others, Nickell 1998; Nickell et al. 2005; Blanchard and Wolfer 2000; Blanchard et al. 2006; Hopenhayn and Rogerson 1993, Scarpetta 1996; Nickell and Layard 1999; Elmeskov et al. 1999; IMF 2003; Belot and Von Ours 2000). The neoclassical theoretical arguments in favour of higher labour market flexibility are based on the fact that labour market flexibilization is a key factor to enhance productivity, increase a firm’s competition, favour economic growth and reduce unemployment (Jha and Golder 2008). In particular, it is possible to draw at least four reasons for why a high degree of rigidity in the labour market increases the unemployment level: (1) the existence of stringent labour market rules determines that, in equilibrium, workers’ wages are higher than their marginal product, leading, in this way, to a misallocation of resources; (2) higher labour market rigidity represents an obstacle to the adjustment of the labour market determined by the changes of the business cycle (Blanchard et al. 2006); (3) the rigidity of labour markets represents an economic “rent” from capital to labour that reduces the profitability of investors and discourages investment and economic growth (Calderon and Chong 2005); (4) finally, the rigidity of labour market institutions protects insider workers, preventing outsiders (especially young workers) from accessing the labour market (Lindebeck and Snower 1988). According to these premises, labour market reforms have been introduced in many European countries since the later 1990s (Tridico 2018), with the aim of achieving three main objectives: (1) the introduction of “atypical” jobs (fixed and part-time contracts) to facilitate the entry of young people in the labour market; (2) lowering of the hiring and firing costs, allowing firms to increase their competitiveness on international markets and adjust the labour demand according to the business cycle (Zemanek 2010; Bernal-Verdugo et al. 2013; Lucifora et al. 2005; Ferreiro and Serrano 2013); (3) reducing employment security (Moreira et al. 2015), aiming to reduce the protection that insider workers enjoy, preventing the labour market segmentation described by insider–outsider theoryFootnote 1 (Lindbeck and Snower 1988; Blanchard and Summers 1986). However, in this general context, it is important to point out that there is no consensus about the real effectiveness of labour market deregulation on employment outcomes (see among other Baker et al. 2004; Bassanini and Duval 2006; O’Higgins 2012, 1997; Dutt et al. 2015; Brancaccio et al. 2018; Arestis and González-Martínez 2015; Montenegro and Pages 2003; Ferreiro and Gomez 2019; Posner 2017). In particular, there are at least three arguments in opposition to the neoclassical labour market theory hypothesis that deserve to be analysed. (1) According to Barbieri and Scherer (2009), higher deregulation does not contribute to a reduction in youth unemployment, but it determines only a substitution effect (that is, the substitution of typical employment with sub-protected workers (Cirillo et al. 2017; Ferreiro and Gomez 2018); (2) According to Kleinknecht (1998), policies aiming to provide greater flexibility in the labour market can have (maybe) some effect in the short run, but in the long run they are harmful for innovation, economic growth and employment; (3) According to Walwei (1996), labour market flexibility policies to reduce workers’ wages have detrimental effects for firms because they lead to “adverse” selection regarding workers (Akerlof 1984; Yellen 1984; Akerlof and Yellen 1986). Finally, it is also useful to recall that, according to the Keynesian school, the labour market has a passive role and that it depends on the level of aggregate demand. In this view, any changes in labour market institutions will not have an effect if they are not accompanied by expansionary economic policies (fiscal and monetary policies) that are implemented for the purpose of increasing the aggregate demand. Considering the theoretical arguments pro and con, the motivation that underlies this paper is to try to evaluate whether the implementation of labour market flexibility has produced the desired effects on unemployment as stated by neoclassical theory or, vice versa, whether it is harmful for employment, as stated by heterodox scholars. This point is fundamental because, if the theoretical argument in favour of labour market flexibility is not able to achieve the objective for which it is implemented, then policymakers should find different solutions for the youth unemployment problem. Compared to other papers in the same field, this paper: (1) does not focus on a measure capturing a specific aspect of the labour market (employment protection legislation, for example; see Liotti 2020 for more details), but on an indicator describing how and to what extent the labour market, as a whole, has been flexibilized; (2) tries to assess whether there are differences between short- and long-run effects of labour market flexibility on youth unemployment; and (3) focuses on the impact of single specific measures (i.e. sub-indicators) of flexibility on youth unemployment. The paper is structured as follows. In Sect. 2, I discuss data analysis on youth unemployment and labour market regulation in 28 European countries. In Sect. 3, the empirical analysis is divided into three sub-sections: Sect. 3.1. The model and variables description, Sect. 3.2. Country fixed and Time fixed effects model and Sect. 3.3. PMG model. Section 4 is dedicated to a discussion on empirical results, and in Sect. 5 the general conclusions are drawn.",11
8.0,1.0,Italian Economic Journal,29 May 2021,https://link.springer.com/article/10.1007/s40797-021-00156-1,Firing Costs and Job Loss: The Case of the Italian Jobs Act,March 2022,Claudia Pigini,Stefano Staffolani,,Female,Male,Unknown,Mix,,
8.0,1.0,Italian Economic Journal,26 June 2021,https://link.springer.com/article/10.1007/s40797-021-00159-y,Are Italians Getting Multidimensionally Poorer? Evidence on the Lack of Equitable and Sustainable Well-Being,March 2022,Dalila De Rosa,,,Female,Unknown,Unknown,Female,"Due to the recent economic crisis, monetary poverty has sharply increased in almost all developed countries, with the incidence of absolute poverty in Italy rising from 3.6 in 2005 to 7.7 per cent in 2020, at household level. However, standard poverty measures consider only the monetary aspect of poverty while neglecting the multidimensional nature of the phenomenon, which is of growing relevance in this ongoing pandemic. Indeed, people experiencing multiple deprivations before the pandemic are now facing additional COVID-19 risk factors: such is the case of housing deprivation and overcrowding, which hinders effective quarantine (WHO 2020a, b), or pollution, which contributes to respiratory diseases (WHO 2018), or school closures, which amplifies education inequalities (Coe et al. 2020). Even though the last decades have seen advances towards implementing a multidimensional perspective on poverty, this is especially crucial for shaping post pandemic policies, identifying overlapping deprivations and prioritizing the most vulnerable groups (Tavares and Betti 2021; Alkire et al. 2020). In the vein of multidimensionality, since 2009, the Stiglitz–Sen–Fitousse Commission has contributed to the debate on Beyond GDP not only in academia, but also in official statistics offices and among policy makers, promoting numerous national and international frameworks aimed at measuring human development and spreading the use of composite indices of wellbeing (Bandura 2008). Similarly, as relates to poverty, eminent authors have affirmed the existence of significant mismatches between monetary poverty and multiple deprivations, confirming the need to consider numerous aspects of poverty for human development (Atkinson 2017). However, most of the literature on multidimensional wellbeing and poverty questions whether it is possible to choose the dimensions of wellbeing without the meaningful participation of those at stake. This 'curse' of having to define wellbeing dimensions and the most suitable normative framework picturing a worthwhile life remains a relevant issue in the study of this phenomena (Alkire 2007; Burchi et al. 2014, 2020). In Italy, considerable effort was made to launch the Equitable and Sustainable wellbeing (BES) as the official framework for measuring wellbeing (see ISTAT 2019). Some indicators of that framework are also included for the evaluation of fiscal policies by the Ministry of Economy and Finance (see Ministero dell’Economia e delle Finanze, 2018), and recently the European Commission has funded a project on the use of wellbeing frameworks for policy (Bacchini et al. 2019, 2020). By contrast, Italy has made little effort for the measurement of multidimensional poverty. Official statistics include four indicatorsFootnote 1 of poverty within the economic sphere of the BES framework, yet even though these indicators go beyond a purely monetary evaluation of poverty, they do not accommodate a broader multidimensional perspective. Moreover, very few empirical studies have analyzed multidimensional poverty in Italy at a sub-national level (Coromaldi and Zoli 2012; Betti et al. 2008; Giuliano et al. 2012). Therefore, the paper aims at filling these gaps by offering a national measure of multidimensional poverty (MPI), employing the National framework for wellbeing (BES) as the normative basis for the construction of the index. The contribution of the paper is twofold: (1) on the theoretical side, it supports the usability of national or international frameworks for wellbeing by proposing them as a normative scheme for the definition of the dimensions of poverty. Considering poverty as a lack of wellbeing, this work is an attempt to offer a solution to the problem of dimensionality by proposing a method that analyses multidimensional poverty within the numerous frameworks for wellbeing of recent creation; (2) on the empirical side, it provides evidence on the level and the composition of multidimensional poverty in Italian regions. The paper does not claim to offer a singular, precise multidimensional poverty measure for Italy, but it provides both an attempt to use a widely-recognized national wellbeing framework to measure poverty and updated evidence on multidimensional poverty in a country where such information is currently scarce. The following section discusses the role and the evolution of academic and policy efforts to measure multidimensional poverty; Sect. 3 presents the Alkire-Foster method; Sect. 4 discusses the normative as well as the empirical methods applied; Sect. 5 reports the main findings; and Sect. 6 offers final remarks.",
8.0,1.0,Italian Economic Journal,23 July 2021,https://link.springer.com/article/10.1007/s40797-021-00164-1,Italian Workers at Risk During the COVID-19 Epidemic,March 2022,Teresa Barbieri,Gaetano Basso,Sergio Scicchitano,Female,Male,Male,Mix,,
8.0,1.0,Italian Economic Journal,09 September 2021,https://link.springer.com/article/10.1007/s40797-021-00167-y,The Effect of Job–Education Vertical Mismatch on Wages Among Recent PhD Graduates: Evidence From an Instrumental Variable Analysis,March 2022,Giuseppe Lucio Gaeta,Giuseppe Lubrano Lavadera,Francesco Pastore,Male,Male,Male,Male,"Over recent years, the remarkable proliferation of PhD education (Auriol et al. 2013; OECD 2016) and the subsequent increase of doctorate holders looking for jobs in non-academic sectors have raised significant concerns in most OECD countries regarding doctoral graduates’ occupational outcomes (Benito and Romera 2013; The Economist 2016). Many country-level empirical studies have highlighted that, a few years after graduation, a large proportion of PhD holders report vertical job–education mismatch.Footnote 1 Such a vertical mismatch is defined as overeducation, i.e. holding a job not in line with having a PhD, or overskilling, i.e. misalignment between the skills acquired during the PhD and those used at work. To the best of our knowledge, limited data availability restricts evidence regarding this phenomenon to early-career PhD holders. Among this early-career population, experiencing vertical mismatch seems to have several negative impacts. For example, vertically mismatched PhD holders seem to report lower earnings than their matched counterparts, as recent contributions have demonstrated. Bender and Heywood (2009) revealed that, among US PhD holders, the detrimental effect of overeducation on wages ranges between 7 and 14%, depending on the study field. Examining Spanish data, Canal Domınguez and Rodrıguez Gutierrez (2013) showed that overeducated doctorate holders working in the non-academic sector experience a wage penalty that varies between − 18 and − 25%. Gaeta et al. (2017) suggested that the gap between wages for those overeducated and those earned by their matched counterparts ranges from − 7 to − 11% in Italy.
 Empirical studies have suffered from the absence of longitudinal data, with very few exceptions (Carroll and Tani 2013). These empirical studies have all been based on cross-sectional estimates of a Mincer equation whose right side includes one measure of overeducation (and/or overskilling). As noted by the extensive literature focused on university graduates (for a comprehensive review, see McGuinness and Bennett 2007; Hartog 2000), the cross-sectional nature of these studies prevents the identification of any causal effect of overeducation/overskilling on wages (Leuven and Oosterbeek 2011). The reason lies in the potential endogeneity of the overeducation/overskilling variables in relation to the vertical mismatch status. On the one hand, the mismatch is “measured as a difference between two possibly mis-measured schooling levels [and this] leads to exacerbation of measurement error problems” (Leuven and Oosterbeek 2011, p. 306). This is particularly problematic when one relies on self-reported measures of vertical mismatch, as is often the case because most of the existing data do not include any objective mismatch indicators. On the other hand, the apparent impact of overeducation/overskilling on earnings may be spurious and driven by unobserved ability heterogeneity (Pecoraro 2014; Sloane 2003). In addition, one might suspect that reverse causality is at work when one studies the link between vertical mismatch and wages. Indeed, low wages might be a source of low job satisfaction, and this might trigger negative evaluation of the usefulness of the educational level achieved and the related skills. In our view, providing robust evidence regarding the effect that vertical job–education mismatch has on earnings is particularly important. Indeed, the existence of any impact of overeducation (overskilling) on wages would lead to substantial private costs (besides societal ones). The presence and magnitude of these private costs might translate into people’s reconsideration of investment in education, i.e. disincentivizing future generations’ investment (Tsang and Levin 1985). In this scenario, evidence regarding private costs would require policies to promote job–education vertical matching acting on the demand or the supply side of the job market. The literature seems to be conscious of the importance of robust and reliable estimates of the wage penalty determined by the vertical mismatch. Indeed, many recent papers investigating the case of university graduates have provided analyses specifically aimed at overcoming the endogeneity issue reported so far (see, for example, Clark et al. 2017; Kleibrink 2016; Nieto and Ramos 2017). To the best of our knowledge, however, the existing literature on vertical mismatch and associated wage penalties among PhD holders has not tackled the endogeneity issue adequately. Nevertheless, there are reasons to believe that providing robust evidence concerning the impact of job–education mismatch on wages among PhD holders is of tremendous importance. Indeed, while doctoral education and its focus on acquiring R&D skills are considered crucial in current knowledge societies (Shin et al. 2018), overeducation and overskilling seem to be widespread among PhD holders, as noted above. This calls for an investigation of the private costs related to this mismatch that can go beyond the correlational studies, ceteris paribus, provided by the literature so far (Bender and Heywood 2009; Canal Domınguez and Rodrıguez Gutierrez 2013; Gaeta et al. 2017). The objective of such an investigation would be to understand whether any detrimental effect of this mismatch on private returns actually exists and, therefore, whether policies are needed to support doctoral studies’ expected private benefits. Our analysis considers the case of PhD holders who have studied in Italy, a country characterized by low investment in R&DFootnote 2 where, as a consequence, the non-academic occupational opportunities for PhD holders—who are specialized in R&D—seem to be limited. Not surprisingly, the vertical mismatch between job and education is relatively frequent (Gaeta 2015; Ermini et al. 2019). Our analysis is based on a rich micro-level dataset created by the Italian National Institute of Statistics (ISTAT) that allows observation of PhD holders’ occupational outcomes a few years after graduation. Our investigation of the impact of job–education matching on wages tackles the endogeneity issue by using the instrumental variable (IV) identification strategy proposed by Lewbel (2012, 2018). This strategy has been designed to be “used in applications where other sources of identification such as instrumental variables (…) are not available” (Lewbel 2012, p. 1) and has been widely used (see, for example, Dutta and Roy 2016; Loy et al. 2016; Tiefenbach and Kohlbacher 2015). Such a strategy is based on building a synthetic IV as a function of the available exogenous heteroskedastic covariates (Lewbel 2012). To the best of our knowledge, this is the first study to apply this empirical strategy in studying the effect of overeducation (overskilling) on wages among PhD holders. The use of this econometric technique is particularly appropriate for our study because of the unavailability of longitudinal data on PhD holders’ occupational outcomes and the difficulty in finding variables that can be used as reliable instruments for vertical mismatch, as noted in previous contributions (Gaeta et al. 2017). Besides addressing the endogeneity issues that have hampered previous analyses of the impact of job–education vertical mismatch on wages, this paper also offers interesting insights into this effect’s heterogeneity. We provide evidence that the wage penalty for vertical mismatch varies between 2009 (when a severe economic crisis hit Italy) and 2018, as well as by the field of doctoral study, gender, and place of residence. The remainder of the paper is organized as follows. Section 2 extensively describes the data used in the analysis. Section 3 presents and discusses the endogeneity issues that need to be addressed when estimating the impact of vertical job–education mismatch on wages and also illustrates Lewbel’s IV empirical strategy implemented in our study. Section 4 details our results. Finally, Sect. 5 provides conclusions.",3
8.0,2.0,Italian Economic Journal,20 January 2022,https://link.springer.com/article/10.1007/s40797-021-00176-x,Bargaining within the Council of the European Union: An Empirical Study on the Allocation of Funds of the European Budget,July 2022,Valerio Leone Sciabolazza,,,Male,Unknown,Unknown,Male,,
8.0,2.0,Italian Economic Journal,07 March 2022,https://link.springer.com/article/10.1007/s40797-021-00182-z,Market-Induced Fiscal Discipline in Europe,July 2022,Gianluca Cafiso,Roberto Cellini,,Male,Male,Unknown,Male,"Public debt has been back in the limelight since the Global Financial Crisis. Even though there has been nothing specifically new with public debt itself, whenever its sustainability goes under strain, it becomes a serious concern for policymakers and the public. As matter of fact, even when it is sustainable, public debt may be a burden, conditions budget policy and undermines economic growth (Barro 1990).Footnote 1 Such a burden is quantified by the interest bill in GDP terms. It is observed, however, that public debt is not a major concern in all countries with high debt-to-GDP ratios. Some of those take fiscal decisions that do not reflect the need to contain public debt, while some others are seriously committed to debt reduction or, at least, make an effort to appear committed. In the recent past, debt-motivated fiscal consolidations—based on expenditure, tax or both—are believed to have had deep consequences both in terms of economic development (Alesina and Ardagna, 2010), social cohesion (Agnello and Sousa 2014; Agnello et al. 2016, 2017) and trust in the EU integration dogma. As for the future, the pandemic outbreak in 2020 has reinforced the increasing trend of public debt and this poses new challenges. Indeed, even though no western government is currently engaged in fiscal consolidations, probably thanks to central banks’ interventions, some expect a time when things will be back to normal and there will be need to act to rebalance towards a more sustainable fiscal stance (Brunet and Pàrraga 2021). Coherently, we believe that the topic object of this work is likely to be again an important part of the political debate at some point in the next years. The objective of our research is to verify whether the interest bill influences budget policy, this is the so-called market-induced fiscal discipline hypothesis (Heinemann and Winschel 2001; Tkačevs and Vilerts 2019), and to test some conditions that might drive that effect. We study conditions related to threshold effects, institutional characteristics and political orientation, we also check the temporal evolution of market-induced fiscal discipline in Europe as a consequence of the different crises and policy interventions over the last two decades. We develop our empirical analysis using almost the full sample of European Union countries over the 1995–2019 period. Our research proves that market-induced fiscal discipline emerges and it materializes more on the expenditure side of the budget, it became stronger after the Global Financial Crisis, and that threshold effects are significant. Furthermore, political orientation and fragmentation drive to some extent fiscal discipline. The paper is structured as follows. Section 2 explains the motivations at the basis of our research and reports on some relevant literature. Some problems intrinsic to the study of market-induced fiscal discipline in the EU are detailed in Sect. 3. Section 4 outlines our analysis and discusses the conditions for market discipline. The regression analysis about the effect of the interest bill on the primary budget and the significance of the conditions is in Sect. 5. Section 6 draws the conclusions.",
8.0,2.0,Italian Economic Journal,24 May 2021,https://link.springer.com/article/10.1007/s40797-021-00155-2,A Dynamic Factor and Neural Networks Analysis of the Co-movement of Public Revenues in the EMU,July 2022,Cosimo Magazzino,Marco Mele,,Male,Male,Unknown,Male,"The size of government should be measured by the resources it commands. Among alternative measure of government size, we can find the public revenue/gross domestic product (GDP) ratio. For decades there has been an intense debate regarding the relationship between government size and economic growth. Surprisingly, in the last 50 years the dynamics of government size in the European Monetary Union (EMU) countries follows a common trend. Tax distortions are relevant for economic growth in the long-run. Recent empirical studies underline a clear link between tax structure and economic growth. However, this link is questioned by Xing (2012), who demonstrated that the evidence for significant tax structure effects depends on long-run parameter homogeneity restrictions. One of the primary objectives for the creation of the EMU was to economically spur the growth process by creating cohesive policies to steer the movement of public revenues. The co-movement of public revenues in the EMU depends on the fiscal policies established by the member countries. According to Hagen and Waller (2013), EMU has come across numerous challenges, in the long-run, trying to harmonize the economic welfare across the borders of the members. Economic factors such as tax harmonization, public revenue equalization, and also the fiscal authority policies hamper the different measures and procedures implemented (Barrell et al. 2008). While the significant countries and key stakeholders want to maintain the status quo, the results prove a challenging position for the policymakers. The aim of this paper is to investigate the (eventual) common factor that has driven the co-movement of public revenues in the EMU from 1970 to 2014. To the best of our knowledge, this is the first study that analyzes public revenues in a monetary union highlighting an unobserved factor. Moreover, the novelty of this research is to complement the time-series analysis with a machine learning (ML) approach, as a robustness check. The discovery of a latent factor driving the public revenues of the EMU member countries would be interesting because it would have far-reaching political implications. If the national economies of this currency area are also interconnected in terms of revenues, then smaller efforts would be needed to complete the monetary union, and harmonizing public debt/GDP ratios remains the only real obstacle to hinder centralizing fiscal policies (De Grauwe 2018). In fact, finding some common factor in the government revenues of the member countries of a currency area can bring this area closer to satisfy the conditions for becoming an Optimal Currency Area (OCA) (Mundell 1961; McKinnon 1963; Kenen 1969; De Grauwe 2018; Baldwin and Wyplosz 2019). This should be particularly relevant in the light of the recent crises (subprime mortgages; sovereign debt; Coronavirus Disease). As regards the empirical strategy, we first apply a dynamic factor model (DFM) to estimate an unobserved common component in the government revenues/GDP ratios for EMU-12. Furthermore, we attempt to provide an explanation of the unobserved factor. Then, we test the results obtained through a time-series model by a procedure of artificial neural networks (ANNs) in a DFM. To the best of our knowledge, this is the first paper that applies this procedure in a socio-economic framework. In fact, it is used by meteorological sciences to predict the dynamics of climate change. Therefore, we used the same procedure to study in a neural network (NN) the dynamic effects of the economic cycle on the revenues of the countries taken into consideration. Moreover, the ML model, following Hofmann (2001) mathematical settings, generated a latent factor in a NN for the first time. It represents the result of multiple algorithms transcribed in Java and Oryx that have interpreted the nexus of matrices in a Markov process as a single composite NN algorithm. The results obtained, after carrying out several different tests, confirm the time-series ones. The novelty of our latent factor estimation model can be split into two elements. The first is attributable to the analysis model. In fact, the very few previous studies (Pan and Wang 2012; Calzolari and Halbleib 2018; Chen et al. 2020) use an inferential model to estimate the latent factor. There is always an assumption based on these analyzes. It identifies a set (K) of common factors to extrapolate, through principal component analysis-probabilistic latent semantic analysis (PCA-PLSA), one or more latent factors. The author then interprets the process using the logical assumption of inference concerning mere empirical conjecture. This last statement fits precisely with the second element that distinguishes our estimate from that usually used in literature: the software used. The study proceeds as follows. Besides this introduction, in Sect. 2 we show a state-of-the-art, focusing on some relevant empirical results provided in the economic literature. Section 3 describes the applied methodology and data, while in Sect. 4 the results are presented and discussed. Section 5 provides further explanations on the latent factor analyses. Finally, Sect. 6 gives conclusions, policy implications, and suggestions for future research.",6
8.0,2.0,Italian Economic Journal,08 March 2021,https://link.springer.com/article/10.1007/s40797-021-00147-2,An Empirical Assessment of the Contagion Determinants in the Euro Area in a Period of Sovereign Debt Risk,July 2022,Hazar Altınbaş,Vincenzo Pacelli,Edgardo Sica,Male,Male,Male,Male,"In the last 20 years, economic literature has devoted a large amount of effort into investigating contagion, i.e. the unexpected transmission of shocks across countries (Rigobon 2016). Although some evidence of contagion can be traced back to the Great Depression in 1930s and to the Debt Crisis in 1980s, the first empirical contributions were only produced in the 1990s, and specifically after the Mexican (1994), Asian (1997), and Russian (1998) currency collapses. The intensity of the propagation mechanisms that characterized these shocks greatly surprised both academicians and practitioners. Indeed, whereas it is reasonable to expect that large countries suffering crises may influence smaller countries (as happened during the Great Depression), or that shocks in one country may affect its main trading partner (as in the case of the collapse in Russia in the 1980s that caused the collapse of Finland), in the aforementioned 1990s crises countries were heavily affected by shocks generated in other countries, despite the limited trading links with those countries. In contrast, in those years, other crises produced a confined contagion (or did not produce any contagion at all) as in the case of the Brazilian (1999), Turkish (2000), and Argentinean (2002) crises. This significantly contributed to the increase of interest in studying the drivers of shocks and, above all, the prevention of the related propagation mechanisms (Kaminsky et al. 2003). The 2008 US sub-prime crisis and the 2010 EU fiscal crisis have renewed the interest about contagion, creating a “near-ideal laboratory” for researchers in order to study causes and effects of financial contagion that arise at the time of stress (Longstaff 2010). As happened in the 1990s crises, in both the 2008 and 2010 crises a shock in small and isolated markets produced a huge impact at the global level. Indeed, even though the size of the sub-prime market in 2007 represented a very limited portion of the financial system, it had a massive impact in the US and across the world. Similarly, although Greece contributes in a very limited way to the financial flows in the Euro area, this makes it particularly hard to explain why other countries were affected by the collapse of Greece. Whereas in their recent study, Caporin et al. (2018) show there is no evidence for shift-contagion in European debt crisis, subject is still open to debate. From an empirical point of view, the proper analysis of the possible contagion’s determinants (if any exists) requires the inclusion of many variables into the estimated models. However, due to the increasing number of variables, the dimension of data space increases and observations become sparse and the situation worsens in presence of small samples. In conventional statistical techniques (in several forms of regression settings), obtaining statistically significance results becomes therefore nearly impossible. Even when significant results are achieved, model assumptions and validity are not hold. In order to deal with high dimensionality, a number of approaches can be employed. One of the simplest examples is to conduct a stepwise variable selection procedure to find the best fitting combination of variables (Siebenbrunner et al. 2017). But as the number of candidate variables increases, this approach quickly becomes time-consuming and mostly not efficient at finding the best or near-best combination of variables. Today, there are more options to deal with so-called curse of dimensionality, particularly machine learning methods (Athey 2018; Athey and Imbens 2019), combinatorial optimization techniques (Gilli and Winker 2008) and their hybrid forms. Their use in economic empirical research is fast growing providing a valuable option instead of simplification and misspecification in highly complex problems in economics. In this framework, the present article contributes to the empirical literature on contagion by investigating the determinants of shocks propagation in the Euro area using data from 2001 to 2015 applying a hybridized approach based on a machine learning, metaheuristic optimization and conventional econometric methods. More specifically, we adopted a three-step methodology. Firstly, we used a principal component analysis (PCA) on the first differences of countries’ bond yields to reveal any co-movement in bond yields’ changes. This allowed us to divide analysis period into sub-periods and to group countries in sub-groups. Secondly, we employed a random forest (RF) model with the aim of searching for government bond yields’ determinants in the identified sub-periods. Thirdly, impulse response functions (IRF) were produced from estimated vector auto regression (VAR) models. In equations, we used bond yield spreads between groups and focused on those in which spreads were regressed on their own lags along with selected variables’ (from second step) lags. In second and third steps, a simulated annealing (SA) algorithm was used for the variable selection (i.e. for optimization) to increase efficiency and reliability of our results. The improved success of RF learning and diagnosis test results on VAR models confirmed the SA contribution to our objective. We believe therefore that the methodology employed and the robust results achieved in this study contribute to literature in both respects. Next section reports the literature review followed by the material and methods section. Results are then presented along with discussions, and the paper ends with some concluding remarks.",2
8.0,2.0,Italian Economic Journal,29 June 2021,https://link.springer.com/article/10.1007/s40797-021-00160-5,Currency Unions and Global Value Chains: The Impact of the Euro on the Italian Value Added Exports,July 2022,Giovanni Cerulli,Silvia Nenci,Antonio Zinilli,Male,Female,Male,Mix,,
8.0,2.0,Italian Economic Journal,03 December 2021,https://link.springer.com/article/10.1007/s40797-021-00175-y,"A Regional Perspective on Social Exclusion in European Regions: Context, Trends and Policy Implications",July 2022,Massimiliano Agovino,Massimiliano Cerciello,Antonio Garofalo,Male,Male,Male,Male,,1
8.0,2.0,Italian Economic Journal,03 June 2022,https://link.springer.com/article/10.1007/s40797-022-00197-0,A New Dataset for Local and National COVID-19-Related Restrictions in Italy,July 2022,Francesco Paolo Conteduca,Alessandro Borin,,Male,Male,Unknown,Male,"Since the initial outbreak of the COVID-19 pandemic, restrictive measures imposed by public authorities to mitigate the spread of the virus have profoundly affected health and socioeconomic outcomes in every country. Variables such as gross domestic product (GDP), employment, consumption, and income inequality have been impacted by the interplay of epidemic conditions, individual behaviors, and public policies, including social distancing provisions (Caselli et al. 2020; Brodeur et al. 2021, among others). One requires adequate measures of these various dimensions to analyze their mutual interactions, assess their effects on health and socioeconomic outcomes, track and forecast their evolution over time, and design evidence-informed policy responses (Hsiang et al. 2020; Abouk and Heydari 2021; de Bondt and De Santis 2021; Bonaccorsi et al. 2020; Galeazzi et al. 2021; Goolsbee and Syverson 2021; Ng 2021; Marchetti et al. 2022). In terms of restrictions, different databases and indicators have been proposed to track public responses, with the Oxford Coronavirus Government Response Tracker (OxCGRT; Hale et al. 2021) emerging as the most comprehensive database on the non-pharmaceutical interventions enacted by governments worldwide.Footnote 1 The OxCGRT also includes a widely used synthetic measure of the severity of restrictions, the stringency index (henceforth, OxSI). The OxSI has allowed for the comparison of containment policies across a broad set of countries. Nonetheless, its ability to precisely track the actual levels of restrictions has diminished over time. In particular, after the first phase of the pandemic, many governments began to favor more localized and targeted measures over nationwide lockdowns. This shift has made it increasingly challenging to fit national-level provisions within the grid of subindicators comprising the OxSI. In this regard, Italy is no exception. To circumvent these limitations, the OxCGRT has incorporated subnational information for a set of countries (i.e., Australia, Brazil, Canada, China, India, United Kingdom, United States). However, the OxCGRT has not yet included Italy. This paper presents a novel dataset of local and national restrictions enforced in Italy. We collect information on measures implemented in the country from official and media sources, tracing non-pharmaceutical interventions enforced by local authorities (regions, provinces, and municipalities). As in the OxCGRT, our dataset consists of two main parts. First, we map qualitative information on the restrictions from different sources as categorical variables. Second, we aggregate these categorical variables to construct summary stringency indexes (henceforth, ItSIs) at different geographic levels. Next, we analyze how the ItSI—at different geographic levels—correlates with alternative stringency indexes and with commonly used measures of economic activity. We improve upon Hale et al. (2021) along several dimensions. First, by considering a targeted population size instead of a nationwide unweighted indicator, we can define stringency indexes at different levels of territorial aggregation (i.e., at national, regional, provincial, and municipal levels). Second, the ItSI overcomes some of the limitationsFootnote 2 of the OxSI by introducing a richer classification of the provisions, particularly concerning the economic activities affected by the restrictions. For instance, we increase the level of detail by adding intermediate levels to the original scale of the primary indicators and decomposing them into more specific subindexes. Finally, our database is a unique source of geographically granular, machine-readable data that researchers can use to assess the impacts of restrictions (e.g., school closings, limitations to production activities) in Italy. The availability of such information on restrictions guarantees a high level of flexibility in constructing aggregate indicators (for instance, one could calculate the ItSI with different weights other than population, such as value-added, output, or employment). In addition to the ItSIs, the dataset includes other variables (e.g., school closures, restrictions on various types of production activities, and shops) that can be used in future studies on COVID-19’s impacts on health, economic, and social outcomes (e.g., education). Data are freely available to researchers.Footnote 3 We show that during the first epidemic wave, the national ItSI tracks the OxSI closely. However, in the subsequent waves of the COVID-19 pandemic, the two indexes decouple, as provisions become increasingly differentiated across Italy. Furthermore, we also show that the correlation of the national ItSI with commonly used community mobility indicators is higher than that obtained using OxSI. Similar results are found when using different indicators of economic activity, such as quarterly GDP growth, consumption, and investment. Accurate measures of restrictions are essential for evaluating individuals’ response and compliance and disentangling the contribution of restrictions to the slowdown of economic activity. Our paper is close in spirit to those by Cheung et al. (2021) and Gros et al. (2021). In particular, Cheung et al. (2021) propose a province-based stringency index for Canada based on the structure proposed by Hale et al. (2021). Gros et al. (2021) propose weekly and monthly stringency indexes—which we label as EuSI—throughout 2020 for 31 European countries, including Italy, that rely on national-level restriction data provided by the European Centre for Disease Prevention and Control. In the paper, we provide a comparison between our nationwide stringency index and the EuSI. The structure of this paper is as follows. Section 2 provides an institutional background on the non-pharmaceutical interventions adopted in Italy since the outbreak of COVID-19. Section 3 describes our dataset and the construction of the ItSIs. Finally, Sect. 4 compares the ItSIs with the OxSI and the EuSI and analyses the relation between national, regional, and provincial ItSIs with available mobility and economic activity indicators at the corresponding geographical levels. Section 5 concludes.",3
8.0,2.0,Italian Economic Journal,03 March 2022,https://link.springer.com/article/10.1007/s40797-022-00189-0,The Grocery Trolley Race in Times of Covid-19: Evidence from Italy,July 2022,Emanuela Ciapanna,Gabriele Rovigatti,,Female,Female,Unknown,Female,"Italy has been among the first European countries to experience a widespread diffusion of the Covid-19, as well as one of the most hit in terms of deaths during the first wave. Since early March 2020, in order to contain the pressure on the overloaded healthcare system, several measures have been put in place by the Government under the guiding principle of social distancing to limit individuals’ exposure to the virus in the workplace and in public spaces. Productive activities deemed ‘non-essential’ and suspended by the Prime Minister’s Decree (DPCM) of 22 March 2020 until May 4th represented about one third of total value added, with percentages of up to around two thirds for the accommodation and catering services component and almost 100% for recreational activities. The impact of the pandemics on the various economic sectors, also due to unprecedented fall in demand, has been uneven. The immediate effects have been particularly severe in manufacturing, transports, catering, accommodation, recreation and culture, personal services, and in large swathes of retail trade. In March and April, the latter experienced a drop in sales of 23% compared to the corresponding period of 2019, driven by the severe decrease in the non-food compartment (− 45%). Instead, the food sub-sector outperformed, registering an overall increase of 5% in sales year on year (Fig. 1). Sales in grocery and total retail trade; year-on-year changes (color figure online) The present analysis focuses on the effects of the Government restrictions to mobility, imposed in Italy between the end of February and the beginning of May 2020, on sales of fast moving consumer goods (FMCGs) in grocery non specialized stores, namely hypermarkets, supermarkets, superettes and discounts.Footnote 1 We document the dynamics of revenues in the period immediately before and during the lockdown, distinguishing between different types of stores, products and by geographical breakdowns. Our findings point to a sustained growth in sales in the 2-month period March–April 2020 of more than 16% with respect to the corresponding period in 2019, a considerable increase, comparable in magnitude to Christmas’ sales peaks in “normal” times. The revenue surge during the restrictions is ascribable to both quantity and price increase, once accounting for the large composition effects at play. As of product categories, the most sustained dynamics concerned medical, pharmaceutical and food products. The type of outlet also represents an interesting dimension of heterogeneity throughout the spreading of the pandemics: restrained mobility favoured proximity stores of smaller size (superettes and smaller supermarkets), while penalizing larger ones (e.g. hypermarkets), generally located in more peripheral areas. Along these lines, we propose an attempt to causally identify the effects of mobility restrictions on sales dynamics, by disentangling them from the general impact of fear of contagion risks. To this aim, we perform a difference-in-difference estimation, exploiting the time lapse between the institution of the first partial red zone, involving 10 municipalities within the Lodi province on February 23rd (treated group), and the total national lockdown, starting on March 9th (control group). Our results indicate that during the partial lockdown week, large grocery chain stores (super and hypermarkets) in the treated province registered revenues around 10% lower than those of their control group. The effect is stronger for larger stores (e.g. hypermarkets), more exposed to mobility restrictions, due to their peripheral location, and for “less essential goods”, such as cosmetics and alcoholic beverages (− 19 and − 12% respectively). The temporary nature of such shocks—which peaked for all categories at the onset of the lockdown, just to slowly fade away when restrictions were lifted—traces their source back to Government restrictions, rather than being the signal of structural changes in the market structure. In view of the protraction of the pandemic and the need for the authorities to continue managing the containment of infections, our study has important policy implications: it provides an estimate of the costs (in terms of lost revenues) of anti-contagion interventions for a large sector of the economy, also highlighting possible distortions among different categories of outlets. Our results prove robust to different specifications. In particular, by exploiting the quasi-random location of the first Covid-19 clusters and the diffusion that ensued, we are able to split our sample according to the levels of contagion risk, and show that the latter does not bias our estimates.Footnote 2 Our work is related to several recent empirical research contributions examining many aspects of the Covid-19 pandemic, and using different data sources. A first strand of literature analyses the effects of lockdown measures and contagion on consumers’ behavior. Three notable studies for Denmark, Spain, and the US present evidence on the impact of Covid-19 on consumer spending. Andersen et al. (2020) use transaction-level bank account data from a large Danish bank to find a decline in spending—following the Covid-19 outbreak—which varies across product categories and correlates with Government’s restrictions. Carvalho et al. (2020) utilise a large high-frequency point-of-sale transaction dataset from a major Spanish commercial bank to find large overall spending declines across various product categories, following a Government lockdown. Baker et al. (2020a, 2020b) use transaction-level household financial data from a personal-finance website to observe a substantial increase in consumer spending as Covid-19 cases increase, followed by a significant decline in general spending. The authors also observe heterogeneity in spending responses across states (depending on the severity of the virus outbreak). Chronopoulos et al. (2020) examine consumer-spending responses to the onset and spread of the pandemics and the subsequent government-imposed lockdown in Great Britain. Based on data from a personal finance app, which aggregates all transactions from linked bank accounts and credit or debit cards, they find that overall consumer spending declined as the UK government lockdown became imminent and has continued to decline since, although with some heterogeneity by age, gender, and income level. Our work is especially related to two other recent contributions. The first one is an article by O’Connell et al. (2020), where the authors use household-level scanner data for the UK to analyse purchase dynamics during the pandemics. They document large spikes in spending on storable products in the 4 weeks preceding the lockdown, particularly visible for FMCGs. The second piece of literature is the work by Goolsbee and Syverson (2020), which proposes a causal estimate of the effects of the shutdown policy conditions at the county and city level in the US on foot traffic, a proxy for economic activity. The results indicate that legal shutdown orders have accounted for a modest share of the massive overall changes in consumer movements. However, Goolsbee and Syverson (2020) consider how often people visit shops rather than how much do they actually spend, which makes their findings perfectly compatible with a surge in grocery stores’ revenues overall. Compared to the the aforementioned contributions, the present article adopts a somewhat different perspective. In fact, our focus is on outlets’ performance (revenues and sold quantities) rather than on consumers’ preferences and spending behavior. The latter is clearly a fundamental driving force within the analysis, yet our main objective is to disentangle the effect on the observed dynamics of Government restrictions (imposed by decree with different timing) from those associated to contagion risks fear. The original contribution of our analysis is twofold: on the one hand, with respect to the aforementioned literature, we propose an attempt of causal identification of the lockdown effect on sales, on the other hand, our estimation method presents some elements of novelty in the designing of distance-depending control groups, borrowed from the spatial econometrics techniques (Diao et al. 2017). The paper is structured as follows. In Sect. 2 we present the institutional setting and the timing of the Covid-19 restriction measures in the different Italian regions, in Sect. 3 we outline the dataset and document revenues dynamics across broad sets of outlet types, regions and products. In Sect. 4 we propose our identification analysis of the the effects of restricted mobility on revenues dynamics, and discuss our results. A final section concludes. Robustness checks, additional tables and figures are relegated to the Appendices.",
8.0,3.0,Italian Economic Journal,16 July 2021,https://link.springer.com/article/10.1007/s40797-021-00163-2,From Micro to Macro: Micro-Foundations of the Italian Business Cycle Co-movements During the Crises,November 2022,Stefano Costa,Federico Sallusti,Davide Zurlo,Male,Male,Male,Male,,2
8.0,3.0,Italian Economic Journal,22 March 2021,https://link.springer.com/article/10.1007/s40797-021-00149-0,Does the Personal Income Flat Tax fit with Economic Growth and Inequality in Italy?,November 2022,Claudio Socci,Silvia D’Andrea,Francesca Severini,Male,Female,Female,Mix,,
8.0,3.0,Italian Economic Journal,26 June 2021,https://link.springer.com/article/10.1007/s40797-021-00161-4,Frequency vs. Size of Bank Fines in Local Credit Markets,November 2022,Francesco Marchionne,Michele Fratianni,Luca Papi,Male,Female,Male,Mix,,
8.0,3.0,Italian Economic Journal,22 July 2021,https://link.springer.com/article/10.1007/s40797-021-00165-0,‘Take the Money and Run’: Dutch Evidence on Inheritance and Transfer Receiving and Divorce,November 2022,Stefania Basiglio,,,Female,Unknown,Unknown,Female,"In recent decades, one of the main issues in many countries and many economic organizations (such as the OECD) has been closing the gender gap between women and men. Gender equality concerns not only rights but also power and benefits, which include, for example, the reduction of poverty. The reduction of poverty is reflected in the economic dependence of women in households; very often, after marriage and/or giving birth to a child, a woman has to leave her job or opt for a part-time position. To put the spotlight on the Netherlands, which represents the country of my analysis, Statistics Netherlands (CBS) says that ‘only 52% of women are economically independent, and only 54% of women are convinced that it is important to be able to support themselves and their children’. Bearing this in mind, does this economic dependence represent an obstacle to withdrawing from a marriage that is no longer working out? Could ‘monetary help’ have an impact in the household, and how could this change the bargaining power between the couple? These and other similar questions represent the motivation for my analysis. What I would like to do in this study is to provide evidence of a potential increased likelihood of divorce caused by the receipt of an inheritance or a transfer. According to the literature, the motives for divorce are a consequence of different factors such as religion, family-related features, the presence of children, and so on (Amato and Afifi 2006). In addition, the probability of future divorce depends strongly on female participation in the labour market. Interruptions in participation in the labour market caused by marriage, as well as by the birth and presence of children, can have long-term effects through lower future wages associated with less labour market experience, making a woman more economically dependent on her husband (van der Klaauw 1996; Pestel 2017). The objective of this paper is therefore to study whether receiving an inheritance or a transfer can, in some way, increase the chances of getting divorced. The contribution of my paper is twofold: first, this work represents, to the best of my knowledge, one of the first to provide evidence on the relationship between wealth endowment and divorce. Secondly, the results of my paper contribute to the literature on bargaining power in couples, since I show how a money endowment affects the decision-making process. In addition to this, what I am demonstrating is that the receipt of an inheritance within the household can lead to a break in the balance within the couple, leading potentially to a marital conflict. My empirical methodology involves the use of the DNB Household Survey (DHS), a Dutch panel data set collected by the CentERdata that allows both the psychological and the economic aspects of financial behaviour to be studied. This panel survey was launched in 1993 and comprises information on work, pensions, housing, mortgages, income, possessions, loans, health, economic and psychological matters, and personal characteristics. For reasons of data availability and to have a representative sample of the Dutch population, I concentrate my analysis on Dutch households of couples in the years from 2002 to 2016.Footnote 1 Starting from the idea that the receipt of an inheritance could affect different aspects of an individual’s life, I perform a Cox proportional hazard ratios model to estimate the probability that a married couple divorces, and the way in which this probability varies through time, identified by the duration of the marriage. My aim is to understand the role of the receipt of an inheritance/gift, differentiating between inheritances/gifts received by the husband and those received by the wife, and the role of other covariates that might have an impact on the transition probability. The findings suggest that when the inheritance/gift is received by the husband, there is a significant negative impact on the likelihood of getting divorced, while when the inheritance/gift is received by the wife, this increases the chances that the couple will separate. One possible interpretation of these results is that the receipt of an inheritance/gift changes the bargaining power between the couple: for the husband, who is probably already in the dominant position in the household, the endowment of wealth by means of an inheritance or a gift does not represent an incentive to divorce, while the results seem to suggest that the wife might perceive a change in the bargaining power between the couple, resulting in an increase in the chances of marital disruption. This result is also confirmed when differentiating between the case in which the income of the wife belongs to the bottom quintiles of the distribution, and when her income lies in the top classes of income distribution. Possible concerns might arise about whether any causal conclusion can be drawn from this work: the receipt of an inheritance is an endowment of wealth that is only included in my analysis (as will be explained in more detail in the description of the main variables) if it comes before divorce (the inheritance/transfer variable has been constructed as a lag variable to avoid any simultaneity between the receipt of an inheritance and divorce). Moreover, I also conduct the analysis excluding endogenous regressors (such as the variable recording the number of children in the household), and the results still hold. The rest of the paper is arranged as follows. Section 2 outlines the theoretical framework. Section 3 describes the data. Section 4 provides the empirical methodology, the main findings and some robustness and heterogeneity checks. Section 5 concludes the paper.",
8.0,3.0,Italian Economic Journal,16 March 2022,https://link.springer.com/article/10.1007/s40797-022-00185-4,Trade Specialisation and Changing Patterns of Comparative Advantages in Manufactured Goods,November 2022,Bernardina Algieri,Antonio Aquino,Marianna Succurro,Female,Male,Female,Mix,,
8.0,3.0,Italian Economic Journal,07 July 2021,https://link.springer.com/article/10.1007/s40797-021-00158-z,Asymmetric yardstick competition: traditional procurement  versus public-private partnerships,November 2022,Giuseppe Di Liddo,Annalisa Vinella,,Male,Female,Unknown,Mix,,
8.0,3.0,Italian Economic Journal,05 April 2021,https://link.springer.com/article/10.1007/s40797-021-00153-4,A Dynamic Oligopoly with Price Stickiness and Risk-Averse Agents,November 2022,Edilio Valentini,Paolo Vitale,,Male,Male,Unknown,Male,"How do price stickiness, uncertainty and risk aversion affect the equilibrium outcome of an oligopoly where firms compete over the demand of a homogeneous, non-storable good? We address such a question by extending Fershtman and Kamiem’s differential game for an oligopolistic market with sticky prices and a non-storable good (Fershtman and Kamien 1987) to a formulation with uncertainty and risk-aversion. We derive the optimal (sub-game perfect) production strategy, and the corresponding equilibrium price, and compare it to the Nash equilibria obtained by Fershtman and Kamien (1987). The main contribution of this extension is twofold. Firstly, we shed further light on the quite controversial issue—which has never been investigated in a dynamic oligopoly with sticky prices—of how uncertainty and risk-aversion affect market competition. Secondly, we show that the impact of uncertainty and risk-aversion crucially depends on price stickiness, as the stationary equilibrium collapses to Fershtman and Kamien’s analogue when prices can adjust instantaneously. We contribute to the literature extending the model developed by Fershtman and Kamien (1987). Dockner (1988) generalizes Fershtman and Kamien (1987) to the case of more than two firms showing that the dynamic oligopoly price converges to the long run (zero profit) competitive price when the number of firms goes to infinity, independent of the assumption of open-loop or feedback strategies. Tsutsui and Mino (1990) introduce the possibility of price ceilings to consider the case of nonlinear feedback strategies finding that, when the price ceiling is not too high, feedback equilibrium prices can be higher than the equilibrium price that arises under the linear feedback strategy assumed by Fershtman and Kamien (1987). Piga (2000) shows that when firms can invest in advertising the nonlinear feedback equilibrium price may be greater than the open-loop equilibrium price, while the latter is above the linear feedback equilibrium price. Other extensions include Dockner and Gaunerdorfer (2001) and Benchekroun (2003) who analyze the profitability of horizontal mergers, Cellini and Lambertini (2007) dealing with the case of firms selling differentiated products, and Wiszniewska-Matyszkiel et al. (2015) focusing on firms’ behavior off the steady state price path. Our model nests the classical dynamic oligopoly with sticky prices of Fershtman and Kamien (1987) that can be viewed as its continuous-time limit with no uncertainty and no risk aversion. Our analysis starts from a discrete-time formulation of a dynamic oligopolistic market with sticky prices which allows to introduce uncertainty and risk-aversion in a tractable manner by means of a special form of the recursive preferences proposed by Hansen and Sargent (1995). Such preferences are largely used in economics and finance (Hansen et al. 1999; Tallarini 2000; Luo 2004; Luo and Young 2010; Hansen and Sargent 2013) and correspond, under certain conditions, to Epstein–Zin’s recursive preferences (Epstein and Zin 1989; Epstein 1991). Thus, we derive the optimal (sub-game perfect) production strategy for symmetric firms and, focusing on the continuous-time limit of the infinite time formulation, we obtain several results. Notably, in the presence of demand volatility risk-averse entrepreneurs choose to produce larger quantities of the non-storable good, vis-à-vis their risk-neutral counterparts, since this reduces the variability of their payoffs. Such behavior is exacerbated when demand shocks are more volatile and, as a result, the steady state value of the equilibrium price results to be decreasing in both uncertainty and risk-aversion. Albeit this reaction to a higher degree of uncertainty can appear counter-intuitive from first sight, it is not unprecedented in models where agents strategically act in dynamic contexts. In fact, also in Fershtman and Kamien (1987) the way each firm takes rivals’ reactions to price changes into account brings about a generalized incentive to produce more. In our model, this effect is enhanced as risk-averse agents can reduce the variability of their future payoffs by further increasing their current production. Other interesting results are derived from the analysis of specific limit cases. Firstly, as it happens for the case without uncertainty and risk-aversion analyzed by Dockner (1988), when the number of firms goes to infinity the steady state price converges to the marginal cost. This is interesting because it shows that his result is robust with respect to the introduction of uncertainty and risk-aversion. Secondly, when the time-discounting factor goes to zero the steady state price converges to the value that would prevail in a static equilibrium with price-taker firms. In this case firms behave as price-takers because they do not take their future profits into account and, given the characterization of price stickiness, their production choices do not affect the current price level either. Thirdly, the steady state price converges to the equilibrium value of a perfectly competitive static market also when prices tend to be infinitely sticky. Obviously, here the result arises because firms’ production choices can affect neither present nor future prices. These limit cases emphasize the important conclusion that uncertainty and risk-aversion do not push firms to produce more when they behave as price-takers and cannot strategically take future outcomes into account. Fourthly, when prices become infinitely flexible the steady state price converges to a limit value which is higher than the price of the static equilibrium with price-taker firms and lower than that of the static Cournot oligopoly. Since in a duopoly such limit case coincides with the deterministic analogue discussed in Fershtman and Kamien (1987), we see that the impact of uncertainty and risk-aversion on a dynamic oligopoly where firms compete over the production of a homogeneous and non-storable good crucially hinges on the presence of price-stickiness. Finally, when we instead consider the special case of a unique firm in the market, we observe that an infinitely flexible price brings about the convergence of the stationary price towards the static Cournot price (coinciding in this case with the monopoly price). The rest of the paper is organized as follows. In Sect. 2 we first introduce uncertainty and risk-aversion in a discrete-time formulation of a market for a non-storable good with sticky prices, then we consider its continuous-time limit and characterize the equilibrium solutions. In Sect. 3 we concentrate on the stationary solution for the infinite horizon formulation and analyze the impact of risk-aversion and uncertainty. Finally, Sect. 4 investigates the limit cases and Sect. 5 concludes. The proofs of all results discussed in the paper are relegated in a separate Appendix.",1
8.0,3.0,Italian Economic Journal,05 April 2021,https://link.springer.com/article/10.1007/s40797-021-00152-5,"The Impossibility of a Paretian Liberal, Game Theory and Negotiation",November 2022,Aldo Montesano,,,Male,Unknown,Unknown,Male,,
9.0,1.0,Italian Economic Journal,03 October 2021,https://link.springer.com/article/10.1007/s40797-021-00170-3,Evaluating Public Support to the Investment Activities of Business Firms: A Multilevel Meta-Regression Analysis of Italian Studies,March 2023,Chiara Bocci,Annalisa Caloffi,Alessandro Sterlacchini,Female,Female,Male,Mix,,
9.0,1.0,Italian Economic Journal,25 January 2022,https://link.springer.com/article/10.1007/s40797-021-00181-0,Foreign Value Added along the Consumption Distribution,March 2023,Silvia Fabiani,Alberto Felettigh,Alfonso Rosolia,Female,Male,Male,Mix,,
9.0,1.0,Italian Economic Journal,15 November 2021,https://link.springer.com/article/10.1007/s40797-021-00174-z,The Wealth of Nations and the First Wave of COVID-19 Diffusion,March 2023,Roberto Antonietti,Paolo Falbo,Fulvio Fontini,Male,Male,Male,Male,"At the end of 2019 an unknown virus spread throughout Wuhan city and the Hubei province of China, causing a SARS-type disease with severe respiratory symptoms and fatal consequences in numerous cases. The virus was later identified as belonging to the Corona virus family and termed SARS-CoV-2, and the related disease COVID-19. At the end of January 2020, a Public Health Emergency of International Concern was declared as the virus rapidly spread around the world. On March 11, the World Health Organization officially declared the COVID-19 emergency a pandemic. By April 21, 178 countries had confirmed cases of COVID-19 infection.Footnote 1 The total count of reported cases and casualties was still rising at the time of writing this paper. Starting from the first weeks of the COVID-19 pandemic, many important questions have been rising within the scientific community. A very difficult puzzle, which is in large part still unsolved, refers to the huge and dramatic differences in the contagion and mortality rates recorded across the different parts of the world and even across different areas inside the same country. The question that COVID-19 seems a “rich man’s disease” is arising from the public debate, motivated by the observation that rich countries seem to have been hit at the beginning of the COVID-19 pandemic more severely than the poor ones. Some indirect links between wealth and the COVID-19 pandemic, such as the age structure of people in different countries, the international flows of goods and tourists, the endowment of the health facilities, can understandably have had a role during the first wave and eventually still have. However, the relationship between economic wealth and the initial spread of COVID-19 infection and mortality is still largely under-investigated. The aim of this work is precisely to assess whether such a relationship exists and if it relates to both the spread and the mortality of COVID-19. It is worth noticing that the focus on the first wave of the pandemic offers a unique opportunity to check for the effects of wealth, since contagion/death rates in the following periods might have been influenced by the adoption of diversified containment policies. The diffusion of COVID-19 has taken a certain amount of time to spread worldwide and countries have reacted with very different speed and severity with respect to lockdown and other containment measures, which generate huge social and economic costs. All this has affected the measurement of the effects of stock variables. Therefore, we measure the effects of wealth on the diffusion and mortality of COVID-19 from right after the start of the outbreak up until the moment when the strictest lockdown measurers started to be lifted in the European countries that were hit earliest and most severely (Italy and Spain). Focusing on such a period, we propose here an econometric analysis to test whether the COVID-19 infection and mortality rates are related to the economic wealth of countries, as measured by their GDP per capita, as well as to other relevant variables that characterize their demographic, health, and economic structure. We merge data on COVID-19 infections and deaths provided by the European Centre for Disease Prevention and Control (ECDC) with macro-economic data collected by the World Development Indicators of the World Bank group for 138 countries. Our estimates show that economic wealth is among the factors that are more strongly correlated with both the early infection and the mortality rates of the SARS-Cov2, together with a high share of elderly population and the country endowment of health facilities. We also check for the robustness of these results to endogeneity, to the possible poor quality of the data on COVID-19 deaths, to the potential role of early restriction policies adopted by some countries and to the addition of subsequent waves of the pandemic. The paper is organized as follows. Section 2 discusses the literature on the interactions between economic structure and COVID-19 pandemic. Section 3 presents the data and the variables used in the empirical analysis, and their main descriptive statistics; Sect. 4 outlines the empirical strategy; Sect. 5 presents the econometric results and Sect. 6 the robustness tests. Section 7 concludes.",6
9.0,1.0,Italian Economic Journal,09 September 2021,https://link.springer.com/article/10.1007/s40797-021-00168-x,The Immigration Puzzle in Italy: A Survey of Evidence and Facts,March 2023,Rama Dasi Mariani,Alessandra Pasquini,Furio Camillo Rosati,,Female,Male,Mix,,
9.0,1.0,Italian Economic Journal,17 March 2022,https://link.springer.com/article/10.1007/s40797-022-00186-3,The Financial Decisions of Immigrant and Native Households: Evidence from Italy,March 2023,Graziella Bertocchi,Marianna Brunetti,Anzelika Zaiceva,Female,Female,Unknown,Female,"In today’s world the issue of increasing immigration has reached center stage on policy makers’ agenda and is also widely analyzed in academia. There is a large literature on earnings and employment gaps, as well as on assimilation between natives and immigrants, and more recent studies on ethnic differences in well-being.Footnote 1 The literature on the nativity differences in wealth, and financial behavior more generally, is still thinner despite wealth holdings and portfolio allocation are important components of households’ economic well-being. Since wealth tends to be distributed more unequally than income, any disadvantage in the asset position of immigrants is likely to exert a persistent influence across generations, with implications for the chances of immigrants to assimilate. Wealth accumulation is determined not only by saving behavior but also by the allocation of financial portfolios. A nativity gap in the latter can therefore exacerbate the above processes. The ability to own a house is another crucial vehicle towards integration, which in turn depends on the availability of credit. Relative to natives, immigrants may have a harder time to achieve access to credit through traditional channels, even though informal networks may alleviate this disadvantage. Cultural differences may play a role as important as that of economic differences in determining a gap in financial behavior of immigrants and natives. Accumulated wealth, together with the financial diversification to minimize risks, becomes even more important in times of recessions, when immigrants often find themselves in a more vulnerable position vis-à-vis natives, since they are more likely to lose their jobs. In this paper, we investigate native-immigrant differences in financial behavior, in particular in wealth holdings and the allocation of assets, employing data for Italy over the period 2006–2014. To this aim, we use the Bank of Italy Survey of Household Income and Wealth (SHIW) dataset. Our rich dataset allows us to present a comprehensive picture of financial portfolios by incorporating a wide range of components, for instance informal debts, which are highly important when analyzing immigrants’ wealth, but due to data limitations were omitted from previous studies. Moreover, our data include a large set of information specifically on immigrants, including their immigration histories, their countries of origin, and their patterns of intermarriage, which allows us to explore potential heterogeneities along these dimensions. Furthermore, our data allow us to control for risk aversion, which is usually unobserved and yet crucial for this type of analysis. Finally, due to the length of the time period covered by our data, we can study the effect on nativity gaps of a large financial shock, such as the Great Recession, to identify differences in the financial response with respect to wealth and asset holdings. While parts of this information have been employed by others, it has rarely been available in its entirety. Thus, the main contribution of this paper is to jointly analyze all the dimensions that are potentially relevant to the analysis of nativity gaps in financial behavior, in a unified setting where we look at a wide set of outcomes, we use a large set of covariates including risk aversion, countries of origin, cohorts of arrival, and patterns of intermarriage, and we distinguish between the pre- and post-financial crisis period. Italy represents a particularly suitable country to study these questions. First, it has recently turned from an emigration country into an immigration country and has faced significant immigrant inflows following EU Eastern enlargements and the unrests in Africa. With its share of foreign-born amounting to over 10% in 2019 (OECD 2020), it is now comparable to traditional immigration countries such as Denmark or the Netherlands, and is approaching the levels of Germany, the UK and the US. Second, immigration could rebalance the population age distribution for a country that—with an old age dependency ratio of 37 in 2020 (as reported by the World Bank)—is one of the most aged countries in the world, and is predicted to age even more rapidly in the future. Third, Italy has experienced a severe recession post 2008, with GDP growth falling by 6% in 2009 only, and with an unemployment rate jumping from 6.7% in 2008 to 12.9% in 2014. Fourth, due to the rigidities in the country’s financial markets including, for instance, difficulties in obtaining a loan or a mortgage, the reliance on informal credit channels involving relatives, friends or the reference community may be even more relevant. Our main results can be summarized as follows. With reference to wealth holdings, a quantile regression approach allows us to uncover evidence of a sizeable gap between natives and immigrants along the entire wealth distribution. The median net wealth of a foreign-born household head is €21,121 lower than that of a native. We capture financial asset allocation decisions using five main variables: the decision to invest in risky assets and the corresponding portfolio share, home ownership, holding a mortgage, and holding informal debts. We find a negative correlation between the immigrant status of the household head and each of these outcomes, with the only exception of informal debts. Moreover, immigrant status is negatively associated with the likelihood of investing in foreign assets, together with the corresponding portfolio share, and of owning businesses and valuables, while it is positively associated with the likelihood of being in a condition of financial fragility. The above described results are obtained after controlling for year and macro-region fixed effects as well as a rich set of observable characteristics (demographic and labor market variables as well as household composition, income, and risk aversion), which should help to diminish the potential bias due to unobservables. To further address this issue, we also apply a propensity score matching strategy, in order to restrict the comparison to immigrant and native households sharing a broad set of characteristics. Reassuringly, the two estimation strategies present a broadly similar picture. We proceed with the analysis by dissecting the results along several dimensions, for both wealth holdings and portfolio decisions. We find evidence that years spent in Italy, countries of origin, and patterns of intermarriage do matter, while an alternative definition of an immigrant as a non-citizen, rather than a foreign-born, does not affect our conclusions. Finally, we show that following the Great Recession of 2008 the financial status of immigrants has worsened in several dimensions. The paper is organized as follows. Section 2 contains a literature review. Section 3 documents immigration trends in Italy. Section 4 describes the data. Section 5 presents our main results on the immigrant-native gap in wealth holdings and asset allocation. Section 6 presents propensity score matching results. Section 7 extends the baseline analysis to account for citizenship status, cohorts of arrival, countries of origin, the influence of spouses, and the effects of the financial crisis. Section 8 points to limitations to our results and Sect. 9 concludes.",2
9.0,1.0,Italian Economic Journal,04 March 2022,https://link.springer.com/article/10.1007/s40797-022-00184-5,"Do informal Networks Increase Migrants’ Over-Education? Comparing Over-Education for Natives, Migrants and Second Generations in Italy and Assessing the Role of Networks in Generating It",March 2023,Pierre Georges Van Wolleghem,Marina De Angelis,Sergio Scicchitano,Male,Female,Male,Mix,,
9.0,1.0,Italian Economic Journal,10 January 2022,https://link.springer.com/article/10.1007/s40797-021-00180-1,Women-led Firms and Credit Access. A Gendered Story?,March 2023,Stefania Basiglio,Paola De Vincentiis,Mariacristina Rossi,Female,Female,Unknown,Female,"“When I was concluding my studies in Law at university, I had an interview in a law firm in France. It went well, but at the end one of the interviewers told me that I would never become a partner. ‘Why?’ I asked. ‘Because you are a woman, they replied’”, said Christine Lagarde, current President of the European Central Bank. “[…] Times have been changed, but women still have many challenges to face. ‘What was true in 1981 is unfortunately still true today […]’”.Footnote 1 Christine Lagarde's words strongly bring to mind an inequality that still afflicts our society. The gender gap is widespread all over the world, even if huge differences emerge among countries. Fortunately, many steps have been taken to reduce the distance between women and men in health, education, economics and politics, but to date, no country has yet achieved full gender parity. In 2019, the average Global Gender Gap score stood at 68%. This means that the remaining gap is 32%. Among all the research fields that could be investigated to study the gender gap, we focus our attention on entrepreneurship and credit access and availability. Current data generally report a fairly common occurrence: female entrepreneurs are less likely to resort to external funds (Aspray and Cohoon 2007; Guzman and Kacperczyk 2019), including credit (Cole and Mehran 2018). What could be the possible reasons for such a finding? These could lie on the demand side as well as on the supply side. In fact, on the one hand, women could simply ask for credit less frequently than men. But, on the other hand, if the supply channel is dominating, women would receive a rejection more often even though they do not differ from their male peers in their socioeconomic features or in their application rate. In a nutshell: do women receive less credit because they ask less frequently or because of their gender? Since both factors may play a role in explaining the phenomenon, it is crucial—from a policy point of view – to disentangle demand and supply side factors. Only by understanding the root causes of women's lower recourse to credit, in all its nuances, will it be possible to plan and implement effective corrective actions. Our work aims to significantly contribute to the existing literature on the relationship between women and credit, using a unique dataset: the sample of the Employer and Employee Survey 2015 (Rilevazione su Imprese e Lavoro—RIL) conducted by the National Institute for the Analysis of Public Policies (INAPP—previously ISFOL).Footnote 2 The RIL is a nationally representative sample of over 29,000 partnerships and limited companies operating in the non-agricultural private sector in Italy. It is worth noting that our data structure allows us to properly address the issue of gender differences in credit access, analysing an ample and diversified sample of firms, some of which have applied for credit and some of which have not. Contrary to other papers that deal with gender bias and credit rationing using only a selection of male and female firms that asked for credit (Blanchflower et al. 2003; Bellucci et al. 2010), our sample helps us to better manage selection issues and disentangle the effect of the demand side factors and their supply side counterparts. In order to clearly identify which channel is at work in the credit access for women-led firms, we use two variables named ‘credit demand’ and ‘credit approval’, built on the basis of the answers of the ‘decision maker’ survey respondents. To the best of our knowledge, we are the first to use a sample that is fully representative of all firms, not only micro and small, to disentangle the two channels. We hence fill an important gap in the literature. Aside from the merits of the available dataset, the interest of applying the analysis to the Italian case lies in the strong gender gap that still afflicts the country, making it a perfect environment in which to study the features of gender-related phenomena. Reverting to the aforementioned Global Gender Score, the top countries stand at 80%, while Italy lags behind at 63%, facing marked gender inequality particularly in the domains of power, time and work, where it has the lowest score of all EU member states. The use of a dataset of Italian firms also offers the opportunity to analyse different cultural attitudes towards gender, given the heterogeneity of the country across regions, with the North exhibiting economic performances more similar to Northern Europe than the rest of Italy. We hence expect lower gender differences in firms located in the north of the country. The country as a whole, with a low gender equality index, could represent fertile ground for potential discrimination in the supply channel. Instead, our results show that Italian firms led by women receive less credit because they sought it out less frequently than men, and not because they are rejected more often. In other words, only the demand channel is significant, while the supply channel does not exhibit any power in practically altering the outcome of the loan application. This result is in line with the finding of Ongena and Popov (2016) and Cowling et al. (2019) but is reinforced by the representativeness of the sample. Self-exclusion is, according to our results, the main reason why female entrepreneurs experience less access to credit, and not unfair conduct by the banking system. From a policy standpoint, the implications are neat and suggest that interventions should be targeted at entrepreneurs rather than bank officers to reduce the credit gaps. A need clearly emerges to pursue financial education campaigns and policies to enhance female entrepreneurship, as well as training interventions designed to strengthen soft skills and women’s self-confidence. Recalling Ms. Lagarde’s words, women should learn how to overcome the fear of being bold and knock-on new doors along their journey. The paper is structured as follows: in Sect. 2, we review the specific literature on the topic of gender discrimination in credit access. In Sect. 3, we present the database, the specific features of the sample and we provide the main descriptive statistics. Section 4 analyses the specification model, the empirical results and some extensions, as well as robustness checks. Finally, Sect. 5 draws conclusions, comments and the policy implications of our results.",
9.0,1.0,Italian Economic Journal,16 March 2022,https://link.springer.com/article/10.1007/s40797-022-00188-1,The Effect of Giving Credit to Social Enterprises: Evidence From Italy,March 2023,Koray Aktaş,Gian Paolo Barbetta,,Male,Male,Unknown,Male,"In many countries, social enterprises (SEs)—organizations that pursue social as well as economic aims—are important private providers of welfare services, which also act in the related areas of urban regeneration, long-term unemployment reduction, and community development. Several of these firms—such as the Italian social cooperatives (SCs)—are not-for-profit organizations, which means that they pursue a social mission and cannot distribute surpluses to managers and employees. Despite having grown in number, SEs struggle to fully meet the increasing demand for services. This could depend on the difficulties in raising new capital that they share with most not-for-profit organizations. As a result of these difficulties, most SEs can only rely on profits to finance their growth, which is often a slow process. Could SEs count on credit to overcome their problems in raising capital? While this is possible in theory, in practice the credit markets are characterized by information asymmetries that banks overcome asking perspective borrowers for guarantees and collaterals. Unfortunately, due to their legal status, SEs often lack the collateral required by banks. Thus, these firms are frequently subject to credit rationing. It should also be noted that a larger supply of credit is not necessarily beneficial for SEs. In fact, being unable to raise risk capital, SEs could rely too much on debt, increasing their total costs of capital and therefore jeopardizing their own economic stability. Whether the positive or possible negative effects of credit prevail for SEs is an empirical question, although the literature in this field is very scarce. In this paper, we try to fill this gap in the literature analyzing the effect of a positive credit supply shock provided by the establishment in 2008 of a new Italian bank (Banca Prossima, hereafter BP)—specializing in SEs and not-for-profit firms—on the economic performance of SCs, the most common form of SE in Italy. To estimate the impact of the new credit provided by BP, we create a longitudinal data set (spanning from 2004 through 2015) by merging confidential data provided by BP with publicly available balance sheet data of SCs in Italy. To address the possible issue of endogeneity (when credit decisions are based on the performance of SCs) and identify the effect of having access to new credit, we first employ a difference-in-differences (fixed-effect) approach by exploiting the variation in access to BP credit across time and SCs. Our main findings indicate that on average SCs with access to the new credit from this specialized bank (receivers) increase their relative production level (10 percentage points), fixed assets (FA) (20 percentage points), property, plant, and equipment (PPE) level (27 percentage points), and employment (10 percentage points) compared to non-receivers. We also show that the effects on FA and PPE come from long-term credit and persist during the post-credit period. We perform several placebo tests to check the validity of our identification strategy. First, we investigate whether receivers and non-receivers showed different trends (as far as the main outcome variables are concerned) before credit was given. This investigation is conducted year-by-year in an event-study specification. We also control for the total debts of SCs (excluding the credit from BP) as well as other key observable time-varying indicators included in our dataset. We do not observe statistically significant differential trends prior to receiving credit from BP. This is not only true for the years prior to the foundation of BP (from 2004 to 2007) but also thereafter (from 2008 to 2012Footnote 1). Second, to better deal with the endogeneity concern, we use an alternative identification strategy. A recent study by Degryse et al. (2019) discusses and highlights the importance of controlling for location, industry, size, and time effects to identify the effects of credit supply shocks. Therefore, we combine a difference-in-differences approach with a matching procedure. We match receivers and non-receivers adopting a propensity score matching approach within homogeneous groups defined by the SCs’ area (macro regions) and sector of activities, based on data measured right before receivers had access to credit from BP. Our econometric specification includes social cooperative and time fixed effects, and we also control for the time-varying performance indicators and the distance (over time) from the moment that SCs receive credit from BP. This setup enables us to estimate dynamic treatment effects while allowing for heterogeneity in treatment intensity (the length of treatment). Our results show that the parallel trend assumption also holds for this matched sample, and that the receivers start increasing their production, fixed assets, and PPE after obtaining credit from BP compared to the matched SCs. Taken together with the fact that very few financial tools are available to SCs in Italy (see Becchetti et al. 2011), our flexible and rich econometric models is a possible indicator of the causal effect of having access to credit from BP. Our contribution to the literature is twofold. First, we provide—for the first time, to the best of our knowledge—evidence on the possible positive causal link between access to credit and the economic performance of not-for-profit SCs, analyzing both short- and long-term effects. Our findings complement the existing studies with rigorous empirical evidence about the potential effect of providing SCs with more credit. This result is quite relevant given that the empirical evidence on the effects of relaxing credit constraints on not-for-profit SEs is very scarce, and the theoretical reasons could lead to both positive and negative effects. Second, our findings have important policy implications. In fact, Western public welfare systems are struggling to respond to the increasing demand for social services from the rising elderly population, the growing number of immigrants, and people affected by the recent economic and financial crisis. SEs represent one of the most effective solutions to these emerging needs, but they rarely have access to capital, credit, or financial instruments. SEs (and SCs in particular) often rely on government subsidies and grants, but the literature about the effects of government subsidies on the growth of some industries shows mixed evidence (Criscuolo et al. 2019). Therefore, our findings might hold interest to policy-makers. In fact, giving SEs better access to credit may favor their growth and could directly translate into help for individuals in need. The remainder of this paper is organized as follows. We briefly refer to the relevant literature in Sect. 2 and depict BP in Sect. 3. In Sect. 4, we describe our data sets, the variables that we use for our analysis as well as our main econometric models. In Sect. 5, we present our results and describe the heterogeneous effects of our analyses. In Sect. 6, we test alternative identification strategies. Finally, in Sect. 7, we draw some policy conclusions.",
9.0,1.0,Italian Economic Journal,29 September 2021,https://link.springer.com/article/10.1007/s40797-021-00171-2,Broadening Economics in the Era of Artificial Intelligence and Experimental Evidence,March 2023,Jan Niederreiter,,,Male,Unknown,Unknown,Male,,
9.0,1.0,Italian Economic Journal,20 April 2022,https://link.springer.com/article/10.1007/s40797-022-00193-4,Insurance Choices and Sources of Ambiguity,March 2023,Daniela Di Cagno,Daniela Grieco,,Female,Female,Unknown,Female,,
9.0,1.0,Italian Economic Journal,23 March 2022,https://link.springer.com/article/10.1007/s40797-022-00187-2,Framing Effects in the Elicitation of Risk Aversion: An Experimental Study,March 2023,Luca Congiu,,,Male,Unknown,Unknown,Male,"Risk aversion can be elicited experimentally through a variety of methods (for a review, see Harrison and Rutström 2008; Charness et al. 2013). Typically, elicitation methods present the decision maker with a menu of monetary lotteries with varying risk and ask her to choose the lotteries she prefers. Well-known examples of such elicitation methods are the ‘ordered list selection’ (Binswanger 1980; Eckel and Grossman 2002) and the ‘multiple price list’ (Holt and Laury 2002). In these elicitation tasks, lotteries can be presented through a verbal description stating the outcomes and their likelihood (e.g., “Win $5 with probability 10%”, “1/10 chance to win $5”). The numeric risk information may be accompanied by a pictorial display, such as a pie chart or bar graph (for a review of pictorial displays in risk elicitation tasks, see Harrison and Rutström 2008, especially Appendix A). Although alternative numeric formats and alternative pictorial displays may fundamentally convey the same risk information (e.g., “10%” vs “1/10”), the literature on risk communication shows that each format and each display may frame it in a way that alters the perception of risk (e.g., Halpern et al. 1989; Schapira et al. 2001; Ancker et al. 2006; Lipkus 2007). For example, in a qualitative study on alternative formats to convey the risk of a disease, Schapira et al. (2001) find that the figure “1 out of 10” leads individuals to declare a lower concern for risk than the equivalent figure of “10%”. A similar but opposite effect is found in pictorial displays: risk information presented through discrete graphs (e.g., human stick figures) elicits a higher concern for risk than risk information presented through continuous graphs (e.g., a bar chart) (Schapira et al. 2001; Ancker et al. 2006). If alternative numeric and pictorial formats can frame risk differently, then their use in risk elicitation tasks might give rise to framing effects in the elicitation of risk aversion. In this paper, I test for numeric and pictorial framing effects in a lottery choice experiment. Specifically, I administer a multiple price list (MPL) to 95 University students in a 2 × 2 between-subject design, where risk information is presented either as percentages (“10%”) or ratios (“1 out of 10”) and is accompanied by a pie chart sliced either in two or ten slices. In the MPL task, participants are confronted with a menu of ten pairs of lotteries. In each of the ten pairs, lottery A and lottery B have two fixed outcomes each and the same probabilities for the high and low outcomes. For example, lottery A and lottery B in the first pair are, respectively, A1 = (100, 0.1; 75, 0.9) and B1 = (150, 0.1; 25, 0.9). The remaining pairs are constructed by increasing the probability of the high outcome by 0.1 and commensurably reducing that of the low outcome, so that the two probabilities are 0.2 and 0.8 in the second pair, 0.3 and 0.7 in third, and so on to the last pair, in which they are 1 and 0. Framings were applied with respect to either numeric risk information or pictorial risk information or both. Taking as an example lottery A4, the description was either “Win 100 tokens with probability 40%, win 75 tokens with probability 60%” or “Win 100 tokens with probability 4 out of 10, win 75 tokens with probability 6 out of 10”. Pictorially, lottery A4 was accompanied by either a standard pie chart with one blue slice covering 40% of the pie and one orange slice covering the remaining 60%, or a ‘sliced’ pie chart with 4 blue slices and 6 orange slices. Risk aversion was elicited under the two (numeric) × two (pictorial) framings. It was measured as the number of the menu row where lottery B, the riskier lottery of the pair, is first selected by the participant. A weighted linear regression shows that the numeric framing (using ratios) and the pictorial framing (slicing pies) reduce the switching row, respectively, by 0.27 (p = 0.621) and 0.84 (p = 0.255). This implies that, on average, the two numeric and pictorial framings do not significantly change the number of the row where participants switch their selection from the safer lottery A to the riskier lottery B. These results remain not significant when controlling for potential confounders, such as the tendency to mentally convert a numeric format to an equivalent one (e.g., percentages in ratios) and the degree of perceived inconsistency of the numeric information with the pictorial display (e.g., the use of ratios with standard pies, respectively conveying risk information as units and proportions). The experiment found, however, an interaction effect between the two framings and the probability which draws the most attention: participants who declare to have focused more on the probability of the high outcome when making decisions exhibit a lower switching row when presented with ratios (− 3.7; p = 0.075) and when presented with sliced pies (− 8.17; p = 0.001). Ultimately, the findings of the present experiment suggest that, in MPLs and similar lottery-based tasks, the elicitation of risk aversion may be robust to both numeric formats (i.e., percentages and ratios), but the impact of the pictorial framing may depend on which probability draws the most attention. The rest of the paper is structured as follows. Section 2 reviews the literature on numeric and pictorial framings and formulates the related hypotheses. Section 3 illustrates the MPL task and the experiment design. Section 4 reports the estimation of treatment effects and potential confounding factors, and Sect. 5 concludes.",
9.0,1.0,Italian Economic Journal,28 January 2022,https://link.springer.com/article/10.1007/s40797-021-00179-8,Retirement Expectations in the Aftermath of a Pension Reform,March 2023,Noemi Oggero,,,Female,Unknown,Unknown,Female,,2
9.0,1.0,Italian Economic Journal,31 October 2021,https://link.springer.com/article/10.1007/s40797-021-00172-1,Risk Aversion and the Size of Desired Debt,March 2023,Elena Lagomarsino,Alessandro Spiganti,,Female,Male,Unknown,Mix,,
9.0,1.0,Italian Economic Journal,18 August 2021,https://link.springer.com/article/10.1007/s40797-021-00166-z,Foreclosures and House Prices,March 2023,Michele Loberto,,,Female,Unknown,Unknown,Female,"During the period 2008–2013, the Italian economy suffered a severe double-dip recession. One of the consequences of the decline in economic activity was the sharp rise in default rates, particularly in the real estate sector. Default rates were high among construction and real estate firms, and they reached their historical peak also for households (Ciocchetta et al. 2016). Therefore, many real estate properties have been foreclosed to repay the debts incurred by the debtors. In the 3 years 2016–2018, the average annual number of pending foreclosures procedures, which lenders must follow to recover amounts still due, was about 250,000, many of them involving dwellings. Since the stock of foreclosures has been vast compared to the trading volumes in the market, it is relevant to investigate whether and how foreclosures affected the real estate market.Footnote 1 This question is also timely, as the economic recession caused by the COVID-19 epidemic is likely to lead to a new wave of mortgage defaults and foreclosures.Footnote 2 In this paper, in particular, we focus on the impact of foreclosures on housing prices in Italy. Although no specific analysis has been carried out so far on this issue for the Italian case, the conventional wisdom was that foreclosure spillovers would have been null or very limited. This belief was motivated by the fact that the purchase of foreclosed houses—which occurs through auctions managed by the courts—was very complicated for many reasons.Footnote 3 However, this situation could have changed since 2015, when legislative reforms aimed to simplify the process of selling foreclosed properties entered into force. Based on our knowledge, the impact of foreclosures on house prices has been studied only in the US. This paper is the first to examine the spillovers of foreclosures on the housing market in a country with relevant institutional differences compared to the US. We believe that this is important to better understand the extent and how foreclosures affect the housing market. The Italian case is quite interesting for two reasons. First, mortgage loans to households are recourse, which softens the feedback effect from house prices on foreclosures.Footnote 4 Second, the inefficiencies in the foreclosure process give rise to a considerable foreclosure price discount (i.e., the percentage difference between the price of a foreclosed home and the market price of the same house if sold in an unforced transaction), which is far higher compared to the estimates available for the US. Because of this considerable foreclosure price discount, one could expect a very limited arbitrage between foreclosed homes and the market for non-foreclosed properties. First, we estimate the foreclosure discount. Second, we analyze if the price-setting strategies of home sellers are affected by the competition of nearby foreclosure listings. Finally, we estimate the spillovers of foreclosures on the prices of non-foreclosed nearby houses. Surprisingly, our results are very similar to some of the most reliable estimates for the US, although the institutional framework is very different between Italy and the US. We analyze a large dataset of listings published on Immobiliare.it—the largest online website for real estate services in Italy—between the second half of 2016 and December 2018. Our data cover about one hundred province capitals, representing nearly one-third of the Italian population and house sales. Overall, we observe the listing history of about 620 thousand dwellings at a weekly frequency. For each home, we know the timing of entry into the market and delisting (i.e., when the property exits from the market), a large set of physical characteristics, the time series of the asking prices, and the timing of price revisions. We estimate that the foreclosure discount is approximately between 42 and 56%. The discount is heterogeneous across cities and varies with the characteristics and the geographical position of the property. It is higher for smaller dwellings and those in suburban neighborhoods. Such a high discount would suggest that the market for foreclosures is isolated from other segments of the housing market. Strong segmentation would imply limited spillovers from foreclosures to the housing market. However, this is not the case. Following the approach proposed by Anenberg and Kung (2014), we show that home sellers have a higher propensity to revise downward their asking price in the same week and the week following the market entry of a nearby foreclosed house. That is consistent with the hypothesis that home sellers consider foreclosed houses as competitors, and because of the considerable foreclosure discount, they react by lowering their prices. The exact timing of the market entry of a foreclosed home is not correlated with some unobservables that simultaneously affect the likelihood of price revisions. It is the output of an administrative process. Therefore, we can say that the listing of a foreclosed home causes a reaction from nearby home sellers. We show that this effect is local. It is statistically significant and economically relevant up to a distance of 150 m from the foreclosed house.Footnote 5 Then it decreases and becomes very small at 400 m. Foreclosures compete in the housing market with non-foreclosure listings. Consequently, it is essential to investigate to what extent foreclosures influence the prices of nearby non-foreclosed properties. Estimating the causal effect of foreclosures on house prices is complicated because they are strongly interrelated. Foreclosures are endogenous to house prices because a fall in house prices reduces the equity value of dwellings. Suppose the value of a house falls below that of the outstanding mortgage debt taken to buy it (negative equity). In that case, the homeowner has the incentive to default on his debt strategically.Footnote 6 Moreover, there can be common shocks that may affect both house prices and foreclosures. A typical example is an income shock resulting from the shutdown of some relevant economic activity within a neighborhood. Some debtors may lose their jobs and no longer be able to pay their debts. Their homes would be foreclosed and sold through judicial auctions. At the same time, a lower income in the neighborhood would lead to lower demand for housing and lower house prices. We follow an approach similar to Campbell et al. (2011) and Anenberg and Kung (2014). We find that the impact of foreclosures on nearby house listing prices is negative and significant. The final listing price of dwellings distant less than 150 m from a foreclosed home is lower by 1.1%, an estimate very similar to those of Campbell et al. (2011) and Anenberg and Kung (2014) for the US. We believe that estimate is a good approximation for the impact on sale prices because listing and sale prices have had a very similar trend during 2016–2018. Moreover, we do not find evidence that foreclosures affect the average discount over asking prices that buyers obtain from sellers. In Sect. 1.1 we make a summary of the most related literature. Section 2 describes the institutional framework for foreclosures in Italy and recent developments in the foreclosures market. Section 3 describes the data used in the paper. In Sect. 4 we estimate the foreclosure discount. In Sect. 5, we demonstrate that the presence of nearby foreclosed houses affects the propensity of home sellers to revise their prices downwards. In Sect. 6, we estimate the effect of the presence of an auction house on non-auction houses. Finally, Sect. 7 concludes. The most related paper is Anenberg and Kung (2014). As in their case, we take advantage of the high frequency of listing data to identify causal relationships. The strategy to estimate the impact of foreclosures on house prices is similar to Campbell et al. (2011). We discuss in Sect. 6 the difference between our specification and their econometric strategy. Other recent studies on foreclosure spillovers are Immergluck and Smith (2006), Harding et al. (2009), Hartley (2014), Mian et al. (2015) and Gerardi (2015). All these papers study the effects of foreclosures based on US data, and almost all of them have limited geographical coverage. On the opposite, this paper studies foreclosure spillovers in Italy, a country with a very different institutional framework from the US concerning foreclosures and the housing market, and covers all the main cities. The literature on the US reaches two conclusions. First, in the US the foreclosure discount is considerable but significantly lower than in Italy. Most estimates are larger than 10%, and the higher estimate is about 28% (Campbell et al. 2011). Second, foreclosed homes affect nearby houses’ selling price: the estimated price reduction caused by foreclosures is smaller than 2%, and the negative spillover effect is local. Cohen et al. (2016) provide a rich overview of this literature.",
9.0,2.0,Italian Economic Journal,29 April 2022,https://link.springer.com/article/10.1007/s40797-022-00190-7,Do the Most Vulnerable Know About Income Support Policies? The Case of the Italian Reddito d'Inclusione (ReI),July 2023,Marina De Angelis,Pierre Georges Van Wolleghem,,Female,Male,Unknown,Mix,,
9.0,2.0,Italian Economic Journal,22 July 2022,https://link.springer.com/article/10.1007/s40797-022-00201-7,The Dynamics of the Gender Gap at Retirement in Italy: Evidence from SHARE,July 2023,Antonio Abatemarco,Maria Russolillo,,Male,Female,Unknown,Mix,,
9.0,2.0,Italian Economic Journal,20 April 2022,https://link.springer.com/article/10.1007/s40797-022-00191-6,Scars of Youth Non-employment and Labour Market Conditions,July 2023,Giulia Martina Tanzi,,,Female,Unknown,Unknown,Female,"The aim of this paper is to study whether a non-employment experience at the beginning of the career harms individual outcomes in terms of later non-employment. Using data on young individuals in Italy, I explore whether these adverse effects, commonly known in the literature as the scarring effects of non-employment, exist and vary across the country, depending on the regional labour market conditions. Understanding if the effects of early-career non-employment are lasting as well as their determinants is crucial in order to implement the necessary policy responses aimed at addressing school-to-work transition and targeted interventions early in peoples’ careers. If not finding a job early in the career can cause joblessness many years later, preventing non-employment of youths has pay-offs well beyond the reduction of contemporaneous non-employment costs, because of their effect in improving future employment chances too. Despite the large number of works that tried to estimate these effects, the empirical literature does not agree unequivocally on their existence and size.Footnote 1 More importantly, only a few works have studied the moderating effects that may exist within a country according to the specific labour market conditions, over time or across regions (Lupi et al. 2002; Biewen and Steffes 2010; Ayllón 2013).Footnote 2 Precisely because of this lack of investigation, I believe that this analysis, which exploits regional differences within Italy in terms of unemployment rates, can contribute to better understanding the role of labour market circumstances in mitigating or in amplifying the scarring effects. Why should we expect that an early experience of non-employment means long-term negative repercussions for young people’s careers and that these repercussions vary according to the labour market conditions? There are three main theories that predict the existence of the scarring effects of non-employment. First, according to the signalling theory, employers have imperfect information about applicants and are unable to differentiate perfectly between persons with poor work skills from those with superior work qualities. For these reasons, they use past non-employment records as a signal of low or high productivity (Vishwanath 1989; Lockwood 1991; Gibbons and Katz 1991). Second, according to the human capital models, early spells of non-employment would deprive the individual of work experience during that part of the life cycle that yields the highest return. This strong depreciation of human capital and loss of specific job skills results in high non-employment in later periods (Pissarides 1992; Acemoglu 1995). Third, according to the job matching theory, non-employment periods may alter individuals’ job application behaviour, making them more prone to accept unsuitable or poor quality jobs that are more likely to end or to be destroyed (Mortensen 1986). It is reasonable to assume that these mechanisms may work differently according to the conditions of the relevant regional labour market (Omori 1997; Biewen and Steffes 2010). As regards the first mechanism, based on the signalling theory, the weakness of the labour market may influence the way in which employers interpret past non-employment records as a sign of individual ability. In regions with poor labour market conditions or in times of relatively high unemployment, past unemployment spells may not necessarily be perceived as a sign of low productivity and the disadvantage of having been unemployed may become smaller. So, the higher the regional unemployment rate, the less severe any adverse effect of past spells of non-employment of a given length in the evaluation of the job recruiter will be. On the contrary, according to the job matching theory, the scarring effects could be amplified in a weak labour market, because in this scenario individuals’ discouragement increases as they become aware that it is more difficult to find a job. Finally, according to the predictions of the human capital theory, the labour market conditions should not play a part in the scarring effects: the amount of the human capital decay only depends on the duration of non-employment, but is independent of the labour market circumstances. This paper explores the existence of scarring effects in Italy and whether their magnitude depends on the heterogeneity, over time and across regions, in the labour market conditions. I draw on a sample of Italian administrative micro-data. These data contain information on all the contracts that were signed and terminated in the period 2009–2018, relative to a representative sample of workers born on 24 dates of the year. The availability of individual data over a decade constitutes a value added of this work, since it allows us to evaluate the scars of non-employment in the medium and long term. In order to estimate them, for each individual, I compute the yearly average time spent in non-employment during the first three years after the theoretical date of graduation (the early period) and in the subsequent 6 years (the later period). Italy is an interesting case for two reasons. First, Italy is one of the countries with the highest youth unemployment rates and one of the most rigid labour markets among OECD countries (Cockx and Ghirelli 2016). If we consider the signalling theory, in rigid labour markets, employers have more incentives to screen job applicants before hiring, because they are forced into longer-term relationships with their employees. Thus, we can reasonably expect that the early experience of non-employment may inflict considerable damage on young people’s career (Kawaguchi and Murao 2014). Second, Italy is characterized by a strong heterogeneity in social, economic and labour market conditions across regions, which I exploit in order to understand whether the labour market conditions are relevant in generating variations in the scarring effects of early non-employment. Empirical investigation poses a major methodological challenge because of endogeneity. While being able to control for many individual characteristics, there may still be determinants of the individual propensity to be non-employed that remain unobserved. It may be that individuals who are unemployed in one period are in this situation because they have characteristics that make them particularly vulnerable to unemployment, such as low levels of motivation, unfavourable attitudes or a general lack of abilities. The unemployment risk for future periods will also increase if these characteristics persist over time. To avoid a spurious relationship between current and future unemployment, it thus becomes crucial to separate the differences in later non-employment, which are causally related to early non-employment, from the differences due to unobserved personal characteristics that are also correlated with early non-employment. A number of methods has been used in the literature to handle this endogeneity issue. Some studies relied on exogenous events (Jacobson et al. 1993; Farber et al. 1993), on the propensity score matching (Nilsen and Reiso 2011; Nordstrom Skans 2011) and on field experiments (Eriksson and Rooth 2014), but the majority of them chose an instrumental variable approach (Gregg 2001; Neumark 2002; Gregg and Tominey 2005; Ghirelli 2015; Schmillen and Umkehrer 2017). In addition, a part of the recent literature has given up on the attempt to estimate directly the effect of early non-employment on later non-employment, focusing instead on the reduced form relationship between labour market conditions at the time of graduation and subsequent individual labour market outcomes. In particular, this literature showed how adverse initial conditions push college graduates into initial lower-quality placements, which translate into long-term penalties on earnings or wages (Kahn 2010; Oreopoulos et al. 2012; Altonji et al. 2016). In this paper, I consequently follow a double approach: after showing the OLS results, I first consider the results of the reduced form relationship, estimating a linear regression model of the outcome of interest on the regional youth unemployment rate in the last year of school or university. In particular, I consider, for those with a high school diploma (degree), the regional unemployment rate of the 19–28 (25–34) age group with the same level of education, measured in the last year of school (university), before entering the labour market. The variation in the youth unemployment rates exploits differences among the 21 Italian regions, among the education levels of the individuals and among the two cohorts of new entrants in the labour market (respectively in 2009 and in 2010). I then move to the instrumental variables approach and, as an instrument, I used, as in the reduced form regression, the state of the labour market at the time of the individual potential entry. As regards the relevance of the instrument, this directly affects the labour market experience of those who are ready to enter and can be considered exogenous to the individual unobserved characteristics. However, it must be noted that the exclusion restriction assumption, i.e. the fact that the labour market conditions at the time of entry should affect later outcomes only through their effect on early non-employment, is likely to be violated. For example, poor labour market conditions at the time of entry may force a worker to accept a bad job early on, which then puts him/her to a path of additional poor outcomes, both in terms of employment and in terms of wages. The possible failure of this assumption imposes a certain caution in the interpretation of my IV estimates. The evidence suggests that the experience of youth non-employment leads to penalties in terms of persistent non-employment. According to the OLS estimates, each additional day of yearly non-employment during the first three years increases non-employment in the following six years by half a day per year. This negative effect is also confirmed by the findings of the reduced form and of the IV estimates, but its magnitude differs according to the methodology considered. Moreover, the results show that the past individual non-employment experience is more scarring in regions with good labour market conditions, i.e. in regions with low unemployment rates. On the contrary, the damage associated with past non-employment seems to be reduced if a worker is non-employed in an area with more difficult labour market conditions. This evidence of heterogeneity can easily be interpreted using the signalling theory. In fact, the scarring effects are lower in those regions where the experience of unemployment is considered part of a typical individual’s labour market history, and unemployment spells are not necessarily perceived as a signal of low productivity. Employers seem to tolerate longer past search duration when the labour market conditions are bad, while they become stricter when general unemployment is low or when things are getting better. These results can thus be helpful in getting a better understanding of which theory explains the existence of the scarring effects, for which there is no consensus yet, and are important from a policy point of view. The existence of negative effects of early experience in non-employment strengthens the case for policies aimed at addressing school-to-work transition and at reducing the incidence of youth unemployment. The rest of the paper is organized as follows: after a brief review of the literature, Sect. 3 presents the data and the main descriptive statistics. Section 4 describes the empirical strategy and discusses the identification issues, the results are reported in Sects. 5 and 6 concludes.",1
9.0,2.0,Italian Economic Journal,15 April 2022,https://link.springer.com/article/10.1007/s40797-022-00192-5,Interlocking complementarities between job design and labour contracts,July 2023,Luca Cattani,Stefano Dughera,Fabio Landini,Male,Male,Male,Male,"During the last decades the growing use of non-standard employment has received extensive attention in the literature (see, e.g. Deakin et al. 2007; Eurofound 2020; European Commission 2010). Millward et al. (2000), for instance, report that in Britain the share of workers hired on the basis of temporary as well as part-time and self-employment contracts has increased since the 1980s. Keune (2013), Cappelli and Keller (2013) and Allmendinger et al. (2013) confirm similar trends in other countries.Footnote 1 While the rising incidence of non-standard employment is often associated with the firm’s need to cope with an uncertain and volatile market environment (Kalleberg 2011; ILO 2015), recent works document that firm-specific factors can play an important role as well. Arrighetti et al. (2021), in particular, show that the asymmetric distribution of managerial resources accounts for large part of the within-industry heterogeneity in the use of temporary and fixed-term contracts and dominates conventional context-based explanations. In this paper we contribute to this line of research by studying how the heterogeneous use of non-standard employment is affected by the existence of interlocking complementarities between job design and labour contracts. We suggest that in presence of incomplete contracts and market uncertainty, firms face two alternative organizational equilibria: in one of them, job designs characterized by high routine task intensity are matched with an extensive use of non-standard employment; in the other, low routine task intensity is combined with a relatively small use of non-standard contracts. These complementarities exist because while non-standard contracts allow firms to adjust to external shocks, they also provide little incentive to invest in firm-specific knowledge. The cost associated with the lack of such knowledge is lower in firms with high routine task intensity, which are thus more likely to use this type of contracts. The opposite reasoning holds for the relationship between small use of non-standard contract and low routine task intensity. We build a formal model of such interlocking complementarities and test the related predictions using linked employer-employee (LEED) data which combines two sources: (a) worker- and firm-level information taken from the SILER-ARTER system, which collects all mandatory communications firms from the Emilia Romagna region (Italy) submitted to regional administrative offices in the cases of major employment events (e.g. hiring, firing, changes of contractual status) between January 2008 and December 2017; and (b) accounting and financial information derived from the AIDA-BVD database, which contains disaggregated balance sheet and profit and loss statement information for all Italian firms between 2008 and 2017. This gives us an open panel with detailed firm- and worker-level yearly information, including contractual basis and occupation. Moreover, we match this data with the INAPP survey on Italian professions, which links occupational titles included in the Italian occupational classification (in turn linked to the International Standard Classification for Occupation, ISCO08) to 255 variables measuring either importance and complexity of tasks, knowledge and skills related to each occupation based on the O*Net content model. In this way, we build a firm-level index of routine task intensity based on the within-firm distribution of occupations. The empirical evidence (pooled OLS, Panel FE and IV) provides strong support for our theoretical predictions: routine task intensity and the use of non-standard contracts are indeed positively associated. This result holds under different model specifications and following a wide range of robustness checks, including panel and IV estimations. We add to the literature in two distinct ways. First, we contribute to the stream of research that discusses the diffusion and impact of non-standard employment. In particular, many contributions investigate how this type of employment affects different components of firm performance such as job creation (Houseman 2001; Saint-Paul 1996), returns on equity (Lepak et al. 2003), productivity growth (Boeri et al. 2013; Ichino and Riphahn 2005; Valverde et al. 2000; Bardazzi and Duranti 2016; Lucidi and Kleinknecht 2010; Damiani et al. 2016), innovation and R&D (Kleinknecht et al. 2014; Reljic et al. 2021) as well as workers’ motivations (Blanchard and Landier 2002; Battisti and Vallanti 2013) and the propensity to accumulate firm-specific skills (Lepak and Snell 2002). Less attention, however, has been paid on the driver of non-standard employment in the first place. On this respect we expand on previous research by strengthening the idea that decision to rely on non-standard employment is not simply driven by the characteristics of the competitive context in which firms operate, but it is rather the consequence of broader evaluations concerning the organization of work, including the design of jobs and their complementarity with different contractual forms. Second, we extend the growing literature on the design of organizations in presence of techno-institutional complementarities (Aoki 2001).Footnote 2 Pagano and Rowthorn (1994) develop one of the first formalization of organisational equilibria in presence of complementarities between technology and property rights. A similar approach has later been used to study the source of organizational diversity in many contexts such as: knowledge intensive industries (Landini 2012, 2013; Gurpinar 2016a, 2016b), automation (Belloc et al. 2020); corporate governance models (Barca et al. 1999; Earle et al. 2006; Nicita and Pagano, 2016; Landini and Pagano 2020). Recently, Dughera et al. (2021) exploit a framework based on multiple organizational equilibria to study the relationship between the firm’s technological and hiring strategies. In this paper we rely on these previous works to investigate how complementarities in the organization of work contribute to explain the heterogeneous use of non-standard employment among firms. The remainder of the paper is organized as follows. Section 2 presents the theoretical model and the related predictions. Sections 3 describes the data and some initial descriptive evidence. Section 4 discusses the results. Finally, Sect. 5 comments and concludes.",
9.0,2.0,Italian Economic Journal,30 January 2022,https://link.springer.com/article/10.1007/s40797-021-00178-9,From Choice to Performance in Secondary Schools: Evidence from a Disadvantaged Setting in Italy,July 2023,Anna Bussu,Dimitri Paolini,Giuseppe Zanzurino,Female,Male,Male,Mix,,
9.0,2.0,Italian Economic Journal,18 January 2022,https://link.springer.com/article/10.1007/s40797-021-00177-w,Are Business and Economics Alike?,July 2023,Carmen Aina,Chiara Mussida,Gabriele Lombardi,Female,Female,Female,Female,,2
9.0,2.0,Italian Economic Journal,21 May 2022,https://link.springer.com/article/10.1007/s40797-022-00195-2,"Are Bankers “Crying Wolf”? Type I, Type II Errors and Deterrence in Anti-Money Laundering: The Italian Case",July 2023,Lucia dalla Pellegrina,Giorgio Di Maio,Margherita Saraceno,Female,Male,Female,Mix,,
9.0,2.0,Italian Economic Journal,28 April 2022,https://link.springer.com/article/10.1007/s40797-022-00194-3,A Short Note on Interest Rates and Household Wealth,July 2023,Riccardo De Bonis,Marco Marinucci,,Male,Male,Unknown,Male,,
9.0,2.0,Italian Economic Journal,01 February 2022,https://link.springer.com/article/10.1007/s40797-022-00183-6,"Voters’ Distance, Information Bias and Politicians’ Salary",July 2023,David Bartolini,Agnese Sacchi,Alberto Zazzaro,Male,Female,Male,Mix,,
9.0,2.0,Italian Economic Journal,13 July 2022,https://link.springer.com/article/10.1007/s40797-022-00198-z,Corruption and the political system: some evidence from Italian regions,July 2023,Vincenzo Alfano,Salvatore Capasso,Lodovico Santoro,Male,Male,Male,Male,,
9.0,2.0,Italian Economic Journal,06 August 2022,https://link.springer.com/article/10.1007/s40797-022-00206-2,Investigating Regional Disparities in Italy’s Well-Being Since Unification (1871–2011),July 2023,Francesco Maria Chelli,Barbara Ermini,Andrea Gentili,Male,Female,Female,Mix,,
9.0,2.0,Italian Economic Journal,16 June 2022,https://link.springer.com/article/10.1007/s40797-022-00196-1,Investment Subsidies Effectiveness: Evidence from a Regional Program,July 2023,Simone Chinetti,,,Female,Unknown,Unknown,Female,"Most developed countries have economic policies to revive and balance growth, especially targeting lagging behind areas. Italy is not an exception in this sense, given the dramatic and structural divide between the northern and southern regions of the country. First, the Great recession and then the COVID-19 pandemic have renewed the interest in the economic effects of public policies. Motivated by tight budget constraints, the importance of understanding whether these place-based policies accomplish their goals has grown significantly. Infrastructure investment, incentives to boost labour market participation and human capital, and subsidies to enterprises to innovate, move or remain in underdeveloped areas are usual features of these programs. In this paper, I provide novel empirical evidence on the effectiveness of place-based policies in lagging areas by studying whether public funding foster private firms investment in innovation. The objective of the analysis, thus, is to evaluate if regional funds can generate additional effects on innovative capital investments above those foreseen by the market. Public funding is usually aimed at influencing the allocation of investments and employment to improve competitiveness, growth and labour market dynamism in disadvantaged regions. Indeed, policymakers use financial incentives to change firms preferences and to push them to invest in projects that, without contributions, would be usually be abandoned. However, this complementary mechanism is far from occurring. The problem of additionality of public aids traces back to the presence of asymmetric information between the central governments, or the local administration in charge of managing the funds, and firms: if the policymaker knew the minimum incentive necessary to activate any (new) investment, the complementary mechanisms would be maximized, and the deadweight loss would be negligible. To draw causal evidence, I leverage the implementation of a regional subsidy program to foster innovative investments for small and medium firms (SMEs) in the Campania region, one of the Italian lagging behind areas, during 2014–2015. Specifically, firms were invited to submit proposals for new innovative projects. Only those scoring above a certain threshold received a subsidy, covering up to \(50\%\) of expenditures in innovative intangible and tangible assets according to the selection criteria set by the region. One of the interesting aspects of this regional policy, whose selection is based on competitive rankings, is that it is possible to build a control group based on the firms that applied to the program but are not among the “winners”. As highlighted by the literature, the use of this control group should strengthen the empirical analysis (Bernini and Pellegrini 2011). Indeed, the rejected pool of applicants may be close to a control group since it comprises firms that are sufficiently similar to the treatment group in terms of characteristics and includes eligible companies willing to receive the funds (Brown et al. 1995). Furthermore, this regional program qualifies itself as an appealing case study for at least three reasons. First, the program rules require that firms willing to be financed in developing innovative activities must operate (and be located) within the region boundaries. Hence, the policy’s local dimension allows me to remove much of the unobserved heterogeneity among enterprises that, instead, characterizes nationwide programs by comparing a sample of more homogeneous firms (subsidy-recipients and non-subsidy-recipients) based and operating in the same region and thus exposed (reasonably) to the same set of business rules and local shocks. Then, an additional program requirement obliged participating firms to request funding for brand new investment projects and develop them only with regional support (subsidy). That is, the program rules forbid firms from combining several public incentives. In this way, I am more confident of estimating a clean causal effect (if any) that comes only from the effect of regional subsidies on the level of innovative investment. Finally, the regional government pledged sizeable funds to foster investments in private firms. Indeed, about 22 million euros of public resources have been distributed to firms to induce them to increase innovative expenditures. To recover the causal effect of interest, I exploit detailed data on the regional program matched with balance sheet data from the AIDA database (managed by Bureau Van Dijk) for treatment and control groups. I employ a Difference-in-Differences approach by comparing the average innovative capital investment of subsidy recipients and non-recipient firms, identified by taking advantage of the assignment scheme of the regional funds before and after the program implementation. According to my estimates, the regional subsidy program has a positive, sizable and statistically significant effect on eligible firms’ capital investment in innovation. The coefficient measuring the causal effect of interest is about 51 thousand euros, translating into a notable relative increase in innovative capital of 1.5 times higher than the 2013 average investment level. Furthermore, by relying on a back-of-the-envelope calculation, I estimate an implied elasticity of 3 - a value somewhat higher but in the range of those documented in the literature (Hall and Van Reenen 2000). In addition, the sizable increase in innovative capital is compatible with the input-additionality mechanism since I cannot accept the hypothesis that treated firms increased spending by about the size of the regional funds they received. Also, I show considerable heterogeneity in the firms’ responses. First, I show that different levels of the awarded subsidy (proxied by the quartiles of its distribution) produce an inverted U-shaped investment response. Then, I conduct an in-depth analysis to understand to what extent firm size, technology level, and economic sectors may play a role in shaping the effect of the subsidy on innovative capital investment. Differently from the main findings in the literature, the heterogeneity analysis suggests that the additionality effect of the subsidy materializes only for medium-large firms and medium-large low-tech companies, with a relative increase in innovative capital of \(+100\%\) and \(+120\%\), respectively. Moreover, I perform a series of robustness checks showing that these results are adequately consistent across small bandwidth sizes around the threshold eligibility. Finally, I document that the program has a considerable indirect effect on low-tech medium-large service firms’ labour demand but not overall improvements in productivity. This paper has ties to two main strands of the literature. The first, and most related one, regards the empirical micro-evidence on the effectiveness of public programs in underdeveloped areas. For decades, economists have been debating the extent to which investment incentives have an economic payoff (see, for instance, King 1977 and Hall and Jorgenson 1969). Further, the regional science literature considers a significant issue of whether local iniquities can be ameliorated through public incentives (Harris and Trainor 2005; Gabe and Kraybill 2002; Glaeser 2001; Faini and Schiantarelli 1987). Despite a large body of research, few have focused on the effectiveness of investment incentives to firms located in lagging areas. Besides some recent studies’ increased optimism, findings remain mixed. Some studies show that capital incentives can prompt additional investment in subsidized firms (Criscuolo et al. 2019; Bondonio and Greenbaum 2014; Schalk 2000; Daly et al. 1993; Harris and Trainor 2005; Faini and Schiantarelli 1987), while others suggest intertemporal substitution effects (Bronzini and de Blasio 2006). Also, the impact of investment incentives on employment is doubtful, while that on productivity appears negligible (or even negative). About the Italian context, most of the available evidence has focused on the impacts of Law 488/1992. This regional policy has been the primary industrial policy targeted to manufacturing and extractive firms willing to invest in lagging areas. Overall, these studies show that the Law 488/1992 subsidies have positive effects on output, employment and fixed assets (at least in the short run) but a less significant increase in total factor productivity (see among the many Cerqua and Pellegrini 2014 and Bernini and Pellegrini 2011). In that respect, my study complements the available evidence and enriches it since my setting consists of a sample of potentially eligible firms operating in all economic sectors rather than in manufacturing and extractive industries only, despite being restricted only to one area of the country. Also, qualitative reviews of the literature, both at the international (Becker 2015; Zúñiga-Vicente et al. 2014) and national levels (Bocci et al. 2021; Cerqua and Pellegrini 2020), confirm the extreme heterogeneity in results that further require investigation. The available evidence suggests a tendency to find positive effects on firms’ employment, investment and survival, mostly limited to small firms and enterprises in low-tech sectors, while questionable effects on productivity. Finally, the second strand of the literature to which this paper is related regards the management and allocation of public resources. Indeed, this study permits to shed light on the effects of place-based policies managed by local governments, that, however, have always had scant attention from the impact evaluation literature, despite the prominent role of local governments in shaping the local economic conditions and the relatively great bulk of public resources that the private sector absorbs from the public sector (Kline 2010). The remainder of the paper is structured as follows. In Sect. 2, I illustrate the features and characteristics of the investment subsidy program, by also describing qualitatively the technological nature of the subsidized projects. Section 3 describes the data and the empirical strategy used to recover the causal effect of interest. In Sect. 4, the main results are discussed. In Sect. 5, the estimates relative to the heterogeneity analysis are presented. Section 6 presents some robustness checks. In Sect. 7, I explore whether the program had indirect effects on other firms’ outcomes. Finally, Sect. 8 concludes.",
9.0,2.0,Italian Economic Journal,18 September 2021,https://link.springer.com/article/10.1007/s40797-021-00169-w,The Chinese Inland-Coastal Inequality: The Role of Human Capital and the 2007–2008 Crisis Watershed,July 2023,Emanuele Felice,Iacopo Odoardi,Dario D’Ingiullo,Male,Male,Male,Male,"Two forms of inequality affect the Chinese territory. The first form, the best known and studied, is the one between urban and rural areas (Lu and Chen 2006; Sicular et al. 2007): for instance, by 2017 in the rural areas the per capita disposable income was around 39% the one in urban areas (2017 data from National Bureau of Statistics of China). The second form of inequality, studied in this article, is between coastal and inland provinces (Yang 2002). In this case, we observe a per capita GDP (Gross Domestic Product) in the poorest provinces (Yunnan, Gansu) being less than 30% of the richest ones (Beijing, Shanghai) (2017 data from National Bureau of Statistics of China). These two types of inequality affect the national Gini index: by 2016, it was as high as 38.5 (World Bank data), positioning China above many Western countries and halfway between the BRICS countries.Footnote 1 With respect to these forms of inequality, there are at least two important qualifications. The first is that while urban–rural inequality appears to be quite stable, the coastal-inland one has risen strongly in recent years (Kanbur and Zhang 1999), thus deserving in-depth studies. The second aspect concerns the relationship between inequality and economic growth: this is indeed very controversial, with the literature being still inconclusiveFootnote 2 (see for instance the literature review in Chen 2010). Excessive inequality, among people or between areas, could be a limit to development due, respectively, to observed and perceived disparities inducing behaviors unfavorable to economic growth (see Barro 2000) or, as in this study, to the delay in the long-term development process of a part of the country (e.g., Capello 2016). For the second type of inequality, we refer to the different local economic strengths that shape the diverging development paths and thus imply the inland-coastal dualism in the Chinese case. In this sense, a “positive” stimulus to change could result from a wider exogenous “negative” event. In particular, considering the rapid evolution of the Chinese economy and the changes forced by the international crisis, we wonder if these events can lead to rethinking local development paths, fostering greater attention on the diffusion of (advanced) education. We expect that the consequences on HC may in turn have consequences on the dualism: how did these two macro areas respond to the 2007–2008 crisis, how the role of HC could have changed according to different development paths? The focus on HC is due to the fact that the growing differences between inland and coastal areas can be explained, at least in part, by the different local endowments. In this respect, HC may play a major role and should be thoroughly investigated for at least three important aspects. First, HC is a crucial determinant of regional economic performance, in the present as well as in the past (e.g. Felice 2012). Second, a large part of the within-country inequality in income is influenced by education (Acemoglu and Dell 2010). Third, the economic contribution of HC could be more important in the less-developed areas of a country (Gitto and Mancuso 2015; Felice 2018), which means that in the case of China it should be an important lever for the “poorest”. In addition, we expect that the response of the Chinese economy to the widespread of advanced education could still be controversial in the analyzed period. On the one hand, the rapid Chinese economic growth would presuppose the contribution of HC as a key economic strength; on the other hand, the China’s development level may not be yet able to fully exploit capabilities of educated workers as an economic strength. In fact, the diffusion of primary and middle school education was necessary for the rapid development of the Chinese economy until the early 2000s, while, more in general, the local labor force does not yet have the characteristics of those of high-income economies, characterized by high productivity and continuous innovation (Li et al. 2017a). The focus on the crisis period is influenced by the fact that China has not suffered from the serious consequences of the 2007–2008 events observed elsewhere, despite a sharp decline in exportsFootnote 3 (Wen and Wu 2019). Nonetheless, we may expect a change in postcrisis economic strengths: for example, a stronger focus on a resource capable of creating innovation and value in the long run, such as HC, as opposed to an export-led strategy based on low wages. This change could favor provinces not uniformly, following differences in past and current investments in education and training. Two comparisons can help us to answer our questions, considering the 20-years period 1998–2017. First, a comparison of panel data analysis on the richer and on the poorer provinces (the first-level administrative division) allows us to identify the differences in local economic strengths between these two contexts, by observing their effects on both the variation and on the absolute values of provincial GDP. We expect that a different role of HC (e.g., a greater effect of educated workers in the more advanced provinces) should help explain disparities in economic performance, although its effects may not be observable in the short term (Chang and Shi 2016). Second, the analysis is applied to two periods, before and after the 2007–2008 crisis (1998–2008 and 2009–2017 years). This comparison serves to studying whether or not recessionary events have affected the role of HC—proxied through enrollment in advanced education – at the local level. It should be stressed that, thus far, the results of the literature on the relationship between local economic endowments and regional inequality in China are controversial. For example, concerning the 1982–2005 years, Hao and Wei (2009) found that physical capital intensity and total factor productivity are relevant in regional income inequality, while a small role is played by HC. On the contrary, Wang and Yao (2003) found a relevant economic role and a huge potential (Wang and Yao 2003: for the 1978–1999 period) and an increasing role in fostering economic development (Zhang and Zhuang 2011; 1997–2006 period) for HC. So far, however, in the studies about the evolution of the Chinese regional dualism two key aspects are still missing: the role at the local level of other conditioning factors, those characterizing the inland/coastal divide, and the consequences of the 2007–2009 international crisis. In this sense, our article contributes to the literature in proposing an “extended” framework of the policies to improve advanced HC—which are, to our knowledge, those more useful for economic development (those with expected higher returns, Barro and Lee 2013). Our measure of HC includes—in addition to the well-established indicator of enrollment in higher education (e.g., Asteriou and Agiomirgianakis 2001)—also the level of enrollment in senior secondary education (i.e., the second level of secondary education, after the end of compulsory school, see Bush et al. 1998). In studying the Chinese economy (e.g., Tuan et al. 2009), the secondary and tertiary enrollment levels have been aggregated and tested as HC proxies. Our new proxy, however, represents an extended level of advanced HC. It could have an important role for a fast-growing economy, which is not yet able to take full advantage of the so-called knowledge workers, as the Western countries are. Finally, we take into account the endogeneity issue that stems from the reverse causality between HC and GDP, and we also test for the presence of spatial effects among provinces. The paper is organized as follows. In Sect. 2, we present an outline of China’s modern economic development, taking into account the events that could influence the differences between the coastal areas and the hinterland ones, with a focus on the role of higher education (Sect. 2.1). In Sect. 3, we explain the models used in the analysis and their application to two groups of provinces. The variables, proposed by the economic literature, are presented in Sect. 4, followed by the results in Sect. 5. In the concluding Section, we put forward some policy implications.",
9.0,2.0,Italian Economic Journal,23 July 2022,https://link.springer.com/article/10.1007/s40797-022-00202-6,Urban Non-urban Agglomeration Divide: Is There a Gap in Productivity and Wages?,July 2023,Eleonora Bartoloni,Andrea Marino,Davide Romaniello,Female,Female,Male,Mix,,
9.0,2.0,Italian Economic Journal,22 July 2022,https://link.springer.com/article/10.1007/s40797-022-00204-4,Social Responsibility and Urban Consolidation Centres in Sustainable Freight Transport Markets,July 2023,Daniele Crotti,Elena Maggi,,Female,Female,Unknown,Female,"Urban freight transport (UFT) plays a crucial role in the achievement of Sustainable Development Goals (SDGs), especially those related to climate action and sustainable cities and communities (United Nations 2015). The growth of e-commerce and B2C sales (intensified with the Covid-19 pandemic) has dramatically expanded freight urban transport flows, increasing congestion level and worsening air quality in the cities (Sarkis 2020; Kohli et al. 2020; Holguín-Veras et al. 2016; McKinnon 2010). In Europe, road transportation indeed causes the highest rate of overall transport emissions: around 72% in 2019 of all domestic and international transport GHG (EEA 2020). According to the estimates, after the drastic fall of emissions in 2020, due to the reduction of transport activities during the initial spread of Covid-19 pandemic, we would expect a significant rebound in transport emissions by 2040 compared with 1990 levels (EEA 2020; see Fig. 1, grey line). However, only the application by the European Member States of additional measures to reduce transport emissions could help in reducing them after 2022. Since the majority of planned or implemented measures regards road transport, the share of this mode is expected to decrease faster than other transport modes, but this process is strictly related to the level of effectiveness of the sustainable transport policies that are now concentrated more on passenger transport than on the freight one. As a result, the sprawling of logistics hubs and inland terminals in the nearby of or within the cities, namely Urban Consolidation Centres (UCCs), aimed at increasing the efficiency and sustainability of transport systems covering last-mile markets, remains a crucial issue (Vidal Vieira and Fransoo 2015; Dablanc and Rakotonarivo 2010; Dablanc 2007). Projections of transport-related emissions (EEA 2020) (https://www.eea.europa.eu/data-and-maps/indicators/transport-emissions-of-greenhouse-gases-7/assessment-2) Since a mix of environmental policies has been widely advocated to reduce the road traffic pollution, according to the polluter-pays principle (EC 2004) several municipalities have widely imposed market-based restrictions to limit the number of polluting vehicles and to let UFT operators internalize negative externalities on the air quality (Müller and Petit 2019; Lebeau et al. 2017). Along with designing pollution charges to enter Low Emission Zones (LEZs) or similar measures (De Marco et al. 2017; Danielis et al. 2013), several city councils tested Urban Consolidation Centres, i.e., facilities located in the proximity of a city centre, which collect last-mile deliveries outsourced by international or national LSPs and distribute them to the final urban destinations with fully loaded small vehicles, which are normally electric or of low emissions (Cleophas et al. 2019; Holguín-Veras et al. 2015; Crainic et al. 2009). According to the most quoted literature, UCCs are considered generally effective in reducing CO2 and NO2 emissions (Katsela et al. 2021; Holguín-Veras et al. 2015), but the long-term survival of those platforms is harmed by some management and financial problems: an unstable demand from LSPs in the long run and double marginalization (Marcucci and Danielis 2008; Browne et al. 2007). Regarding the first point, branded LSPs do not have the right incentives to outsource deliveries (Holguín-Veras and Sánchez-Díaz 2016; Soh et al. 2015), also because thus far the freight transport providers had a weak orientation towards CSR-based pricing schemes. Actually, even though these strategies are different for each industry (Sweeney and Coughan 2008; Ciliberti et al. 2008), in the supply chain and logistics sector they consist in: (1) the size and complexity of transportation networks; (2) the necessity to provide customers with personalized services; and (3) the need of cooperative solutions with other industry players and local stakeholders involved in the supply chain, including actions to look after the whole consumers’ welfare (Isa et al. 2021; Gatta et al. 2019; Paddeu 2018; Marchet et al. 2014). Concerning the second problem, adding up a distribution layer to existing supply chains tends to raise final freight transport rates, thus reducing the cost efficiency of whole parcels distribution (Janjevic and Ndiaye 2017). In this paper, we consider how the enhancement of CSR-oriented policies might make the UCCs more attractive for the LSPs. Generally intended as pricing strategies increasing the utility of all the final consumers of goods and/or services (Porter and Kramer 2011), socially responsible and stakeholders-related policies in the UFT markets recently emerged as of key importance, also due to market signals coming from more environmentally conscious consumers (Björklund and Johansson 2018). Following a recent and growing literature about the strategic role of CSR in oligopolistic competition (e.g., Planer-Friedrich and Sahm 2020; Kim et al. 2019; Hirose et al. 2017), the paper addresses the following questions: (1) what is the UCC impact on the competition among LSPs in the UFT markets?; (2) would CSR pricing schemes increase the LSPs’ demand for UCCs?; (3) considering UCCs, is there a competition interplay between environmental policies and CSR pricing schemes? After reviewing some successful UCC experiences considering CSR practices, we put forward the idea that the application of those strategies could enforce the public environmental policies, by giving a contribution to the challenge of fighting the climate changes in urban areas, decarbonising freight transport market. Consequently, UCCs could become a long-term winning solution for UFT sustainability (Lagorio et al. 2016). Moreover, referring to competition, CSR practises by UCCs could answer to the need of LSPs of increasing their environmental commitment in the light of their customers. Therefore, UCCs could stabilize their own demand and become more attractive than in the past (Björklund et al. 2017). To the best of our knowledge, this is the first study considering the interplay between CSR strategies and the oligopolistic competition among LSPs in last mile freight transport market. We designed an Hotelling-like model of UFT competition, based on the assumption that CSR-oriented firms maximize a combination of profits and consumers’ welfare (Planer-Friedrich and Sahm 2018; Königstein and Müller 2001). The paper is organized as follows. In Sect. 2, a background on CSR in logistics and strategic competition is provided, including a qualitative review on CSR strategies in UCC cases. A model to study the impact of CSR strategies by UCCs on the competition among LSPs is presented in Sect. 3, while Sect. 4 concludes the paper with final remarks.",2
